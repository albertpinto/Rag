 Alright guys, let us review what we did so far. Last time we did two things. We did, if you remember, we did boosting and we did a little bit of the recommender system or at least we got started with the recommender system the main point with boosting was and we in particular took gradient boosting as an example boosting is a process of creating a strong learner using weak learners in a way that you grow the you learn slowly you take a weak learner you fit it to data you see what mistakes remain when it makes the prediction in the case of a regression problem for example you take the errors from a model so far and then you use it to build another model, another weak learner, whose job is to be able to improve upon the model by just fitting to the errors. So if you look at the screen, we said that we have the overall model is built out of pieces. It is built out of, let's say that any given level, you take the model that you have built so far, and then you add one more model to it, one more weak learner to it, that is fitting to the residuals, for example, in regression, residuals left behind by the previous model. And you attach a damping factor so that it doesn't go in overfit, but broadly speaking, that's what you do. So the first model that you build, like the zeroth model that you build is as simple as it can be. It is simply, it is basically a baseline class, a baseline regressor. It says that ignores all the predictors it says that the predicted value is the average of the learning data target value all the levels of the target value values that you get in the training data you just average it and that's your prediction well it is a very weak predictor but it at least it makes sense. If you have no inputs, the best that you can do is predict the average. You do. Then what do you do? Next, what you do is, so that is the prediction from the zeroth model. You look at the residuals that you're left with, yi minus y bar, that becomes the residuals and thereafter you try to fit a function to the residuals when you fit a function to the residuals a learner to the residuals that's what your learner is now your learner quite often people in boosting they tend to use decision trees but it did not be you could use what you want if you use that and fit it you're not fitting it to the Y, the actual labels, you're instead fitting it to the residuals from the zeroth model. You will get another model now, F1, a decision tree, let us say, for F1, a weak learner for F1. Now you have F0 and F1. You would put it together, F0 F1 to get the overall big model which is F0 plus F1 up to a damping factor. What will that model do? That model will now make predictions. Once it makes prediction, makes predictions, it will have its own errors, own predictions. Those predictions will have its own errors and those errors now you'll fit to f2 and you will keep on going up to a certain extent you can choose M is equal to 20 or 30 whatever it is 100 you can keep on going sequentially building building an ensemble with weak learners are we together guys so far this is the basic idea that we said we are pursuing. Now with this basic idea, when we apply it in practice as you will see today, it becomes quite simple. This is the gradient boosting, it's accelerated version, massively parallelized and optimized version is XGBoost. These things work very very well. Now that was the process of boosting which is different from bootstrapping or bagging. Bagging is bootstrapping and aggregation, simple aggregation. In bagging the advantages you can build models in a massively parallel way. So for example, when you're doing parallel computing, if you look at big data frameworks like Spark, for example, you look at the history of Spark's machine learning library, MLlib, and you realize that they had random forest ever since the early days of machine learning, early days of Spark, sorry. In one of its earlier implementations they already had random forest why random forest is very in a way in many ways straightforward to parallelize if you you grow a lot of trees in parallel on bootstrap samples of the data and then you can do uh sgd uh stochastic gradient descent of some form and you do have the implementation essentially in place. Boosting is a little bit more tricky. How do you do sequential on a massively like how do you get a parallelism or distributed computing when you are dealing with something like boosting? It's a little trickier. And you can see that it showed up in a framework like Spark after considerable time it showed up. So then people worked hard and they figured it out actually, some very clever fellow figured out how to do, this is the extreme gradient boosting, how to do a parallelized implementation of it. And then now people have tried to do more of it. They have put it on GPUs. GPUs are extremely good with parallelized computing or distributed. This sort of divide and conquer approach. And for example, CatBoost, another variant of it, and very, very good variant. It can run on your GPU, your your video cards gpu there's gpus and r if you can push a computation onto gpu a machine learning computation onto a gpu you clearly have a winner because gpus do matrix manipulation orders of magnitude faster, sometimes hundreds of times faster than the most powerful workstation grade general purpose processor. For example, Intel Xeon versus GPU, whichever GPU. So let's say commonly in the data science machine learning, you tend to use nvidia i don't know why others haven't caught up yet but nvidia is very very popular so you take a typical gpu like rtx 6000 it's pricey rtx 6000 or rtx 8000 will easily8,000. So these are not small change, but the computational benefit it will give you is almost a hundred acceleration over your typical Xeon processor. Now the Xeon processor is a general purpose processor. It can do very complicated instructions, whereas the computations on the GPU are very very simple in many ways. Their multiplication of matrices and so on and so forth. But for the things that the GPUs do, they do very very well. So now there are implementations of these things on the GPUs. Of course our whole next bootcamp, the TensorFlow is all about GPU computing we'll see sorry deep learning we'll do it using PyTorch principally actually a tensor flow also we will do but not to as an extent if I thought is more neat like beginning to feel more natural if you have been doing numpy which we all have been doing in the first in the second workshop. PyTorch is almost a no-brainer with very little learning curve. You become familiar with it. And that is its great power. It's very expressive, very simple framework. TensorFlow is quite powerful and has an equal mind share, but it certainly feels like a quite different metaphor for programming compared to numpy and so forth. So anyway we'll come to all of that in the next. So besides bagging and boosting what else do we have? We have stacking. Stacking is taking some really well tuned algorithms. You go and train your independent learners if you so want you can even take some strong learners though and then what you can do you can stack them you can create a last level learner here what does the last level learner do? It treats the predictions of each of the learners as its input and says, can I see a pattern in the way they're predicting and the strengths and weaknesses in the prediction? Maybe some learner is predicting well in some region and not in other regions of the feature space. So can I do a further learning on the learners themselves. And when you do that and then make a final prediction, the most obvious example would be just a linear combination, just some weighted combination of the predictions of the other learners. Giving more weightage to a learner, you trust more and less weightage to another learner and so forth. You could do that for regression, for classification, you can do some sort of a sigmoid of that. But more and more, what is common is not to do something linear, but to use one of the more powerful, like a really powerful algorithm at the tail end of it. And what you put there typically is a SVM or a neural network. You put a few layers of a neural network or sometimes just one layer of a neural network. And we find that you can sometimes, not always, sometimes get a pretty good prediction from the sum total of all of it. So this is called stacking. Now stacking is of course much more sophisticated. A lesser version is just a voting classifier. Voting classifier just says you create an ensemble, but you make it out of all sorts of different algorithms. You still do a committee, a jury or a voting, but one may be a decision tree, one may be a support vector machine, one may be something, HG Boost, one may be logistic regression, and one may be a LDA or a QDA, and so on and so forth. And you take a whole bunch of classifiers and you just take a direct vote. So that would be the equivalent of saying, a voting classifier is the equivalent of saying that all of these votes, these weights are essentially one. And you're just taking the average of them, or rather one over n, n being the number of classifiers you have in your model. And so you're just taking a direct average of these. So that's a voting classifier, essentially bagging the response, the predictions from each of them. Now, so we learned three ensemble methods. We learned bagging. One example of this is random forest. and then the extremely randomized trees and so forth, et cetera. I won't write all of them and boosting, gradient boosting. As I said, Adaboost, I didn't emphasize too much. What Adaboost does is it takes a slightly different approach. It grows a tree, and then the mistakes are amplified to build the next tree, and so forth. So you keep on composing, but by amplifying the mistakes. I didn't go too much into the detail. It's usually other people would perhaps start by teaching Adaboost first and then doing it. But I thought because gradient boosting is so very dominant these days, let me stay with the main one, but okay there it is AdaBoost. AdaBoost and XG. XGBoost is essentially just a very fast implementation, fast gradient boosting. Right and then there are other variants of it, like for example, CatBoost. I believe Dennis, you used CatBoost and got pretty good result on your Higgs boson, isn't it? It is a very strong classifier. A very strong classifier. So just to give you a perspective, the project that Dennis took up was the Hicks was on and he's already getting a pretty good accuracy of 83% isn't it? 83, 84? I'm looking at 84. 84 excellent. Which is the state of the art isn't it the leaderboard is at 84. uh the measurement is actually called um the approximate median significance has its own formula but you know 84 is pretty good yeah pretty good what is the highest there What is the highest there? For EMS, it's like 3.8 or something. Okay. Yeah, like I'm not sure about accuracy though. Okay. All right. So here we are. Then after this we did another topic. So today our lab is on boosting. We won't get time to do the recommender system lab. That is actually part of the math course. I will do it there. The recommender system was based on the idea that you can do user-user similarity or item-item similarity, but sometimes just by observing users engaging with items, either listening to music or watching movies or buying things on an e-commerce site. By just looking at the interaction matrix here, you can look at the interaction. That is transactional data. Maybe in some database it says that this user bought this thing. Out of that data, you can create a matrix. The matrix of the user interaction with items. This user bought this and this item and maybe give this rating for a movie or a song or something like that. Based on those things, you can ask a question, you can make a hypothesis that the items have certain traits and the user has certain taste for those traits. And can we find that low dimensional representation? Here obviously we took an absurdly low dimension, a two dimension. We said a movie can be a thriller or not, and it could be humorous or not and so we put all the movies somewhere on the spectrum two dimensions are obviously absolutely low they don't capture all the sort of traits of a movie but it sort of illustrates the point so every item can be projected into the space and then every user can be projected into the space based on their taste. So items have traits and people have taste. And so you can see that there's a person who will like Sleepless in Seattle quite a bit because of their choice of a certain degree of humor and a certain action thriller, like the level of action they want to see in that. thriller like the level of action they want to see in that so this is the main concept now the hypothesis that you make is that if you represent a user the taste of a user the degree of taste for each of the traits you can represent it as a vector those are the attributes of each user how much Stacy a user has for humor, for action trailer, for let's say this is action trailer, this is a human and so forth. And likewise, every item can be represented with as a column vector saying how much of humor action trailer it has and how much of humor it has. And therefore the inner product could be part of as the cosine could be thought of as a good indicator. A pretty good indicator of How much a user is likely to Resonate with a certain item. So this space, this hidden space of taste of traits that you say, the more formal word mathematicians use is latent space. Latent means hidden. See, your matrix is the obvious data. This is transactional data. If you look at it, this is transactional data. But you're hypothesizing that the reason people buy things or engage with certain items or what certain use is because items have certain traits and users a certain degree of taste for each of those traits. Vaidhyanathan Ramamurthy, And so you have a latent space, not just two dimension, but typically 10 or 20 dimensions or hundred or 200 dimensions which are true representative of the item, but it is that which is not there in the description of the item. but it is that which is not there in the description of the item right but it is something more abstract and that emerges that pattern emerges only when you look at a lot of interaction data and how do you go find it there are methods like alternating least squares and we do singular value decomposition and so forth and when you do that you're able to you're able to pinpoint a user in the latent feature space and you're able to pinpoint an item in the latent feature space then you can look at similar items in that space you can look at similar users in that space you can recommend items to a user by looking at what items surround the user you can look at users to whom you would like to sell the item by seeing what users surround an item so you can run a campaign from a marketing perspective to sort of advertise those items to those users recommend those users to buy this item with perhaps greater degree of success than just brute force broadcast advertising. So that is the world of personalized advertising. That's where it comes from. That's where the personalized ads come from. So we live in a world of personalization. This says e-commerce would not be possible today without that because e-commerce inherently implies that these big giants the walmarts the amazons they have an irrationally large number of items in their inventory we are talking about 100 million items or something like that when you have so many items you can't really have a web page displaying them even by categories in any category there would be a very very large number of items and there'll be way too many categories so recommender systems are actually pretty crucial to the e-commerce world a lot of people purchase things that they have been recommended and that drives a lot of revenue and traffic so that's that for Netflix of course but one of the big selling points was that they could they could tell users that you like this movie they would watch it and they would actually find it useful to their liking. So that's that. Well that's a summary of what we did. Now I would like to move on to the lab part. Any questions before we move to the lab guys? If anybody has a question, please ask them. Asif, so for the loss function for the boosting, so is there a function called like the boosting loss function the loss function is not usually specific to rather method yes and no see think about it regression has a well-defined loss function right yeah which doesn't terribly depend on whether you're using a decision tree or you're using a linear regressor or polynomial regressor. It is y minus y hat square. It is the error term. To it you can add further regularization terms, et cetera. For decision trees is the number of nodes. In gradient boosting, for example, the extra term that you add is the damping factor lambda that's your hyper parameter to regularize it so that is the loss function there the loss function likewise you can write for classic fire now what happens is that the answer to that roughly is yes in the theory when you do gradient boosting you can once you write the loss function and you look at the residuals and you train your model on the residuals there is the gradient of this sum loss over residuals and so forth. There are a few bits of formula and a bit of mathematics unique to gradient boosting. I did not cover that because that will get us into all this business of gradients and so forth. There is a little bit of that, but if you have to look in broad strokes, your approach should be that yes, every algorithm has associated with it, its specific loss functions. But one simple way to go around, at least at this level, is to know that for classifiers, the loss function would be the cross entropy plus some regularization term and for for regression it would be some squared error plus some regularization term and invariant of what algorithm we use ultimately in broad strokes this is it are we together now when you do other boosts and when you do, for example, gradient boosting, there are nuances and they affect the mathematics of it. Like for example, when you do decision tree, you go through a process. You don't directly, I mean, you do have a loss function. Yes, that's true that you're trying to minimize, but you also go about it using you know entropy the the entropy and the information gain or the genie the index value is changing it where does it have the best bit points and so forth so those are the mechanics of it the internal mechanics by which you bring down the loss you do have a loss function and then you bring down those loss values. So for each algorithm, there'll be its own special bit of mathematics. Like for example, for support vectors, we saw the, we saw how we wrote the constraints and how we wrote the stuff in search for the maximum margin hyperplanes. So those things do remain. So yes, gradient boosting also has its own bit of . Okay, yeah, that makes sense. Thank you, Asif. Because boosting is computationally intensive, is this a classifier that we use secondary to optimization from others or can we just go straight to boosting right away? Actually, XGBoost and this CADboost, they're very fast actually. We'll see this in the lab now. Let's look at it in the lab. You'll realize that they are very very fast. Having said that, remember gradient boostingand Oswal, He or She else they form a baseline. So let's take an example. Suppose you have a model, a simple interpretable model, and it gives you 94% accuracy, right? Or precision or recall or F-bend score, whichever metric you care about. Now, let us say that the gradient boosting gives you 94.7% accuracy or the metric value, whichever metric you are pursuing. The question needs to be asked, should I use this black box model? And even if you don't, does it make sense to use the simpler model at least for explainability, for interpretability. To cross-check if in all the regions of the feature space, the predictions of the two models match. If they do match, you might still use, let's say, a black box model, boosting, backing, all of this. But you may choose to use the more interpretable model when you're having a conversation with business to explain why certain things are happening isn't it so for example if you're an entrepreneur trying to sell ice cream on the beach if it can predict what the temporary what the amount of ice cream you'll sell on the beach would be the entrepreneur may be pleased that you made an accurate prediction he has to buy she has to buy a finite amount of ice cream and not have it go waste or lose business both ways on the other hand the entrepreneur will have this nagging question in her mind that what if the wind changes right because whether predictions are not exact so what if the temperature is a little bit higher what if the weather changes what factors really affected it another basic question is that if you have an interpretable model you can ask good ask me, what should I do to increase my business? Do you see that? Isn't it? So interpretable models go a long way, much further than black box models. You use black box models by compromising on interpretability to a large extent. And when you do that that you lose a lot of value your approach should always be to start simply and that's what i've been trying to do if you notice that in this workshop i have taken you and emphasized feature engineering and showed you that at least for the river data set none of the black boxes beat feature engineering. Do you remember seeing that guys? For the river in the flat, none of the black box models, random forest, vector machines, boosting, we'll do boosting today and see. They don't beat it. They at most come close to it. So feature engineering, thoughtful feature engineering, it still remains the gold standard. Right? So keep that in mind. Never forget that there is a no free lunch theorem. So we will go back to our river dataset. Just to remind you, the river dataset was, what was it? Yes. The river dataset looked like hang on the river data set is visualize the data set yes so if you look at this visualization just as a reminder this was our data set and with this data set we have we call this inner region the river and then this the sand that's the intuition we use to make progress with it now as we went through this workshop progress with this. Now as we went through this workshop, as we learned new algorithms, we tried to apply each of these algorithms to this data set. And it sort of was a reference point to see how these algorithms work. I hope by now you must have seen that with scikit-learn, for example, With scikit-learn, for example, going from one algorithm to another from a pure lab perspective, from a pure code perspective, often was as simple as just changing the name of the classifier. You did not have to do much. Import the classifier and just use it. And we could do, because it's a nonlinear, we could do it using feature engineering and logistic regression itself, which is just as a recap, this is it. You saw us do the the, and you saw us do polynomial regression, classify logistic with polynomial terms. We did a more feature engineering approach. We did all of this. We did a plot of the we extracted the river. If you remember, we looked at the distance from the river. So this was the best. This was a gold standard because if you remember the goodness of this model was Jay Shah, MA, RNIDA 1C- If you looked at how well this model was residual the residuals are perfect if you get this approach. And with this, when we got a perfect map of the river, and from there when we did the feature extraction, this was our goal. goal we said if we extract our features well then we can build a very simple in fact one-dimensional logistic regression model nothing can be simpler we transform the problem to a very simple problem which was a classifier in one dimension the dimension being with the feature being distance from the central river. And when we did that, we got a confusion matrix that was pretty good. This is a good matrix, as you can see, of diagonals are rather small. And the classification report said we have F1 score of 84 and 92 percent and all things taken together the 89 percent is what we came out with as the weighted average so weighted average of accuracy is 89 percent now for one we got for sand we got 92 percent accuracy for river water we got 84 and we got a pretty impressive au roc curve area under roc curve just as a recapitulation what is the area under roc curve should it be more or less guys do we remember that should be a lot more bigger we want this thing to write for a very small false positive rate we want the true positive rate to achieve close to one isn't it because between true positive and false positive there is always a trade-off okay you want a very high true positive rate by giving up by having a very small false positive so that is that and so you want the area and IC curve to be high. This is a report on how much misclassification we got. Then we started trying all the algorithms we learned in this course with their decision trees. Decision trees also gave us pretty good result, but not as good. A simpler decision tree, for example, gave us a weighted accuracy of only 77% a 12% less accuracy not as good decision trees tend to fit hard the area and ROC curve doesn't look so good by the way you guys can play with it if you make your decision tree more complex this results will improve. You can try that a play with the parameters of the model that have a parameter super Sajeevan G. Then when you visualize. Then we went to random forest and we said if we use a random forest. How would it look Sajeevan G. When we tried to random forest. It turns out that random forest pretty much achieved almost as good a result as feature engineering this one is not as good a little bit worse but give or take it's pretty much gets close to it it's a pretty good number the area and our secret is pretty good so between random forest and feature engineering what which one would you prefer feature engineering yes because it's an interpretable model so given two models which which are equally good at prediction you always pick the simpler model then you do the feature importance of obviously X1 is more important because the whole waviness is along the X1 direction. Then they are both important. Then you tried support vector machines with the RBF kernel. When you do that, once again, you look at the accuracy report it turns out that the SVM by default parameters didn't do very well but I left this as an exercise for you that tune the SVM and see if you get better results the hint was that it can do very well it can do state-of-the-art results like other but you will have to tune this and so that was an exercise for you guys to learn to tune the SVM then you you have the reports by now you must be noticing that you're falling into a pattern irrespective of the classifier your model diagnostics remain the same the methodology remains the same and the code for changing the classifier usually is not a big deal this is the precision recall curve by the way this is the feature importance the shapely value that i talked about so today i'm sorry i'm yawning today i had to i had to be up since six in the morning after sleeping at two. So okay, here we go. XGB Boost. What does this stand for? XGB. Extreme Gradient Boost Classifier. So this is a pretty state of the art these days, DES and CAD Boost. Notice something very interesting. It fit into the model with just 190 milliseconds, which is very, very fast. Not only fit to the model, it even did the prediction in that same amount of time. So obviously boosting this XGBoost, it doesn't eat up too much of your time. It is actually pretty fast. You've got the job done. What is its accuracy here? Oh look at this. This is also comparable to random forest. It got this with same good accuracy. This here missed a point, but pretty close to state of the art. You get good results with this. The ROC curve also looks very good. Very nice. But guys, do you notice something very interesting? After learning the whole theory of XG boost and gradient boosting and so forth, when it comes to the doing of it, this was, sorry, this was pretty straightforward. Do you notice that? The only line different is this, the constructor. We are creating a XGB classifier, extreme gradient boost classifier, rather than SVM or a random forest or so forth. Everything else remains the same this is something very nice about scikit-learn and modern libraries they help you interchange classifiers very transparently very sort of easily you can plug in different classifiers into your model and do that now obviously i haven't gone into all the complications here, namely the hyperparameter tuning and so on and so forth. That's where a lot of the grunt work remains. Once you get good results, then the business wants you to make the best possible model, which means that you sit down and you start doing the hyperparameter tuning. Or it used to be what you would do in a re so one of the things i was leading to is we wouldn't have time in this workshop but i'll be giving a session on something called automated machine learning or maybe a day to it or a three four hour session if you guys are maybe i'll have a sign-up sheet for that it would be it would be for our session if you guys are interested just register for it if enough people instead I'll make it some very nominal fee we'll have four five hours of it or $20 depending $10, depending upon how many people register for it. And then we will learn about automated ML if anybody is interested in that. What it means these days is, see now you have many algorithms, right? By now, if you just count on your fingers how many algorithms you have learned since ml100 and ml200 just for classification you would have counted about many logistic regression directly logistic regression with polynomial terms linear discriminant analysis quadratic discriminant analysis decision trees random forest grid then boosting where you have XGBoost and we'll do gradient boosting and CAD boost, support vector machines. How many did we count? Roughly about 10. Each of these things further has a need for regularization, isn't it? So you need to regularize the data l1 regularization l2 regularization decision tree pruning how will you prove in a decision tree whether you use jenny you use entropy right and so forth there are many many hyper parameters in all of these models so not only do you have to pick the best model you have to pick the best hyper parameter in each of these models. A k nearest neighbors, kernel k nearest neighbors, then what is the kernel, which kernel to use, what is the value of the k. So this process used to be very tedious. It is very tedious. And actually, when you do that session with me, I'll make you do one data set, one new problem, not this river, but some problem, more complex problem. And I will ask you guys to find for each of the 10 algorithms to find the best hyper parameter. By the time you're done with that, all 10, you would be pretty tired, it could be a long day, just for one data set. And then the whole point is that you will learn such things like grid search, randomized search, and so on and so forth, for the best hyper parameters of the model, model tuning path, and you will realize what the day to day life of a data scientist looks like. Even basic things help, like for example, the data imputation that you do when missing values are there, how do you impute data, what data you do, how do you take care of a class imbalance, let's say you use mode or something to take care of that anomalies. Then how do you deal with scaling of the data? Do you use standard scaling, min-max scaling, Grover scaling? What do you do? So there are many choices that you have to deal with. And it has been sort of a bane that you, if you are in this space, you try a lot of things. We use our army of people to do that so there has been a body of research that sort of asked this question that can we not do it in a more intelligent way and people have been trying to do that let me see if i can give you guys a reference in case you don't have to come to my workshop you can read the book on your own let me at least give you the idea of the book where is it there's a Murphy's law the books are usually under my nose till I try to find it in the moment I try to find it it is nowhere to be seen I said it is here I just can't see it okay so let me give you guys an idea of that book by going to Amazon you can try reading it on your own there are very few books on this topic automated machine learning yes to my knowledge yeah so practical automated machine learning but this is focused on microsoft this is the book this is a very good book actually Wow this book is available free on Kindle by all means guys go and get it I didn't know that well I learned something let me go get the free version I think the PDF of this book is also available. This automated machine learning by Hutter. Somebody mentioned that the PDF is also available online. They have very graciously donated this or made it available for everyone LinkedIn book automated ML dot org yes this is the book I would strongly suggest you get this book this is the whole field of neural architecture search and hyper producing a chapter this hyper parameter optimization it's a lovely field it's a fast emerging world because you know this is insanity to be sitting there and tuning every model and then finding which model is the best for a problem so there is a quite a bit the future is actually somewhere in here so I will be giving a workshop, maybe one day, a workshop on this. I don't think it will be within four hours, but it's really a lovely field. It's something that most people are not aware of, unfortunately. Most people don't know that you can actually do all this and that there are libraries available that can help you do that. So we'll learn about that. The lab part of it,, will do in that particular workshop. But these are COVID days that these things, it's very hard to say anything that we plan to do will actually happen. Anyway, so this is, you realize that you can do gradient boosting. Again, the accuracy is a pretty state of the art. Notice that all of these complicated models, nonetheless, they don't beat feature engineering. At most, they come close to it, very close to it. This is the ROC curve for the XGBoost classifier. Code is exactly identical. By now, guys, are you feeling that as I scroll down the page, it's beginning to look rather repetitive? Yes. Yeah, this is it and in a way it's a beauty of modern algorithms that a lot of the code you can recycle a lot of you can sort of template eyes it and then just keep changing the algorithms and keep printing it out so what I do for example in this is one of the things that some people do is they will create an array of classifiers and then just run through them in a loop to see which one gives you good results and then they'll pick the best I wouldn't exactly advise you that why would I not advise that to you guys could you could you give a reason for it why is it not a good idea to just run through all the classifiers in a loop and check the accuracy uh is it like a difference between feature engineering and uh using auto ml or like no the the reason deep learning value the default values of a classifier without any hyperparameter tuning is really not an indicator of how well that classifier will actually perform when you have really tuned it. It's like apples and oranges. That is right. Before they're all ripe. Exactly. So, you know, this is a classic game people play if you want to show that your product is much better than the competition you will do a so-called performance benchmark you will bring a highly tuned version of your product to the benchmark and you will make sure that you will use the competitors version with absolutely out of the box without tuning and then you'll do a bake-off and then you'll show see my my product is so much better obviously this is this is quite a game people play in the software industry and there's something of that here and then some people of course get unscrupulous what they will do is they'll go to the competitors product and turn on logging tracing and turn on all the things that will slow it down. And then in their own, they will have a highly optimized version and then they'll do a bake off. So while that is unscrupulous, I hear people end up doing essentially that inadvertently without realizing that they're doing it. Out of the box, none of these classifiers will give you state of the box, none of these classifiers will give you state-of-the-art performance. There is a lot of lemon to be squeezed, I mean, juice to be squeezed from the lemon by tuning it, by hyperparameter. So what you have to do is you have to do the most tuned version of each of the algorithms and then compare it. You might say that by looking at just zipping through all the classifiers with default, you'll get some sense of what are good candidates. Some people believe that. I'm actually skeptical of that. In general, I know that so much is gained by tuning the algorithm. so much is gained that it is not worth just blinding so for example look at the data set 2 which was a regression problem linear regression didn't work but all we had to do is add polynomial terms and it worked isn't it and it worked like a charm after that so it goes less goes to show you shouldn't ever try to automate in a unless you do it very intelligently and this will feel of automated ML is about how to do it more intelligently how to more intelligently find the best models and the best parameters of the model are you saying that a seed then it does require somewhere human intelligence as well uh no go ahead does it require human intelligence no actually tuning at least those true for so far see people did see we take one classifier right and it has certain hyper parameters you can do either a grid search or a randomized hyper parameter search yeah and so by the way did i ever explain grid search and grid search in the randomized search no no then maybe remind me that i must cover today i must explain these things to you. So these are the classical approaches. The trouble is that if you have 10 parameters, you're looking into a 10 dimensions hyperparameters, you're looking into a 10 dimensional space to search for. Computationally, it is brutal. And this brute force searches are not the intelligent way to go about it. So there are two things you can do people have said that use your human gut feeling your understanding of the problem and then let that guide yourself but recently there has been more to it the approach that people take there's a whole bit of mathematics and nice lovely body of work that is emerging and rapidly expanding it says you know there is a more intelligent way to do two things just hyper parameter tuning you can say but more than that it is called neural architecture search let us find the best architectures the best uh classifies let's built it up from the ground up in some sense to solve the problem. So for example, one example that we use is after deep learning, if you look at the state of the art COVID net, see, as of this moment, AI has still not reached the level that it could be useful in a practical level for COVID detection because it lags behind in specificity compared to the biological methods you know the testing RT PCR RT PCR is more or less the gold standard the hope was that in the AI community hope that they would catch up the problem is not that they can't the mathematics is not there the problem is that there isn't enough data to train the neural network. So that is at this moment from a practical perspective, we are not there yet. But if you look at COVIDNet, for example, one of the things you will see is this thing. We have used automated machine learning to discover, use that along with human judgment and bringing together both to create a really good mural architecture. And when you look at that architecture. The first thing you see when you look at it is, oh goodness, nobody could have thought of this. Vaidhyanathan Ramamurthy, And we together. Nobody would have been able to think of the architecture, just as it is. to think of the architecture just as it is so that's the world we are entering it's a very exciting topic and in fact when I do the deep neural network I feel now that it is almost pointless to do it the old way that people have been doing the deep neural network so it's an interesting experiment I'll be doing it from the latest cutting-edge research backwards I'll first introduce cutting edge research where we are in the state of the art and from there go back and build the foundations rather than slowly build the foundations and then do some more and do some more and we'll never reach the cutting edge, which is the traditional way you teach this field. But I want to as an experiment do the other way around let's see if it works anyway so this is a by now you do so this is for classification let us now try to do it for regression let's go back to our California housing data once again you remember we are trying to predict the value of the house we did a lot of pre-processing and so forth so here I've used actually not just XGBoost but also gradient boosting both of them come with in Python and in R and cat boost I didn't get time I let you guys entertain yourself with it you take boosting here is the boosting implementation let's see how much does it take to do boosting. By the way, boosting XGBoost has one limitation. When you do one hot encoding, you notice that your feature names have developed this less than sign and so forth. XGBoost will fail actually if you have any of the features with the special symbols, less than, greater than, comma, et cetera. It doesn't work. It expects alphanumeric and underscore. So one of the things you'll have to do, especially for it, is you notice me renaming ocean proximity less than one hour from ocean to, just rename it to something else that doesn't have the special symbols. And once you do that, you do the normal train test split. You notice that there are no special characters in the names. Once you are there, then you do the same thing. So what did I do? It could have been linear regressor, except for this one line change, there are more changes. You do the model fit, but note guys, how many hyper parameters there are that you could have tuned? Do you see that? The more complex the algorithms, the more number of knobs that you can turn, hyper parameters there are that you can play with. By the way, learning rate, rate right this is the lambda that we talked about in advance the dancing of the learning rate how slowly you want to learn and remember one of the lessons we learned is that the slower you learn sometimes the better and how many trees are you growing one after the other sequentially these many trees here potentially these many trees here. So there are a lot of things to tune in all of this, but when it comes to predictions and everything, good. Oh, you notice that it makes 84%, 84.68% is the coefficient of determination, the R squared. How does that compare let us say to random forest let's go and see what happened in the random forest random voice was eighty three point six six and mean squared error was zero point zero five and what do we have here saying but it's a little bit better he boost is a little bit better. HDBoost is a little bit better. Why it is because HDBoost tends to overfit the data even less than random forest. So if I had to guess what was the reason for that very tiny improvement, it could be this. So how good is this model? Let's look at the residual plot. Residual plot looks pretty good the this is a normal distribution of the residuals right you don't see any pronounced heteroskedasticity it looks pretty good your prediction versus error they seem to have a good relationship 45 degree more or less relationship feature importance you can do the feature importance here again these these values are more or less the same as other ones what other models there is another version of boosting that we can work which is gradient boosting so one thing you notice that when you build this model, XGBoost, it built itself in how much time? 404 milliseconds. It's a very simple problem, the California dataset. Whereas if you do it with gradient boosting, it takes 50% more time. Not by far, but a little bit more time. This is not really a clue that this is XG boost is faster than gradient boosting, but it's not really a definitive clue. It's sort of there. When you deal with more complex problem, I suppose it's a bit more pronounced. Again, you have a lot of hyper parameters in the model that you have to play with. The prediction, oh, what do you notice? Remember that was 0.84, right? With XGBoost, 84.68% was there. And here, what do you get? Compared to that, 84.68 becomes was there and here what do you get? Compared to that 84.68 becomes 78.8. So gradient boosting, the traditional gradient boosting is actually at least without hyperparameter tuning, it is noticeably worse. Right? And it takes a bit longer. Which is why I mean, most people use XGBoost, CADBoost and so on. However, it does build a pretty good model. You don't see any heteroscedasticity. You'll see normal distribution of the residuals. Borderless 45 degree. And between the two, feature importance using this yellow brick is here and then remember the shapely thing that i taught you about the more sophisticated way of finding the interdependence and the feature importance so this is it and so that is it guys today's lab is this now the lesson here to learn is do you notice that in a way the theory gets more interesting but the labs hopefully once you have practiced this enough i hope much of the machine learning labs now are beginning to feel simple are you guys feeling that it is simple guys more says uh yes like more repetitive now repetitive Repetitive, you know. So this is it. Repetition of practice brings speed. Once you have practiced these things enough, for you, it is just a marginal cost to pick up the new algorithm from a practice perspective. And very quickly you can become productive at it. What you need is a methodology, guys, more than the algorithms. I've also taught you a systematic way to approach a problem. When you approach a problem in a more systematic way, you don't make mistakes. You are careful. Now, I invite you to go and look at the various notebooks that people have posted on these datasets and see whether we have done, on average, a much more careful analysis than most of those are posted solutions or notebooks on Kegel or whatever I don't know if any one of you bother to compare you would realize that just being careful you come out pretty far ahead of the average it might not be the best but you come out pretty far ahead of the average, isn't it guys? Yes. Let us do that. Let's go and verify. This is our notebook, right? With all these things there. Actually, I haven't done that. Let's do this exercise together. California data set. I'm sure there must be Kegel notebooks for that. The data set is built in Colab also, Asif. Oh, okay. It's built in. Yeah, that's right. It must be. It is it. Two years. How many notebooks are there? 282. Okay. There we go. Let's look at a typical notebook. 157 notebooks have been posted. This seems to, okay, I'll just go to a Python one,ness language because here we have a Python solution this seems to be the best voted money this is shaping complete tutorial for beginners let's go and look at this the best one and see how we are doing with respect to the person so by the way, something's noticeably, the first thing that strikes you is that this fellow has actually given an introduction, which is a positive sign. Most people just jump into the code. We should explain what you're doing. He's doing that, he's taking the city coordinates. By the way, remember we did the log transforms and things like that. Let us say if he does the log log transforms he doesn't have a table of content or titles or break it up into sections new functions Cleveland Ohio is it California did he set which data set is it do a special feature engine on the California we'll come to know when we visualize the data yeah so he's visualizing the data let's see how did we do for visualization did we do a visualization do you guys remember days were we careful enough to do visualization here it is i will take it so we have a visualization in fact we have a interactive visualization so we do have visualization what else we projected it onto the map as a scatter plot this fellow seems to have been also a scatter plot we have so let's see what we missed and that is a great way actually to learn by seeing what you have missed below is the graph showing the size and location of the cities so this is the city location so more of this feature engineering length vector between districts and so this is actually very good yeah i remember this person doing that very impressive he did an interesting feature engine he said what matters is distance to the nearest metropolis length of vectors between districts and nearest towns so somebody has contributed this if you notice this graph it's pretty nice actually he's telling you how far is a house from the downtown effectively or distance to the biggest city what are the biggest cities in the bay area it is i suppose san francisco san jose los angeles and san diego or those four cities they probably are taking so this is a nice thing we haven't done that so it is something to learn from. We can do that. Then, actually I remember seeing this one in the past at some point. So he has taken this data, which is very good actually. Then he has done a stratified shuffling and then he has split the data. Now the question is, why that? Can anybody answer this question I leave that as exercise but before you split the data why has he done a stratified split of the data can anybody that I guess so that your test will not have more of one city and less of the others yeah because data has imbalances there may be more points from one than the other so you don't want to take a split in which let's say that all the data points that belong to a small town they are only in the training data and not in the test data or all the some particular feature let us say all the points that belong to an island, Catalina Island belong only to the training data, but not the test data or to the test data, but not the training data. That would be bad. So stratified sampling is good. So read about it guys. That is it. Then the rest of it is labeled by, we did it actually straightforwardly. We also have this we have this And then he stops that's about he uses XG boost So you notice that when he does as he was do you notice that his root means squared numbers are huge So one of the things he did not do while he did feature engineering which is very good because he did not use the simpler algorithms the standard linear recreations other variants of it there is no feature engineering so for example he did not do the log transforms this data set sort of calls for log transforms this particular guy who's done a very good job seems to not have done that let's try the next guy um i will limit myself to python here who is the next space explaining your model predicting by the way this fellow also has not done feature importances or or shapely values this person has done done shapely values seems interpretable machine learning. Shabnam Rezai – And he's putting quite Shabnam Rezai – Where am I, I'm not able to scroll. Yeah. So this is it. View the data you see well this graph certainly needs aesthetic improvement with Well, this graph certainly needs aesthetic improvement with still pretty good training. So he has not done stratification stratified sampling. This is general straightforward. He's using random voice regression because his point is to show Shapely. Do we have this in our models guys? In our notebook, do we have this? Yes, we have this. Shapely is important. Those of you who took my boot camp remember that we gave a whole day to interpretable MLI and Shipley and all of that. So this is good. A short notebook. It is good. Which is the next one. Let's go back and try the next best. Complete tutorial for beginners. Let's try a luck here. Hello, Caggler's. There's that. Oh, we have this, obviously. The other notebooks didn't have this. Scatterplot leaves something to be said. Then, yeah, more of the scatter plots. We do have this, but we have a better version of this, I suppose. Let's go and see, do we have it or not? Yeah, you see that? We have these, we have these, much more sort of deep, informative versions of these. But let's go and see what else is good. He's looking at this and from this, what does he conclude? Well, I hope he concludes that we need a transformation, data cleaning. So this is the imputer. He does a null value imputation. I haven't taught you imputation. So we didn't do it in this particular dataset, but this is a new dial up the size a little bit it's very small please okay let's see so this is the imputer you know missing value imputation uh that i talked about it but we didn't do it in the data we can do the imputation these things we have the scaling of the date oh Oh, that's it. So he hasn't done the modeling. Let's go back and try somebody who has done the modeling Optimized modeling 24 volts Import data missing value Import data, missing value. He notices that there is a skew. Hopefully he's, see sometimes you can just look at the pictures and tell how the person is thinking. Hopefully he notices the right skew and does something about it. Notices a lot of outliers and he computes the medians and so forth unique unique for median total number is finding the median of all of these checking for null values EDA exploratory data and so let's see what he does figures he notices that there's this big pillar probably he'll remove that. Population versus housing not terribly informative. Removing outliers, good. So he has actually decided to drop outliers, which is very good. Population. So in every dimension he's removing outliers. And then this looks very similar to what we have. This is just the correlation matrix. This is the correlation matrix. is the correlation matrix we have do we have that guys we have that obey this is as the count plot the box plots do we have the count plots box plots etc days here it is so this is what we have you see it guys we have this so he also has this. Pre-processing, he does that. Normalizing the data, do we normalize the data? He is using min-max scalar. We use the standard scalar. Cumulative variance curve. So he is doing a principal component analysis to figure out how many things matter. we haven't done it in a notebook because PCA did we do PC at this time we did it in a 100 so he's looking at the PC of values which is good then modeling so standard he is that he's doing a simple linear regression then he's doing the residual plot this is not really the way we do it is a little bit better how do we do it guys residual plots let's look at it yellow yellow bricks we use that which I would consider a little bit better linear regression here we go also we do all of these log transforms etc it's a good notebook so we could not though I we seek to have been better in some areas it's very good so now he's doing stepwise regression he's trying to eliminate features by that by the way guys let me just warn you don't ever use this stepwise you know backward and forward or feature subset selection remember that's the one chapter of the book i never taught you because i believe those methods are fundamentally flawed you should never use it so he seems to be using it he is regularizing seeing if that makes a difference and then what does he conclude his elastic net is the regularization with L1 and L2 isn't it we'll do regularization next time our next lab is that rich then he's bringing decision trees and in the decision tree he's coming to the confusion he is doing some hyper parameter tuning parameters here do you notice this guys this is what i was talking about it is not parameter tuning the right word should be hyper parameter tuning parameters here do you notice this guys this is what I was talking about it is not parameter tuning the right would should be hyper parameter tuning these are there how deep can you make the tree and so forth this is a topic we'll cover in a moment after the break with all of this is coming up with future importances do we have future important since we have that random forest, etc. Actually, and then again for grid search that. Yeah. So now he's using the right hyper parameters to the best feature random forest and so Sajeevan G. And that's And that's about it. That's that. So, so guys, what do you feel? Have I trained you to a level at which you can write a very good notebook amongst that can stand on its own and be respectable? Do you feel that you have been trained to that level now? Yes. Yeah. Yes, Ashish. You can do that guys. Machine learning or this field of machine learning, you know, people get very excited about the latest and greatest research papers. What is the latest breakthrough in transformers? What's the latest breakthrough in CNNs and so on and so forth? But when you actually deal with a business problem, when you deal with real scientific data, you will realize that you go slowly you know you will you will observe that quite often the best researchers they speak slowly they think slowly they're very careful and methodical that is where the gem is great things are done when you do it carefully methodically and deeply you don't rush through it so don't rush magic is not in the fanciest algorithm it is good to use fancy algorithm but fancy algorithm in a in the hands of a disciplined scientist is far more effective do do it like that any other notebook worth looking before we take a break? Maybe one more. Let's take end to end machine learning 18. Let's try a look with this. Introduction. Yes, we have these histograms you got right. The ETA, the maps, the scatterplots, the pair plots, the correlation matrix, data pipeline, right? And model training, model tuning. No one seems to have been the law transforms in sector of the data. I wonder why. Okay, so by now we are getting rather low in the list. Maybe we can try a look with most words. This in terms of language, I think we have already done that. All right guys, so I will will that calls for a break after the break I'll explain what hyper parameter tuning is and how do you do that would that be okay guys or should I just finish it quickly and that would be the end of it no I think we can take a break. All right, let's take a 15, 20 minutes break. It is 8.31 by my clock here. Let us take a break. Pause recording. We are on record. So suppose you have an algorithm think of an algorithm let's say whether it is a kernel okay and then right or you are talking of SVM or you're talking decision tree boosting the gradient boosting boosting and its variance XG boost even logistic regression let's say and I'm deliberately taking classifiers with the same argument applies to regressors. So with all of these, there are many, many hyper parameters that you can choose, not to mention that how much regularization to bring in. Right, whether to use L1, L2, whatever it is for trees how deep how many nodes there are in the trees and so forth the learning rate for boosting these are all hyper parameters in a random forest how many estimators to take right how many predictors or features do you want to give as a consideration like a split point in the in the in the tree like while building the tree so how do you find the best value right so there are many ways that people think about when this has become practically a stable staple diet of interviews so people usually suggest the approach grid search the only reason I'm mentioning it is it is what people expect you to answer so if you ever ask this question you should say that see we can do grid search and explain what grid search is but do emphasize that they're actually better ways to do things so what does grid search do so suppose you have two hyper parameters let me just call it hyper parameter one which we just call it HP one and hyper parameter two this can take on values arbitrary many and I'll just take a continuous variable literacy so for example let's take regularization in regularization you have the lambda you know so suppose you do beta J let's say square a ridge regression you have SSE plus this. So what is the lambda value you should take? In SVM, it is the C is equal to the how much of a budget do you want to take? And in an RBF, like when we take our RBF kernel, what is the gamma? All of these. So they can take any arbitrary real value. So how do you go search about them? So there are a few basic things that you can do you first take hyper parameter one let me just call it h1 and you choose an array of possible values so for example if it is kernel okay in the kernel you could take one two three four all the way to whatever you think reasonable let us say a 50 neighbors you can take so let us say that you have another hyper parameter which can take values from 0 to infinity so how do you go about it so one thing you can do sometimes is you can go over 10 to the power for example the cost in a SVM you could take 10 to the power minus 4 10 to the power minus 3 10 to the power minus 2 10 to the power minus 1 1 and then maybe you can go to a few positive powers also 10 to the power 1 10 to the power 2 these are all possible values you to take I'm just taking just two examples of a hyper parameter now you realize that if you do h1 Cartesian product h2 how many possible combinations are there let's say that these are 50 and these are 1 2 3 4 5 6 7 7 so you're looking at effectively 50 cross 7 combinations isn't it guys so quite often the the grid search approach essentially suggests that what you do is you take for h1 for uh let's say a value h1 in this value 1 to 50 for h2 in now this value you know 10 to the minus 4 10 to the minus 3 10 to the minus 2 10 to the minus 1 1 10 100 let us say you lay it out like this then the tb uh sort of in the consistent 10 to the minus 2 10 to the minus 1 this is 10 to the 0. for each of these two combinations you can build a model you know you can do a model with h1 and h2 values isn't it you can build your classifier or your regressor with these two particular values and then you you look at the metric let us say that you're doing classification and what you care for is the accuracy. Yes. So you can maintain a table which would say h1 X two and the metric accuracy, you can maintain a table. And then what can you do at the end of running through the two for loops. right you are essentially going through every point of the grid because this cartesian product is this grid right these are the h1 values these are the h2 values so at every single combination a point combination of these two you will have a certain value there and from this value you will then pick the minima value you will then pick the minima I'll say maximum the the most accurate let me just say maximum maximum that maximum accuracy so on cross validation on validation set so that would be your best combination of H1 and H2. Does that make sense guys? It's a simple idea, isn't it? Take every possible combination of the parameter values, take some discrete values. That's why the word grid. Grid implies discrete set of values. And when you take a discrete set of values and you go about searching for it, you'll find some good combination that is good. Now, one further thing that you could do is you could zoom in, zoom in. So suppose you come to know that a K is equal to, so suppose you went in the beginning, K equal to 1 10 20 something like that and so suppose you zoom into 10 in the second round zoom in means next what can you do you can take values like 5 6 or 6 7 7 8 9 10 you can just look in the vicinity of this 12 13 maybe plus minus 3 to see if instead of 10 any of the neighbors give you a better value in the same way suppose you find out that the cost or whatever it is the other h2 the best value was 1 so what can you do now you can search in the vicinity of one. Try out whether 0.7 is good, 0.8, 1, 1.2, like, you know, 5, something like that. You can zoom in and try out local values till gradually you find the best value of H1 and H2. Right? H1, H2.2 right so you're basically or in simple terms you're you're trying to do arg max h1 h2 for the metric accuracy of the model this is what you're trying to do. So it could be accuracy recall, whatever, whatever it is, whatever metric that you want Area under ROC curve You can pick your metric and then you you pick those values that are the best. This is called grid search If you look at textbooks, they'll often mention about grid search. It is the answer that you're supposed to give in interviews, et cetera. But actually it happens to be amongst the ways of doing it. Perhaps one of the less efficient ways of finding the best hyperparameters. There's actually a better way, which surprisingly is when you have h1 h2 what you do is you don't do grid search actually you take random combinations of okay so what you do is you take random values of h1 h2 are we together so suppose you want to do 100 for 100 search events what you do is you randomly pick h1 h2 values when you randomly pick what happens is you actually have an advantage. Let us say that your H1, H2 form a rectangle, right? This is H1, this is H2. In a grid search, what happens is you end up picking values only along this. Isn't it? This. And so how many grid points did we pick up? Let's say, let's take this count and we'll see how many grid points did we pick up let's say let's take this count and we'll see how many we are talking about so one two three four five six what three four five six thirty six you have 36 values now see what I'm going to do if you go so this is great now from this if I go to randomized we'll take the same thing but we do something different we say that randomly pick let me use something else some other color randomly pick 36, two, let me randomly go sprinkle these values here. One, two, three, four, five, six, seven, eight, nine, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35 6 27 28 9 30 31 32 33 34 35 36 do you notice some difference between these two graphs case these two plots if you look at how many if you do randomized search and you ask how many X 2 values did you sample how many H2 values did you sample? How many H2 values did you sample? You will realize that you sampled 36 values. And how many H1 values did you sample? Also 36, isn't it it because when you project it down when you predict along each of the axes you will have lots of values all 36 values will be here because it's very unlikely that any of those values would overlap if you take random numbers so you get a more uniform search a richer search of the feature space let us say that your best value turned out to be around here what can you do now in the next round you can just draw a circle and you can test for more values in the vicinity of this so today we believe that randomized search, this is called randomized search for hyperparameter optimization. Asif? Yes. This assumes there's no local minimum, right? That is true. See, unfortunately there are. And that's why I call these methods not to be the very best methods, but it certainly is better. See, unfortunately, most people Sajeevan G. A they don't know hyper parameter tuning at all. They just go with vanilla values or random guesses if they know hyper parameter tuning. The only thing they know is grid side because that's what they have been fed in the textbooks and so Sajeevan G. And it used to be true but now we know that randomized searches are better better than that right so you can do this and you can zoom in and in the randomized search also zoom in and repeat in this circle again do randomized search next to the best possible value and ultimately you'll find the best hyper parameter models so these two are very popular if you look at the code on the web in the secret literature and so forth you'll often find grid search and randomized search often being used just now we saw a Kaggle notebook if you I don know if you noticed, it was a double for loop for two hyper parameters. It was basically this, this, this sort of a thing. Different values of the hyper parameter you would need in the model. It was a regression model though, looking at mean squared. But the idea is the same. And what I'm telling you is that if you must do that, instead do randomized search. It's just a superior way of doing it. But now comes the state of the art these days, right? And this is where automated ML and these things come in. We have a way doing hyper parameter or that my spelling is terrible hyper parameter optimization my my station hyper parameter optimization but here is a bit more involved that's why I say that let's have a dedicated session for it in which I talk about how these things are done. These are non-trivial methods. These are not just the above two. Let me just say that the above two are the poor man's methods. They are very popular. Most people don't know anything beyond that, but there is actually a systematic way to optimize your hyperparameters. And we will do that at some point. Today is not the day because a much longer discussion. I just planned that idea that there was a systematic way to do hyperparameter search. And there's this whole world of neural search and so forth. We won't go there at this moment. But today what I would like to do in the last few minutes of this class is summarize what is it that we learned. So if you ask what is it that we learned in this course, I would say the biggest thing, the number one thing is do your analysis thoughtfully. The more you think what you're doing, the better you'll do. It looks almost like a cliche or a trite statement to say that. So this is it. You know, if there is one message I wanted to give an ML 200 the overarching messages be thoughtful in your analysis. If you are, you will always outshine you'll create an outstanding notebook go slowly. Go methodically. and more or less you will get somewhere very very definitively you'll have a good analysis of the data and it's quite a pleasure you know when you can start with any data and you know that you'll find every data has some story to tell you know something interesting to tell you about and you it's a journey and you enjoy that journey you know it's not a haphazard or a quick rush to build a model and then say we have done it. See in coding in normal software, there is a lot of ego of one upmanship. One guy will say, oh, you took one week to do this problem. I did it in one day or half a day or two hours and so forth and at some phase I suppose we all have indulged in this sort of sort of a game because we think it is measure of intellectual superiority to be able to finish a problem quicker may be true for normal software development actually it's not true even then. My basic rule to my team at least is write code as slowly as you can because code takes not much time to write, but 10 years people will maintain the code. So every single bug will come and haunt people later on or you later on. So it is much better to write code slowly, correctly, as far as possible, write the first time, buffer it up with unit tests, lots of testing and, um, things like that, documentation, et cetera. Now in machine learning, it couldn't be truer. People who do machine learning are not here. The game is not to be fast the game here is to have the most predictive model the most high performing model the most interpretable model a most useful model usefulness and speed have nothing to do with each other and speed have nothing to do with each other you create good useful models when you go slowly when you go slowly almost surely you'll hit it whereas if you rush ahead it's a you know luck of the draw you might hit luckily once and you may feel pretty good about it most of the time you won't hit it if you look at the brightest people in the field they are very thoughtful slow people they are not people who rush or have this ego thing that I did it in 10 minutes anyway most of these data sets large data sets and algorithms when you apply they will take hours and hours and days to run on the machine anyway so where is the rush you might write code in five minutes but it will still take hours to run on the machine so iterations are slow you might as well instead of doing a lot of useless iterations you might as well be very thoughtful and do careful so that's Субтитры подогнал «Симон»