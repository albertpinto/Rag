 . So last time we stopped the lab when we were doing the auto dataset. Do you folks remember that we did the auto dataset? I'll wait your recollection very quickly for a few minutes but we won't spend too much time on the auto data set it's a landmark data set you must have say that oh yes yes i will zoom in uh one second is it big enough yes okay so we did the auto data set. If you remember, it was our first foray in real life investigation of multidimensional data. So there was a lot to learn, lots of techniques that we had to pick up and aspects we had to take care of. So this is the data. We looked at it. There are many, many variables. One of them, name we threw away, origin we converted into a categorical. So we also realized that horsepower was flawed. It had some missing values. And because it had missing values, we had to do some correction for the horsepower data. We had to get rid of the name and do that. So you're beginning to see, I'm starting to bring you closer and closer to complicated real life data sets. This is a good beginning actually. We looked at the histogram, how are these predictors related to each other? And how do they look? So we got some clue. The miles per gallon has a significantly right skew. Many other variables, displacement has that, and weight has that, horsepower has that. Acceleration looks pretty evenly distributed. And we could see could see actually I'll make it just a little bit smaller uh is this okay or is this too small for you guys that's good reasonably yeah I need it unfortunately with age I need to have like I have only this much real estate available here I need to see everything on a screen. Okay, so when we looked at this, we saw that there was a peculiar nonlinearity, the one that we often associate with the Zipfian distribution or a reciprocal distribution, reciprocal power law distribution. And so it played itself out. We saw the skews in the data. So these are things. And we learned some beautiful visualizations that we can do, violin plots. These are the violin plots that we did, and the box plots that show as the outliers. And then we looked at the correlation between the variables only to realize that some correlations are pretty strong. We found that there is a strong correlation between weight cylinders displacement right so what do we do about that we keep kept only one of them because otherwise you would have multicollinearity problems so we did a naive baseline so one of the things we do is univariate. So first we do the univariate regression just as a refresher. Suppose you do a model of horsepower, mileage versus horsepower. This is the distribution. This is question eight of your textbook. You look at the distribution and if you look carefully, you see this reciprocal law distribution. One of the things that I have found very successful, by the way, your book doesn't talk about these things, but I have found very successful, and this intuition comes from physics, is that in physics, the reciprocal power law dominates. If you look at the Coulomb's law for charge, it's a reciprocal law. If you look at gravitation, it's a reciprocal law. And reciprocal laws are everywhere in nature, at least physical sciences. So we are trained to see reciprocal laws in effect. You see data like that and we often start suspecting a reciprocal law. So by taking log transforms, you can easily convert it to linear. When you do that, you notice that we went from, we will from, we, we will see, we will go when we build a naive regression model, it doesn't fit the data very well. We get 56%, 56 and a half percent R squared. So one moment before we move forward on the reciprocal intuition that you just shared, is it general that it's at the power of one or are there? No, no, no. You don't know the power that is the point with the reciprocal law you write it as this wise proportional to one over X to the power n. And okay, you don't know what n is, but it doesn't matter because if you take the log of the reciprocal law, it becomes a linear law and just becomes a coefficient. You see that okay, the coefficient. and so that's the beauty of this yeah i wish it was more mentioned in machine learning text which would they see in real life you apply everybody has their own bag of tricks and beautiful tricks right these are my bag of tricks that you get to learn but i wish but i find it so useful so many places i find it useful But I find it so useful. So many places I find it useful. So that's that. Now, if you do regression with the original model, clearly you get an effect that is not... Oh, my goodness. This is... Sorry. Let me slow down my mouse. It is moving a bit too fast. Because with this big zoom factor i need to display pointers now how many how much do i scroll depends upon yeah i was going to do something mouse display change the mouse size change the amount settings Change the mouse size. Change the mouse settings. Give me a second. Cursor speed. Select it. Run the mouse wheels multiple. How many times to scroll? So OK, let me decrease it a little. Yeah, much better. So here we go. When you notice this model, you realize that you get 56.5% R squared. But when you do this, you see very clear heteroscedasticity, right? If this word is heteroscedasticity, sounds a little hard, this pattern, this conical pattern, variants changing. I always give a mental picture if you love shallow combs think of it as this pipe whenever you see the picture of shallow comb do you see this a pipe in his hand so does it look like this pipe inverted somehow it always reminds me of that i don't know if it reminds people but do that. So we look at the plot and there is a variance between what it should be and what it is. And when you plot the prediction on the line, it's not such a good prediction, a 55 person gain. You try this with residual analysis. So what happens is if you do it on the other hand with a log transform, see here here I have taken certain log transforms of this input output, then you would agree that this particular residual plot shows far less heteroscedasticity, isn't it? Or you can't find the Sherlock Holmes pipe here in this image very much. Then there's a better agreement. Of course course this is as far as you can go the prediction line looks pretty good as one variable models go this is the best that you can do then comes what so this is me applying a trick that we did what happens if you instead just blindly do a power transform you will get some of the benefits but not all the benefits if you instead just blindly do a power transform? You will get some of the benefits, but not all the benefits. If you look here, I got an R squared in a single variable itself of close to 72%. If you just do a blind power transform, you'll actually, you will improve, but not as much. You'll get to 64%, which is halfway between 55 and a half and 72, somewhere there, a little more than half right you could do polynomial regression it doesn't take you much further you stop here so that's the lesson the lesson there is guys that when you can visualize the data and think about it see what head and tail you can make out of the data if If you can make head and tail of your own, bring your intuition, bring your own mathematics and hit at it. That is the way scientists traditionally used to think. So a multivariate regression. When you look at all the features, now you have a lot of features to consider. Whenever you guys build a multilinear regression, even a linear regression, so you have the null hypothesis. The way null hypothesis effectively manifests itself in scikit-learn, there is a dummy regressor. You can pick a measure, centrality measure, median, mean, something. It will just go and predict that always. It gives you a baseline of, a coefficient of determination will of course be zero or more or less. But the thing that you look for is what is the mean squared error? Because you'll compare your errors to the mean squared error, right? So it is 56 here. Now let's look at it. And it's one of those things, guys, and people don't do it. A very disciplined approach would always start with this so do that so we are going to build a linear model without any consideration we'll just say okay go fit the data and see what happens you notice that the mean squared error has reduced from 56 to 11. obviously the system has learned something our coefficient of determination is 80 percent which is really good with lots of variables your, your R-square is improved. But there is a cautionary tale. The more variables you throw into a model, R-square will keep on increasing. The question is, is your model improving? So R-square is a dangerous thing in that respect. You don't know. So you need to do residual analysis. And the moment you do, lo and behold, Sherlock Holmes' pipe has come back. Heteroscedasticity is very much evident. So you move forward a little bit. You say, all right, let's do the things that we used to do. We used to do scale. Well, first of all, just let's blindly apply. Some things we do. Maybe we forgot to do the scaling we forgot to apply a power transform let us do all of that you do all of that and you notice that well from 80 percent you go to 81.6 percent and your mean squared error has gone from 11 to 10. well some improvement but your residual analysis, once again, you start seeing shallow comb spike. Then you keep going down. Model 3. So this is where we are trying to be a little bit more thoughtful, and we are trying to do this. First, what are the things we do? We throw away variables that are highly correlated. Keep only one of them. That will remove, what is one of them that will remove hetero that will remove what is the problem that that will remove when you keep only one of the many highly correlated variables features what is the problem that it cures multi-coloniality thank you thank you kids so multi-coloniality that problem goes away. Then what do we do? We do some judicious power transforms law. I handlock transform it because that's sort of the intuition coming from the graphs. Then with all of that and still using standard scaling, you do this. And right away, you build a model. And what do you notice? You get an R squared of 87%. And look at your mean squared error. Do you see that? It's much, much less. Would you all agree that this is a pretty impressive mean squared error? Definitely. It's very good compared to the just blindly applying a model or even with just blindly applying power transforms it used to be 10 10.3 here and what was it 11.2 there and a baseline dummy model giving you 56. so all the way from there root mean squared you have come to a value that is now 0.01 and your r squared coefficient of determination is 86.8 87 percent give or take right generally those numbers are considered very good for real life situations which you see here right now how did i one of the things is how did i decide which variables to keep and wish to throw away? That is where making friends with data, spending a lot of time poring over it. Ultimately, the secret is, as they say, hiding in plain sight, right? To use a metaphor, here it is. When I looked at this with a physicist's eye or with a trained eye, when everything is in a way hiding right here in this plot, it is hiding in plain sight, right? All the tricks that you can extract and you get ideas, what should I do? What should I do? All of them, the fact that you see the right skew, the fact that you see a reciprocal behavior, the fact that you see these very highly correlated things. You see this where my mouse is? By the way, is my mouse visible now? Big and clear? Yeah. So the fact that I see this strong correlations, you start drawing conclusions, you start taking notes. And that's what I do. I usually, what I do is I spend, like when I have this, usually I will take a cup of coffee and I'll keep looking at it. And not just look at it, I take paper and pencil and start writing notes of what I think about them, right? So note-taking is at the heart of writing. I mean, lab notebooks. And then later on, of course, I encode it somewhere and you see me at the end of it at the end of it getting to 86 87 percent uh of a model which is which i would say arguably is very good so i leave you as a challenge you guys you can do better if you go on spending more and more time onto this data and trying more tricks perhaps you can squeeze a few percentage more try it out see how far you can go the cook's distance it helps you find points of high leverage so there i have to explain you're encountering it for the first time but there are a few points of high leverage but not that many right so linear regression it still works it isn't that it's you have to abandon it these will tell you those points of high leverage that are hiding in there in your data set that can ruin your analysis so it is still good we address the very important topic of feature importance feature importance is in linear model where you have scaled the data, you have one of the luckiest situations where literally the coefficients tell you what is the order of importance, the magnitude of good. Remember, data must be scaled first. If you have scaled the data, I find that it is a pretty good way to find how important those factors are. So according to the scaling of the predictors, the coefficients of the predictors, I find that weight, horsepower, year, and acceleration are the dominant things. After that, it begins to get rather thin, right? Even the top three dominant ones are weight, horsepower, and year. Acceleration is at single digits, origin and the rest of them are at single digits. But then I talked about a far more elaborate technique called Shapley values. In a way here, it is a bit of an overkill, but I brought it in just to introduce this early. After the great mathematician and Nobel laureate in Shapley. So once again, you see weight, horsepower, here are the dominant factors. Then come acceleration, origin, et cetera, matter very little. Then there is such a thing as a partial dependency plot. All of the factors being changed, if you change, let's say one of them, weight, how much does it affect the model, the mileage? It turns out that weight is a very, very steep impact on mileage. You can see that this is huge. On the other hand, horsepower, if you want to increase the horsepower, that also is pretty steep, maybe a little bit less. And then comes acceleration, which doesn't seem to have as much impact. It's a little less steep, the relationship, which is also borne out by this plot. So guys, one thing I wanted to show that say, do you notice that even when a simple case, when we do linear regression, we do a very fairly thorough analysis right i hope i haven't missed anything but if i have you can tell me we'll keep adding it to the notebook but i would invite you to do such thorough analysis most of the time when you hire people or when you see notebooks on the web most people don't do careful thorough analysis i have you like how many of you have been looking at notebooks on the internet? Like Kaggle. Kaggle and so forth. You would agree that not many people who post their notebooks do it thoroughly. Some do, but most of them don't, isn't it? So get into the habit. Being a methodical scientist increases your your chance of finding something deeply insightful in data that you know that a cursory examination would miss so that was that so today let us move now to the to another data set which is the california housing data set at this moment in california if you are a homeowner, you're jubilating that your home prices are skyrocketing. If you're not a home owner, you're feeling terribly frustrated that home prices are going up. Well, this has always been the situation in California. It turns out, we'll go and look at the data from the 1990s and the story was pretty much the same. So this is the California housing data. It is a classic in machine learning and data science. We are going to learn about it. So let's look at the predictors. Longitude, latitude are your x, y, effectively think of them as your x, Y coordinates. They sort of give you the coordinates of where you are. For a given house, it tells the housing median age. Like what is the median age of people in that block? Our total rooms, how many rooms do people tend to have in houses in a particular block, right, or in a house? Total bedrooms, how many bedrooms they tend to have. Population, what is the total number of people, number of people living generally in that block? And it is pretty interesting that close to 11, 1200 people. California is densely populated. It shows how many households typically live in a block, about 400. Median income of people is, I'm hoping that this is $100,000 or $10,000. Something is the unit, we'll have to look at it. But the median income in 1990 seems to be, if we just take it to be, well, that's pretty high. I don't know what to multiply it by. We'll have to look up the housing documentation. Medium home value is what, 179, 180 000 you observe that 180 000 today will not even get you a garage right how much the prices have increased in the last 30 years now you know that in california we all love to stay close to the beach, right? And it's some people enjoy nature, some many people enjoy nature, they enjoy the beach, some people go there as a status symbol and so forth. So one of the things I wanted to say is that okay, so I won't run this notebook because I am not sure that it will come through properly, but you can style your note. Actually this much I can do. Let me see this much I can run. No, I won't run this. Things will get messed up on this laptop. So when you look at the missing value analysis, do you guys see missing values? when you look at the missing value analysis do you guys see missing values can you look at this picture and tell which of the features has or have missing values the total bedrooms the courses are things yes sometimes you you have you don't know how many bedrooms there are in a house so that is significant and you can see that there are two zero six four zero records the only field in which you have less are but then looking at this So that is significant. And you can see that there are 20640 records. The only field in which you have less are, but then looking at this number, it's encouraging. It's you missed just by a couple of hundred thousand, maybe 200 record, 207 records are off, not that bad. Right? And this is a relationships between, if this is missing, what else is likely to be missing kind of thing. We'll ignore it. So what do we do? We have the luxury of data. You know, we have so much data, 204. What is it? 220,640 data sets. Out of that, if you throw away 200, not a big deal. So our way of treating for missing values, not always the best, but here in the interest of speed we did that so one of your homeworks is this guys can you instead of dropping the rows can you do a missing value treatment look into panda's documentation it has many ways of doing it you could for example replace the number wherever the number of bedrooms is missing you could just replace it with the average number or the median number of bedrooms, substitute it, or you could do interpolate it. You could do many other things you could do. I will give you guys a notebook on missing value treatment, but till that happens, look into pandas. It's very easy to read the documentation and you'll get it. You can draw the map of California, which looks pretty impressive. It's very easy to read the documentation and you'll get it. You can draw the map of California, which looks pretty impressive, it's invisible. Oh, so I apologize why this is invisible, but it should be a map here of California. Okay, so here's the thing, the map of California, if you compare it to, now this is simply a scatterplot of data and median home values so let's ponder let's dwell upon this what so i'll just shrink the thing here when you look at this map the it's a heat map of where house prices are house price is high what can you tell just looking at this map What can you tell? Just looking at this map, do you see in your mind a picture of California emerge? The map of California emerge. And can you tell what this region could be? It's hot on the edges, the beaches. Yes, beaches. And yeah, this is it. And you can see all along the beaches, there are high prices going almost into Santa Barbara and so forth. Then there is a patch of emptiness. And then once again, the Silicon Valley, the San Francisco Bay Area is hot, right? And then off the San Francisco Bay Area, you see, can you tell what this is? This area could be? Sacramento, and you see this Highway 80. Practically, you see the Highway 80 going there, right? And from there, from Sacramento, you see this thing that comes down to Tracy and Merced and Fresno and so on and so forth. And you see this housing that is somewhat, you can literally see just by plotting the data that you are implicitly looking at the invisible map of California. And now I put these things side by side and you can see it much more clearly, isn't it? Our home prices here again on the map without it. Now, by the way, one thing before you run it, make sure you have installed the folium package this uh folium is a map package for python make sure you have installed it all of these notebooks by the way folks and they're close to 25 notebooks or interaction interactive labs that are there uploaded on the website please do review them i tend to put a lot of notes and comments there and many things that i don't get time to talk about in lectures, because you know we have limited time during the lecture sessions and in the lab sessions. I have actually mentioned in the lab notebooks. So if you read them, you'll get, you'll get some perspective there. There are a couple of PNG files that it reads in that we need to download from somewhere. Oh, housing map PNG and California housing scatterplot PNG. Have I not uploaded it? I thought I uploaded it to eminent. Okay, let me fix this right after this lab in the break room. I'll do a break. I'll do something. slab in the break room i'll do a break i'll do something yeah or maybe let me do something today evening i'll fix this little glitches in the notebook because it's running on my machine but not on your machine i'll make sure that whatever hard wiring is whatever i forgot to upload i'll upload it i thought you uploaded last time yeah i thought so too both of those let me double check yes if you refresh your file you'll probably get it i think i didn't see as well okay it's all right i'll make sure that whatever is there online is the latest and greatest right in fact what i'm looking at in this laptop itself seems not to be the latest version but that's all right okay uh we can get the sense so when you look at the housing data guys from looking at the auto data now you realize that you have a template you have a methodical process to walk through. We are looking at the. What do you call this graph slides histograms histograms Yes. So these histograms. How many of them seem to have a pronounced right skew would you consider uh this one to have a pronounced skew total rooms no okay let's look at it one by one uh uh what do you say about longitude what looking at this what what do you observe why are there two peaks Longitude. Looking at this, what do you observe? Why are there two peaks? Maybe those two areas we saw on the map? Exactly. Because California is along a diagonal. So San Francisco is a little bit to the west of Los Angeles area. And that's why you see two peaks in the longitude. Likewise in latitude, of course, you see two peaks, clearly. Los Angeles is a much bigger area at a lower latitude. San Francisco is a much smaller area, relatively speaking. And so it has a smaller peak at higher latitude. Now, housing median income, more or less, do you feel that it's more or less symmetric? It is, but what is this weird thing, this long, long, long thing sticking up? These are your Mac mansions, right? These are ginormous things. Raja Ayyanar?nil They have just rounded it off to, I think, 50 or 60, because they have used some rounding mechanism. You have a long pillar of all the old homes. So in a sense, the data has collapsed all homes older than a certain age into one pile. That would be insane for Europe. Oh, yes. Then all of Europe will fall in one bar. 99% of their homes. All right, so total rooms. What can we say about the total number of rooms? Less than 10,000. Yes, I don't know why it isn't 10. Oh, in the block, the total number of rooms in the block, yeah. So what we noticed is that most of the number of rooms are small. In a block, how many rooms can there be? Actually, let's look at the total number of rooms. The median value, well, the mean is, one of the limitations of this is it doesn't give you the median but you can literally do total room start median it will give you uh but mean is around 537 in a block when you have 537 and your graph goes to 40 000 what is it trying to tell you your scale is off no it just means that most people in California seem to live in single family homes or townhomes. But California is not without its high rise residentials. There are apartment complexes that rise up and high density living, very high density living also is there. But it's so little, so little of high density living is there. But it's so little, so little of high density living is there. Just looking at this map, you can tell that they stand out as anomalies, isn't it? In fact, in Fremont, is there a single high rise for apartments? I don't think so. There isn't any. Right. So they, they are there, but they're few and far between and they create the outliers. The same is true for the bedrooms. They are causing the outliers. So one of the things you could do is you can say, well, the dynamics of extreme high rises is different from normal homes. I may just choose to remove that data and I'll deal with that data separately, build a separate model for them, right? So I'll give you a hint. People like the Zillows and so on and so forth, and other companies who do insurance on this, predictive insurance, et cetera, they tend to do that. What they tend to do, and actually I was searching how many people think of doing that. The few notebooks that I saw don't do that the right way to do that is at some point cut off the outlier because they will skew your results may skew your results and you deal with outliers separately think about it this way guys how is what is the dynamics that goes into high density living apartment complexes high rises is completely different from the dynamics of people looking for their homestead. You know, their little house with a patch of grass in front and behind. Isn't it? So you should treat them differently. They're entirely a different class of problems. You should remove it. Likewise, you should remove the Mack Mansions. Mack Mansions are not the ordinary middle-class people whose need is a simple house with a front and back patch of grass. Isn't it? Mac mansions are a completely different game. And so when you see total rooms, et cetera, populations, once again, you should consider chopping outlier and redoing another. I haven't done it, by the way. Here, I haven't done it by the way here i haven't done it the reason i didn't do it is i have one of the things as outlier detection i haven't exactly taught you about and then but we haven't also said how do you make multiple models for multiple regions different parts of the feature space all of those would come now median home value, do you notice that once again, anything about 500,000 has been squashed into 500,000. Is it 500,000 or 5 million? 500,000. So in those days, in the 1990s, 500,000 was considered an outlier house price. Do you notice that? Very rich people lived in those homes. Very rich people lived in those homes. Today you have a travel buying a simple flat. What are those called? Condos for 500,000 in Fremont, in our neighborhood. You also notice a pronounced right skew everywhere. Do you notice the pronounced right skews in the data? So one basic thing is that when you see a value like that, you say, hey, it will at least screw up my linear models. These are outliers. Let me chop it off. It's a treatment. I'll leave it as exercise for you to do. You should learn to do that. So then I make this plot. And this plot is, as always, very informative. It tells you a lot. I have color coded it by distance from the ocean because that's a big big dynamics in california right uh how far you live a little hut next to the sea can be worth more than a mansion inland isn't it and so those things you'll see reflected now one of the things you notice that this picture is already getting very busy and hard to read right so i'll leave this as an exercise for you carve it out into plots like into smaller plots or four by four plots if you a four by four plots, if you can, four by four or four by five plots, make different groupings of plots, right? Don't make it like this. So then proximity to the ocean and latitude, what does it tell you? That a lot of people, these are low latitudes, a lot of people in South Beach, in LA area prefer living there. A lot of people, like less than one hour to the beach. So yeah, a lot of ocean people are here, but deep inland, as the latitude increases higher latitudes people are well i don't know there's not much to make out of it this is about generalities you can say inland um now i've again broken it up by ocean proximity and so on and so forth so it turns out that if you're living on a on an island do you notice this where my mouse is would you expect this to be the case if you live in an island which island are we looking at here catalina right and so uh you realize that catalina does not support high density living So you realize that Catalina does not support high density living. Those are all very exclusive places. And you see it reflected in that. So you see guys that you're just looking at the data, you can read a story of it. Right? You can literally read a story of that. So for example, when you look at the ocean proximity and median house value, what can you tell? Looking at this chart, island homes tend to be very expensive, isn't it? Clearly very expensive. And near the ocean homes do you see that near the ocean homes are also stretching out into the expensive territory but in those days not so much whereas inland homes are all in the middle class category clearly do you see this blue thing here or the turquoise kind of color it is all in the middle class category that's i suppose is a little bit more clear when you look at the uh now when you look at the age what can you tell looking at this age about island people what can you tell about their age? Most of them are pretty old. They are pretty old. They're above 30s, right? So it takes time to earn the kind of money that will land you on an island. Wow, they don't show above 50. Yes, interestingly, they don't show up above 50. And so that is the point. You look at data and you see value. When you look at ocean proximity, you can see these homes are very expensive. Inland homes are classic middle-class homes. And they have a lot of outliers. There are a lot of Mac mansions hiding in there, inland also, right? To it, if you just go, every time I take from 880 to 580 to 680 and I drive towards Livermore, I see all sorts of Mack mansions on the hills. So I suppose that's what that is. Anybody here who's living in one of those? No, okay. So correlations. We look at the correlation. From this, where are the strong correlations you see between latitude and longitude, right? Well, that is given because they have to be, the locations are. Now, look at this. The most important thing is median house value. What is it strongly correlated with here? If you look at this field, it is most strongly correlated with, which of the fields is it most strongly correlated with? Income. Median income, exactly. Why? Because, it's again one of those things, people, wealthier people, they tend to live in homes that are wealthier, right? The converse is also true. If they move into a neighborhood, the neighborhood automatically starts getting, the house prices starts going up. To which Fremont and these places, now Fremont where we live, it has a disproportionately large relative to the nation, a large population of PhD holders and patent owners and highly qualified people earning quite a bit of income. And so the median house price here has become horrendous if you're trying to buy a house. So that's that. 68%. But the other thing you notice is that the direct correlation to other factors are not that much. The second biggest factor is latitude. Isn't it? Which makes sense because in California, most of the population is localized to specific latitudes la or bay area right and it also goes to the saying here the real estate wisdom that in real estate prices three factors matter as the cliche goes and those are location location location that's right so let's see right uh what we have here now our data has categoricals so you need to do treatment for categoricals as we talked about in the past we get them into dummy variables right and then now what do we do? We are going to build a model. So I'm not going to build a baseline model. Can I please leave it as an exercise to build a dummy regressor? Please build a dummy regressor. Please also build a model without all these log transforms and other things. So this I leave as an exercise for you. I've directly short circuited to a sensible model here you you come here and then once you do all these transformations on the data now the data looks far more symmetric here guys does it unskewed data right so once you have to remove this queue then you do the so first you build a naive linear regression model we get a r squared of 69 percent pretty respectable mean squared error of 0.1 you look at the residual plot not strongly a pattern but maybe there is a small scope for improvement right uh because there is a funneling out and funneling back in in the predicted model, right? There is a, do you notice this funneling out and then funneling back in? So, well, let's see if we can do better. Ah, we look again at the Koch's distance to see if there are any large number of high leverage points, not many. There are some hiding in there. Let's try. Remember, we learned about regularization. So the most natural question to ask is, would regularization help us? Let us regularize. Oh, we went to 69%. How much were we before? We were to 69%. How much were we before? We were at 69%. So did it help us? Did regularization help us? No. It means that this data, actually the fact that 69 is where you are in spite of that means that regularization or rich regularization is not what's going to help us we make the prediction error plot there is quite a deviation between y and y hat oh and at this moment i stopped because obviously i haven't taught you uh some other things so i'll ask you this question what are the other things you could do to the model at this moment What are the other things you could do to the model at this moment? And how would you add features? You could, for example, do polynomial regression to take care of nonlinearities. Isn't it? You could do that. The other thing you could do is you could bring in data sideways. What do I mean by that? See, there is a statement made which says that more data trumps smarter algorithms. Generally, that is interpreted to mean that when you have a lot of data, even a polynomial regression, all the overfitting goes away, and so on and so forth. That's what people what people mean actually while that meaning itself is valid there is another way to look at it another way to look at it is bringing more features data as features so for example here what is missing econometric data distance from the nearest highway. You all know that highway comes somewhere, life changes. Whether or not a public transit train system is there, public transit system is there. A bird goes somewhere, what happens? Bird goes to Livermore or wherever it is that's going on that side, till wherever it goes, what happens to those economies? They go up. So I invite you to do this analysis. Try to get that data sideways and join it to this data and see how does it improve your models. Now, we have learned only till linear regression so far. That is not the end of the game. Of course, in this workshop, you'll learn about a lot of other methods, ensemble methods and kernel methods, namely trees, forests, gradient and boosting and support vector machines and so on and so forth. And you will realize that those nonlinear models, they lift the predictive power up quite a bit. So we will get there gradually. But in the meanwhile, I'll leave it as a homework to try out polynomial regression so on this problem kyle i don't know if you're still awake this late uh maybe kate could you please take notes or harini could you please take notes let's have this homework in this we have to apply the dummy regressor without any transformation see how much the naive model does without any transformation whatsoever dummy regressor naive single variable regressor no no no naive all predictor model right and then builder base model kit yes and then also so there is a something called a dummy regressor and then then also try the normal power transformer see how far it takes you right also if you notice i have not scaled the data scale the data i deliberately left it out so that you could try out scaling the data right scale the data all the method structured process that i have taken i've missed deliberately a lot of them as homework let's do that then you should do that first right yeah we should do that first and then to your point kate let's also add that let's have a simple linear regression model in one variable pick a variable that you think is most important see how much of a lift do you get right and that would be for the base model right no that's other than the base model other than the base model pick one predictor builders linear regression model now i didn't do feature importance do a feature importance here right do it by both the methods just by looking at the coefficients after standardizing and also by doing Shapely. Do the partial dependency plots. See if those things all agree with it. In other words, do everything here that I have done in the auto dataset. Use that as a template and do everything here that we haven't done. So that's your homework guys. So with that, I will take, we are one hour into it. Let's take a very short break of no more than 10 minutes and we'll start again.