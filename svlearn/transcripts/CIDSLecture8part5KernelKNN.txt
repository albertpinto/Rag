 So now I'll give you guys one further improvement upon Kearney-Risneba. So now, suppose you have, when you look at the data, and I'll slightly exaggerate. Let me take this mouse from here. Oh, yeah, I have my hand, actually. This is touch sensitive, so I should just be able to use my finger. Awesome. I keep forgetting that. And then I should disable touch for the time being. See, suppose you're trying to, let's look at this function. So suppose the underlying reality is complex. So I'll just draw the reality and then draw the points. Something like this, right? This is the underlying reality, yellow light, but you can't see that. What you can actually see are the data points. Something like this. Now I ask you this question. tell me the value tell me the value at a point that I haven't seen which is actually let me just for the sake of argument remove these two points I ask you at this value of x, let me just mark it as x naught, sorry, x naught or x tilde just to imply x tilde, what is y, predict y? So what will happen is suppose I asked you to look at three nearest neighbor. I'll take an example. So what will happen is you will take, hopefully, you will take these three examples. And your prediction would be, what would your prediction be? Your prediction would be the average of these three, which would be, this is your y hat. But what is y? Y is, this is the actual y, isn't it? So you had a gap, you had an error. Yeah, y minus y hat. So you ask yourself, is there a way that you can fix this? Can you think of a way to fix this? What if you listen to the nearest neighbor more than you listen to the next nearest neighbor and a little bit less to the one a little far off? Not time series. Hold those arguments. See, this is new. You're in new territory. Don't do similarity. But in time series, yes, same thing happens. More weight is to the nearest one than the... Don't bring that because at this moment, just look at it in its pure form. Actually, time series gets its intuition from here, not the other way around. So suppose you are here. Would you agree with that? If it rained yesterday, that's a stronger predictor of raining today than the fact that it rained one week ago. Yeah, so yeah, sort of in a way, bias and conditioning. But this is it. So in other those let's believe what you do is you take the k points and sort them sort by distance let's call the nearest point x1 second nearest point x2 x2 and third nearest point x3 x2, x3, sorted by distance. And now you compute the distance to all of them, d1, d2, d3. So you computed the distance to, distance to the three points. And now what you do is you want to take a weighted average you want to somehow not see initially what were you doing you were doing y and associated with each of them is a y1 y2 y3 this is fact right this is your data right x1 y1 are the data points Right? X1, Y1 are the data points. What you do is initially what you were going to do, just K and N, you would do Y hat is equal to one third Y1 plus Y2 plus Y3. Just average it isn't it but we are saying that don't do that somehow in and this method is called a kernel knn for a reason that you'll see shortly make your y hat somehow uh a weighted average so let's say y1 times some multiplying factor let's say some kernel k1 plus y2 k2 plus y3 k3 all right so for example those could be multiplied this by three this by two this by one you get the intuition the nearest neighbor multiplied by more or something like that. Does it agree with your intuition, guys? The only question, therefore, that remains is, what is this kernel? You realize that kernel, this K, are called kernels. Kernels. Kernels. Now, when we talk of kernels, don't take a computer science meaning of kernel. In computer science, the moment you say kernel, it means you think operating system in the core of the operating system, right? No, this is a mathematical kernel. Kernel work keeps popping up in this space quite a lot, actually. And we are getting into this territory now. So kernel is, let me define it. Specifically, it's a distance kernel. Now let's define what a distance kernel is. Distance kernel is a monotonic continuous function of function of distance monotonically decreasing function what does monotonic mean means as distance increases it has there is no chance the function will climb up right so suppose i plot distance you know that distance can only be positive is of course greater than equal to zero distance is there now for every value of distance kd it can only decrease so what can be a decreasing for it cannot increase it has to decrease so all of these are valid i can just say what about this straight line it is decreasing is it monotonically like this is valid but what is not valid is if i had said why is this not valid it's more because it starts increasing right so monotonically decreasing and this is this is valid yes and we're coming to anything yes all of those and so let me turn this off so what happens is that this is okay then another thing i could do i could do a step function is this okay is this monotonically decreasing well it stays flat and then it falls this also is okay right i could do like you pointed out uh gaussian decay function that too is fine right i could use a cosine because, cosine curve falls down, cosine wave that also could have been there this is fine anything that is monotonically decreasing will suffice so long as it decreases with distance and at d is equal to infinity it goes to zero are we together and therefore now what happens your y hat other words it is x1 kernel 1 plus x2 k2 plus x3 k3 with of course the normalizing constant you have to normalize it down to uh one right otherwise it won't make sense so for example you remember we did divided by three when all the k's are one you have to divide by three right so k1 let's k2 let's k3 and so forth right you do that and you get a weighted you know weighted average what we are looking for is a very simple concept weighted average isn't it and so you get a weighted average a weighted average isn't it and so you get a weighted average any questions guys so this idea and i introduce you to the very interesting word today kernels kernels are everywhere in machine learning right we'll encounter them a lot and in fact the next week we will start with a new class of algorithms called support vector machines they are all kernel machines and i thought today would be a good segue by first introducing you to uh some notion of it. So this norm thing, isn't this pretty hard to do? I mean, like figuring out what component of the, say for instance, you had three neighbors. Yeah. Two of them were right at the extreme and one was over here. Then this guy would predominantly make the... The nearest neighbor would have a dominant effect here but if you had like say three of them very close to each other then that then what would be the one third one third no no no so see here's the thing suppose your data points are like this one two three yeah four and you're trying to make a prediction at this point good right so what will happen is that this will get a heavy weight yeah these will get lesser weight but ultimately there's a denominator that balances it out that sort of scales it down and so what will happen is that whatever value is here right the real value predicted value will be and let's say that these guys are predicting values here and here right this so what will happen is because you're taking the weighted you're more likely to be slightly less than the nearest neighbor which is what you want Which is what you want. Which is what you want. You want to come closer to the nearest. Yeah. In other words, nearest neighbor should have a louder voice. It's a simpler way of putting it. Let me write it down. Nearest neighbor should have the loud, the biggest say the vote say and the prediction so this is the question of kernel nearest neighbor so now here's some suggestion guys you realize that kernel is a common sense thing right whenever you do nearest neighbor so now here's some suggestion guys you realize that kernel is a common sense thing right whenever you do nearest neighbor thing it's always a good idea to use a kernel i would say that by default you should use a kernel right to not use a kernel is also a kernel so for example let us take this line which remains flat a flat kernel is the ordinary knn isn't it irrespective of how far you are the weightage is the same but other kernels improve upon it so generally uh and and the reason i mention it for example in the textbook knn is there but kernel canon is not emphasized sometimes you get pretty good improvement simply by using kernel-knn rather than knn. And you see the intuition why it speaks to common sense that it will often work better. So use kernel-knn. Next week, we'll do some notebooks using kernel-knn and you'll see what it means. So what you're saying in some words is like a microscope, you need to look at all the small neighbors and see how the values are kind of gradually smoothing out. What is really happening? And write a function which can mimic that. Exactly. Zero in there and then make up local prediction. Yeah, local prediction. Now, related to it, now comes an interesting thought. Okay, so there are two thoughts that I want to do. I'll park one idea, which I shouldn't forget. Kernel plus linear regression is an idea. I'll park it for a moment. Now, let's go back and ask this question. So what is k? What are good kernel functions? So it turns out when this idea came out a few years, many years ago, actually, lots of people began to create their own special kernel. They all had the quality that they would decay. And then they'd say, this is better than this, and this is better than that, so on and so forth. So in the end, obviously, we'll do it in the next lab, you'll see that I'll come out with many kernel functions, the Gaussian kernel decaying, the whatever, I mean, the linear kernel, the sine wave kernel, so on and so forth. There are many such kernels and they all have fancy names. After a few more years of cooking up kernels, people realize that actually it doesn't really matter which kernel you pick, so long as you pick a sensible kernel, the answers are very close to each other. It matters a little bit if you're super optimizing, yes, find the best kernel, but generally, so long as you choose a sensible kernel, you will get an improvement. And we'll do the lab with it the next time. Next comes a very interesting thought. And this is a thought that is lovely, actually. So let's look at this data point. Once again, where is our data point? Blue we use for data points, right? I keep forgetting what color we use for data points. Suppose you do this. You ask a slightly different question and so you can imagine that this will go back, this is your data set three. Data set two right, data set two, sorry. What if I told you that suppose you take a certain number of points, you take a neighborhood, again think of your epsilon neighborhood kind of a thing, right? You find k nearest neighbors. But instead of taking the average of them, you realize that this reality, I can use to make a locally linear model. Because what they are predicting here locally is this, right? because what they are predicting here locally is this right this all of them are predicting different lines isn't it locally this is the line and that line is a pretty good prediction line for points in that neighborhood isn't it or another way to put it is smooth curves have tangents those tangents are pretty good reflections of what's happening locally and so those tangent lines are linear models so what you could take is a few neighbors and just build a linear model are we together so that is where you put your neighbors together and you build a linear model and then you can go even further you can say when i build the linear model. And then you can go even further. You can say, when I build a linear model, I'm going to give differential weightage to the errors, the loss function, the errors. I'll put more weightage to errors made in predicting the neighboring values versus predicting the far-off values. You can keep on doing many optimizations like that. And so when you put KNN together with linear regression models and put the two together, you again have a fertile ground for many ideas. And so, I'm just giving you a sense of the classical in the tradition of machine regression, how many things people have tried. This is what we call piecewise linear regression. This is piecewise linear regression is yeah, this is very close to it, pretty much it, but except that here, the piece sizes are not by discrete, you're not breaking the X axis in discrete sizes, but you're picking it by the number of points, but amounts to almost the same. You can think of it as it is piecewise in nature, but by K, by number of neighbors, whereas you could do it discrete by fixed size segments along the X axis, right? Things like that. So there are many variations and you know, you can sit down and cook up more variations. So this is good for the missing value imputation if you want it to. Yes, missing value imputation and so forth. K nearest neighbor is often used for that. So guys, this is about K nearest neighbor. How much energy do you have to learn about yet another new topic? Or should we skip it for today? Is this particular small line Kn? Yeah. Is it same as when the kernel is not kept to the media no no no no no no no it has not to do with that no see no no you're getting it wrong see here i'm taking this forget about kernel altogether just simple knn then five i'll take ten points near that point wherever i want to make the prediction i will take ten nearest neighbors and i'll build a linear regression model out of those ten points it's like fitting a straight line it's fitting a straight line it will be just a tangent line right tangent line kernel is just a degree one kernel kernel is just a straight line k is equal to a degree kernel is effectively straight flat kernel no no that straight kernel has nothing to do with the tangent that you've drawn do you know the locally linear model you drew you're mixing the two up so here's the thing take the k and n and forget the kernel part altogether and now so let me repeat it what you do abhishek is so here's the thing suppose these are the points i'll take points like this these are your neighborhood points now observe these neighborhood points how they are they are like this if i were to build a simple linear model i just gave you these many data points these are your k nearest neighbors k k and then k nearest neighbors right i just picked the k nearest neighbors i i'm putting it here and i'm saying okay draw me a regression line through these what is the regression line you would draw this part is okay and then based on that line we will predict the model that's right now if we say that from that point what is the distance of these points you could also do that yes like while training the linear regression no no no so be with me see you could entirely forget about kernel and just draw a straight line through the data. Then there are adapted versions of it, which says, hey, you know what? The errors that you have when you're writing the loss function and training the model, you could introduce a kernel on the error, saying that neighboring points predictions getting wrong is much worse than further point predictions getting wrong. getting wrong is much worse than further point predictions getting wrong right so it will fit more more aligned to the nearest points and there so then the kernel would come in and you could use any one of the smoothing kernels that's that that's that that. Okay. So guys, so you remember this for next time for the SP in the three. I just thought your baby distance kernels are basic. Yeah, like the jumping. It's a segue into that. It's a segue, but it's a very basic distance kernels are the simplest sort of kernels. when we go into support vector machines actually you know what we so okay here's the choice guys we took a respite i hope you found that clustering was a lot easier except for den clue where i brought in the field there was not much mathematics here right so it was a respite from mathematics we have two big topics to cover today is the eighth lecture isn't it so we have four more lectures we we have ensemble methods like random forest gradient boosting which will be more or less mathematics free and we have support a kernel methods support vector machines and so forth which are heavy in mathematics so which are heavy in mathematics. So not very heavy, but heavy enough. So which one would you like to do first? Support vector machines. Support vector machines, okay. It ties up with that. It ties up with that. It's a good segue to that, right? But that kernel is slightly different and more complicated, but it is it, it. So we'll do support vector machines next week.