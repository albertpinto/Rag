 Clustering. Detecting clusters. So the definition of clustering is to detecting clusters clusters in data. We say in data, but actually in the features in data's feature space. In the data's feature space, of course, where the data is projected. You're looking for clusters. The approach or algorithms that do that are called the algorithms that do that are called clusters. And there are many, many celebrated algorithms and more and more are coming right you create one more clustering algorithm work under some advisor in a university go get your phd one more right so lots of creative activity that is happening in this now we will start with what is one of the simplest algorithms so So you ask this question, well, clustering is, isn't it obvious how to cluster data? It is in computer science, we learn about sorting, isn't it? You can sort data. You see they're unsorted, you can sort data, right? You can search for data, you can sort data. Most of the computer science textbooks are filled with these methods. Why clustering? When you look at data, let's say that I show you cows and ducks. Here we go. I'll take questions in a little bit. Suppose you have ducks, cows, cows size and let's say this is weight. Now, the human eye, given this data, how long does it take the human eye to detect the clusters? It is instant. But the human, the I in living creatures to the best of our knowledge has gone through at least 100 million years of learning. So you realize in machine learning, the more data you throw at it, the more time you give it to learn, the better it becomes. The eye is perhaps the most remarkable machine learning tool in our universe that we know of. It has gone through over 100 million years of training at it, and it instantly finds the clusters. It instantly recognizes the patterns. Even fishes of eyes. Now, for computers, it turns out that this is actually at least an NP-hard problem. You can't find it exactly. Computers, there is no algorithm that will, in a deterministic way find the clusters for you. Well, that's a shocker. You would imagine this problem is as simple as searching, sorting and so forth. Why is it that we don't have a deterministic solution to that? The reasons are varied. Here, I made the clusters very obvious, but let me make it less obvious. Let's say that you're talking about geese and ducks. Let me take two different colors, one for ducks and one for geese. I'm deliberately creating an example that will sort of overlap. Now, how many clusters are there? Suppose you couldn't see the because you could go making clusters like this. You could say, well, you know what, this is a cluster and this is a cluster too. Or you could say, I see sub clusters here. This is a cluster and I see clusters here, separate clusters here, right? And within the clusters, you may see other sub-clusters, right? So for example, you see sub-clusters. Do you see that the cluster, what you define as a cluster, depends on your optics, depends on the resolution at which you are looking at? Are we together? In the very fine limit, every point is a cluster. And it is hard because of the overlap in data and so forth. It is hard to have a deterministic algorithm for cluster. It doesn't make sense. Common sense we are putting it is, it doesn't make sense because what is missing is what's your resolution and so on and so forth. I mean, many factors. And so clustering is a probabilistic it's a machine learning exercise. Not an algorithm in a typical algorithm textbook but as a machine learning algorithm right So how do we find clusters now that we realize that clustering is little more complicated than we thought let's look at algorithms to. we thought let's look at algorithms to uh to find clustering so i will mention today three classes of algorithms two will cover hopefully or maybe only one will have time to cover but we are going to in this course do three of them one is a k means two hierarchical, what your book calls agglomerative clustering. By the way, this is chapter 10, I believe, in your textbook. And the third is density-based clustering. density density based plastic we'll start with k means and and related related so this is what we will start with today what k means says or at this family of algorithm, what they say is, suppose you have data and let's again go here. I'll take a whole lot more ducks and a whole lot more cows just for illustrative purposes. And in fact, let me see, let me even erase this. Why am I not able to erase? This pointer has become so big, I can't do the erasing part at all. Yeah. Let me do. Our goal in k-means clustering is to find the center of gravity, to find the two clusters, right? Let's say there are two clusters. I tell you there are two clusters. We say k is equal to 2. Somebody has told you privately that, hey, I'm pretty sure there are two clusters in the data, right? Your job is to find those clusters, right? And you say, okay, how am I going to find it? The K-means approach is that finding the clusters is equivalent to finding their center of gravity, their centers. Because if you can find the centers, then every point can be assigned to the nearest center, isn it so for example i could just say this and this and their center is here their center is here but how do we detect the centers we agree right there are two centers there are two clusters let me call it c1 c2 but our job is to find the centers of the two clusters how do we do that and that is the game and so i will make a lot of diagrams i don't know is there a way to copy paste this but i don't know uh shapes let me try no i don't know how to do this. So I'll keep redrawing this picture over and over again. Just to right click my view. Oh yes, I seem to have copied. Okay. Control C and right click copy. And let us go somewhere else. Let us go here and do paste. Beautiful. Beautiful. And I wish I could just erase out the boundaries. Ah, with this big fat cursor, it's a little hard. I think that's when you're erasing you need to select a larger eraser I think so that may be it you're right stroke small large large eraser okay all right we will go we'll go with this and we will remove the centers also the highlighting that I did in the center and I'll bring back the white points. And I'll bring back the blue points. So pay attention guys, this is a very important thing to pay attention to. What do you do? The k-means clustering algorithm, k-means clustering algorithm, it works like this. Initially, you don't know where the centers are. All you know is that we are looking for two clusters. Randomly, pick two points. So any two points, pick, right? and declare them to be the centers as centroids centroids centroids a fancy way just say think of center of mass all right so let us take some points if you're lucky we'll end up with one point in the white cluster one one point in the blue cluster randomly right but we might not be lucky we might be very unfortunate okay so let us say let's look at the lucky situation first and by the way since i've put in so much effort in this part, let me copy paste this because we'll use it again and again. Control C. Control. Okay, we'll use it later. Let's go here. Let's say that we have a lucky guess. We were good and we picked two points. One point here and one point here. This point. These two points. Then what happens? You ask for every remaining point, you ask which centroid it is closer to let's say that this is a and this is b which is it closer to and assign it to that centroid so would you agree that with this we will end up assigning all of these points to a and all of these points to B. Would you agree with that, guys? Yes. You would do that. So that is step number one. So the step is assign. And this is the cycle that you will go through. This is the cycle. Cycle has two parts assign each point to its nearest centroid right and b recompute the centroid. Centroid is the center of gravity of each cluster. And this is the cycle that we'll keep repeating. Iterative process. So here, if I look at that A, the cluster that is around the point A, would you agree that the center would be somewhere around this? This would be A prime, isn't it? And B, you would go somewhere around B prime, right? Yes. And so you would make it that A has moved to A prime and B has moved to B prime. Now that you have recomputed the center, you repeat the thing, assign all points to A prime now, A prime and B prime. So you would realize that the cluster remains more or less the same, isn't it? This is a very clean data, it remains the same. And if you recompute the new centroid, will it move? Will it move to some other location now for each of the cluster? No. It will start to stabilize. Yeah, it will stabilize, right? So the clusters have the centroids are very quickly stabilized this was obviously very clean data and we were in a lucky situation and do you notice how quickly and the so that therefore the the end condition is terminate terminate when centroids stop drifting. Drifting. When they stop drifting, they may oscillate around a little bit. Wiggling is okay. Let me just say, wiggling a bit is okay. Is okay. Right? So we are done. We just found an algorithm to find the clusters. But you can say, well, you know, you're giving us the best of all situations. Let's take something that is not as pretty. clusters but you can say well you know this is you're giving us the best of all situations let's take something that is not as pretty when you said we compute the centroid what what was that you take this cluster around a and you find its center of gravity of all the points but when you stop saying it is the zone of gain the nearest nearest. See, you have this data space. You have data points assigned to A, all the points that are nearer to A rather than B. Each point asks which of the centroids is it closer to? So you would agree that that circle refers to all the points that are closer to A and the other circle refers to all the points that are closer to A and the other circle refers to all the points closer to B. Then recompute the centroid. So that's what you're doing. So now we can say, well, that's cheating. Let me deliberately take some terrible situation this time around. I claim that you randomly picked points and you got a here and maybe you got b right right what about this this point b now this is about as terrible as you could get isn't it what will happen now once again assign to each point the the assign each point to the centroid that is there and you will realize that you will end up with This will be the centroid C1, and this would be sort of the centroid, I mean obviously there is not such a big circle. Even this should not have had such a big art but sort of flow with me would you agree that if you assign each point to the nearest centroid either to a or b it will divide itself approximately like this would you agree well you say well this doesn't hardly c1 and c2 they hardly look like clusters, isn't it? They're about as bad as it could be, all right? And what you have done is you have gone straight through the two clusters, right? So this looks like a disaster in progress, but let's see what happens next. You find the centroid of the A cluster. Where would you find the centroid of the a cluster where would you find the centroid of the a cluster would you agree that the centroid of the a cluster will turn out to be at a point like where should i put it something like this a prime data point or it can be it can be anywhere imaginary point so and the centroid of b is because i see more white uh more white actually more white points and here there are more blue points it will be slightly shifted it will be b prime would you agree do you notice that in the lower half they are lesser blue points Would you agree? Do you notice that in the lower half there are lesser blue points? So it will be shifted slightly. And so now this becomes the new way of looking at it. If you look at this new way and you ask these points, are they closer to this or that? And you would agree that pretty much except for maybe one or two points, it ends up, B ends up owning this. And A ends up owning, except for some points, maybe one or two points, but ignoring one or two points that may be there, ends up owning this. A ends up owning this. Would you agree? So this is your C1 prime and this is your C2 prime. Do you see that, guys? It's slowly resolving itself. It's slowly resolving itself. So it has a tendency to move in the right direction. And so what has happened is that our thing, our A has moved to, sorry, what is the journey? Let's look at the journey. A has gone here and B has gone here. Now what happens? The next time you do it, A, A prime will go where? Would you agree that it will end up here? Because it's the center of this so this will become a double prime right and it will go here convince yourself that the next migration of the centroid a prime will be to a double prime here isn't it because a is slightly to the right it will end up owning all the right upper points b slightly b prime is slightly to the left and it will end up owning all that and so here you will end up with b double prime right so b prime migrates to b double prime are we clear guys do we see this right how do you how do you quantify wiggling i'm just trying to understand the big thing is you you just set an epsilon so okay let me answer your question a wiggling is you can you can hand choose an epsilon limit and you can see if the centroid movement delta of any centroid centroid in iteration i and centroid i plus one the movement of a centroid let's say a is less than the distance the distance that it moves the delta or you can say the distance that it has moved distance between these two is less than equal to epsilon, you say I stop. So you just set a small amount to stop. Thank you. That is it. In practical terms, what happens is the k-means clustering actually for data that is most data it converges pretty fast. There are a couple of pathological cases in which you can get screwed and I'll mention that. So first thing is that do you notice that it is remarkable actually how quickly it converges isn't it? This is lovely. Now comes a question so well then you would say this is a cheap algorithm actually not because every time you're computing the distance of each point from the centroid and if you're in a if you're looking at euclidean distance you distance between two points x 1 and x 2 right is equal to you realize the square root of the distance along each of the axes. Actually, let me write the word x and y so that it's more meaningful. x vector, y vector, then it would be x1 minus y1 square, you know, plus x2 minus y2 square, etc et cetera. Basically, you're doing a summation. I'll just put it this way. So the number of dimensions all the way up to plus x dimension d minus y dimension d squared. So how many squares do you compute? D of them for every pair of points, isn't it? So it gets computationally a little bit heavy as the number of dimensions grow. You have to do a lot of distance computations. So be that as it may, still it converges pretty fast. There are a couple of pathological cases and therefore there are many variations of it, of this algorithm. One is this, suppose data was like this now, let me just put it this way. Yes, for the distance. It is L2 now, it's Euclidean distance. You can use whatever. So yeah, this is a good question. Which distance should we take? Now we learned about all sorts of Minkowski now. Pick one, it will lead to its own clustering. Okay. all sorts of Minkowski nodes, pick one. It will lead to its own clustering. Right? So I was trying to ask the difference between like, is it a norm or is it a metric? Like in terms of the distance that is. See, Euclidean distance metric, right, is there. But you can always go to non-Euclidean spaces. Nothing prevents you. So for example, if the data is projected upon a hyperbolic space or something like that, you can use a non-Euclidean metric for distance. So let's say that you have data like this. And so this is one cluster. And suppose you have another cluster. And I'm just trying to contrive an example of where these things fail. And let us say that happen chance, you pick the initial points to be, by bad luck, to be this and this points, A and B. Then what will happen is the data will partition itself. Do you see? Equally split itself up and down. And no amount of recomputing, the centroid is going to move you away from A and B. It could be, I mean, obviously, imagine it's a dumbbell. I didn't put enough white points on the other side, but imagine that they do exist. This one also has the same number of points. So imagine dumbbell, you happen to have the misfortune of sitting right in the middle of the dumbbell, both the points. Then your algorithm is not going to move anywhere or ever going to make sense, right? It will stop. It will declare in the very first step that this is your cluster. That's it. These are your two clusters. And you know that they are bad clusters. Intuitively, you can see they are bad clusters, right? So what do you need? What you need are two things. First is you need a quantification of how do you compare to one clustering exercise with another clustering exercise. Suppose in the run one, you do once. So this is the white clusters and then there are the red clusters, which is this. Clearly you see that the red clusters are better is the white clusters and then there are the red clusters which is this clearly you see that the red clusters are better than the white clusters isn't it the white lines is clustering would you agree right so but while it is intuitively obvious how do we quantify it how do we know that this time it sucks so there a trick. It is a measure of cohesion. See, intuitively, why do we look at the red clusters and call them better? Because all the points are near each other. They are cohesive, isn't it? They're all very near each other. Whereas if you look at the white cluster, many, many points have vast intergalactic space between them do you notice that this this vast space it was distances between them right there's a lack of cohesion there and from that intuition we derive a measure which is called for any cluster for a given cluster measure which is called for any cluster for a given cluster cluster c i let me just call it c i right so for example this is c1 c2 what you do is take every pairwise points x i x j and compute the distance between them, the squared of the distance, Euclidean squared of the distance is more relevant. And what you do is you sum over i, j, all pairwise distances you compute. So you realize that all pairwise distances in the red cluster will be much smaller than pairwise distances in the white cluster, clustering, isn't it right if you are confused please speak up guys think about the white clustering the white cluster is like this it is saying that this is a cluster so this is if you just join all the points you realize that there will be some small distances between the points but there will also be these big guys right big distances between the points, but there will also be these big guys, right? Big distances between the points that will come up. So you're this thing. And then, of course, you can normalize it by, if you want, the number of points that are there. So this is called WSS. It's between one WSS. Well, let me just say within my, you know, using this as your problem, WSS for the ith cluster, right? All we are doing is adding up the square distances. And what does WSS potentially, what could it possibly mean? Could you sort of guess its expansion? some square distances of course you can average it put one over n there outside to average it out based on the points so within within clusters some squared distance of people, then what happens is you can say now there are two clusters. So I can find the complete WSS is equal to the sum over each of the clusters, CJ, each of the clusters, J, like J, J, the clusters belongs to the set of c1 c2 in this particular case two clusters w ss j add up the two wss right so therefore this becomes a if you think in terms of j belongs to c1 c2 and then we can generalize to c k clusters let's see k clusters and summation over and 1 over n is uh now the well okay you can do all over points etc you can do this distance is x i x j within a cluster x i i'm sorry i j belonging to uh sorry within a cluster remember the spare wise distances are within a cluster as we just now did belonging to c some particular cluster j because actually well i use j here let me use oh boy let me use k here well c m m clusters are there let's say c i j belonging to c k i j both these points belong to the cluster c k so in other words by adding up all of these measures very intuitively you can see that it measures the cohesion right of the clusters and this is the measure that you can use so what can you do now okay we have a measure but what what what do we compare to what you realize that clustering is sensitive to where the choice of initial conditions the initial points that you start with if you're lucky you'll get good clusters if you're unlucky you'll get good clusters if you're unlucky you'll get bad clusters so with k means what you do is you don't trust you don't trust your initial one run you run it a few times you try do and runs of this training and find the smallest WSS. Whichever cluster gives you the smallest WSS, that is your best answer amongst the tendrils. Now in practice, what happens, right, is that you will be unlucky once, very unlikely that you'll be unlucky twice, thrice, et cetera. Or people often do it 10 times if the data sets are smaller, but if the data sets are ginormous, now you're in a bit of trouble. You suppose if the compute time of this, right, is a giant space that will take three days to do. You don't have the luxury of running 10 runs, right? Or running, unless you have infinite hardware, 10 runs in parallel. So people do all sorts of tricks. What they do is they do this. One trick is some modifications they do on the base algorithms. So these are all modifications. I'm just mentioning it to close the chapter on this. What you can do is when you choose a random point, let me just call this random point X0, right? For point A, the initial point and X0B. By zero here, A, the initial point, and x, 0, b. By 0 here, I mean the initial point. What you do is go back to this cluster in such a way that you, let's say that you randomly happen to pick this as your first point. I'll just take this example. What you do is you search for the point farthest away from it. The farthest away point could be possibly this point does this look reasonable right far away point you take a far away point so if this is a you make the far away point b if you're looking for three cluster you will find the point farthest away from a and b now right and and so forth you'll continue along that. So then you realize that you're much less likely to be in a pathological situation. Very likely you'll converge onto the best cluster right away. Am I making sense? You know that this will quickly converge to this, A prime, and B will quickly converge to B prime right so this is one thing that people do so there are all sorts of adaptations of k-means and tricks people use in implementation one one moment I'll come to questions the second thing that people do when they do is see you have to assign point by point each point to a cluster, which cluster it belongs to. Now, there are geometric arguments. One of the earliest one was the use of structures called KD tree. We won't talk about it now, but we'll talk about it in engineering math. KD trees and its generalization are called metric trees. What they are roughly speaking is is they take the entire feature space and hierarchically divided based on some distance metric right some distance metric such that when when you do that in each sub region there are certain number of points and you don't assign each point to a centroid but but your bulk assign an entire region to a centroid easily. Are we together? You can bulk assign a region to a centroid quickly, and that makes it much, much faster. This creates accelerated versions of K-means clustering. In practice, that's what you do. If a metric is available and can be constructed, you do some sort of optimization like that. And the acceleration that comes about from that is, of course, tremendous, but the cost is that you have to first build that metric tree or the KD tree, for example, commonly. You have to sit and build it on the data space. So you do it in two stages. That is it. Now, I will take questions in a moment, but I want to finish the train of thought. There is one thing I left out. Without seeing the data, how do you know what is the best K? How many clusters there are? That question remains to be determined. K means clustering algorithm answers only half the half the problem it says tell me how many clusters you want to find and no matter what number you give it as you will see in the labs it will go and find exactly that many number of clusters but that number may be wrong example, in this data sets of cows and ducks, suppose I tell find four clusters. Now, how do you find four clusters in the data? It will artificially go and find four clusters. Right. It will end up dividing your cows and your duck clusters into two, most likely, isn't it? So it will do that. And you know, it isn't the best way to go about it so that brings us to the question what is optimal k k is the hyper parameter of the model isn't it k the number of clusters is the hyper parameter of the model let me write that thought down what is the optimal number of clusters and the answer to that is quite interesting actually for us to find so this becomes a hyper parameter of this k-means clustering to find so this becomes a hyper parameter of this k-means clustering how do we do that so what we know we know the wss the best wss for each value of k what we can do is we can start with k is equal to 1 k is equal to 2 why k is equal to 1 it may be true that the data is just one cluster you don't know isn't it one cluster may be it k is equal to two k is equal to three all the way k is equal to the number of data points itself of data points itself data points itself now what you do is you create a table you say k and so one two three four five and you put wss now what would be the wss associated with k is equal to n zero exactly and the k is equal to one if you look at the cows and ducks you can imagine looking at the cows and ducks the wss will be pretty appreciably large isn't it right because they are really two plus clusters and so between them there is a lot of what do you call sort of if you think of them as galaxies there's a lot of intergalactic distances right so you you start with wss one two for the value one and so you come with this and then what you do is it is called the ELBO method. The ELBO method now comes the formal name is Scree plot. So look at me, if you guys will look at me remote and so forth. This is my hand. Do you see that the bend in the hand, the most pronounced bend is where? In the elbow. Either it is straight and there is no bend or the most pronounced bend is at the elbow. So, what you do is you take the data for k is equal to 1, 2, 3, 4, 5, 6, 7, all the way to n. What you do is you start with WSS here. What will happen is, let's say in this situation, WSS rises to this. You know that after this, in the case of cow's index, the more clusters you choose from, the more it keeps going down, and ultimately it will go down to zero. This will be the graph of WSS for a given value of k, isn't it? This is k. Would the WSS values look like this, guys? At every increase in k, you will get smaller and smaller clusters by definition, right? And so your WSS will look like this. Plot of WSS will look like this. First, take a few seconds to convince yourself that this is how it will look. It may not have this pronounced bend. For example, it could have a bend like this, like this, but at some point it will have a bend. Either it will be a straight linear decrease or it will have a bend either it will be a straight linear decrease or it will have a bend you agree with that when there is a bend look for the look for the elbow where is the elbow here at which value of k anyone two two it's at two so we say that because the elbow we found the elbow so we say that what your screen plot what your elbow method is telling you is that the optimal k is equal to two. Are we together? This is your best K. And that's how you detect the, that's how you find the best value for the hyperparameter K. So now I would like to summarize this and then I'll take questions. This was a very interesting way to cluster data. It is perhaps the oldest way. In k-means clustering, the main steps are randomly picked centers. I've done it for two clusters. Now you can generalize it to three, four, five, whatever you want, or pick those and declare them to be the initial centroids. Assign each point to its nearest centroid. So now you have clusters, right? A centroid so now you have clusters right a centroid and all its associated points make a cluster you have found two clusters now of the cluster don't trust it yet find recompute the centroid of each of those clusters and mark them as the new centroid and then reassign points to the nearest centroid and keep repeating keep doing it in a cycle. Terminate when the centroid stop shifting, which means that they're drifting. In other words, a little bit of wiggling is okay, like within some distance, if it just wiggles, the nature of probabilistic methods is when there's noise, they'll always drift a little bit. What will happen is some points will keep just oscillating, changing loyalties. They're sort of the people, what are these important points? In two political countries, which have two political parties, usually there are some people in the middle, every election, they change sides. So sort of like that, right? They might keep changing voters, swing voters, exactly. And so there may be some swing voters in between. And so there may be a bit of a wiggle room, we can, but you can stop at any point because you're not looking for perfection you're looking for effective answers effective clusters. You stop that is your K means clustering algorithm now just on this point, do we all understand it, or is there any ambiguity or anything I need to clarify of what the algorithm itself is right okay so then we'll go to the next stage you see that what is the distance measure you take euclidean or any kind of a reasonable distance measure it you can even take non-euclidean distances very easy you can go with all sorts of things metrics so we need a measure you could try different distance measures would this be plot different yes yes of course your notion of distances matter they matter because distance is distance like for example how far it's I'll give you an example uh how far is from me uh to there right well it is as the bird flies or it is like then finally there right so then he is how far is he compared to let's say that I don't have to do that to go to my room then it suddenly looks at my room is much closer to me than Avijit is so how how you define, the word is geodesic. The distance between two points in any, given any metric is the geodesic. So then if somebody asks you to partition, then they should also give you a measure, right? Yeah, implicitly, you always have a measure. That is one of the things. You have to, first thing you do in your mind is pick a measure. Most people pick Euclidean but you shouldn't i'll give you an example what is the distance from here to india the shortest distances you go straight to the earth right so all distances are basically only uh about 12 13 000 kilometers but if you fly with an airplane you can't actually go board straight to the center of the earth you have to fly around it. So implicitly the distance is non-Euclidean distance. It's a geodesic or what is called the great arcs. So the shortest distance, and as you know, it's approximately 18,000 kilometers, not 12, 13,000 kilometers it comes because you have to fly on the surface of the earth, over the surface of the earth. over the surface of the earth. So distance definition is central to all of this. I have a quick question. Please go ahead. So hypothetically speaking, if the data is close to being linear, then how would you cluster? By linear, you mean uniformly spread out? Yeah. Yes. Okay. So hold that that part i'm going to answer that for you huh so no okay yeah okay so but let me just review that but that's a very interesting question you asked now we understand what wss is it's a measure of cohesion it's just within the cluster what are a pairwise distances squared squared because it's it's a nuisance to look at square roots. And a square also amplifies for the same reasoning that we did some square errors. It amplifies what? Large distances. It escalates or makes them stand out. So when you say what is cluster, but we only know where the cluster is at this point because I'd sign with. No, we don't know where the suspect no you don't do that see in k means clustering what you do so that is another point so hold that thought in your mind where does the cluster end that second question will do but let's just review it then we'll answer it one by one actually uh please give me two minutes i'll drink some water Actually, please give me two minutes. I'll drink some water. Right? So on and so forth. See, ultimately, the results you get is based on your initial setup. And then you set up an experiment. You decide what your feature space is. And you decide what you're going to look for. And then you go looking for it. And the results you get depends on that. Whereas going with that, if if i chose let's say i want to see how many widgets are there how many pages are there in that group and that's why i chose name then i do that my distance functions can be completely different from had i gone with yeah in fact distance function would be just just binary if the names match, distance is zero. If the names don't match, distance is one along the distance axis. That's it. And you use that because you can't argue that Abhijit is closer to Banerjee, but further away from Kartik. That wouldn't make sense. That has to come from the domain. The domain has to specify. Domain has to tell you what it is, right? That's how it is. So in all of these K-means clustering, all clustering algorithms, one of the most important thing is what is your metric for distance? And surprisingly, that is often the secret sauce of companies. People spend, you know, learning a good metric itself is a machine learning exercise. Here we are learning the, we are learning to, you know, walk first. We are learning, assuming sort of, okay, let's take Euclidean metric and let's run with it and so forth. So we get the basic idea of clustering. But life gets more interesting when you ask, okay, what should be my metric? And especially for people who have done search engines and so forth, often the metric is the secret sauce in many situations. You don't want to reveal that you learn that i see the screen plot when you come you get that optimal end right is that an indicative of the number of dimensions in which actually the data nothing to do with the dimension nothing to do with the dimensions it has only to do with how many what is the best number of clusters k means clustering can find that's all nothing to do with the dimensions uh somebody on the um and zoom had a question yes uh as if uh so uh okay can you go to the plot of that n like you started from one two and right so you said like the competition here is quite uh heavy i mean it gets uh compute heavy uh program so in practicality like we'll start maybe with k equal to one compute for some time k equal to three and then make a decision if you find the elbow or not the moment you find the elbow see here what i would do is by the time i reach k is equal to four i would say no point moving further i already know my answer so what people do in these uh sorry i'll just sit down i'm a bit tired so can you guys see me yes yeah what you do in real life is first of all you know the very first time you do these computations with the data you don't know what the optimal case right so for example these things are typical exercises inside facebook etc you have a giant social graph you're trying to cluster you find the groups and so forth so you don't know but after a little while what happens is you know it so k today and k tomorrow are not going to be terribly different the next time you do the clustering so you already know what the ideal case and you just search around it that did it drift did the value of k change in the last month or in this right okay you don't go and search like ab initio means you don't know right in the first day you have to try a lot so in practical terms you don't have to the second point that i made is that you know the number of tries people tend to like for example when you run uh this uh sk learn scikit-learn it will ask you how many number of tries otherwise it will assume but in reality what happens is is people use other optimizations, like for example, taking the furthest point. They know that it worked on the data once, the data will only slowly change. So it must work on the data the next time they do the same exercise. So in real life, now you learn a lot from experience and you use that to decrease the amount of computations you have to do okay so uh if you just like i want to calculate maybe some rough estimate like uh my data points change from million to five million and my optimal k was maybe six uh if i want to recompute everything between four and seven four and eight at most okay that is that is a most likely it is between five six seven that's it right and and the most likely situation is if you're just going from one million to five million you will find that your your optimal value is still sitting squarely at six okay and the other thing is that how much of a degradation there is if you stick to six, right? See, one of the things is you found six, you're going to cluster by six, your data changes, data pattern changes. So it is called model decay. Your model decayed. The reality has changed, the shape of the clusters have changed. But how much change is acceptable and how much is at what point you say that now it's screwed up generally in practical life a little bit of decay you tolerate because it's still working and the computations are expensive if you're doing big data computations right so you wait for a little bit and then you recompute okay okay got ited you for the first modification to k-means. The first one is saying that when you take the initial points, you said randomly pick two centroids, two points and declare them to be centroids. The trouble is there could be pretty bad choices. So what you do is you take a point randomly and the second point you take to be the as far away from the first as possible. Got it okay thank you right and then generalize it to three so suppose you have to find three clusters you find a point find a point furthest away from it and then you find the third point which is furthest away from both right and therefore you can keep generalizing to k clusters that's what you use and the other optimization is uh instead of assigning one point at a time to a centroid and finding it out and computing, which is brute force, you can use the geometry of the underlying space. You can use these things like KD trees or metric trees. They are ways of sub-partitioning the space into rather more granular, more sort of a coarse grained, more bigger boxes, bigger regions, such that each region, you don't compute it for each point, but for each region. And you know with confidence that I can assign this entire region to this cluster. So to this centroid. So looking at a block rather than... Block, exactly, exactly. In the California data set, it's. So looking at a block rather than block. Exactly. Exactly. In the California data set, it's almost like looking at a block for median income, et cetera, rather than each house. So that makes the process run much faster. All right, guys, so we talked about K-means clustering. I noticed that we are running out of time it's going to be one o'clock we have a choice in the afternoon we can go into other another algorithm which also will be intense but i think um maybe we'll is this okay are you all fine with tuesday maybe we'll let's do it slowly and properly so let's keep it for tuesday for the second one and next week for the third one these are all big topics let's understand them properly we have another six minutes any more questions before we break for lunch so ask him so if i'm looking at if my value is smaller so does this mean i'm seeing the global structure and if i take my k value to be higher like it is smaller that is right the bigger the case yeah the bigger the k the smaller the resolution the more close you're you're finding sub clusters okay so so it's kind of balancing between the local and the global exactly exactly that's what you're doing so that's where the concept of that That's what you're doing. So that's where the concept of the UMAP came. UMAP is quite related to that, but it is an idea coming from the... See, UMAP is a way of projecting data into lower dimensional space by looking at manifold embedding, which is a topic, again, a lovely topic for math class, not for here, because I have to explain what a manifold is right uh not for now and those of you who are taking this is a first encounter with machine learning absolutely ignore all of these tangential questions don't don't be don't be worried about it first let's get your foundations right. Then when the time comes, you will learn all of this in the next course. More of a general question, right? So clustering is an algorithm that we're learning right now. When you're approaching a segmentation problem segmentations are clustering problems market segmentation are almost always clustered for example your t-shirt sizes are determined how did you come up with small medium large extra large to excel etc clustering data clustering