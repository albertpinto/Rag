 Support Vector Machines. Now we are coming specifically to the support vector machine. What is it? See, I will first, before going anywhere, I'll point out an interesting observation. We'll do a little experiment. Suppose I give you data points along a coordinate axis like this. This is your coordinate system. Imagine that you have a circle of data. Inside are mostly. I hope. And you have to distinguish them from our points pink points or red points. So you ask yourself, obviously since we have done the lab, you have a hint of how to go about it. Suppose this is my x1 and x2 axis, two-dimensional data data you need to write a classifier for this what would be your instinct how would you do that based on everything that we have learned so far in the labs how would you solve this problem using let us say logistic regression so just use can only use not just use can only use limitation rule can only use logistic regression classifier you're not allowed to use anything else so you know that if you use logistic regression classifier what will happen it will draw a straight line right and it will fail you all agree with that right yeah no straight decision boundary but then if you so now we'll do an experiment. What I will do is for every data point, x1 and x2, we are going to do a transformation of x1, x2, to suppose this is your data, this data point here, x1 and x2 it gets transformed to transform to x1 square x2 square right so now let's see in this space new space so this is the original feature space this is the transformed feature space, right? Now, let's see in the transformed feature space, how does this look? So, all points, you know that the x1, so let me call these things as x prime and this as y prime so sorry x2 prime x1 x2 prime right square all points blue points is less than equal to radius square would you agree right but now in the transformed variable x1 is just x prime and y1 x1 and x2 x2 square is equal to what by definition it is x2 prime isn't it literally it's a definition i'm going to a different space in which the axes are x prime 1 prime is equal to x1 square x2 prime is equal to x2 square and i'm looking at the data and so what does this equation now look like x1 prime plus x2 prime is equal to less than equal to some constant is that a linear equation yes it is the equation of a no line in this space it is the equation of a line actually and where all the red points are outside so now do you think we can apply logistic regression in this new space? So we will ponder over this journey very carefully. What just happened is an interesting piece of magic. We took a nonlinear problem, a problem that evidently had a nonlinear decision boundary, but by an appropriate transformation, we went to another space where the problem became linear. So you may argue that, see, well, you know what, this is about a circle. What if it was an ellipse right so then let's take the question of ellipse ellipsoidal data then once again suppose you have points like this and we'll generalize gradually you know that ellipse is nothing but a deformed circle, right? So in the original space, x2, right? What you need to do is transform it into another space. Now, you realize that the equation here is x1 squared over a squared plus x2 squared over b squared is equal to some constant c square right equation of an ellipse again you just make these are your x primes x1 prime x2 prime and once again you will end up with the equation of a straight line and you can draw your decision boundary in a similar way now without going through more use cases I'm going to make a statement from all of this the statement is let me take a few more examples remember that you had a sign you had data for regression of a classification whichever you want to take like this in the original space of x1 x and y this equation was this is a regression problem this is a equation you write as as you cannot write it as beta naught plus beta 1x can be that wouldn't do but we this was not okay not possible but the moment we created went to a higher dimensional space this was okay x squared plus beta 3 x cube you realize that when we went to this space the problem began to solve itself right we did this exercise we can add a few more terms this is it so what is this this is a now let's ponder over it from a different light instead of thinking of it as polynomial regression now suppose I change the names again as this is x1, this is x2, this is x3. So then what does your equation look like? Pay attention to this the equation this is a subtle point but it captures the gist of what i'm going to say just by polynomial expansion you can now write it as beta naught plus beta 1 x 1 plus beta 2 x 2 plus beta 3 x 3 isn't it this is a linear equation equation isn't it it's a linear equation in a x1, x2, x3, and y in 4D space, right? Where the axes are x1, x2, x3, and y. Do you agree with it, guys? Right? So from this, we will have a lesson. A lesson is any curve or surface which is curved, which is not linear, you can lift it to a higher dimensional space and in the higher dimensional space, it will become linear. Are we together? No, no, that is different. Box cog says for hypothesizing, what is the real relationship between the target variable and the predictor variable? What power of the target variable is really linear in X? And slightly related, sometimes you can do that but this statement is more general what i'm saying is no matter what the curve is or what the surface is or for example a decision boundaries of a regression the best fit hyper surfaces you can lift it to a higher space where it becomes a hyperplane. Because this is an equation of a hyperplane. It's a linear equation in a hyperplane. That result, you have to agree that it is true. In other words, any curve, however crooked, can be lifted to another higher dimensional space where you know is guaranteed to linearize itself right and that is a fact mathematically we can see it in many ways for example any function can be expanded in polynomials if it is a transcendental function it will have infinite number of components, infinite number of polynomials. If it is not a polynomial, then you can represent it with finite degrees of polynomial. So polynomials is one basis to write that function, expand that function and take it to a higher dimensional space. There is another way that you can do that. That is using the Gaussian functions, bell curves. Those are called radial basis functions, that's a fancy word for it. But basically what it means is that you can think of any function as a summation over many, many bell curves. So I don't know how to do that. Let me try, just off the top of my head let's see if i can bring the intuition here suppose you have a curve which is like this like this right and how many bell curves can i use i could use this i could use this i could use this i could use this i could i could have lots of bell curves here right do you realize that if i have a lot of bell curves more around the center what will be the sum total of all these bell curves it will be the upper one right this will be the sum of all these bell curves right so what what you are saying, and this could have been going like this, it could have been going like this. So then you can again put some bell curves here. So any function can be expanded in some way. I won't get more precise than that. I will leave the word meaningful vague, intuitive, meaningful orthogonal basis. Why? Because we want our axes to be perpendicular to each other, Cartesian axes, right? So orthogonal basis. So that is a statement. So in other words, anything can be lifted to a higher dimensional space. Now, sometimes you have to worry because if you have a sine wave, for example, then when you try to lift it into infinite polynomial degrees, you will end up with a polynomial of infinite number of degrees. Polly to explain it. Because the sine wave has infinite number of bends so it will take polynomial of infinite degree to capture it simply put right that's the intuition so you have to be willing to lift data and this is a this is a point that i want you to ponder over can you imagine what it means to lift data and go to a space which is infinite dimensional. We have difficulty imagining three is hard enough, isn't it? But four gets us into difficulties. Beyond that, we can't. But at least conceptually, we can. And we can go to infinite dimensions. It just turns out that most of the interesting things that happen, not most, a lot of the interesting things that happen, not most, a lot of the interesting things that happen actually happen in infinite dimensional spaces. For example, this thing, the wave function. We don't think of atom as electrons. When you look at electrons, protons, neutronsrons you think of them as a wave function you don't say it is here you realize that right you can't tell the position of an electron or the speed but what you can tell you can write a wave function it's sort of for this diffuse cloud right quantum thing a quantum wave function then which is sort of somewhat localized somewhere but if you try to get too precise about it that you have no idea how fast it is going it's the heisenberg's uncertainty principle so these wave functions that represent an electron right and it's more than that xp like but okay let me just cheat and write it as this this is this thing right starting as an equation right for the equation you can't destroy the equation right no i didn't get that excluding this wave function right everything is wave function yeah the equation uh shooting is wave function yes captures the fact that the relationship that kinetic and uh the relationship that kinetic inputting kinetic and potential energy right is the total energy of uh total energy of that so in other words del square up to a proportionality constant right plus the potential of a right uh of a wave function this is a kinetic by the way gradients you can one basic intuition is the sharper the gradient the faster something will move right but anyway it turns out that in quantum phenomena this is the gradient this is proportional to in fact equal to um i d dt so when you take the time derivative it's a measure of its energy but we won't get into that so let's not nice this is it it's it's a statement of the kind it's a very like once you get the idea that what does kinetic energy of a wave function map to it's a kinetic risk potential is the total energy so this wave function lives in a space you ask this function where does it live like what does it look it turns out it lives in an where does it live like what does it look it turns out it lives in an infinite dimensional hilbert's hilbertian space right now what is hilbert space we will do when we do the math course but it turns out that hilbert spaces are very very important especially in fact there's infinite dimensional hilbert spaces they have certain very good properties and They have certain very good properties, and we use those spaces in the support vector machine machinery. The the whole theoretical framework for this they are central. We won't go into that at this particular moment, but just take it for granted that you can lift something in a higher dimension space. It's not enough good to hire higher and it's okay to go to infinite dimensional space are we together the basic idea is that if you have a point in the original space you can always go you can do a transformation to let me just call that transformation phi you can go to phi of x such that the in this space the space the problem the decision boundary let us say boundary or the best fit hyperplane for regression is linear which is lovely right it becomes a straight line And you saw an illustration of it when I was doing this. You literally saw that it became a straight line in a different space. There's always a space in which every problem can be linearized. So then the question is, how do we go find that space? Isn't it? And in general general it's pretty hard you can do that but computationally that would be very hard but it turns out that you can you don't need to the interesting insight that is there is called the kernel trick but we won't get too much into it but i'll i'll say it like this that at the end of the day what matters is the dot product of x it's a function a function is a so this is a kernel function with any of the points i right kernel functions very much like the distance kernels that you these are always functions of dot products a kernel function is function of the dot product x so let me just say x i x j x i x j so you take the dot product between two vectors right and now think of any kind of function of it right and you will realize that well that's the definition of a kernel dot product now the typical now how is that related to our thing? Using these kernels, you can actually write your function. So let me use the notation that your book uses. What it basically says, I think it deliberately, it brought in fx, if I remember, fx, which is, think of it as your y, is equal to the way it said it is, beta naught plus summation over i belongs to only the support vector so what happens is this is the final result you can do the dot product of this point with every point right to find it out but it turns out that of kernel x with xi of this point obviously i keep forgetting to put the this so you could represent it like this and I believe this is yi is this there or not in the notation give me a moment I want to be exactly adherent to the notation that your book uses give me a moment is there yeah it should be there remember we were always doing y i times the distance right so what we have done is we have replaced remember we used to do things like y id xi greater than equal to something constraints right this is actually generalization of this is the kernel so you can write your predictions as this it turns out that you can write it like this now the beautiful thing is this equation actually is in a even though you went to the higher dimensional space the only thing that mattered is the dot product you can convince yourself that the dot products remain the same in this space and that space right two vectors right the dot product is a same in this space and that space, right? Two vectors, right? The dot product is a certain quantity and you want to preserve that. So the amazing thing that happens is. And we saw that intuitively that to make a decision boundary, only the support vectors matter as oh, sorry, as is the set of. I'm trying to gloss over most of the math here s is the set of support vectors so you take the dot product of this point with all the support vectors that's all that matters and when you do that and then you do the kernel transformation. So there are many kernel transformations, three in support vector machines, our libraries are very, very popular. One support vector is given x and xi is equal to quite literally, it is dot, it is literally this x x xi i mean i keep forgetting the the vector symbol please forgive me the simplest support vector you can think of is literally the dot product isn't it i'm sorry not i'm sorry what am i saying the simplest kernel that you can think of is simply the dot product would you agree right and then this is the this is called the linear kernel you're not doing any transformation to the dot product well you can be a little bit fancier you could do equal to one plus x x i to the power d right you can add some beta naught yeah if you want some constant so what are you doing now when you go to degree d and you expand it out you realize that you're going you're introducing polynomial degrees of x do we see that guys this thing right we are introducing polynomial degrees of x but it is still in terms of dot products because computationally dot products are far easier to just find than actually going to that space higher dimensional space and then drawing the decision boundary and all of that you don't have to do that. And then the last one, one more kernel that's very popular is the radial basis. I mean, this is the Gaussian kernel minus xi square, right? Up to a gamma, there is a gamma constraint here. up to a gamma, there is a gamma constraint here. Are we together? So let me write it in a cleaner way. Now, imagine that this is written like this, e to the minus gamma, a modulating factor, x minus xi. Now, what is x minus xi? It is the distance vector, right? Difference vector. Square. So look at this kernel. If you expand this, how many polynomial degrees of x will you get? It will be again infinite. It is infinite. That's why you say that you have taken this data and in effect, you have lifted it to an infinite dimensional space. So let us look at the kernels and we'll talk about it. Let's talk about the kernels a little bit. Polymer kernel. This is the uh gaussian and by the way cook up anything else that you want right so for example most libraries will come with these three built-in but they should not be the limit you should you should create your own kernels and see what happens so let me give you an implication of it if two points are very far like one is in this direction and one is in this direction what will be approximately the dot product so if this is xi and this is x if this is almost 90 degrees what will be the dot product zero diagonal there's zero it won't matter much right so you would agree that most points matter which are in the direction pretty much where cosine theta is not zero right you have appreciable cosine theta so in other words this looks for vectors or it is influenced by points in the general direction of the vector right but it is sort of linear polynomial complicates it but pretty much is the same thing you go to a different space and you do that but this radial basis function is more interesting while the two are simple the first two let's look at the this gaussian kernel so this is e to the minus gamma x minus xi square guys i'll just forget to put the bar the vector bar so this for me is the same as e to the x minus xi will that be okay if i write it like this without the bars over it so if you see me do that same thing right so now look at it suppose two points so suppose this is x and this is x1, sorry, this is x1 and this is x2. So this is d1, this is d2, right? Now you realize that when you expand this, this is just looking at the distance between the two isn't it this is the distance thing is basically d i squared right so when the distance this gaussian function is how does it behave? What is the shape of this? This is the shape of it, isn't it, guys? This is how the Gaussian looks, right? e to the minus, suppose this is x, e to the minus x squared looks like this, right? What does it mean? If, and so suppose this is d, then this is e to the minus d squared, will look like this right what does it mean if and so suppose this is d then this is e to the minus d square will look like this so it means points that are closer will have more influence on it points that are far away won't matter is is this clear or not far off points won't matter because they are too far off and so e to the minus gamma d2 square is approximately zero. Right? That kernel part simply won't matter. The only ones that will matter overwhelmingly are its around it. Which is why I call Gaussian kernels as the torchlight. Somebody asked this question a little while ago. Why can't we give the boatsman a torchlight? Who asked that? You asked that, exactly. So Gaussian kernel is the, so in the previous approaches, you were not allowed. Gaussian kernel is the so in the previous approaches you were not allowed gaussian kernel is effen effectively the torchlight so how many how far can you see with the torchlight limited zone right if the banks are here what wherever you are you can shed light just around your boat right you can see the banks near you, the margins near you. Are we getting the intuition guys? What it means is suppose you have a decision boundary, which is actually convoluted, a real life, but the Boatswain doesn't know that the real decision boundary is something like this, very complicated. Actually, let's make it even more complicated let's make it let's say something like this let us say this is the truth but the boatsman doesn't know that what you have is the person is sitting here is rowing the boat let's say where do you want him to be rowing the boat let's say rowing the boat here if this person has a torch light he will be able to see the data points only around this region isn't it local are we together not? He'll only be able to see locally. Now, given the fact that he can see locally, he will detect some houses here and there. So I will just take the conclusion of what he will do. He will draw his maximal margins like this, and then try to stay between the margins and say, oh, I see the banks because why? I saw a house here. I saw a house here and I saw a green tree there. Sorry, where's my green tree? Here's my green tree. Here's another green tree, right? So this person is able to see locally what's happening. Am I together? So locally you make your margin, but in the very small, you realize that every curve locally looks like a straight line. And so every river, however much it meanders, in a very small region, it looks straight, isn't it? And so what you're doing is you're using this as your guide, right? You are shining light only locally and finding support vectors there. Are we together? And if you do that because you're not looking globally, you realize that your decision will keep changing and you will sort of flow along any curve that is there easily. Does that make sense? Imagine the boatsman is shining the light locally and every time drawing the maximum margin hyperplane. Right? Or in other words, detecting the riverbanks. Then the boatman will have a pretty good time rowing straight to the middle of the river. Is that intuition making sense? And that is it. What this kernel does is, in some sense, it tells you how you're shedding the light. That's one intuition that I have. Making sense now. So this is it. And that is why, for example, the Gaussian kernel is almost the default. It's the most popular kernel that is used in support vector machines right now let's focus on this parameter and before we break so e to the minus gamma d square right i'll just write d d being your x minus xi square right so this is your d square is equal to this now the bigger the gamma what happens to the the influence so suppose you have a gamma is equal to one let's say gamma is equal to one so suppose i make gamma is equal to 10 what will happen now? It will decay faster, isn't it? So e to the minus d squared gamma is the same as 1 over e to the gamma d squared. You increase gamma, denominators starts getting bigger and bigger, isn't it? Faster. So this number will start becoming smaller, much quicker. So you can say, therefore, that this is, maybe gamma is equal to five. And then you can have maybe you're getting the idea, basically, right? Gamma is equal to 10. I'm just using big integer numbers. You don't usually do such big things, but let it be. So what does it mean intuitively in terms of our boatsman and the torch we kind of expand how much we see exactly lighted cars or how much narrow lighted cars the bigger the gamma the more local you are right and the smaller the gamma the wider the light you are casting the more you are looking that it is basically gamma determines the radius gamma determines the radius of the shining light of the torch light big gamma small radius small gamma big radius are we together and that is it so what happens is if you know that the data is highly non-linear and there are subtleties you know all over the place well you will have to choose bigger gamma right if the data has small, gradual, undulating curves, then you can take a pretty small gamma, shine the light wider. You don't have to keep on making hyperplanes every 10 feet. Find riverbanks every 10 feet. You can sail quite some distance before you once again look around. Am I making sense? In other words, you can go quite some distance, still once again look around am I making sense right in other words you can go quite some distance still assuming that the river is straight before you recompute so this is it guys this is the concept of support vector machines there's a lot of mathematics intricate mathematics I will do one thing I'll write a a, I'll release a notebook in which I try to explain some of the math using intuition, using our river and boatsman and bank, river banks, using our sort more mathematically precise. Today, I mean, obviously while explaining it, I have not been very mathematically precise, but I hope you got the big ideas. Are we getting the big ideas? Yeah, that is it. That is it. So there is a lot of underground machinery. Like for example, when we come to this result that says y is just a function, prediction is just a function of beta naught and y i times the kernels right now let's look at the simple situation what does this become when you take kernel is a linear kernel this equation becomes let's let's go and see what does it become this this y hat becomes some beta not this let's say or something plus linear kernel means x xi the support vectors right this and what does this look this basically these x i's are like your betas right beta 1 beta 2 beta 3. That's what it begins to look like. Are we together? If you look at it as a dot product, this is nothing but if you think of it as summation over Xi, right? In some sense, you're just looking at the dot products here. So you can think of it as a linear equation. In fact, you can map it as a linear equation convince yourself that it is this suppose there are two support vectors x1 x2 let's expand it out plus x dot x1 plus x dot sorry let me use this notation x2 so they will be component wise uh if you expand x vector as well i use one and two for the data points let me do one thing i will use the i subscript at the top the points because i don't want to mix the point I'll let it be let it be I've already used a subscript so now I can write it as beta naught plus X can use x i j yeah I one yeah x one well well obviously now it gets let me just call it x 0 everywhere this is x 0 x 0 1 plus x i 2 x 0 2 you see this go forward right now these are what these are your you can call this beta now you now sorry a plus you will have x i 2 x 0 1 plus x i well uh well this was x 1 1 x 1 2 right because i is equal to 1 here, and i is equal to 2 is this, 2, 1, 2, 2, x, 0, 2, plus this. So this equation, I can put these two terms together because they belong to the same coefficient. So I can write it as x, 1, 1, plus x, 2, 1 for these two terms, and then write x, 0, 1, first component of the x vector, plus now this one. I can club these two terms and then write x zero one x zero first component of the x vector plus now this one i can club these two together and call it x one two x zero two plus sorry uh plus uh when i add these two up x two two x zero two so when you have just two support vectors is it is like this x zero two so when you have just two support vectors is it is like this and what is this this is you can think of it as beta one you can think of it as beta two isn't it so your equation becomes beta naught plus beta one x zero one plus beta two x zero two right the point. And this is literally your linear equation. So when you take a linear kernel, the a dot b, so you notice something very interesting. We have been saying this for the longest time, that, well, okay, of course, this is a very linear equation. Given those support vectors, you can define your betas like this. You end up with a linear decision boundary isn't it if y is equal to this you see that it is linear in x so with a linear kernel you can only make a linear decision boundary with a polynomial kernel you can make a polynomial decision boundary with radial with the gaussian kernel well it gets much more interesting you have to do things as you do, you have infinitely many terms. So you have to be careful. So it is linear in infinite dimensions, but you don't bother. What you do is you solve the problem very locally. So this last part of support vector machine's journey, the only thing that is, so let me summarize it. Any data can be lifted to a space which is linear. But when you compute it, there is a kernel trick. Kernel trick says, just look at the dot product of this point where you want to evaluate something with other data points in particular with the support vectors and that is enough for you to get the answer you don't actually have to compute the transformation to higher dimensional space it will do the job for you now there's a lot happening mathematically behind the scenes that we are not looking at right but this is the main idea so long as you remember this idea that it is a magic done with kernels or dot products and how it is done maybe you can sort of forget about it but the point is you can lift any curve or any thing to higher dimensions linear. If it is linear, everything that we have learned so far applies, isn't it? Maximal margin hyperplane, soft margin classifier, hard margin classifier, all that theory becomes applicable. So the only other extra is there is a magic trick to lift it to a higher dimension. And then there's a computational trick, which is that you don't actually have to find the transformation function it turns out that the kernels are enough or the dot products are enough and that is the theory of support vector machines but you observe this that what it says is it will there is no assumption built in especially when you use the Gaussian kernel. It makes no assumption about the shape of the data, isn't it? The decision boundary could be any curve, arbitrary curve. When you do logistic regression, what is your assumption? We assume a linear hyperplane, I mean a hyperplane, right? So this, when you do a discriminant analysis, we made an assumption that data looks like Gaussian, spread out Gaussian, right? Each class. Here, you're not making any assumption whatsoever. It's a very general approach that is applicable, which is why this is actually very widely applicable when it came out it did lots of things for example handwriting recognition uh medical imaging even today heavily used in the medical imaging industry right many many things it is support vector machines are there from embedded devices to giant machines, giant computations. It's one of the workhorses of the industry. Now, recently, there has been three groups of three class of methods are very dominant in machine learning at this moment. One is kernel methods, of which I gave you an introduction. The next week, we'll learn about ensembles. Those are like ensemble methods, very popular. And deep neural networks, again, very popular approach. And nowadays, emerging graph neural networks. So quite often, these four dominate the scene in solving problems there are other algorithms of course the no freelance theorem says that any given situation you don't know which is the best but by and large from experience you see that when the problem gets tough generally you find yourself using these methods all right right, guys. So that's all I have. If there's any question, you can ask me. Otherwise, we'll break for lunch. I have a quick question. I am confused. So X0 and Xi. So Xi is the points or the support vector? I don't know what you're saying. I apologize. I don't know how my phone stepped in. Okay, go ahead. So we have a dot product of X0 and Xi, right? Right. So Xi are the support vectors? Exactly. They are the support vectors. And what is X0? The point at which you want to evaluate, if it is if it is classification you want to know whether it is a blueberry or cherry okay in other words x0 is the test point where you need the answer okay thank you that's it okay so this is it in other words all you have to do is look at x0 or that point x. I put 0 there, just for simplicity here. But given any point x, find its dot product with all the support vectors. And you are well on your way to solving the problem. That's what the big message is. And so dot products are important. You will realize this kernel, these dot products, they're magical creatures, actually. this dot products, they're magical creatures actually. We do dot products, but then we step away from it. But within machine learning, they keep popping up all over the place. Almost everything or a lot of things can be written as dot products. For example, your linear equation itself is a dot product, beta naught plus beta hat dot x, isn't it? It's a dot product. So dot products are everywhere in machine learning. And therefore, and again, one way to look at it is, if you just want some extra reading, learn about kernels. And how these kernel methods are used for data, for predictive modeling and so forth. Last week, when we discussed, we said like in higher dimensions, your data becomes narrower in terms of a Gaussian curve. That is true. So we start dropping off higher dimension stuff when dimension reduction. So here you don't do all that stuff. You just map it out into the space. Those problems sort of remain. See, when the original space itself is high dimensional, when you lift it to, when you do this kernel methods, right, there is something very elegant. I'll come, maybe I should talk. You go to even higher dimension, right? And then you ask yourself, now that the problem has linearized there, can I, what is the hype? what is the dimensionality of the hyperplane that captures this data? Right? It will so turn out that you can project down from there to a much lower dimension. So suppose you started with 10 dimensions. You went to infinite dimension or something like that. Whatever. And from there you do PCA, it's a dimensionality reduction technique. You call it a kernel PCA because kernel took you to high and then your PCA, you come down and you might come down to three, but in your three dimension, the data is now well separated and looking nice. And then you do the analysis in the three dimension. Okay. So we're going to come to that or yes as we as we make progress with this workshop sooner or later i'll cover these topics in the lab okay all of these things are part of it and you know we'll keep iterating it in the next workshop in the next all of these ideas will come so you'll go through multiple iterations of it. So why not remain in the height? What's the problem if we have separated it? Because you don't know the transformation equation from here to there sometimes. You know, you never found, like, for example, I told you in the case of a circle, x1 squared and x2 squared are your new dimensions. Great. The problem with support vector machines, as simple as, you don't have to find the transformation that takes you there. You just get away by doing dot products. So there's a subtlety, computational subtlety there. That's what makes it efficient. And so then from there you fall down to three dimensions, which would be three dimensions, two dimensions yeah so which is why and obviously we'll do dimensionality reduction the traditional way for example your book mentions pca principal component analysis a very very powerful method actually next time maybe we should do dimensionality reduction it's a core topic before i move ahead so then kernel pca is to do the extra journey you first go up to higher dimension space and from there you reduce down to a lower dimensional space Vipul Khosla we break for lunch? Was this fun? Support vector machines. I hope you found it. The end part, of course, when I went to the nonlinear situation, I did do a lot of hand-waving arguments because I wanted you to just get the intuition. The math gets a little bit tricky. It's elegant, but it's tricky. You have to carefully develop it. When do we use linear kernels and when do we use Gaussian kernels? Oh, linear kernels are computationally very fast. So if you suspect a data is linear, start with a linear kernel, see what the performance is before you go to polynomial or gaussian kernel okay it just helps and we'll do all of these exercises in the lab kyle you will see where one works but the other doesn't. And generally, see what happens is from the domain, right? If you suspect that nonlinearities are given, then you don't even bother trying the linear kernels. You go straight for the Gaussian. All right, guys. Now, there is an adaptation of it. I gave this example of classification here. So what I haven't talked about, and your book also doesn't talk much about, is that you can use support vector machines to do regression also. It's a straightforward extension. Why don't we do that in one of the Tuesday sessions? Right. But just take it for a fact that any generally any classification algorithm can be adapted to do regression, and we can at some point see how support vector machines can be used for regression All right, so it is quoted to here. Is that a general statement you're making? I'm making a general statement. Not specific to something? No, no. Generally, I mean, of course, I put a safe harbor, generally. But generally, in the general case, it's true that most classifiers can be adapted to do regression. Classification algorithms can be adapted to do regressions also. So the basic intuition is, see, if you can find a decision boundary, you're finding a hypersurface. Whatever technology you use to find a hypersurface can be used to fit regression data to that hypersurface. The difference is that in regression, data sits on that hypersurface. The difference is that in regression, data sits on the hypersurface or near it, and in classification, the hypersurface is the demarcation between the two. That you convert into an equation for the regression. Exactly. That's how you do it. That's it. Basic intuition is that.