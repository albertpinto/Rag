 Now we're going to talk about something called attention. And I will give you a sort of a hand-waving argument in the beginning, but today we'll go deep into the concept of attention and self-attention because that's the foundation of transformers so at a very high level what attention is is this go back to the recurrent neural network and you say that uh see look at this we are producing where are we, maybe I won't go there. I'll just recap it here. If you think of RNN, what we are doing is there is a word W1. There is a H naught. It produces H1. It produces H2. it produces H2, word two goes in. And finally, you have the H final minus one produces the H final and W final, right? It goes in here. So what happens is the problem that we have is that this thing, this context, this understanding, it begins to forget the earlier parts of the sequence. Isn't it? But then you may ask that, see, suppose we use more memory and what we give to the decoder. So this is your encoder. And logically what you give to your decoder block and i won't draw the same picture over again what you give is not just this but why not give it all of it h1 h2 hf because this guy remembers the early part of the sequence isn't it this h1 h2 they are very good at remembering the early part of the sequence that's what they were doing do you see that simplicity so what we are saying is that don't just pass it the h final but pass it h final minus one h final and so forth all the way to h2 h1 what if we passed in everything all the hidden states that were ever produced, right, to the decoder. Now it should not have a problem of the fact that the early part of the sequence is forgotten. Maybe early part of the sequence is not there in H final, but it is surely there in H1, H2, H3, H4, right? And so let's give everything to the decoder to decode. Right? And so let's give everything to the decoder to decode. Are we together, guys? Logically, it's a very simple thought. Let's give it all the hidden states that were ever produced. So, Asif, is this kind of an improvement over an LSTM? Yes. I mean, let's broadly use the word RNN. So when you give it all the hidden state, what you're seeing is this is the beginning of the attention models. Attention models, right? Attention mechanism. What you're saying is let this decoder attend not just to the final state, but all the previous states also right this is a very hand waving or sort of a high level way of representing it but that is the core idea because you just ask yourself suppose i could expand the amount of the size of the state we are giving to include the previous one won't the decoder be able to do a better job? And the answer to that would be, of course, except that now you're using a lot of information for the decoder to compute through, isn't it? So now then the question comes that can we do it in a smarter way? So the way people initially did attention is they would do it, attention, they would do RNN plus attention. So if you go back and look at the original paper, you will see original papers, they were applying attention to RNNs, which roughly amounts to saying that in the decoder part, I mean, hold on to your intermediate states, the intermediate understandings till you reach the final understanding or the final context and use all your intermediate understanding, so hidden states to then go and proceed and produce your decoding, right? The idea immediately made a lot of sense and it led to some pretty good results. So that was around 2015 and onwards, 1450, end of 1415 onwards, these ideas began to come about. And then came a pretty, and so by the way, I haven't gotten into the mathematics of it and so forth, because the only ones that at this moment we can cover within the scope of this workshop is the most important one, namely the transformer. So we'll out with a landmark paper that says, attention is all you need. This sentence, if you Google up on the internet, you'll realize that it is an iconic it brings up iconic pictures of encoder decoder next to each other and i will i will cover this paper in detail today we'll walk through the paper literally and the first major transformer that came about was bert it's a very interesting thing but so suppose you think of the encoder part and the decoder part and somehow they're connected um decoder but it is not an rnn it's a different architecture uh but sort of tends to focus and just giving you a little bit of a heads up kind of a thing of what's coming but tends to emphasize the encoder part. The GPT tends to start, GPT-123, they tend to emphasize more the decoder part, and they managed to get away doing all the things that RNNs could do using this transformer or attention mechanism. That is why the word, the famous paper is called attention is all you need. In other words, you don't need RNN. You can get away with just a engine, a sort of a mechanism that uses attention. So now let's make it real. How do we do that? The way, and I'll break it down into pieces but before I do it let me go back to the initial thing and the mathematical summary that we had. What did we summarize it? Have we understood the concept of softmax? What does it do? It sort of spotlights the bigger value, isn't it? Highlights it, accentuates it or exaggerates, or whatever word you want to use. That is one thing. Dot product tells you the alignment between two vectors. And then the third is an observation that random pairs of vectors in high dimensional spaces, they will tend to be orthogonal to each other. So just recapping these three results. And now comes the building blocks of the transformer, we'll call that the architecture. So the way the transformer works is quite interesting. It says, let's go back to our old sentence that we had, the cow, and can I just shorten it for the time being? The cow, okay maybe I'll bring it all over again, the cow after it ate a lot of, was that the sentence? A lot of carrots jumped over the moon in its delight. Right? So now when you look at it, I've deliberately introduced a few words that when you see it, you can clearly say that this is about the cow, isn't it? And it is not referring to carrot it's not referring to the moon right for human beings that's very easy but for machines and all of these it's pretty hard actually and likewise it's in its delights whose delight the carrots delight The carrot's delight? The moon's delight? Or the cow's delight? You see how this needs to know what does the it refers to? What is the context of the it and the it's here in this sentence? Right? So keep this thought in mind and we'll now see in a very little sense what the word attention means. So let's do what we learned yesterday. Yesterday, was it yesterday? Day before, on Monday, sorry. What is it that we learned on Monday? We learned about vector space representation that we talked about word embeddings remember that are semantic aware the meaning aware aware in some sense not completely meaning aware but sort of will cheat and say sort of meaning aware representation of words or semantics of it. So examples of that we did. What was an example that we covered in detail? Can somebody tell me? Of word embedding? King and queen. Right, we took the example that you could do king minus queen is equal to man minus say England UK minus London. So you could see relationships, these are not exact equivalents, they are pretty close. You start seeing these beautiful relationships in these word embeddings and we did one particular word embedding algorithm. You'd like someone to recall what was it the word to wake exactly we did word to vec right uh vec word to vector and this implementation if you remember we did the the particular variant that I went into detail was the skip gram. Negative negative. Negative sampling. Right? Remember we had this. So I won't go into the details of it today, but suffice is to say that for every word, you can create a vector for the word wi, that's a word wi, you can create a vector of a much smaller dimensional space let's say a 500 dimension or 300 dimension which in the world of natural language processing is a tremendous achievement because if you look at the one hot encoding you're looking at 10 000 dimensional or 100 000 dimensional space for a word because that's the size of the vocabulary if you do one-hot encoding, right, as a recap. So let's go back and say, all right, we will now take this sentence all over again and represent each of these words with its vector. the vector for the, right? Vector for the, there'll be a vector for the cow, right? There'll be so on and so forth. Are we together guys? So each of these words in the sentence, we can represent as a vector, right? And so we have a sequence of vectors. Let me call this vector V0, V1, v2, v final. Or why don't I represent it in the language of the paper itself, x0, x1, xf, sort of like this. These are your vectors. Actually, no, I take this back. Let me just leave it as I apologize. There's one more thing I have to do to this. So I'll just leave it as v1, There's one more thing I have to do to this. So I'll just leave it as v1, v2. These are v final. These are your vector word embeddings. So far, so good, guys. You realize what we did? We just took the English sentence and we replaced each word by its associated vector using let's say word to make or something. Good so far guys? Yes. Now comes a problem. See, in a recurrent neural network, the autoregressive quality was implicit. In other words, you would feed in the first word, W1, you know, in the W1, it would produce the hidden state 1, that you would feed into W2. So the order was implicit because that's the order in which you train the recurrent neural network. because that's the order in which you train the recurrent neural network. The idea here is not to train word by word till you hit the end of the sentence. In this mechanism, remarkably, you feed it all the entire sentence at one go. So imagine that what you're doing is you are taking a block and you're not giving it one word, you are giving it all the inputs at the same time, v1 to v final. It all goes in parallel into this. Right? Now you can say, well, how is that possible? What about those hidden states? Now you can say, well, how is that possible? What about those hidden states? Isn't it sequentially built in? Because it seems to break our way of thinking, isn't it? That one, the next word improves upon the understanding that we have of the sentence so far. The partial sentence so far. But this deals with things slightly differently and a bit more abstractly so you'll have to pay attention so what it says is associated with each word you somehow represent its position so v1 and you create another vector which is for position somehow you do that and you concatenate the two together and this becomes your position this is your input vector so this you call the word embedding and it it is invariant to where in the sentence the word is but you need to remember the position of the word because you can't just feed word into a machine and forget the position now you're not feeding it one by one so you need to do some position encoding position embedding let me just call it position encoding right or embedding vector. You're taking a position at coding vector. Somehow you represent the position. Position of what? Position of the word in the sentence. Position of the word in the sentence. Are we together guys? Somehow you represent it as a vector. So now if you think about it in a more abstract way, between the word embedding and the position encoding, the complete information of the context of the word in the sentence is captured, isn't it? The word can be represented, word is agnostic of the sentence, it's encoding, but the where in the sentence it falls is captured by p, a p vector, and the concatenation of these two, and typically what you do is if your word is 512, then your position will also be 512, right? Word encoding is 512. So things like that, you sort of equal this value you give to word embedding into the position embedding vector, and you concatenate them together to create a vector of double the size, roughly. Well, this all looks to be a pretty complicated business, but you do that and it turns out, and the most remarkable thing is what I'm going to explain. This very interesting chain of reasoning actually does work. And it works amazingly as you saw in the lab yesterday. So now what happens is you are... What if the word appears more than once in the sentence? Yeah, that is right. So there will be, so for example, let me take this word. Suppose, look at this word here. Where is it? Lovely sentence. The, the, you see that it's appearing twice. So what will happen is that the word vector for the the will be the same at both the locations. But what about the position vector? This will say position one and this will say position 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12. Somehow the 12th position will be encoded here. It won't be the number 12, it's a little bit more complicated than that, but somehow, so now you realize that this vector clearly says that the word duh is there, and this vector clearly says, X1 clearly says that this is X12 is like this. Right, even though it's the same word embedding vector here, but the position part, the second half of the vector is different. Did you get that? Yes. Yeah. So that is it. So you got the position also now. Then what you do, and this is the most remarkable thing, you feed it. So now this is, it is not the, so this is of course wrong. You don't do it like that because you can't just feed the words in parallel, but now that you have the words have their position aware in the sentence, you can feed them. All the words, right, corresponding to W1, W2, to WF, right? So they map to their this and their position, W1, P1. Ultimately this you feed into something. Let me call it something. This something is called a self-attention. So okay, let me just say it's an attention block. I'll just use the word attention. Attention block, right? What happens is now, what what does this attention block do it does the same computation on all of these words in parallel right so what it does is so i'll represent a word by xi can i represent a word in the sentence by xi which is basically made up of the word embedding and its position embedding right what you do and this part you have to follow guys to understand the transformer you will multiply it by three matrices one is called the q matrix the same word will go into the v matrix and xi you'll multiply by the k matrix now what is this this seems rather abstract when you do that you do it in such a way that the output and for no reason this is you can change it typically becomes that the output and for no reason this is you can change it typically becomes another matrix called the q this becomes q i v i and k i right these are typically a 64 dimensional much smaller dimensions so you know here you fit in something like XI's were close to 1, 0, 2, 4, 5, 12 for word embedding, let's say 5, 12 for position. So you sent in a 1, 0, 2, 4 dimensional vector perhaps. And now you are condensing it down to a 64 dimensional vector. Three vectors, though each vector goes in and three goes out. So the effect is this layer. It is almost like x i went in and it's scattered into three things Q i V.P. Vi and K i V.P. Isn't it, but the dimensionality reduced. Now if you remember there was one situation where we reduced dimensionality on Monday itself. When we created word embeddings by going from one hot encoding down to a much smaller dimension, we actually didn't lose much, we gained something, isn't it guys? we gained something, isn't it guys? If you do the encoding properly, you became semantics aware, the meaning aware of representation of the word. So you realize that sometimes reducing the dimensionality forces a network to sort of get the handle on it, to get a sense of the word and its position, right? And that is something that happens and there's a very magical thing. So these things have names. The QI is called the query vector for Xi. VI is called the value vector for value for xi and likewise the ki is equal is called the key vector for xi right so you burst out xi by multiplying by three matrices that will reduce the dimensionality but produce three lower dimensional vectors. Now, what is the intuition behind those vectors? And this part you have to pay attention to. See what it does is you have these words, X1, X2, X3. What it will do is it will project it into the 64 dimensional space, let us say. So think of the key value. So somewhere there is this point K1, somewhere there is a point K2, somewhere there is a point K3, right? K3, somewhere there is a point K4, right? K5. four right k five actually let me interchange these things okay three and let me just put this one okay five right this or maybe maybe I'll put it this way okay let me not put it here let me me put it somewhere else, k5. You just scatter, these values become spread out in the k space. Then you generate in some sense, you can almost think of it as the attention or the context associated with the word Xi is more like this in this query. So you say that suppose I take this word, let me take a word. The word is W, the fourth word, maybe the third word. Let's take W3. Let's take the third word. So you look at the Q3 of this and you project it in this case space. And you see where the Q, remember they are all 64 dimensional vector space. So they can all be put all 64 dimensional vector space so they can all be put in the same vector space you take q3 and let us say that your q3 happens to fall where should i put it doesn't matter really let's say that this this one falls here q3 right this one falls here, Q3, right? Then what you do is you ask this question, what is the dot product of Q3 dot each of these guys? K1, K2, K3, K4, K6, K7, K8, K9, K10, K11, K12, K13, K14, K15, K16, K17, K18, K19, K21, K21, K22, K23, K23, K24, K24, k3, k4, k5, right? So you will end up with the dot product of q3 with each of these. And so far, now remember what did I tell you? That in higher dimensional space, space traditionally most of these vectors if you take two random vectors you do dot product it will blow up it will go to zero it will be very small right but if q3 happens to be close to any one of these k vectors like for example k1 you will realize that k3 dot k1 is bigger bigger than let's say q3 with k2 k2 to k5 isn't it of course a q3 yeah so uh are we together right Right, K2, K4, and K5. But this is bigger. Do you see that this is bigger? This is near each other. So Q3 is dot product with K1 will be bigger. Guys, am I making sense? Is it straightforward? Yeah, just one question. K1 to K5 are the key vectors, right? Yeah, K1 to K5 are the key vectors, right? Yeah, K1 to K5 are the key vectors. So now what you do is... Bottom of the screen, that's K3 dot product with K2, K something, K5. Yeah, yeah. So guys, let me repeat what I said. So first of all, you have the k sitting all over the vector space. Suppose there are five words. So k1 to k5 are sitting there. Right. Now what you do is you take the q. You also have q1 to q5. Isn't it, guys? And v1 to v5. So let me put it this way your x1 went in x2 x3 x4 x5 this went in and what came out what came out in the is k1 v1 q1 k2 v2 q2 because you're just multiplying by some matrix that is producing these results and why in the world are we doing it hold that thought in your mind it's a puzzling thing why in the world would you do that if the dimensionality the example that you're picking is different from how you explained earlier right the x had a 512 dimensions and q were smaller dimensions yeah they are smaller you reduce the dimensionality which is easy to do you when based on how you multiply um with a matrix the appropriate matrix you can always do dimensionality reduction yeah so it won't be x1 to x5 it'll be a lot more correct no i don't understand what you see there are five words in the sentence there are five words each word okay okay got it you break it up into each word, so these corresponds to each would remember that, how did we get X one, and just as a recap, a word becomes X one because X one is V one and P one the word embedding. Vaidhyanathan Ramamurthy, vector and P one vector the position embedding and the joint concatenation of these two is the excellent okay so each of these 12345 corresponds to the five words. So each of these 1, 2, 3, 4, 5 corresponds to the 5 words. Okay. So what happens is multiplying those matrices, you get this result, right? 45, 64. Why 64? Nothing magical about 64. Just pick 64. Typically, you pick powers of 2, and it's a hyperparameter of your model. You can play around with it. But you have reduced the dimensionality quite a bit, one order of magnitude at least. Now what happens is I can do this funny business. I can ask another table, what is Q1.k1? What is Q1.k2? Q1.k3? Q1.k4? Q1.k5? Right right i can find these dots and then i can also do q2 dot k1 q2 dot k2 and i can keep going down uh you you pretty much get the idea that we can keep going down and ultimately we'll have way down here we'll have q5.k1 you keep going down q5.k5 right you can create all these dot products right i'm writing it this way just to make it dot products writing it this way just to make it dot products. Asif? Yes. Was this similar to the recommender system you introduced to us earlier? The one with the genres and... See only in the sense that whenever you reduce dimensionality in systems, quite often, you're trying to extract the essence of the information, right? So what you're trying to extract the essence of the information. So what you're basically saying, and that is where the word attention comes in in a moment, given X, which was a big vector, X size, when you reduce it to these, you're hoping that these vectors K1, V1, Q1, they have some logical meaning, useful meaning in some sense. So for example, in the recommender system, you use a cross object, an item. When you look at the matrix, there's hidden space, the latent factor space of lower dimension, you get the taste and you get the traits of items and taste of people for those traits in the items. So there is a relationship, but it sort of comes from the mathematics underlying it. Are we together? Yes, Asif. Thank you. See, all of these, that's why if you remember, I said that there was value in us all doing the engineering math, the mathematics of data science. We couldn't do this time around or this year because the enrollment was very low into COVID. But then, you know, underneath all of this is very elegant and simple mathematics. So all right, we generate this. So now what will happen? Think about just Q, I mean, for lack of a better thing, I just took Q3. Or if you don't, why did I pick Q3? Just pick Q, I don't know, I just took Q3, huh? Doesn't matter. Any one Q, when you do the Q3 with respect to all of these things, in this picture, do you see that it just happens to be near K1? Isn't it? Guys, if we look at this picture, would you agree that Q3 happens to be close by to K1? Correct. Right. But not to the other ones, K2, K5, K4, K3. It doesn't seem to be as near, maybe a little bit near K5 also. So what will happen is your dot products of, so now looking at this, you can say that Q3, right, dot K1 likely to be, likely to be, to be to be biggest maybe Q 3 will get k5 next biggest and the rest of it Q 3 versus the rest of it right the rest of it rest likely to be close to zero would you agree just looking at this picture yes right and that is something interesting that is the beginning of an idea this dark product in a very rough way it is the germ of a concept called attention. What you say is that the word, the third word, it is a landing when you take it to this vector space. So the word is the query. Like you think of the word as a probe, the query word. Let's say the word, what was it in our sentence, the third word? I don't know. I forgot. Or maybe I should change it to some other word to make it more meaningful i forgot the sentence is that the cow maybe the second after maybe i should have taken the second word it would be more relevant the cow but anyway let's say that this is it what is happening is what this is saying is that the third word is paying a lot of attention to the first word and some attention to the fifth word but it is not paying much attention to the rest of the words okay that's how you do that but there is a little bit of a mathematics to that. What happens is that when you take this dot product, you will, so suppose you take, so let me just represent it with the qi guys. I may be here in this case three. You multiply it with some kj. You dot product it with kj, right? And then what do you do? One of the things that happens is, so you do this. So what will you end up with if you do this? You will end up with five values, right? You will end up with five values, right? QI, K1, QI, K2. In this case, QI, K3. Whatever QI, here the QI was three. So K4, QI, K5, right? You'll end up with five numbers isn't it guys right so just looking at that picture there we saw that first and the last q 3 suppose if i is 3 then this seemed to be big and this seemed to be big you get you get five numbers output if you dot product qi with each of the k's and there are five k's you will you agree that we'll end up with five numbers yes sir right now we are asking this question that if this is the germ of the this is the germ of the idea of attention. You're saying probably the QI word, maybe third word, needs to pay how much attention, what proportion of attention to each of the key, each of the words, right? And you think of the key. So here is one intuition for key. Think of key, the key so here is one intuition for key think of key the key of a word is its location in the vector space right go back to the intuition here do you notice that the k's are in some sense uh they are the locations they are the address of the word in that vector space each of the words in the vector space so that's a rough intuition you can carry. Think of K, let me write this word down. Think of K, K-I as the address, K-I, the address or location of the word wi in this vector space that's what we are saying and so the query word when it goes there the qi of a word you take any word q i word i and then you go into that vector space what are you doing you're basically saying which other word am i close to this qi is close to right and so the near these numbers that you produce they give you the near the degree of nearness of qi to these different addresses isn't it and so now comes the magic suppose i soft max this what does softmax do the operation what will this do q i and k j it will tell how close q close the query term qi is to the address of the word j. Right? In other words, how aligned is qi to the kj vector? Are we together? So far, so good? Guys, please give me some feedback. This is a little abstract. I want to see if you're understanding or not. Otherwise, I'll go back and re-explain. It's pretty clear. It's pretty clear, right? Okay. So now comes an interesting thought. So there is a little bit of a technicality for reasons of, you remember we talked about the exploding and vanishing gradients and all of that. So to make sure that the gradient during back propagation is sensible, and this again goes back to the same reasoning, whatever was the dimension of this, let's say d, dimensionality of the space, right? Remember here it was 64. So what's the square root of 64 eight so it just so happens that from a technical perspective computational perspective it makes sense to have a denominator here so that the dot products don't become too large right otherwise you have problems during the gradient descent but it doesn't matter so you take the softmax what will it What will it happen? It will convert all of these into probabilities. Isn't it, guys? Each of these numbers, when you softmax it, you will end up with pi1, pi2, pi3, pi four, pi five. Think of these as a measure of nearness of QI vector to each of this word, how much attention the word I is paying to these other words, right? And now what you do is you multiply it with the word, the value of the word itself, kj. Multiply it with the value of the word. So now what happens? This is the same as multiplying this with v1, v2, v3, v4, v5, right? This array. You're just multiplying it with this value in a pairwise multiplication are we together guys and this is the end of our long journey if you understood this much you got it we are almost home now so what we are saying is that for each word if you want to find out which other word it is paying attention to, word or words it is paying attention to, you take the query vector of that word and you check its alignment or do its dot product with the key vector of all the other words. And when you do the dot product of a query vector of a word with the key vectors of all the other words, you'll end up with this array of numbers. Now once you have soft maxed it, probability numbers, and you multiply it by their value, so what will happen is, let's say that this one, it will stand out big. This will practically disappear, practically disappear, and this will partially stand out. Remember in the diagram I made, the p3, let's say that I is equal to 3. Why don't I do this? 3, 1, 3, 2, 3, 3, 3, 4, 3, 5. I is equal to 3. So you realize that in this, the P's, the 3 will stand out. 3 and 1 are close to each other. three will stand out, three and one are close to each other. This is very, this is if anything negative, I mean, this is very small probability. This is very small probability, very small probability. This again is a big probability, reasonably big probability, right? But this P1 is, so what we say is that word three is paying a lot of attention to to word one and also some attention to word five right in the way we did this example you would say that it is doing that right so this entire thing together is called the attention this is the famous a formula of attention that has become the celebrated formula and that's the reason I tortured you guys by taking you through the entire derivation or sort of a reasoning about it. You would say that the attention of the word I is QI dot product with KJ, right? When you do this, basically the K of all of them and you sum it up and then you can sum it up and so forth. Then you softmax at softmax. And remember to do this extra little bit of dimensionality of the space. People write it as dk. And then that softmax you multiply that softmax with vj, right? So then you will get to know, you will get a sense of, once you multiply it by the vector, the value that is really prominent will blow up. So the value, this attention here will be prominent, and the fifth one will be prominent, right? Somewhat prominent, right? Somewhat prominent, not too prominent. Let's say maybe it will be partially prominent and then word one, word two, word three, word four, word five. You would say that the third word is paying a lot of attention to this, right? So it is saying this, and then it is paying a little bit of attention to this, right? And to other ones, it's paying close to zero attention. Right? That's the meaning of the word. So here we say, say that W3 is paying, and we said that it's paying a lot of attention, paying attention to attention to w1 w5 in that order a lot of attention to w1 and less a bit of attention to w5 right and now you can go to every other word and you realize that all of them, they will be paying attention to something or the other, isn't it? Maybe this guy is paying attention to, for the sake of argument, I will say a word three, right? Maybe W2 is also paying attention to W1, and maybe W3, we already did, W4 is paying attention to, what should we make it pay attention to? W2 seems lonely, let's give it there. W5 is paying attention to somebody, let's say W4, right? And sometimes you can be paying attention to multiple words, for example, W5, for example, maybe paying attention to this and to W2 also, right? And so forth. Are you getting the idea? So you say that, you know what, this all sounds very abstract. You just multiplied, you took a word vector, the xi with this position embedding and coding, and now you burst it out into this random, you multiplied by some weight matrices to produce the qi the the query the key the the value vectors and you concord a meaning to them and then you defined the concept of attention that this word is paying attention to that and so forth and now uh how do we know that this is happening? And we'll come to that. But for the time being, believe that it's happening. I just believe that we can train the network to do this, to pay attention to the right words. So what does it mean paying attention to the right words? What it means in real terms is, let us go back to this sentence. When you look at it, the word it here, it should pay attention to this, right? And the it's here should ignore other words and it should go and also pay attention to cow. Right? Is this becoming a little bit more real now guys yes right so that is it that's it uh you contextualize the sentence and have the right words pay attention to the right thing so now this it's and this cow are so far separated isn't it but nonetheless you do this mathematical magic and you make sure that the relevant words are which need to pay attention to each other are paying attention to each other so that is the learning part somehow as you may have anticipated we'll bring in a loss function somehow and then from the last function we'll do a back gradient descent. You know, we'll compute the gradients back prop and compute the gradients. And then we'll find, we'll update the weights. And so those weight vectors, you know, WQ, WK, WV, initially you start out with random values. So this entire thing is totally ineffective. But as you go through iteration after iteration and somehow you you notice that this is not paying attention to the right thing you make it pay attention to the right thing the weights get trained and once the weights get trained then you feed it any sentence and the right any given word will be paying attention to other words that it is related to that it needs to pay attention to are we together guys right so now let's move forward what do we get you get this attention and then from this there is a little bit more to it but I'll just leave it at that particular value so suppose you get this output z1 actually I won't do that so what do you do you have you have just multiplied it with this attention block, you have given it the x1, xn number of all the words in the sentence. You got your k, k1, q1, v1, and kn, qn, vn. And from from this you got the attentions you computed the attention what is the attention vector here what is the attention vector here now there's a problem the problem is relationships and things between words are highly non-linear, right? They're complex, but when you multiply it by a direct matrix, WQ, WB, WK, these are just matrices, isn't it, guys? So a matrix multiplication is a linear transform. Right? So you realize that we haven't introduced non-linearity into the game, isn't it? It is like in the simple neural network, you remember that if you do z, so how did you go? You would go the input x, and then it would be a dot product with w plus b right this would be z z is linear in x just multiplying by a weight weight vector and adding a bias vector doesn't make it doesn't make it capture non-linearities right This is the equation of a straight line. So what did you do? You passed it through an activation function, right? Some sort of a nonlinear function, either ReLU or tanh or sigmoid or something. You passed it through a nonlinear activation. Are we together with me guys? So we need to do that. This multiplying by w's is great and hopefully if the weights are magically correct it may even do the right thing. But language is inherently very very sort sort of nonlinear thing. So what you do is, the full attention block is made up of this part. This part is, let me call it the self-attention part. Attention part. And then you feed it into a simple feed forward network. Feed forward network. What is a feed forward network? You know, your dense hidden layer. forward network feed forward network what is the feed forward network you know your density layer one layer two layer whatever it is you feed it into that am i making sense guys yes and then the the output that you get would be inherently non-linearly transformed would be inherently non-linearly transformed. And because it is non-linearly transformed, now the magic begins because now you can sort of deform it to learn all sorts of non-linear behavior and relationships. So this was essential. You remember we realized that non-linear activation functions are at the heart of neural networks. So the first block was not enough. It just did a linear transformation basically. You need the second. So you would say that one attention block is made up of two things, two sub blocks. One is this self-attention part. It produces your K, Q, V, and finally the attentions, right? Attentions. And then you pass it through the feedforward network. And then the vector that comes out for each of the words, again, just as you have attention vector for each of the word, here you will end up with Z1, Z2, Z3. Suppose you have N words in the in the sentence so all of this z vectors will come out right all you're doing is you're taking the this attention and you're just applying a sigmoid function or some sort of activation function to it right and maybe you feed it not just to one layer to a couple of layers if you want but the idea I mean it's it's easier to understand if you just think of it as one layer, which means that all you're doing is applying an activation function. So this break bursting it out into this cake, you we and then doing a relook, you get a nice non-linearly transformed set of values attentions that z1 z2 now what you do is here's the so here is what you do the way the thing gets interesting is this is you say well this is not enough what if i treated z1 z2 zn as as though they were word vector word encodings and pass it through another block exactly like this right so let me call it block number one or attention number one attention block block number one then you pass it to attention block number two which does exactly the same thing you know burst it out and does. And so you pass it through for no rhyme or reason, you pass it through attention block in the original architecture. And I think this is still there in many of the implementations, six blocks of attention, right? So if you think of a dense neural network, remember we used to pass the data through a few layers of the dense network. You can think of it that in the transformer world, you're doing the same thing. Instead of using a dense layer, you're using this attention block, which is made up of this weird matrix multiplication, self-attention part, and then a normal, you know, dense layer, right? So it is a slightly extra piece is added to each of the layers. And so you take these blocks, and so you input x1 to xn by the time it goes in. And finally, when it comes out of the last block, what you do is, when it comes out of the last block, so you will again get z1, except that this is z1 of the last block, what you do is, when it comes out of the last block, so you will again get z1, except that this is z1 of the sixth block, and this is from the first block, all the way to zn of the sixth block. You are sitting upon this. Are we together, guys? So you chain these together as though they were layers. And this entire set of operations that you do, all these blocks together, the six block, you call it a head. A head. Have it together. In the encoder, head in the encoder. Well, now comes a bit of a mechanical part. Do you remember convolation neural net? If you got an image, suppose there is an image of a cat, right? This is an image of a cat and the three channels of it, the RGB channels. Let's say that this image is 64x64. What did you do? You passed it through filters. Do you remember guys, you passed it through a lot of filters? F1. Yes, convolved it. F2. Right, F2. 11 filter would detect the egg the horizontal edges one filter would detect the vertical edges one would try to capture the diagonal something like that. Raja Ayyanar?nilam? We are just giving a sort of a emotional aspect that each of these filters will try to do something orthogonal to the other filter. Raja Ayyanar?nilam? And pick up different features from the cat image. Raja Ayyanar?nil image. You remember that guys from the CNN talk at the CNN week? Yes. Yes. So you try the same game here. What is a filter or a kernel in a layer in the world of CNN, in the world of NLP and this transformers, it is the heads. Heads are head is like a filter. So what will happen is you take not one head, but you take multi-head. You take many heads. So what happens is that you pass it through not one, but actually a typical transformer architecture has eight heads. actually typical transformer architecture has eight heads head one all the way to head eight right and by now you're seeing guys how complicated it is getting now we are still talking about just the encoder the head of course is made up of what each head i is made up of six Each head is made up of six attention blocks. Isn't it? And just to give you an idea, head has, each head has six attention blocks. Then each attention block has what? Has one to self-attention part. And then the feed forward network and feed forward network, right? Feed forward network also is there. Maybe you can take it for simplicity. You can consider it one layer of feed forward network to introduce the nonlinearity. So the self-attention block it creates the dot products. It helps you align words that should pay attention to each other together and the feed forward net part introduces the necessary non-linearity that you need whenever you're solving any non-trivial problem in DPL networks, real-world problems. All of it put together, all of these put together will produce ultimately, what will they produce? Let's say that you're taking just for word one word one what it will produce is you will end up with z one of z one from coming from so let's say that they are for n words n words so this will produce the n one of the first from the first head this will produce the z two from the first to the eighth head, all the way to Z2, the second word, you will have an output from each of the heads, right? And likewise, Z1 to, what am I doing here? Sorry, I'm just making garbage here. Z1, Z2 notation wise, from the first head. So this points to the head, the superscript is the head. So this will be Z2, superscript, one, Z2, from the second head, Zn. Are we getting this right? So suppose I have five words or n words. This is it. I will get for each word I'll produce an output from the first head. I will produce an output from the second head all the way to producing the output from the last head, the eighth head. So far so good guys. That's a whole lot of outputs. Would you agree? Yes. Well, that is not enough. The next thing you do is to, you then say, well, all of these outputs, how do we feed it into a final feed forward layer so everything put together you want to feed it into a feed forward layer and so forth so you can't feed see feed forward layer expects a single dimensional vector right so what you do is you concatenate this from each of the layers. So suppose you're looking at word one, word one, you would do word one from the first layer, word one from the second head, sorry, not second layer, second head, eighth head, one. You concatenate this to produce a giant vector, right? So this has eight heads, this specific architecture? Yeah, the classic transformer research paper had eight heads. And when people implement it, it's very common for them to implement it with eight heads. And each head has six encoder. I mean, each head internally has six of these attention blocks layered on top of each other like pancakes, right? Why six? There's nothing magical about six. Six is enough. Why eight head? Again, nothing magical. You can increase it, decrease it. It's a hyperparameter. You can play around with it, right? Well, eight seems to have worked with the original guys and eight is commonly used in the reference implementations in the open source, eight heads, right. So the closest intuition I can give you to head to other things is think of a head is like a filter. In a CNN, in many ways, it sort of is like a filter. That is the intuition I keep. Maybe if you consider a block as a attention block as a filter, then I would say that think of a head as a sort of parallel networks. Because data from the first head just goes through and produces the output through all the attention blocks internally, Likewise, attention in the second block, right? So you have to develop your own intuition. So what do the heads do and why do we need so many heads? See what happens is that when you relate a word to another word, a word may be paying attention to another word because of, let's say grammatical importance some semantic or sentence structure importance right so you want another head to create relationships between words on a new dimension right so one is for example on the grammatical dimension one is for example context is trying to figure out that these two are related and so forth the same way that filters what do filters do filters give you the relationship they do independent feature extractions isn't it guys in many ways the filters independently they try to extract features that the other filter is not right the best well trained network will have these filters all doing different things. So think of the head also like that. They extract or they look at a different aspect of either the grammar or syntax in that sequence. They automatically get trained to focus on different aspects. And so the outputs they produce are not the same, they are different outputs. They just like filters, they produce different outputs. Or they are noticing different things. Are we together guys? So that is the encoder, and it is not enough. So now I'll go to give you an an idea of what it is or how this encoder decoder works i oh my goodness it's 10 39. maybe i'll rush through the decoder a little bit quickly so the way people give the art the drawing they make is that here you give the x i right all the x 1 x 2 all the words you pass it at the same time how did you get to x1 x2 before it there was a plus layer in which you put the word embedding sorry let me call the word vector v1 to vn and it then you join the position encoding p1 to pn you remember the position encoding somehow we encode the position it's actually a technical not terribly important how we do it time permitting we can talk about that it's very simple you just pick it up from some harmonic functions then this becomes this it goes through this your six layers at At the end, and then it is not that they are multiple they're basically when it is going, you realize that they are multi heads, there are eight of these heads doing exactly getting the same input and you know I will. Eight of these and then each 12345 oh I should stick in one more. each one, two, three, four, five, oh, I should stick in one more, six of these. And at the end of it, you concatenate all of it to produce this one, Z1, Zn sort of. And what you do is something very interesting. It already looks like a whole lot of transformation. So you wonder exactly how much struggle people must have gone through to figure this out. You multiplied with yet another vector W zero, right? One more sort of a transformation you do weights that must have found that this is useful. And then you finally get the outputs. Are we together? Once you get that, you get this Z, this z this again the final z1 to zn and then what you do is this vectors once you have produced it you break this into q and k you remember q k and v and you throw the q away because it's sort of the query you take the k and v this is your encoded it's sort of the query you take the k and v this is your encoded encoded what we used to call the thought vector thought or context context is captured by key value you know in other words a value of a word or sort of value and a word and location in some sense of a word in that abstract space. So you have the encoder. So now I'll write this encoder here and I'll be a little bit sloppy here. So some of the arrows may be sloppy. So what you do is, and then there are a few feed forwards, etc. You take this and you take the K and the V that comes out, and then you feed it into another layer, which is a decoder. But the decoder, see remember when we used to do the decoder, you used to parse in the hidden state, you parse it, but you do something else also. Just like this encoder, there is a mask encoder layer here. Mask encoder like layer here like layer here what is that doing and and i'll give you the idea we'll go through the paper in a moment what you do is suppose you give it english here english sentence sentence. So all of it will go in, KVs will be ready. Then here, you ask this guy to produce the first word. So let us say that in the other language, it produced the word, let me use a different symbol for the word, a pi1, right? It produced the word pi1. What you do is you take this pi 1, and here, you again have a lot of, you fill it, you fill it with a pi 1. And these other blocks, you just set it to minus infinity, minus infinity, minus infinity, etc. So in other words, you mask it out. It is meaningless. So this is another block like this. You feed it. It is called a mass block because it's getting input smart. It will get pi one and it will get now think of the decoder here. It is getting the this pi one converted into a pot, basically a context vector here. But then it is getting the Kv from this side the encoder side the coder side and it is all going through this and we'll obviously have glossed over a lot of details here and then it will produce pi 2. you take pi 2 and you feed it in here it becomes pi 2 which is the second word got produced and the third word gets produced and so forth. So what we are saying is very interesting actually, in the decoding part, whatever output goes, it goes back in and the KV, the encoding thought vector is coming from the encoder, right? Which is already in one fell swoop. The entire sentence has become the context and it is helping translate. So what it is doing is you produce V1, you use V1 in the context vector to produce V2 and then you use V1 and V2 to produce V3 and so forth. And ultimately you will have a, so it is not m so some m number of words because it will usually be different from n the number of words in the previous in the original language you will have you would have ended up producing right so whatever is i gave so guys are you with me this was a very complicated and long architecture, but its main pieces are not that hard. The main piece is that you take the input xi, which is made up of the word embedding and the position embedding. You burst it out into k1, v1, and q1, kvq. So any xi, you burst it out into kvq after that you treat the q as the probe as the query and you say that if the q of a word is sort of the probe where if i do a dot and key is the location this word needs to pay attention to which other word and that can be found by seeing where the qi is in the case vector space and with respect to the k vectors whichever k vector it is close to that word it is going and paying attention to are we together guys right and that is it once you get the attention the rest of it is a lot of neural network mechanics of it there's a little of neural network mechanics of it. There's a little bit of mathematics. You scale it down, you multiply it by the values to amplify the values. And then ultimately you get a map like this. Every word will be paying attention to some other word. Sometimes two words and so forth, to different degrees to all the other words. And it doesn't mean that it's not paying attention to the remaining words but very little close to zero attention to the other words and then what you do you take this and you say well now i have attentions and in some sense that is the context right and you you use that in the decoder part but the the real details are there's more to it, because it turns out that you don't have one block, attention block. You do that, then you multiply it with a feedforward network to get the linearity, and the two paired together makes one attention block. And you then do daisy chain, you daisy chain six of those attention blocks. And then you realize that, well, that isn't enough, you just sort of created just one head. Now do the same thing with seven other heads, eight heads, right? Eight heads, 12 heads, whatever you want, but eight is the one mentioned in the original paper. You do that. They are like filters, roughly like filters in the CNN. They pick up different aspects of the sentence. If you think of the sentence as an image, right, and the heads as a filter, that to me is a close and sort of a rough analogy so then you have eight heads that's why it's called multi-head attention the word that is there is multi-head attention and then all of it there to just produce the encoded part and then you have to do and there's some more matrix multiplication for good measure and at the end of it what you have produced is the encoding and you take the encoded thought vector from here and then you feed it into the decoder but when you do that while this is a multi-head encoder uh have a attention blocks but this attention block in particular guys please mute yourself i can hear you and the child speaking All right, I muted him. So then it makes common sense when you're translating, what is the total context you have? Suppose you have translated three words, right? In Hindi, you would say, , right? Three words. And then you have the whole English sentence it may it seems common sense that the total context that will help you generate the next word will be the English word encoded as well as the the partial sentence that in Hindi or in French that you have created so far. That is also context because that will inform the next word you make. Otherwise, you may do all sorts of grammatically odd things and so on and so forth, or semantically odd things. part feeds in the partial sentences, partially done part of the sentence. And this part feeds in the original in translated language here, translated language. And this is, of course, the original sentence encoded as attention, right, into those contexts. And you would agree that if I put these two together, then I'm ready to decode the next word and the next word and the next word. So see, this is a pretty complicated architecture, but once you, in fact, the first time I read it, I was like, what is it it and why do we need these complications and the paper it moves pretty fast actually if you read the original paper it does move a little bit fast and so it takes you a few readings you have to come back and read the paper a couple of times before you begin to get the hang of what really is going on with that paper. So what I would like to do is, it's almost 11, if you would spare me, I will bring up the paper for a moment. It is worth going to the paper and I'll very quickly go through it. I won't, it's late night, I won't hold on to you for too long but let me go there. So where are my transformers? Transformers. Here we go. So let's go to this paper. So let me first, before that, I'll explain this, guys. You heard me talk. This video will be there. I explained this thing over two different sessions, the Transformers, just a couple of months ago. So those video recordings are there on the YouTube. It is here, guys. And so if you so prefer, you can go read that. Because in this one, I literally read through the paper carefully, line by line, and I explain it. So I would encourage you to go and watch those sessions also. Now this is the original paper, Harvard University, some Harvard NLP group, they took the paper, they took the entire text of the paper and wherever they would say something, these guys implemented it in PyTorch. So you can literally see them implement the ideas in the paper and come up with the full transformer implementation, which is very good. I find that very illustrative. Then Jay Alamar has a very good blog article on the illustrious, it's called the Illustrator Transformer, very well talked about in the community. Go look that up. Then McCormick also has a tutorial on BERT, which is fairly good. And a couple of other resources, Hugging Face, of course, you're familiar with. And these are all curated list of resources. What is Hugging Face, guys, is the Transformer library. And then I'll just go to this paper for a moment. And again, as I said, after this discussion, deliberately I didn't walk through the paper carefully because there are these two videos in which I walked through the paper very carefully. By the way, in the first video, you can use it as a review of this whole Softmax business. So attention is all you need. This is the paper. Can you guys see this on my screen? Oh, yes. Can i make it little bigger even bigger yeah that's pretty good yeah sure so now uh if we look at this as i said this came out on 6th December 2017. And the reason I dwelt so much on this thing guys is because it brought about a sea change in the field, you know. When you, when one paper makes such a profound difference, I suppose if we really want to learn this subject deeply, it makes sense to understand that paper well and that is why I took you guys through so much of technical details which we didn't do for other aspects. So this is it. Now let's see if, go read this paper, see if it makes sense to you. So remember I talk about the hidden state, the recurrent models typically factor computations along, do you see this? Sorry, this hidden state and the input of position T, this inherently sequential nature precludes parallel limitations, we talked about the limitations, you can't do parallelization, right? So now the background of their work, they're trying to do massively, see, here's the beautiful thing about the transformer, guys, that I should mention. You realize that you can feed the entire sentence at one go, isn't it? Once the network is trained, you can feed the entire sentence at one go and out will come the translation. the translation but more than that you could you could be feeding other sentences in parallel to a whole mini batch of sentences do you see that guys right so each sentence will be treated differently it will have its own attention and so on and so forth but you can feed a mini batch of data and this system will work just fine. So that is it. You can feed things into it very, very fast. Now, let's look into this. Yes, this picture is the iconic picture. If you Google it, if you Google the word attention is all you'll find the internet full of this picture. Now in this picture, by the way guys, can you see my mouse? Yes, no problem. Yes. So do you see what we are doing? You take the inputs, inputs are the words. For each word you do the input embedding. What does the input embedding mean? you do the input embedding what does the input embedding mean word embedding right or something like that you do that then what do you do you do the positional encoding you do that right somehow and then what do you do you feed it into multi-head attention so what what does the multi head here refers to? The eight heads. And each attention block internally is made, each this multi head attention, a head is made internally of six layers of attention. And then it takes out in the ultimate result you do some, you know there's more to do, you add some norms, normalization, right? And there is one more thing you notice there's a short circuit path from here straight that bypasses the attention altogether and it sort of gets added to the output of the attention blocks do you see these guys this short circuit where my mouse is moving feedback right so this is this is called residuals or short circuits or skip connections. I talked about that when we talked about the lost landscape and we'll do much more later on. But just consider this to be a good practice, a best practice. It leads to a lot of benefits. And then ultimately everything else is once it comes out, yeah, so this is one attention block, it says an attention block, you take the result and you pass it through finally some feedforward layers to add more non-linearity. Again, you do normalizations. Again you do some short-circuiting and you are ready. You have encoded, you get your key a k and v vector associated with each of the words that went in here after this long journey. So they are here. Now what you do at this end in this part, in the decoder part here, you again take the, only the words that have already been translated in the other language, translated words, the sequence, and the other things are masked. That's why the word masked is there. Because you know you don't have the rest of the sentence yet. You take that in the beginning, for example, you will have nothing. So you take nothing and you feed it here. The key values, it goes in here. It will trigger exactly the same mechanism. Do you notice that this, these two, the decoder is almost, this top part of the decoder is practically an exact copy of the encoder. Do you see that, guys? The decoder, the top half of decoder is exactly the same, almost the same. In fact, it is the same, more or less. Not quite, but almost the same as the encoder. It has multi-head, add norm, feed forward, add norm, multi-head, add norm, feed forward, add norm, right? And there's a little bit, the top skip connection is the same. The difference is what goes into it, right? Okay, so from that perspective the the partially translated words will go in here and the context from the english sentence let's say if you're translating from english in the the whole english sentence is here both of this goes in makes common sense it decodes and it will keep outputting and pointing to the right words using the softmax. Once again, word after word after word at the top, every time the softmax will say now this word, now this word, now this word, now this word in the translated language, let's say in Hindi. So you end up getting step by step, picking up the words that the softmax is pointing you to and building the sentence in the translated language. All right. Before softmax. Come again. What is the meaning of linear box before? Oh, linear is just feed forward. These are just linear layers. See, what happens is that, remember that the feedforward networks, OK, this linear is just a, it takes the data, flattens it, and then gives it to the softmax. That's all. There's one more multiplication with W0, remember, we talked about, and so forth. It's like the final flattening of the convolutional neural networks yeah sort of like that pretty much that's the intuition and then it goes into it so this architecture actually if you think about it they must have really struggled and tried many many things before they hit upon this right but when they hit upon this in hindsight once you understand it it it's, and I don't know today, but one day it will make total sense to you that, wow, of course this will work. And it doesn't work. Oh boy, it works. We just saw that in the lab yesterday, how effective it was. Sir, the positional encoding, if that goes in the coding layer also? No, no, this is the positional encoding of the Hindi sentence. You need to do the same thing. That also needs to be there. Otherwise, how exactly? Exactly. That is it. So therefore, you achieve the results. So now let's scroll down this paper. By the way, guys guys do watch my videos that i have posted there on the class web page from the past because i go over this paper in much more detail and i talk about the softmax also in much more detail right so remember the vk qkv does it make sense now guys yes sir yes and this uh amazingly you know he starts this concept and very quickly together into matrices the key values are also packed together into matrices in just this one line he goes ahead and says everything and so you can see that the paper is a bit dense in the sense that you need a little bit more, you need to read it a few times to get the hang of it. In practice, we compute the attention function as a set of queries simultaneously packed together into a matrix Q. So the Q is the query matrix. The keys and values are also packed together into matrix K and V. We compute the matrix of the outputs as this. And this equation is the sort of the celebrated equation. Whenever you see any blog on transformers sooner or later, you'll see this equation. So now you remember Q, the Q K transpose, this is a matrix notation because you're doing a lot, all the words in parallel, but word by word, what is the Q KT at the top? It is the dot product and what was the purpose of dividing by square root d k dimension see what happens is that the dot product now so a dot b so let's say x vectors a dot b is a i b i a 1 b 1 plus a 2 b 2 plus a 3 b 3 in three dimensional space would you agree oh yes because there is a additivity for all of those dimensions uh it's a mathematical thing that when you take uh k like a k dimensional space and you create random vectors the mean will be zero but the variance will be the variance will be d right so by dividing it by the square root of variance namely the standard deviation you scale it down so that those dot products don't blow up. Right? And you prevent the dot products from, you know, it's just a scaling factor so that later on when you're doing gradient descent, you know, the computing the gradients, it's just convenient to do that. So think of it as just a technical detail to make sure that your computations happen. but even if you erase that square root of dk from the equation the idea the attention still remains attention so that is just a way to make sure the computations are fine right so the architectural diagram above i have a quick question come a little bit closer to the microphone please yeah are you able to hear me asif yeah i can now um so in the architectural diagram in the page above um so the output encoding you send uh english is going through the input the uh the output encoding hindi is going just for the training phase right later on it stays idle while it's inferencing right no no no that is not the way it is what happens is that you take the english so okay let me explain all over again see english sentence goes in the whole sentence it will become the context vector going into the decoder right here this this arrow. Right? Okay. Then the first time there is no Hindi word there. It will produce a Hindi word that will get fed back in here at this end. To produce the second word now, the context of the English sentence plus the first Hindi word will together be the input to the decoder. Oh. And that will help it produce the second Hindi word or the second French word. And that now to produce the third Hindi word, you again have the whole English sentence as a context and you have the first two Hindi words going in as a context to produce the third Hindi word. Are you getting that now? I see. So there is no feedback there. So I misunderstood that. Okay. But then what happens is once it has translated the entire sentence, now you can go and do a comparison with label data in case you are in the training loop. Now you can do a comparison of the label data and say, oh gosh, I have errors, there's a loss. And then you can compute the loss, do the gradient descent of the loss. I mean, you can do the gradient computation and back prop of the losses. And then you can take a one step back you know you can do the gradients descent step towards a better a better set of weights i can back propagate through the whole network okay through the whole network it's a pretty long network so guys can you imagine what how big these transformers typically are pretty big right you can imagine a typical floating point number is like you can pack many of them in a kilobyte. But these things, hundreds of them in a kilobyte, but these transformers, when you save them, they are practically close to like 300, 200, 400, 500 MB. Then they almost start approaching gigs, gigabytes. The GPT-3 implementation is so big, apparently it can't even be downloaded onto a workstation. And you can see why, just look at the huge number of heads and the huge number of layers, and it's a pancake of so many things so these are very complex networks but they're amazingly effective and one reason i took you through the whole thing is because this really did change made a sea change in a nlp world so anyway after that, I hope now this equation, guys, do we all understand this equation? All parts of this equation? Read, learn it in my notation. I forget this matrix because it's a bit too condensed. It is true, but you do it in one step. But if you break it up into the fact that attention of the word QI. Will be the softmax of QI with all the K, all the different words. will be the softmax of QI with all the k all the different words multiplied by the value and it will amplify which value it should pay attention to right the rest of it is just architecture like standard computer science there's nothing very uh there is very very interestingly thought out architecture so now when we do BERT and this is about the position encoding, maybe at some, please watch my other video and you'll see the explanation for the position encoding, right? The rest of it is just explanations and so on and so forth. And the beauty of this is it's highly parallelizable. The main beauty is it's parallelizable. So you could just translate a lot of text very, very quickly. The machine translation results are. And so there are some results they have produced. And the accuracy of translation, there is a benchmark called the BLUE. And so in the BLUE benchmark, you can see that the transformer models when you take a big model what it does to the state of the art you notice that it pretty much beats all of them in its very first implementation itself so from english to german and english to french but do you notice that it just off the bat in the very, when this paper was published they pretty much must have had the first implementation and out of the gate it beat everything there and then of course after that a lot of work has happened and I invite you to go find what the latest blue scores are for some of these giant transformers. It's much higher apparently. So that is that. And it is much faster also. Somewhere they give the training cost time also. And also the cost is much lower in some sense and it runs much faster. All the goodies are there. Today, no one questions the value of this architecture. So this is the reference architecture, but out of this, the BERT is actually simpler version of this in many ways. It only uses the encoder part and gets away with it. On Sunday, we'll do the BERT. So Asif, what is the purpose of the harmonic function? That is the position encoding. Watch my other video. See, I don't want to go into that because this is already past this. And my India team has been waiting. You know, my India meeting starts at 10.30. We have gone past it. Today we had a long day from 7 to 11.15 now. But basically those harmonics are used to 1115 now. But basically those harmonics are used to do position encoding. I've explained it in detail in the other video. Do watch that. When I mean the other video, which video am I saying? Yeah, do you see this transformer part one, part two? Asif session transformer part one, part two from the past. So the first one, I go much deeper into the softmax and I use a Jupyter notebook to explain it in piece by piece. And in the second one, I spent a lot of session in one Sunday on the transformer paper itself. I spent a lot of session in one Sunday on the transformer paper itself. So now with all this background, guys, I would suggest that I prepared you to watch the other sessions, those sessions. You'll find those sessions to be mostly review. But there are things I covered there which I haven't covered here because there the approach during the Sunday is to read the paper, you know, have a guided reading of the paper. So you guys are ready to read the paper. It's something that's really is worth reading. And then we will on Sunday, we will do BERT. Are we together? And that is it guys. I'll stop the recording. Any questions before I stop the recording you