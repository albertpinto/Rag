 So today is our lab session and in particular we will go through some basic exercises in natural language processing and most of these are commonly used in the industry. Things like a chatbot, you ask it a question and it will try to answer the question. Or you have a review. Or some text, a consumer or customer has written some text and you want to do a sentiment analysis of that text and determine is that a positive review or a negative review okay then sometimes you want to extract you know locations and people's names and organization names from text like to get a sense of the context what is is it all about? Things like that. Sometimes you want to extract keywords and sometimes you have a sentence with a fill in the blank and you want the natural language processing to, one of the tasks of natural language processing is to ask, can you complete this sentence? Right? Or another example is you give it a few sentences and you say and now come up with the next sentence come go ahead and write the next sentence and if you think about these these 10 years ago these would have looked like the stuff of dreams the performance would have been very very poor. People have been dreaming about it for a long time and there were all sorts of statistical techniques 10, 15, 20 years ago but they were not as powerful as what they are today. What we have today is by no means close to perfection. There's a tremendous scope for improvement which is why NLP remains a very very active area of work. Now it's a bit of a big big topic that we have or that we are going to cover in great detail and so what I did is today also because we have a shorter session in view of the vice-presidential debate I have kept the lab hopefully a little easier and simpler to understand. Now, before we go into the lab, your homework I thought I'll explain. At this particular moment, go and just explore these libraries that I have used. In this lab, I've used only one library, the Hugging Face transformer library. It's very easy to use. It's a it has essentially made it a child's play to use and yet it has very powerful use cases. It is a testament to the power of transformers. So change the inputs, try it out on something that makes sense to you. Take some paragraphs or text from your workplace and see if you can make these things useful to yourself in the workplace. So that is that. So with that introduction, let me go ahead to the to the screen. Which library again? Yes, I'm sorry. Did I start recording? I believe I have started. Go ahead please. Kate, you're asking a question. Yes, for the homework you said we should explore libraries. Did you mention the specific library? Yes, only one library. And that is what I'm going to use today right here let me know when you folks can hit my screen so this is a very very good library to do natural language processing I was thinking about it and I thought see at this moment it would not be fair to write extensive amounts of code using low-level libraries because we are doing this lab ahead of doing the transformers and attention models. So instead of going from a very low-level primitives, you know, programming stuff, building an entire transformer, you know, the layers of the transformer neural network, which we don't know, what can we do to get a sense of the power of transformers? So I have, in view of the backwards nature of the way this week is progressing, I've created a lab which focuses on the practical utility of these things, of these things of natural language processing and I have limited to one library which is actually a pretty influential library these days it is called the hugging face transformers this is a link to that now this week I mean you can consider this lab as a huge exercise in a transfer learning now if you remember transfer learning is the ability to take models which have been trained on some data typically vast amounts of data and using them directly in your work with either fine-tuning in the end or just applying it and hoping that it will do the job for your problem set too. So today, so we'll use this Hugging Face library. It's a, it is a large collection of pre-trained transformers, just one kind of models, transformers. And so we will use some of them and see what it does. Now I would like to, before I go into it, I'd like to show you what it can do. So if you go to this website, where am I? Hugging face. This is, can I hide this control shift? I just want to hide this control panel. Control alt shift H is the alt shift edge thank goodness and now suppose I go to hugging face website this is a website of Hugging Face. It has a cute little face that you'll soon become very familiar with. And what they have is a large collection of libraries, I mean, pre-built transformers. Now, do you notice how many stars there are? 34,000 stars on GitHub. That should speak to how influential this library is. And suppose you want to do something, all you need to do is pick something. You can just go and pick. And if you just go, you'll see that there is a large number of pre-trained libraries that people have put. So for example, just from the name you can see what will this do? This will go and summarize text for you. And to summarize text, the way this works is you literally, and believe it or not, you just need two lines of text. One to just create a pipeline and we'll talk about pipelines, and then apply the pipeline to your text and out will come whatever you want. And the language of this, the way the API has been designed is so remarkably or elegantly simple that it's just a pleasure. I mean, especially we can learn and be very productive with any one of these in a couple of minutes so for example let's take the word summarization actually let me do one thing the summarization of text I did not make it in the lab in the code that I'm going to walk you through can you please take it as an example, homework, that you can do it on your own? So do you see you have all of these different models that people have contributed to do text summarizations? Now, you can use any one of these and I will show you how easy it is to use these. You can go to these models and do it. These models are explained. I mean, there's a lot of explanation and things associated with this Hugging Face Library. Here, Transformers, meta-learning, so on and so forth. You could go here and you can get for example as some of you may have heard these days distant the BERT is very very important isn't it so not but GPT-2 GPT-3 now GPT-3 is not publicly available but GPT-2 is so So here we go. You can literally start, or you can use GPT-2 to finish your curve. So this is, we went through this exercise. If you remember, we asked it to finish sentence for some person named Ray. Let's see who it is. Curiously. Well, there you go. It believes that this person is the most influential thinker who invented the first let us say neural network and let's say you know you can see that it is thinking and now in lab, we will do the same exercise. And it is surprising in how few lines of code you can make it work. So anyway, I will leave this. So let's go back now to our lab. And once again so once again we'll go to our lab now so guys are you getting a sense of what we'll do so in this hugging faces library these are the tasks you can do sentiment analysis as you will see in a moment a text generation you like we just saw an example of text generation you write some sentence and you ask it to go complete it it can write and this is how you make it right things people have gone pretty far actually they have tried to Google has tried to train the AI the neural networks to write some sort of novels and things like that, not very good quality ones, but it seems to be doing a possible job of it. So there is a tremendous scope for improvement. We are not there yet, but it's still amazing how far we have come in the last few years. And I must also mention that most of the progress has been done in the last two years. of the progress has been done in the last two years. So the things that you are seeing here, the examples in the code, none of this was possible in 2017, or at least not to this level of efficacy. We could do it, but not so much. Before 2014, it was the quality, these things were almost like dreams. Like if if you ask what what could we do in 2005 very nothing close to this so all of these things are very very recent right so quick question this particular one that you're showing right now you still haven't uploaded the homework right you'll be doing it after the lecture no no no it should be uploaded uh hang on it isn't i started No, no, no, it should be uploaded. Hang on, it isn't. I started... Homework 4? Homework 5. Oh, 5. Yes. Is it not there on the website? Yeah, probably it is not visible yet. Not visible yet. Let me see from the Linux, the Ubuntu machine where I was uploading it, if there is a final step I need to do. Yeah, there probably is a final step. Give me a second guys. I'll make sure this I apologize. It's there now. If you Thank you. Our task for homework for Homework five. Yes, that's the most current. I was still wondering homework four description. Oh, yes, yes. I haven't done that. I apologize. I'll give you. So, all right, let's come to the homework five. So, when you use transformers, the magic mantra here is this one word pipeline. And I would say that the way they have designed the libraries is so pluggable that with one line you can achieve a lot. So we will start with the use case of sentiment analysis. Sentiment analysis deals with one fact. You look at a statement, statement a paragraph or text and you try to determine is that something positive or is that something negative that is being said right it is of tremendous value on large commercial sites where lots and lots of reviews are coming and you want to keep track is that is the overall sentiment drifting positive or negative and so forth like you have to do it at scale and so you can do it at scale only if you have machine learning to assist you you can't have reviewers reading millions of reviews for reviews so here is how simple it is i wrote a sentence here so this is an import by now you must be familiar that to import something this is a statement we use line number one but whether you guys do I should I make the fonts even bigger is this clear that's nice yeah so so the library is called transformers and you notice that I'm importing just pipeline now when you go to pipeline there are many models all of these models are named models means if you know the name of the model you can create that directly and then this device has to do with the fact that you can run it on the cpu negative numbers will run it on the cpu apart zero and positive numbers will run it on your gpu if you have a gpu right so this is not strictly needed but since i do have the gpus uh this is it so now is the sentence clear guys what does the pipeline do can you guess it creates a transformer named sentiment analysis for text classification that will classify a text as positive or negative. Should I repeat that? It will create a classifier, a transformer-based classifier, which will take input as a text, any arbitrary text, and it will output a positive or negative sentiment, right? And it will give a certain degree of score to it. So this second, third line, I hope sort of makes it obvious. The output, I directly printed it into the screen. So when you feed it some text, some of us love the deep learning workshops at Support Vectors, it came out with a sentiment that says positive. And it seems to be pretty confident about it with a probability of 99.97%. Would you guys agree with that statement? With that evaluation of that statement? Yeah. So this is it. I took an example. Now you can, not only can you pass it one sentence at a time, you can actually remember that in PyTorch, and these are all PyTorch-based models. Now they are also coming up with TensorFlow implementations. But generally, it's a tensor, so you can pass in a mini batch of data. If you pass it a mini batch of data, so here is a mini batch, we have many lines. So let's see how well it does. And does it do well or not? Some of us love the deep learning workshops that support vectors. Today the sky is shining and the birds are flying, which by the way was true, I suppose, in Fremont today all is well that ends well the autumn leaves wafting in the gentle breeze again true for this season there's some those of you who have a lot of deciduous trees must be noticing the leaves falling then I deliberately on line 8 I gave it an empty sentiment to see which way it drifts. It was a dark and gloomy night. And then there is good grief. The turmoils of election are upon us again. And finally, even a train. Oh, maybe I should have said the train slowed down as it neared the station. I can redo this, the train, the train slowed down as it neared the station. So let me start running it from the beginning to make sure that we get there. So the first time it will take a little bit of a time to load the model. There it is. It has loaded it. And then it will very quickly be able to compute. So let's see whether we agree with its evaluations. The first sentence, this sentence, some of us love the deep learning workshops. It is fairly positive. It's 99.97% or 96% that it's a positive sentiment. Today, the sky is shining and the birds are flying. Do you notice something very interesting? It has come to the conclusion that this is a positive sentiment. Then, all is well that ends well. It liked it as a positive sentiment. The autumn leaves wafting in the gentle breeze, positive. It was a dark and gloomy night. It marked it as negative. But look at the next one. Good grief. The turmoils of election are upon us again. It is fairly sure that it is a positive sentiment. And it's quite interesting. The word is grief, right? And one would imagine that it might find a negative connotation to it, but it didn't. And turmoil. Right, and turmoil is there. And still it said it is on the whole a positive review. In other words words it doesn't contain words like horrible and so the idea is that when you train this you can clearly see that it has been trained on customer reviews customer reviews don't use the word turmoil and so on and so forth so um so it has been very sensitized to detect specific things which are negative been very sensitized to detect specific things which are negative and let other things alone. Then the train slowed down as it neared the station. Well slowing things down seems to be a very negative consumer state sentiment so it has marked it as negative. So when you look at this you see both the strength and the weakness of this system. You can clearly see that it has been trained on a data that is very consumer based so it also means that what what will you do if you have a different domain of data suppose you have your own data set in which you have labeled positive and negative what you can do is the first step you can do is don't first step you can do is, don't have to train the whole transformer from scratch. You can start with just opening out the last few layers and training them, fine tuning as it is called, fine tuning the transformer by feeding it your own label data. You don't need to go and retrain the entire transformer from scratch because training something like BERT or any one of these big models is a computationally very expensive task. It takes a long time to do it, and you have to feed a tremendous amount of data. And so the whole point of transfer learning is you can actually make it more in tune with your own text and your own sentiments of your domain by simply fine tuning it, retraining just the last few layers. Which should be your first approach. If all fails, then of course you can go and try out more training it from scratch. And there also from scratch. And there also, there are big models and small models, GPT-2 in birds, et cetera. These are big models. And the Hugging Faces has come up with the Distilled Bird, a relatively smaller model and more something you would want to train if you have to train it from scratch. So this is it. But do you notice something, observe, that we are nowhere having the training loop you know running through the air box and training I directly use the pipeline to do it and one very elegant thing and very Python ik thing that they did is that when you instantiate the pipeline yeah the return value is actually a function that function can be directly applied to text. Do you see how elegantly it can do that? If you look at this sentence, you would imagine that pipeline is an object on which you would call some method. If it was Java, you would say sentimentalism object, and then on that you would say dot predict, or dot evaluate, or something like that. But the beautiful. The beautiful thing about the way this library has been designed that when you create a pipeline. It is automatically a callable function. Jay Shah, Dr. Anand Oswal, Ph.D.: I consider that rather neat and in the simplicity, it. So that is that. So this is it. So this now what else can we do? You may say, well, you know, these are some things that I cooked up. Then let's go towards something a little bit more heavyweight. By the way, what happened to the blank statement? One second, one second, please. Do you notice what happened to the empty statement? I think it skipped. Yes, it just skipped it because it realized that there are no tokens in that. So yes, go ahead Albert. Actually in the statement above, your first sentiment direction on your first statement, some of us love the deep learning workshop. If that has been replaced by some of us love some negative thing. So I mean, what I'm saying is the training creates a subjectivity for the transformers to work in a certain way right let's say the amorphous now the let's put something which is yeah so see what happens is that machines don't understand irony and this is one of the limitations. You will see that. To say it would be an ironical statement. Nobody loves being fired at our jobs, right? But it will come out with a positive. So this whole study of irony and sarcasm, there's a lot of active research to deal with it. But at the moment, the state of the art doesn't deal with it very well picking up there's a concept right so that is it does that answer your question yes yes and if you remember this goes uh adds more dimension to the sentiment itself there's more context yes right so and now we'll do that a little bit more. So here we go. Suppose I go and so I have added into your project, you will notice that in the tech session, I've added about 100 reviews from what is actually millions of reviews of Yelp. Right. So respecting the fact that some of you don't have very powerful machines or laptops, 100 rows does the job. It proves the point. So this fact is there. So what we are doing is this code, hopefully will look fairly straightforward. I just create a sentiment data frame to store the results. Now what we do is for, I open that Yelp review file and read it line by line each line is a record the record contains many things now this line 15 is the one that needs a little bit of description this is called regular expressions regular expressions is a string magic thing if you know it of course you'll find it straightforward if you don't know it you'll find it a little bit annoying that it is there let me see if i can show you what i mean by that if we go to the text directory where is the yes smaller yelp review let's go there yes do you notice that one record looks like this it is a JSON mess right raw text yes somewhere in there is the actual review you know the text of the review is right here right there is a lot of things but this is the really text of the review so we need to extract from here to here what do we need to do we need to extract this part that i've highlighted does that make sense guys yeah you're extracting the text in each line of the json right I need to extract the text. So how would I do that? I need to ignore all of this and just go and capture this text, find, match this text. So for that, in the world of every programming language, there is a facility called regular expressions for string. So the regex, right? Regex. So you use the regex for that. You import re and so it will say find all matches for this. Now this regex, there are many language of its own. I'll give you a little bit of an introduction. I'm saying look for the word text, close of quote, in colon, in a colon and beginning of quote this. So if you look at it here, that is what it starts with right this is it i'm saying that a very good way to know when the text starts look for this part what i've highlighted right and after that after you have done that you can do the dot star dot means anything star means however many Star means however many. Till you hit the end of quote. Because if you look at the text, this thing here, if you notice very carefully after the word appalling, there is an end quote. So we want to capture everything till that. And that is what this does, it captures everything till that and that is what this does it captures everything till that the question the question mark prevents it from being a hungry capture in other words you don't want you to get the biggest text that matches your constraint but the smallest text that matches your constraint if you don't put the question mark then what would happen is it would go like for example the last quote ends here it will pick it this up which you don't want isn't it you don't want it to capture date and all of that because that is not part of it so this is a little bit of regex magic so you get the reviews so now it turns out that there is only one instance of it so first you check if there is anything why is this important this is important what if the review is completely empty there is no point in feeding it into the sentimental transformer remember empty text is no use so I'm just checking if the reviews is not empty then there is only we know that there will be only one match it is an array of or a list of reviews in that there will be only one match. It is an array of, or a list of reviews in that, but there's only one match. Then I, from the left, I strip the prefix and from the right, I suffix the code. So that we are just left with the actual text. So this is basic string manipulation. And then what do I do? The real, the machine learning or the NLP part, it turns out is just one line. You do sentimental review. It returns you, unfortunately, a list of dictionaries and the dictionary has only one element in the list. So I'm just getting it out. And the dictionary has two key values. One is the label and one is the score. So what I'm doing is I am creating, I'm just adding it to the three columns of Pandas. So if you look here, I created a Pandas data frame in line 11. What are the three columns of that? Label, score and review, right? Review being the actual text to compare and so we have we got the result result will contain the label positive negative score like how strongly you believe it is that and the actual text that for each text so I'm appending that here. And you do index just computed and this is a computed hundred reviews. Now guys, I must mention that those of you who are new to either Python or to regex, there are excellent websites that help you create this magic expressions, right? There are online regex builders. If you search for online regex builders if you search for online regex builders you can do that and they're also excellent regex tutorials regex is something if you don't know i would strongly advise you to do that it's one of the most powerful things in the world of natural in the world of string processing and when you do natural language processing you are talking about text and things so in the pre-processing stage just as here you will often be using regex and so get used to regex which is one reason i i deliberately put it here see other way i could have done is i could have come up with a csv in which all the reviews were already extracted and given it to you but i wanted you to have a little bit taste of real life where text comes usually in not the format that you want and you have to extract it. And so get used to regular expressions if you're not familiar with them. Did you have to import anything for this asset for Jadix? Come again? Do you have to import any library for using uh in python it is called re if you're doing java it is java javax that i think util.regex or java.regx or something like that so i just wanted to point out something in the register yes. So it's stopping with a quote even within the reviews, because I saw some of the reviews stopping with a backslash. No, let's go back and check. Let's go and check. Reviews are, do you notice that all the reviews, so we just look at the tail end of it. Do you notice that this ended here with appalling in quotes? Let's look at the next one. The first one is just look around. You know, I was told to just look around. There is a quote within the review. It's backslash. Oh yeah, those are escape quotes. So those escape, regex is, and that is where the regex is so smart. Because you have escaped the quotes codes it will ignore it so in other words I escape it is a particular unicode character at that moment it is not the code all right yeah I'm afraid it's doing that actually because down in your notebook somewhere it's stopping the string with a backslash which means it has talked with the backslash code and the codes from it. Okay, let us go and see it is possible it did that. It is possible. Where did you see that? In the notebook? Yeah, the third or the fourth one. Yeah, you see this now. No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, because it is this is pandas it is just showing a few lines so let us do one thing uh let me do it put a three dot if it's a if it's a full line it would if it has one character it will give a three dot so review uh let us do this we have this result and let us do review now do you notice that it doesn't end it doesn't like 0 1 2 decimal like hang on review and so what you can do is for line for review and let's see if this works quickly I'm trying to do something so that the whole thing will come through review there we go and sometimes and then let us print the number also huh yeah X number I'll start it with F yeah and I would start with line breaks and then index so for review and I'll just put an index here index is equal to one and I will say index index plus review plus and review should also be under this review plus and it is possible I made it easily possible I may have made a mistake and once again, and I'll put the word end here for fun. Right? So we know that this is like this. Let's try it. Also you want to increase the index also. So I would, agents would call, yes, you're right. Actually escape double code. So I did not ignore the escape double codes. So I need to improve my regex so what it does is when you create this regex statement you can you can tell it what not to so what not to consider your terminating character right so that. That looks very complex. Thank you. It is. It's fun when you get it working. It's just absolutely awesome. But if not, then it can be a bit of a problem. But I look like it's- Sorry. Asif, why do we need a panda? Oh, I just tried to put it into a beautiful frame. You don't need pandas. You don't need pandas. You don't actually. So review in sentiment. So yeah, you can play around with it on your own. So now with that is some text is here and now let me just put, now just let me just put the sentiments so that you have that. So hang on. There we go. So it is just giving you, see, I'm just doing it to create a nice table, negative. So would you agree, as someone who has worked in the museum let's go and read the very first one as someone who has worked in them i was eager to visit the gallery on my most recent trip to vegas when i saw they would be showing infamous eggs of the house of blah i knew i had to go tucked away near the gallery and much hidden from you it is what real estate agents would call okay so somehow i didn't pick up the whole sentence but for some reason it seems to have come to the conclusion that this is a negative statement for yeah i was actually horrified this place is still what do you say to this guys this negative looks right i love the big the guns seems to be good this smells lukewarm defrosted tasting clearly negative on happy days finally have a cane snare so forth and the other the whole experience is awesome is positive reviews for this place on facebook's are great so positive amazing food in perfect location positive three stars for food but the service was awful well negative I'm traveling out of town it's amazing whatever it is so see at this particular moment sentiment analysis is at a place at which I would say that it is it has crossed the threshold of being useful it is certainly useful and when you look at data at enterprise scale it has a lot of applications in practical life is it a perfect tool it's a very imperfect tool it doesn't catch irony it doesn't catch sarcasm? It doesn't catch a lot of the nuances. Having said that, you can improve upon it if you have a very specific domain of text, right? And you are willing to sit and label the text. You can have annotators who will annotate the text as positive and negative or whatever. Then you can train this. Very easily you can train this, right? That is it. Asif is it yes go ahead wrong to say that it's just picking up on the adjectives and deciding on the positive and the negative of the sentence or the no it is not in fact that is one of the things that you think the the beautiful thing is that it is not picking up just on that it's more complex than that try it out try out sentences without in which you can say things without necessarily using adjectives right okay i think what i see is it has got a probably a list of words for positive negative and slide accordingly in the sentence when it comes across. No, it is not. That's what I'm saying. See, one would suspect looking at these examples that you can just maintain a catalogue of positive and negative sentiments and then therefore deck it. To some extent, see, sometimes the signal is very clear. Like in some of these, the signal is very clear. Then it can do it. But if you think about it just look here uh look at the text that i gave you the train slowed down as it neared the station it comes to the conclusion that this is a negative statement now how did it come to that conclusion slowed and down in the word slow yes maybe slow. But look at the other word before it. Good grief, the turmoil of elections are upon us again. It's focused on good and said all right uh it was a dark and gloomy night it picked up on gloomy on the other hand the autumn the autumn leaves wafted in the gentle breeze you would agree that there is not a single positive word here adjective here right it describes the mood right If you walk out of your house, you will see that nature is in this state today. But it got the mood as positive. Maybe from gentle. Perhaps from gentle. But how about this? Today the sun is shining and the birds are flying. Shining? Well, shining is not necessarily shining. I'm shining a laser light into your eyes. I would say it still would take positive. It would take positive, yeah. So you can sort of guess. It is not a very perfect tool, but it's a little hard. As a post hoc, you can give interpretation that it's picking up this or that. But in reality, it's a very complicated neural network our picture we learn tomorrow go ahead is it every that has a neutral tone you know some of these can be termed as neutral right so yes you can do that so that is one of the things you learn when you do the natural language processing month so suppose i make it fine-grained and i say that can i have three grades positive neutral negative it's very easy to do right you just need liberal data and then you can very easily do that that is actually one of your homeworks later on you can do that you can do whatever you want. You can have 10 different classes if you want. On a scale of 10, you can rate 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. So we'll come to that gradually. The only thing is you need to create labor data with that. So this was for Yelp. I'm sorry. I have one more question. Go ahead, Rajesh. On line number 15 at Regis, you had certain special characters as a format, which were like, you used it to get the text. So will it be like different formats for different people or will it be the same? Yes, yes. It is called, so these are called patterns in the world of regular expressions. You are, you have this, this magic language is the language of patterns, straight patterns. And then what ari.findall is doing is it's applying that pattern and searching for that pattern in the line. Now, learning regex is not an overnight thing, guys. Typically at some point in your life, you'll have to give three, four days solid to it, morning to evening, before you become good at regex. But in my view, the time is really worth investing. Regex is one of the most powerful things you can do and a necessary thing for doing string manipulation. If you're doing anything with strings and text, not to know regex is like not to know driving in US. Yeah, and also I'd say for log analytics, it's very useful. Yes, yes. Outside of course, in normal programming, it's a world enterprise world, very useful. But in machine learning, it's to the extent that you're in the natural language processing world it is very important that at some point you pick up regex because the pre-processing of data is a fact of life data never comes in the way that you expect it you have to pre-process it and this regex is a staple is one of the facts of life that you I mean one of the great tools you'll use when you're pre-processing data so that's that now what about the fact that we might be talking about some other languages we we said English is fine but can we do the sentiment analysis in some other language so this is spanish now also i use this as an occasion to show you what happens inside you know when we just said pipeline sentiment analysis and so on and so forth what you can do is you can be a little bit more sensitive and give it a particular transformer that has been trained to deal with multiple languages. So this is a multilingual. It is probably the same architecture has been trained with more data, data from different languages. trained with more data, data from different languages. Now, this line needs an explanation from Transformers is okay. Auto tokenizer and auto model for sequence classification. What it does is that, see when you get the model, what happens is this transformers, they have the head chopped off. They have been trained, but then the heads are chopped off so that you can plug in a classification into as many classes as you have in your target right so your target could be on a scale of one to five rather than just positive negative or you may want to just have positive negative and so forth so usually what you do is you can it gives you the flexibility of attaching your own head at the last you know the last layer to these So most of these pre-trained models have the option that they come headless. And we'll see it in a little moment. So what this does is when you do auto model for sequence, you're saying take this model and make it for this classification process. So here we go. We take this model and we are saying get it ready for sequence classification now in the natural language processing world a text is a sequence of words so for classification classification into some scale some levels positive, one star, two star, three star, and so forth. So that's what you do. And the journey is like this. You take a text, you tokenize it, you know, you break it up into the words. And there's a little more to it because those words, remember, they have indices like in the word dictionary. So there's a bit more to it. We'll learn about it just in a minute. Then you need to give it, like when you create the pipeline, you say you're doing sentiment analysis, get it ready. But what we want to do is, if you want to use this model, which has been trained on multilingual and auto-tokenizer, this word is very interesting. What it is saying is detect what this model needs, what kind of tokenizer this model needs and create that particular tokenizer. What it means is that the size of the sentence, the input vector to the model, what is the dimensionality of the input vector, and all of those alignments take care of that. Not only break it up into words, but take care of the dimensionality and padding and so on and so forth for that, and then feed it into the model. And so these are the three extra lines. This is just a model name. You create the model here, and then you create the tokenizer. So, what do you do? You take a text, it will first go through the tokenizer to become tokens, and then from the tokens, it will be fed into the model to get the prediction, isn't it? And so, now, and therefore, the meaning of the word pipeline what you're seeing is you're building a pipeline somewhat similar to the scikit-learn notion of pipeline that we are familiar with in which we are giving it the tokenizer and the model maybe i should have written it in reverse order just to emphasize that first the tokenizer, then the model. And so then you do this. You give it some Spanish sentence. By the way, I don't claim to know Spanish. I just use Google Translate. I think I don't even know what this means. I forgot that. The marvelous experience. The marvelous experience, thank you. So yes, the marvelous experience. And then thank you so yes the marvelous experience and then i asked it to um score it and it seems to have said five stars right so um marvelous what you would say about some i don't know restaurant or something like that so that is that you can deal with different now the limitations are if you are wondering many languages are not supported for example korean is not yet supported any of the indian languages are not yet supported in this transformer nobody has contributed a transformer trained on those things so that leaves a great opportunity for you if you guys want to take initiatives for your favorite language all you have to do is take the model and train it, or maybe last mile train it on your language. You just need a database of label data, label text, and just feed it and train it. That's all. So understanding the inner details. So now we're trying to understand the inner details, the same thing. We'll break it up into parts. See, when you go and get the basic sentiment analyzer, it goes with English, right? And there is this particular model that it might end up with. So here we are taking this model and Distilbert, et cetera, we'll talk about tomorrow and later on. And what you're doing is you are creating a bottle excuse me you're creating a model and you're creating a tokenizer you're saying auto tokenizer go figure out what kind of a tokenizer you need for this particular model so it does that so let us go inspect and see what it all looks like. You suppose to the tokenizer, you give it two lines. One line is this. I give it a positive line and a negative line. And think of it as your mini badge. You're creating your mini badge of data. Padding is true. It means remember that they have to be padded to be the right length. Truncation is true. If it is longer than that amount, truncate the text. Return it as PyTorch. Like, you know, in this workshop, we are only dealing with PyTorch and try to be PyTorch compatible for everything. Hugging faces was initially implemented in PyTorch. Now they have started supporting TensorFlow in addition. So now I have my mini batch batch and then what do i do into the model i feed the mini batch now one of the things i do is you know if by default if you don't have this so suppose you have your code as this it is enough you're just passing do you see how intuitive and easy to understand it is you have your input tensor which is text, and you're passing it to the model. The model will first do what? It will do your tokenizing, and then it will apply the transformer to it. Right? But you'll get the result, which will be the label and the prediction, the label and the score. But if you do this, it will also tell you all the hidden weights in the neural network which is quite interesting the reason i'm mentioning it to you is that you can actually do a deep inspection of these weights and see where the strengths are like which of these uh notes are very powerful or have significant weights and so on and so forth and which of them are muted so that is it so we get the result you know you give the model you get the result except that when you do it like this what will you get thus the last layer is missing in a classification problem multi-class classification problem what is your last layer or any classification problem typically what is your last layer in neural networks guys the softmax the softmax either softmax if it is a binary classification you might use sigmoid so it is sigmoid or softmax so let's take softmax for default what this is producing is just everything till before the last layer it is the input to the last layer so this result you need to feed into the softmax right when you feed it into the softmax right then you get the predictions and when you get the predictions remember how do you get the predictions and when you get the predictions remember, how do you get the prediction says There how many sentences did we feed in to write a two data points, right? So we expect that the predictions will have Two arrays as result now that is fine. There are two rows, but why are there five values? Can you guess guys? Why are there five values? are there five values can you guess guys why are there five values five stars five glasses yes five star four star three star two star one star right it's the leichhardt scale now just by looking at this can you tell guys just look at the sentence here some of us love the deep learning workshops at support vectors would you call it a positive or negative statement? Positive. Positive. So the five star, you see that the energy is distributed, the soft maxes, probabilities are distributed towards the positive, five star, right? 20% that it is a four star, very low percentage is a three star, and very very very low percentage that it's a negative sentiment what about the other one a horrible food now what happens the probability is a del that towards two star three and one star and maybe three star and it is a fact because in reality a lot of people if they don't like something they'll give it three star they won't necessarily give it two star or one star depends upon how strong a critique people are of food of any of the things and how harshly they judge three and lower are generally in business considered bad news and if you look carefully most of the weights are divided between those low scores so does it seem to make sense guys you can go and inspect the hidden weights all of the neural network weights and it is I'm not printing it here because the notebook becomes huge because when you print out the weights you'll be surprised on how deep and complex this neural network is like the tensors will get spilled out and they're filled with lots and lots of weights. It goes reams and reams of pages. Asif, question. In line 31, why are you using star star tokens tensor? Oh, it is a Pythonic thing. When you give it a map, whenever you pass map as an argument, this is the way you pass map as an argument, right? This is the way you pass map as arguments. What is a map? Key value dictionary. Whenever I see, look at this token, a token stenson, it will produce a dictionary of key value, key values. All right. It's just a data structure. Whenever you have data structure as a dictionary, see when you pass something, when you want to pass the values of a list, you give star and the list name. When you want to pass the key values, as a long list of key value from a map, you say star, star, that that maps name or that dictionary's name. It's a very Pythonic syntax. So, Asif, I have a question. Yes. What does tagging a text data specifically mean for sentiment analysis? So each word should be tagged as positive or negative? That's it. For sentiment analysis, positive, negative, one it for sentiment analysis positive negative one star two star depends upon what your purpose is but yes that is what you do okay so that's the labeling the data right you don't usually use the word labeling yeah in a way you can say is predicting the label see you can label data in which somebody has given you the sentiment to train, and then later on you're making predictions. So yes, you can call it that new data, you're labeling it in some sense. Yes, you're attaching labels to it. So this is one thing, can you relate? Yes, go ahead. So what does that dictionary include? I didn't really understand what the token is. Okay, so Nisarg, would you mind if we take this up on Saturday? Because now we are talking about Python syntax. Okay. I don't want to like blow you off, but the thing is that I need to explain it to you with examples and it will take us 20 minutes. This syntax is a very odd syntax. You find it only in Python. And it is really worth knowing it properly. Okay. And one last question. In 9.32, you were using dim equal to minus one. Oh, yes, yes, yes. So what happens is that when you don't know how many output labels are there, suppose you know explicitly there'll be five, you can say five, right? But when you don't know, you are saying, you know, figure it out based on the data. Minus one, and that is another Pythonic thing. And NP, like when you do PyTorch and when you do uh and numpy and everywhere minus mean minus one usually stands for go figure it out for example the number of rows go figure it out from the data i won't tell you whatever it is right that's that's what it stands for you have to for. So if you'll figure it out from the input. Yes, exactly. Yes, that is it. That is it. And so then it does work out. And so, so that is that. By the way, remind me Alex, I'd like to explain this syntax. This syntax and for list the equivalent is star list. Both of them are very Pythonic things and they deserve an explanation separately what do i call this when i bring it up what i just say that a list and a list and maps as arguments function arguments right so that is that so well we we went through one thing what about this i took a bit of text here do you guys recognize this text yes just very fast coming from can you guess where it is coming from your website exactly from a course I hope you have read this it should be familiar to you so what we do is we give it the text and now we ask the machine this question that AI this question what is the easiest way to read a paper do you see that somewhere in there that is explained sorry I will bring this here somewhere Somewhere in there, do you see this guy? It is explained. I'll give you a moment to read this text. Did you read it? I have before. I remember the IMRC from the data. That is right. So once you have, when you reach this section, somewhere all papers broadly, for the easiest way to start reading the paper is to first read the abstract and so on and so forth. So let's see how well it does. You're asking the question, what is the easiest way to read a question? And then the other question you ask is what happens in due course of time? Somewhere in due course of time, it becomes blah, blah, blah. Let's see how well it does. These are two questions, right? And this is somewhat, you know, you're beginning to get a clue on the drivers of chatbots and things like that. So here we go. What happens? You create a pipeline once again for question answering. Now, can you imagine anything simpler than this? Think about guys. It is like all that tremendous power available to you in this library with just a couple of lines. And when you do this question answering, the first question, it answers that uh the first question is what is the easiest way to reading a paper it says first read the abstract skim over the introduction and so forth would you agree with that so would you agree with this answer definitely what is the score here like uh it's just saying how confident it is that it has answered your question and so forth it's just four percent five percent yeah right so it is not very sure about it but this is the best it could do but actually it did a pretty good job isn't it line 41 where you're saying create a pipeline with question answering yes like that's a it's kind of a keyword for transformers. It is a transformer that has been trained for this purpose. See remember you train a transformer to various tasks, right? Various datasets. This transformer has been trained to answer questions. And that's already available to us. That's already available to you. Isn't it wonderful? And now in the second part, I will show you with the one of the labs will do is I will have you train a transformer from like simpler transformer from scratch. And you'll see how do you go about it in detail in depth. Okay. you go about it in detail in depth okay but I didn't know guys the first time I encountered these things I'm super excited and absolutely amazed is there anybody else who shares that sentiment yes yeah and then look at the second thing what happens in due course of time you know you're asking very normal questions and you have just given it a paragraph for text, reading comprehension. So I'll let you determine how well it would have done on the SAT exams. So here it is, question and then the same text. And it says, what happens in due course of time? And it says, it becomes familiar and easy. Okay. Pretty impressive impressive isn't it right then you can uh also ask it to fill in the blanks i suppose the sat exam has a lot of it used to have a lot of fill in the blanks i don't know if it is still there right so uh sat is optional uh starting this year. Oh, right, right. Because of the COVID. Not because of the COVID, but yeah, it is optional. Because people are saying that it is a debt against the disadvantaged people, isn't it? Something like that. Yes. Yeah. Nisarg, go ahead. Yeah. Asif, I was wondering, where do we get these names for different pipelines? On the HuggingFace portal, all the documentation is there. See, here's the thing. We did it. I think you've forgotten a little while ago. We just did it. Where are we? Oh my goodness. I keep hiding this silly loose thing and it keeps coming back just go to the portal do you see i will say uh question just put the word question and do you see right here is question answering oh i see so so many of them are there you pick one choose which one you like. Wow, so it actually uses all the available models in its... It will pick one default, but I think that, and then you can change it to another one, or you can go, and if you have trained, see what happens is to train this model takes a lot of effort and a lot of data. And the lovely thing about AI is people are very willing to share. It's a very open community and open source mentality so they share all that just go pick yours you see how nice it is yeah that is it so now here we go a film master exactly you know when we do the fill in the blanks let us go and see what what it does uh fell or sorry oh gosh blank yes do you see how many mask based things there are models there are to help you with that. Any reason why they're called masks? It's a technical word because what you're saying is that when you train it, you mask one word and you ask the transformer to somehow guess what word it was. That is where the word mask comes from. But more intuitively, it just fill in the blanks so here is it i took this and i'm saying some of us here is a fill in the blank underscore underscore uh the the deep learning workshop training at support vectors very much right so suppose you have this statement let us say what it says let us see if it is even intelligent so when you run this it comes it comes forward and says that some of us enjoy the deep learning workshop training training at support vectors very much I'm happy that AI robots seem to enjoy it too okay so some of us enjoy some of us appreciate some of us enjoyed the deep past tense some of us love the deep learning workshop some of us like the deep learning workshop would you agree that it seems to have a pretty good sense of what to fill in yes so this is it and the next thing you can do with Yes. So this is it. And the next thing you can do with NLP and these transformers is you can, guys, do you notice that even though this week was for natural language processing, I have almost exclusively focused the lab on transformers. Transformers have become by far the bigger piece of natural language processing. It's sort of the hammer these days that makes most of the problems in NLP look like nails. The other thing that is used is the recurrent neural networks, the LSTMs and GRUs, but not as much as they used to be used. And since I haven't covered that, I didn't put those examples here. So now we talk about text generation. Go ahead. To understand what we're talking about with the transformer, right? So the body of knowledge that it is using to come to the conclusion is the paragraph that you're providing. What the transformer has is essentially the capability to use that body of knowledge to come to the conclusion. Correct? Exactly. Exactly. Okay. Exactly. And that's remarkable. You know, if you really ask this question, did I even give it enough context? For example, it could have, it could have said some of us hate the deep learning workshop training at support vectors very much. Or some of us eat or run or jump or whatever, any verb you can put here. Some of us hammer or whatever. It doesn't matter. But of all the things, it seems to have picked just the right one with so little context. But of all the things, it seems to have picked just the right one with so little context. Right. And that's a, that's a remarkable thing these days. Let's look at text generation, which uses by default a GPT-2 model and see how well it can complete your sentence. So here we go. You say come up with next 60 characters in this. The easiest way to start reading a paper is to first read the abstract, skim over the introduction, and short circuit straight to the conclusion. Right? You feed it to the text generation pipeline. And again, exactly. Do you notice that all you're doing is that you're changing the name in the pipeline to text generation? What does it do? in the pipeline to text generation. What does it do? It writes the following generated text. The easiest way to start reading a paper is to first read the abstract, skim over the introduction and short-circuit straight to the conclusion. Then it says the paper is a good starting point for any student who wants to learn about the history of the world. It is also a good starting point and so forth, it will go on. So this one, it seems to have missed, isn't it? Because we are not talking about the history of the world. So, well, you can rest assured that AI still cannot become our overlords and replaces yet. Let's try another text there are many legendary researchers in AI the most important researcher in AI is right and let's see what it comes out with there are many legendary researchers in AI the most important research in AI is the famous mathematician and philosopher Albert Einstein. He was the first to use the word intelligent in his book, The Theory of Everything. He was also the first to use the term intelligent in his, and so it continues. So if you look at this, guys, it contains both the limitations and the strength. This was not possible. You see how coherent and grammatically correct things it is writing? Almost be convincing. It's almost convincing, right? You have to be in the field of AI to know that well Einstein didn't contribute big things to it. But if you go and read his theory of everything he does speak about intelligent you know intelligence in the world universe and whatnot and so forth right so there is more to it I'm sure that text must have been fed to this the then another functionality you can do with NLP, I think this is the last one I'll talk about, is name entity resolution. What it means is, can you pick up locations, people, organizations from a text? So let's take a piece of text. There are many excellent AI workshops offered by Support Vectors, which is located in Fremont, California. Asif Kama is one of the instructors. So let us say if we give it the job of name entity resolution, what can it do? What it can do is it picked up the word support and it says it looks like an organization. It picked up the word vectors and it says that too looks like an organization it picked up the word Fremont and says that is a location would you agree guys that it's a location the word just the abbreviation for California and it came to the conclusion that this too looks like a location then it picked up these two are safe and says well that looks like a person and come out in in three pieces it comes to the conclusion that it also refers to a person as if can you touch upon why it broke it down is it looking for the number of no it is leaving you the possibility see if you look at these two right let's take this it is saying that fr just the first two can be a location because you know it stands for France right and it is saying that the same two when attached to this word becomes Fremont and that too is a location. Okay. That's the way to interpret it. Okay. That's that. So it's a pretty good. Now I'll tell you a use case that we, I it for my work we got we deal with a lot of data that is very sensitive this is the HR data you know people's performance reviews or something like that so when we brought the data when you wanted to do analytics on the data one of the concerns was that the big data, people always have this paranoia that big data automatically means violation of privacy. So to deal with that paranoia, we had a constraint that you cannot take into your big data cluster anything that may be the name of a person, PII or C know and the identifier for the person or anything that identifies the client name the organization so when we try to take in text I guess what I used many years ago to strip out all of the PII and CII from the text. I used a named entity resolution. So these are practical uses that you can put. It's I would replace all of those locations with masks. Mask being like blanks. Blanks, yeah, you just XXX. So the difference, there is an entity recognition in spacey also right so that takes the whole world where this it is there see all of these technologies are there in many libraries see if you go and look at nltk the one on which i notice that the most of the books of uh natural language processing are written they all start with nltk it's a good starting place it's large library a simpler version is the text block if you remember i started you guys with text block and deliberately i did not take that example let me take that example too just as a reminiscent of what we were doing. Where are we? NLP Libs, NLP Libraries. So just to say the other things that we covered and we didn't, so if you look at text block, we are saying, consider these lines, Charles Dickens lines. If you remember, we are saying, a tag it as parts of speech. What is each word a part of speech of? It was the best of times. So you know, proposition of the so on and so forth. So it identifies, it can do that. You ask it, tell me the nouns. And it seems to have come up with all the nouns. You ask it to give a sentiment, it also will give you a sentiment of the sentence. Language detection, it can tell you what language it is. You give it the text, it says that the language is English. So NLP has all these tools to do that. I deliberately avoided examples that we had already covered in previous labs. Another is translation. Just to remember, you have this text of Dickens, it was the best of times, it was the worst of times, it was the age of wisdom, the age of foolishness. It's iconic, sort of a text in Western literature. You pass it through, translate it into Hindi, apologize to people who are not Hindi speakers, which probably means most of you, so then those of you who can understand Hindi would agree that this is a fairly good translation, not without mistakes, but it's a pretty good translation. Yep. Right. And it's pretty good. So this is the power, the state of the art in natural language processing is this, guys, the sentences. It can detect the sentences, it can detect the words, it can do all of these things. And if you remember, we did all of these things in our very, very first lab. And then it can also do keyword detection. So for example, this was it. You give it some text and you ask it to detect all the keywords, right? And it will detect all the keywords for you. Oh, I didn't give the code here, why did I not? Okay. Oh, this I left as a homework because it is there in your source code. So if you go back to not the notebook or here you will see it in the NLP somewhere keywords keyword extractor so here is it you're extracting the keywords using gen sim and rake two of the libraries right so there are many libraries for NLP, predominantly written in Java, C, C++ and Python. The libraries that are written in C, C++, they always have a Python binding. So pretty much if you know Java and Python, you're well covered in this library. It's a very useful Stanford library is written in Python. I mean, sorry, in Java. And now it has a opening to that. Then there is an open NLP library. The many libraries, which library should you use? That is a question. So here is a rule of thumb guys don't use an L TK in production it's too slow right despite what your textbooks and all these books in the market tell you don't use that unless you don't have any other choice first of all it doesn't have the semantic aspect of it so you'll have to couple NLTK with Gensim to get the semantic aspect. Your first choice should be Spacey because Spacey is designed from the ground up for high performance. It's an opinionated library. It gives you only one way of doing everything, not too much flexibility, just the opposite of NLTK. But Spacey gets the work done reliably and at high performance, which is what you want in production. In production, reliability, scalability, performance are the big words. So do Spacey. When we do the NLP month, we will do a lot of Spacey. The other thing is, besides that, you should use transformers as much as possible. Now, when it comes to transformers, how would we use transformers? spaCy has a direct binding with transformers, the hugging faces transformers. Transformers hugging faces has pretty much the market share, pre-trained transformers. Then, when you're doing PyTorch, I'll take you through the exercise of creating your own transformers from scratch. It is a good exercise, but if you can avoid it, training it, because you need a lot of data to train these objects, these beasts, you should be aware of it. Your first instinct should be transfer learning or just fine tuning a model, existing model. PyTorch is actually very, very good for this. There's a PyText library which sort of makes it very convenient to do all of these things, new work, any kind of custom work that you want to do. You can do it in PyTorch. It's very powerful. The Allen NLP, I didn't cover. I want to cover, I wanted to, I was sort of in two minds whether to do Allen NLP or to use Hugging Faces. Allen NLP just sits upon Spacey and Hugging Faces and PyTorch, right? These three things. It sort of creates a higher level of abstraction the reason I didn't do that is it's for things where it works it works very well if you remember we did a demonstration of it on Monday but at the moment it is it is very rough on the edges it's a new library it is rather rough and I thought that once you become familiar with things then we'll go and do it in the In the month of NLP will do the Allen NLP also right so species hugging faces the species transformers and add an NLP I'll focus largely on that. Which will be a little bit different than what the typical books do see all those books are good to go with NLTK, but going to production is an entirely different game. And it's important that we do that. Now I want to do something, guys. See, go ahead. So I had the opportunity to work with Matt. Do you know Matt? Full name? Matthew Honnibel. Sounds very familiar. Now please remind me. So he's the one who created Spacey. Okay. And I gave a lot of inputs to like how to enhance it. So I worked with him about like I think two years back. Excellent. Wow. To enhance this, his parser, like to make it there to take it to production that time it was not ready for production it had a lot of works so when I was working on the translation work yeah I work yeah so I used specie and we had a lot of bugs so we fix that oh very good so nice we have one of the authorities amongst us good great so I trust trust when we do the nlp part you are willing to assist everyone else with their lab work uh yes yeah nlp yes nice i see one question in terms of understanding these so there were different libraries that you talked about right like spacey beingCy being one, then NLTK. So when conceptualizing the role that one of these libraries play, is it reasonable to assume that they all have similar exposed interfaces? Very similar. See, that is it. Important is to learn the theory. Libraries come and go. If you ask me five years later, which of these libraries these libraries would be there or 10 years later my guess is none of them would be there there would be something even better right this field is exploding uh premjit think of it this way just about everything that you are seeing most of it was done in the last two years quite literally so my question is coming from here as if if i think i'm putting my product manager hat on right what i see here is any investment done in the space is bound to need significant surgical change to the code line very soon partly yes though in reality what you do is see when do you take a model to production? When the business people say that it is effective, remember the effectiveness of model, the boxer statement, all models are wrong, but some are effective, some are useful. So when a model is useful and the code has been written and trained with the library, you take it to production, people generally tend not to touch it. They leave it. The new functionality they do with the new libraries. People see, here's the thing, too much investment goes in making something bug free, as you know, Pranjeet. In Oracle, we used to think about our S-base kernel, right? How much effort goes into it in creating that kernel and making areas of it robust so think what it would mean to just replace it completely from the ground up with something new hard to imagine right so you leave it there let the use cases that it serves serve it and then from there onwards if the new thing new thing comes up on the horizon you use it for the new use cases new thing comes up on the horizon, you use it for the new use cases. Actually, okay, not to distract the rest of the team, my question was exactly on that area there, Asif. If you take something like S-Base, it has about 20 years of depth, right? 20 years of evolution brought it to where it is. So when we look at that now, there is 20 years of fixing things and stabilizing. Exactly. What I see here with all of these things that we're talking about, they are fascinating. But if you put a lot of investment in terms of a business return from it, it needs a lot of nurturing, a significant amount of investment needs to go on year by year, right? You can't lock it and move on. Yes, that is true. And so that's why you should pick libraries that are enterprise grade. So how many libraries are there in the NLP space? Hundreds. But what I'm mentioning to you, and this, the Hugging Faces Transformer, the Spacey and so forth, these are the well-established industrial strength library in which tremendous amount of contribution has been made by people. So they are likely to last at least for many years. But nothing prevents from yet another breakthrough from coming by. Speaking of the breakthrough, there's a paper called Attention is All You Need. Then recently somebody came up with the paper Hopfield Networks is All You Need. So obviously it's a very provocative paper but it hints at all of these things, they hint at the fact that we are at the cusp of a revolution and things are moving very, very fast. So what happens to the recurrent neural networks and the use of LSTMs do a lot of this work. All that libraries are mature. They are very stabilized, rock solid, except that now you realize that there is something that beats them. you realize that there is something that beats them. Right? So if you open any book published before 2018 or even 2019, there is a great risk that they're teaching you the things that are now obsolete. Okay. All right. Thank you. So I say one more question. So how is this distal bird different from albert it's a sparse out model it's a much simpler with far less parameters than uh than the oh sorry than even albert so all of these are good see here's the thing bird has led to so much of activity, these transformers, that there are many, many variations of it, each good in its own way, for its own specific task. Hugging Face has created the Distilled BERT. They believe that it's much simpler. intention of being much simpler than BERT. BERT is like big huge thing right then there are all of these things long format this and that Roberto this that there's so many variations of BERT and not to mention BART right so for example for text classification and so forth there is BART so which one do we use it depends upon trying it out remember the the no-freelance theorem. You first try it out, then you see, and all of them are based on the same architecture, but with different optimizations and different trainings. And you try it out, and then that is the cookie cutter model, you know, pre-trained models. And then, of course, relax the last few layers and retrain it on the data, depending upon how much data you have. You know, suppose you have a billion rows of data. Now you have the luxury and a lot of hardware available to you. You can, in a month, you can spend a month on that massive hardware to train a model from scratch, but most of us won't have that luxury. So all we can do is use a pre-trained model or relax the last few layers and retrain it based on the available data. Let's say that you have a data for 100,000 items or 1 million items and label data, which is very expensive to create. And then you can now fine tune it on that. Or even fine tuning you can do with even smaller and smaller data. Just train the very, very last few layers. That's all just disable the gradient descent and weight updates when you're training of all the other layers and that is see those are the deeper technical details we haven't gone into right so guys uh now i'd like to spend the next 20 minutes on something before i open it to question 15 minutes. See guys, we have covered various neural architectures. We covered the foundations. Three weeks we covered the foundation ideas, the back propagation, the gradient descent. Isn't it? All these foundational ideas we have done, we learned to create the dense neural network, we learned batch norm and all of these things right then we did congulational neural nets and now recurrent i mean sorry natural language processing this week next week we'll do gans and geometric networks and so forth and i'll try to give you a little bit more and in the interest of those people who absolutely want to stop with the foundations I want to make sure that you have a pretty good panoptic view or like sort of a broad view of the subject after that we'll start going in depth but there is one thing that I have not covered it is all right to train a model but how do you take a model to prediction a production how do you take a model to production? How do you build an application, a web application or a mobile app that is sitting on top of your models and it is using your model for inference? I would like that to be your final lab. I will take a very simple example. I will deliberately write most of the code because it with letting you adapt to it, play with it and so forth. But I want you guys to get a taste of what it is to create an end-to-end AI pipeline. Would you guys be interested in that or should we in the next lab focus on the topic of the week? Interested. But the compromise would be that then you won't get much of a lab in in things like the GANs and the the geometric neural nets, the graph neural nets. See most of you are going to continue with you. Is there anybody here who is not going to continue to the rest of it? Everybody is, right? Then I feel much better. I want you guys to get started on the big picture because fundamentals was the big picture. A big picture would not be complete till I show you how to take everything to production and how to have an end-to to end application in a pipeline. I would like to focus on that time permitting I will make the homework on GAN and geometric and make it available. If you get time, do it. Otherwise, if it looks like overload. Remember, we are going to do these things in depth in the part two, part three. That is, that is one thing to mention. The other thing to mention is the reinforcement learning seems to have gotten enough number of votes. So right after this workshop, we'll do a very small workshop on reinforcement learning. But we'll first finish part two, part three, and then somewhere in January, I'll do that. Now, the other thing I wanted to say is that guys, and especially those of you who have been doing ML 100, 200 with me, remember that all those quizzes are available and the sum total, they're already coming to close to 400 questions now to practice on. For what it is worth, as you get ready for your interviews in January, do please practice it. And the other thing I will start doing sometime later is I will release every week two, three data sets and problem sets, simple ones. I'll make sure that the total effort needed to do or solve those data sets is no more than one or two hours. And I won't tell how to do it. And it is for you guys to figure out how to achieve the end goal. The total solution would be like, let's say, 50 lines or 100 lines at most. And would you guys be interested in that? Every week, just as we discuss the quiz, we'll discuss the solutions to those datasets, those prediction models. Yes, sir. Not many people are saying yes. Okay. So maybe you can create a poll. One of you, could you please create a poll? How much interest there is in that in Slack? I will start doing that hopefully in a week or two in the beginning it will be very simple data sets you know just toy data sets as a warm-up exercise the point is that as you go and look for jobs i want you guys to hit the ground running you know that any kind of data set you can immediately have a systematic approach to handle it and take care of it in your workplace usually work in the workplace is easier than working the interviews if to the question that you asked about would you like would we like the data sets my answer is resounding yes but i But I think what I'm speaking for myself here, and I think there are a few more of like me in the group is what I feel. I still haven't done these homeworks rigorously, partly running into a bunch of things. So I think we need a Saturday where we kind of clear the path in terms of full speed being able to execute through the homework pieces in a way we feel very confident until that time when we take these data sets and play with it I think the total productivity is not at least for me it's not something that I feel happy about for myself thanks for that valuable and honest feedback so guys I'll ask you a question. Are you reading the lab books in conjunction with these lectures? Are you getting time to read those? It is important that you read those lab books. What were you saying, Anil? Yeah, I'm reading the lab book. Yeah. I'm a little behind. I need to catch up. You do need to see what happens when we have discussed something in the class. Material is fresh in your mind. If you attack it in the book, it's easier to read. But the further away in time you are from that topic, the slightly harder it gets to read the book. Yep. So you want to review it pretty soon. So I said, you'll be releasing the solution for a housing data set. Are you reviewing it today or today I'm too tired. As you can see, I had a rather tough long day. Okay, so did you release the solution or it'll be later? I'll do it over the weekend at some point. I don't know how many people would be interested in that. See all of these optional things I'll do over the weekends and so forth. See, the housing solution would be far more interesting to people who have done ML 200 and have gone through the whole exercise of the California housing data set, isn't it? So let's take it over the weekend. At this moment, I want to focus on making sure that you guys all have a broad, if we have official time, I want to make sure that we use it for getting a broad picture of the subject and the fundamentals. Once fundamental is over, then we'll take it up. Also, I noticed the poll of whether we have a one-week gap or not. So the way the poll goes, we won't have a one-week gap between part one and part two because not enough people voted for it. Oh, where is the poll? I didn't see the poll. It is there in our Slack. voted for it. Oh, where is the poll? I didn't see the poll. It is there in our Slack. Look for polls. There are only two polls at this moment active. One was for reinforcement learning and one was for whether to have a gap of one week. More than one week, don't even ask for it guys, because there are many people here who are looking to change their jobs in January. So for them, in fairness to them, we need to make sure that everybody is up and running on their feet by January. Asif, I am not able to find it. So I thought, Asif, we were told that because in the last lecture, not many people raised their hands or said that they wanted one week. That poll was invalid. Yes. So let's consider that. By the way, how many of you would really, so here's the thing. Dennis went ahead and created the poll. So I thought it's all right. I'll respect the poll once it has been created. Yes, I do remember making a decision. But since after that, one of you created a poll anyway, I'll respect that that i'll honor that but even with that honoring that i don't see enough backing for a one week gap yeah i think after that someone said that this is not a valid poll so people might not oh okay no so it is a valid poll if all of you come back and say we need one week to catch up catch our breath and catch up on our homeworks i will do that because it's no use but keep moving ahead and if you guys have fallen really behind when we catch up it does make sense uh yeah i think when we catch up it will make sense like but no more than one week guys no more than one week one week i think that's good like a week yeah and uh well actually voting for it even now I believe there are nine votes in favor I just saw it yeah nine out of 26 is still a small number of votes but there are only a few against it so it looks like 15 people have voted Asif where can I see this in an accident call because I am not Actually could one of you Prachin I pinned it to the channel just now. You pinned it to the channel. Good. See, here's the thing. Let's make it even a bit harder. We need a super majority. Yes, Satya. I think everyone needs to vote. That's the thing. Voting is important. Absolutely. Anil sir, please change your vote. Why don't we take a poll right now because everybody's in the class, 26 people. Yeah, I can raise my hand. Click on it, guys. One more thing, I want to use the last 15 minutes. Usually we don't get time. I haven't had time to sit and create a proper survey to see how you guys are doing and how much you're liking, which if anybody has any feedback to give uh so far we are into five weeks now uh please do let me know are you guys finding it useful anybody has a feedback absolutely you are right good peaceful just uh difficult to keep track on exactly what the homework is for which file we download that is true that is true I need to write that down that is on me on the lab channel labs and homework seem sensible on that slack channel I'll do that so I said I have a question. I was interested. So, so far, you know, I've attended a few workshops and I feel, I think one thing which might help at least me is, you know, say working on a project and have like a completed project on GitHub or something just as a proof of work. That is exactly what part two, part three are Nisarg. You remember the bootcamp you took in October? It is along those lines. Nisargadottya Yeah. But in that bootcamp, I felt like we were going from, although we dedicated one or two weeks to one topic, we are still moving pretty fast from a topic point of view to complete a full project. And I feel like if someone is really interested in a particular topic and they dedicate more time to that particular topic, at least they will have a end to end project in one topic. Rajinder Balaraman I will tell you how we are planned to do it. okay this is like see the way i'll tell you how we are planned to do it let's take computer vision part there will be some basic lab so to make sure that you have all the fundamentals there image segmentation etc then there are two three labs the the real projects for example the lung cancer the dicomICOM data, right, medical data. We are going to analyze that. When you do that, by the time you're done, it's a fairly massive project. So it won't be that every week you'll get a new dataset. You will carry the same dataset forward pretty much through the workshop. Are we together? Means you will be working on different aspects of that image processing given the data set. So end result will be, you will have completed a project in which you did many, many exercises on that data. And they will, I mean, if you think from a perspective of posting a blog, a medium article or a giggle, it will be an impressive and long one long notebook so so i was coming from more from like my own perspective i'm more interested in working on time series and i was just wondering if there's one project i can focus on and try to apply say rnns or LSTNs or GRUs. Absolutely. See, remember there is one thing that I've been saying for the project, right? Bring your own data, right? BYOD. So bring you, if you have access to data that you would like to analyze, make it the context of your time series. When we do that, right? So, Asif, is there a place where uh or any known big data data source like in oh there are so many like amazon maintains a curated list of available data sets kegel maintains kegel's number is 35 000 data sets now then uh google and google maintains it yeah google does it and there are many many websites which are trying to they're all curating the best data sets and they're just begging for data scientists to review it if you go to most of the data sets that are available in cable there is somewhere between zero and one notebook and the notebook that is there is like two elementary so there is such a vast opportunity of taking the data set and doing a good analysis of it because the guy who contributed the data is hoping somebody would analyze it I see a data.gov is also Kaggle competition if that's the case you know? Yes you can go you guys can pick up that is an excellent thing pick up a see there is one thing that we haven't started on the group projects right now in the fundamentals it was not necessary but the next two next four months right is the months where you should do group projects and also the lab work that is there the projects that are there or the the assignments homeworks that are there for part two part three i don't want to scare you guys but just a basic thing you won't be able to do it alone unless you are willing to commit a huge amount of time to it the only way you can do it along with your work is if you form groups, steady groups, and do it in teams. Because they are team projects. I will expect an application, end-to-end application in production in the cloud, fully trained and working. As we go into that. you getting that so one of the things that we haven't done is formed our partners formed our friendship networks slack I you're in the slack now I do form your teams and as you form your teams let me know the name of the team come up with a nice inspirational name for your team and the team members. All right. And then we'll create private channels for you in Slack, which is only for your team members. Because some of these things, like for example, the COVID data analysis of that in the neural network or the automated machine learning ai and so those are a big projects and secondly the training those neural networks will take a gazillion amount of time and when you train it in the google cloud assuming that you don't have powerful machines to train it locally surely you want to spread the cost between four or five members you don't want to foot the entire bill are you getting the sense of the reality we are heading into guys so form your teams form your teams this fundamentals was okay we had easy labs but this is not real life and by the time you get out in January I want to make sure that you're in a position to scare any interviewer who manages to interview in data science you should be absolute uh stallions racehorses in the field as if yes where's the fundamentals class uh regarding homeworks and all that um because for the people who are not too familiar yet with the coding behind a lot of these libraries, how much, how proficient are you expecting us to be upon entering the next, the deeper ones? Because I think the problem is you don't know, we don't know how deep we gotta get into the syntax and the variations of the code. See, I'd say Patrick, what it is is,'ve seen your code so you're ready simply put you did a pretty good uh notebook on covet for philippines and in my view you're absolutely ready so i wouldn't worry about it but if you have not been doing the deep learning parts it's high time to catch up right the homeworks, the code, you see all the code that I've given. One easy metric is, are you familiar with all the code that I've given you? And are you comfortable with it by now? Because that is the template for most. If you look at it, any project you go into, more or less that code that I've given you is the template. If you have fully understood it and can reason through that code, you're ready. I understand the intuition, Sif, but let's say I want to build it for myself to make, to produce it a certain specific way. Does Stack Overflow also help or like, is of these places are they're very good Cora and so forth there's certainly good resources to use please be a little take it always with a pinch of salt because they're good solutions and they're brilliant solutions and they're lousy solutions there having said that Patrick here's a thing just go through the code that I have released, for example, homework five, go into the SV learn directory and see all the Python code. Can you understand it completely? If you can, it is fine. You don't have to be able to write code like that. That comes through practice, Patrick. There's nothing great about that code. It's just a code that any experienced guy would write. But and so the very fact that you're learning the field doesn't mean that you have to write like that. It will come very easily. In a few months, it'll easily come to you. Come January, you'll be quite, quite conversant at all this. See, here, this field is not about coding. It's about thinking, knowing the concepts and from concepts reasoning down to code. All right guys, so that's all I have. If anybody has any questions, please ask me otherwise. Thank you.