 ðŸŽµ Through the river, through this ancient city goes a river. Like all great civilizations, it's remarkable that most civilizations are founded next to large water bodies, through rivers, through oceans, through lakes and so forth. Chicago is next to Lake Michigan, I suppose, and so on and so forth. We all live next to the sea. So this ancient city is next to a famous river that in the West we call the Ganges and in India we just call it Ganga. And many of you obviously have visited it or had a dip in it and so forth. It's considered of great relevance. So this particular city has houses, let us say, houses. I'll just mark the houses with the triangles on one side of the river, on the river bank. In fact, if you Google up any picture of Bonares, you'll see something quite remarkable. One side is absolutely densely populated with houses. They are called the ghats. The one is the ghats. And densely populated with lots of buildings. And even back down, it is all buildings, buildings, buildings, buildings, and more and more buildings. So it's just this area you can say roughly speaking is like a non-agricultural like they are housing of some form housing or commercial or something like that everything is here people live here in other words this site is the let me use a more appropriate term this site is the population center on the other hand, remarkably, as you can imagine, population center tend to be expensive real estate. That's the lesson we learned from the California housing in our last lab. And of course, we know it from practical experience. On the other side, you find trees and just plain land and agriculture. So I'll just draw some trees but actually not even trees they're shrubs or just fields agriculture we culture this site is agriculture culture and terrible spelling but okay agriculture and this forms the context of our setting the stage for understanding SVMs so the problem statement is this if you pick a geographical point point X So this is your X1 and X2, let us say. You pick some point and let us say that you pick a point either here, here, here. You need to determine what is it? Is it a population center or is it agriculture agriculture land or are you in the middle of the river. Right. That is the so does it look like a classification problem, guys. Will treat it as a binary classifier will say, can I build a house here or do I grow food here. The question is, I will say, can I build a house here or do I grow food here? The question is, build or find a house here, a house here, or is it agricultural land? Land. That is a question. So it's a binary question. So we are going to classify between house and agriculture. This is our target space, isn't it? Let me write it in the way that we write the categorical variable. The space is made up of house, ag, agree, these two things and we have to create a classifier that will take given X we have to do a classifier FX that will be that will take you to either a house or a agriculture land so does this look like a typical classification problem guys yes right yeah yes so by the way there's a little bit of a So does this look like a typical classification problem, guys? Yes. Right? Yeah. Yes. So by the way, there's a little bit of a curiosity as a physicist, this may invite you, that it is a remarkable fact that if you look at rivers throughout history, you always find civilization or population centers only on one side of the river, not on the other. Well, things have changed. For example, if you go to the river Thames in London, you do notice that there are embankments now and there's equal population centers on both sides. So the river seems to be going through a population center. But without those human engineering efforts, if you look traditionally as how rivers have been and civilizations have related to it, you always find population centers on one side and agriculture on the other, or your plain land on the other. Now, I'll pose you guys a puzzle. Why do you think this is true? It has nothing to do with machine learning, by the way. But I don't know if you ever observed while you're traveling throughout the world that population centers tend to be on only one side of the river. Unless it had- Asif, I think one of the reason is the floods. Yeah, because I live on the other side like the Yes, so but why are the floods there on one side but not the other I Think it's because of the gradient or The way the water flows agent is there So one side is steep the side of the population center we steep, the side of the population center is steep. On the side of the agriculture, it will be gentle slope, right? And water easily rises up the slope. But why do rivers behave this way? One bank of a river is steep, other bank has a gradient, slow slope, and so tends to get flooded more. The rotation of the earth. It was an overman post. Come again. come again it was an overtime erosion over time sort of yes it is part of the answer okay yeah a rotation of what? Earth. Rotation of the Earth. So why does it erode in one direction? Actually, so I'll give you the answer. It's very beautiful actually. There is a force in the force that is created, a form of a centrifugal force. It is called Coriolis force. Coriolis, yeah. The Coriolis forces cause the rivers always to drift one way in the northern hemisphere. Have you noticed that most rivers tend to go from north west to south east in the northern hemisphere quite often, right? As they move, the rivers also drift. They never remain at the same place. They move over the hundreds also drift. They never remain at the same place. They move over the hundreds of years. Now, the way they drift, they drift in such a way that they have actually drifted in this direction. So when they drift in this direction, ultimately you have the embankment, but the place that they have left is used to be the bottom of the river and that is first very fertile and secondly has a gentle slope which is why one side is it's just alluvian soil for example on this river it's a fertile alluvian soil and you can do a lot of agriculture so anyway for the physicists amongst you this was a fun so today i'm taking obviously a long-winded way back to our main topic lots of digressions that's what happens when your instructor is fond of the topic lots of digressions so all right we'll come to that so now the question is once again let's get back to our river suppose your river runs like this and i'll i'll pose the problem like this imagine that it is pitch black because that's how a machine learning algorithm sees it x1 x2 right you have to find a decision boundary so that you can say one side is our houses and another side is agriculture is trees let's use the word trees even though it wouldn't be trees it would be shrubs or agriculture that how would you do that you would say that you know I am looking for the median of the river the middle part in the river right a river is here this is what I am searching for so now let's try to do that. Suppose you have houses like this and a tree like this, something like this. So think in how many ways you can flow a river. You can flow a river, for example, like this and you see that it still fulfills your criteria the green river isn't it guys you could flow a red river because you can't see the river you could also have flown a river like this now between the white River let me give this names to the river. This is A, let's say this is B river and the white river is the C river. From a data perspective, which of these seems more convincing to you? If you had it you would say why because you suspect that if you have seen this data there is likely to be a tree here that is not there in the data isn't it and I like you to be a house here sort of extra polite and convince yourself that these narrow rivers that you can flow probably are unstable solutions. You take a different sample of the data and you would be able to flow a different river through it. Isn't it? So if you had to choose which river is the best to pick, which is the best river you can flow through your data between the houses and the agriculture your best river would be what characteristic would it have compared what does C have compared to a and B broad is wide isn't it is the is the broadest is the widest river that you can flow through your data are we together right the basic intuition is if you want to be robust, take the widest river that you can flow through the data. isn't it this is our intuition intuition to have a stable good solution is that right guys is this intuitively obvious what I'm saying? So ponder over it for a moment and see, would you agree with that intuition? Any questions? Is it more or less obvious? I hope so. So this, well, you don't in machine learning, you wouldn't be taken too seriously if you say in a professional environment that I'm trying to flow a wide river through the data. So it needs a more technical or sort of a more math-y formal language. And the formal language you say you want to draw, you are searching for so these banks of the river are you are searching for the maximal margin actually let me write it in capital because it's a term that you should remember maximal margin hyper plane hyper planes so this river this bank of the river let me just call it bank of the river one. Bank one of the river. And this would be, this thing would be bank two of the river. A river has two banks. So these are the two banks of the river. So you call them a bank is a hyperplane. Well in our case it is just a line. A line is a hyperplane using more formal language because in higher dimensions that's what it would be. So far so good guys? So do you see how this translates to this? You are searching for those river banks that gives you the wid this translates to this? You're searching for those riverbanks that gives you the widest rivers, right? A widest river to flow through the data. Any questions and then then the decision boundary the decision boundary is simply the median The Median Hyperplane. Median part Or hyperplane. Between the two riverbanks or if you want to use formal language is basically the path so if you think of yourself as a boatman a little boat so one intuition that I can think of is imagine that you are this little guy and the boat and you are rowing so you want to hit neither the tree nor the houses late at night you you don't want to get stuck either on this side or this side what path will you take the path that the boatman will row is the decision boundary is it looking simple and intuitive guys suffer so when you do that what we have done is, we have taken the first great milestone on our journey of understanding support vector machines. This particular algorithm is called literally by its name, maximal margin classifier. I hope the name is suggestive of what it tries to do. What does the maximal margin hyperclassifier try to do? It searches for the two hyperplanes that are as, that a river as wide as possible through the data. Clear intuition guys? Right. Now, here is the thing. Sometimes people confuse and say, these are support, this algorithm is support vector machines. No, no, no. It is the first step in the journey towards support vector machines, but it is a little a little gets a little bit more complicated these houses that help you know that are hugging the and the trees that are hugging the hyperplanes so the river banks the houses and trees banks are called, what are they called? They are called support vectors. Are we together? imagine, think of them as lighthouses. Houses for helping the boatman, the boat, navigate. So in the sea we have these lighthouses along the coast that give you a sense of how to navigate, get closer to land. In the same way think of these as lighthouses except that on a river or if you go to the city you will find that there are no real lautaises. It would be the light shining in people's homes or what people do in this city is something actually pretty poetic. Every evening in the water you find this beautiful little earthen lamps made in a terracotta baked clay lamps with lights on them and this practice of putting these little lamps into the water goes back thousands of years. It is really evocative whenever I think of my hometown. Vaidhyanathan Ramamurthy, With a picture of these beautiful lamps in the water comes so well. Here we go. Think of these as light lighthouses or lamps. Vaidhyanathan Ramamurthy, In the water. Vaidhyanathan Ramamurthy, Or lamps. Vaidhyanathan in the water or lamps in the water and the water edge are we together so do you see that so far the intuition is very clear if you look at this picture you realize that if you are in pitch dark and you are a boatsman and you see let us say that you see one light here and you see let us say that you see one light here. And you see, let us say that you see another light here. And you see a light here on the other bank of the river. Right? You know that this side is agriculture. Agree? Maybe the agriculture lights are green in color and these are more like more like amber in color. Sajeevan G. Or something like that yellowish in color. Sajeevan G. I'm just taking distinguishing marks. Right. So they stands for houses house house you know that this is a house and you know that this is a tree or a shirt. Sajeevan G. Can you as you imagine that you're in pitch darkness? What is the river that you can infer as a boatsman? Would you agree that the river that you would infer would be something like, let me take the color of water. The river that you would envisage would be essentially this, isn't it? Where this data point and this data point and this data point are sitting. These are your support vectors. How many support vectors? You realize that you just need three points to draw your river and therefore you need only three data points to draw out your decision boundary your decision boundary is simply the part that the boatsman takes flying through the river so ponder over it for a moment guys make sure you get the intuition the geometric intuition here is it looking easy so far imagine you're in pitch dark and you see this maybe i'll color these differently a yellow light these are yellow lights and these are green lights you don't do you think that this is enough for you to be able to draw your maximal margin hyperplanes, or simply put, discover the banks of the river and then ply your way through the center? Little bit of thought, anybody who disagrees or thinks this needs clarification? clarification any thoughts guys all right so if it is clear to all of everything go ahead were you asking something yeah I was saying like if there is a bend in the reward then no so we are going there it's a very good part hold that in your mind at this moment we are going there. It's a very good thought. Hold that in your mind. At this moment, we are looking at a very simple river, a segment of the river, and this is it, a straight part of the river. Okay. So then, but we'll come to the more complicated situation. That's why I said it's a first milestone, right? But in a straight river, this is enough to do that. And so that brings us to some very remarkable conclusions. This algorithm, which color should I use to emphasize the beauty? Maybe I'll use this. algorithm is robust and this is something that startles you when you see that it's obvious robust against outliers isn't it so for example suppose there are houses way out here and there are green things way out here do you think they matter? All of these things far away from the decision boundary? Do they really matter? They don't matter, isn't it? This house and this tree, they do not influence these outliers, right? As we can see, outliers are almost irrelevant, are practically irrelevant, relevant, terrible, relevant to went to the discovery, to the learning of the of the river path. In fact, it is the inline data that matters most. That is one startling observation. The other startling observation that you have is the model after that becomes so simple. Do you realize it is a shockingly simple model? Is a shocking model. Just a list of support vectors. So let's explain this word support vectors. So let's explain this word, support vectors. See, this is a point of data. This is a point in the feature space, isn't it, guys? So suppose I put a coordinate system here, X1, X2. You would agree that this represents a point. This represents point X, let me just call it XA, a vector. This represents a point XB vector. And this represents a point XC vector. Every point in a Cartesian space is just a vector, isn't it? It can be written as vectors. So the word vector is therefore understood in this context. The next question is what about support? You're saying that you need only three vectors to support your decision to flow the river like this. Isn't it? Your decision boundary is supported or proven with just these three vectors just a small just a very small list of list of support vectors so you may have a data set that has thousands and thousands or tens of thousands of points, but your support vectors may be a very small list of, you know, a handful of points along the riverbank. Asif. Yes. If your green support vector was an actual outlier. You mean this one? No, no, no, you're not an outlier. So the assumption is that around the green point, there are lots of other green points. Right. But if it's very far out, let's say. Yeah, it is. See, outlier is invariant of the classifier you use. Outliers, outliers. And you can discover outliers by the usual manner of outlier detection. So detecting outlier is easy. The hard part is that when you have outliers in your data, how do you make your algorithm not be hijacked by outliers? Now, if you remember for linear methods, outliers were pretty nasty, right? They would screw up the model and they would do all this outlier search and so forth, make sure outliers are not there. But one nice thing about support vector machines is they are very robust against outliers. They couldn't care less because they are searching or not being influenced by outliers.'re rejecting the outliers they are actually searching for those high value inline points a b c right that will help them support their decision boundary and points are vectors in cartesian space and thus the word support vectors support vectors. Do you see that guys? So what we just said, go ahead. I'll say just one question. So that the boatman rowing there, so that's the decision boundary, right? Now, if let us say pick a point between that line which the boatman is rowing and that other blue line you know the one on the right yes that is still not uh you know the what is that called the green green area it is it is in the river so how do we avoid that problem no so let me try to parse what you said. So there are two riverbanks, right? This is the bank, let me do this here to the left and to the right. And the boat has discovered what the two is. So what we will do is it will first draw this line. To draw a line, you need two points in a Cartesian space, isn't it? On a hyperplane. You need these two points. You draw the first line. Now you have, okay. Yeah, this is an important thing I should mention. The other plane has to be parallel to this. So what will you do? You will start with drawing little planes parallel to this and see how far you can go till you hit a green tree and then you'll stop. Right? So you will stop here. The first green tree you make, you stop expanding your river and you say, now i need to stop because now i've hit land isn't it and so the boat can see that and therefore it will draw these two big blue lines and it will take a path midway through this which is the decision boundary so but to decide whether it is a it is a house a tree, it has to take that whole margin into consideration, not just that line, right? Yes, it has to take the whole margin. So what it does is, see, this is given data to you, this is training data. The light, the houses are burning yellow, let's say the trees are burning green, the light in the trees is green, let us say. yellow, let's say the trees are burning green, the light in the trees is green, let us say. And so they are well there, you're trying to learn from the data. So you're trying to flow the widest river you can through this data. So how do you do that? Learning is always a iterative process. You flow a river through the data and then you try to make the river optimal the widest possible so that is the implementation of the support vector machines and support victim machines when they came out they were notoriously expensive to implement computationally they were so the theory is very simple but when you try to implement it it gets rather rather intensive computation because in a plane you can imagine so many rivers that you can flow through it. For example, you could have one this from this Obviously that wouldn't have been a good river. It's not the maximum margin hyperplane river, the widest river that you can flow through the data. Are we together, guys. I believe you got that? Is your question answered, Abhijit? Yeah. So guys, let us recap what we learned because this is a foundation. First of all, the intuition is we want to flow the widest river we can through the data because that would be a good solution. It would be, it would have low variance. In other words, if I take a different sample of the data, I shouldn't end up with a different river. And for that, try to find the widest. So more technically, so in my mind, whenever I think of support vector machines, I always think of river and the widest river you can flow. But that's not that that is a language unique to me. So don't open a textbook and expect words like river and riverbanks. They will use more formal terms. They'll talk about maximal margin hyperplanes and the decision boundary will be the path of the boatsman. That's how they call it. So this classifier is called maximal margin classifier. It is not the support vector machines algorithm, which is, we're going towards it. But confusingly, these points that help us make the model, so the model is just the collection of points on the riverbank. Those are called support vectors. They are vectors in a Cartesian space and they support the decision boundary. Right? So that is that. Now there are two problems that I have a question. Come again. I have a question. So they support vectors like three support vectors here, right? So, but they belong to two different categories of data yes right and the finding the widest river is basically finding the maximum gap maximum separator which separates the data of two different categories that is true okay and that's why it is computationally intensive because we can start with any river, but finding the widest one is the computation. Yeah. Actually, the linear part is not that hard. We can quickly get to it. But when I get to the full support vector machines, it gets harder. Even this takes some effort. But as you move forward to the more complicated one, the full support vector machines, then it begins to get hairy and you'll see why. Sir, but what if there are multiple categories, like basically multi-class class data? For the time being. So that is one limitation of what I've just explained. I will tell you how support vector machines overcome that. But fundamentally the theory of support vector machines very much like logistic regression is inherently a binary classifier. Blueberries versus grapes and so forth. That kind of a thing. So here, let us think as a binary classifier. So it's good that you mentioned that. And we will see how these things are generalized or harnessed or any binary classifier can be harnessed to do multi-class classification. Got it. Thank you, sir. So far so good, guys. Now what we do is, if we look at the limitations of it, and there are many limitations. One is that you can only tell between houses and trees. So binary is one aspect of it, but that can be easily addressed. Let's look at more complicated aspect of it. If you have lived next to the river and seen that rivers flood during rainy season, especially in the tropics and nowadays,, in the temperate regions also, we know about the floods in Michigan and so forth. So rivers flood, they overflow their banks and complicated things happen. So then let us say what happens when rivers flood. You find that they drag in, and again, I'm giving this narrative to just explain the concepts so they will they will wash away a few trees isn't it guys you will see a few trees washed away into the river so then the question comes suppose you just see two trees here but most of your trees are here most of the things are here what should you do would it make sense and this is a question i'll pose to you to redraw the decision boundary and say well you know uh fact of the matter is i'll use a different color here which color should i use let's use this color it It's a pretty multicolored thing. And should this become your new river? Do you see this thing? Should this become the new river? I'll ask this question. Oh, this is really multicolored. Should this be the new river bank? Basic intuition says that no, you know. But sometimes you might find a tree or two in the river that is perfectly normal the river is wider its boundary shouldn't be determined by the first tree that you hit upon right in other words you should allow for mistakes and this becomes even more apparent if for example you realize that this particular green tree could easily have washed its way and come here right gone to towards the other side but then you get into even more problem because if you try to draw your river where is that i draw your river but now your river is beginning to look rather narrow do you see you're just left with this little bit. And your basic intuition says that this one point, this one tree that has washed into the river has essentially hijacked your model. Isn't it? You somehow don't want it to make the decision of where the bank of the river is. Would you like to agree with that? Is that intuition clear? Would you like to discuss? It is clear right? And those of you, I mean if you come to for example Benares, you see it happens in the rainy season, Anil, isn't it? Yes. And actually there's more complication if you are there. In cities like in Benares, there is such a thing as a houseboat. What it means is there will be little houses like structures but they are actually sitting upon a boat, right? So they are triangles sitting in the water. Once again, and this triangle may, you know, this guy would have gone to the other side of the, well, now this picture is getting rather busy. This, a few of these houseboats are floating around on this or that side of the river. And once again, you don't want just a few of these to determine, to make big decisions, you know. Ultimately, this is the population center. And just a few isolated points shouldn't make the decision. So what you do, and by the way, I'm explaining all this without mathematics. We'll go into the mathematics now. So one thing you would agree that perhaps, perhaps, should we should allow for a few contradictions or mistakes. Would you agree? Don't make your decision bound. Don't agree on your riverbanks just because you made one or two points. Meet sufficiently many points before you're sure that you have hit the riverbank with the population on it. Likewise, on the other side, meet enough trees before you decide that, OK, now we are talking of the other riverbank on the agricultural side. Are we together, guys? We are being more permissive. Otherwise, a river would not be wide enough or would not even exist. For example, with this tree here and this houseboat and this houseboat here, you realize that it's practically impossible to find any decent decision boundary, linear decision boundary, right? You will end up with a no solution situation. So you need to be a bit permissive. So we need permissiveness here, permissive. But then how permissive? It can go a bit too far, right? You keep meeting house after house after house, and you can't just keep saying, no, I'm still not in the bank. I'm still not in the bank. You realize that that becomes rather counterproductive. At some point you have to accept the reality and say, well, now we have hit the river bank. Is that common sense? So we need a certain threshold, right? And to do that, what you do is you set up a cost. You set up a penalty. So the two steps you do is for each mistake, you set up a penalty. Epsilon I. Suppose it is the ith point, you set up the penalty and then you put a constraint saying that sum of all penalties has to be the sum. Okay. I use epsilon for, let me, because the symbols have been to look rather surprisingly similar. Epsilon I, the summation over I of the epsilon I, in other words, the sum total of all the penalty C is equal to this and constraint that you want to, this is sort of like a cost. Total cost of penalties. You can afford right cost of penalties and then you have a budget then budgeted penalties then have a budget. Say that you can afford only C greater than or less than equal to some particular value. All right. Let me just say, C, well, let me just say, this is a, typically the word C itself is used. So let me use little c here, little cost, C. This is common. So you can say that your little c has to be the budget. Why don't I use the word budget? People use the word see in the literature. So I was using see Let me use the word some, some of all penalties, right, some of all penalties. So then the sum of all penalties must be little less than your budget. So the language actually in the support vector literature is a bit at divergence with what I say, they will call this C as the cost. But intuitively, it is more the budget for mistakes that you're allowing so that the sum total of your mistakes or the penalties do not exceed the budget you have. So? So I suppose this is the trick here. I mean, it's one way of writing it. And I used to find it a little worrying in the beginning when I was trying to teach myself this thing years ago. So we came across our first constraint. We should do this. Cost of penalties is this. And this is in the language of mathematics. You call it a constraint. This is a constraint. And what we are going to do in the, let's take a little break and then we are going to learn that this entire problem, this entire problem is a classic is a classic constrained optimization problem. Constrained optimization problem. By the way, guys, do not be scared with these big scary words. There's nothing to it. These are very simple, very obvious words. And when we develop the theory, you will realize that actually this is just common sense written out in symbols and we'll do that. So, so far so good. Before we take the break, are we in, can we review and make sure that we understand maximal margin hyperplanes? Maximal margin hyperplanes is the search for the widest river that you can flow through the data or search for those two hyperplanes that will give you a maximal separation between the two classes, houses and let's say trees. The decision boundary will be the median going between the two hyperplanes and so is itself a hyperplane. One good quality about the support vector, this maximal margin hyperplanes is, it is really this class of algorithms is very robust against outliers. Outliers become irrelevant actually. Very, very nice. The other thing that's nice is the model is so simple you just have to tell which are the support vectors that help you that make the decision boundary or you know which vectors are points points are vectors which points list of points basically that specify uniquely the decision boundary or and the hyper planes the river banks you can say it in whichever manner that you want to, but that's about it. That's the gist of it. Now, the trouble with this kind of a thing is real life situation is data has noise, it has mistakes. And so you don't want to be hijacked by all these mistakes and noises. you want to keep a you want to define a penalty term for each mistake and then sum all the mistakes and then say that you have a certain budget for mistakes you should not exceed that budget otherwise your solution will become either very hard to find or very unstable you take another sample of the data and your river will look a different way in other words it wouldn't be wide enough in simple intuition terms are we together so therefore we have a constraint all of machine learning problem is optimization problem in the sense that you take a loss function you decrease it and so forth that's a mathematical language in which we encounter all this intuition so in the let's take a little break and after that we'll come to that by developing the mathematical language was I said you need to just summarize it again I will just go back to the previous screen for one second and repeat what I said. So what I said is that if you have a decision boundary between the blueberries and the cherries, here you see it with this greenish line, then how do you decide if a certain point is representing a cherry or a blueberry? The intuition is that if you take this orthonormal vector W, the blue hat, and you say that the further along you are in this direction, based on your distance from the decision boundary, this distance being positive, this distance being negative, based on how far you are from the decision boundary, you can tell whether it's a cherry or a blueberry. A cherry, if you are far, a huge positive distance, very sure there's a blueberry. If it is a huge negative distance on this side. And for small distance, there's a certain degree of uncertainty. So that brings up the usefulness of a distance function. How distant is a point from a decision boundary? So we go through a derivation and we find that the distance function can be represented as this term. It is a dot product of the unit orthonormal vector, W, with the point itself, along with a bias term w naught the w naught is actually the negative of the distance from the origin to the hyperplane so minor the distance needs to be moved to go through the origin. So, you have to move minus w naught distance. In other words, in this direction for the hyperplane to now become still remain parallel to itself, but go through the origin. That's when we have thinking. But the net result is this, the distance from the hyperplane of a point is given by this term. Two factors are affected affected the orthonormal vector of the hyper plane and how far the hyper plane is from the origin right now the orthonormal vector why do we give it in terms of orthonormal vector hyper plane's definition and is distance from the origin because it is something that works for higher dimension. For a line you can go with the slope but what do you talk about when you have a hyperplane in high dimension? It turns out that you could actually use every hyperplane in whatever dimension has an orthonormal vector and every hyperplane will be a certain distance from the origin and these two things uniquely define a hyperplane. So we will take this results, bring it back to our current pursuit and see how we can quantify all this intuition of flowing a river through the data into a more mathematical language. Are we together guys? So here we go. So let me bring back that intuition. I'm saying that let me draw the in the same way that it was drawn here. Suppose this is a decision boundary. Boundary. We are seeing that a given point X, X, the, and let's say that this is the origin and from the origin, you have a, the shortest path from the origin to this is, vector here is W hat, unit vector. Actually, I will use the notation that is in your book, in the book, textbook, they tend to use the word beta naught, beta rather than W. So I'll just, omega rather than, so I'll use beta here, unit vector. What is a unit vector? Unit vector is a vector whose size is equal to, what is the magnitude of a unit vector? One. So that brings us to our first constraint. It is a constraint on the, the constraint on the betas, because a beta is nothing but beta 1 beta 2 depending upon how many dimensions there are suppose they are d dimensions uh let me just say n dimensions uh and a dimension then there will be an orthonormal vector in the n dimension space where we are talking about let us say r to the pi, the R dimension, Cartesian space of n dimensions. And so we came to the conclusion that if you are actually, let me take a point a little bit here, more x is here. We are seeing that from this point, if you want this distance, this is the distance of x from the decision boundary. This is given by the result that we had. Distance of a point from the decision boundary is beta naught plus beta hat dot x. Isn't it? The distance function. And if you understand that we have gone most of the journey in now formulating this problem as a, this river problem, flowing the widest river through the data, let us say that, let me bring back your yellow points, a few yellow points, and a green point here. Where's my green point? I'm going to get rid of this. Green point here. Right, so suppose we flow a river through this. This is our decision boundary. through this this is our decision boundary are we together now you ask this question this vector or this vector or this vector you realize that they are So, each of the points, vectors, under banks is the same distance, let's say m, let me call this distance m. Would you agree that all of them would be the same distance m from the decision boundary? Right? So this is your boat, right? Reminds both right it makes me a Greek mythology in which you are between what is it a rock in a hard place it used to be psycho or something like that anybody remembers the Greek mythology those two monsters in the sea who were waiting to devour you and your ship and you have to silence that is right thank you so it's like that you know you want to avoid both the banks otherwise you'll capsize your boat you want to go through this so you'll be equally distant from both of them so you agree that all this so from this we derive that the support vectors in the simple case the margin the points case the margin the points on the margin margin are given by D effects is equal to M are we together this this is very obvious isn't it because this distance is after all D of X here distance from the hyperplane this looks obvious isn't it guys please tell me that it is. Yes. Let me say that this, the direction of this river, which direction that you're flowing it, is given again by an orthonormal vector, a beta hat. So our other constraint is beta hat size is equal to one, which means that beta one square plus beta two square, beta n square is equal to one right this is our constraint number two right and now let us be a little bit more clever let us say that we call these points the negative points houses are you know as a target variable when you give it you can say y is equal to minus one and here for trees y is equal to plus one right as a y minus one plus one why not zero one why not just house and tree you'll see it's a mathematical convenience you'll see what it is and you'll see it shortly so you realize that for trees for all tree points let me mention this as a crucial observation, for all tree points, what would be the constraint? distance let's say that this distance is positive this distance is negative DX for a tree for any tree point X tree is greater than equal to man isn't it positive yeah and at the same time distance of any house point is less than equal to m would you agree guys right they would be minus m sorry less than equal to minus this crucial minus is important right so now if i write x t as a x minus or x house as the positive distance or We don't need to do that. Now, these are two different constraints, isn't it, guys? You would say that find me a river says that this constraint is fulfilled for tree points and for house points. It's like this. So there's a clever mathematical trick. If you multiply this with the Y for tree What is the Y for tree plus 1? Right. Y tree D X 3 let us multiply now This happens to be 1 plus 1 and so this constraint will remain if you multiply this equation with plus 1 it will remain this on the other hand Y house is equal to minus 1 if you multiplied by d x house multiplied by d x house what happens when you multiply an inequality with minus one what does it have what happens to it reverses the inequality yeah the sign reverses And so this will also become like this. Do you see this, guys? And so it's just a clever mathematical trick. It's just being clever with the symbols. And so you can write it as a simpler equation, which says that for all points in the data, you want to find that decision boundary, such that irrespective of whether it is a house point or a tree point, this constraint is fulfilled. Do you realize that these two become exactly the same equation? Right? So this is our next constraint. This is without penalties. Without penalties. So if I were to write it down, let us write all the three constraints. So summarizing For an I will use the word hard Margin margin classifier for that maximal margin for maximum margin classifiers is no allowance or budget budget for mistakes, right? And so we are talking about the hard margin for hard margin classifiers that is for for the definition of hard margin classifier is these are those maximum margin classifiers where there's no allowance for mistake the opposite of that is side yes soft margin classifiers this write it in the next sentence classifiers budget forest yes are the maximal margin classifiers margin classifiers. By the way, is my handwriting fairly legible guys when you later on review a classifier where the budget C for mistakes is greater than zero. right it's not zero right it is some value it is some value that you set typically the values could be 0.1 0.01 so forth a certain amount of typically you do it in powers of 10 this budget so we are talking about hard margin. For hard margin, margin classifier, we realize that our constraints are that yi, distance xi, is greater than equal to m some m isn't it guys at the same time you you also say that the beta the beta vector beta had the unit vector is equal to one because that goes into the definition of d in other words i.e beta 1 square plus beta 2 square beta n square is equal to 1 right this is the second constraint right let me just say equation 1 this is let me just say equation one this is equation two are we together so any kind of river that you flow where these and by definition of definition as we saw dx is equal to a beta naught plus beta 1 x1 plus beta n xn, right? So I'm just trying to map it to the notation of your book, right? And so you could rewrite this whole constraint equation as saying that this thing, therefore, can be written as y i d x i can be written as greater than m is the same as is the same as writing y i beta naught plus beta 1 x 1 plus all the way to beta n x n is greater than equal to m right this is This is obvious, isn't it guys? It just comes from the definition of this. So in your textbook, actually I've mapped it now, this intuition to your textbook. So your textbook, this is by the way, chapter nine of your textbook. The reason I mapped it to that notation so that it becomes easy for you to go and review this step. Actually, this derivation is not mentioned there. And it is my sort of a crazy way of deriving it, or looking at it from a more intuitive perspective. But I hope that once you have understood it, you can read more formal books and get the intuition. So this thing you find innumerable videos and support vector machines and on maximum margin classifiers and so forth. They all talk of the orthonormal and they sort of go a little bit fast because I suppose this is understood usually by people who really care about it and they have a lot more mathematical training sometimes. But I thought I will go through the entire derivation for you here now comes the next question the soft margin classifier uh by the way uh can i uh ask you guys one thing it's 9 15 you know in 15 minutes it would be 9 30. can i take some more time today we started a bit late is that okay with all of you can we stretch yes so 10 is a hard constraint all right let me try to respect so I love to drop over 9 so now the only difference is why I remember the times the distance the distance from the decision boundary must be greater than equal to m but in some cases for some points you allow mistakes are we together so for every a point data point you have a certain penalty factor. You hope that most of the penalty factors are zero and some of them are non-zero. Are we together? But for those terms, you allow a decision boundary smaller than M. In other words, suppose there is a river here. This is a point, this is a point, this is a point. This is M. You allow for this point. This is less than M distance. Let me call this mistake q you would agree that click you the mistake point isn't it a mistake point then for q you would agree that a mistake point then for q you would agree that q is the the the q dist the q point a distance of q is less than m isn't it it is not as far this is m this is some small distance or distance that is less than m so that is why m minus some amount right so you define whatever it is the epsilon q is m minus dq this is the mistake that this guy made do you see that guys how much penalty or how much how severe is the mistake is based on this. So far so good. Can you please repeat how you wrote this one EQ is equal to. So the Q this point is Q let us say. Yes. So the distance of Q from the this line, whatever it is, it is not as much as the distance of the river banks from the center, from the decision boundary. Isn't it? So what's the DQ is it is less than M. center from the decision boundary isn't it so what's the DQ is it is less than M right and how much is the mistake there is a Q should have been here it is instead here it should have been ideally here but it is here so this is the mistake EQ that you are the penalty that you're allowing for the softness that you are allowing for got it thank you and that's what i'm saying here and so that is essentially the point that we are making here and so you allow for some mistakes and then the rest of the equations so again to come back to it what are the basic rules you want to say that for all points dxi is greater than equal to m up to some tiny mistakes that each of the points can make most points should make no mistakes some point can make but then there is also the constraint the summation of all the epsilon actually let me use the other epsilon symbol so it doesn't look like summation symbol other epsilon symbol so it doesn't look like summation symbol the sum of all the mistakes should be less than equal to a constraint isn't it guys this is and finally the one obvious statement is a beta hat is equal to one i.e beta one square plus beta two square plus beta three square, beta n squared is equal to one, right? So these three, so your root is that this is just a small adaptation over the hard margin classifier, right? This is it. So these are, so what you're saying is give me a decision boundary that respects these. In the hard margin classifier, you're saying is, give me a decision boundary that respects these in the hard margin classifier, you're saying, give me a decision boundary that respects This By definition, where are we okay that respects this right now the machine learning algorithm has to just go creep over the has to just go creep over the feature space using some clever means and gradually a flow to find the best river that it can flow which respects those conditions for those two different algorithms are we together so now we took care of mistakes and the soft margin classifier is it effective very effective guys but there is yet another problem with rivers and our model while for the simple case it works in real life rivers tend not to run straight do they So let's use that intuition. Now, let us say, but rivers. Like this, right, more formally oftentimes the decision boundary is non-linear is a non-linear, hypersurface, isn't it? I'm using fancy language, but I hope, guys, whenever you understand things, remember that if you bring intuition to mathematics, you'll realize that all of mathematics and certainly all of machine learning is saying something very obvious. We're just writing it more precisely, the obvious statements more precisely. That's why I always say that math is the easiest subject in the world. Coding is much harder than math. Everything, as I have spent my life, as you know, I went through IIT, engineering,, electronics engineering and I did theoretical physics, mathematical physics. I did a lot of math and a lot of physics. I did computer science, right, and all of that hardware, software, everything. From my experience I've never found a subject so absolutely obvious and easy as mathematics. The trouble with mathematics is as mathematics. The trouble with mathematics is how do you see it the right way? So one picture that somebody, you know, in the social media, people keep forwarding pictures. One recent picture somebody forwarded, I don't know who, interesting. It just showed that there was a picture of a big stone that mysteriously was floating in the air, big rock. It was floating in the air. And the thing is, you're puzzled. What is going on? Why is it not falling to the ground? It just is levitating on its way. And you stay puzzled for quite some time till you invert the picture. And when you invert the picture, you realize that, well, actually the picture is in the water, the rock is in the water. And what you thought of as sky was actually the water and what you thought of as the ground was actually the sky and so on and so forth. A lot depends on perspective. And mathematics is always, all of machine learning theory boils down to, if I may say so, and go on a tangent for a moment, it is all the search for that perspective which makes the problem look trivial. In mathematics, that's why mathematicians often use the, will say statements like that, this and then this, and from this we prove this, and from that, as you can see, this happens, and therefore it becomes trivial. So when they use the word trivial, they mean it. What it meant was, not trivial in a derogatory sense, but through a chain of reasoning, they made the problem equivalent to something that is self-evident, right? And that is the pursuit of machine learning. When you do that, think of it like that, that every algorithm you have to look at it in a way that makes it look obvious. Support vector machines are notoriously hard for people to understand. Most people in fact do not understand support vector machines. You can try it. If you have data scientists in your workplace, walk up to one of them and say, explain to me support vector machines. Explain how they work, explain the theory of it and you'll get blank stare. Quite often, some one or two geniuses might be around, but people find it hard. But in my view, it is hard because you don't look at it in an intuitive way. And always try to relate things to stories. Like for example, this river metaphor that I am using to carry this learning forward use something like that from your real life it helps then it becomes very real and easy to get the intuition off so we'll do that so suppose you have a non-linear decision boundary what do we bring to our rescue we bring something that we learned in ml100 remember we learned that every we recall we learned in ml100 remember we learned that every we recall that every curve can be linearized in some higher dimensional space So I have a silly analogy since I'm giving so many analogies. You know, even the most wicked person is absolutely perfect in the eyes of his or her mother. I'm pretty sure that Hitler's mother must have been very fond of him and so forth. And you can think of all sorts of villains in history. But in the eyes of the mother, the child is always perfect. So I always say in a silly analogy that every curve and therefore is generalization, a hyper surface. There exists a higher dimensional mother space in which that curve becomes linear and i'll sort of illustrate it with an example that you have seen so suppose you have a parabolic curve like this right so this is some you can write it as an equation as some beta naught plus beta 1 x plus beta 2 x squared right there is one bend in this curve. So you realize that maybe a quadratic equation would be good enough. And your high school algebra tells that indeed it should suffice if it is truly a parabolic thing. Now, how do this is two dimensional right this is our is two-dimensional, right? This is R2. How do I linearize this in quotes? You go to a new space, which is three-dimensional, right? So this is YX, which is three-dimensional, in which the axes are X, and this axis, let me just call it X1 is equal to X, X2 is equal to the square of X, let me just call it x1 is equal to x, x2 is equal to the square of x, and the third dimension is y. And you will realize that if I write it as in that form, this equation becomes y is equal to beta 0 plus beta 1 x1 plus the beta 2 x2, isn't it? In R3, in R3. And this is a linear equation. R3. And this is a linear equation. This is an equation. It's as trivial as that. It's a linear equation, a hyperplane. In fact, it's a plane. It's not even hyper, but I'll use the word a plane. It's a plane. Actually, I meant to put the word hyper here, but here. Actually, let me just leave the word plane. It's a plane. It's a plane. It is a plane. A plane is flat, isn't it? It's some plane. I don't know. Some plane here. It's an equation of a plane in a higher dimensional space, which is three-dimensional space. And this intuition carries forward. There are two kinds of curves, curves that can be represented with a finite dimension polynomial and curves that cannot be represented by finite dimension polynomial. So they will need an infinite dimensional polynomial space. But polynomials are not the only basis functions. You can use sine x, you gaussians, and so forth. These are the things you will learn in the engineering math class that is coming up. So we won't go into it, but just to give you an intuition that you can build your axes out of any kind of functions and then expand or write your curve or imagine that your curve or your surface, your decision boundary, is a finite dimensional expression in that basis function, you can get away with it. Or sometimes you literally want infinite dimensional spaces. So you can take your data and this is hard to imagine. How do I draw a line in an infinite dimensional space it seems rather impractical there is no pencil that will help you do that of course but in mathematicians tend to generalize with abandon and say well i can still imagine that so we'll do that imagine a higher dimension space so so now what what has that to do with this well the statement is very simple. Since any curved decision boundary, well let me use the more formal term non-linear. What is non-linear is therefore curved. What is this? non-linear or in simple terms curved decision boundary boundary any nonlinear decision boundary is straight in some higher dimensional space, let me call it D, which is a high dimension space. Now think of the river. So what it means is that our meandering river, this river that sort of in R2 becomes what? It becomes a nice simple straight river. Do you see that guys? It becomes a straight river. River in some higher dimensional space. And so if that is is true then all you need to do is go to that higher dimension and use everything that we just use the maximum margin classifier technology find the support vectors and come back am I making sense guys making sense guys? You're getting the intuition. So if in some higher dimensional space, the river is truly straight, then all I have to do is have a magic manner to go to that higher dimensional space and then find, solve the problem there. Just the way we learn to solve the problem this two step process like a two steps a go to a linear high dimension space B then bring Then bring back your maximum margin classifiers. Classifier. These two together is essentially the gist of the algorithm support vector machines. This together is the support, the algorithm support vector machines. But it leaves a fundamental problem unanswered. How in the world do we discover this space? given data set isn't it so it is very easy to say that go to that other mother space in higher dimension where the river straightens out but how do you get there how do you discover that space that problem remains there right do you guys understand the situation yeah right so the answer to that is actually very very elegant and it is in fact the last insight that vladimir vatnik got so i will state it for you in simple terms i'll just sort of give you the intuition what it says is that a lot of problems in machine learning can be formulated as dot products, right? Dot products. This dot product, a dot b, right? So imagine that there is a vector. You remember that a vector is invariant of what reference frame you look at, right? And if you have two points, how close they are is given by A dot B. Now, it turns out that the nearness is preserved as you go to higher dimensions. Close points, I mean close. Now, I won't go into the full mathematical specification but i will just say that a point can be defined using its dot product with its neighbors so if i say that there are these three points and the dot product with one and two and three i give you you would have no choice but to place the point here because you have written a set of equations of dot products of that x dot a is equal to something, x dot b is equal to something, x dot c is equal to something. When you solve for this equation, you will end up solving for x. So you can actually specify a lot of things, reformulate a lot of Jay Shah, Dr. V. S. S. Jay Shah, Dr. S. S. Jay Shah, Dr. S. S. Jay Shah, Dr. S. S. Jay Shah, Dr. Sajeevan G. Want to solve a problem like find the decision boundary and you can specify the decision boundary as a product, then you have to do a dot product in the higher dimensional space. So there are two steps involved. Sajeevan G. A go to Sajeevan G. Higher dimensional space. Sajeevan G. Be do the dot product. Sajeevan product. And C, kernel of the dot product. Let's say kernel of the dot product. Kernel means any function, any function of dot product is a kernel. Are we together guys? Kernel is the word that is reserved for that, of dot products and now here comes the magic come back with results and the kernel trick is this sometimes you can do the dot product in the higher dimension without actually finding all the details of how to go from here to there it's a very interesting trick you know there's a higher dimensional space in that higher dimensional space you need the dot product you can actually get the dark product and and your functions of that products without necessarily finding explicit mapping or translation rules from this space to that space. Now that's a little bit mathsy. I'll give an extra session for this beautiful math actually. It's a very elegant math in some dual space and so forth, but we'll do that at some point, right? It's an extra session, certainly not beyond the scope of this class. And when you do engineering math, we will go through this carefully. And so the support vector machines became powerful. And Vapnik realized that he had to hit upon something when finally the last piece of the puzzle fell in place. And he realized that, well, every problem can be linearized. You can use you can flow a river through the data. And what is more, the process of going to that linear. Every problem can be linearized. You can flow a river through the data. And what is more, the process of going to that linear, you don't need to solve the whole problem. You need to solve only a small part of the problem, so long as you deal with dark products. And thus the technology of support vector machines. Support vector machines is one of the simpler kernel machines. The scope of kernel machines is huge. Next time we will use kernels to do nearest neighbors. Kernel nearest neighbors. We'll talk about dimensionality reduction. Let's keep those things for next time because I noticed that we are running out of time today. But this was, guys, the theory of support vector machines. Did you understand it, guys? Did you understand most of it? Anybody found it interesting? running out of time today but this was guys the theory of support vector machines did you understand it guys or did you understand most of it anybody found it interesting yes yes mostly got it yes so so i said this is a widget so just one point here there so here what is the kernel here so kernel is basically sort of a function which translates from the lower space to higher space kernel of just think of some things that X I X J the dot product function roughly speaking their functions of dot products, but actually in an intuition form, it is basically translate helping translate the those points in the lower dimension to higher dimension, their functions, mathematical functions. Kernels are mathematical functions that help you give something for example, distances. Okay. The beautiful thing is if you can formulate your problem as a kernel problem, then the translation rules from this to high dimensions, which may be bizarrely complicated, you don't even need to worry about it. You can sort of get to the kernel faster than you can get to solving all the rules of translating from here to there. And that shortcut actually saves you a lot of computation and makes this whole thing very, very practical and relevant. See, the first thing you see the kernel theory come together. It almost feels like magic. Wow. You can actually do that. embedded here and how Vapnik must have felt in 1960s. When he came up with this theory, he sat upon it for almost, what was it, 20, 25 years before it finally got recognition. But when it got recognition, people knew right off the bat that this is fundamental, this is a breakthrough. You know, the time for recognizing that algorithm had come, it had been discovered way ahead of its time. We use kernels everywhere, even in deep neural networks and everywhere. So we live in a world where kernels are taken for granted. We will delve into kernels much more in the audience at this moment. This is a practical methods workshop. So we won't delve more into the kernels, but certainly in the engineering math, the math behind data science class, this will be a hot topic. We'll do it in depth. And we'll do the whole derivation of it in depth. When is that happening? Pasif, sorry. That is something I want you guys to make a predictive model for. Predict the day. Okay. I don't know see it's a we are living in the coveted world it's a new it's an ensemble we have to do an ensemble of all these people voting machines you know yeah you guys can all guess and see where the guest lands if i have to guess it will be see it depends upon when i have enough people maybe i should open the registration and say we'll start when we have enough people maybe i should open the registration and say we'll start when we have enough people yeah let me do it like that it is actually it's a surprising thing one would think it's an abstract thing but i think of all my courses the boot camp and the math class the math of data science are the most popular they usually go housework some of you have how many is there anybody who has attended my math class the math of data science I have did you like it was did you guys find you did you guys find it relevant to what we are doing in talking yes that is actually the ABC of machine learning yes it's the heart of it. So as of here, we are finding dot product and coming back. And what are we doing again, they find the dot product. See, I did a lot of hand waving arguments, I hit a lot of mathematics. But what I'm basically saying is, you can find the decision boundary using some clever mathematical tricks in higher dimension and solve your problem right without actually knowing explicitly how you went from here to there yeah okay can you scroll up a little bit here yeah uh one second uh yes so here we are trying to find what a and b okay to a is we are this one. Okay. Any two points. What I take your X and some reference point X and XI. They will have a dot product and any function of the dot product is called a kernel. Roughly speaking any symmetric I mean obviously dot products are symmetric. So any function of the dot product is considered a kernel so in support vector machines we typically use three kernels one is the linear kernel what is the linear kernel the linear kernel is literally the dot product between X i X J the kernel is kernel xi xj is quite literally the dot product i'm using this is the dot product also written as xi dot xj right like that but because dot is sort of gets lost in writing so i write it with this Angela. Then there is a polynomial kernel. The polynomial kernel is polynomial kernel is XI XJ is quite literally that a polynomial of the dot product. You allow for, you know, the basic term also, XIXJ, to the power D, some power, polynomial power D. And then there is a more powerful and quite ubiquitously used kernel. It is called the RBF kernel, RBF. Now what in the world is that cryptic word? Radial basis functions. Yeah. Well that too is cryptic. What is Gaussian, you know, a lot of, you can write it as a bunch of Gaussians. So your kernel would be written as the kernel X J is equal to e to the minus gamma this is a suppression considered a suppression factor X I dot X J right I believe square it is a square and you can put in D 2 or something like that. The typical Gaussian, the two factor divided by two is subsumed in this gamma. It's a suppression factor. What it means is if you plot this E to the I, you will realize that it can, if it is gamma is small, it will be like this. It will be stronger and stronger. Gamma will suppress it. You see that this curve is being suppressed down as gamma increases. What is gamma? By the way, this is a Greek symbol gamma. And so what is the intuition behind it? The intuition, the way I would suggest, think of the intuition is, see a polynomial is simple i i showed you that a curve can become straightened in a polynomially high space right a polynomial of degree three for example three dimensional four dimensional space and so forth that is the polynomial kernel the radial basis function kernel is interesting it's sort of an infinite dimensional kernel of sorts uh you take It's sort of an infinite dimensional kernel of sorts. You take there, but yeah, so we'll come to that. Let me not go there. What it basically means is that suppose you have a complicated river. Let me make this river like this. Like this. Remember that these points are like lighthouses shining. So if you can find lighthouses, enough lighthouses, support vectors, right? Enough lighthouses shining. And maybe a couple of mistakes in here, which by the way, those mistakes are also called support vectors because they help support the decision boundary you realize that you can draw your you can navigate this river using these lighthouses isn't it all you need is lighthouses along your path does this make sense guys am i uh yes oh sorry this yellow thing should stay as a boatsman you just need that the riverbank should be lit right the metaphor is you just want as a boatsman just Boatsman just want the river banks and mistakes to be lit with enough lighthouses along the way. Is this intuition obvious, guys? Isn't it? As a boatsman at night, you just want the riverbanks and the mistakes to be lit with enough lighthouses along the way. If it is, you can navigate your path, isn't it? And support vector machines is just a means to find these lighthouses. Support vector machines, M, is an algorithm that cleverly finds these lighthouses are called support vectors. Well, I hope that this workshop series in my little training place truly represents lighthouses or support vectors to guide you guys along your learning journey. Yes, and that's the name of my company. With that, we'll stop guys. I'll take questions. and that's the name of my company with that will stop Kaiser question so if say the width of river is say the river is narrow right in that case that also means that the classes are overlapping you allow for mistakes, Pradeep. Because you allow for mistakes, it may be true that a river may have washed to the other bank. Or one of these boat houses is sitting on the bank of the agricultural site. Because you allow for mistakes in between, the overlapping regions are the mistakes, isn't it? You can still flow a reasonable river through it so long as you have a you have a good budget it depends on individuals to decide how much budget they want yes or more formally see ultimately the proof of the pudding is in the eating when you're building a support vector a machine classifier let us say how do you choose the budget you choose that budget which gives you best accuracy of performance on the test so if say it is binary problem sir so let me finish that so uh sorry to interrupt you so in other words that budget is a hyper parameter of the model isn't it so in a support vector machine there are two hyper so for example budget is one hyper parameter let me just write it down parameter so that is part of the nodes hyper parameters are see the budget part of the nodes. Hyperparameters are C, the budget. Actually, most people don't call it the budget. They call it the cost, which I find non-intuitive. In the literature, you will see that the way the psychic learn in your textbooks call it, they call it the cost. Actually it's the opposite, is the budget for mistakes. And the other is each of these for a gamma is the gamma for for our BF dimension degree of the polynomial for Polly In polynomial polynomial kernel, you still need to decide what your DS Sajeevan G. MLD isn't you'll figure it out. You can tell it just go figure it out. It will do that and so forth. So if you have certain the degree of actually I would say that these days the libraries, just to figure it out in the room. Sajeevan G. But there are certain hyper parameters built in typically use our BF quite often if your problem is complex. So these two become at least two of the hyper parameters in the model okay so for the non-linear problems like where the river is not straight the rbf can help finding the lighthouses either the polynomial or the rbfc remember linear is out of the window isn't it right so you'll have to try polynomial or RPF one of the two and so if it is binary classification problem yes so in that case I'm just wondering up with the linear regression will be more efficient just for the accuracy point of linear logistic regression draws a straight decision boundary in the feature space so you would get screwed your river is non-linear decision boundary is non-linear so it won't work so the burden to go to higher dimensional space to linearize the problem is on you and remember we have been doing this if you go back to your data set 2 for regression what did we do it was a sine wave like structure we went ourselves to polynomial degree in the language of support vector machines you would say that you have used a polynomial kernel yeah so let me say it one more way see you realize that finding a linear decision boundary is solving a linear problem. So all of machine learning problems ultimately go through two phases, whether it is support vector machines, how it is deep neural networks or so forth. The first phase is somehow to linearize the problem. All of these things, the deep neural nets, in many ways they're clever ways, roughly speaking speaking they're clever ways to linearize a problem to find that representation space in which the problem is linear and then solving a linear problem is easy so those of you who have played with neural networks remember what is the last layer of a neural net it is just a simple linear regressor node, right? Activation function, one symbol, or it is just a logistic function or a softmax function, which is generalization of logistic. Both of those are linear classifiers. So what happened? You solved a whole big complex non-linear problem by taking the input and adjusting the weights of the layers of the neural network in such a way that it found the true representation says that the problem becomes linear. And then in the last stage, you solve the linear problem. This is pretty much the way complex problems are solved. You find ways. You can do feature engineering, as we did last week. You can use clever tricks. You can use these powerful algorithms. They all just help you linearize the problem once your problem is linearized then you use a last step is a linear algorithm to solve it thank you sir can you again explain a little bit about rbf sir how the rbf functions as a function how does it work i would to. That's a little bit outside the scope. We will, why don't we do it? Either let's do extra session or do it in the engineering math because it can be a bit intimidating. In the audience, there are a lot of people who want to delve on, stay with the practical side. For them, that math can be a bit overwhelming. That's why I sort of glossed over the last stage, some of the kernel level math also for that reason. It is there. We will learn it, but we learn it in the other workshop. Carefully. Thank you. Asif, I know in this one, like in support vector, we don't have to find that higher order space and we just did this trick, right? Dot products. Yes. But in, in, so why, why, why didn't we play the same trick in earlier problems with why we have to find polynomials in our other algorithms? So we are learning. So what happens is that see when we can do by hand, it's the gold standard. It's an open model. I see. The machines are black box models, right? So it is very difficult for you to find out what is the space that this machine discovered. Yeah, but there's no need for- Interpretability is a huge casualty here, right? At the end of the day, support vectors is still remarkably interpretable because through the data set, what did it do? It says, don't tell me how, but I discovered these lighthouses. And once you know the lighthouses, the support vectors, you can literally see the intuition. You can see how the support vector machine is making its decisions. It becomes evident. So in that sense, it remains interpretable. But you lose this fact that how in the world did you pick these as the lighthouses? Yeah. What was the reason behind it? And then it says, well, welcome to the land of mathematics and let's talk constrained optimizations and dual spaces. The business guy will run away. But you still get that function at the end, right? Because you've got those lighthouses. Yeah, exactly. That's that. So it's all beautiful, Matt. We'll cover it, guys, at some point. But okay, we have three minutes left. Somebody said 10 is a hard cut off. Any other questions? Was it fun, guys? I find support vector machines to be one of the most beautiful gems in machine learning. vector machines to be one of the most beautiful gems in machine learning so even if we you know after a limit and then it will gonna start overfitting right doesn't matter we cannot increase the budget unlimited so it will start overfitting them right no what happened is if your budget is too small it starts overfitting because it tries to find the narrowest river that may still exist, or it may not find it at all. To make your budget too relaxed, you have bias wide error. So imagine that your budget is very wide, right? What will happen? A river, huge river will come up that is basically flooding the houses and the trees. You can make out and there will be so many mistakes, bias errors, isn't it? It's a simple, more or less straight, simple river, hugely wide flowing through it. So a large budget leads to bias errors. A small budget leads to variance errors. That's sort of the bias we're interested in this situation. Thank you, sir. As if, I think just one thing that helps me is the way to visualize functions is after all also a vector. It's like infinite number of vectors connected exactly functions are vectors and once it's vector then you do linear transformation and preserve the distance between the dimensions right and that's what i meant when i said radial basis function they are basis unit vectors in an infinite dimensional space whose axes are this bell curves there's basis functions but that's getting a little messy Let's do it in the relevant workshop. All right guys, I hope you enjoyed it. In the end, I did gloss over some of the higher level mathematics for a reason guys. From a practical perspective, you need to know the intuition and I hope today you learned the main intuition. It's a beautifully argued thing and to think that one man in one Russian winter sat down and thought through it is just amazing and it turns out that he was not the only one, there were other people who were thinking along similar ideas a few years later and so forth. Today when you look at the support vectors here you see a reference to a lot of researchers, but at least one of them, and in fact, he's considered the leading light on the topic. He did discover independently, and perhaps before other people, a lot of this years and years ago. Quite remarkable, isn't it? All right, so with those words, I'll stop the recording and I'll let you guys go. If you want to stay back for something, you're welcome to, but I'll stop the recording.