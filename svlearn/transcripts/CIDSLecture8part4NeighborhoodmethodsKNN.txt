 The methods that we are going to talk now are quite different from the very parametric models, you know, that we built like linear regression, logistic regression, etc. that we built like linear regression, logistic regression, etc. So we are going back to supervised learning, predictive models, and we are going to make predictions. Now, when we did linear regression, remember we built a hypothesis that there is a straight line. Then in the case of logistic regression, also we built a hypothesis that there is a straight line or a straight hyperplane as a decision boundary. Non-linear methods just makes a hyperplane into a hypersurface, right? That's all it does. But you do hypothesize a certain shape for that in those things, which is why you call them parametric models, because what you go searching for are the parameters of your model the best beta naught beta 1 beta 2 and so on and so forth those are parametric models now there is a class of models which are called non-parametric or some people call very high number of parameters in some sense but i'll just call them non-parametric or something or even forget parameters we will just call them non-parametric or something. Or even forget parameters, we will just call them neighborhood methods. So we are going to learn a new type of algorithms. These are called neighborhood methods. What does it mean, neighborhood methods. Here you don't build any model whatsoever. What you do is you go with this intuition that a point can be well represented by the company it keeps. It's an old saying amongst people too, you know, you can judge a lot about a person by looking at the company of the person, the friends of the person. Somewhat similar to that, let's go back to our old example of classification. Let us say that you're trying to classify and remember this was our decision boundary, but there is no decision boundary here. We had a blueberries and cherries. Some smaller cherries than expected. Do you remember that? And you can have, I think we said this is weight and this is size. The two feature dimensions. Those of you who are remote, are you able to see me clearly? Yes. Yes. So now, suppose, forget about any kind of algorithm. At this moment, if I were to ask you a basic intuitive question, if you are here at this point, is this a blueberry or a cherry? And how do you know that it's blueberry? Distance from decision boundary. No, no, imagine that you don't have a decision boundary visible. You haven't learned any decision boundary. It was a blue. That is right. It is right surrounded by blueberries exactly so what you say is that look around all you see are blueberries right so judging by the company it keeps the neighbors and you can pick neighbors so you can say a k neighbors k neighbors You can say a K neighbors, K neighbors, where K could be one, two, three, et cetera. So in the simplest case, you can just look at the nearest neighbor and say, what is the nearest neighbor? I will point, any point, I'll predict it to be whatever its nearest neighbor is, but then nearest neighbor can be unstable, right? Because as you know, in real life, your neighbor might not be the best representation of you, right? So take the average of a few neighbors. Look around. Which is the majority? So suppose you take K neighbors. Take the majority. Majority. So if I were to point here, Oh, sorry. If I were to point here, So let me call this A, B. What would you call this? Let's say that you did the five nearest neighbor. Ah, four cherries and one blueberry what would you call it this is blueberry what would you call this the votes are four in positive four favoring cherry one favoring blueberry one one point says blueberry. Who would you go with? Simple majority say it's a cherry, no? Because it's a cherry. So this is it. Now, suppose I were to sit here, C point and ask, what is it? So let's again find its five nearest neighbors. Ah, just by one vote, it turns out to be cherry. Right? Now, as you can see, it just sort of won because of one vote. Right? And so you can decide what you're doing is, do you see that it is effectively creating a decision boundary of its own? Just by looking at the k nearest neighbors and predicting here we took k is equal to 5. We can make predictions for every point in the feature space we can tell whether at that point it's a blueberry or it's a cherry so far so good guys any doubt any doubt so i need some feedback is it very clear all you do is look at the neighbors and find the majority so the the last one should be blueberry right why you So the last one is blueberry. Oh sorry, my apologies. Yes, you're right. Absolutely correct. Sorry, I was being absent-minded. It is a blueberry. Why did I write it as a cherry? Just checking that the students are paying attention. Yes, right. It's a blueberry. Good catch. The probability of getting to a blueberry or a cherry, so it depends on what you select. The closer to the boundary you go, the lesser the probability it is to be incorrect. That is it. So what happens is that even though you see it, but these are discrete numbers. You can say, you know, three votes for blueberry, two for cherry. So you can say probability is 66% of blueberry, isn't it? But you still mark it as blueberry because by definition, the majority goes to blueberry. Are we together? So it's a very simple algorithm that just counts, finds the neighbors and counts. So this is for classification. Likewise, now, so this part is easy, guys. So if the value of k is low, then the outcome may not be right, right? Yeah, it may not be. That is a good point. So for example, at this point let's say this point right look at this point that i just circled is this the nearest neighbor happens to be a blueberry, but something tells you that cherry would have been a more appropriate answer because if you look at more neighbors, it certainly is cherry, isn't it? If you look at the five nearest neighbors, it would have been a cherry, but you look at only the nearest neighbor, it looks like blueberry. And that is why K, this hyperparameter of the model can change your answer right and so there is a very interesting fact that i'll mention remember i talked about the bias variance trade-off do you guys remember the bias variance trade-off? So the way it is is that when k is equal to 1, high variance. Why high variance? It's a very intuitive answer. Actually, every time you take a different sample of the data, you'll end up drawing a different decision boundary, isn't it? Because your answers will change based on what samples you're comparing it to, k nearest neighbors you're comparing it to k nearest neighbors you're comparing it to isn't it i take another set of data you'll find different neighbors and you are your nearest neighbor may have changed in the other sample right which is why k is equal to one is high this thing and k is equal to the total number of points is high large approximately high bias the moment you increase k what will happen so for example in the limit that k is equal to n what will be the answer suppose they are one suppose there is one more blueberry than there than there are cherries right so every place will be marked as blueberry isn't it because wherever you sit if you have to look at all and neighbors all endpoints it will turn out that everywhere blueberry wins because it has one extra vote right So that is in which you you're grossly wrong, right? That is a situation of high bias in your data. And this same argument applies to regression. Also, let us say that let's go back to our regression situation you remember the troublesome data sets that used to look like this and we linear regression model would fail and then you had to do a lot of polynomial regression and all sorts of tricks to make it work do we remember this guys this is our data set two or something like that right and so here look at this method i say what is the value at this so suppose this is y this is the prediction this is x now suppose i say predict at this value predict what is y hat. Right? So you say, huh, you want y. Let me find the x neighbors. So you find that this point is here. This point is here. Actually, these are all points. These are neighboring points. All the neighboring points that actually exist in the data, you see what the y values are. And you notice that the y values are this, this, this, this, this, right? So what can you do? If you had to predict y here, would it be sensible just to take the average of them, right? So for regression, so remember remember for classification let me write it down for classification take majority of k as votes for regression take average average of k neighbors right so you just take the average value of this and let us say that the average value of this happens to be i don't know this you're pretty close to the answer that is a pretty good prediction isn't it would you agree right and so you say well you know what why is it that on the eighth session of this workshop you are showing such a simple method there is no point in learning the rest of machine learning that we have learned so far let's use k me a k nearest neighbor everywhere so simple you know you don't have to do much why can we not argue like that multiple reasons no no no the reasons are actually hang on let me bring it down so i can see you guys. Yeah, let me, sorry guys, I feel like sitting down, but I do feel like seeing your faces, so please give me a moment. No, don't. All right, and I'll just turn this a little bit down, right. So guys, can you see me? And I can see you, which is better. So the point is, if I have only K nearest neighbor algorithm, it does answer your question. Why is it bad? There are multiple reasons why you want to learn other things. First of all is, well, of course, it has a hyperparameter in the model, the lesser of the problem. Your answer will change based on the value of k isn't it then there what was the answers that you guys gave you need the data right yes yeah that is another that reason is valid you need to hold on to k data so to make judgment about each inference point, you have to compute the distance. You have to sort all the K points, isn't it? To find the nearest neighbor. So you have to compute distance to all the points and find the K nearest neighbors. That's computationally intensive. There is no model, right? It is just holding the instances in memory. So that's why you call it, it's a form of instance-based learning. You hold all the instances in memory. When a point comes, you go about searching for its neighbors, which is computationally intensive, and then do it. So what has happened is while a training cost is zero, inference cost goes up. But you say, well, that's fine. We'll just throw hardware at it and be done. What's wrong with that? It turns out there's another problem which we talked about, which is the high Potter curse. Dimensionality. The curse of dimensionality. Yes. The curse of dimensionality. In other words, dimensionality the curse of dimensionality yes the curse of dimensionality in other words nearest neighbor works well in low dimensional spaces when you go to high dimensional spaces what happens the the the meaning of neighbors gets significantly undermined because your neighbors are far away? So you would agree that your political views are probably not well reflected by the person sitting there in Texas. It's too far away, right? And it pretty much goes to that. Reality is local, especially in nonlinear situations. And most situations, there's a lot of complicated things happening. So reality is local. You need to build local models which you don't get to build if locality itself is lost because in high dimensional spaces it's very sparse and all the points are far away you see that right go ahead no but you can cook up all sorts of distance methods. Remember we talked about, for example, the Minkowski distances. And so there is a body of work, for example, people have found, there's a paper there, people have found that, remember we go to Minkowski, Euclidean is two, Manhattan is one, right? People have found that if you go to fractional dimensions, like the fractional sort of norms, the norm of L is equal to 0.5. You remember that it becomes like this. We did that. The picture looks very differently, pointy ones. People are seeing that sometimes for high dimensional spaces, for some data sets, going to a distance norm, which is very low norm, fractional norm, below one, helps. So there's some evidence, but basically none of the results are so satisfactory that you can say a k nearest neighbors generally works. It works in low dimensional spaces. Even then you have to compute distances. Right. And distance, what you're trying to do is somehow trying to argue that those distances are not so far, right? They are far. In high-dimensional spaces, they are far, basically. So that's the problem with k-nearest neighbor. But should you use it? Basic hint. If you are in low-dimensional data, k-nearest neighbor, right after trying out your dummy classifier to get a baseline and one you know one variable classifier one variable regression to get the next level of baseline improvement you should always try out k nearest neighbor and the same things apply do the confusion matrix do the roc curve do everything for classification do the residual analysis and regression, and see how well k-nearest neighbor works. Generally in low dimensional reality it will work very well. Generally it will work very well. And so if it is acceptable to put computational cost a little bit per inference, because every inference you're now finding the nearest neighbors then and in reality you don't see what happens is data you put into all sorts of geometrical structures like kd tree so you don't have to compare it to every point you can find the neighborhood pretty quickly by going to the sub region that it belongs to one moment, I'll take questions, then you can do that. And so it is not that bad. And so if you can do that, you should, it should be a go to option. In fact, it is an option that people forget quite often, people ask me questions, and I look at the data. And I noticed that there are only four variables. variables. And one of the first questions I noticed, they would have tried many, many things. And then if I were to ask, what about k nearest neighbor? They wouldn't have tried because sometimes it skips the mind. We get trapped into a mindset that we have to use a parametric model, linear regression, or random forest, or whatever it is. All of those are good good but what about k nearest neighbor because it's cheap it's relatively and you could use it so you should use it right okay questions you mentioned in the case of high dimensionality using a norm where l less than one works better people some people are observing. No, I'm not saying it works better. I've read in one paper that it seems to work better. Yeah, the intuition that I connected with that, because we were doing gradient descent, then it was implicitly doing dimensionality reduction. Gradient descent has nothing to do with that. That intuition is not correct. Gradient descent has nothing to do with that. That intuition is not correct. Gradient descent is not correct. If you look at the loss surface and the intersection that it makes with... Oh, you mean the contour, the contour plots that you make on the ground. Okay. On the parameter. So only in the case of an optimization that's happening from that yeah it made sense because a pointy uh surface for the the norm function or what comes out of the norm function no we don't use the norm not really uh the reason for that is the z-axis is lost you can't get rid of it you can't reduce it away and what you're looking at on is on the hypothesis space the contours of equiloss equiloss surfaces contour surfaces but i'm not able to connect that with this you shouldn't you shouldn't connect The two are completely unrelated. Where is this coming from? This is just a statement. This is a very simple intuition that quite often the friends that you have define who you are. Right? Simply put, it's like, you know, look at your neighbors. I put it in a non-mathematical way, not quite precise actually. Your friends may be living far off, but your neighbors define who you are. And the fact that L less than 1 used as the... It is just a basic thing for high dimensional spaces. You can use, because k-nearest neighbor implies neighbor, nearness. Nearness implies notion of distance metric. And if you define your distance metric with a norm, not Euclidean, not Manhattan, but even less, L less than one, some people report that for some datasets it's successful. That's all, it's just an observation. See, in machine learning, there are a lot of open areas, lots and lots of open areas high dimensional statistics and high dimensional machine learning we are only now beginning to get a handle on it or beginning to get a sense of it we don't understand it well i don't understand it well maybe some cutting-edge mathematicians are way ahead and i need to catch up but we are learning things about it so uh that is that where is my text sensitivity yeah so you had asked us to compare against different methods right so you have the null hypothesis and we have this yeah this will probably be like kind of i know i don't want to say gold standard but as close as you could get to the truth to the null hypothesis and are all the other things somewhere in between they will show up or you will get something better not really you can beat with a parametric model you can beat carrier's neighbor sometimes right but to give a feeling whether you are in that ballpark yeah whether there's still more work that you can do yeah i'll give you an example see i'll give you an example let me let's say that these points are not there. Right. And I want you to give me value at this point. So you notice that at this point, all your neighbors are here. So you will end up with a negative value. Right. You'll predict something negative here. But parametric model, let me bring it here, a parametric model would tell you that it is positive and your k-nearest neighbor has given you negative. Why? Because there are gaps in the data, right? There are regions where data is so sparse and that is the power. A good, your polynomial model just beat K-nearest neighbor. So see, remember, you can easily figure out that there is no way around no free lunch theorem, right? Everything is situational.