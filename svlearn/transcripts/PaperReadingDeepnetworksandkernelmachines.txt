 This paper just came out, I would say, a week or two ago. It's a very interesting paper. Usually I don't get papers that come out that are absolutely fresh. We have been covering the classics in some sense, the big papers, the bird, the attention is on your feet, and a variety of other papers which are very important, considered in some sense landmark papers and well established. Today I'm going to cover a different paper which has just come out. We do not know how much impact it will have but it certainly makes some very strong points. It is as it belongs to a line of investigation that people are pursuing fairly vigorously these days that tries to understand what exactly is a neural network doing. When we take a neural network and it has lots of layers in it, a deep neural network, you have enormous amount of parameters these days 100 million 500 million billion parameters or even 100 billion 200 billion parameters is considered pretty like normal we have reached a stage at which neural networks are so big it is very hard to impose interpretability on it and see what is it doing so what we do is we apply interpretability by different means like you know we make approximate models to mimic its behavior in local agents of the feature space we do things like that but we have no direct way to just layer by layer see what it is doing. Our current understanding is that, and it sort of is motivated by convolutional neural networks, we see that each of the convolutional neural networks, what it does is layer by layer it seems to do feature extraction. In the early layers you have the primitives, geometric primitives, the edges and things like that being extracted, and then higher order representations are gradually built out of those primitives. So the current understanding is deep neural networks work through representation learning or some sort of a hierarchical learning as you go deeper and deeper into the network, the next layer builds upon the representations that the previous layer has learned. Which is very good, but we seem to have hit a little bit of a wall. It doesn't further interpretability beyond that. So a lot of people are very seriously looking at the deep neural Network and asking, can we look at it from a different perspective? Can we reorient the way we understand this whole thing? Perhaps in such a way that it becomes easier for us to grasp what is happening inside. And if we can grasp what is happening inside, perhaps we can further, we can accelerate the pace of research in deep neural networks. As you know, we are making a lot of incremental progress, wonderful incremental progress, though some people believe that we are not making fundamental breakthroughs, including Jeffrey Hinton himself, who seems to believe that we have hit a wall. So, obviously, there is a need for more work and more theoretical breakthroughs to further or take things further. I consider this paper interesting. At this moment, I suppose that the research community will gradually weigh in on this paper and will see how much impact it has. But I thought it was interesting, Sirki, what you made some very good points. We are going to take this paper, dissect it, see what it says. It's unfortunately a set of very simple statements at night, but very insightfully it makes good statements. So let us pursue the reasoning that is there in these papers. Now in this particular paper, the title itself is quite provocative. It says that every model I would say that every model learned by gradient descent is approximately a kernel machine. Now, let us limit our sense to deep neural networks for the time being. In deep neural networks, if you remember, deep network or deep learning, essentially the pillars of our progress have been, one, the basic ideas have been that you do forward compute, two, you do backward propagation, propagation of gradient computes. Basically, gradient of the loss with respect to the weight of each of the layers, you keep on back propagating it layer by layer. And then the third step you do is the gradient descent. next value of the weight of any weight vector is essentially the current value of the weight vector minus the learning rate typically we use alpha and then we take the learning step so i hope this part is pretty well established in our minds now that the forward pass is just matrix multiplication. Simply matrix marks and activations. And activations at each layer. So these are the three steps of a deep neural network training. Training is. And of course, evaluation is just, eval is just the first step after the weights have been trained. Now. We will, I will just, so the crucial thing here is to note that of course we do gradient descent. This is what we do. We do all sorts of gradient descents. We do batch gradient descent, stochastic gradient descent, momentum-based gradient descent methods, Adam, this and that and so forth, right. So, but ultimately we do gradient descent. Gradient descents are considered first order methods because this is a first order derivative of the loss. So just to recap, L is the loss function. It is some function of Y and the gap between Y and Y hat. So for example, a loss function could be y minus y hat squared, right? Suppose I do this. This would be a sort of a sum squared loss and if you divide by this it is an MSE, mean squared loss. So there are many loss functions. You can have cross entropy loss, loss, and so on and so forth, lossification, regression, and you can cook up a more interesting loss functions. But invariant of which loss function you do, at the end of it, it is back propagation of the gradient computes, and then you do the gradient descent step. You actually take the gradient descent step. Now, coming back to this paper, which is not really, it says every model learned by gradient descent. So that's a pretty general statement. It's saying that not just deep neural networks, but just about every model that you learn by this process is approximately a kernel machine. Now let us recap what are kernel machines. Kernel machines is a synonym for or also called support vector machines. Let us recap what they are for a moment. And, but before we recap, just observe what the title is saying. It's saying something quite profound. It is saying in a sense that if you're doing gradient descent in your model, you are basically some form of a support vector machine or a kernel machine, approximately. And I would say that that's quite a startling statement to me. Let us see how they go about proving it. It turns out that they prove it in a fairly mathematically rigorous way and we need to walk through this. So what are kernel machines? Suppose you have a very intuitive picture and this is an extremely quick recap that I will give you. A kernel a target a space a kernel space kernel space where every data is linearized so for example if you have points let's say that you had a data like this and data here showed up as let me say a decision boundary really was like this here showed up as, let me say, a decision boundary really was like this. And let us say that you had points like this. And let's say I'll put green points here. This region is green. Let's throw in a few mistakes. And a few mistakes from the other two. Something like this. If this is your decision boundary, what what kernel machines try to do is they try to find a space where the separation could be something let's say that it be so it becomes a straight line like say can i bring your ruler here okay let me see if i could all right let's bring it here much better and All right, let's bring it here. Much better. So let us say that it becomes something like this. And do it becomes a linear decision boundary with its uh sorry with its uh with its margin there's a maximum margin hyperplane and all of that but here we won't go into that so in other words if you remember, the intuition we used is how can we straighten the river and flow the flow the widest river through data in the kernel space. Now for each point x, if you remember, we assume that there is a mapping function. There is a mapping function that takes it to a point. This from here, that is the phi of x do we remember this guys that we need a function a mapping function to a different kernel space where x goes to phi of x then we say that the beauty of this is we't trick says we don't actually have to compute file we can get away but what we need to do and find support vectors which is that these certain lighthouses that can guide or define the decision boundary so for example you may pick up some lighthouses, maybe this, this, this, or this, and a few lighthouses, lighthouses, or support vectors, that help define the decision boundary. That clearly help define the decision boundary. And all you need to do is given a sample point, given a query point, point point x an arbitrary quake point if you want to decide whether it is yellow or green all you need to do is find the the similarity of this point with respect to every uh these lighthouse points these four lighthouse points let me just call it a K light house points. If you can figure it out, what is the similarity of this or how close is this point in the kernel space to a few lighthouses, you will be able to tell. For example, if you point Xs here, let's say it is this, you will agree that it shows that if I can relate it to these points and I can basically come up with a way to tell that it should be yellow based on certain things. So obviously I'm using hand-waving arguments. I don't want to go into the whole mathematics that we went into. Now, what is the similarity in the feature space? Similarity in the kernel space. Now, if you remember, this we realize is the dot product of the feature vector, this and the xi. And these are the two vectors. We define the similarity as simply the dot product, which is the most intuitive notion of similarity. So we are looking at the dot product of these two points in the kernel space, and that's a measure of similarity. And the whole point of support vector machines or kernel machines is perhaps you don't need to do that for all the points in your training data, but you can handpick a few, the so-called support vectors you make them into lighthouses and quite often they are enough to guide you through right in the worst case all your all your training data can become sample or lighthouses or support vectors so this is your essential theory of support vector machines so as if um just so that we can understand that concept clearly, right? The concept of the lighthouses, so I have not done support vectors earlier. So that's the, I'm trying to kind of get a intuitive sense for it right now in this conversation, right? So that the lighthouse or the support vectors that you mentioned, are they necessarily around the boundary line? They are. And the proximity to one or the other tells you which side of the boundary line you are on. Is that the intuition? Somewhat similar, yes. That is the intuition. You can take that to be roughly the intuition. So that is that. So there are two steps to it. You need a kernel mapping function, what you call the mapping function. Function. And you need a kernel function. So this is these are called kernel functions. Kernel function. And it's to be distinguished between the mapping function itself. And using these two things, actually the entire theory of kernel methods is developed. And it's a very rich literature that we have here. It's a whole world in itself, just like deep learning is a whole world in itself. If I were teaching the same workshop, let's just say, 15 years ago, then I would be given a long four months workshop on just kernel methods. But at this moment, we are going to limit ourselves with just this very basic explanation. Now those of you who have taken support vector machines or things like that in the previous workshop with me, do you guys remember what I'm saying? Anyone of you would like to confirm? Yes. The idea is that when you use this, that's the maximum margin classifier. Exactly. The widest river or the maximum technical word would be maximum margin and i've tried as much as possible to make it intuitive in terms of real uh things that we see rivers and so forth um but that's what it is i'll say can you give me these these terms that you use you know i had this question when when I took the session on support regulations also, the lighthouses, are these sort of common terms used to explain? No, I use those terms. You use them. I feel like they're going to put lighthouses in their banks. Okay, all right. You won't find this in the literature the literature is more abstract it clearly explains the concept and gives the intuition i was just wondering these are my interpretations to explain these things so you won't find it so so now our nil space is always a higher dimensional space right compared to the space that you started with correct? Not necessarily. I'll give you a counter example. So let me pose this question here as part of our notes so that you have it. The question is are kernel spaces always higher, higher dimensional spaces compared to, to the input dimension, right? Input dimension. This is the equation. So I'll give you a counter example that will hopefully, but broadly speaking, yes, it is true, but there is a counter. I can give you a really quickly a counter example. So let's say that you are looking at data which happens to be in this circle and from the circle, you can generalize to an ellipse or go to a higher dimension to a sphere or an ellipsoid but let's keep our discussion simple suppose data points are here okay and the green data points are, actually let me not use plus, they look like crosses. Let me use something like this, right? And we can throw in a few mistakes here and there, and a points, let's throw in some noise here. Throw in a couple in some noise here. Throw in a couple of noise points here. Noise points and we'll throw in a few yellow points outside also. Sorry, crosses outside also for what it is worth. So now you look at this problem and certainly the decision boundary. What is the decision boundary here? It is this guy. So the decision boundary looks circular. Circular, right? In other words, nonlinear. Are you clear? we got that now we ask this question that was arithia speaking isn't it yes yes so i i guess i recognize it was so now imagine that i do one transformation So imagine that I do one transformation, let me call this axis X1, X2. So suppose X1, X2 goes to this mapping function, which converts it to X1 squared and X2 squared. As simple as that, right? And see what this transformation does to the data now my new axes are let me call it is equal to the phi x1 phi x2 right so in this phi 1 phi 2 space each of these points when you put you will realize that that see what is the constraint equation here. X one square plus X two square is equal to one. Some value, some radius. I just take maybe I take one with the R squared. Some R squared. Would you agree that this is the constraint equation for the surface? Yes. For the circle, we all know that this is the equation for the circle. Now what happens in this space, this is phi x1. This is phi, the x2, so this is phi1 rather, let me just, phi, phi1 of x, phi1, let me just write phi1, phi2, phi2. that this would become this. Let me write it here because that will illustrate the point phi1 plus phi2 is equal to a constant r squared. This is the constrained surface now. Now, what is this equation about this equation is actually a line that goes like this right this is the line for Phi 1 Phi 2 is equal to you can. You can convince yourself. So when phi two is zero, this much is r squared. And when phi one is zero, this much is r squared. And now all your points go here. And all your And all your green points are outside. So you have gone to a space which is linear, but what's the dimensionality of the space that it's here? This is like a linear. Yeah, but the dimensions, this is our this one is r2 euclidean space this one here and this is also two-dimensional euclidean space isn't it yes just the plane so in other words when you go from here to here using the phi mapping it is not necessarily true that you have to go to a higher dimension sometimes you don't need to go to a higher dimension you can stay in the same dimension but you just need the right transformation okay but if you ever go to a lower dimension right it can be either same or higher dimension yes usually i mean you don't see it go to lower dimensions see in practical terms what happens right you use the gaussian kernel and such things if you remember the rbf kernel or the gaussian kernel they go straight away to infinite dimensional inversion space if you remember from the last time we go from any problem we take it to infinite dimensional Hilbertian space. Hilbertian space. Hilbert space or Hilbertian space. Because everything is expanded in terms of infinitely many gaussians, if you remember. So that much you might be not remembering that got technical by the end of the civil protection. So in practice what you do, people do is they directly go and launch it to a high space and figure it out. But in, at least in the theory, you don't need to go to a higher space. Okay. So this was our basic thing. So now again, I'll end it with one basic intuition, KX. If I have to, given a query point, a probe, an XI and a data point, so we'll use this word for query or probe. Query point, data point, and this is the probe, this is the actual data point in training data. We'll use that and what we're seeing is it is the dot product of Phi X and Phi XI, the data coin. So how similar this point is depends upon a kernel function, which is nothing but the dot product in some kernel space. Some kernel space. And that is the one important thing to carry forward in this paper with that being there you say and there's a little bit more you see that y remember it's a linear equation right so y can be written as the out can be written as basically in terms of the kernel so you take the kernel functions x and xi all the points in the training example right and what you do is this is the point that i'm going to mention to you you take a linear combination of these linear so this is a linear combination of all the similarities add a bias term also for what it is worth uh b and then sum over i mean linear combination would imply this so in other words it will be like a1 kx x1 plus a2 kx x2 in the data point all the way to b. Are we together? It will be like this. And there is one more thing that you have is that given this you may also apply a non-linearity function, an activation function to it. I'll just write it as G because the paper writes it as G. A non. Function. Overall, and you can say that Y is equal to this. So it is the linear superposition of the similarities to each of the data points and throw in a bias term if you want. And then if needed apply a nonlinear activation function. So that is what is being said here in this particular equation that you mark this. So this equation now hopefully makes sense to you. So with that background, let us go and read this paper from the very beginning. I will read out the abstract. This abstract is... Asif sir, can you increase the resolution? I am doing that. That's exactly what I'm doing. Give me a second. much better yes okay so it goes on it says and um so first of all i always like to go straight to the keywords what are the keywords here so yeah you'll be talking about gradient descent we'll be talking about let me make the fonts these are the prints kernel machines are support vector machines the deep learning so it will limit itself to deep learning. Our current understanding of deep learning is representation learning. And now comes a new concept, which I'll explain, the neural tangent kernel. So this is of growing interest in research these days. People are taking this quite seriously and paying attention to it to see what all things it is saying. So these are the key words. You know every one of them except the newly tangent kernel label. Now let's start reading the paper. Deep learning successes are often attributed to its ability to automatically discover new representations of the data rather than relying on handcrafted features like other learning methods. In a sense, the power of deep learning was that you didn't have to do a lot of feature engineering. The idea was that hierarchically, layer by layer, it would discover the feature on its own. Isn't it guys? Right? So any doubt in the first sentence? We show, however, that we are talking about here. Let me take this. We show. This sentence is the most important sentence. It is just one sentence. I'll repeat it again. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines or support vector machines. A learning method that simply memorizes the data and uses it directly for prediction via a similarity function. So do we understand what he's basically saying? Is anything to the fact that we need to just pick out a few lighthouses and remember them. And once we have a similarity function or the kernel function, then that's all we need. Given that kernel function and given a few support vector machines, we can solve the problem. We can do classification regression, whichever let's take classification. We can do that. Is that easy now intuitive guys? So I say like, what does this memory exactly means? Like, is it, if you memorize, like is that not like overfitting? No, the problem is if you memorize, is that not overfitting? No, the problem is if you memorize, you can only respond to data points that you have seen. The point of support vector machines is it sort of remembers a few instances, the support vectors of the lighthouses. It memorizes only a small bit and then it uses to generalize from there. In fact, support vector machines are very robust to overfitting because they memorize only a few data points. They discover the real goal, the real ones that matter in the data and light them up as lighthouses in effect, make them into support vectors and use them. That's how the support vector algorithm works. So it's sort of- Okay. So is it like a, in essence, like a generative model, right? Like, so based on few points I can generate and get like something else. Not a generative, it's a discriminative model, but given a few points, I am able to do the classification for any point right for any point feature space i'm able to do the classification still discriminative but yes okay so as if a quick question here uh why wouldn't you use the word centroid for the support vectors because there's nothing no relationship of centroid to support vectors is it is it not the like if it's trying to distinguish between two spaces aren't these support vectors in effect finding the the centroid of the space? That's not it? They are not. I think they're not. You have to sort of go through that class, the ML200. Okay. Because we developed the support vector theory in great detail, and it's too little a time now to cover the ground, but suffice is to say, it is not based on the centroid, I think. No, the difficulty I'm having is trying to, if something is on the boundary, it is closer to the other side as well. That is true. So me trying to find a cosine similarity with something at the boundary could easily be, I mean, the cosine similarity with something on this side of the boundary would be the same as of something on this side of the boundary would be the same as something on the other side of the boundary correct see here's the thing suppose i'm looking at a point like this right uh this point now if you look at this point what you do is you you do the kernel with respect to all the points but what will happen is the similarity between these points will dominate on the right will dominate and what they will do because they dominate the overall sign of your Y the sine of Y, which is with a sine as in symbol, plus or minus, minus one or plus one, it will tend towards, let's say that this side is positive and this side is negative, right? So usually people recommend minus and positive. So what will happen is it will tend the value of y will tend towards negative it does not deal with similarity you are doing the function you are still doing ai kernel with respect to all the four like for example here there are four points so you are doing with this point and xi you are doing the similarity with respect to each of the four lighthouses XI, you're doing the similarity with respect to each of the four lighthouses. But the net result would be, and I'm just removing the non-linearity transform, the end result of this would be, and let me throw in a bias term, end result of this is this will still come out negative. And so you would know that it's a yellow point. Okay. points okay so in essence these sample points or these subset of points that the support vector algorithm is able to bubble up are salient representation of that set yes that is it the whole point is that it discovers which points are the most essential in this whole story that help explain the rest of the data. Okay. So now the next sentence that is here. So guys, I'll review, reread the most crucial statement. We show however, the deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for predicting via a similarity function. So the whole question then is, excuse me, what is the value of this? So the value of this is this greatly enhances the interpretability of deep network weights. You know these weights that you have, it gives a lot of interpretability to it or to the parameters. It elucidates by elucidating that they are effectively a superposition of training examples. So you know if you realize that the weights are nothing but superposition of the training examples, what they do therefore is somehow they encode the essence of the data that they have seen in some sense, right? They extract the essence of the data in many ways. So that is that. The network architecture incorporates knowledge of the target function into the kernel. What is a target function? At the end of it, a neural network is nothing but, given x goes in, it is y of x, isn't it? It is, or another way of writing it is, a y is some function of x with respect to the weights, the weights of the model. This is, at the end of it, if you look at it as a box, this is what a neural network is. Would you agree, guys? Not only a neural network, every single thing, every parametric model is essentially something like that. It has some function of the input. The only question is we don't know what function, we expect it to be a differentiable function, so on and so forth. We impose good qualities upon it, but it is. So that's your target function, the target. And so they go on to say that this improves understanding, this improved understanding should lead to better learning out there. So the hope is that because now you understand or have better intuition into what is going on, it will accelerate some of the developments coming. So despite its many successes, deep learning remains poorly understood. Which is true. In contrast, kernel machines are based on well-developed mathematical theory, but their empirical performance generally lacks behind that of deep networks. Yeah. So today what happens is that the kernels that we were using in support vector machines, the RBF kernels, the polynomials, the linear kernels, if you use those kernels that we were using in support vector machines, the RBF kernels, the polynomials, the linear kernels, if you use those kernels, the kernel machine performance is below that of using a deep neural network, for example, a CNN for image classification. So that has been established fact, right? That is why support vector machines at one time were the king of MNIST or digit classification and so forth. But today our deep neural networks have taken over. So we have this dilemma. Kernel machines are very well understood, they're very intuitive, but deep neural networks are not really understood at all, much at all, but they give you superior performance. And that's why people sometimes say that maybe this new genre of algorithms cannot be interpreted in all sorts of non-sensitive statements. So anyway, we'll move forward with that. The standard algorithm for learning deep networks and many other models is gradient descent. Here we show that every model learned by this regardless of architecture is approximately equivalent to a kernel machine with a particular type of kernel. And then you go on to say many more things but I will now go to the meat of the matter. So this is a kernel machine, the basic equation of a kernel machine, which says that the y can be computed by looking at the similarity of a probe point. So given a probe point, query point x, and you're trying to find the y, all you do is look at its kernel similarity to a bunch of lighthouses, to a bunch of them. In the limit, you can find its similarity to all the points, all the training points, and take a linear superposition of them or linear combination of them. And that is it. If you need to throw in a non-linearity, you can throw it in. But that's about it, right, into this argument. So now what happens is that this A i, the coefficients of the similarity are typically functions of the labels itself, plus, minus, and so on and so forth, you know, one minus one. Kernel machines, also known as support vector machines, are one of the most developed and widely used machine learning methods. So the point is that they are very well understood. We know their entire books written about the mathematics of support vector machines. Mathematicians have really gone at it and made it on very solid foundations so in the last decade however they've been eclipsed by neural networks and multi-layer perceptrons and so forth and people used to believe that kernel machines can be viewed as neural networks with just one hidden layer with the kernel as the non-linearity right but now these guys are going to But now these guys are going to make a different statement. So we'll watch out for that. This equation, guys, is what is it? This is a gradient descent. We would have written it as W next is equal to W minus alpha gradient, the loss of the weight. Is this true? We would have, this is implicit, so I won't write this. This is the way we write it, isn't it? So what it means is that this is the same as this. In this paper they use the word epsilon for the learning rate. So that is all right. Now comes the big result that they're going to show. The result that they show and in the rest of the paper they prove it is that you're looking for that similarity function between the query point x and each of the let's say some other point x' and typically the other point is a data point in the training data it says that that thing is actually nothing but the gradient flows like the word people often use is that these are the tangent gradients or the tangent radians are being used. So we will go over it in a more specific way. So I will read it from here specifically. The kernel machines that result from gradient descent use what we term a kernel path. If we take the learning rate to be infinitesimally small the path integral between two data points is simply the integral of the dot product of the model's gradients at the two points where the path over the path taken by the parameters during gradient during gradient descent right So what happens is that suppose x, you know, if you take, so when you do gradient descent, w, you know, the weight vector, it will go from this to some other point, w final, right? So this is a very nonlinear, it will shift to this to some other point, w final, right? So this is a very nonlinear, it will shift to this space in the feature space, in the parameter space, let's say, this is your w space. If you think of it, this is your 100 billion or 100 million dimension space or whatever, but for simplicity, I assume it's a two dimension space. So you wait, so you know, you can think of your parameters, it's much easier to think of it in two dimensions. So what happens? As the gradient descent happens, your weights will take some part, you know, they'll go along certain values. Any one of the weights will shift a little in small increments at each step of the gradient descent. So what we are saying is, if you look at how the response changes, why changes with respect to the weight, right? That is the most important thing. And it turns out that if you look at this, this is again a dot product. Guys, do you see a dot product inside that dot project? Now, immediately gives you a sense that there is a we're getting close to the notion of similarity here. Specifically, the kernel machines that result from gradient descent use what we term as a path kernel so this is the path and what we are saying is that this point let's go let me let me draw it out here again sort of Let me bring that equation down here. Okay. Okay. This, this is very illustrative. If you look at this, you started out at this point. And W final after a few steps of gradient descent, it has gone from W1 to W2, W0 to W1 to W2, right, to W3 here, W3 here, to finally W final. This is the path that the weights have taken as the gradient descent happens step by step. So at each point, what you can compute is that y is the output. You know that the black box is essentially x goes in and y comes out. So what you can do is at each point, you can compute and it is a function of the weights. There is some function, y is some function. You realize that it is a function, some function of weights and of course x. So you can take the gradient of y with respect to the weights. You know how much does y change? If I just change the weight a little bit how much does the output change right that is that sensitivity of the output to changes in weight is given by grad y right y of x so at each of these points, W naught, etc., etc., you can compute the gradient. Gradient, this gradient, it sort of measures the gradient. Let me give you the physical intuition here. Gradient of Y. When you write the gradient of y with respect to the weight at the point x, I'll write it in words. At point x, that is the x input, with respect to the weights, the parameter weights, that is what this is. What is the intuition? Intuition is, it is the sensitivity at input x to to sensitive after of what of the output to small changes in the input. I take that back. It's small changes in the parameter. Weights. The weights, yeah. Excellent. To the weights, how sensitive it is. That's what gradients measure. Derivatives measure sensitivity. And now what we are saying that if you go back and look at the definition of that this k x x prime, I'll just write it here, it is saying that the similarity of the point in the kernel space between x and another point x prime is nothing but you take a path that the weights have taken, let's say some path that the weights have taken, that the weights have taken. It's a some part that the weights have taken. And this is the gradient of the weights. At the y of x. At this point x in the parameter space, what is the gradient dot product it in my notation, I would write it as this dot product it with y of x prime of the weights so this is sort of the notation I'm deliberately picking a notation that we are used to right and you compute this so this is a dot product a dot product is ultimately itself a function and you compute it over this entire path. Let's say this is W naught. This is W1. This is W2. This is W3. Let's say let's make it more W final. So now what does it mean? Let us see what it means. What does it mean to take the path integral so the explanation is very simple suppose you have two points let there be two input points x1 and x2. These are two input points. Then you compute k here, the k. So let's go here at this point. Let's say that the gradient of this of just an arbitrary point given a query point, think of it as any arbitrary point x right which is not in the data so remember you have to make prediction for x any arbitrary point in the feature space you have to tell whether it's a cow or a duck for example right and that point is the entire any random point in the feature space treat it as a query point and you have a limited set of the data points on which you have trained your model so let's take this what you do is you find y of x with respect to the weight at this point let us say that at that point the with respect to grad y of X1 is this with respect to the weight. And let us say that the grad of this, but no, I shouldn't use this. Let me use a different color. Let me say blue. Goes like this, grad W of y x2 at x2 so now look at the grad the red vector which is it closer to the yellow or the blue yellow the yellow one right it's closer to the to the yellow one and so you would say that here this is much more similar then you can go and do the same thing here and see what values come out and now what do you do see the integral can be also thought of as just sum sum or average right that's a that's the intuition behind integrals. I mean, just take sum. If you take discrete point, integrals become sums. You go to this point now, and now I'll show you the picture that he has drawn, which is very descriptive. And let me. Look at this at this point your uh the y of x the gradient of y it is closer to x1 or x2 similar to the gradient with respect to x1 or x2 x1 x1 what about at the point w2 again x1 again x1 then what about w3 same x1 same it seems to be similar so what you do is i mean obviously he has taken three points an integral is nothing but the sum you know of discrete points in the continuous limit so if you you keep on doing this, at the end of it, what do you conclude? This guy is more influenced by what? X1 or X2? X1. X1, right? It's more similar to X1. So if you look at the value that is produced, the output that's produced by X1, it will be a closer representation of the output than x2, okay? And that is the main thing. It's a very simple idea, very elegant idea that they represent, okay? So I'll just read out the sentence, the figure one, how the path integral measures similarity between examples in this two-dimensional illustration, as the weight follows a path on the plane during training, the model's gradient vectors on the weight plane for x1, x2, vary along it. The kernel is the integral of the dot product of the gradients and this y, the x and the probe and the x1 over the path and similarly for x2. Because on average over the weight path, this quantity, you realize that this quantity will be greater because these two vectors are more similar along the path than this vector. along the path than this vector, y1 is more influential or has more influence than y2 in predicting y, predicting the output for this query point, all else being equal. And that is the gist of what they're saying. Now, there is a, I don't know, what is a time check? now there is a i don't know what is the time check uh how many of you are interested in the mathematical derivation which is one pm we are already past an hour uh let me go i have one question about this so about this dot product so it seems that you are assuming only the cosine of the angle right but what about the magnitude of the vector it's not product is both magnitude sensitive also remember i'm not right i'm not scaling it down by the magnitudes it is a full dot product right so so for some points it can be that even though the vectors are not similar but magnitude wise it can be more similar to the x2 that is right that is right with a little bit of a cover then i'll come to that when you work through the mathematics right uh that sort of gets taken care of but okay uh but yes in theory that's possible so first is we define some terms the the word the tangent kernel is it comes from previous literature people have been researching it for quite some time associated with the function so this is your y y is equal to some function of x which you don't know some unknown function of x and the parameter vector means some some some weight at this point some of the weight, and so it defines it to be the dot product. This is just a matter of definition with the gradients taken at this point. The second thing is now that you have this definition, the integral of this, the path kernel, so given the tangent kernel, the integral of the tangent kernel over a path is the path kernel. This is a point. This is it. And the theorem says. Suppose that you have a model like this and it makes it goes on to say that in the limit of a very, very, very small learning rates. So learning rates going to tending to 0, it's extremely in the limit of learning rate going to 0, tending to 0. I see what happens is that this statement will come true. Why? It's nothing but a linear superposition of the kernels, similarities with respect to the data. And that's a profound statement. You're saying that in any method that uses gradient descent, you'll always have AI is equal to, well, profound enough. I mean, to me it's very interesting. is equal to well profoundly enough i mean to me it's very interesting um uh so it is a linear combination of the kernels right or the similarities to known data points what it means is in some sense a neural network develops two things an internal memory of the data which it encodes in the weight and ability to generalize from that. So that's the way I see it. So now it says that this is true. Now the question is how do we prove that this is true? The first thing is you realize that we wrote this part is W, if W next is equal to W minus epsilon, a gradient of L, then W next minus W over epsilon is equal to minus grad, right? So that is the result they are speaking of here. This is very easy. Now what you do, and this is the part, when you take the limit of epsilon becoming very small, this is basic calculus. This becomes dw, and you treat epsilon as a variable dt. Let's say epsilon is equal to dt, a very small change of the learning rate. So for very small changes of the learning rate, the change in the weight is equal to minus. Right. And so you realize that you can write it like that. Till now you have been thinking of epsilon as just a constant learning rate, but here we are flowing it. You're saying it's a variable and for, if you just look at how the weights change with respect to the loss right the gradient of the laws by changing small changes to the learning rate how much does the weight change that is proportional to or equal to the negative of the last gradient so that's the statement they're making so they're converting this discrete to continuous to a continuous statement. So this concept, this is called the gradient flow. This is known as the gradient flow. And there has been some literature about it for some time. Now comes the chain rule and the rest of it. Just observe that this statement is of course true. This is nothing but chain rule. Y is a function of the weights, right? And it is also a function of time. So if you want to, not time, T is what is mentioned here as time is actually the learning rate flow. So dy dt is, this equation is obvious, right? This is just the chain rule, right? And so when you plug it in with this fact here into this equation, it becomes this equation, which again is very straightforward. This part, all you're doing is you're substituting the right hand side from here and it becomes this, as you can see. Well, that seems interesting enough what about what about going a little bit further so now you say that all right i don't want to do it directly like this i want to say dl dwj is equal to dl uh partial derivative of the loss with respect to the output for each of the data sample, the sample points here, and yi d weight j, which is again the chain rule. And then, of course, when you do it like that, you'll have to sum over all of the i's and so forth. So this is the part, this becomes this. We can write it as this. Again, nothing but your basic calculus chain rule. And surprisingly, that's all you need to do. After that, you're looking at the result quite a bit. So when you rearrange the term, it becomes this. It's the same thing as this. Take the minus sign out, take the, just take this guy to here, and then you end up with this sort of things. Now they say that suppose, right, you apply some notational stuff, L prime is equal to dL dy, right, this part, L prime. So you have the sum over all L primes times this part. And they are saying that this part is essentially, if you look very carefully at this part, you will realize that, see this, what is this? dy dwj, dy i dwj. So it is the function of y at x and this is the function of y at what y i is nothing but y xi right and therefore what is this when you do the dot product over j this is nothing but grad of y with respect to the weight at x dot product with grad of y with respect to the weight at x1, right? And so you have already shown that this tangent kernel is involved. And so in the limit, obviously, this is a limit equation. Now, if you remove it, like if you have dy or dt is equal to something let's say x would you agree that dy is equal to x dt which means that y is equal to the y previous value why not right plus x dt except that x in our case is this minus blah blah blah right so this equation therefore follows now what is the value of this equation it basically says that if you want to see the change of y right with small fluctuations uh then you just have to look at that a path sort of a kernel now the rest of it is you know just going through more vocabulary, creating more terms. You can normalize it by multiplying by this and denominator. So this takes care of the idea of that, remember you were saying of the normalizing factors and so forth. In some sense, it does that. This is it. You continue here. And the rest of it is not terribly interesting. All it says is that if you can create this, this is your kernel. And this part, if you write it as this L prime, it's a matter of definition. You just define it to be L prime. So then your equation therefore becomes this. So long as your definition of AI is this complicated integral. That's all. It's just change of notation. But at the end of it, what you have ended up showing is that in the limit of very small learning rates, and surprisingly, I mean, it's not something that you would expect suddenly, this result pops out. Which says, hey, in the limit of very, very, very small learning rates being used, the output is nothing but a linear combination of similarities to input data, of training data, which is quite an interesting statement that it is making. Now, there are a few remarks. It says that, remark one, so it says, the first thing it says is that, see, this differs from the typical kernel machines in that A and B depend on X, right? Nevertheless, the AIs play a role similar to the example weights in ordinary SVM, support vector machines, and the perceptron example. Examples that the loss is more sensitive to during learning have a higher weight and so on and so forth. See guys, this is a mathematical theorem. What it means is you cannot say that this is not correct. It is correct. In the limit of epsilon going to zero, this statement is great. So it's a mathematically rigorous proof. We can continue on with that, but let's go to the interpretation of it. What it is saying is that if two points, let's say that you want to find out what is the value here, why? What you do in the simplest term is of all the input data points, see that if you change the weights, how do the weights change for this point of the loss with respect to the loss? How does the gradient flow here and how does the gradient flow for that other point? Two points whose gradients flow similarly. In other words, will be very close. So their outputs will also be very close. Right? That's quite an interesting statement. Now, we need to start taking seriously these gradients, tangent, gradient, you know, kernels. And that's what it says. It says that that is what it is trying to find which similarity based on those gradients a kernel that comes from the gradients and so the rest of the paper is quite straightforward you get a given a query you feed in the k the the similarity to each of the data points and then you find it out and a linear combination of all of them what each of them is saying will be the answer right so i will reach out reach out the conclusion here we are running out of time so it says that even when they generalize well, deep networks often appear to memorize and replay whole training instances. As you know, see what happens is we have known for a fact that when you train a complicated model, it does generalize. But it tends to often just if you give it the same data as the training data, it seems to make perfect predictions quite often. It seems to have internally memorized the data. And now we begin to get an intuition of why it does that, right? Because that's what it is trying to do. The fact that deep networks are in fact kernel machines helps explain both of these observations. So what are the two observations it's explaining? Deep network weights are superpositions of training examples applying the kernel with an okay even when we generalize now we need to go to the previous sentence. Experimentally deep networks and kernel machines often perform more similarly than would be expected based on their mathematical formulation. So there are a lot of people who are working very actively on this now. Now the fact that deep networks are in fact kernel machines, basically, it turns out that they're equivalent or they are support vector machines, helps explain both of these observations. They also shed light on the surprising brittleness of deep models whose performance can degrade rapidly as the query point moves away from the nearest training instance. So it also explains that, you know, suppose you have trained data in one by taking data points, a training data which occupies one part of the feature space. When you try, when you ask it to make prediction for points away from the feature space, the deep networks perform very poorly. Right? So it sort of gives you an explanation because none of the training points are similar to it, isn't it? So isn't it? So now, whose performance degrade rapidly as the gray points move away from the nearest training instance, since this is what is expected of kernel estimators in higher dimension spaces, which is it was already known for SVM. Perhaps the most significant implication of our results for deep learning is that it casts doubt on the common view that it works by automatically discovering new representations of the data in contrast with other machine learning methods which rely on predefined features so that's a big statement actually i don't know how much what others will say about it but to the extent they have made a pretty interesting statement they say that we have always believed that deep networks would learn representations of the data and so on and so forth but what they are seeing is if they are nothing but support victim machines and they look at similarities they point they pick those crucial points and look at the similarity of a query point to this, you know, those data points, those lighthouses and some or all the points if you wish in some kernel space and then if that's how they work, now how do we reconcile that with our understanding that if they are representation learning engines, right, they learn higher and higher order of representation. So I'm sure there'll be more work and more discussion. We'll see what comes out. This paper just came out a week or two ago. As it turns out, deep learning also relies on such features, namely the gradients of predefined functions and uses them. So there's gradients, right? All of these gradient descent doesn't select features from that. So I won't go into the rest of it. And now it will be fairly easy for you to read this paper. More significantly, how learning path kernel machines via gradient descent overcomes the scalability bottlenecks that have long limited the applicability of kernel methods to large datasets. So the interesting part of the breakthrough is not just for deep learning. It is also to the whole world of SVMs, kernel methods, because there the problem is the kind of kernel people were using, they did not scale well. There was the sort of the gram matrix, which used to become very large and quadratic computations and a terrible performance. SVMs became very, very slow actually for large datasets. You couldn't do. And what these people are saying is that, now they can be cross-fertilization of ideas. While you can apply kernel machines to deep learning, the other way is also true. Deep learning has developed a vast literature of findings. So if we can apply this new path kernel back to support vector machines, we can backport a lot of the results from deep neural networks back to kernel machines. And that could lead to a sort of again erroneous of those kernel machines over again. Essentially, that's what they're saying. So they go on to say the significance of our result extends beyond deep networks and kernel machines. In its light, gradient descent can be viewed as a boosting algorithm with tangent kernel machines as the weak learners and path kernel machines as the strong learners obtained by boosting. So there's more connecting to different pieces of the of machine learning and you know boosting and all of that so we won't get into that that in a sense is the space for guys you understood the gist of it it's pretty straightforward isn't it any questions it has uh i mean the understanding uh the main core understanding is simple intuition is simple but it does like they make very important observation multiple important observation right and the derivation is so easy isn't it right right yeah very straightforward Very straightforward. Yeah. But if it does, I mean, if all their observations are true, it will probably lead to a lot of convergence between classical and deep learning. That is true. Actually, this is all very recent work that I have been observing. How can, if you keep following the archive, and you see what papers are coming up. They're getting a lot of attention. By the way, this paper is getting a lot of attention. The tangent kernels, which is the dot product of this gradients with respect to weighted points, which are called tangent kernels, they're getting a lot of attention these days from a group of researchers. So let's see what comes out of that. It's certainly been taken quite seriously. So, guys, this was a theoretical paper. We have been doing a lot of practical papers, the birds and this and that. I thought it may be interesting to take a deeper dive into how people are researching and thinking about the foundations of deep learning. Any other questions guys? That's all I have for today. One more thing, it looks like the debate between the classical method and the deep learning will continue to go on multiple parameters like computation and the understanding interpretability and so on. Right. See, to know that something that looked exotic, completely different, you can actually see it from a familiar perspective. It's a big big thing to see them as kernel machines which are so well understood i think it furthers the understanding of deep neural network and the other way around the fact that deep neural network has made such wonderful progress in so many areas now means that if we could backport this path kernels which were not used in standard SVM theory, they were using very simple kernels. But if we can backboard this theory back to kernel machines, there would be quite a range. And so they'll be coming together, these two approaches, now that people realize that there's a lot of overlap between these two okay so as if i mean i'm i don't have an understanding of svm just the level what you described today but listening to this the paper reading what i'm wondering is doesn't this pay away for more explainability for neural networks in the future certainly in fact that fact, that is the hottest concern and one of the hottest concerns in deep learning, the fact that we don't even understand what in the world is going on inside that box. And interpretability and explainability is a big deal of it. So this paper actually goes towards the interpretability of deep neural networks that we will talk about a little bit more in the regular sessions. See, most of the efforts to interpretability, they just take an empirical approach. They look at the consequence of the predictions or they make locally approximate models. And in a way, they're trying to, you know, it's almost like blind men and they're touching the elephants and they're making some approximate conclusion in what the elephant looks like. That is all very useful and in practical terms is all very useful. And in practical terms, it's definitely very useful. We need to make locally approximate modules and so forth. But this is more a deeper thing. It says, let's try to understand what's happening inside a new event. And so it brings back certain degree of interpretability. So- The assumption I'm making here is with support vector machines, I'm assuming those were considered to be fully understood. The sense once those support vectors have been identified, there is a methodical explanation for what the model is doing. Is that the correct understanding? That is absolutely correct, actually. We do understand SVMs far better than the dynamics of neural nets. The theory is very well developed. One thing is that SVM sort of arose from a purely mathematical line of reasoning. There was this guy Vladimir Bakhtin sitting in Russia. He actually developed some of the core ideas of SVMs in the 1960s and he was completely ignored for 20-30 years. Still he was rediscovered in the by I think some big company AT&T or I don't know some company here in the US and now he's today a professor at MIT. He's quite old now but yeah so it started out from a very strong mathematical foundation and that has always been a problem with svm people who come to the svm theory they know that it's very sort of an interpretable understandable so long as you are you you remember your vector calculus and probabilities and so forth because it does go into the math like you see me right i go into the tangents and this and that these are not hard these are elementary stuff if you remember your engineering math but if you don't then they look hard i think that the math part is one as if i'm what i'm trying to when i say explainable i'm taking it back to the business context right so the interpretability of the math should eventually translate to being able to explain in a business context what some of the implications are certainly yes so it is it certainly moves the needle forward quite a bit a work like this but but there is this is one there's a whole slew of work going on along this direction and interpretability of deep learning is a huge topic we are trying to everybody seems to be trying to look at it very very seriously uh asif can you explain that superposition again superposition is this is just this right so that is not the memory part like it's it like based on the few support vectors it does the generalization because you have a1 of x like i'll just take an example a1 of something of p plus a2 of q what is this equation it is a superposition of p and q with a1 and a2 as the weights. So superposition is just a term saying you take some contribution from P, some contribution for Q to produce a final results. And so with that being there, now you look at this equation. It is just a superposition of all the similarities or a linear combination of all the similarities. That's all. Oh, okay. Yeah. a linear combination of all the similarities that's all oh okay yeah all right guys any other questions if not i wish you all a good afternoon enjoy your lunch i'll see you tomorrow.