 So today is Tuesday evening. This is the extra session. Instead of doing a research paper reading, we are going to discuss this topic called tidy data, which we didn't get time to cover in our regular saturday session tidy data refers to a form of data a way of representing data that makes it very easy to do many of the data science exercises that we do very easy to do many of the data science exercises that we do. Data comes to us in many forms. Generally in a very haphazard form, one of the first things we have to do with data is we have to canonize it or standardize it or represent it in a way that makes it sort of amiable for analysis. way that makes it sort of amiable for analysis. In some sense, you can think about it as a continuation. Remember that when we got data, we standardized the fields, the features, with some normalization or some standardization or scaling and so on and so forth. That was at the level of a feature of the data. Here, we are talking of the data per se, data itself. We can get data in a way that is not so easy to analyze and quite often when people try to analyze data in whatever raw format they got, they have to indulge in a lot of transformations, a lot of complicated analysis. So there was a movement that started from the R community, and I'll give you the name, the name escapes me of the great person who started it, his name is Vadej. Anyway, I'll remember that. He wrote a very interesting article in which he said that the data should be represented in the following way, and if it does, these are the benefits that would ensure in our world there is a whole movement called and around that called tidy verse a set of libraries called tidy verse it is first making the data into the tidy format and thereafter doing all sorts of analysis with this. Now, it is fairly well established now. Pandas gives you pretty good support to convert data into the tidy format. We will talk about what that tidy format is today. But in order to do that, I took this example of a weather dataset. This weather dataset comes in many files. It is exactly as the name suggests, about weather in different cities on different days. So imagine that there are weather stations in all these cities and they have measured on different, they have been keeping track of the weather in those cities and the weather is measured through various features such as temperature, humidity, wind speed, and so on and so forth, pressure and so forth. So then when you gather the data, initially when you look at the files of the data, it comes in this format. So I hope you are, I'll increase the font, should I increase the font size more or is it pretty legible on everybody's screen? It's okay. What happens is first of all the data comes in many files. I have just taken three files, temperature, humidity, wind speed, there's pressure and there's more and so on and so forth. I have skipped that because to illustrate the point three is enough, but if you look at any one file, we read this file called Temperature. And by the way, all of these files are available on the course, the same GitHub site. So this notebook is already there on the course portal and you can run it. When you run it, do play with it and see if or how it works so the temperature data just look at the first few rows if you look at the first few rows you find the data is in an interesting format on a given day it tells you what the temperature was in various cities okay so for example in uh iladAD this was the temperature. I believe it's in Kelvin, so don't be alarmed it's not in Fahrenheit. So it is in standard units. Physicists tend to use Kevins. Now then on another day which happens to be this day, the data, this is all fairly old data now, nine year old data. This is the temperature on the various cities and there are many, many cities in between, I believe 30 or so cities in between. So the table is actually very wide. Yeah, five rows and 36 columns. So 36 cities are represented. Obviously, it is showing five rows and it's showing the temperature for different dates. Now when you look at the data, this is as good a way of representing data as anything else, but what happens with this sort of data is when you try to analyze it, it becomes clumsy, very clumsy to analyze this sort of data. And the question therefore is, how should it be represented? So here is a way to think about tidy data. Think of data in a format such that each row of data is one logical unit of data. So for example, you could say, so the context is clear. It's measurements about one context. So pay attention to the words that I'm using. I'm saying one row of data should represent features about one entity in a given situation. So for example, it could be, let's say a child. Suppose you're a pediatrician, you're taking children, there are lots of children there in your database. On a given date or a given age of a child, you can say this child On a given date or a given age of a child, you can say this child named Joe, on this particular date, had this height, this much weight, right? And other biometric features, right? So height, weight, and, you know, size of the, I don't know, the eyesight was this good and temperature was that and so on and so forth. You could measure certain things. So what you are doing is you're setting the context. It is about a child on a given day, isn't it? So that is the way data ideally should be. There should be a couple of columns that sets the context for the data. What is it about? And the rest of the columns, the rest of the feature should be about measurements associated or measurements that are contextualized by these two columns. Let's say that this child on this given day has this weight, this temperature, this height, and so on and so forth. Are we together? Another example could be that, well, you know, if you have, for example, in the past, let's say, what are the data sets that you look at? Geyser, right? On a given day, this was the eruption time, this was the waiting time, right? So you have contextualized it at a given instance, a given time, or a given day. This was how long the eruptions took, and this is how long was the waiting time. Or if you look at the iris data set, for one particular flower, instance of iris flower, this was the petal length, this was the petal width, this was the sepal length, this was the sepal width. So what are we doing? We are saying about this flower, one instance of a flower, the entire row is about that instance of a flower. Getting that right. Now if you look at the data, while this data is as good a way as it is, is it about, what is it about? Is Vancouver, look at the columns, the cities, are they representing measures or are they attributes of something, some object? These are not measurements. Most of these columns are actually context right they are in data warehousing language you would call them dimensions these are the dimensions of the data measurements are here but these columns are dimensions you don't want it like that what you would ideally want it is that what would be a context for a weather one city on a given date for a weather one city on a given date forms a context then for that city one row of data should be about one city on a given day are we all together guys and then you can say that yes then you can have temperature uh temperature pressure whatever it is the different measures of the weather associated with that particular for that particular city on that particular day does that make are we together we want data most of the columns of the data will be measures because in data science we deal with features we deal with measures to a large extent right so you want a column that says city and all those column names to be populated in there yeah instance that is right and so what so ideally what we want to do is we we want to what what you what you say is that this table which is wide you want to turn it around and sort of make it long each row should be let us see the see the column should be the the context should be city and dates let us say and the column should be temperature wind speed pressure and humidity and so on and so forth, weather attributes. Because weather's real feature of the weather weather is quantified by temperature, humidity, wind speed and so on and so forth. Those should be most of the column and only a couple of columns should be given to setting the context, which would be the city and the date let's say. So now the question is, how do we do that? say right so now the question is how do we do that okay now this this used to be actually one of the uh questions uh i used to ask actually an interview i would get into interview people even now i do that so um if you anyone if you come and interview with me for the day job you you may be surprised or shouldn't be surprised if i throw the same question at you. You get the temperature, humidity, wind speed, and all in this format, raw format, which is pretty hard to handle. And I've literally said, you can literally sit there and see people struggle with trying to analyze this. Very simple analysis gets a little bit trickier. They do it, but people find it a little harder to do. So what can we do? If you look at this data, it a little harder to do so what can we do if you look at this data uh it is of course made up of what look at the columns these columns are not measures these columns are actually dimensions or actually there's context of cities things we don't want it like that so how can we change that this is people this is called unpivoting those people people who use Excel, they often call it as unpivoting the data. Or the other way, in Pandas, we think of it as melt. So what is white? Let's melt and reflow into a vertical form. But ideally, what do we want? We want the data to be, each row should be an observation. Each column should be a variable, right? This is the context of tidy data. Roughly each column that is not an identifier or dimension should be a measure. A data frame should represent a logical unit of observables. Stating it simply. Actually, let me fix the formatting And I won't get a chance and I'll post all of this to the updated version to the website. So there we go. Right. It's not on the portal yet, so that's fine. Oh, it is. It is there. tidy data with pandas is not on the portal. I didn't see it. Okay. I'll put it immediately. What do we do? What you do is you take this data and then you call Pandas as this lovely property called melt, function called melt. You say that which data frame are we talking about? You take the data frame that you just loaded, temperature, and you melt that temperature data frame that you just loaded temperature and you melt that temperature data frame and you say what is the what are the id uh what is the id variables is a date time and a variable name city right so when you do that and then you say value name temperature because ultimately if you look at this data once again these are the values isn't it these are the values this is now id of the data is the date on that particular date it is already given to you so one column you just leave it as it is but what you want to do is you want to take the if the if the column headers have to become an attribute of their own, you need to give it a variable name. So that's the variable name. Actually, I wish I could hide this. Let me see if there is a way to hide this column. Okay. Let me remove it. And this also for the time being. Oh, sorry. Can I hide this also? No, I can't. All right. So look at this. Data now, suppose it is in this format, here's the thing. Does this look more easy? On this given day in this city, this was the temperature. Much better. Much better, right? Because if we can represent it like this, and here is a sample of 20 rows, this date on this city, this was the temperature. It feels like a data science data, something very, very natural to us when we do different kinds of analysis. So for example, now you can actually do a describe. When you do a describe, did I do a describe here? Yeah. When you do a describe, see how logical it looks. Given a daytime temperature described, on this day, daytime, how many daytime values you have? There's city values are here, temperature you have. Well, of of course there's a minimum temperature maximum temperature remember this isn't kevin 321 kevin says the maximum temperature minimum temperature is 242 kevins right okay kelvins and uh so so forth so once you can do that now comes an interesting fact you have many data points many files there's temperature there is humidity there is wind speed and there is something else two three other things uh there and we have read all of this data in this peculiar format but what can we do now we can read all of that data see I could read all the other all the other files, I can convert into the same format. Wind speed, I can convert it into the same format. And then what can I do? I can join these measures because for a given day, for a given city, I may have the temperature, I may have the pressure, the humidity, the wind speed, and so on and so forth. So I can now join these measures, right? By collating them associated with a given date and a given city, isn't it? So when I do a join, and to those of you who are familiar with SQL, this should feel very natural. We are joining tables because the keys are the same. If you look at the key as city and date time, then associated with any city and date time, you can have temperature, humidity and wind speed. And we can do that. And once we do that, like this, the observations, they don't get jumbled up when we... No, they don't. Why do they not get jumbled up? Because you're saying collate them, or the index of the thing, the way to bring data together is that only values belonging to the same date, time, and city should be joined together between tables. So in one table, date times, another table's date time must match. That's what you're saying. That is the meaning of the word join. It's a SQL term, join. How do you join two tables? So this is the way of the word join. It's a SQL term, join. How do you join two tables? So this is the way you join the tables, one table for temperature, one for humidity, one for wind speed. And when you do so, you will end up with a nice table. And now this is the sort of table you are used to reading from the CSVs normally from the csvs normally isn't it does this look much more familiar now right uh easy to analyze and once it is in this format then if you do rod this data dot describe uh like it is still raw data at this moment because we have to do some more cleanup so then it will tell you these are the attributes of winds these are the things and if you drop ns you notice that there are a lot of unavailable missing data here throughout right so once you have missing data and you know that the data has already been sorted by date would it be reasonable that if a if a temperature today is missing i could probably take the average of yesterday and tomorrow's temperature right as a sensible approximation that would make sense yeah it sort of makes sense it may be off but quite often it may be right and so then what you can do is you can do a missing value treatment so today i'm introducing you also to the fact that you don't have to drop all the unavailable values, you can actually fill it in. There are many missing value treatment strategies. That is a subject of another notebook, we'll do it another day. But this is how you deal with one easy way that you can deal with is to interpolate. That is, if you know that the data is ordered in such a way, you could just go and interpolate the data is ordered in such a way you could just go and interpolate the data and when you do interpolate the data then you will be you will be fine but occasionally what happens is you cannot interpolate sometimes because too much is missing for example if you interpolate in the first row what happens i said that you can interpolate by taking the row above and the row below and averaging the two. But you'll have a misfortune with respect to missing values in the very first row, isn't it? Or the very last row. Very first row will miss the row before and very last row will miss the row after. So if there are missing values in the first or last rows, you'll have to go and hand treat it then. Right here. We noticed that that's a limitation of interpolate. We can't do it. And so, you know, for laziness sake, I've just dropped it here. So now that we have dropped it, we have the data. Once you have data in this clean format and with missing values, either a fixed or treated. What if two successive values are missing? How does that work? See what it will do, yeah that's a very good question. When two successive values are missing, it will go and fill it with the next values prior and the next values average and fill both of them with the same. Okay. Yeah, none of these are perfect strategies. These are rough strategies that the system comes built up with. One strategy that I use is I often build a regression model on that column as a target variable. And I predict what the value there should be. So interpolation is one of the cheaper or very obvious ways to deal with it. Now suppose we have this and now let us try to do some analysis and see how easy the analysis is. Suppose I want to know the weather in San Francisco. Filter it down. So whenever you filter the rows down, the technical word people use in data manipulation is filtering is what people usually call it. Another word is restriction. You're restricting to a city, which is San Francisco. So do you see how easy it is to restrict to the city? It's very easy to restrict to the city, right? The weather. And so now you can see the weather in san francisco suppose i ask another question i don't care about wind speed just give me temperature and humidity so in pandas how would you do that project down to only temperature and this so you could just say give me only these four columns these two of course are the dimensions and these two are the measures temperature and humidity or you could have done a drop another ways you could have said weather dot drop what would i drop from this what would i do wind speed and then i could have gotten the same result right the other suppose i were to ask see the infamous double square brackets yes the double square brackets a lot of people ask me this question as to why do we have double square brackets when we get the x for I mean when we build the model we use double square brackets right data x and the reason is that otherwise you won't get a panda's data frame right or else it's a series yeah it becomes then it you'll have to limit it to only one column so the columns of a panda's data frame are all each each column is a series right yes good so next question is suppose I want to do this. Tell me the average weather of each city. So there, what do you want to do? Given a city, for all days for that city, you want to take the temperature and you want to find the average. Likewise, do the same thing for humidity and for mean. So how would you do that? Very easy. You say weather group by city. So what happens is all the data associated with the city will be kept in a group. And which columns are we looking at? Temperature, humidity, wind speed. And you take the average of it, mean. So Pandas, actually, for data manipulation, it's very good. It helps you do these things nicely so uh for reference guys not some of these things i mean while a lot of features of pandas you use every day these features exist and they are probably not as commonly used though once you become heavy in pandas you use it very regularly make yourself familiar with this now that you do this means in the column what what is the column that you want to set for this. Now that you do this means in the column, what is the column that you want to set for this? Because the moment you do mean, three new columns will pop up, right? Those columns need to be named. Instead of one, two, three, they should have names. You can give it mean temperature, mean humidity, mean this, right? So you could have just done a dot columns is equal is equal to this right so well here we go means dot sample now what do you get you get the sample so looking at this uh what would you say is the if you just look at this in the list that you see in the sample which is the coldest city that it looks like, you can begin to get and see that here, perhaps Toronto is the coldest. Isn't it? Toronto looks rather cold, Pittsburgh looks rather cold, and Miami certainly looks warm, you begin to get. What happens is whenever you do a group buy, it's one of the inner details. There's something called a two level columns. Columns have, do you notice that mean this, this, but do you notice that city is on a separate line of it's own? You want it to be up there. You don't want this strange notation. These are called two levels in a table. It's a technicality, but at this moment we can sort of forget the technicality and remember that whenever you do group buys, remember to do a reset index. When you do a reset index, what happens is city goes up, also becomes part of a single. And so you notice that once you reset the index, city mean temperature, so you have a table, a summary table. In people who do data warehouse, it would remember that this is, what do you call these sorts of tables? Aggregate tables, right? Whenever you have a dimension hierarchy, now you have aggregate measures, cities, temperature, cities, humidity, city measure. dimension hierarchy now you have aggregate measures cities temperature cities humidity city measure of course those if you don't know are not familiar with data warehouse and of course ignore whatever i said and so now you can look at the value and now suppose you say that you know why should i have this 0 1 2 3 uh let me this index 0 1 2 3 is meaningless the real index is the city the real key of a row is the city itself right in the summary table so what you can do is that you can convert the city into a index itself and sort of remove it from the list and when you do that you will get something like this this is a technicality I'm just mentioning it to you, not specific to tidy data. Tidy data means you could have kept city as the column and index would be any number. But you can choose to make city the index and then temperature, et cetera, like this. So then you can find correlations. and then temperature, et cetera, like this. So then you can find correlations. There is the correlation between temperature and humidity and speed. Are they related? Let's see, temperature is related to itself. Temperature is negatively related to humidity. Does that make sense, guys? Would you agree with that? Rainy days are a little cooler? Yeah. Right, isn't it and our temperature is also uh negatively correlated to wind speed which i hope you would agree windy days are cooler days okay and humidity and wind speed are also related windy rainy days are windy uh windy and wet days go together. Right? Rainy days are also often windy days. So well, you can do summary statistics from there. Then what else can you do? You have the weather data. You could do you suppose in the same table, you want to show a city in the same table, you want to show a city and the mean temperature. How is it relative to mean temperature? You could merge this data, data, you could merge it with the means table. Again, merge is very similar to join. And now you have a table in which you show temperature of a given city and compare it to the mean. So, well, of course the the mean temperature so what does it mean this day is colder than average or hotter than average in atlanta this day first row does it mean that it's a hotter than average or colder than average colder than average older than average it's also rain. It seems to be raining on that day quite a bit. And the windy day. This is a windy day. Let's take some other random load. Toronto. In Toronto, again, we have a rather cold day with humidity is a little bit, it's a drier than that. And wind speed is pretty much lower. It's basically a cold sunny day from what we can make out, right? And so on and so forth. So anyway, this notebook is about two concepts, tidy data and how to, once you tidy the data up, how can you do group buys? You can do aggregate statistics. You can do a join of tables and aggregate statistics and merging of it. So if you can, folks, please go through this table from the beginning. And I can just run it for you. Let's learn it for you. Let's see. And that is it. One thing I wanted to show, OK, there's still more to do. Okay, there's still more to do. Okay, so one thing I wanted to show is, which wasn't obvious. See, when you do mean, you see the city as a column. If you want city to not be the column, you want it to become the index. You can make it the index, you made it the index and so on and so forth. Now, what else do I want to say? Do you notice that when you merge a city, merge it like this, left on city, right on city, values will get repeated, isn't it? Because if you look at this data, we have dates here, right? Phoenix has a date and there are many rows of Phoenix. But when you look at the mean date phoenix here has just a mean temperature there is no date associated so when you join two tables one with two columns two columns as the key date and let's say city as the key and the other one just city what will happen is that it will merge on the city and ignore the date. So whatever the date is, you notice that at the end of it, whatever the date is, all of them, let's see if there's any repetition here, Nashville, Nashville, Nashville. Do you see Nashville? It has one date and Nashville has another date. But the mean values that you join to it are the same, 288 here and 288 here. This exact row and this three columns, exactly as it is, it is merged. This is the nature. If you're familiar with SQL, this will look very comfortable and easy to you. If you're not familiar with SQL, then you have to. If Pandas is the first framework for data manipulation that you're encountering, then you need to spend some time becoming familiar with it. So what would I suggest you do? There are two excellent books. Let me recommend those two books to you for pandas so in other words i'm saying now that it is high time you you spend quite some time becoming familiar with pandas pandas for everyone this is a very good book pandas for everyone and the other good book is pandas in Action. The first and the third book are both very good books. Pandas in Action just came out, is much more recent and has some of the newer functions that people are using. Pandas for Everyone is a slightly older book is small, you can finish it in an hour. And there are about 30 chapters there. Right? So while this is there, one more thing I would recommend is there is Pandas cheat sheet. There are many good cheat sheets, but the one that I there are many good ones, but usually they tend to be big. You can pick any one of them. Let's look at this cheat sheet. Yeah, this is actually a good two-page cheat sheet. I would suggest that, yeah, tidy data. This is the one I was going to tell, because it is literally about what I'm talking. Tidy data, topic of today. I'll put it on Slack. Would that be right? Let'll put it on or may not be is still till it becomes second nature to you in fact i still do that i have this cheat sheets and um what i tend to do is pin it to the wall or just you know tape it to the table when i'm really practicing data manipulation or doing a lot of it i tend to tape or hold keep these cheat sheets next to me, near the monitor. Very easily visible because then what happens is when you are coding, you're not searching the web. You're looking at the cheat sheet and it logically makes sense. So I would like to go through this. How do you create a data frame? We are all familiar with this. We have seen this code so far now. This is it. This is how you create the choosing method chaining this is a again you see method chaining will ignore reshaping the data now you remember that a wide table we made it into a long table this was the melt function that we used you could have used melt or you could have used pivot right A pivot is the opposite of melt. Remember, melt is the unpivot in the SQL language. Sorry, Excel language, unpivot. So if you want to do the opposite of melt, you do pivot. You do a pivot table. And concatenate of two tables is very simple. Concatenate like this. It is here. But the most important functions I would suggest is melt and pivot and merge. There is also merge. We'll come to. Then sorting and indexing, of course, if I leave it, drop columns, which columns to drop. Subset of an observation is simply putting a Boolean clause in between. And how would you summarize the data? These are the ways to summarize it. One of them we used just now, which one did we use? We used the mean. Remember to find the average temperature. So one of the lab exercises you can do is take this tidy data file and instead of the mean, now find the median temperature right or find the standard deviation of temperatures add all those extra columns to the data that could be some good practice for you to do that so do that and merge merge is what we did just now given a column x1 x2 and x1 x3 if we do a merge what hap what will you get you will get x1 x2 x3 you know it's a way of doing a join very similar so you can do join you can do merge right the rest of it is plotting etc etc we won't get into but see pandas it's not a big library of many many functions we are talking about some four functions core functions these are statistical like summarizing data min max median count etc straightforward but the main things that we learned are for tidy data we learned melt pivot and merge melt pivot and merge. So folks, please do take out time and practice. Melt, pivot and merge in your exercise this week. It will really help you later on in life. And have this spreadsheet or this sort of a cheat sheet or any other cheat sheet you like. I like this one. Just print it out while you're practicing all this so that is about tidy data guys and that's all any questions hey yeah thanks uh is the author by any chance hadley wickham wickham, thank you. He did a great service by actually starting this whole tidy data concept of tidy data. And he has done many other things, by the way, I'll let you discover that. He's prolific in the contributions he has made. But tidy data is certainly one of the things. So I'll post a link to a paper so we can probably take a look and see if that's the right paper that's why I didn't want to let me see this original paper that what you are referring to yes the journal is definitely correct uh where is that where did that? Hang on. So I can actually print the, you have to download it and then again goes through there. It got downloaded. One second. Tidy data. Absolutely. Actually, would you mind putting the PDF directly? Yeah, I can do that. It's a lovely, lovely argument. Very easy to read. You can easily finish it tonight, right before you go to sleep. And it's really worth learning. The argument he's making is very simple. Let's not work with messy data. And then he gives a definition of tidy data, which basically means one row should be one, one observations about one entity, like a one context so many many examples he gives in this paper yes this is an old one right if i remember reading it years ago what is the date of this on 2014 12 by then 14 year 14 is there a recent one no no this is it but um hadley wickham has done other things look up other things that he has been up to but this is certainly one of his great things all right guys any other questions Any other questions? I have a separate homework question. Yeah, please go ahead.