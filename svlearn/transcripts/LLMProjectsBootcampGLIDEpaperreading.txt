 I'll wait a minute. I enjoy the images in the paper. Yes, the's get started. About an hour ago, we covered the paper U-Net. If you remember, U-Net was made up of basically something that resembled an encoder-decoder architecture with residual connection synergy. Isn't it? Now between the time 2015 and 2022 or 2021, what happened in between? What were some of the bigger discoveries? Come again? Exactly, attention got discovered. So what people did and so what this builds upon are ideas of attention and attention is heavily used. So glide is essentially that process. And remember, we talked about super resolution. Taking a blurrier image to infill it with more details. So that brings us now in the direction of diffusion models. I'll explain what a diffusion model is. So three core ideas we'll use. Diffusion, or rather reverse diffusion, attention, and unit. And when you get these ideas, basically get it, and in some sense I won't just do glide, I will do the broader thing, the latent diffusion models itself in in a continuous because i don't think that at this moment when if you are reading the paper so i might as well explain all of them together right so the ideas are beautiful they're simple fortunately to grasp but they're really elegant ideas so let's pay attention to it so So we will put three things together. A diffusion process, diffusion, B diffusion models, secondly VUNE, which we just covered, C attention. Right, attention are Attention transforms. So let me go over them one by one. The easiest is attention. What is attention? Suppose I give you text. Or let's just say next to a lake, right, in moonlight. What is it? It's a statement, isn't it? It's a sentence. There is a, let's say that there is a house next to a lake in moonlight. and there will be each word will be paying different amounts of attention to different other words. And that brings us to the self-attention equation that if each token, each token, ti gets broken up into k , query and value . Key is like the location, the coordinates of it. Value is the actual value and query is when you're looking at the word, a house, how much attention should it be paying to other words? So you do that by k dot q. So let's say q dot k, I would rather say given a query vector q i k j v j, right, up to a, if you remember there's a softmax which you can, for the time being, not too much attention to this the subtleties but basically query aligns with which of the key which of the token keys locations and based on how similar it is to each of the keys you pick up proportionate amount of value That is roughly speaking what you are saying. Isn't it? So that is your attention or rather self-attention. So attention we know. So given a sentence, we can break it up into tokens. So you will have tokens T0, T1, Tn and then of course the last CLS token, which will have attentions going to all the other tokens. That is that. UNET, just to recap, what was the UNET architecture? You had condulation and decombination using transpose condulations right and then data would be going through this journey but you also did skip connections why did you do skip connections to bring in the context whereas the forward journey would capture more of the essence of the features in the data, the conjugations. So that was the unit. Now comes the interesting thing. First enhancement. While you are busy with unit, why not throw in, because it's for good measure, some attention units in between? measure some attention units in between. It will make the unit even better. If you remember in the neural architectures we showed, the moment you add attention units, attention heads in between, generally you get more focus, more semantic thing because now each part knows how much attention to pay to other parts and so on and so forth. So that's the attention part of it. You throw in some attention. Like that. Then here comes the interesting part. You took this sentence, you pass it through a transformer, this token, there is a house next to a lake in moonlight, and you pass it through a transformer. That's more specifically the encoder part of the transformer embedding. And what do you get? You get your token vectors, v1, v1 v and v cls so i'll just mark it as this these things came out of it and then the cls came out of it isn't it those final vectors attention vectors came out of it and now what you do is something and now comes the diffusion part that we need to understand we know that we have two things we have the unit that is amplified or augmented with attentions we also have a transformer through which we normally put text and get attention vectors now comes now how do we bring all these ideas together that's the part about glide we'll do that now so we need to first understand what is diffusion see you take a picture oh i like this robots meditating in a vipassana retreat how many of you have been in a vipassana retreat one of you only one okay what going to. So it's funny. So a Khaji wearing a red bow tie and a purple party hat. So now what you do is suppose you have this picture, some picture, or what picture should it be? Let's let it be a house because house is something I actually still know how to draw and let's give it be a house because house is something i actually still know how to draw and let's give it a chimney right here is smoke coming out of that little cottage and give me a moment guys guys please follow this argument very carefully now how diffusion models work that's latent diffusion models work is very, very interesting actually. Suppose, and by the way, I'm going to mix up the glide and do the whole picture a little bit for, because it's worth doing that. Suppose I inject Gaussian noise into it. What does it mean to inject Gaussian noise? For every pixel, right, the eye, suppose I add a Gaussian noise. What is Gaussian noise? It's value I generate a little bit, shake it up a little bit. So if it is dark red, now it will be a little, some it would be reddish, but maybe a little less red. If it was not green at all, the little bit of green has come in, right? So I jitter it up a little bit. So now what will happen when you introduce it, it will look a little noisy picture of the image. So you get a noisy, less Gaussian noise, we'll get a noisy house. Let's say I mark the noisy house like this. And I just add some noise. You can still sort of see the this. So this is step one, this step zero step one in me i repeat the process more gaussian noise right add more gaussian noise and then it will become a picture that is much more and maybe there is some outline of a house. Right? And I keep doing this till after k steps. All you see is you can't figure out perceptually where the house is. Generally, if k is equal to infinity means all information is lost. In reality, you don't run it infinitely an infinite number of times. Practically what happens is it loses information rather rapidly. You, huge difference. And by the time you run, the forward process of diffusion. Are we together? Now comes, why in the world would I do that? Why in the world would you lose information? And there comes the reverse part. See, you realize, first of all, there's something interesting about it. First is that it's a Markov process. That's a big word. What is a Markov process? Amrit? Yes, history is lost. So the picture that you get here, let me call this step two, the picture of step two is derived from picture of step one. And it doesn't matter how you came to step one, that picture, right? It has no relationship to the original picture. Do you see that? So an example is a drunken man. So suppose you're very inebriated. Today is Saturday night. So some of you do look a little bit Saturday evening. So I can see the effects. Are there in your backpacks little bottles? Who knows? So, OK, so what happens is suppose you're deeply drunk and you're walking home, but you have no idea which way the home is. So at any given moment you make a step, you end up somewhere, but you have totally lost orientation. So the next step you take and where you end up depends only on where you started with, your previous position, isn't it from the previous position in 180 degrees or 360 degrees you could have taken a step in any one of those directions but you realize that where you land up in the next play after one more step has some relationship to the where you were now because it will be in the vicinity of that place but has no relationship to anything in the past you have no memory of that right so that drunken man's random walk is your mark of process in a way right so i i trust you all have a guilty look on your face. So this looks very familiar. OK, so there you go. Now, is there a way to reverse it? And that is the insightful question to us. Diffusion process is okay from information to noise. But if you make this as tiny steps, you could argue that if I take a perfectly clear picture and I add a little bit of noise, surely I should be smart enough to filter that noise out. Does that make sense, guys? Does that make sense, guys? Right? Are we getting this? So I should be able to, or how could we do it? Amongst other things, I could use super resolution. Or I could use UNET or something like that to just infer the noise mask. So suppose I could do this. I could say, let me call this A, B, C, all that. So suppose I know that B is equal to A plus noise, right? A Gaussian noise. And what? Some noise we have added to the pixels. And suppose you've trained a network, you take your u-net. So let me make it as u-net like this. Would that be better? Just to remind you it's not just with a residual connection, skip connections connections it's a unit right suppose you give it b and you say learn the reverse diffusion process namely what you need to produce is noise noise, the noise mask. That's very interesting. You're deliberately not asking it to learn the cleaner image. You're not saying, give me A. You could have, but you're instead saying, tell me what noise did you find what is the gaussian noise that you find sort of embedded in b are we together so suppose you find that noise mask here right let me just call this the noise mask mask, then what you could do, you would argue that A is B minus the noise mask. Would you agree? Right? Because ultimately, that's what it is. Right? I could recover that. Am I making sense? And usually what you do is, you do it a little bit say in latent models, while this makes logical sense, what you do is you play very, very carefully. You don't trust this learning completely. Right? How would you learn this? Your standard, you know, you train the neural network and blah blah blah. You get it. We have done that. What you could do is you just subtract a little bit of the noise. So you don't strive to get back the perfect image. But you just want to make a small step in the right direction. In each iteration are we adding the same noise mask no it's different different and at each stage you want a unit to learn to unmask or undo or learn the noise mass that has been added interestingly you're not asking you to learn the original picture but then because noise has a well-defined shape, isn't it? It's Gaussian, it's bell-curved. So irrespective of whether it was a cat or a dog or a horse or a house, noise is noise. It has a bell-curved distribution. And we are asking you to determine that. Go ahead. But this noise is random. It's random but of a shape, not purely random. It's a Gaussian random normal. Right, so the pixel values, if you plot the deviations from zero, they will show a bell curve distribution. Models in your sets, right? So you can do that. You say that sounds like fun. We are up to something. So you can take. So if you just assume that a picture has been converted to less of a picture to all the way to pure noise. Then I could use the reverse process, reverse diffusion process, theoretically, to get back to some approximation of the original, step by step. Right? The trouble is, if you end up with pure noise, it's a little bit hard to recover the whole thing. You need some guidance in other words that random complete pure noise i could have shaped it into a dog or i could have shifted it into a house or i could have shipped it into a rocket isn't it by different ways of eliminating them so we need something to prompt it in the just just nudge it into the right direction. So the way I look at it is this way. See, when you take noise, in it you can imagine some structure, right, and there's an infinite number of structures that you can imagine that it has. And by selectively taking different noise masks out of it, you can sort of impose the structure because it's a reversal. You take some structure, add noise, and then you're sort of taking it out. All that the network is learning is some things don't make sense and some things do make sense. For example, if the network has only seen digits, then it will always create a noise mass that if you subtract from that, sort of digits will begin to gradually appear, because those are the only real informative shapes at most. Isn't it? So it will gravitate towards those final states. So now comes the idea. So now we come to, so now that you know the diffusion process, let's try to get smart and see what Glide does. Glide was very interesting actually because before that came DALI, which was based on GPT models, purely creative models, which shook the world, but it could create a carrot on a radish taking a dog for a walk or something like that right but uh uh it couldn't create very photorealistic images so what this thing does is it says let's put these three ideas together, diffusion model, unit, and of course it would be a criminal offense to do anything without a transformer thrown in somewhere there, and attention is thrown in somewhere there, isn't it? It's the new world of attentions and transformers. So let's put all these ideas together. And when you do that, what you get is, now come back to it. We have attention. We have transformers converting a text into this prompt. Prompt for guidance, you know, text to guide it. Nudge its way towards some object, some shape. Now what you do is, what is paying attention to what? What you do is you is paying attention to what what you do is you make this attention there pay attention to each of the tokens like have all sorts of complicated uh thing sitting there right and what you do is when you have all these attention heads paying attention to all your transformers tokens the text token what happens is it becomes a text guided reverse diffusion do you see that right it becomes a text guided reverse diffusion now one thing to remember guys that and this is something people tend to get wrong that that in the reverse diffusion, right, whatever noise mask you're discussing, you don't just subtract the whole of it. Subtract a little part of it because you never truly fully trust the noise mask. Because you do it slowly, very slowly, and because it's a guided process over and over again, the text keeps prompting because you just remove the little bit of noise, some outline emerges, and again you pay attention to the text. Again something emerges, and again you pay attention to a bit more text. Are you getting it? So there's a lot of continuous focusing. What are you asking me to make? Ah, a house by a lake in moonlight. So that continuously, it's like an echo chamber. You keep hearing the sound over and over and over again, metaphorically speaking, as you go through the process of reducing the noise. So what does it do? It forces the neural network to sort of bring out shapes that somehow resonates with this. Making sense? Right. And now comes the approaches that this talks about. One is sort of a clip-based approach. Let me talk about that. So suppose, look at this. Clip-based approach is sort of an ablated steady kind of approach. See, suppose I gave it noise and I said, okay, come up with something. What would it do? It would reverse it into and produce you something. For all you know, it might produce you not a house by a lake in moonlight but it may produce you a tractor and say well what a splendid job i did i produced a tractor isn't it now on the other hand if you do um voice guided i-guided, then it might come up with something. So how do you make sure that what it produces is really trustworthy and aligns with the text that you're doing? There are two sort of approaches you can do. One is obviously this, guided with the text, as we did, because without guiding it with text, it could produce God knows what. Isn't it? So the other way you could do it is there are two approaches. Approach number one, clip. Clip guided. What is clip guided? What Now what have you been using clip for, you guys? Do you remember? To create caption images. But also what does it do? what does the clip thing ultimately produce it's a model that produces what a sentence embedding a sentence vector isn't it so this text this sentence s will produce a vector s are we together now suppose you go through this guide, like this process. Keep on doing it and you end up with a picture that sort of has a house by a lake. Here is my lake and here is my moon. Does it look reasonably realistic, guys? Here we go. So here is my house by a lake in moonlight. Now I can convert, clip does what? Clip converts an image also into a vector. So this will be the vector embedding of the output image. Right? And now what can I do? I can ask myself during inference that, hey, by the way, embedding vector of the text so that the score between the output and the input center, the prompt, is high. You could do that, isn't it? And therefore, you not only don't trust the decoding process like you know the prompting process the transformer during the image generation but you on top of that also bring in the power of clip so i brought in two powers isn't it you not only trust the standard attention mechanism of the transformer but you brought in the contrastive loss kind of thing and so now guys you i hope you're getting a sense of why I started this entire workshop bootcamp with, what did I start the bootcamp with? Sentence encoding, right? Because you see it appear again and again as a helper. So that is one way of producing an image. There is actually another way to produce a better image which they found to be useful. They asked this question, it's sort of an ablation study kind of a thing. Suppose suppose this sentence was empty. I take two scenarios in which the guiding sentence is just empty versus the guiding sentence is a house next to a lake in moonlight. Now what happens? You would imagine that if you don't give it anything, it will produce some image, random, something it will produce, not as good. Whereas if you guide it, it will produce, through attention mechanism, a much better image. Would you agree? So now think about it as vector space arithmetic. The idea is very beautiful and simple. Suppose here is the picture. Here it is with text, with prompt. Prompt. Prompt guided. So when this image you encode it becomes this way right and suppose you produced an image which was without any guidance whatsoever some some image no word no words now what can you tell about this journey from here to here? As you go from this blue vector, actually, let me change the color. If you pick any point here, if you take this vector, would you let me call this vector a or this b what can you say about vector c would you say vector c is at least better than vector b right because it's in the direction of the right thing what about vector d even better a is of course better than both C and D. And here comes the funny part. What about vector E? It is even more in the direction of the meaning of the sentence, isn't it? Does that make sense? And so one way that you could generate good images is actually at each stage, given the voice prompted image. From that, you find this difference, this gap. And you instead, this amount, you instead go in the opposite direction. Take this vector and you produce this vector. I need a lovely new color. Then you produce this vector, this vector. You produce this vector. right? Which is basically this vector that you learned, which was the gap between this vector that you learned, which was the gap between B and A, the difference, added back to A to produce E. You see that? It will make the picture even more realistic, hopefully, or align with the text prompt that you gave. Does that make sense, guys? And that is blight. There's a second method of blight. It turns out that that is actually more effective. It's just vector arithmetic. You see how simple it is? So in case you have forgotten your vectors, now might be a good time to learn it because machine learning is all just vector arithmetic a lot of it is just vectors probability theory vector transformations calculus are you taking a question, Asif? Go ahead. I kind of at a very high level, I think I appreciate very well what you're trying to say. But let's assume that we take house, lake and moonlight, right? Yeah. There's a proportion to it and there is an order in which it has to appear, right? Say, suppose I say that I want the moonlight shadow to be on the lake and my house has to be in the background versus my house has to be in the foreground. So the way that those three proportions it will try to make it close would be from its prior memory of whatever the clip it has taken and three things it will try to put together. How will it put it in the right proportion no because of the because of the image embedding in a way um see the text because of the contrastive laws right when you say the house is in the background right and i only see its reflection in the water you are using words that mean something and that semantic vector will give you, basically what that will give you is, let me use a different color. I'll use green. So are you saying that if I give a more detailed sentence, that painting will be different than just a place? Yeah, try that actually. When you change the sentence and the more, if your new sentence is semantically more informative, your picture will change. I see. But it should have seen those things prior, right? I mean, like at least the proportions of it. No, no, no, it doesn't. Moonlight leads to no lake and house, correct? That is right. By being trained on millions and millions of pictures, hundreds of millions of pictures pictures it knows all about houses and lakes and moon lights so i was playing with the lion data and i asked a dog chasing a bird it only showed me one picture out of all the 400 million but that is because it's the only picture that landed close 400 million believe it or not is a minuscule data set remember we are talking of a thousand dimensional space in a thousand dimensional space what is a formula it does not even qualify as a state debt no 400 million billions or trillions 400 million i meant yeah 400 million is a minuscule number. Miniscule number, I see. So if you give it a dataset to search for, because you're doing image retrieval, even though it knows the meaning, it may not be able to find a good match for you in just 400. You need to give it a dataset to search for in a few billion, because a dog chasing a bird is not as common a event imagine trying to try to take a picture of a bird i mean a dog chasing a bird it's an extraordinarily hard picture to take and for that reason there are not that many pictures available of that but if i ask it to create that it will create correct it will create a dog it knows a dog it knows a lake and it would know a bird and it will try to put these three things together in the image format correct it will do that and in fact there's a paradox in the amazing paradoxes in a way while it may not be able to find an image that matches your criteria because such an image may not exist in your minuscule data set up for hello Hello. Just froze. Did he freeze or I froze? He froze. I think Asif froze. So how do we find. Um, it was, oh, he dropped out. I think he will have to rejoin. Post a quick message on Slack. Okay. Hopefully he will be back soon. I was seeing some of the capabilities of GPT-4. Ah, yeah. And it can guide you through an entire servicing of machine parts and things like that. That's nice. It does image and basically context. So if you are a mechanic and you can take pictures of it and say, what do I do next? And so it can give you a sequence of steps. It's quite scary. Oh, that sounds like looking up YouTube videos and how to do repairs. Yeah, but I don't know how it is able to figure it out. So that's Maybe it's accumulated all the YouTube tutorial knowledge. I don't know where all this is going. Kate, I'm assuming this was the last session right or the last paper yeah yes i i believe so well it was the three things that are going to be oh there are connections asif is trying to log back in so there's three things he was uh three papers he was going to cover the um theET, the GLIDE, and then the latent diffusion, and I believe he was covering kind of a combination. Harini is urging us to wait. Yeah, because generally we at least go through until 5 to cover the paper readings, right? So hang tight for a little longer. you I I think he keeps his machine on and that zoom something there's probably a memory leak in his that whatever the notepad or the windows thing he has I think that's right yeah I know I stopped trying to run jupyter notebooks during a Zoom session because it would freeze me, my machine up too much. So it's like just focus on the Zoom connection. you A quick question for you, more on the logistics side. If you are having a Zoom session, can you, Simon, how do you do the YouTube live, given that a session is going taking the Zoom recordings and posting those on a Google Drive to save them. Kicking off the YouTube live streaming, it's kind of a new thing that I've seen Asif enabling. We saw him use a persona and do that part but um it looks like the streaming can continue even uh if the zoom session continues even if the person who kicked it off the host like gossip does not so so apparently it's still recording oh there he is all right folks i'm back i apologize i have no idea what happened but technology yeah technology the the rebooting seems to have helped so where am i let me share my screen again i hope i'm sharing it. Yes. So guys, the idea is that you train a network to become very good at converting, not basically Gaussian noise, into a shape that is semantically equivalent to the text that you gave, from that you gave. So look at how well it does, like Glide does. The results are actually very, very impressive. Now look at this, a hedgehog using a calculator. I don't know about, I've never seen a hedgehog use a calculator, but it seems pretty realistic. A courtier wearing a red bow tie and a purple party hat. Now robots meditating in a vipassana retreat. So now this is a very obscure one, if you notice, unless you know what Vipassana retreats are. How many of you know what it is? Only one person in the background. Okay. So these are what this meditation retreats are supposed to be completely quiet. And a whole lot of people, they get absolutely crazy at the very thought of being quiet. They can hardly be quiet for one hour, far from 10 days. So this is it. Then, so you can see that these robots are totally quiet. They can hardly be quiet for one hour, far from 10 days. So this is it. So you can see that these robots are totally quiet. A fall landscape with a small cottage next to a lake. This reminds me of my life upstate near the Canadian border. But while it is very impressive, do you notice something? The prompts have to be very evocative. Semantically, they have to have rich content. Isn't it? I mean, even the style is transferred, right? Like you can see the different Starry Night or Salwar Doordali, right? the different uh starry night or uh Salvador Dali right yes exactly a surreal dreamlike oil painting by Salvador Salvador Dali of a cat playing checkerboard right and it gets more and more interesting a painting of a fox in the style of starry night. By the way, what was Starry Night? Do you guys remember? Van Gogh. Van Gogh. Yes, Van Gogh's painting. That's that. So it has figured even that out and it knows what the Starry Night painting is. At this moment, I would say that that is when you, at least I begin to feel foolish next to one of these AI models. next to one of these AI models. So, Asif, a question for you. Is this, like if somebody questions this, like why it did that, are people able to go and explain and say that this is how this painting was generated and give it fully explainable, or this is still yet a mystery? No, it is not a mystery. Actually, let's look at this painting of a fox in the style of a starry night. When you say starry night, it pretty much knows what you're referring to. You can do an image encoding of the starry night vector. And the painting of a fox in the style, we can pretty much see that it will guide it to something like that. So this is actually, given the prompt and given how this algorithm works, it's about as interpretable as it gets. If you know, the details are in the process. So I was looking at it from, say, different domains, right? Like if you want to give a solution for a different domain, which is not a painting or this, and you have the prompt and then you see the end result of it, can you turn into the building blocks and say, Hey, this part of it contributed to this portion of the solution and something like that, where you have a chain of events, chain of thought processes that you bring together and say, this is why we think this is the right solution for whatever we are trying to do. Sort of yes and no. Let's just say that it's sort of vaguely interpretable, but not absolutely interpretable. Many things you can't be too sure of. So the- For example, why is this fox having such a huge tail? Right? Van Gogh never painted a fox with such a ginormous state. So not everything is clear and something suggests artifacts of the model. If you think about it so long as it is looking foxy enough the gradient descent and the learning in that area won't matter so much anymore. Isn't it? In the image generation process, stage by stage, as you're trying to create super resolution images, allowing with the prompt, it will stop improving at some point. So something has to tell it that this fox is not really realistic. The tail looks bigger than the fox itself isn't it so you literally see the weakness in this image and this is what was true by the way with all of these right if you look carefully you would see some artifact that shows that the process is not perfect. I don't know if it is obvious, but to me it seems that as a painter, like if you make a boat, do you notice that this boat's median line tends to be bent? Looks to be bent? Yeah. It's a crooked boat. So things like that in all of these images, if you stare at it really hard, you will see some artifact that may give you a clue that, you know, that whole process of somehow shaping that noise into some meaningful or text aligned or prompt aligned image. That leaves a little bit yet to be like some imperfections still there. Is there a quantification for this kind of imperfection to be able to better the learning yeah yeah in fact it's a very active work as you know mid journey does a better job than stable diffusion and stable diffusion has improved and people have learned a whole lot of prompt engineering uh following this there were other lots of papers like dream booth etc i won't when you do that Generative AI course, I will go into much more detail. But because in this one we have a lot of territory to cover with LLMs, I won't go there. But in that, in the other course, see at this moment I'm just scratching the surface. Think of it like that, just scratching the surface. We go pretty deep and we cover a lot of papers and we see the hows and whys of it and what has been done to make these things better. So, alright guys, so that is that, and with that I conclude today's paper readings, all the paper readings. Now, is there any team willing to do a presentation? Not really. So one thing I would like to do is move straight over to the Cambrian Explosion event. Would you guys like to do it and finish it all? Then you'll be free and you can decide to stay or go or do whatever you want. I hope each one of you brought some ideas. I'll just stop the recording here.