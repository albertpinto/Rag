 So last time we didn't do enough labs on clustering. In fact, I didn't post the code online. This time I realized that we need to do more of clustering because it's obviously used heavily in the industry. So what I did today, I brought labs, or there are labs online, for two types of clustering, K-means and density-based. I will add the lab shortly for hierarchical clustering, spectral clustering, and maybe for optic, just so that you can compare and contrast the results so i will start the clustering exercise with a data set which i call the blobs data set the name will be descriptive as you see so first we load this data and we are going to try to cluster i said Asif, we can't see your screen. You're not sharing it yet. I apologize. Is it better now? Yeah. Okay. So we'll start with this dataset blobs. And we'll apply k-means clustering to it. This dataset is again available in the GitHub. The usual code, one of the first things to know with clustering is you should scale the data why do we scale the data raw data why should we scale it if you don't then your all these minkowski norms your measures they won't perform well example, if you measure the height of an elephant in meters, but if you measure the weight in pounds, you realize that it makes doing any kind of distance measure very much skewed, isn't it? Mostly it will be about pounds variation than weight so two things first of all because units are involved you should make the quantity dimensionless scaling it within a range of zero to one or using the this standard scalar which will do the Z value, as we talked before, it makes each of the features dimensionless. Most values are between minus two and two, minus three and three. So that is one, we must scale the data. Whenever you do clustering, do that. Now, what I have done is in the BLOPS data, you'll see me deliberately take this data, scale it, create a data frame, and then I have added the label, whatever the initial label was for the data. So then, now, when you have clustering problems, you don't have the luxury of labels. You don't have the luxury of knowing which cluster a point belongs to. The whole point is that you're going to cluster and find clusters and assign points to clusters. You don't already have that. But here, because I generated this data myself as a hint to see how algorithms are performing, I have in a way, in a separate column, put the real cluster IDs that they belong to, the ground truth. The ground truth in most situations is not known. I put it here just for us to learn or compare the algorithms or the k-means for different parameter values. Now, it isn't always true that you don't know. Sometimes what happens is you do know it for a few labels, but not for a lot, I mean, for a few records, but not for a lot of records. records, but not for a lot of records. So then for those few where you know you can go and see how well it did in assigning the clusters, the points to the right cluster. So anyway, when you look at the data, think only of X1 and X2, don't think of the label at this moment. It is not part of the clustering exercise. It is our later verification of how well the clustering went. So we take this data and we visualize it. The visualization code is straightforward. You do a scatter plot. And when you do a scatterot, what do you see? What can we say about the structure of the data? The clusters seem well formed. They seem reasonably well separated with a little bit of overlap. And are the clusters globular? Yes, they're mostly globular, they're roundish, convex shaped. So They're mostly globular, they're roundish, convex shaped. So this is a place where we do expect some degree of success with k-means, remember, from the discussion we had. So let's try that. When you plot these things along the x-axis, this is how it looks. And you can see why it would look like this. You would have one hump here, one little hump here, and one big hump here, which is why it is big hump, little hump, big hump, along the x2 axis also. Now, you do a density plot. It's pretty much a kernel density plot. And the kernel density plot, and I'll at some point take some time to tell you what kernel density plot is. It is literally a density map, lines which are at the same density of points. That's density it is literally a density you know map lines which are at the same density of points that's all it is so this is the cluster clusters these these are the density just creates the contour contour lines showing points of equal density looking at it obviously you can see it's fairly globular data and it it is well-peaked. It so happens that I did actually do that. We know the answer. I created it like that. So let's do the k-means clustering exercise. Now, remember that k-means has a free parameter, k, parameter we need to tell so just look at the inner loop we'll first look at the inner loop came from speaking right yes so k you apply k-means and you say how many clusters you want in the range of one to ten one to nine you fit k-means to it, and then you ask for predict. Now, you may be confused and say, well, wait a minute. This is not a supervised learning exercise. Clustering is not about making predictions. But it's a slight abuse of term, but it works because everybody is used to fit and predict for regression and classification. Predict here means tell which cluster it belongs to. And so we can get all the Y cluster assignments for all the data points. And then we can, the K means calls the WSS, the inertia, well, sort of related. It is the inertia and we append it into a table. Now we do this exercise that WSS is an empty list here. We do this exercise for in a for loop one to ten. Why are we doing it for different values of k? Should we do a scree plot? Yes, because we are, we will hunt for the optimal k, right, and to draw the scree plot. So that is what we do. First for loop is this, and then what we do, we plot the clustering for that value of k. Let us plot it and see how it looks. So I've tried out nine values. It made sense to make a three by three grid. This makes a three by three grid. This makes a three by three grid. Now, the rest of it is by now looking pretty familiar. Actually, five was not needed. I could have made it five, but I mean, three, three by three is enough before what it is worth. I left the option open to add more values in the range or k values. This is it. The rest of the code hopefully looks straightforward. So let's look at k is equal to one. Just find one big cluster. That is obvious. It will just take the entire data set and call it a cluster. Would you call it a good clustering? Probably not, right? Then for k is equal to 2, it has assigned this. Would you call this a good clustering at k is equal to 2? No, not even here. k is equal to 3 doesn't do it, right? But it seems to be getting better. It's trying to differentiate. k is equal to 4. It seems to be doing a pretty good job. And K is equal to 5. It seems to have gotten it. This is perhaps the best one. Perfect. And K is equal to 6. It begins to degrade. You see it getting worse, right? And thereafter, the clusters begin to split. You can see. So by the time you reach k is equal to 9, it is pretty bad. Four of the five clusters have gotten split. So any thoughts here, any questions? So Asif, how would you do this in higher dimensions? I mean, like it's not possible like these are very nice set out data points and all right. And I kind of understand the creep or not also. But is there a way to get this kind of appreciation saying, yeah, we did not split something we were not supposed to. And we didn't bring two clusters together, which we're not supposed to either. Yeah, that is where the weaknesses of these things. The first thing is that clustering is usually done along with dimensionality reduction. You saw that when we did this clustering, one of the crucial steps was a huge dimensionality reduction. Likewise, you could have used PCA or kernel PCA to reduce the dimension of the data and then done clustering. Very common to first reduce the dimension using PCA, kernel PCA, and or use methods like spectral clustering, etc., which effectively cluster in a much lower dimension. So at that point, do the dimensions and the number of clusters kind of come very close to each other? Is there some number which says that, yeah, this is kind of looking, the data is showing this and I think the clusters are also, they're kind of fitting into, say, five or six different boxes of their own. Is that what usually happens? See, what happens is if the low it is called how much of the variance of the data the lower dimension captures, roughly speaking, if the lower dimension captures the essence of the higher dimensional data, then the clustering in the lower dimension is a fairly good representation of clustering in higher dimension, it works. But when you lose a lot, it's a very lossy projection to lower dimension. And then what happens is even if they are clusters in higher dimension, you don't quite see it. So imagine one easy way to imagine is imagine a blob stack one top of the other in the y axis, right or the z axis. Imagine xy plane and along the z-axis, there are five blobs, they are separated, but stacked one on top of the other. If you were to project it down, it is quite possible that it will look like just one blob. Things like that, I mean, at the top of that, I would bring an intuition like that, that if you can't, if the PCA mixes up things too much, then you lose the clusters, which is why you can also try clustering in high dimension. Trouble with high dimension is sometimes there is no way to make progress. If the number of dimensions is huge. Remember, the variants of the distances all begin to look pretty similar. That is the main problem, the curse of dimensionality. You have to go pretty far to find the neighbors. And it turns out that almost everything is not much further than the neighbor. That's a problematic aspect. So in other words, the whole theory of unsupervised learning, in my view, it's still evolving. It is like a lot of these things have a huge scope for improvement. So visually, if we cluster, so we see that five is the appropriate number in this cluster, isn't it? K is equal to five. Let's see what the scree plot says. When you look at the scree plot, it's not so obvious, but I suppose I'll just decrease the font size, guys, so that we can show it in one screen. Look at the scree plot. You notice that there is a pronounced bend at 5, k is equal to 5. But at 4 also, there's a bit of a bend at 3 also and two. And so this is one of the things you have to remember that with scree plot, you have to ask which is the biggest bend, biggest angle bend. And the biggest angle bend, if you look at it, is at five. And so five is the right number of clusters to pick. So you have to get a little bit trained to pick that because it's not so obvious where you'll pick ELBO. So you could have picked ELBO here at three, but it is distinctly at five, those. And that is that. So when you write K-means clustering code, I would urge you to follow a strategy very similar to this. Do the clustering for various values of the hyperparameter. Now we will do the same thing. So any questions, guys? Is this code more or less self-evident or do i need to explain anything in this so ask if you take tenors you have to mute yourself yeah so when you choose the loop right you've chosen 10 is that how you do all the time and then the loop right you've chosen 10 is that how you do all the time and then and then if i don't get your elbow then i go further okay right so what if there were 16 real clusters i will learn because the data will guide you then you increase that's a good start that's a good start because in real life what happens is there's a computational cost also right so so this is the screen plot do that obviously you don't need this annotation the screen plot is just a simple plot this is the annotation report because i wanted you to ponder over and realize that actually this is the best point but this is as good as it gets now let's do the same thing using dbScan clustering. So for dbScan, I have created a little utility for you because this code can get more involved. I would strongly suggest that you read through this code carefully. I will walk you guys through it. So there is this, let's look at this code. It says you cluster. What are we getting? Epsilons as a list of values. Why? And neighbors as a list of values, list of integers. Why is that? and neighbors as a list of values, list of integers. Why is that? First we realize that a given epsilon radius of neighborhood is a float, is a number, real number, right? But we have a list because we are going to try different values of epsilon. And we are going to try different values of the number of neighbors needed. Do we remember dbS had two hyper parameters epsilon and neighbors number of points in the neighborhood right and lastly we of course need the data frame that we are going to cluster on so for this data frame the first thing is I have sort of chosen a hardwired rule. If this answer is in quotes available, you know, for some data or for data, then the data must contain a column called label. Right. And so if it is, then it is a label data, otherwise not. So when data is not labeled we can do this we can take epsilon and number of points now the db scan cluster will tell us how many clusters there are isn't it db scan automatically you don't have to tell how many clusters there are it simply it will find it on its own and tell you that I found X number of clusters. And we'll see that in a moment. So that is it. This is our output. And this is a metric on the clustering. Silhouette score, it tells how well the clustering went. If the data happens to be labeled, in other words, there's a label column, we will also compare the clusters against the known labels to see how well the clustering went. So there are all of these measures to tell how well it went. So I won't go into all of these. I'll leave it as an exercise for you to read. So I won't go into all of these. I'll leave it as an exercise for you to read. It's a two-minute read on the Scikit, well, not two minutes, it's a 10, 15-minute read on the Scikit-learn website. And so first what we do is I create a data frame to store the quality metrics for each value of epsilon and number of points, each pair of hyperparameters, I want to store what was the metrics from all of these different methods, silhouette and this and that, homogeneity and so forth. So I create an empty data frame to store the clustering quality metrics. Then I do know that I'm going to plot it out. So let me give you an example of what I'm trying to do when I cluster it here do you notice that I've plotted it out for different values of epsilon and a number of points right those are the two hyper parameters so I've essentially I have drawn the plot. So this is a very complex plot. It has a lot of subplots. Do we see that, guys? It has a lot of subplots. So therefore, a little bit of caution has to be there. You have to be careful. You have to first know how many rows you are going to have and how many columns. So each particular value of number of points in the neighborhood is a row. So, for example, if you go here, do you notice that it says number of points is 10? Here also 10, here also 10. So row, in a row, number of points is fixed and in a column num the epsilon is fixed right you notice the epsilon here is 0.10.1 and so forth so we create the matrix so a little bit of care has to be there that you count this and then you create so many subplots now how did i come to this number eight and five when i'm doing the figure size figure size is important guys matplotlib if you get your figure size figure size figure size is important guys matplotlib if you get your figure size carefully then the plots look pretty right so i've given basically eight units to row and five units to eight is to five ratio very close to the golden ratio and golden ratios things look good anywhere near the golden ratio so what what's a golden ratio sorry golden ratio is the thing golden ratio is all over nature whenever you see something beautiful you'll see the golden ratio lurking around so there's a whole obviously wikipedia entry on the golden ratio but it is this value 1.6. So you divide 8 by 5, what do you get? 1.6. It's one of those artistic things. See, like I told you, in a past incarnation, I used to be a painter under an artist. And so you picked it up along the way. So it sort of gets built into you that if you can't think of a better answer, just pick the golden ratio, which is why I picked it like this. I see. Peak size, does it take care of the resolution of the monitor? Are you able to factor that in see you can set the dpi i think in my support vectors common i've set the dpi of the image so so there we go it's a cluster for each company but most likely if you don't set the dpi it will matter right how well it looks will depend on that partly so so now what do i do for every combination of n points and epsilon i have to do the clustering isn't it i do the db scan clustering so for that i've created a function what that function returns is the cluster the actual clustered result and the cluster it turns out that the clustering the way clustered result and the cluster. It turns out that the clustering, the way dbScan does the clustering is interesting. It will label every point with a number. And every cluster that it finds, it will number it from 0 onwards, 0, 1, 2, 3, 4, 5. But all the outliers will be given a unique cluster id it will be called minus one so when associated with a data point there is a label of minus one it means that it is a outlier outlier right so when you are counting the number of clusters first of all you need to count how many labels there are right unique labels you find but then that unique label may contain minus one if it is minus one you need to subtract one from it from the total isn't it because if you have five if you find five clusters it will give you minus one zero one two three four there's six values you have to you have to subtract one from it. That is something you have to be cautious about and remember to do it. Another limitation is if the number of clusters is one, you can't find the silhouette. It's a limitation of this method. It will throw up. It will error out. So you have to put a guard against it. it's just a api thing that don't do it if you suspect that number of clusters found would be one it will fail so if it is one then you pre-assign it to a zero value silhouette right and so now what i have i know what epsilon was i know what the number of points was i know know how many clusters were found, and I know what the silhouette is. Right. So these two are the results, part of the results. Silhouette's score is the metric on the clustering and number of end clusters is the number of clusters for epsilon and endpoints hyperparameter. a number of clusters for epsilon and endpoint hyperparameter. Now, if the data happens to be labeled and we actually know the ground truth, we can do better. We can take, now we have y and y hat, pretty much similar, and so we can compare y and y hat, right, against each other to see, using a whole variety of measures, and come up with an answer. Right. And we will see that. And so we store all of that into a quality, uh, data frame that we created empty data frame that we created. By the way, this is Pandas way of adding a row to a existing data frame, appending it at the very end. It doesn't have a direct append method. This is, I mean, this is what I've gotten used to unless they have added append method recently for rows. And you plot it out with this generic title and so on and so forth. So I did that. Now there is this dbscan cluster. Let's go and look at it. This is the function here. It contains some of the complexities so first thing i do that if you pass me a data i will deliberately cluster only in the first two columns right at this moment uh because i wanted to illustrate this point uh look at the clusters and plot it so deliberately i did it it like that for the sake of these labs because we are looking at two-dimensional data. And I called the axis, I renamed the axis to X1, X2, independent of whatever data that you have, original feature names. And then you do the DB clustering. You find then in the data, you append the predictions, you know, Y hat fit labels, cluster IDs, you find then in the data, you append the predictions, you know, Y hat fit labels, cluster IDs, you append to this data, then you find the number of clusters, which is here. What do we do then? We want to separate, I wanted to separate out the points that belong to a cluster and points that are outliers how would i know which points are outliers y hat would be minus one right so i see do you notice this is my way of this is my way of splitting it filtering it down to either situation in clusters are those data points just cluster points are those data points whose label is not minus one outliers are those data points just cluster points are those data points whose label is not minus one outliers are those data points whose label is minus one then this is uh by the way this is a trick because people keep asking if i don't know how many clusters will be discovered how do i give colors to use so this is the way to do that what you do is you you create values between zero and one right and how many values let's say there are 10 clusters it will create 10 values and for each value then now you go and get because it's a fractional value you can go into one of the color maps of matplotlib color maps are spectrums what is a color map let me just do that here mat plot color map let's look at the reference. So do you notice that this is the standard reference? I will increase the size. This is how you can pick up. So when you say it, and typically you can ask for it on a scale of 0 to 1, and it will fall somewhere along the spectrum here. So there are many. If you want to distinguish things out vividly from each other, use one of this. I think I like plasma, which I have used. I think you used Verdes, right? I used Verdes. For me, Verdes and plasma are very popular with me. If you want to just use monotonic sort of a thing, then of course you can use sequence maps or sequential color maps. For example, this is yellow green. One of the things very popular is red blue. Red blue is where? Reds, yellow or brown, purple and red. So no, it is not in this list. It'll come later. Yeah. And this is another set of sequential map that you can get by name itself. You can see that it's a little harder to distinguish it. Yeah, red, blue, diverging color maps. Yeah, diverging color maps, red, blue. This is very common, very, very popular. So then cyclic is, of course, it will cycle back on itself. And quantitative, like if you want to have a discrete. So where is cyclic used? I mean, is it something that low and high you want to kind of bring it together? That's where where it could be used. That is right. When you want to highlight the middle values, you use this is it or the opposite. See look at this cyclic, it highlights the middle values, this one highlights the tail values. So things like that. So these are color maps. And knowing the color maps helps you create aesthetically pleasing plots. Not that my plot is particularly aesthetic. Any artist would frown upon it, but for what it is worth. When you're using a qualitative color map, it just picks one per... Yeah. So what will happen is the color changes in discrete steps. Yeah, so yeah, so what will happen is the color changes in discrete steps. So in this one, the set tool, you know, this one is very popular. I see a lot of people using the set tool. Because it agrees with many companies' corporate colors. I've seen people use it. So anyway, see, ultimately it's partly aesthetics, partly personal choice, partly corporate branding and so forth. And of course you can create your own color maps. So that's the color map. So what am I doing now? I'm just plotting only the clusters. How can I do only the clusters? Because I've already separated those points out. Do you notice that we separated those points out a little while ago? So we are doing only that. So to clusters, we are giving a spectral color map, isn't it? What's the advantage is that the the blobs will look like different colors on the other hand for outliers we have chosen to take gray as it's a black colored and 0.3 is just alpha so that is actually CMAP. I need not have mentioned at all. So this is unnecessary here. This is it. And then you give a title. You say at the top, what is the epsilon? What is the number of points? And what is the cluster? So now, guys, does this code look straightforward? I'll let you stare at it for a couple of minutes. By the way, guys, do you in general as you read my notebooks at home, do you find my code reasonably readable? Like is it clear what it says? Yes. From the comments here. Very nice coding. Thank you. If you want me to add more comments in anything like as after the lab, I can add more comments in anything like as after the lab i can add more and post it again so now let's go back to we were at uh which one will be blobs came blobs db scan so now you notice that because of this function it's lovely in one line i can get all my clustering done all i have to do is give it a list of epsilon's to search for best cluster list of neighborhood values to search for and it will create every possible combination of the two and do the clustering right so it is a convenience method and and obviously i have written the function specific to two dimensions at this moment. But it's surprisingly useful, actually. What happens is, like Sachin asked this question, you try your very best somehow to reduce data in two dimensions, and then you keep praying or keep your fingers crossed that there's still some clusters there. Very common to do that. So this is the neighbor. So let's examine this as a data scientist and see what is it saying. If you look at the first cluster, when epsilon is very small, means your radius of neighborhood is very small. And you say, but the number of points that you expect in the neighborhood is also not too great. He just says 10 points in that neighborhood. How many clusters do you find? find five clusters all right but you also the usual five clusters but do you notice that there is one tiny cluster here can you see where my mouse is guys green cluster you also find another cluster right here you see where my mouse is? And yet another cluster here. And yet another cluster here. So five or nine clusters. So attached islands. Yeah like literally like attached islands. So if you think about it, why did it find those clusters? Because our neighborhood is too small so even a little bit of discontinuity here will cause a break in the cluster and a new cluster will start forming on the other side remember there's a reachability argument if you cannot reach across 0.1 is too small a distance you can't hop across it will it will just stop it will say this is the boundary of my cluster right that's the explanation for that now it gets even tighter now we are saying not only that epsilon diameter the radius of radius remains small but i expect even more aggressive standards i need 20 points there for it to be a cluster. Now you realize that you're calling it a cluster only if it is a very dense region, isn't it guys? It has to be a very dense region, then you'll call it a cluster. And so the number of cluster decreases and the number of outliers increase, right? And if you really look at this cluster cluster for example, this cluster has split into two clusters, really, this blob has split into two clusters. And one can guess that it was because somewhere in the middle, right, points could not meet the high standard of 20 points within a very tight radius. So you see this phenomenon. Of course, if you get even more aggressive, you say 50 points in this epsilon neighborhood, but then what happens? You're expecting far more density than the data supports, isn't it? You're looking for extremely dense regions. This data doesn't have any extremely dense regions. So it will say, I did not find any cluster. Everything is an outlier. Another way of thinking about it is that outliers are points whose densities, those regions or densities, are below the threshold that you want right now let's look at this point what i will look we looked vertically now look horizontally here the number of points is the same but as i make the neighborhood more and more relaxed not a very tight neighborhood a bigger and bigger radius what happens do you notice that this cluster, this tight bundle, which used to have three clusters has now become a single cluster, which is good, isn't it? This is good, this is good, this is good, this is good. So do you see how much difference it made just changing epsilon by 0.1? Yeah, it's a big change. It's a huge change. And we see a couple of outliers we might not believe them to be outliers but maybe they are outliers but now for the same number of points i change the epsilon even more i make it 0.3 more relaxed then what happens these clusters do you notice that look at my mouse bottom right cluster from here do you notice that? Look at my mouse, bottom right cluster. From here, do you notice that there is a bridge across? There is a bridge across to the central cluster. From the central cluster, once again, there is a bridge across somewhere here. And from this central cluster, there's surely a bridge across to the top right isn't it it is within the epsilon neighborhood so what happens is that all of these get marked as one cluster are we seeing that guys is this making sense but if you notice carefully there is no bridge across to the top left-hand blob. So this blob remains resilient. It stays as a cluster. But these rest, which are near each other, they get marked as clusters, as a single cluster. You relax the radius of neighborhood even more. Then what happens? As you would expect, now you can actually hop from here to here. You can hop from the central cluster to the top left cluster right which is what has happened and you now are claiming that there is just one cluster right and in fact more noticeable is by the time you reach here do you notice that outliers have disappeared in this one and in this one there are no outliers disappeared in this one and in this one there are no outliers row one column two yeah when you say outliers but it's only five clusters how but do you see these gray points are they gray or are they blue gray gray gray they don't belong to any of the stuff yeah it is claiming it doesn't belong to any of the clusters. Right. So that is that. Now, look at it. Can you go back? Like that thing. So we went from 0.2 to 0.3, right? And suddenly the clusters went from 5 to 2, right? That's right. I mean, like, and it's, if you look at it visually, we know that it's 1 is to four on the right side right the two clusters it's actually a majority of it has been gone in one right yeah the sweet spot is somewhere in between we know here we can at least see how do we know now we are like playing between 0.2 and 0.3 right you can we can pick something in between 0.2 to 0.23 so that is one of the lessons you learn like once you know that your answer is around 0.2, what will you do? You will pick all values around 0.2, 0.18, 0.19, 0.2, 0.21, 0.22. So you do the second pass on it ultimately to determine the best epsilon right so this is that right now what happens when you keep epsilon the same but you relax the number of the number of i mean you need even more points for it to be a cluster very tight you notice that you you ended ended up splitting the central cluster and some of those cluster points actually became outliers. Do you notice that this green cluster here in the top left actually became part of the outlier family, right? So you're looking for very dense regions. Now in this 20 points is a pretty aggressive goal. You look at for 0.2, you still get a clustering that's pretty good, but number of outliers have increased compared to here. If you compare these two, you notice that more points are marked as outlier because your standard is higher. You expect a denser region. You need at least 20 points in your neighborhood for it to work right and when you go here at 20 points you know epsilon is still 0.3 but at 20 points what happens at least the situation is improved compared to the top top had one cluster just two clusters Now at least you have three clusters. Isn't it? So this might make you suspect that 0.3 may not be bad. We just have to be careful with the number of points. And if you do that, you go to 50 points and what do you see? 0.3 also works. You see that Sachin? This is the interesting thing. Now I won't go more into it. I'll let you ponder over which one you consider to be the best. And sometimes there are no perfect answers. So what is the next step we do? This of course is not okay. You realize that this is, we have lost the game here, but this looks pretty good. Right? Now let's look at the cluster metric so obviously i made things a little easy because uh the the plotting returns you the quality metric and so you can all you have to do is print out the quality data frame how do you print the data frame you literally say quality i've added a little bit of style to it different, you literally say quality. I've added a little bit of style to it. Oh my goodness, I shouldn't have done that. I'll have to re-download it. At this moment I should, but there was a green color to those lines. Give me a moment. We will re-download it. This was the blob data set isn't it because on this machine I don't have the full setup. DB scan for old faithful DB scan for blob. Okay. All right, let's take this. Give me a second. So it's notebook 30 DB scan on the blobs data set yes blob data set okay also kale has crashed so you can change me to okay let me do that right away before i forget um oh how do i make the give me a second participant list Kate. Interestingly I don't have host privilege anymore. Oh, did you hand it to Kyle? Yes she started this. So you have the co-host privilege so you should be fine. Okay. All right. started this so you have the co-host privilege so you should be fine okay all right all right so we'll take this and i'll put it sorry guys on this laptop i haven't set up the whole uh environment because i use this laptop only for teaching uh comprehensive this comprehensive. This is week nine. Okay. I don't want to even try running it on this machine. Okay. So when you run it on your machine, you'll notice something. It highlights in light green. When you run your machine, you'll find that all the key important values are highlighted. The best values in each column are highlighted. Can you go slow? Between two rows? Which one? What do you mean by highlighted? OK, let me explain that. First, let's look at this table. It contains a combination of epsilon and n points, all possible combinations that we looked at. Then we find the number of clusters, how many clusters they found. Then let's look at the silhouette score, which is the best score? In this column, silhouette score, which one looks the best? This one, right? This is what I'm saying. That using, if you run this line of code here on your machine, because in my case, I have to run it from start and I would not dare to. This laptop is Windows laptop and I don't trust it at all for anything. So this is highlighted. If you are having the file, please run it, you'll see this highlighted. So what it is saying that if you believe the silhouette score, the best number are 0.3 and 50. If you believe in the homogeneity as a metric, then once again, the best is this 0.3 and 50. If you believe in completeness, the best is the multiple best this and but this one which is saying there are only one cluster which makes sense is one big giant cluster and it's uh for all values of Epsilon and this it seems to be producing the same same results Epsilon is 0.1 and 50. so and the number of clusters that it is saying is zero everything is an outlier not not quite good so when everything is an outlier it is obviously not the best answer you might want to go with this there's only one cluster which also is not good so completeness is a useless one we go here we measure which is the highest one here a useless one we go here we measure which is the highest one here point nine this one nine five yeah it is also claiming there are five clusters and you find those clusters that epsilon is point two and the number of neighbors is ten so let's go and look at this epsilon is point two and the number of neighbors is ten it must be the first. It is saying that this is a good clustering. Would you agree that this looks quite good? Right? The other one was 0.3 and 50. 0.3 and 50 is this. Would you agree that this looks like pretty good clustering? Yeah. So, and they're very close to each other. Like 0.2 and 10 and 0.3 and 50 combinations are very clear. The only difference is outliers, right? Just where the outliers are positioned. That is it. Now, if you look at the end, and of course, the 0.3 and 50 is the second best answer in this second best answer here. If you look at the adjusted Rand index, you notice that this one favors 0.3 and 50. so most of the metrics are favoring 0.3 and 50. now if you look at adjusted mutual information now these concepts guys here's the thing read it otherwise on tuesday i'll explain each of them carefully right they're interesting actually and they have very interesting definitions we'll do that so here also this seems to be the best right so the majority vote says what this is your best clustering isn't it most of the metrics say this is your best clustering even the metric that doesn't say it has it as a second best clustering so let's go with 0.3 and 50. would you agree no so this is the way you make decisions guys and this is how you decide what to keep so far so good right and of course here we can see the clusters most of the time you're not so fortunate so you have to look at this table and make head and tail out of it right so well you say well you know what this data set was very simple it was well separated out let's try something real data so to go to the old faithful geyser and do the k-means there. Pretty much the same exercise code essentially remains the same. Remember it was eruptions. How long the eruption took and so therefore how long you have to wait is the target variable. To wet your recollection, this was the data, right? Eruptions versus waiting. Do you remember? Now you notice that there are two blobs, small eruptions, small waiting times, big eruptions, big waiting time, isn't it? So let's see if we try to cluster this data, what will we see? Of course, we can see that they are effectively just from the histogram and from the kernel density plots, you can see density estimator plots. You can clearly see that yes, there are two clusters, small and big cluster. And it pretty much spells it out. But now let's pretend we didn't see that. So we will again try k-means clustering for different values of k from 1 to 10. So you notice that what does it do? 1, 2, 3. So if you were to look at it, you would say 1, not so good, right? 2, would you say 2 is good? Yes. 3, well, you know, this looks split. 4, things are getting bad clusters what to us looks like clusters is getting split and five six seven eight are not even worth looking let's see what the scree plot says same code ah there is a very distinct and pronounced elbow at two. Isn't it? So it means that the K-means clustering work really well with this. What happens if we apply dbScan to it? Oh, the title should change. It should say dbScan. I'll fix and upload it. Minor title change. So this part remains the same. Let's go to dbScan. Now that we have created the utility, guys, do you realize that? It's just one line. With one line, you can get these beautiful plots and the quality metrics in one go. So when we do that, let's look at the first one. Is this good clustering? Not convincing, isn't it? It's pretty, but it's not good clustering. Yeah, this one comes up with three clusters. It's small, large, and somewhere in between it has found a mini cluster. Right? This one gives us two clusters here and here. And of course, if you relax, if you make your radius of neighborhood too large, you'll end up making a bridge between everything. You can have reachability from one end to the other end through this bridge. Now, let's look here. If you keep the radius very tight and even increase the number of points that you expect. You expect even denser clusters. What do you notice? That a lot of points which used to be clusters have gotten marked as outliers. But then you relax the neighborhood radius, it begins to split out and there are some outliers. Again, it has split out, but there are no outliers. So when you look at it, guys, would you say that there are some outliers. Again, it has split out, but there are no outliers. So when you look at it, guys, would you say that there are significant outliers? Probably not, right? So then let us see which clustering would you say is the best. Here. So we see the variation. But one thing, guys, do you notice how sensitive the data is to the choice of hyperparameters? The clustering is to the choice of hyperparameters. Very. So now, since we don't have the labels, obviously that data doesn't come with labels, as most real life data doesn't. So we can, when you do the quality metric, we get for each combination the silhouette score. doesn't so we can when you do the quality metric we get for each combination the silhouette score now which is the highest silhouette score once again in your notebooks when you run it that answer will get highlighted automatically because of this highlight max what's it point this one right the next one this one yes so this is seven five six two eight eight and seven five so yeah so basically these two are giving us the right answer let's go and look at it point four and three point four and three this one would, it says that this is very good. Would you agree? Yes. And the other answer that it is advocating is 0.4 and five. Right below it. And it says this, they basically have the same clustering as far as I can see. Both of these are good answers and others are not right so you now know you can pick your hyper parameters either of the two so i'll be seeing it guys how how to do db scan now i wanted to illustrate a weakness of the db scan so and its sensitivity to hyper parameters so I was tempted to put the picture of our galaxy the Milky Way just to remind that the most intuitive galaxy that we know of and live in is literally our own world, the Milky Way. So now, what I have done is just just so you know, do you notice that I have created I've taken data, I'll just generate data, in which one data has a center at , another data, another Gaussian has a center, two Gaussians, one at , just say Bell Hills, one at , one at . But I'm doing something interesting. Look at the covariance matrix here. The diagonal is one, but the off diagonals are 0.5. So there's 50% correlation. In the other one, the off, the diagonals are asymmetric, 3 and 5. Means the bell curves are much more spread out, isn't it? By a factor of 3 and factor of 5, the bell curves or the bell hills are much sort of more rounded hills. They are not peaked hills. And I deliberately did that to create two clusters of different densities. So which cluster will have more density? The mu one sigma one cluster or mu two sigma two cluster? The second one. No, the first one, because the first one is tight. The second one is more spread out. So if I sample 10 points, they are much more, or 100 points or 200 points, they are much more likely to be separated out from each other. Separation, okay. Separation. So we have C1 and C2. So now I put these into two data, just little mechanics to create a data frame out of all of them by concatenating that. And so let's visualize and see what we produced. This is the data we produced. And of course, because we know which data point came from which Gaussian, we can sort of reveal the ground truth just for our own convenience. That this is what we expect the clustering to find. But of course, the clustering algorithm doesn't know this. Right? Here, we'll limit ourselves to dbScan. When we do that, let's do it for different values. So here's the thing. This is what I was saying. In this notebook, I've run db run baby scan out here not at the top so you do the clustering and what do you notice just look at the first row how vastly different are these clustering from each other isn't it do you see huge differences one find 15 cluster another finds 19 8 and then 1 just with minor changes of epsilon right and now if you so maybe if you look carefully can you can you hold that as well yeah no no one second before i go there observe this fact guys that one cluster the lower left left cluster is very dense the upper right cluster deliberately i made it less dense. That is why the choice of epsilon becomes hard here. So what is the question you wanted to ask? So how did the clusters go from 15 to 19 and then reduced to 8 one? The reason is that in this, the number of points you expect is three, the neighborhood is very tight. So what happens is with this tight neighborhood most points become outliers but in this case a lot of those outliers are marked as clusters because now you're willing to your radius by that you want to search for is higher right so you can easily form bridges because you have a bigger bridge. So basically you found some clusters in the outliers itself. That's why exactly the four clusters that came are actually all can be traced back to outliers. Right. From that is for outliers, according to the first model. Yeah, yeah, that is right. And so you go and you improve this situation improves, but there's still this many clusters floating around. Right? So you notice that epsilon zero point, sorry, n points actually here, I'm what number of points is three. And epsilon I'm changing, that none of the epsilon values seem to be working. isn't it? So maybe the answer is in different value of the number of points. Let's go down here and scan and see that yes, this looks pretty good, isn't it? Where my mouse is. These two are looking much better in this row compared to what we had before. This one and this one look much better. Would you agree? And then let's keep going down and then well, this also looks good. So these three in this bottom right hand side, these three look pretty good. Let us see what the clustering quality metrics say. Once again, the highlighting has disappeared i hit that what is it saying let's look at the salewik score which is the best one what is the index of the best one highest number seven point six one yes number seven and it says what one point two and six one point two and six this one so it is saying that this is according to the silhouettes code this is the best one would you agree Yeah, it looks pretty good or believable. Let's look at what the other metric is saying. Homogeneity says that the best one is, as far as I can make out this guy, this is also number seven. Pretty good. It's agreeing. So number seven gets two votes, row number seven. Now completeness, which has the highest score of completeness number eight but that doesn't count because the number of clusters it found is zero right so that won't do which is the next best answer seven three votes for seven let's go to v measure it's it's it is also this one right by now you're getting pretty confident that that's best clustering isn't it 0.92 and 0.88 right so this is it guys what i wanted to show you is two things first is that you have to do hyperparameter search and so all the votes are in for this but the difference between this and others for very small change in values are dramatic do you notice how different this is from this one from any of its neighbor the best answer differs radically from its neighbors where the values are only hyper parameters are only slightly changed do you see that guys yes so this is a point to remember that db scan you must do hyper parameter search otherwise you can't trust your clustering right finally i'd like to take i know it's going to be five i would like to take, I know it's going to be five. I would like to take one more example, a fun dataset, which I created. It's called the Smiley dataset. Once again, I've renamed the columns to X1, X2, so that I can feed it into my utility function easily and not have to do any work. So the visualization looks like this it's a smiley now why is this interesting or why should i consider this a serious data set because it has non-globular clusters do you see this convex a concave you know this curved, non-convex shapes. So we- Smile. Yes, you should wipe the smile out of some algorithms. So let's go down and see how many clusters we can. So when you count just yourself, how many clusters would you say there really is? Six, and then the outliers. So now let's see how does k means do this is just the density plot i found this picture funny the kd plot nice so the same code for different values of k would you say that k is k is equal to 1 is sensible now 2 now 3 now k is equal to 6 a disaster isn't it because these two are merged right lip is parted do you see any cluster that looks reasonable to you? No, unless you like your eyebrows and mouth split. Yes, you can't like any of them. So this is an example where K-means fails, right? It doesn't do as well. No matter what you do. So just because you are doing hyperparameter search doesn't mean that you'll hit upon good clustering. The data tells what is good and what isn't. And for this data, it's bad. So when you look at the scree plot, can you tell a definitive elbow anywhere? elbow anywhere all of those points are marked is there anyone anywhere that you can see you can say that i see a definite elbow too gradual too gradual right when you have too gradual and suppose you pick which one four or did you say albert you prefer seven four and seven let's go and look at four and seven four reasonable at least it got eyes separated but the eyebrows are different color eyebrows separated eyes are still the same and nose and mouth well reasonable let's look at seven seven everything is perfect except that it has spread the lips isn't it so you could say that in view of this maybe seven is the best answer but not to be trusted because you know as you can clearly see it has mangled up the lips and it shows in the scree plot So the lesson here is guys, if the script doesn't show you a clear elbow, it should wake you up to the fact that k-means may not be the right approach to this problem. Are we together? Right? So that is something, excuse me, worth waiting out out now let's apply db scan to smiley same data everything so now i have applied smiley for different values of the hyper parameter what do you notice when k is equal to 1 let me zoom in a little bit. When, so not K, when epsilon is 0.1, number of points is 10, this seems to be doing okay, except that the top of the nose has become a cluster of its own, right? In the eye, you have developed a cluster, yet another cluster, and the lip, at the corner of the lip, there's another cluster, right? Of the mouth, there's another cluster. So not quite good, but this particular value, 0.15, gives you this cluster, six clusters. And you notice that dbScan is pretty consistent for it remains sort of stable for different epsilon values beyond a certain. all seem to be saying 6666 everywhere right lot of places they are saying six are we together guys right so it here it's and it got the outlier points quite well if you look at the clustering quality metric all of the good answersar? Which is the best answer here. Raja Ayyanar? The best answer here seems to be 668. Raja Ayyanar? They all all these answers are saying six you notice 664 is saying 6664 is saying six, this is also saying 66 so the multiple combinations that seem to be telling the same answer right this one, this one seems to be the best overall so 0.2 and 20 that seems to outshine everything else uh fairly 0.2 and 20 is 0.2 and 20 yeah this one so it says that this is the best clustering that you have. This one. Yeah, yeah. They're practically, I mean, I don't know what the difference is. They are the same. Right. I don't know which little point it missed or what not, but it is the same. So yeah, these are good clusterings and dbScan. So if I were to look at this data set, it is practically a success story for dbScan, so long as you do your hyperparameter search, isn't it? And even in high dimensions, if you is, 6675, 668. Yeah, these two. You can pick these two, whichever one you like best. And that would be good enough, right? So you would say it's pretty stable here to hyperparameter changes, right? And with that, it's 5 o'clock i'll end any questions on clustering no nice visualizations yes nice visualizations too