 All right guys, so we are going to review the solutions to the quiz. This quiz is about care nearest neighbor approach to classification and regression. So the first question was, the curse of dimensionality is which of the following the correct answer is the curse of the dimensionality it's a curse that adversely affects neighborhood based methods in high dimensional spaces the neighborhood grows to too large a proportion of the feature space when searching for k nearest neighbors so the big idea is this guys just to recap suppose you have a thousand points on a one-dimensional line unit line if you put thousand points they are they look rather close to each other now suppose you go to two dimensions you realize that now you have a square unit square and they have uh they are not as near each other some of them if you randomly spread them uniformly spread them sort of but they would be not as close to each other isn't it now you go to third dimension a cube you realize that a thousand points are beginning to look rather comfortably settled they're not crowded in the unit cube and so as you keep on adding dimensionality your volumetric space becomes or your hypercube becomes rather sparse so that in hundred dimensions your thousand points don't look too much they look they have gone from being dense data in one dimensions to being very sparse data sparse data means what that when it's hard to find neighbors, the neighbors are far away. Another way to argue is the way we argued, if you take a unit hypersphere from the center, how far do you have to go to capture half your neighbors or even one third of your neighbors or something? What happens is in high dimension or even one third of your neighbors or something what happens is in high dimension or even one let's say one percent of your neighbors if you say you'll take one percent of the neighbors to do care nearest neighbor how far do you have to go it turns out that in high dimension spaces you have to practically go almost to the edge almost to the periphery right to capture enough neighbors to do your care nearest neighbor. When you go that far from the point, the question of neighbor, the word neighbor begins to lose its meaning. It gets diluted. Well, the whole care nearest neighbor approach or neighborhood methods are based on the premise that your close neighbors are a good proxy for you. They are representative of who you are. In other words, you are defined by the company you keep. So when you can't, when your neighbors are very far off, for example, if I have to find your neighbor and your neighbor is like 50 miles away or across the state border, and your neighbor is like 50 miles away or across the state border, it is not so clear that his life is representative or her life is representative of your life, right? Those neighbors can be living entirely different lives. So that is the main point. The point of neighborhood is that in the neighborhood, certain characteristics you expect to remain the same and so you expect the target values to all be the same target variable value to be the same more or less and that condition no more holds true that's why you can't do that question on that is if the data becomes sparsely on a height in a high dimensional space Does it only impact the distance methods? Does it not actually no impact on other methods other models? No, because if you don't use distance or the concept of neighborhood, then you are not impacted absolutely not so So it yeah, it depends on which algorithm we choose but in general to see generally very high dimensional data and not and just having enough data causes problems it's a classic problem for example in genomics etc you have so many genes and yet you have data from like 30 patients sometimes 100 patients or so so that's a different class of problems there we have techniques to deal with them but but whatever other method may or may not work okay the neighborhood methods begin to fail okay The second question is when, suppose you have a data set, a training set where there are M instances of the data and let's say you pick a K that is almost that, suppose you have 200 data instances and you take K to be 200 or something like that, 190 or something like that, approximately equal to K. With this value of K, the K in an algorithm is likely to create a predictor that exhibits what? What will it have? See what will happen is, all the instances will say, will be, whichever point in the feature space you take, most of the instances in the training set will be there. So whatever the majority classes, if you're doing classification between cows and ducks, let's say you have 150 cows or not even 125 cows or 150 cows, let's say, and 50 ducks. Data is slightly asymmetric asymmetric now suppose you take k is equal to 190 irrespective of whatever the weight of the animal is weight and size of the animal is what is the majority prediction likely to be it will always be cows, you know, because there are just so many cows in the feature space. If you take k is equal to 190, you'll have to go far and count a lot of the cows. And those data points will all be shouting cow, cow, cow, right? And so your answer from the classifier will look like, will always be cow or almost always be cow. In fact, always be cow. That's the problem. That's the problem with high taking very large numbers. It comes down to the baseline classifier. Do you remember the baseline classifier or the zero hour classifier is what? It basically says the majority class wins. Isn't it? And for regression, it just begins to look like the average of the target value. So how much ice cream will this person sell on the beach? Well, it turns out just take the average of all the data points. Whatever average you get, that seems to be the answer always so don't take don't take too many neighbors that has a bad effect you have high a bias error so model is very bad the variance is very low your answer is not changing at all much that's that the K in acronym, K nearest neighbor refers to, what does it refer to? So there were some quite interesting choices, a parameter and so forth. By the way, I, because I'm sharing my screen, I shouldn't look at the statistics because you'll see, I think some of you tripped on this question. It's a hyperparameter of the model, guys. Not a parameter. K-NN is not a parametric model. There are no parameters. There's nothing to learn. You just hold on to the training set and then in the training set you find the nearest neighbors. So parametric models are parameters to learn. This is lazy learning. There's no learning actually taking place in K-NN. easy learning. There's no learning actually taking place in KNN. So it's a hyperparameter of the model that helps you decide, I mean, that you have to tune to get the best model. Next question is KNN is considered a nonparametric model because the correct answer is it does not build any model at training time at all, simply saves the training data at inference time it finds the k nearest neighbors and their target values to draw an inference together so that is that so i said that is the lazy evaluation right it is also a lazy evaluation in, that is one of the questions I ask here. The optimal value of K should be what? It should always be N, the total number of data points. Well, that is a bad idea, isn't it? It will lead you to the baseline classifier. It is always one, so that also is wrong. You might think that the nearest neighbor is the most representative no that leads to high variance overfitting and the correct answer is you can only find k like any hyper parameter in machine learning you can only establish its value by doing what a search for the best value by building models again and again for different values of k and you have to test it out in the validation set right you have to do cross validation to establish the best value for that is this clear guys any questions so far this is i'm taking it quite literally from the notes that we went through or something like that similar to that every view it So here the decision boundary looks to you like what is it very complex very simple. What is it? It's very complex very complex. When do you get complex overfitted decision boundaries when K is small or large? small when k is small or large small small right and you have small k so this is really listening to very small number of direct neighbors so it is very sensitive to noise do you notice how it has detected all sorts of islands in the data right so that is that uh was this an approximation? Because I literally counted the neighbors. Yeah, it's just an approximation. It's a small number. I mean, this curve is not exactly correct. It's just logically. I'm trying to convey the concept. I'm sure that if we take this data and we actually draw the decision boundary using the right mathematics in using code the decision boundary will look slightly different but yes it is just to convey the idea it's not not representative yeah so don't don't read too much into it it's it isn't actually I just do a complicated decision boundary and then I started putting blue and yellow points all around it. So this is it. The other question is given a training data set of about 200 points, this particular suffers from what variance errors or a bias error. So what is it? Obviously it is high variance errors, right? You take another sample of the data, your decision boundary will change and it exhibits significant overfitting to the data. I hope most of you got this one right. On the other extreme, this decision boundary, let us say that this is the same data. Actually I couldn't, I wasn't patient enough to, I should have been smart. I should have first drawn this figure made two copies of it and then drawn to decision boundaries but I did but suppose you take this data here the decision boundary it seems to have the opposite problem it is too straight maybe it needed a few bends for example maybe it could have bent a little bit here isn't it there quite a few bends. For example, maybe it could have bent a little bit here. Isn't it? There are quite a few blue points out here, but this decision boundary did not bend at all. It's more or less a straight line, not a good thing. So it represents, it can't be k is equal to one, is not between 1 and 3, is not 200 because if it was 200, decision boundary would be far away. All the points would be marked blue because here blue is the majority. So it's probably a large number. You don't know how large a number is. So some number greater than 10. Then this problem says, this kind of a model, what is it more likely to have? What kinds of errors? Is it like bias? And it has probably under fit the data. You probably need a little bit more flexibility in the decision boundary. It has underfed the data. Next question is in the KNN algorithm, K is equal to 1 is likely to produce what? Well, obviously by now it should be obvious that it will create a decision boundary that is somewhat like this, isn't it? It is likely to overfit the data and exhibit high variance. It will draw excessively complicated decision boundary. Next question, different values of K lead to different degrees of bias and variance. Of course, K is the hyper parameter that controls your bias variance trade-offs. Isn't it? In K and in algorithm. that controls your bias variance trade-offs. Isn't it? Now, by the way, this question was actually a really easy one. I hope most of you got right. That K in KNN doesn't stand for the number of noisy neighbors. I thought noisy was cute. You didn't like my high dimensional prison of Azkaban? That was even better. Yeah. I was trying to introduce some human to this. Anyway, a canon is possible only in those feature spaces that support a notion of distance obviously right neighborhood means it is ill-defined unless you have a notion of distance likewise KNN is a late it's a lazy learning algorithm no brainer of course there is no learning you just save all the data it is instance-based learning of course it is a learning. You just save all the data. It is instance-based learning, of course. It is a parametric model that builds a linear algorithm that builds a linear model in K parameters. So I say, what do you mean by instance-based? Means you just remember the instances, memorize and hold on to the instances. So all you do is you take the trading data and save all of it. Then when you have to make a prediction for a point, you go search amongst your instances and find the nearest neighbors. Oh, okay. That is what this means. Asif, in the question about distance, it doesn't necessarily mean it should be discrete values, right? As long as your data is expressed in distance. Yeah, remember that I mentioned to you those fundamental criteria of water distances. Distance between x and x prime is always positive definite. It's a positive value, zero positive. It is symmetric, the distance from x to x prime is the same as distance from x prime to x. And it follows the basic Schwarz inequality based condition that the straight distance, the geodesic from X to X prime is always shorter than any detour you take through a point Z. I'm just curious how that can be expressed in tabular data as if. See, one thing you do is once you have scaled the data, quite often people try their luck with just applying Euclidean distance. Assume the data exists in RP, it's a P-dimensional space, and you just apply Euclidean distance and try your luck, see if it works. If it doesn't work, try other Minkowski measures. If they don't work, then you try to build a more complicated distance measure but so long as you can construct a distance measure between two points it's straightforward and people go people go pretty fancy actually and how they define distances if you look at the literature of distance definitions in machine learning you'll be quite surprised at how rich the literature is. Because as if let's say you have data that are clustered in dates specific dates like when a patient goes to the ER can you express that in terms of distance? Data that's clustered so there's a panel data kind of thing. Last year's data, this year's data and so forth. The answer to that is, see, it's a, to be able to define the distance when one is not obvious is tricky. Because if you can infer a good distance measure, that itself is learning, isn't it? You have learned something from the data. Yes. So sometimes the, sometimes the purpose of itself is learning isn't it you have learned something from the data yes so sometimes the sometimes the purpose of machine learning is to come up with a good distance measure itself that is a machine learning task in itself what is a good distance measure and you know we we go to all of this lens for example in NLP when in the next workshop that you do. What is the distance between two words. What defines it, you can say that you can use Jay Shah, Dr. How many characters. I need to change to go from one word to another. So for example, you can say cat and hat are hat are close because you change c to h and it becomes a hat but that's a very silly way you know that semantically it doesn't make sense or typically things sentences that contain the word cat don't contain the word hat isn't it so unless it's a mad cat wearing a hat So unless it's a mad cat wearing a hat. So then what is a good meaning? How do you define distance between words? So one of the nicer things you can do is look at how often they co-occur in documents and close to each other. So things like word2vec and GloVe, they create embeddings. These embeddings are, you create a latent space, a hidden space, the embedding space, in which you project these words, you learn to project these words as vectors. And when you do that, then somehow the semantic richness of the relationship between words is preserved. For for example one of the surprising and pleasant surprises that emerged when people did these embeddings was that they could write an equation that said that if you take a king subtract man from it the vector man from it add the vector for woman to it it will be surprisingly close or be approximately the vector for queen so you could write beautiful identities like king minus woman plus man is queen or people began to notice something very interesting they began to notice that if you take literature you say united states and Washington, D.C. If you look at the distance between U.S. and Washington, D.C. words, I mean, this sort of phrases, and you look at the distance between, let's say, Philippines and Manila and or India and Delhi, you'd be very pleasantly surprised the distances are almost exactly the same, approximately the same. surprised the distances are almost exactly the same approximately the same so it means that suppose I want to find the capital of a new country that I don't know the capital of let's say I want to find the capital of Germany all I need to do what do I need to do I can say that Germany minus its capital, C, is equal to, let's say, US minus Washington, DC. And therefore, that C, the capital of Germany, is equal to the capital of Germany plus Washington, DC minus US. And hopefully Berlin pops up. Is Berlin the capital of Germany I hope it is it is it is right I don't know after the reunification first we were taught when I was young we were taught that Berlin is the capital of West Germany and born is the capital of East Germany so now Berlin is the capital of both okay so things like that and so so those are word embeddings, and you have a notion of distance now. So that is the beautiful world, you know, this literature of what is distance embeds itself into questions like, what is the real space in which you want to submerge this data, you know, you want to put the embed this data? What's the real vector space? embed this data? What's the real vector space? And things like that. It gets quite interesting actually. So the couple of things I can do guys, KNN it turns out and kernel KNN are just the tip of the iceberg. I didn't teach you a lot of other neighborhood methods. For example, you can do a lot of it. For example, TSN, UMAP, and so on and so forth, manifold learning and so forth. There is a rich field that I didn't teach you and I won't get time probably to teach you in the deep learning workshops either. There's sort of things out of the scope of any of the formal workshops. I'll probably hold the extra sessions, the seminar sessions or things like that. And in that will cover those So keep attending the seminars on the Sundays. Keep watching out or sometimes I mean just create an ad hoc workshop one day workshop to cover just a small bag of topics if you're interested join will nominally price it or make it free whatever it is alright guys the rest of the questions have to do with regularization it was just supposed to be a review I'll stop sharing my screen and see what this course you guys got an average so let me see what the average scores are looking like for each question. One second. So let's see. Okay. So 16 people took this test, it seems. How many people took it? 16, I believe. the foot no one two three four five six seven eight nine ten eleven twelve eleven is it eleven 31 why am i seeing two sixteen people one two thirteen people three four five six seven eight nine ten eleven okay eleven people have finished their things. I fear you haven't. And the first question, which was, which was that question? Oh goodness, I wish I didn't have to do it like this. Chris. Isn't the order different for each person? No, no, no. Yeah, the question does become different for each of you. So I'll have to pick up. No, this sucks. I'll have to do something. Okay, what I'll do is I'll just click on anybody's answer. Yes. So this was like, the question was, what is the curse of dimensionality only one person was off almost all of you got it right the second question was also most of you got it right which was which was that question when you take a very large what happens when you take third question was so let me see the questions that most of you got wrong rather than going one by one. Ah, there is one question that got tripped very, very badly. Which question is this? K is considered a non-parametric method because there is no model that you're building that you just hold on to the instances and find nearest neighbors and make a prediction, make a majority production using that So that is that when maybe I should have emphasized. I hope this quiz was a learning situation for that. Oh goodness. There was one question that had most of you get it wrong. What question is it? I probably didn't teach that topic well. Or the optimal value of K, how do you find the best value of K? So it is through the hyperparameter search. You don't know. You have to test K as a hyperparameter. How do you find the best value of a hyperparameter? On the test of validation sets. Whatever comes out to be best on your validation set is the best value okay you can't just learn it in the model itself so that is that which other question did poorly one question all of you got right I mean mean, yay! What did I teach well? This is the one. When K is equal to 1 that it causes high variance. Very good guys. You got that one right. There are many questions that almost all of everybody in fact got all right. So yeah, this course looked very impressive there was only one question that had 31 percent pass rate almost all of them are 90 plus percent or 70 80 90 pass rate yeah i'm impressed these were not i hope you guys realize these were not easy questions realize these were not easy questions. They were tricky questions, isn't it? So good guys, that is all for the quiz today. I hope you had fun. Hãy subscribe cho kênh La La School Để không bỏ lỡ những video hấp dẫn