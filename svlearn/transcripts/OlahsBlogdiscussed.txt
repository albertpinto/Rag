 Are you guys reading those chapters in the book that are the homework for this week? Any progress, are you reading those? Reading, yeah. Yeah, please do read those. So guys, we are discussing this very nice blog by Christopher Ola, and it is about networks, neural networks, manifolds, and topology. It sort of shows the link between these three things. Now, neural network is something we have been introduced to in the first two weeks. You probably don't know what a manifold is and what topology is. Anybody here with a background in manifolds and topology? All right. So I'll just give you a very simple... So, Asif, I think it is the study of the shapes, right? And how it deforms and even if it changes the shape, if it does not, geometry does not get deformed, like you have to poke hole to actually understand the behavior. Yes, that is a pretty, at least a pretty good understanding of it. Close to it. What you're saying is the homeomorphisms and that is one of the or diffeomorphisms more like it. Those are the things that we will talk about today. You will see me introduce a few words that are at the heart of machine learning. See in machine learning and this paper sort of alludes to it but doesn't get too much into it, there is a fundamental hypothesis all of machine learning is predicated on a hypothesis. It is called the manifold hypothesis. If this hypothesis is not true, machine learning would essentially not be possible. If the hypothesis is not true, machine learning would essentially not be possible. So what is this hypothesis? We are going to understand it in a little bit. But before we do that, let's try to understand a few things. manifold. What is the topology? What is the topology of it? When we say topology of a manifold, what do we mean? Manifold. And then C, we'll talk about things like homeomorphism and diffeomorphism. Diffeomorphism is something this book did not mention. Diffeomorphism. But this is actually more relevant to your networks actually. They're not just homeomorphisms. They're like diffeomorphisms they're like different organisms so what are these things we will understand that and what is the relationship i suppose machine learning to manifold center quality So imagine that you have an elastic sheet. I'll just take a square sheet, but it can be any shape. And in that sheet, you have, let's say, a shape, round shape. This is let's say a shaded red and this is white. Suppose I say it is stretchable sheet, rubber sheet, elastic sheet, some form. Think of it like this. If I ask you, can you make this look like this circle to become an ellipse so how would you do that uh you can stretch it you can sort of stretch it and you can say there we go it has become this are we together right So now it has become an ellipse. What you have just done is a transformation. You would call it both a homomorphism and a diffeomorphism. Diffeomorphism is a smooth transition. So let me just call it a Diffio morphism into this particular shape. Because you can think of, in some sense, that this state represents a state in which the ellipse. So let me parameterize it with a time that goes from 0 to 1. At 0, it is this red circle. At 1, it is the ellipse. So this is t is equal to zero state, t is equal to one state. So you can say that in some sense, you could say that equation of this is one minus t, the circle, if you say that the circle manifold, the initial manifold, plus t times the ellipse manifold. Let's see if this equation makes sense. This is a mapping from 0 cross 1 times, this is what, a 2b space going to a two-dimension space. Well, I wrote some fancy mathematics here. But what I'm seeing is very simple. This R square is the sheet, this rubber sheet. And this R square is, of course, the target stretched out rubber sheet, isn't it? And now we are saying that actually this is a mapping in which you have a continuous deformation of this into this parameterized by a factor t so now in this equation try putting t is equal to zero what will what do you get at t is equal to zero you will get the circle at t is equal to one when you put t is equal to one it will become the ellipse does this make sense guys think of this equation and as a very simple equation i'm saying that as you increase t from 0 to 1 it initially it will look like a circle and gradually it will start looking like an ellipse does that make sense guys yes it. It does make sense, right? So this is a classic example of a saying that we have performed a homeomorphism. And usually when it is done in a very smooth way, it is not just a homeomorphism, it is a diffeomorphism. So we can be a little bit more exact about it. Now, generally, smooth transitions are preferable. If we can get away with smooth transition, it is really very good. Now, if you understand this much, then let us add some wordings to it. This is, I did it as a 2D surface, but suppose it was curved. This is in a higher dimensional space. Okay, this is, this was just the plane here. So here was your circle. I'm just doing the same thing. Gradually I'm taking you there. So this is your red. It's the same thing. But now there is a third axis. This is x2, x3. Now suppose I say that you can deform it into like take this and make this circle sort of bulge out. Can you do that? Can you just sort of start pulling here, pull up and create this red bulge here. Can we do that? Yes. We can do that, right? All you have to do is take this and just put something underneath it and gradually start pushing it up. And now you have ended up with a surface that has gone from this to a much higher dimensional space. The surface is occupying a three-dimensional space now, this manifold, right? And yet, even though it's occupying a three-dimensional space, you would say that it is still in itself, it is two dimensional. The property of that rubber sheet has not changed. It is just occupying a higher ambient space. So the ambient space, the ambient space the outer space is ambient space was a three-dimensional but in the beginning it wasn't occupying or it had no presence in the third dimension but by making a little bit of a push here or bulge here onto the red part now you have you have occupied all the whole ambient dimension all three things are there but you would still say that the same essential quality of the of the of this elastic sheet has not changed so when you say that the essential quality has not changed you would say that the topology has not changed. Topology is the overall quality that remains invariant to stretching and bending and all of that. Are we together? So you can smoothly stretch and bend and do whatever, like for example, you bent it out, do whatever you want, but it remains what it is. Just don't tear it, you can't bring a knife to it. Are we together? So long as you don't bring a knife to this sheet, everything that you do preserves the essential quality of the sheet, because it's a reversible change. You can go if you can go from here, you can go back also. Would you agree? So this can go here, but it can also go back. So these are reversible changes. Yeah, so is it like, can we say that, is it like a 2D manifold embedded in a 3D space? This rubber sheet is a 2D manifold that was initially embedded in a 3D space? This rubber sheet is a 2D manifold that was initially embedded in a 3D space. And this is now occupying all 3D space, significant part of all the, you know, it's gone on and occupied the third axis also. But it is still embedded in 3D space. It is a 2D manifold embedded in 3D space. So, let's ask the question. Yes, I wanted to ask that one. So how is topology related to geometry? How we'll come to that? Okay. I see these fields, I am giving you a geometric way of looking at topology. Actually, if you answer that, when you look at topology, the initial version or the way it is taught, it starts with something called point at topology, the initial version or the way it is taught, it starts with something called point set topology. So people start very abstractly with algebra, right? And then they develop it from there. Whereas what I'm doing is I'm giving you a geometric interpretation of topology. In a very geometric way way i'm showing you what topology means really correct correct so i'm just saying let this be a set of points and blah blah blah and then it is uh you know these properties hold true and so forth so my question is like uh no so my question is like even though i mean the geometry won't change out the object even though we can bend and do all sort of things with an object right yes the geometry not the geometry the essence the geometry is changing you look at the shape the shape has changed but you see that the logical property hasn't changedologically, it's still the same rubber sheet. It has just been stretched. You have introduced a bulge with your hand. So what is topological property, sir? Like when we say that what is the property? So let us take an example of a topological property. See, suppose I take a rubber sheet which is solid like this. So let me take this example consider a this is a solid rubber sheet and now think of another rubber sheet which is which has a hole in it now is there any way that you could deform one into the other any kind of stretching that would cause this to happen no it can't i mean the only way you can do it is you can stretch it to the limit so that it perforates in the center that's a catastrophic event it's not a continuous deformation there is no way you can deform this into this so you say that this is into this. So you say that this is the topology of this A is inherently different from topology of B. People use some fancy language homotopy and so on and so forth. Let's not go into that winding number and so on and so forth. I'll give you an example of that. So see you take this rubber sheet, right, you fold this thing so you say that these two are equivalent more or less right even though this has been maybe stretched maybe bent and so on and so forth but this is not equivalent to this guy they have different topologies. Or suppose you take this, imagine a surface which has two donut holes, a two hole donut. In India, you would call it jalebi. Do you remember that? There is no way you can take a jalebi, a two-hole doughnut, and deform it into a sheet of paper, into a continuous flat sheet. It is just not possible, isn't it? So you would say that this topology C, in a very intuitive say, is fundamentally different from the topology of A and the topology of B. The people use more words but let us use some very rough words here to get the intuition. Are we together? Yes. So that is the meaning of saying that things have different topologies fundamentally. Now there is something very interesting that you will realize. And it is a joke in the mathematics about mathematicians, that if you give a mathematician a donut and a cup of coffee, he will sip the donut and eat, eat the coffee mug. So what does that mean? So you have a coffee mug here. So you have a coffee mug here. So that means 3D and 4D. What is that? It means like a three-dimensional and four-dimensional manifold. Embedded in four-dimensional manifold. Right. See, think about this. I'm just approximating coffee mug. And think of a donut. This is your donut. This is the hole. I'll just mark the hole with the solid. This is the hole. And what you can do is imagine that you took a little bit of the mass here, solid mass here, is there solid fluid on it, and you start bending it a little bit. And then it became like this. Right? Then you stretched a little bit, took more material from here, and you stretched it a little bit to this. And then a little bit, you keep on deforming it, and then you take some of the material here and you make it. So now what does it look like? So it begins to look like a cup, but you can do it. This is again a whole. So you realize that you can do a homeomorphism, a smooth deformation. From a donut to a cup, and this is so popular, I just realized that Wikipedia actually put it in its web page defining what a homeomorphism is. So I wanted to show you that. I was very interested in it morning. I was looking for it. I suppose the joke has become so popular that, okay, I will now show you this screen here so guys look at this visualization do you see how a cup a donut becomes a cup look at this. Are you seeing the point guys? Yes. So this is called, this is the heart of homeomorphism. See what is interesting is when two things are homeomorphic and when two things are not. When two things are not homeomorphic it means that they are fundamentally different. So for example and why do mathematicians play with such silly ideas you may wonder. You know they seem to have too much idle time, it seems to me. But it turns out that this is actually much of what we do comes down at heart to these basic ideas. If you want to say, you can go about fields like machine learning, et cetera, in a cookie cutter way, just this library, that library, that Python library, this R library, and pick up gazillions of things. Or you can sort of go straight to the root, the foundation, I guess. I'll give you an example. So imagine that you have a ring. That intersects each other, you know, people often care. In the US, it is and in the Western Hemisphere, it's common for young couples, I don't know if it is still common to be often given a gift of a ring. But this is supposed to be person A and this is supposed to be the spouse. And these rings, they're locked in like this. They're locked in. You realize what I'm saying? You any deformation that will open the rings? Think about it. Is there any smooth movement that will unlock the rings from each other, unlink the loops? No, sir. Ah, sir. Yes. There is no such deformation, isn't there? And in fact, that is the symbolism of giving young couple, I believe, this sort of thing. They are trying to, I'm not very sure, but somebody here can clarify for me. But I suppose it symbolifies that the two people are joined forever. They should stick together or something like that. That's the symbolism. So anyway, so that's that. You can't uncouple it. It's forever coupled. So you would see that these two things have completely different topology. different topology, different topology. Isn't it? You can't go from here to this. It sort of actually one of the things that I worked on during research in theoretical physics is my research group, it wasn't my idea, it was my professor's idea. They had hypothesized that the reason the proton is so stable and it doesn't decay at all is because it is actually something like this, a knot. I mean, it's sort of in a very rough sense. It is a topological solid basically. But in a very rough way, you can think of it as a knot that you cannot unknot. You cannot go from here to here. So for a proton to decay, in other words become a lesser thing or disintegrate, it needs to be unknotted. And it takes tremendous energy to do that. You can't just do it, which is why protons are so stable. I think that we know that protons live at least 10 to the 38 years, to the of our knowledge is greater than his lifetime of a proton is greater than 10 to the 28 years nothing has that lifetime. Vaidhyanathan Ramamurthy, So it's very stable so that goes again back to the idea that these certain geometries are very stable and you can go from that to this would this would be considered a trivial geometry. from that to this would be considered a trivial geometry, relatively speaking, trivial geometry, simple geometry that compared to this one. So you can't go back to this. Anyway, those are the ideas. Another example is, suppose you take a paper. So you have a sheet of paper and you align these two roles. You will get a cylinder, You will get a ring, actually a ring. Let me just take a very long bit of paper. So to illustrate the point, just imagine a long strip of paper. And if you align this ring with this, what will you get? You'll get this nice structure. with this, what will you get? You'll get this nice structure. Isn't it guys? If you tape this edge to this edge, let me call this A, B, C, D. So if A and C are taped and B and D are together, they're joined, what will you get? You'll get this structure. But now let's do something slightly different and do the opposite. We align. And this surface, if you look at it, it will have an outside and an inside. Has a... And an inside. Isn't it? This is basically a can of soda or your can of food there is an inside surface and there's an outside surface but now we do a slightly different experiment oh what just happened There we go. And suppose you do the opposite. You connect A to B and B to C. In other words, before you connect these two, you give it a twist in the page. Maybe I can do this and show it to you guys uh and you will know that then the concept of geometry is topologies different things will become clear so see guys i have this sheet of paper can you see it if i do no what do i get i get a mobius strip mobius strip right now it is a cylinder yeah the cylinder and this is the outer surface so it has an inside and outside but if i give it one twist then it becomes a mobius chip and a mobius chip only has surprisingly one surface it doesn't have an inside and outside it has only one surface right so if you start roaming around the movie strip you will realize that you'll come back to the original place after having traversed all the surface right so now if i tape this thing is there any way you could deform this just by without cutting this paper is there anything you could do to make it into you can try all sorts of things but will it become the simple cylinder again no no you know right so you would say that a mobius strip cannot be cannot become a slend cylinder through any homeomorphism there is no homeomorphism that will take you there that will take you there. Simple guys? Yeah. So that is the idea, the core idea I will talk about here. So now that we understand these words, homeomorphisms and diffeomorphisms, diffeomorphisms are just smoother ones. And remember that inherent in this is the invertability of it. You could always go back. That is why I gave this sliding scale 1 minus t times the original thing plus t times the final thing so it's a little bit of the original little bit of the final at t is equal to 0 it's only the original at t is equal to 1 it is only the final diagram that you are making now how is that relevant to us but before we go there let us start putting tick marks to it so now what is a manifold let's see if one second in terms of uh something that you did on your one note the paper is cut um the the document is cut a little to the left edge it needs to come a little more to the right is everybody else seeing the same way yes yes yes you you are right the document on here, I have not gotten to the document. Right. Okay. Better? Yeah. Yes. This is the trouble having too many controls on the mouse. On my mouse, I have one, two, three. So I have scroll up, scroll down, scroll left, scroll right, right click, left click, and not to mention a forward click and a backward click. And I am still getting used to this mouse. So it's really convenient of course, to work with it, but sometimes just go up. All right, so far we understand. Now, with all this background, what is a manifold? So think of a manifold as a sheet of, elastic sheet, right? Which need not be plain, which can be crumpled up. So for example, a donut is a manifold, a donut, sorry. Donut is a manifold, right? A sphere is a manifold. Any surface that you can think of, right? Smooth, I mean, so long as it is smooth. So the one condition we expect is smooth. You can't just iron a sheet with a very sharp edge and call it a manifold. When a sheet has sharp edges, it is called an orbifold and we won't deal with orbifold. Machine learning, the gist of it is, it's manifold, smooth surfaces. And the surfaces can be in higher dimensions also. So in three dimension, we think of two dimensional surfaces. We live in three dimensional in the space. So I'll use the word ambient. Ambient spaces, the space in which data lives. So our eyes see data in three-dimensional space. In this three-dimensional space, surfaces, when the natural thing we think of surfaces, the surface of a teapot, the surface of a, you know, just a bed sheet on the bed, right, or my writing tablet or anything like that these are all surfaces and they all come as two-dimensional surfaces but when mathematicians think of surfaces and spaces are in machine learning the first generalization is the data comes in arbitrary many dimensions so let us say that the data comes in a pdimensional space, initial space of p-dimensions. So you generalize from three dimensions. See a human mind cannot visualize higher dimension but logically we can sort of imagine that data is existing in p-dimensional space. So any surface there would be of one less dimension. Just as in three dimension, a surface is two dimensional. So in a P dimensional space, a surface would be P minus one dimension. Are we together? Any hypersurface? And you would call that a hypersurface. So that brings us to the concept of a hypersurface. And Asif, one quick question on, so you made a statement about, generally in machine learning, we deal with smooth surfaces. Yes. Now, would there be a writer that will add on to that to say, the smooth surface requirement is within an interval? Abounding. So yeah, that is there there so what you say is see machine learning generally in real life when you solve a problem the input space is not unbounded usually it can be but usually is not unbounded there is some region into which you are trying to model occasionally it is possible to model for the infinite extent also. It depends on a situation by situation. So one example that I would give is, let's say you have a sine wave. Now, a sine x goes from minus infinity to plus infinity. You can define sine x. So if you're trying to model sine x on the entire interval, the function that you will, that will approximate that will be entirely different from a function that just looks like one period, right? Look at this one interval, one period, right? So let me call this a limited box B, and this is the whole thing. So in other words, when you do X going from zero to two pi, the model that you would build, that you will need to build, let me call it GX, would be different from the model that is here. So for example uh if you did the ml 100 200 do you guys remember what did we approximate this with how many bends does it have two bends and so we can approximate it as a polynomial of degree three three right so we could do it as so like we could do it as a third degree polynomial means and now instead of betas and use w's weights in machine in deep learning w1 x plus w2 x squared plus w3 x cube so this will suffice to describe it in b but it will not suffice to describe it in this interval, right? Because in this interval, there are infinitely many bends. And of course, you would realize that if I want to do this fx, I will have to write a polynomial in infinite number of terms. In fact, that's what it is. When you expand a Taylor expansion of sine x has infinitely many terms and many of you would remember that x minus x cube over three factorial plus x five over five factorial and it goes on and on and on like minus x seven over seven factorial. There's no end to it, right? So the the question in machine learning always is, you're building a model for some practical purpose. So what is the domain of relevance of your model? Is it the entire extent or is it a limited extent? All right, Kringer. Yeah. Now, so with all that being said, invariant to all that, you can imagine a surface which is infinite extent. For example, you can think of a plane that has infinite extent, right? It's just an infinite bed sheet in that sense. So a hypersurface may or may not be limited or bounded, all right? So given a hypersurface, if I add one more condition to it, which is interesting, roughly speaking, and this is a very rough way of putting it, roughly, if your hyper surface does not have sharp creases. Did I spell creases correctly or is it C-R-E-E-S-E-S? I don't know. Creases. That's correct. It is manifold. What does that mean? So one example of that is, let's say that you take a sphere. Let's take a sphere. So you can take any region of the sphere, tiny region, and imagine there is an ant here. Oh, by the way, do you guys know that in Fremont at least, there is a massive ant problem that has suddenly come up. Terminix informed me that they are getting 1400 calls a day about ants. So unexpected result of climate change because the earth has become so hot and dry that this poor ant is moving into people's home in search of better ambient conditions and some moisture and they have become in a very real sense climate change refugees so anyway so imagine that you're well that's a digression. I don't know, is anybody else noticing that? Yeah, in San Ramon also same thing. Same thing, yeah. So imagine that you're a little ant. If you're a little ant, do you, this sphere, the little bit of surface that you see, and imagine that this is a giant sphere, what will it look to you like it will look to you simple flat surface I take a Cartesian surface X Y you can draw your XY coordinates does that make sense guys right so this is the and sitting here And to it, the world looks flat. There. Now, let's bring in some terminology. The part of the... Oh goodness, what did I just do? Sorry, I'll have to turn off. No, this is a... Once again, I successfully managed to go to an entirely different page. I think I'll disable the touch aspect of this writing pad. One moment, let me disable touch. This is it. Touch off. Okay. So you would say that her world is world is her world is equivalent to an R square Cartesian flat surface would you agree I little flat plane yes would you. So this little world, the little world that an ant sees, the little world that an ant sees is called a map. Because it's the map that this ant has of her world. Now what about the next ant? There is another ant here with its own little map. There's other ant but then sooner or later, oh goodness, what did I do? I have to figure out why it keeps jumping around. I apologize. There has to be a quick way for me to not do what I just did. Jason, I apologize guys. Now once again, is there a way to pin it down to this? Pin page to start. What will that do? Pin this there is a way to fix it and not see it move. I can't see a way of doing it. Alright guys, forgive me for this. Till I become smarter at using the surface. Please bear with me. So, suppose you have another little ant who has a world like this, the yellow ant. Now look at the yellow ant. and it has its own little map, right, which is r squared. But there is a problem to it. You know that the sphere is curved. So now, see, in reality, whether the ants can see it or not there is a little deformation of their worlds right actually let me make it in white and yellow let's say that the world of ant yellow ant is this and the world of the white ant is something like something like this so now there is a problem this and the white and has a coordinates and has a coordinates X and Y and p yellow ant would say as x prime y prime isn't it so far so good guys and you realize that the yellow ant is sitting here and the white ant is sitting here and see the yellow ant and the white ant are not different because these two maps, one can be diffeomorph to the other, right? Morph to the other. Means you can continuously change this world to this world. How? Make the ant go this path. If ant, white ant goes to the yellow ant to have a conversation, a conversation, what will happen? Then the world of white ant will become the same as the world of the yellow ant. Would you agree guys? So if white ant goes and meets the yellow ant, so it is at the location of the yellow ant, what has happened to the white ant's map? It is identical to the yellow ant's map. Would you agree? Yes. Would you agree? Yes. It's like this, you know, the white ant is sitting with its map and it is going and meeting the yellow ant. Right. So the thing is, gradually this map will change into this map as the white ant goes and then white and comes back. It will get back its own. Now there is something very interesting. This point P, which is xy, has a different interpretation in the yellow hand's world, which is x prime, y prime. So you would agree that once again, there's a homeomorphism between these two points. There is some function, some way to tell that x prime y prime is some function of x y right it is some function of x y you can do that and so it's so long as this condition is fulfilled you see that this surface in which you can smoothly move back and forth is you have a manifold. Roughly speaking, it's a compact, well there's something housed off compact, there's many many things here. Technical definitions are there but I'll keep it to basics. It just means that they're smooth, you can go smoothly from one guy's perspective to another guy's perspective. another guy's perspective and this is very real guys you know we all were little ants thinking that the world is flat right and now we don't think so we know that we are on a sphere so you would say that sphere is a manifold manifold right because local patches look like partition spaces, Euclidean spaces. So in other words, criteria is local patches look like partition spaces, right? Or whatever. Here it was R. So I'll just say Rn, some way. So with that understanding of manifold, now we understand all the concepts. And now we are ready to read this paper. Do we understand what is a manifold, guys? This one? Guys, if you don't understand, stop me and I'll repeat it. Asif, if you can repeat it again. OK, so I'm saying that a manifold is something whose local patches, so on which if you put tiny ants on, to each of the ant, her world will look like, for example, on the sphere. Let us say a sphere r2 local regions look flat i.e they look like cartesian space so this is true for a sphere isn't it to? To the ant, to the ant, if you want to say, to the ant, look flat. So any kind of a hyper surface, broadly, where this is true, right? And there is a way, it may be curved, curvature is perfectly fine, but if two ants are separated from each other, but they have a way to reconcile their perspectives. So the same point P, I represent as X, Y, you represent as X prime, Y prime, and we have a rule to translate back and forth, then we are in good shape, right? We are looking at, what we are looking at is a manifold. Am I making sense now, guys? That is all a manifold is. Locally, it should look flat. So for example, you crumple up your bed sheet, and to an ant, the little bit of space that the ant can see looks flat, isn't it? But to us human beings, for the longest time, the earth looked flat because we didn't travel far. So we imagine that if you went too far, you'll fall off the edge. Does that clarify Anil? Yes. Anybody else guys? Okay. So you know, these are preparations to this beautiful blog at this moment. So this is a manifold. What is the topology of the manifold? It's the structure. Think of it roughly as the structure, right? Whether it looks like a flat, whether it has a hole in it or whatever it is, that is a homeomorphism and diffeomorphisms are smooth transitions from one looking one way to another while still preserving the topology, right? You don't change the topology of the manifold you just bend it a little bit and stretch it out and bend it and so forth so for example a cup and a donut they are homeomorphic to each other because you can smoothly transition a donut into a cup and vice versa i said one question so the locally, it will look flat. But non-locally, how it will be? It could be flat globally also, right? Flat or a deformed surface. A hyper plane is globally flat. Right? Right. But it need not be. It could have bends. It could be deformed surface in any shape, any hyperplane, Right? Right. But it need not be. It could have been... It could be deformed surface in any shape, any hyperplane, basically, a hypersurface I mean. That is right. In fact, just as a small digression, what Einstein said in his general theory of relativity, once you understand manifolds, what he said was something very simple he said that if you think of space time itself as a fabric as a mani think of it as a fabric of space time it's a four-dimensional manifold but then what happens is that just as you take a surface and you put a heavy object on a rubber sheet, what will happen? It will develop a bump, isn't it? If you take a, let's say, and this is again a digression from this blog, but I thought I'll mention it to you. You take a plane, this is two dimensional, but imagine it's four dimensional. If you put a heavy object here, you expect that a deformation will form on the fabric, on the elastic fabric. This is it and so now imagine so with this deformation there and I'll just exaggerate the deformation here. What will happen to a point that happens to be traveling in a straight line? It will come and it will get bent because the surface here is bent. Do you see that? A particle or a body that is traveling in this direction will get bent. Isn't it? It's spinning by, but then you see there's this deformation and it bends. You can see, you know, in the mall you go, there are these little, these surfaces kept there for children and they'll roll a coin and the coin will spin many, many, many times and then fall at the bottom. Do you guys remember that? Seeing this in malls? So this is it. So imagine that unknown to you, the space-time itself has developed a curvature because here is a big object, heavy object, let's say the sun. And this happens to be an asteroid or something like that. when the asteroid comes within the solar system or sort of comes within the orbit it gets bent automatically and gravitation the explanation of gravity is that that the reason gravity exists is because the masses i mean the explanation is whenever you have a mass in space time it deforms the space time manifold and the degree of deformation, the curvature, the curvature here is directly proportionate. The degree of curvature is directly proportional to the amount of mass that you have here, or the energy that you have here. In fact, roughly speaking, people talk about . I won't go into the details of it. That is all that general theory of relativity says, that the curvature is energy, and the amount of energy at a place is directly the mass, controlled by the mass there in the space-time. And that's why one thing gets pulled into the orbit of another thing, or gets attracted to another thing, and so on and so forth so this is it guys you know when you when you understand these concepts of uh manifolds and homeomorphism etc a lot of these deep ideas they begin to uh become quite simple after that so now but let us go back and make progress on that we understand manifolds and topology now the So, we understand the problem of morphisms, we understand manifolds and topology. Now, the fundamental hypothesis, what is the manifold hypothesis? Sir, just a second. What is the manifolds and topologies? Let me write this sentence down. What is the manifold? And this is fundamental. Is someone asking a question? What is the difference between normal, marginal and differential? I can't hear you Harini. I think you are speaking but I can't hear you clearly. What is the difference between normal,ism and diffuomorphism? Anybody can repeat that what she asked? Ask your third question in your bullet point if you go to the top. Homeomorphism and diffuomorphism. Yeah, the difference between those two, yeah. Just think of it as very smooth transitions are diffeomorphisms, that's all. When you gradually transition one thing into the other, if you do it very smoothly using the mapping, if the mapping function is differentiable, it's a diffeomorphism, right? So in other words, if X goes to, so X goes to Y and there is a function fx, if you can take the derivative of f, if it exists, it's a diffeomorphism. And what it means is, geometrically, what does it mean for a function to have a derivative everywhere? It is just a fancy way of thinking that the function is smooth. Are we together? That's all it means. Not all functions can be smooth. It can be continuous, but it need not be smooth. For example, this function, is it smooth? No. There is a non-differentiable point at the bottom which is why i said this is a crease this is a wrinkle right this is a very strong crease here so long as you don't have creases so you so long as you don't have these edges in your function your function is a diffeomorphism simple enough guys Simple enough, guys? Yes. So now we will go to this manifold hypothesis. So the best way to do that is I'll go back to the foundations that you learned. See, regression, the classic regression is there is an X point and there is a Y, which is a target variable. You do things like this. You have points like this. When you have a point like this, you become hopeful and you say that a good prediction line is this. Y is equal to 1x. This is how you learn in elementary machine learning books, one x this is how you learn in elementary machine learning books isn't it up to a error term am i making sense guys does this look extremely familiar now yes so this is but what is a line now just ponder over it and think back we went from from we are saying that data is in r squared right data is in r squared but the the so ambient space this is the ambient space where data is presented where data is presented to us. In common sense terms, what it means is, we will be given a table, x, y, a table of data. We're given a two-dimensional data. But the only hope you have of solving this is that you find a one-dimensional line yellow is a one-dimensional line right this is r1 ie r it's a one-dimensional line are. It's a one-dimensional line. Now look at an alternative data set. If the data set is like this, so let me call this dataset A, dataset A, dataset B. Now, in which of these two cases will a regression model be successful? A. And why is that? There is pattern mapped by length. See, there is a relationship. Y does have a relationship to X and it shows itself by the fact a linear relationship that all the data points sort of reflect that relationship. Here all you can see is that if anything you can say Y and X are completely unrelated seem to be seen to have no relationship isn't it would you agree guys so you wouldn't try to build a model here in fact your model will be a null hypothesis there is no relationship between the two now observe the situation where there is a well visible relationship in a and a vis and lack of a relationship in b what can you tell about the data points in a versus data points in b geometrically what do you observe geometrically what do you observe anybody would like to venture i guess geometrically what's the distinction between a and b so is a is like a stretch surface sorry a is like a like a stretch surface no no i'm saying data two-dimensional data x y data oh okay cases in one the data looks like that in the other look at the white points and the data looks in the second case b like what it is in the lower graph so geometrically what is it what what are some of the differences that you see when you just visualize the data? So space B is more dispersed. Space A is more concentrated. But the distribution of the data is different. Line and a space, line and a surface. Yeah, so you know, you guys are coming to it. See, if you look at B, the data seems to occupy all of the two dimensional surface the two dimensional space input space right it's scattered everywhere but what do you see about a most of the data is close to a lower dimensional surface namely a line do you see that almost all the data points are either sitting on the line or close to it that almost all the data points are either sitting to the line. Isn't it? So now let's try to generalize from that. What about this? I could have the points here. It need not be a line. Suppose I have a situation like this, x, y, you would still say that the data is proximal to that, the data is proximal to. So generalization of this is that if you have a relationship in a plane, you can build a successful regression model, which is a relationship between y and x, if and only if the data are close to some function, some curve. And what I'm saying is very common sense. I'm saying that if you're looking for y is equal to f x epsilon this f is nothing but a curve 2d do you agree with that guys this function is a curve and so most of the data has to be close to the curve right up to up to this error term has to be close to the curve right up to up to this error term okay feedback that this looks obvious does this look obvious guys simple right some function now what you say now observe something in higher dimension so suppose it was 3D. 3D, your y would be some function of x1, x2, and this would look like a surface. All the data will be proximal to the surface in 3D and in higher dimension. So now generalize it to higher dimension and y would be the sum function of so let us say that dimensionality of this is Rn. So the vector space, let me make it one plus one because it is x the dimensionality of x and y. You would say that y would be some function of x, isn't it? And this would be n-dimensional hypersurface. Does this make sense, guys? Is this generalization making sense? In two dimensions, the function lives in one dimension, namely it's a curve. In three dimensions, the function lives in one dimension, namely is a curve. In three dimensions, the function lives in two dimension. In other words, it's some surface. And you can generalize to higher dimension. As simple as that. So when you say that this is true, you're saying you're making the hypothesis that all machine learning is possible see you you can find a relationship only when a relationship exists and the existence of a relationship means there is a hyper surface in the ambient space right which is of a lower dimension and the data is close to that most of the data is sitting on it or is very very close to it up to an editor right otherwise you wouldn't be able to do machine learning geometr on it or is very very close to it up to an error term right otherwise you wouldn't be able to do machine learning geometrically it's a very obvious statement isn't it so now we understand this manifold hypothesis manifold hypothesis just says that This is just says the date data given in ambient space of D dimension, let's say of capital actually live closer proximal to a lower dimensional embedded manifold. Embedded means this line, this curve is sitting inside that a higher dimensional space embedded lower dimension embedded manifold. Manifold is just your a model. So this is the manifold hypothesis. So far so good guys. So, I say what is that like, like a space RD will actually live after that, what is the word? Asir Shahbaz Ghani, Ph.D.: Will actually live proximal to a lower dimension. Oh, proximal. Okay. Asir Shahbaz Ghani, Ph.D.: Means close to, proximal, close. Just use the word close with that sense. Close to a lower dimension embedded manifold. Otherwise, you can't build a model so guys do we understand it i hope this was something by now fairly self-evident would you say pradeep are you getting it yes sir arcade it's good yeah all right all of you are good with that right okay so then we are ready to read this beautiful block let's go from the beginning and try to read this so what it says is that imagine that all the data point that you have by the way this picture in this in the printing because i made an image out of it to put it into this notepad so it is actually like this. It is one big blue line, a blue line like this and a red line like this. If you read the original blog, it's like this. So data is here or here. So suppose you're trying to write a classifier. You know that you need a decision boundary and how should your decision boundary go? It has to go like this in this space, isn't it? So if this is your x1 and this is your x2 and these are your data points, it turns out the data is most of the data points are here. He has made a solid line. Realistically, you can make it a little bit more rough line in the sense that data will be proximal to this surface. Suppose this is the reality. You want to write a classifier, you need to build a decision boundary that looks like the dotted green line. That makes sense, doesn't it? Speaker, guys, please. sense, isn't it? Speak up, guys, please. Correct. Yes. Yes. That makes sense, right? OK. So now how does it do that? What it will do is when you take a neural network, so when you say that I have a neural net, ultimately, right, so suppose this is the input, x1, x2 are the input that go in and then there's some hidden layer right so uh there's hidden and there's hidden and so forth at the end of it what comes out is some transformed variable let me call it x a prime x b prime that are coming out of these nodes these are the responses of business and then it finally goes at the last node right the logistic unit at the end so you know that the last thing what does it draw what does a logistic if you recall a logistic unit logistic regressor builds a what build a linear built a linear decision boundary. So what does it mean? If this guy has to successfully be able to classify, what you need to do is you need to take this space and somehow do a homeomorphism of this, homeo of this to some place where a straight line, a straight decision boundary, let me mark it in green, a straight decision boundary would suffice. So this is the decision boundary that this line is, this is the last unit is building. So for that to happen somehow the data has to become the blue has to somehow be here and the red curve somehow has to come here completely in this region and this region isn't it you need to take the original manifold and you need to deform it into another manifold in which a straight line goes through because that's what the last unit is doing. But then for a straight line to be the correct classifier, all the blue points should be here and all the red points should be here. Makes sense? Right. So that is what it does. And you see that happen. You look at this. So you may have all sorts of hidden layers, not just in the hidden layer, not just two elements, you can put more. But at the end of it, for you to draw this decision boundary that is here, we need some representation that divides the data. And you notice this is a green line. Where is a green line? This is a green line. Right. So all the blue points are up and all the red points are below. Right. And so what has happened, do you notice that this curve has been deformed? This curve, let me use some other marker. This. Look at the blue curve. Blue curve has become this curve. There's a homeomorphism of this curve into this curve. And there is a homeomorphism of the red curve into this. Isn't it? Gradually, it has been deformed from one to the other isn't it and so this is your like i said that the new axes are x a x b this is your x a axis this is your x b axis right and the initial axis if you think of them as x, what is it, x1? What did I initially say? X1, x2, right? This is your x1 and this is your x2 axis, right? So initially, this manifold that you have here, it is gradually deformed to this other manifold where the curves drawn on the manifold, their shape has changed isn't it but when the shape changes the beautiful thing that happens is a straight line can cut and separate the two curves easily the green line now straight green line which is the logistic layer the last logistic layer decision boundary of the last output layer isn't it does that make sense Isn't it? Does that make sense? Yes. This is it. And this is the beautiful idea that he's trying to say. And then he gives this visualization. If you see how it does it, how these functions do it, it is a provable. And the thing is the proof of this is quite simple if you think about a point let us look at the proof of it suppose you have a point x and you translate it to x plus um omega naught do you think that this is a homeomorphism all you have done is you have taken a point and you have moved it with omega naught, right? This becomes X becomes X plus omega naught. Is this a homeomorphism? If you take a plane and every point you shift, will it still smoothly transition to the other plane? Yes. Yes. A translation therefore is a homeomorphism. This is easy. Translation is homeomorph. And is it invertible? How do you invert back to the original point? You just subtract W naught. Right? Likewise, the next factor. Likewise, the next factor, suppose I take x and I multiply it by w1 x. Is this a homeomorphism? It is a stretching, right? You are just stretching it by an amount w1. Make sense? Correct. Right. So is this invertible? Of course, you can go and divide it by W one and you'll get back to your original point. If I divide it by W one, you'll get back to your original point. So this two scaling is basically stretching two is a homeomorphism is invertible so now look at this suppose you have a function z is equal to w naught plus w1 x1 and now let's go to two dimensions x1 plus w2 x2 you would agree that this is a homeomorphic transform. This is a smooth transition. All you have done is stretch the plane a little bit and moved it around a bit, right? And now comes the interesting thing. After that, suppose you apply a tanh function, a sigmoid function of some sort, right? Suppose you apply a tanh z. What is tanh z? E to the z plus e to the minus z, e to the z minus e to the minus z. I hope I got it right. Correct me if I got my pluses and minuses in a twist. But this is it. This is your tanh. Now this function, is it, what about this? Does it what is what about this does it lead to does it take z to a homeomorphism of z when i go from this tan x z is it a homeo and the answer to that is yes because this function is differentiable i can do because i can do uh let me write it because so as if you're I think you change the sign for tanh if the e to the power z minus e to the power minus C and then positive on the bottom there we go because I can differentiate tanh z is differentiable with respect to z. So in other words, it's a smooth function of z. So I can gradually deform z into tanh z, right? Therefore Therefore it also is a diffeomorphism. And so what it means is that in my layers, when I take this network, let's say any one of these nodes into which X1, X2 goes, what is it doing? It is doing a tanh of Z, where Z is this transform. So you're doing a set of homeomorphic transformations and the accumulated thing is still a homeomorphic. So your points X1, X2, ultimately they're just smoothly moving to some other point in space, which is what we see. If we go here, every point, every point here is smoothly moving to some point here. Do you see this nice square grid? And what has happened? The square grid has been deformed. Do you see that the same grid is now stretched out? You can see a bulge here, you can see a squeezing here, but it is a smooth transition from one to the other. And that is what the neural nets do what they do is they deform the space continuously the input manifold continuously until the two classes become linearly separable and when they are linearly separable the last output layer just draws a straight decision boundary for a classifier right so that is what he's trying to say and much of the paper actually talks about that then he goes on and says you can do more than that if you think about this one this picture so so far we understood here so far guys right did you ask a question so when we say the like the transformation so is it like a linear transformation or is it a fine transformation no it is fine sir see it is a smooth homeomorphism it's a non-linear it is a smooth okay it's a deformation see imagine that you have an elastic sheet right and then you put your fist into it you you realize that it's a non-linear deformation but it is still a smooth gradual deformation isn't it but it's non-linear inherently the moment you bring in tan h tan h is non-linear up to z it's aine transformation z as w naught plus w1 x1 etc ah okay got it got it yeah tanh inherently introduces a non-linearity then comes a beautiful thought so you can sometimes look at the data and tell remember this was a point i was telling you about you can look at the data and tell how many nodes you need in your neural network so suppose you solve this problem you have the red in the center and blue outside right so you if you take a neural network with only two nodes x1 x2 and what do they produce then these are the two hidden layers right what do they produce they have their different weights so how many weights will be there there will be four weights here and a bias term and a bias term here. So they will produce, let's say, XA, XB. And let's say that XA and XB go in and feed into the output layer. So now think about it. XA, XB, whatever it is is it's a homeomorphism smooth deformation of this let me call it x1 x2 it would be a smooth deformation of this to this isn't it now you can convince yourself that there is no whatever stretching you do the red circle will stay inside the blue circle if you stretch it a little bit you might get a red circle here and you might end up with a blue circle here. Isn't it? But you won't be able to take the red out of the blue. So there is no straight line that you can draw. There is no straight line that you can draw that will separate the red from the blue. Actually, I should have used... You can do it afterwards. Sorry. Let me separate the red from the blue actually i should have used sorry let me make the blue into a blue properly first so there is nothing that you can do will convert and you can in your mind try to stretch and do whatever you want to do with this plane there is no way you can go from this two-dimensional surface to this two-dimensional surface now give it a moment in your mind and see whether what i'm saying makes sense or not harini you need to fix your microphone. I can't hear you at all. What happens when we pull the red up? Try dialing, Harini, please do one thing. Take your cell phone, you can use your cell phone to dial into this Zoom. Try that because your laptop is not working. I think she's asking what if we pull the red part up up exactly but that is you are now going to three dimensions right harney you have just the right idea your idea is correct you you are now lifting the thing up so to lift it you need one more dimension isn't it? Right? What does that mean? Do you mean that this thing here, you need one more dimension basically, you need to go to a three dimensional space, you need one more node coming here, which will produce Xc, a third dimensional space, because if you could lift it into if you could give me a three dimensional space all i need to do is make the red in the third dimension make the red sort of stand out here and in this plane again the same thing about this plane and the blue part to be a circle here and therefore i can easily make a decision boundary like this. That's the point you're making Harini, isn't it? Yes. Excellent point. That's exactly the right point. And there we go. So in other words, what have you proved? You need at least three hidden nodes in the hidden layer to be able to solve the problem. And that is the point he's making. Sometimes by just looking at the topology of a problem you can tell what neural network you need and that was essentially the point i was making in the class also remember i i took a slightly different example i took the example that if your data set is like this uh what was it? This is all blueberry kind of thing. And this is some red colored berry. Oh, what happened here? This is all red berry. You need at least one hidden layer with how many nodes in it for this problem two two you need two nodes in this and let's reason why so i give the explanation that you need two hyperplanes to solve this two lines to solve this one line per logistic unit so you need two of those magic boxes to solve it now let's reason the way Ola reasoned it he's saying that see suppose you look at it as a manifold can I somehow stretch it in such a way that this begins to look like a solvable thing yeah I can right if I stretch it really really hard what i can do is gradually i can make it with if i stretch it really hard i can make all the blue points sit up and all the red points sit here effectively and then i can draw a thing like this so food something like this though this would be quite a stretch maybe a better stretch you guys can think of a better deformation maybe this is not the right one by the way we can find that out why don't we do it as a lab homework let us see that if you put two layers here you give it x1 x2 i'll give take this data and what let's see what xa xb turns out to be and let's plot xasd we will also learn whether it looks like this or looks like something else so let's take this as a homework and say what does xa i'll write it here what does x a x b space look like play around with it it's very simple you take a data something like this and you just feed it to two layer a hidden layer with two nodes and get your answer. So that is the gist of what he's saying guys. Then he goes on to other things. So he says that the moment you take a third layer, the red lifts up here, separates up and you see it happening here. You see that the red has lifted itself out from the blue plane? Right? The blue is here and the red is here. It has deformed. Obviously, it doesn't look like a circle here. I oversimplified it. It begins, it gets stretched out. But in a three-dimensional surface, now you can do a separation. You can simplify this even more suppose your data is like this is there a green line that I can draw that will separate the blue from the red I cannot do that isn't it guys look at this data but if I lift it if I could somehow do this and then then this is blue. Let's say that the blue is here. Where's the blue? This is blue. Now I can draw a decision boundary that goes like this. And that is that. That's a basic idea right it becomes even simpler that you need one more dimension to do this so in other words to solve this problem you need again how many nodes do you need you need two nodes to do this because you need to produce x a x x a x b you need a two-dimensional plane to solve this so that's how you reason about solving problems looking at the data and solving problems now he goes on to something else he says that see there is a problem of knots suppose you you're given data like this people in knot theory talk of knots and unknots so you tie your shoelace, it is not a not we think we are tying a knot but mathematicians would disagree and they would say actually are tying an unknown. Rajeshwari Janakiramanan, Because it can be you. It is not because you can actually unravel it. You can pull the strings and then it will open itself out. you can pull the strings and then it will open itself out and when you pull the string gradually so what you have done is that what you think of as a knot has gradually deformed into the straight string do you see that guys and therefore it's not a real knot whereas what is a real knot a real knot is what you see in the picture above this is is a real knot. Why? Because now you have these two rings. This is the ring that I was talking about that's popular in the US, I suppose. The young couples are given this kind of a ring. This thing, is there any deformation that will classify this data? Think about it. Is there anything you can do to these rings says that the red separates from the blue you can go to the fourth dimension and then separate it you can go to the yeah yeah so what you can do is you can keep going to higher and higher dimensions yeah you can separate but Raja Ayyanar?n See, it is a fact that all knots are unknotted in higher dimensions. So, general statement. I think that statement is true. Maybe there's some peculiar notes that cannot be. It can be. All knots can be opened up in higher dimension. So that's that. So where does this all lead us to? It's just a way of thinking about what neural networks do. So you know, this week we gave to the fundamentals of neural network, understanding what do they do, and having a strong geometric intuition, at least I find, helps a lot. Okay, that's that. Then the rest of it I won't go into. Then there's some speculations and so forth but this was the idea that I wanted us to cover in this talk and that is it place so I hope it gives you some intuition of what exactly those networks are doing without you and why do they work yes if this was a good paper and the fact that he picked it out for us to read kind of gives an opportunity to read a very focused piece to understand something fundamental thank you you can also line bottle example is good example a Klein bottle is a great example theoretical physicists love it great that you brought it up you know something very interesting about klein model you think that klein bottles can only exist in four dimensions and yet people doing image processing showed that local patches of images can be mapped like there's a homeomorphism of a patch of an image because of the color gradient you know the the the gradients and so forth are two of all things a klein bottle if you guys are interested we can discuss that paper well actually take it as a self-reading because next week i have a different topic to uh we will discuss in the a new paper that we will discuss in a new paper that we will discuss next time but it's something to know that these you know these beautiful geometric shapes are all around us it's the reality that we don't see and it's there in fact images and patches of images are mapped to the most natural mapping of an actor you know patches of a natural image is to a client bottle so can you hear me now i can hear you clearly okay uh i have like when you say when now we found the linear we found the decision boundary in a higher dimension space, right? The knot and all. So as we used in kernels, so that result is used in the lower dimension space do when you apply all this hidden layer with many neurons is you're lifting data to a higher dimensional space where the data becomes separable okay right so there is a very deep connection between all of this and what we discussed previously see all of these fields right all of these ideas are very interconnected See all of these fields, all of these ideas are very interconnected. Okay, yes. And the manifold, is it mandatory? It need to be a flat surface always or only the Cartesian space it need to be flat? No, no, Harini. The whole point is manifold is a general hyper surface. So a crumpled up brick sheet in your house is not flat. Yeah. Right. So here's the thing. You're a mom, you'll know this. Every morning you make it a flat surface. Correct. And your child sleeps in the bed and next morning, what do you find that surface to be? Crumpled up. Yeah. Crumpled up. That's your manifold. Okay. So it can change. Okay. The only condition of a manifold is that if you are an ant sitting on that, locally it looks to you flat. Okay. And also manifolds not only have curvatures in the MV space, they also have stretching. So for example, it is almost like, you know, the bed sheet has some amount of spandex or something in it and your child just finds it and, you know, takes a big ball and wraps around it and so on and so forth. So there is a bulge in the bed sheet because there's some amount of elasticity to it or spandex in it. I don't think anybody makes bed sheets with spandex just hypothetically. Yeah, okay okay okay. So it can have stretch, it can have stretch, deformations, anything. The whole point of manifold is it's a generalization. If it is absolutely flat it's a trivial manifold. That trivial manifold is a hyperplane. But you want a sphere, the surface of a sphere for example is a manifold it is stretched but locally if you sit anywhere as an antonosphere it looks flat isn't it in fact for the longest time we all lived on this big sphere that we call the earth well not a sphere but somewhat like a sphere like an apple and we all thought that the earth was flat because why we are like ants we see only a little bit and wherever we look our eyes were telling us that the earth was flat because why we are like ants. We see only a little bit and wherever we look, our eyes were telling us that the world is flat. So therefore, the earth is a manifold. So, so just to make it clear. So when we see a small surface like an ant that needs to be flat right for it a manifold. Right, it is always flat in the nature. Yeah, so the idea is you give you, why are we putting the emphasis on the flatness? See, flatness, local flatness, any function that is differentiable says that you can find the derivative of x. Locally, if you look, it will look flat. Do you see? Loc see locally it looks like a line right and so in higher dimension this curve becomes a surface so locally it looks flat so when you say that locally it looks flat all you're saying is in many ways you're seeing that it's a smooth differentiable it's a smooth surface right that's all And one very rough way of carrying these thoughts around it, think of a manifold as just a smooth surface. Okay, thank you, sir. The opposite of that would be like, you know, absolutely like, you know, edgy bend like this or discontinuities right gap that is not a manifold if you have tears in it or gaps in it or if you have non-differentiable non-smooth points in it then it's not a matter all other things generally speaking the sort of data space machine learning people look at they are all manifolds right okay sir face machine learning people look at they are all manifolds. Right? Okay, sir. All right guys, let's take a little break after that those of you who need help with your homework. Is there anybody who still needs? Harini you are going to try that out right? Try that out. Sir, I thought I will try it out and then get back to you. Do you want me to do it today or? Whenever you want. There's no hurry. Okay, then I'll get back to you. Do you want me to do it today or? Whenever you want, there's no hurry. Okay, then I'll get back to you in the week. So I'm about to stop recording guys. Sir, I wanted to ask,