 Are you folks able to see my screen? So the new topic that we'll talk about is maybe I should take a string and let me start a new page also here. Okay. Is dimensionality actually... Give me a moment so dimensionality reduction techniques are they're pretty much at the core of machine learning they they address this question is the situation simpler than what you think when we get data suppose we get feature vectors like this X 1 X 2 XP let us say that we get data like this and then whether or not there is a target variable let me forget the target there is just this data exists a p-dimensional space so these are your feature data features of the data now the question that dimensionality reduction asks is that, is there some other space which is simpler? The space representation which is simpler right or of you are has less dimensionality now what does that mean we're going to answer that question. But I am going to explain this to you geometrically. See, imagine that you have data, once again going back to the classic examples that we have been taking. Suppose you have data as this. A classic example of data that we keep taking all the time. This is data. This is your x1, x2 axis. And data comes to you like this. therefore exists in two dimensions isn't it you look at this data and you ask this question geometrically geometrically is there a surface that the data is very close to. So is there a surface or is there a curve says that most of the data is close to it. If you can find such a space, then you can say so let us see this in this would it be reasonable to say that yes I can see such a thing it is this line think of this line this line and suppose I create a new coordinate system on this line let me create a new coordinate system to the center to the center of gravity of these points let me call this direction pi 1 and pi 2 so now if you take any point let's take this point this point which point should we take let's take this point this point you realize that this point or can be represented using the x 1 x x 1 x 2 coordinate system but it can also be represented in a different coordinate system this phi 1 phi 2 it also has a phi 1 phi 2 value a different coordinate system which is the x-axis is along this line phi 1 axis and phi 2 is perpendicular to this and so the same point I can write as in terms of phi 1 and phi 2 axis does that that make sense guys so far? I'm just using two different coordinate systems. I just rotated the coordinate system in such a way that now the first axis, the new x-axis which I'm calling phi 1 is closest to the data. Most of the data is close to that is closest to the data. Most of the data is close to that. And then Y2 is perpendicular to it and it gives you a little bit of the variation. It captures that. Now, you may ask, what is the purpose of this? Why would I want to do that? But before I go into the purpose, just stare at it for a while and make sure you understand this idea, what I did. I'm saying the same point can be represented in two different coordinate systems. One is the original coordinate system, and one is a different coordinate system, phi 1, phi 2, like this, which in some sense is very close to the data. It's a system where the first axis, phi phi 1 most of the data points are close to it so far so good guys yes so now comes the interesting thing you can say well maybe as a simplification i may not care about the phi 2 point itself so suppose a point P exists Phi 1 Phi 2 maybe this little bit of jitter along Phi 2 does not matter I can just represent it approximately as just Phi 1 itself a point can be represented by just the value just the distance along this distance you know this distance which is phi 1 and that is enough and i can ignore phi 2 altogether maybe it's a bit of noise and things like that or it is sufficient to describe the data most of it by just looking at phi 1. would that look like a plausible argument, guys? So it is phi 1 and then a little bit of fluctuation along phi 2, which I may choose to ignore. So to be able to do that is called dimensionality reduction. You are saying, now, what is the dimensionality of the point now in the new coordinate system? It is a one dimension, right? It's a line. 1 dimension, you're giving the coordinates of a point just along this line 5, 1 line. Whereas initially the data was in 2 dimensions. Do you see this point guys? Is this easy to see? I want to make sure that you all understand this geometric point. It's important. Which means that initially it was a hyperplane. Like in this two dimensions, initially the data was all those dots of points, right? And it was given by five 1, phi 2, two-dimensional coordinates. Now I'm creating a new orange coordinate system. And I'm saying that only phi 1 matters, phi 2 we can sort of ignore. Because phi 2 is anyway very small, close to zero, right? I can write it, okay, let me put it this way that this is this is equal to is approximately equal to zero right or some epsilon amount some very small amount of phi 2 some very small amount and because it's a very small amount I can ignore you see that that a fight to if you just look at it the coordinates of a point along phi 2 is very small is just this little bit right rise above the 5 1 9 Are you getting it? Yes, it's a very simple argument. The whole question is. Is this I have a, I have a quick question. So, the relation, there is a relation between next 1 and next next to it's a linear relation. Right? Uh, does it also carry forward to any other predictable relation between? Yes, absolutely. So, in other words, you realize that X, two in this particular case, X, two, let me write you the white white color. X, two is some equal to some beta naught less beta 1 X X 1 isn't it that's a linear relationship you're seeing this linear relationship people typically write it yeah so this is it in the same way Phi 2 is equal to some other coefficients beta naught 0 tilde plus beta 1 tilde phi 1. This relationship is still preserved. What it means is that we just need a mapping from x1, x2 to phi 1, phi 2. And that is a straightforward mapping. You can write it at you. So you say phi 1 of a point is equal to these are usually called loadings let me call it theta 1 X 1 plus theta 2 X 2 theta 1 1 1 2 and phi 2 is equal to theta 2 1 x one plus theta two two x two right so there are four constants that will transform a point x one x two to phi one phi two right so because it's linear in the original space of course it will remain linear relation you can transform it to a different system where it falls to the x axis for the transformed axis. So these are the transformed axis. But still there is a relation. Can we still reduce the dimension? Yeah, so what you are basically saying is that even though this relationship holds only Phi 1 matters Phi 2 can be ignored can be approximately ignored it depends on the data so I'll give you in this particular example of data it seems to be so yeah right ignore based on data it depends on data what just happened sorry depends on data but in this particular example if you look at it you know the phi 2 values are very small and phi 1 captures pretty much the location of the point very well right so the linear relationship still holds. The point is you just move to a different coordinate system. Another way to say it is this. See relationship between x1, x2 or phi 1, phi 2 they capture the geometric shape of the data. Shape of the data doesn't change just because you change your coordinate system isn't it that's a way to think about it relationships and remain the same so now while and by the way this is all very elementary coordinate geometry you have done all this at some point in high school now let me go beyond to the point that you haven't done at some point in high school. Now let me go beyond to the point that you haven't done. So here you realize that in this particular data you seem to be able to find a relationship a lower dimensional space. Now let me give you some thought experiments. Think about this. Consider this data set. These situations. But do you think this, let me just call it case A, and the case B was the case that you were looking at, in which case A looking at in which is B right now Casey do you think data really is occupying a lower dimensional surface like it's close to a line or a curve what would you say about Casey would you say about case a right so this seems to be less data does not seem reducible same reducible to a lower dimension civil And here, the data does seem to be close to the line. line diagonal diagonal line isn't it so what it means is that in some situation you can reduce the dimensionality in some situation you can we are going to learn about one particular technique today it is called principal component analysis Today, it is called principal component analysis. It's the simplest dimensionality reduction technique and yet very powerful. This is like linear regression. It's simple but very very effective in the same way principal component same way principle component especially in areas of medical sciences biological sciences it is very extensively used signal process many areas so interestingly this thing was discovered and rediscovered in history many times over initially the discovery was actually attributed to some, I believe some French mathematicians or something. Later on, and some very interesting history is there, it turns out that there was a person in India whose name was Kosambi. Kosambi did work on this, I think in the pre-independence or independence time of India 1940s. He actually discovered the principal component analysis on his own. I suppose other people did it later in my view but they all sort of independently discovered it around the same time. So when Kusambi discovered it he wrote a paper and you have to realize these were the times when it was if you if you're coming from Asia India China etc these were post-colonial or just getting out of colonialism these were economically devastated lands and scholarship was more or less destroyed great universities didn't quite exist there were just one or two of them and access to top journals was not there so it's interesting that this person kusambi discovered this um he was actually from bananas from bhu He was actually from bananas from BHU So one day I walked over to be achieved the math department and I said, okay Is there a plaque or a statue or something in honor of Kusambi? And so the irony is that they all looked at me blankly and said who's Kusambi? So It's a I irony that we don't... A great mathematician was forgotten. He did a lot of great things, but one of the things he did was principal component analysis. He discovered it, or even if you say that other people were discovering it, he rediscovered it on his own. So what is this idea of principal component analysis and what is the big deal about goes back to this intuition look at Casey and Casey this question can we find linear and therefore hyperplane hiding embedded In the feature space, it's a very important question it has. Feature space that most of or are very close to so we generalize from a line to a high plane and to a hyper plane in higher dimensions but intuition is easier in two dimensions so just think of light so when you look at the case a the answer would be no when you look at case B the answer is yes right there is also a case where it doesn't work let's say that you have a relationship like this case C if you remember I gave you a data set like this do you guys remember your data set to sign wave so here if you look at it is there are all the data near a lower dimensional hyper plane no it is close to a curve this curve is there this curve is not the line this curve is not line isn't it because it's bent lines are straight right so this case C shows a limitation of principal component analysis in other words it will not work principal component specifically looks for a line or a hyper plane are we together so there is a lower dimensional curve or a surface in data it may be there but principal component analysis will not discover it nonetheless it's a powerful technique this is the limitation of. Nonetheless it's a powerful technique. This is the limitation of that so it's a limitation. Let me mark it here in the notes. PCA won't work well here. So now we know the limitations and we know where it does and does not work. So let's think about it. How does it work? So I'll take you in a very interesting journey. See, it's a. Remember I talked about something called the covariance matrix long, long ago in ancient days when we started this workshop, do you remember covariance? The concept of covariance between two variables. Okay. Well, we matrix. Do you remember those covariance matrix? So let me write it. The suppose you have two things. X1, X2 covariance between X1, X2 is equal to what? You go to the center of gravity of the data you go here and from here which is the x minus mu x1 the center of x1 mu1 we can call it mu1 x2 minus mu2 you multiply is basically proportional to these two quantities divided by their individual standard deviations right sigma one sigma two sigma one sigma two do you remember this definition of covariance we did long, long ago? If you have forgotten, this was the definition. What it measures is how much does X2 increase when X1 increases, right? So the result was, and it is very much related to correlation. So think of correlation if that is more intuitive to you so here let me take this case now here what do you expect covariance of x1 x2 to be approximately equal to zero isn't it they don't seem to vary when x1 increases x2 could be decreasing or increasing you know you can't tell there is no pattern in the data isn't it on the other hand if you look at data like this you expect covariance to be greater than zero isn't it on the other hand if you see data like this you expect covariance to be less than zero or do we realize the three situations this is just a review guys of what covariance is and what the definition of covariance was good go and listen to the lecture lecture lecture 2 which was on covariance it will give you a bit of a background on that so now I'm going to use the covariance and show some very interesting magic. I'll do that using a concept called matrix. Imagine that you have a point, any point. Let me use the point x1, x2. x1, x2. Let us say i can represent x1 prime x2 prime as some rotation around x1 x2 rotation of the x1 x2 points't it? All I have done is I have rotated the initial point P to P prime, isn't it? P prime, and this is P. And I have applied some rotation operation, rotate by theta degrees, isn't it? Now, when you work out the equations, you will realize that this can be written as x1 cosine theta minus x2 sine theta if you want the derivation we can go to it but I think let me know at the end of the lecture if you guys want a derivation and we have to do that x2 is x1 sine theta plus x2 cosine theta, the original. So the new values of x1, x2 is like this. Now, some of you may vaguely remember that this used to be the equation of rotation in trigonometry or coordinate geometry in your middle school, high school classrooms. Does anybody remember these rotation equations? Oh By the way, I might have got my plus and minus between this and this a bit wrong. I think this is correct. So You guys remember these equations You want me to prove it I can prove it get into trigometry. You can or you can take it as a fact of life. Take your word. So I'll tell you where it comes from. It comes from the identity cosine theta plus phi. Expand it out. What does it mean? And sine theta plus phi. Expand it out and see what it means. And then you will you will find the answer. It will basically lead you to this. So you'll have to remember those things. Cosine theta cosine phi minus sine theta sine phi. On the other hand, the second one would be sine theta cosine phi plus sine phi cosine theta and so forth and so from this the above identities follow but leave it like that and so you can write it as in in this thing can be written in the most succinct notation cosine theta minus sine theta as matrix notation same to the cosine theta x1 x2 by the way if you don't get this mathematics don't worry when you do the pca is just a couple of lines of code but this is just to understand it so this is called a rotation matrix a matrix matrix of rotation by theta this is one now sometimes all you do is take a point x1, x2 and you just stretch the vector. You multiply it by lambda. So you go to lambda x1, lambda x2. You realize that you just stretch the points. So when you stretch it, you can write the stretching as actually, you can write it as actually x1, x2 prime is equal to lambda. You can convince yourself that this is what it is, x1, x2. So make scaling matrix. It's the scaling matrix. And now comes an interesting thought. If you look at rotation and scaling, both of which are given by two simple matrix operations, it turns out that it forms sort of an algebra of transformations of points on a coordinate plane. Let's say that you take this point X1, X2 and you take some other arbitrary point X1 prime, X2 prime. Think about it a little bit carefully. This point, this point. Now, what I can do is I can think of it first as a rotation to this point and then as a scaling a scaling from this to sorry a scaling operation of this green vector to this vector. So let me call this point a P to P middle let me call it P prime or P tilde and then finally to 2 P prime so a P to P tilde or P tilde sorry P tilde just happened P tilde is a what it's a rotation do you agree guys the P the movement from P to P tilde is just a rotation and from P tilde to P prime is a scaling does this make sense guys so every movement of a point to any other point can be thought of as a rotation or a scaling. Are we together? So what you can do is you can apply to X1, X2. First, you can apply the rotation matrix, the rotation, and this will give you the P tilde. And then you can get the p prime by saying scaling on the rotation on the rotation on p tilde let me write it in simple term this is on original p and scaling on P tilde P prime is scaling on P tilde and in terms of notation this is x1 x2 and this will be equal to s times r s matrix r matrix x1 x2 right and so what you're saying is that this will be a more complicated matrix and so every matrix can essentially be decomposed into sort of pieces atomic units of what it does to a point this is a bit of linear algebra I don't want to go too much into the linear algebra it's just a point to know that actually much of linear algebra becomes very very simple if you think of it in only two dimensions and then from there you can generalize it otherwise the books of linear algebra can be pretty abstract but there isn't much to it if you just sit down on a piece of paper draw it out using geometrically you'll realize that much of linear algebra is saying some pretty elegant and simple things beautiful things but they're all very simple and obvious and very powerful things actually powerful reasons for this so one doesn't will follow here so we say that any matrix any matrix matrix M will take a point P to M to some other point P prime would you agree right it will basically take a point x vector to x prime vector to a different point does this make sense guys in other words x1 x2 goes to x1 prime x2 prime that's what a matrix does it transforms every point to some other point well you say that's very interesting now how is all of this matrix notation useful to us and here comes the very interesting connection that it's a it's a most startling and remarkable connection and it is the foundation of the pc the main idea that mathematically elegant it's this you take a unit circle. So take a point. Take a unit circle. What is a unit circle? What's the radius of a unit circle? One. Take a unit circle in your plane x1, x2 and consider all the points along the unit circle. So how many points exist on the unit circle? Of course infinitely many but we'll approximate it with some finitely many points. I'm just putting circles around it. And then you do what to each point apply the matrix M to each point so when you do that what will happen is you'll end up with a different matrix. What it will do is that it will deform your circle. Now, just take it as a fact that it happens at this moment, if it is not obvious. It takes a little bit of time or convincing for you to see that it will become something like this. or convincing for you to see that it will become something like this it will deform it into an ellipse and you know it may not for example if the scaling is not there but then all it will do is it will just rotate the circle but I'm taking an arbitrary case in general the matrix will deform those points on the unit circle into a points on an ellipse are we together guys huh so I'll write this result M will deform a unit circle into ellipse when you have an ellipse if you remember the cave ellipse we talk of something called the principal components the principal axes of an ellipse you does this a word ring a bell this is your principal axes the major axis access the minor axis of the ellipse minor axis do you remember these words guys from your high school algebra but take it as a fact the the line that goes along the main diagonal is considered the major axis the line that goes along the perpendicular diagonal is considered the minor axis. Now when you generalize to higher dimensions you don't talk about major and minor axis, you talk of first axis, second axis, third axis. Are we together? Now this particular direction, this direction, let us have a unit vector along unit vector along this direction. So think about this point. So think about this point. What happened to this point? At this point, which was, let me call it U1, do you realize that this point just got stretched? And likewise, a point U2, this point also, this point moved to U, right, and this moved to, this got stretched here, this actually got shrank, this point also got shrank to this point. So I'll just highlight those four points along the circle. And I'll just take two vectors these points are ultimately the two vectors u2 unit vectors right because it was a unit circle so there's something very special about unit vectors the matrix when it applied to the unit vector u1 it just stretched it it became lambda times lambda 1 some amount of stretching of the unit vector do you see right so u1 got stretched to u1 prime likewise m u2 got stretched to some lambda 2 to actually didn't get stretched to it got shrank to you to prime. Do you see this guys are making some very simple statements. It makes sense, isn't it? Now, this is a little bit of algebra that is coordinate geometry and algebra is word going and rushing it up. But we are also going to do the engineering math class. All of these things will cover in slow and careful detail. But at this moment, I want to come to the main idea so what it now there is a fancy name for this you won those vectors or those directions where only stretching happens are called eigen eigenvectors of m right and lambda 1 lambda 2 are called together? So eigenvector and eigenvalue, these are two terms. And that is all actually. What I just introduced you to is a matrix, u, which is just eigenvalue vector u2, right? This vector. Now, you remember this vector will have x1, x2 component. This is actually u11, u12, u21, u22. This is this, right? And a lambda vector, a diagonal vector, d is equal to just lambda 1, lambda 2, 0, 0. I can create matrices out of these eigenvectors and eigenvalues. And there is a most remarkable statement that under well-behaved conditions, generally well-behaved conditions, which we won't go into, it gets a bit technical, UD U transpose. Any matrix M can be decomposed into two components, the eigenvector matrix and the eigenvalue diagonal matrix so it decomposes into UD you transpose the rotated version of U and this is called the eigenvalue decomposition so this is getting a bit mathematical here but the intuition is exactly what I just mentioned a circle going to an ellipse eigenvalue decomposition I can value decomposition and now well you may say well that is all pretty lovely but what does it have to do with what we are dealing with and let's now connect it to it the relationship is the beautiful connection is to our world let me call it PCA is this if your M is if your M is the covariance matrix that is the covariance x1 x2 so sorry x1 x1 itself this is just the variance part of the matrix x1 a covariance of x2 with itself which is just the variance and covariance of x1 x2 covariance of x1 x2 as you can see these two things are exactly the same this particular matrix this is called the covariance matrix matrix typically it's written M as in this particular case of data, you write it as Sigma because it's rather tedious to write code. So people typically write it as Sigma 1 1 Sigma 1 2 2 2. Sigma 1 2 Sigma 1 2. Implicitly the meaning is understood or you can sometimes people write it in books as Sigma X 1 Sigma square I'm sorry I take this back here this is the square of it and a square of Sigma 2 square and these are the covariance part of it actually how should I write it did you not use the word Sigma sometimes we can do that okay but we just write it as covariance X 1 X 2 and covariance X 1 X 2 why if you remember covariance of a thing to itself is nothing but Sigma the variance variance is Sigma square right so I won't write it in other notation now beautiful thing so what it says is that if you create a covariance matrix and you apply it to data something that a data that has no relationship like random noise you see this is random noise right do you see any relationship here guys there's no relationship right this is just Gaussian noise Gaussian noise. The data point is centered around here. And to this, apply the M transform, the covariance transform. If you apply it, something beautiful will happen. It will become this data. So in other words, when you see data like this and you search for this relationship, now what is this? It is your ellipse here. The principal components are just the eigenvectors. phi 1, phi 2 that we were searching for originally are nothing but but you know, a principle, let me just call it directions you know principle let me just call it directions that define the line and nothing but the eigen like this now there is some very beautiful relationship when you write the diagonal matrix as lambda 1 lambda 2 if there is truly a situation like this where the data is strongly correlated then what will happen is lambda 1 will be much much greater than lambda 2 right and you can generalize it to higher dimensions but i'll leave it to that and so what you can do is you can approximate it as lambda one zero and any point you can just represent it along the one unit vector you know the phi one component is the value along this unit vector and so it's a long intellectual journey what would we do we did something quite quite remarkable and we may have gotten lost let me refresh what I said what I'm saying is first of all I reminded you what covariance is and what covariance between two this x1 x2 for data what does it mean there is a certain formula for it we go over the formula we have done this in our second lecture so you can go back and do one but just to recap here the covariance is zero here the covariance is positive and here in this case the covariance is negative so we stop there that is about covariance then we go and do some basic amount of linear algebra we say that you know when you take a point you can either rotate a point or you can scale a point when you rotate this is the equation of rotation so there's a rotation matrix or you can scale the point scale the vector make it bigger you can rotate a vector make it bigger a point make it further from the origin but in the same direction so that is a operation. And now if you mix scaling and the rotation, you can basically take a point to some other arbitrary point. So every transformation therefore can be thought of as a scaling and a rotation. Now comes the interesting thing. If you look at the matrix, the M, therefore any matrix is basically a point to some other point and therefore it's arbitrary transformation which we can think of internally as some rotation and some scaling. Now comes an interesting fact. You take all the points on a unit circle and you apply an arbitrary matrix to it, well behaved matrix, what it will do is it will stretch it out into an ellipse. Not always. If there is no scaling involved, it may just rotate the circle into another side which itself is an ellipse but that's about generally an arbitrary matrix will make it into an ellipse the points will go now you ask for those points on the unit circle we just got stretched which did not get rotated you'll realize that they only for such ones which only gets traced and not rotated now the direction of those the the point side is the eigenvectors the two eigenvectors would be the direction of the first check in the direction of the second string but you always in by convention say first stretch is the bigger of the stretching now you say well alright so therefore this comes if you apply the unit vector the points on that unit center which are all unit vectors with them those four special points will just get stretched you want in YouTube will just get stretched and minus u1 and minus u2 will just get stretched. Minus U1 and minus U2 will also get stretched. The other two points being minus U1 minus U2. So let's focus on only U1 and U2. They both get stretched, or rather shrunk if needed. But they don't get rotated. Now if you create a matrix out of the eigenvectors, these vectors, and out of the eigenvalues, which is the amount of stretching you get two vectors the UND but you will read together there's something very interesting and it's one of the sort of one of the crown jewels of linear algebra when it comes to a statement then it says that you know what any square matrix will not any under certain well-behaved conditions matrices can be decomposed into UD you transpose like this it's called the eigenvalue decomposition now what is this relationship to dimensionality reduction that is it if you create a matrix which happens to be the covariance matrix then the beautiful thing is that the covariance matrix right if you decompose the covariance matrix into the eigenvectors and right you will find the directions you'll find the u1 u2 take. Take the matrix and decompose into u and d. The beautiful thing will be that you'll find a way to find the line along which most of the data is close to. That will be your u1 axis. Am I making sense? And if you think back about it, some things follow. u1 is the direction of maximum variance. The maximum spread in the data is along follow u1 is the direction of maximum variance you know the maximum spread in the data is along the u1 direction right the the the longer part of the the principle elliptical ellipse so the idea is that data is where is our data this data right it got stretched to well okay let me not write it this way because we started with centered around zero so I should make it centered around zero here also so what this thing did is it made it into an ellipse and this direction is your u1 direction the principal component and basically using because you can transform a point from here to here you have you have a means to do a move to a new coordinate system, which is based on U1, U2. And if you ignore the U2 direction, you have reduced the dimensionality of the problem to just U1. So every point can be just specified on how far from the origin it is along the U1 axis. So you have reduced a two-dimensional problem to a one-dimensional problem. Now comes the generalization. What is true in two dimensions can actually be true in arbitrary many dimensions. It becomes a hyperplane. The data is shrunk-wrapped in an ellipsoid or a hyper ellipsoid and the principal axes together they all form a plane that the data is close to are we together and to be able to find that first of all that itself is learning you found a very interesting pattern in the data it is not occupying all of the feature space it is limited to a very uh people use the word sub manifold but i'll just use the word surface a surface hidden in the larger feature space and most of the data is just close to that it is almost like you know you have a room and the room has three dimensional space but let us say that you have a. All the data is at a, let's say that you have a sheet. Of metal or sheet of something, and it is at a diagonal direction. Imagine a 2 dimensional sheet. Of metal, which is at a diagonal in the room. And now also imagine that there is a source of heat in the very center of that steel of that metal plate now what will happen if you really think about it at any given point in the room there will be a certain temperature right but if you think more about it, you'll realize that temperature is pretty much controlled by only 1 factor. If you just assume that air is the bad conductor. So, the air is unaffected. Temperature will be higher only above room temperature only along the metal plate because metals are conductors and the temperature at any given point will be dependent on how far it is from the center of the plate where the source of heat is where the hot object is placed so if you know if you get data about temperature of points along the in the room principal component analysis what it does is it discovers in effect the steel the metal plate and says that all you need to know is an x y coordinate system centered on the plate itself from an a coordinate system with respect to the origin of the you know the center of the plate in fact you can go even further and you can say that it is actually a one-dimensional problem because temperature heat spreads. Symmetrically isotropic, or it is symmetric in all directions. So the only thing that matters is distance on the metal plate from the center of the. Plate the center of the sheet and so the only thing that matters is radius are. So, what have you done? You have changed the problem of temperature in a room at different points to the problem of just distance from the center of the plate and whether or not the point is on the plate. If it is off the plate, you just assume that it's room temperature. If it is on the plate, then the only thing that matters is how far are you from the center of the plate. Are we together, guys? So you can simplify the problem. It is not just that you reduce the dimensionality of the problem. You actually discovered something interesting, how the data is behaving in the feature space. There's a pattern to it. And when you discover the pattern, one pattern you discovered was clustering another pattern is that you can actually describe it with the lower dimensional space a line is a lower dimension than a plane right and so forth you can describe it just as its coordinates along a line for example and that is dimensionality reduction so we just learn principal component analysis that is dimensionality reduction so we just learn principal component analysis that is only one you can do dimensionality reduction using other techniques more powerful techniques what has happened is the world is often very nonlinear like for example our data set to a PCA will not principal component analysis will not discover the curve because it's inherently not designed to very simple method but there are other techniques that will discover the plane right that curve. One technique is very simple you use polynomial expansion of the space you take the coordinates as not just x and y but you take the coordinates as x1, x, x square X square X cube and Y now you're looking at a four dimensional surface four dimensional space and in that four dimensional feature space actually what looks like a curve in two dimensions becomes a hyper plane a straight hyper plane in four dimensions I see if are you sharing your screen or it is frozen like here for long time because I'm just talking I'm not moving it I'm just summarizing what we learned so this is it guys that's all I have to teach you today today was a bit of a longer session but this is dimensionality reduction theory now theory we went through quite a bit of a longer session. But this is dimensionality reduction theory. Now theory, we went through quite a bit of complicated mathematics. I might have gone a bit fast. We'll do it in greater detail slowly. When you do the math of data science, if those of you who are joining that, you will learn these things slowly. You'll learn linear algebra. You'll review your calculus and all of that. So all of these ideas will begin to look very very simple to you at that point but at this moment if it is a bit too fast or it went over your head I apologize for it but there is no other way to explain this except to the way I did the good news is that the practical part that we'll do on Saturday is very straightforward. It is literally like 3 lines of code. It is even easier in some sense and clustering. Very easily you can find that if it works, it works if it doesn't work. For example, for the, for the sign wave data set, it won't work directly. So that's that and that is all I have on this topic on this any questions no not for me any doubts right if you can scroll up to when you started the dimensionality reduction, maybe I misunderstood somewhere. You said if there's no linearity in the data, then the, yeah, yeah, yeah. Then the. Fail. Okay. That's it. Okay. That is it so in other words PCA works only when there is correlation present strong correlation present it's a simple and powerful technique it has limitations but it is very very widely used you see it's your first course of action when you're doing dimensionality reduction you should always start away right off the bat with PC are we together guys any other questions guys in you the doubt by the way was it more or less clear I Without the mathematical background, perhaps you didn't absorb 100% of it, but did you get the main intuition of PCA? Yes. Nice. Yes, sir. It was clear. Very good, guys. And so in that case, I'll end today's session. Let me end the recording.