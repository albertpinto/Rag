 All right folks. Excuse me. So, since many of you have joined just now, I'll start all over again. Today is day six of our bootcamp. This is Saturday morning, 10am Pacific time. Beautiful and sunny and I see people are still streaming it. Today represents a pivoting point in our bootcamp. So far, we have been dealing with large language models. Now, if you large is a relative word, what looks large today will look tiny tomorrow. There are rumors like when models reached hundreds of millions of parameters, they look ginormous. But today, if you have a few hundred million parameters, most people don't even consider you much of a large language model. If you remember when we did the LoRa paper last time, we found ways to fine tune with much smaller models and sometimes the so-called smaller models were in that range, hundreds of millions of of parameters as opposed to billions of parameters when we did blip2 we said that blip2 does a wonderful job it it just sandwiches a q former between a big visual encoder high performance visual encoder and a big large language model that is generative and it all it does is in many ways it creates alignment between the visual and the textual that was the blip to architecture and that was q forma if i remember right q forma even in the original paper was about 180 or 188 million parameters. So a few hundred million parameters is now being considered a feat of brevity or smallness that you can get away achieving something with so little. So today we are in the range of billions of parameters. This is 2023. as we make progress there are increasing rumors that and and very plausible rumors that the next set of models that are coming on the horizon they are all in trillion soft parameters there are even rumors of hundreds of trillions of parameters. I don't know how credible those rumors are, but I suppose every AI company is wondering if the models that they're building is big enough and if their competitor is making a model 10x that size. So big seems to be the movement in corporate efforts at AI. On the other hand, within the AI community, besides the big giants like Google and the Facebooks and the Microsoft and OpenAI, there is another sort of stakeholder or another kid in town which is the open source movement open source movement has been seminal in advancing computer science in creating excellent software as you know i've been personally both a great beneficiary of open source I mostly use as far as possible I try my best to use on the open source and at the same time a big proponent of open source for many many years ever since the at Linux came out. I started with Red Hat, I believe Red Hat 3.2 was my first Red Hat. And today we are on Fedora, what is it, Fedora 20? 9.2. Yeah, Red Hat is at 9.2 and Fedora is at 34, Fedora 34, right? So the world has changed in 20 years, well, more than 20, 30 years, 1994 to almost 30 years. The world has changed quite a bit, but open source, if anything, 1990s was a time when people thought that open source is toxic. Today, nobody thinks that open source is a force that dominates almost all of software development. But open source is having a bit of trouble at this particular moment with the AI community all of a sudden, because as language models become large and very large, you can't load it. Open source is often done by people, by companies who don't have that vast amount of budget quite often. And often a lot of breakthroughs are done by people quite literally sitting in their basements in a very real sense, not just a metaphorical sense. Today, for example, a LoRa adaptation can be done by any one of us. Many of these new changes, so open source has taken a completely different pathway. It is about efficiency, efficient fine tuning, small changes, things that can work with limited hardware. Because the big constraining factor between the corporations and the open sources, factor between the corporations and the open sources the corporation seems to have vast quantities of these gpus or ai accelerator cards tpus that the rest of us do not have access to and in a way i want to make that point of bifurcation the context of the next few boot camp days. Question is, if you even if you work for big corporations for the next few days, assume that you don't assume that you're in the open source camp. You have each one of you. Most of you have built your own machines by now. Thanks to Sukpal, many of you have really shiny machines. And with these machines you have one GPU, two GPUs. Some of you with deeper pockets have four GPUs. But I don't believe anyone of you have reached 8 GPUs. Sukpal, anybody ask for 8 GPU machine yet? Not yet. So this represents a limit of where you are so the projects that we will do we will ask this question how do you get up to 90 percent or 95 percent of what the big models can achieve using much smaller models originally that has to be one context today is also a bifurcation point we are going to extend what large language models mean the the whole word large as you know large has become relative today's largest tomorrow's not so large so we'll drop the word large and we just speak of language models. That brings up the question, what is language? And I want to spend a little bit more on the linguistic aspects of it. What makes for a language? If language is a vehicle for communication, then text is not the only form of language, isn't it? We speak far more than we write. In cultures where there is no sort of a written language, quite often, and there are many, many cultures which are very racial. In fact, I would imagine that the new generation only seems to be talking in emoticons. So the emails have gotten like, first of all, people used to write when we were kids, we were taught to write, letter writing was an art, we were actually taught how to start a letter, how to end it and gracefully and so forth. Then came emails, which made it somewhat informal. Then came, I don't know, texting, right? And I don't know what is next. Even the long texts have disappeared. And now it seems to be just some very cryptic words and emoticons that are there. But at the end of it that too is communication the young are able to communicate perfectly fine with that isn't it and many many times many times what communicates may not even be the words but how the word is pronounced for example For example, a biblical history is that of Shibboleth and Shibboleth. One way for a tribe to know that there is an intruder in their midst is the tribe knew how to pronounce the word Shibboleth, whereas the way it was written, when an outsider would pronounce the word, apparently, it would be as Shibboleth, and you would know this is an outsider would pronounce the word apparently it would be as chibboleth and you would know this is an outsider right so it means language if you realize if you generalize the concept of language as a means of communication then just the pronunciation just the accent itself Communication, isn't it? Today, just with our accent, we communicate a lot. We may be saying the same words, but we are communicating that we come, like for example, when I speak, there is semantic content in what I'm saying. But there is also other content. You're listening to me and from my accent it is pretty clear that I come from South Asia, Southeast Asia, right? I come from India. It is more or less evident from the way I form the sentences and things like that. Other things may be clear. One can deduce perhaps whether the person, when you hear somebody, is educated or not. You can infer, you can infer which part of the country the person belongs to in us for example, in the East Coast, they distinguish based on whether they use a Rurian or non-Rurian accent. For example, the non-Rurian accent would eat up all the Rs. So instead of saying laughter, they would say laughter. The R goes silent, or daughter, or water, rather than wattle. So sort of. And a lot is conveyed. For example, Southeast Asians have a tremendous difficulty telling apart telling apart wine from wine. I don't think I can get the pronunciation right either. One is with a water and one is B. And I hope, at least I tried to make a tongue gesture, right? I can't tell the distinction, but of course, it's pretty obvious to every other culture. So a lot is communicated in not just how we, what we say, but in how we say it. And in fact, in our mispronunciations, in our misactions, a lot is conveyed and much is conveyed in other forms of data, for example, in the US, quite often, just looking at the tie of a politician, you can tell their affiliation, the color of the tie. Is it blue or is it red? And so in other words, language has many forms. Once you realize that you see that, of pictures, a picture is worth 1000 words in a very literal sense. Pictures are narrators of things that happened or may not have happened. As we move towards a collapse of reality, you can create all sorts of pictures of things that are impossible. For example, an elephant sitting on the back of a bird as the bird flies in the sky. Today, we can generate that. And Patrick would do a great job in post-mortem. So this is generative AI. So what is it? What does it convey when you see that? It conveys a different message. It conveys talent, creativity on behalf of the person who created that. Though it conveys obviously a non-real scenario. You have videos. Videos are a language. So if you broaden the concept of a language to many modes of life, many modes of text, sound, because all this accent and pronunciations belong to sound, and spoken word belongs to sound, to images. Now images are the standard images in the visible spectrum, but there are images, the visible spectrum is a tiny part of the electromagnetic spectrum. You can have images, for example, NASA for years has been having images in all sorts of the entire electromagnetic spectrum from deep radio waves, which are miles and miles long or tens of miles or hundreds of miles long wavelength to on the other hand from there you can go to millimeter wave microwave then you go to the visible light spectrum you go to then you go to the visible light spectrum, you go to beyond ultraviolet, infrared and visible, then ultraviolet, and then you go to deep ultraviolet, and finally you come to x-rays and then to gamma rays. Those are extremely high frequency electromagnetic waves and each of those produce their own imaging of the sky. It turns out that most of the data, if you were to look at from NASA, they are images that are not in the visible spectrum. Visible spectrum takes you that far. Why are they not in visible spectrum? Because visible spectrum has the problem, like every electromagnetic wave goes through attenuation. So things that are far off are not that visible after a little while. The light becomes very faint. So all of those considerations come in. You can still get their radio waves and you can still get pretty good images of them. So for one thing, I would invite you to, if you can still get their radio waves, and you can still get pretty good images of them. So for one thing I would invite you to, if you want to get a sense of how different things look in different form, different modalities, look at the so-called pillars of creation. One of the first images that you take, usually, it's almost a tradition like a hello world you shoot up a telescope into the sky and then you point it at the pillars of creation and you take a picture look at the picture of the pillars of creation and different spectrums in the x-ray spectrum in the visible and visible yes you can actually but it's very very faint you can take it in the gamma ray spectrum in the and so on and so forth x-ray and so forth and they are different things they they tell different stories In some spectrum, you see just as one hazy cloud, pillars that look like the pillars of creation. In another part of the spectrum, you see glistening bright stars there. The whole thing is teeming with stars, and it is wonderful to see those emerge. So that speaks to the fact that language, to the extent that language communicates some facts and truths, there are many, many modalities of doing that. Amongst the many things, depth, if you're in a submarine or when you're doing a lot of scientific or other experiments, depth sensors, to know the depth of things. You can get a stereoscopic image of something simply by doing a depth study, bouncing laser beams, for example, or in any one of the ways you could get depth studies. So now you can do depth studies. Radar can tell if there's things in the sky by doing, again, bouncing radio waves against objects in the sky by doing again bouncing radio waves against objects in the sky right and that are there then there is also imaging that comes from for example the accelerometer what's an accelerometer guys that is the normal capacity moving that'll be that's right so what happens is accelerometers are extremely important because, you know, for example, your cell phone drops it. Now there's accelerometers that tells you it's dropping. Right. And in fact, that's how these days you watch Apple Watch and these smart watches know that you might be needing emergency help if you fall and don't get up or you fall 20 feet down a cliff the accelerometer tells us what can accelerometer tell if you know the initial point acceleration as you know is the second derivative of distance so by by an integration process seeing your acceleration over time you can tell what the velocity of the person has been across time, isn't it? If you know the initial position, initial velocity of a person. And not only that, just by acceleration, you can tell how far the person has gone. In a very intuitive way, when you're going from here, let us say from San Francisco to Los los angeles silicon valley to los angeles you know that the distance you have gone pretty much is a function of your acceleration behavior over time how much you have been pounding at the acceleration foot pedal or not if you have had a heavy foot and you're going a nice cruising at a nice 120 miles an hour you'll either reach la in four hours or you'll meet a cop on the way anybody who has had experience with that four hours four hours you guys have done that did you did you meet the call you need a lawyer if you meet the cop then you have to next meet a lawyer to get out of it. So well, people do it. No, they don't issue ticket above 100. They say, I have above 100 twice, but they give you under 100 because above 100 they have to put you in jail. Oh, wow. There you you go so you have accelerometers these are inertial measurement units inertia or inertia like for example the gyroscopes that tilt etc they can be a lot of data that is how your satellites go around right that's how you keep a ship on course so they all when you look at data in any form behind data is a story isn't it they're certain the way i look at it and i want to use this point to broaden the concept of language see See what produces data? There are certain forces at work, there are certain phenomena, and data comes from the empirical observation, some measurables, some measurements associated with that phenomenon. So something happens that produces the data. So behind every data, therefore, there's a narrative about some phenomena that has happened. And so you can say that data is the language in which that what happened, the story of what happened is conveyed. And therefore, in a more general sense, data is narration, data is a language. Are we getting a sense of that? When you generalize to that. And so when you say that the, you can look at language as a very multimodal thing. You can have the same narrative, for example, or what happens when, let's say that you are, you see a car go by at very high speed you see the visual and you see the audio isn't it you see the audio happen at the same time if you're a cop you see both the acceleration and the velocity of the car so different forms of data has come to you. Now, if you were a scientist, looking at the sound waves, you visual narrative, the audio narrative, the inertial data, are we together? And so forth. And so you could say that the more modalities that I synthesize, the better I understand what happened or I can communicate what happened. And we all know, most of you are parents or some of you have had parents, of course, so have heard bedside stories or are telling bedside stories. So you know that this is a crucial point I'll make because I want to do that. That if the purpose of language is to communicate well, so when you tell stories to children, what do you do? You gesticulate. And then there was this big sun coming up from behind the hill in glorious red. And so you're using expressions, you're using voice and the turn of the voice, right? Or maybe even showing a picture and things like that. So the more modalities you synthesize, the better you convey something. So they are all part of a language. And so I would like to use this as a segue into multimodality, right? Here hereafter we'll do a lot of models which are with about many modalities synthesized together so one question that i that you could ask is suppose i have a picture and a textual description of the picture and the sound of what happened right so somebody recorded that a car zoomed by at 100 miles an hour and making a pretty loud whoosh sound you have a sound recording and you have a picture in which you see as you see a pic the blur of a car zipping by would you agree that they're talking about this they're telling you the same story. So now think about it from an embedding space. If you have an embedding space, the latent space in which the meanings of these 3 different modalities, data, these 2 different instances in 3 different modalities are there, wouldn it be reasonable to say that they should land more or less at the same point in the latent space? Isn't it? And that is the purpose of the things that we are going to cover today. We are going to say let's have many modalities that harmonize with each other. If they're describing exactly the same thing, they should land more or less the same place in the latent space. If they are about similar things, they should be nearby. Right. So we will talk about that. And that will be a pivot point from today's from today's boot camps onwards now with that preface let's go back and look at what we have once again just a quick reminder you have tears most of you have been taking help chanda has been helping a lot i have been a bit busy but i've tried to help you but we have people to help you praveen has been helping a lot of people now tira is back right she'll be helping so there's a and of course you guys have all been helping a lot of people now tira is back right she'll be helping so there's a and of course you guys have all been helping each other please reach out that we are there to help you on your way and those of you who are still struggling with project one now is the time to take the solution the solution we uploaded we keep uploading more and more richer versions of the solution with almost everything there. So if you haven't caught up and finished project one, take a hard look and ask yourself, do you want to finish it or do you want to start with the solution that we have posted, a sample solution, and use that as your baseline to launch for and do your project to the second launch for and do your project to the second week's third week's projects right because the first was the hardest part the the goal was that you learn that ai is made up of a pretty long journey from theory to implementation i suppose you all have learned it at great pains that it's a long journey to go from here to there. And the point that I'll keep emphasizing this again and again that it takes a lot to create an AI centered enterprise. You have learned about data infrastructure that you all have created. Models and algorithms were the only thing that sort of you assumed AI was about, but it isn't. The platform, the operation of the platform, the security and compliance, we haven't gotten into. But over time, we'll devote some time, one session, one week, to the interpretability of AI models to quite some time and the ethics around it. So once again, you must have seen that if you are a team of six, some of you are very focused on the models. And it has been happening. I've been visiting each of your teams and seeing that it's happening. Somebody is specializing in the models, somebody is specializing in the data engineering part, somebody is specializing in the infrastructure and the operations part. And this is how it happens in the enterprise. The full ML Ops stack is multiple cycles and people caught in multiple cycles. But they're like gears. The turning of one turns the other. And so this is our journey. Once again, just to remind you, this is all that we think it takes to build a production grade AI system. You ingest data, you clean it, you validate it, you transform it, and as needed you may label the data sometimes. Labeling is very painstaking, very hard, very expensive. So there's a tremendous amount of emphasis on self-supervised learning or unsupervised learning these days, or auto-labeling of sorts. I believe companies like Scale.ai, their entire business proposition, value proposition is that they annotate, they label data for you. Then you do the actual machine learning part, experiment, train, tune, which is your ask measure learn cycle once you have trained models now that's not the end of the journey then comes the next cycle taking it to production making sure this model is genuinely better than the previous one that it wasn't trained on a stale data there is no model drift that there is no address it is not open to adversarial attacks right you may have made the best model, but what if somebody could easily poison by using it? So all of those things have become considerations at the deployment stage. And sort of intersecting at the model training, of course training, you do all of that, you store the models, you monitor, you do all of that you store the models you monitor you do all of that, so this is your whole landscape and you'll keep seeing this picture. And we bring this picture over and over again every day in this weekend so that hopefully this becomes second nature, we know in the beginning, it looks like too many moving parts, but after a little while it begins to look quite simple. Is this is what it is. So what's the lesson plan for today? Yeah, Asif, are you taking questions? Oh, yes, please go ahead. So if you go back, right? Is there a way for a nascent organization to understand that they have reached the level of maturity where this thing is like a fine grain machine that is running? And do we have champions in each of these vertical sections or there is an overarching person who is kind of Mr. Know-it-all who runs the show? It's very hard to find people with expertise across all the infrastructure, right? Oh, absolutely. That's why I emphasize that most people are caught in one of these cycles. But generally, most organizations don't have it, but they should have an overarching chief architect who is generally deeply literate in AI and the math behind it, not at a superficial level, and is also a good systems architect. These people are somewhat rare birds, but if you can find these unicorns, if you can find these unicorns, definitely get them at all costs because most of that and so i just put this to complete your thought i'll just say one thing see if you're in management engineering management has been there for decades now the first thing you realize is something some things are self-evident you ask developers to develop a feature they will will all develop it right. They, in the old waterfall model, there used to be milestones like code complete. Rarely do teams meet code complete. They all do a feature complete. But the moment, the next milestone used to be feature integration, the integration complete. Rarely would any organization keep its deadline with respect to integration complete. You must have seen this in your little boot camp itself. Each of your six members would have done something and feel very proud of that. That yes, in principle we have solved the problem, divide and conquer, except that it is divide and only half the conquest, right? The other half of the conquest is putting it all together and making it work. And that's where all hell breaks is, is that the experience that people had? Right? So this is it. So you realize, what do we realize that people's expectations are different. I write a function with this signature, you wrote a downstream function that would use my functionality, but expected a different signature. Right? They're small. This is the impedance mismatch and alignment issues that we have to deal with. And then comes alignment issue between organizations and their cultures. I spoke about the three different tribal cultures. The tribal cultures of data scientists who only want to talk in python and talk about models and algorithms the culture of data engineers who do all the heavy lifting and they scratch their head and say why are these people so crazy yesterday they wanted data in this format now they want it in that right and what's up with them and they are moving mountains to get data ready indeed in any machine learning project the actual data science or AI research, sometimes it's just 10% of the effort. 90% of the effort is sort of split between data and what comes before and what comes after. The data engineering, the feature engineering, feature storing, deployment, the whole MLOps is where it all goes. So there are these different cultures, the people who are completely gurus of operations, they are paranoid, they have to run a rock solid scalable, high performance infrastructure. So they can't understand why a bunch of PhDs would dump on them a model that for each inference takes three seconds right or five seconds you see how different the cultures are so what you need is once again you are seeing a cultural or an impedance mismatch between organizations and what what you need is somebody who is all the time bringing them together and making sure you don't have impedance mismatch or alignment issues. Or without that, you see what happens in organizations. I keep quoting Carl Stoy and Anna Karenina, that all happy families are alike. Each unhappy family is unhappy in its own unique way. You guys are all working in companies that have some AI initiating what the other going on tell me that this happens as if uh if i may so say so uh the handshake between the folks who are doing data engineering with the data science team is probably the most messiest thing um right now And part of it is the data science team wants to have more features to be engineered. And that back and forth usually ends up, we spend a lot of time on that back and forth. So I think some of these things, probably we need to kind of have better best practices kind of things going on because right now if you start a project from scratch a lot of time is spent in this back and forth which reduces the efficiency across no very very good point guys. See, in my previous job I was the chief architect and senior VP. So management aside, here is what I saw. Data scientists would love to have the data out as CSV. Or in a simple place where all they have to do is select from a table and just load it into a Pandas data frame or something like that or into a pytorch data set but even basic things how do you produce the data you'd say oh data engineers will write the sql and give it to them right so data engineers would look at the data scientists and say these guys are supposed to be bright why can't they write their own sql why do they have to open a jira ticket and then just wait there for data to be produced for them right so yeah exactly and then they would when i looked into it there would be genuine issues practical issues it turns out data scientists did not have access to the data lake why did they not have access to the data lake? Data is precious, production data. Privacy issues are there. A security team was sitting in there and saying, no, we give very limited access and why do you want to see this data? And so you have to break a lot of barriers, very practical issues you have to deal with. You have to make sure that your data scientists are able to do. SQL is still easy, they pick it up. But the moment you talk of any big data store, you ask them to retrieve data from HBase, right, or from Mongo or things like that. But that's where there's a bit of learning curve and the question comes whose responsibility it is, especially when data needs a lot of cleanup. Data scientists would always say data engineers should do it it's their job right and at what point does it cease to be data scientist's job and becomes data ingenious job you have to sit there and decide you have to work through the intricacies because if you completely bifurcate the responsibility time is lost right literally it's so annoying a data scientist opens the Jira tickets and just sits there waiting for data to come back. And data engineers get really frustrated that yesterday he wanted this and today he wants something new. Why couldn't he have given me all the requirements at one time? Right. So what they don't understand is data scientists, learning is an iterative process, as you see in this cycles, right? They do something and they realize they need something more. Right? And then the other frustration is, you hope that some features would be useful, they turn out useless. And then the data engineers are angry. We worked so hard to produce this and they are not even using it in the model. Right? So it's part of the culture you know this is how it goes yeah so paul's question when enough is enough okay i'll take that in albert your question so how did you successfully set up your organization for that data engineers and data scientists work well how did you it's quite simple because we didn't have teams we had a flat hierarchy because i ran the organization in a very academic way so repeat the question we didn't hear what the question was his question is how did you make those people work together albert's question and uh support's question is when is enough enough improving the model let me answer both these questions see guys people have different ways of doing it i had a very non-traditional way of doing it i I spent way too many years in academia and saw that in academia there was great efficiency. I worked in NASA for a small stint and again noticed that it just works great when there is no hierarchy. See in academia, a graduate student, a humble graduate student can raise his hand and completely demolish a tenured professor if the tenured professor is roman right in a seminar it's totally okay and the professor wouldn't mind he would at most thank you say oh i didn't think of that part of the culture is to put egos aside and to focus on a pursuit of truth or the mission. And I've always tried to bring about that culture in my teams and everybody's styles differ. I faced a lot of resistance from when I wasn't upper management, but and from HR and so forth. So that I would give people fancy titles. Somebody would be software engineer senior software engineer principal manager senior manager director senior jk all of those fancy titles but everybody would nobody would report to anybody else it was a completely flat organization. So to your question Albert the basic rule was I would just ask them to sit together. NVIDIA also does it. It's completely flat. But then how would you hire? So what would a person have? Both the skills or one skill? No, no. It's very hard to find people with dual or triple skills, multiple skills or interdisciplinary people you hire for in my case i hire for potential not for the skills they actually have because my experience is if you hire for skill skills have a very short shelf life that skill will not be needed soon and this guy is not capable of anything else so you hire for potential intellectual potential and then you let them pick up whatever they can in that but sometimes you can't sometimes you know budget doesn't allow you hire just for skills usually it's an unfortunate decision i usually regret decisions where i've hired for school just because i needed to urgently hire somebody for a skill skills have a very short life and not only that people who advertise themselves based on skill the subtext is they're probably not very bright there's only one skill they're good at right whereas bright people tend to be versatile so i i now to sukhopal's question when when is enough enough? It's quite the way you'd look at it is you can keep on improving your model forever. In Oracle, where I work, there used to be a very good piece of wisdom in the database kernel database main infrastructure group. It said that I think it was the what was his name today? It means escaping me. I'm sorry. So a little while ago i couldn't i couldn't remember the name hubble telescope now i can't remember the name of the the name of the great architect so anyway he said that any when you're done you can always double the performance and once you have doubled you can double it again and you can double it again and there's no end to it. The only thing is, as in the beginning, doubling the performance is easier. Later, you have to go down to the core of computer science to double it. Was it Jim Gray? Say that again? Jim Gray? No, it wasn't Jim Gray. It was a guy at Oracle. I see. No, it wasn't Jim Gray, it was a guy at Oracle. The person who literally led the database group and shame on me for forgetting his name. A very bright guy. So that's how it is. So the thing is, performances can always be made better and better. When do we stop? There's a simple way to stop. If your purpose is the bragging rights to show that you're the state of the art. Never stop. If your purpose is to accomplish something of commercial value. All you need to do is not keep improving, because that is the risk. You reach a point of diminishing returns, after which every little improvement costs an enormous amount of money, enormous amount of effort. So you ask the business, what is good enough? How much accuracy is good enough? So I'll give you an example. We keep taking the example of breast cancer diagnostic. If it is a screening test, what do you want? You want the high recall at the expense of precision or accuracy right a worst case what is the absolute absolute screen suppose it has 50 or less precision half the people it calls as having cancer they'll don't have cancer but it has the virtue that every person who does have cancer is caught is it of value? It is of great value. You don't need to improve that model. Why do you not need to improve the model? Because downstream there are other models. This model is in a tool chain, a medical tool chain. The patient will be sent now to the next diagnostic test. And at the end of it, before the surgery door, where the surgeon is standing with his skull pill, with a big invitation on his face right hopefully there is one more diagnostic test which has high precision which sacrifices like which goes for precision sacrificing everything else it will clearly tell whether the person has or does not have cancer right because they trust that early on in the chain, all the like there are no false negatives there at that point. I will go ahead. This one's more language. Is there is there really a case that the party against. Patrick's question is that a case that would argue against multimodality? For example, in the future, would we see programming languages that has drawing involved? That's right. Oh, so Patrick brought up a thought that is actually absolutely brilliant. I wouldn't have thought about it uh patrick says amazing thought actually he says first question is is there a case against multimodality at what point it interferes with communication rather than augments it and it gives an example what if you could create a programming language in which you would write a statement and instead of writing a for loop around it you just made a squiggly little circle a loop diagram and you say well put the text and the image together and do it actually i hope they create such languages it would be so lovely right you could just draw your code in this right but um i don't know i don't know but it's a good point i mean it is again one of those points, multimodality is a fact of life. See, look at it. A lot of people go with video because video appeals to many of our sensory aspects, the visual, the audio, right? It appeals to both of those and there was a belief, if you look at educational theories as an educationist, I can tell you that there was a great belief that videos are so much better than scribbling on the whiteboard right everybody was busy creating very flashy powerpoints and videos and videos with that today actually there is there's a retrenchment coming from the education also having worked in an educational tech company we found actually this one i found from a brilliant educational leader uh summer somerset who i think was one of the pioneers of micro learning she pointed out to me i didn't know that she pointed out that today She pointed out that today people are realizing that a podcast generally leads to better absorption of material than a video because video causes sensory overload and distraction. So this goes to your point, Patrick, perhaps. But anyway, I don't know the answer to that. Anyway, so guys, this is our cycle. And this is what we are learning in this bootcamp so what is it about today today I hope the lessons that you learned was it's hard to do Enterprise scale AI but we have learned to do it we have scaled a pretty high mountain all of, right, with some Sherpas helping you. Now, what are the common pitfalls that you encountered? Common pitfalls are the assumptions you make. When you wrote your sentence encoder, many of you just used an English encoder. Isn't it? And that didn't work. sometimes you weren't careful and you used then we brought in multi-modality images and you said oh my goodness now how am i going to align the two so you have to make trade-offs at every point i hope you learned that the trade-off between two opposite forces to be versatile versus to be high performance in a specialized situation if you only do english text you can do far greater much more accurate retrieval isn't it just replace your english when you're just dealing with english if you replace your sentence encoder with a multilingual encoder, I hope you notice that performance goes down. You gain generality at the expense of performance. Right? Similarly, I don't know, as you make progress, you'll realize that yes, we can do multimodal encoding. Like I said, all of these modalities can be projected into the same vector space latent latent vector space but when you do do that what you notice is that uh and by the way i'll hold my judgment i'll let you decide what it does to your accuracy or the quality of the information retrieval right so it's always a trade off, you have to give something to get back something. So that is one of the big lessons that you have to learn. Also, the lesson that you have to that you learned, hopefully, that different teams can solve the same problem entirely differently. For example, I saw creative usage. One of you, could I now start handing out clues to the solution or it's premature? How many of you have, no, I shouldn't. Okay, then I will still hold that. I won't because multimodal solution I'm ready to release. So I'll wait for one more week. So those are some of the lessons to learn and going deployment is its own hell, right? You must have realized that when you set up whatever it is that you set up and made things work, it wasn't easy, right? So you have to know that everybody's life is tough, data engineering, data science, and the MLOps people, the DevOps people, all of them are doing and working very, very hard to make an enterprise AI system successful. That's how it goes. So today's topics are, we are going to do, and it's almost time now. It's almost time for us to do, we are going to do three papers. One of them is ImageBind. Please do it first. The next is AnyMal. Oh no, the third I said LoRa papers, but no. Okay, so let's keep LoRa. We will do LoRa today. I haven't posted LoRa. I'll post the LoRa paper again. Those of you who attended the Neural Architectures course have been through LoRa with me. I'll be doing it all over again as a review and for the benefit of those who are new. LoRa is very important. LoRa today is a segue into the next phase of our boot camp which is fine tuning the models so far you have only been using pre-trained models you haven't been fine tuning yet isn't it but now lora is your first steps towards fine tuning the models that you have tuning the models that you have altogether. So these are three papers. I also put another paper for reading silent reading. Voice box, you can consider voice box optional for today. I'll take it off the table. And I'll add the LoRa paper. LoRa literally is pronounced like written like that with a little o. It stands for low rank adaptation. So what is it about? I will let you discover on your own. So first do Imagebind. Then do Animal. And then do Laura. We will be doing paper readings today we'll do quite a bit of paper readings and uh here is here is something your project for this week is to integrate image bind at some step of your project. I'll leave it as a mystery which step it is most conducive to. So integrate image bound. Animal model has not been released, so you can't integrate that. Lora, I'll give you a small introduction. The big problem that Lora Sol says, most of us don't have a GPU bank. Right. So we can't load a 70 billion parameter model in a little machines. So what do we do. One of the big boons is that you don't have to train a 70 billion parameter model, or using a model for inferences far cheaper than training a model why inference at inference time why so i made a statement i want somebody to justify or contradict it i'm saying that given a model using it for inferences far cheaper than training the model see when you're at theencing stage you have already calculated like think of it as a mathematical equation which has all the coefficients covered you come in from an embedder encoder or whatever basically get the embeddings value putters you apply a mathematical formula and you say what do i think of this right so you're using pre-computed value but when you find during a model you're doing all the vector operations of how many data i have inserted what does that vector look like when i insert a new one how do these mathematical equations change so obviously it's very memory intensive yeah you're very close to it i'm looking for a much more precise wording uh go ahead you know gradients gradient descent again just store the gradient i think you have been hiking and you tumble down the hill did you have a gradient descent event just one pass right forward there's no backward thing that you need to yeah now she that you also said the right thing you guys are all very close but but I wanted to be specified. Go ahead, Sachin, why don't you articulate it in full sentences? Sachin Goyal, Ph.D.: There's just one pass that you do the forward pass, right? There is no backward that where you have to do the back propagation and then do all the gradient descent and so on. So it exponentially goes up when you're you're tuning inferences just one forward pass that is right guys and that's it all of you are hitting at the answer more or less but by using different language the point is imagine just imagine a neural network whatever its internal structure is when you're doing inferencing mathematically is just a function f dot isn't it onto that function you've given input outcomes what does it do it does two steps and it's always alternate alternation of two step matrix multiplication matrix multiplication is is massively parallelizable and very efficient right and the second thing is a distortion remember what does what do neural networks do they are distortion engines they distort the input manifold basically right so you do a distortion operation even those distortion operations for example relo and gilu and all of it, they're extremely cheap. Quickly they can do it. So the forward pass is easy. But the moment you make a neural network trainable, you open up the weights, you unfreeze the weights, and these are all synonymous words we use in this jargon, what happens is that the autograd capability of your framework kicks in. What is the auto grad capability? Every time it computes effects, it will compute the derivative of effects, the gradient of effects. Why do you need the gradient of effects? Because in the back prop, in the backward journey, that's how you'll update the weights, the gradient descent step that Srinath mentioned. how you'll update the weights the gradient descent step that she not mentioned isn't it and so the if you look at the memory cost and compute cost it's very very high you have to hold a lot of intermediate states and you have to hold the gradients along with that so the memory utilization other aspect that happens is you can't do a lot of quantizations, right? So these days one of the tricks that you use is you use a model instead of using full 32-bit or 64, forget, I don't think anyone is doing 64-bit floating point operation with the neural nets, it should be prohibitively expensive you do 32-bit so you scale it down to 16 bits or maybe 8-bit now for forward pass you don't take that much of a damage it's just fine or when you do back prop and you update the gradients the trouble is the grads see here's a basic mathematical intuition. If you take a function any function and you integrate over it it smooths the function, the integral will be smoother than the original function, and if you differentiate it it points out the jaggedness of that. Are we together? Try it out with any function you'll see that it does that right? So when you see if you now I'll let you from there to connect the dots that if you if you quantize it too much, what happens to the jaggedness? You lose some of the resolution. Right? So you begin to lose things around this anyway there are many implications of that so you if you could only do forward pass with a big model you can actually which is the inference part you can do inference on fairly reasonable hardware so for example today we know that typically uh eight way like if you have a um eight gpu server right if you're a rich company, you'll have 8 GPU, each GPU being H100 or A100. It is fine for most inferences. In fact, for a vast majority of inferences, 4 GPUs are enough. Quite often, 1 GPU is enough sometimes, right? But four is minimum because if you're loading a 70 billion parameter model, you will compute and realize that four GPUs, each GPU having about 50 GB, 48 GB RAM, is about what you would want, right? Or 98 GB VRAM is what you would want. It does just fine. The training takes much larger model. So the message of LoRa is, what if you could do your training of the model without using so much hardware? And how would you do that? The solution to that mystery is LoRa. It's a beautifully argued paper, very useful. After that, those of you who are doing anything with images, image generation, like mid-journey or stable diffusion, you would realize that paper review will start at one actually more like 1 30 but three o'clock we'll have team presentations or maybe maybe 3 30 or 4. and 5 30 guys i hope did you guys bring something new that you found all of you how many of you did just a few guys please don't do that find something new sit down with your lunch discover something new we are a community we learn from each other right our the learning experience would be as rich as everyone doing is spotlight right what looks are fun because you make one dish but you get to eat 10 isn't it this is literally potluck of ideas potluck of findings do please bring something to the table please take this seriously from my side this cambrian explosion series that we have started please it's a potluck of ideas potluck of findings please bring something new to the table, contribute something. But take it seriously guys. I know you guys have busy lives, but please take it seriously. All right, that's the plan. So please go ahead. I'll post the LoRa paper also. First read ImageBind, then read Animal. Then, I mean, actually first read ImageB glance at animal but definitely read laura right and uh at 10 at 1 30 actually we'll start the paper reading you have two and a half hours two hours and change enough and i will be coming to each of your rooms and quizzing you on the papers asking you what the main ideas are. Now what have we learned so far? Does this look like what are the things we have learned? These are the lessons. And the big lesson is the interplay of enterprise-scale AI architectures design and employing good models. Good architecture matters as much as good models in creating a successful solution. That's our takeaway. Sorry guys. What is today? Were you there last time? Yes. Yeah. All right guys, any questions before we end? Any questions? Ask the link to the papers, please? Yes, they are already on Slack, except the LoRa paper, and I'll put LoRa also there. Mahasmeh, you were there in the previous course also, so you should be having the LoRa paper, but I'll post it again. Okay. And what is Flare? Sorry, I missed the last session what is flare active rag as if so we'll we'll go through the papers also in the afternoon so we read it now and then we have like one o'clock right yeah so we will read the paper then asif will come by or one of the chas will come by your room quiz you on the paper to see how well you have understood them and finally at 1 30 we'll do paper reading cool for remote for remote then i see where we joined at 1 30 again or you know i hope you you are busy doing paper reading with your team yeah no but the zoom call the zoom call yes yes the zoom falls back at 1 30. 130 right okay cool