 Let's take a detour. Into baths, right? So forget about everything that we did about regularization and all of that. We will do one simple thing. Imagine a circle. Circle. What is the equation of a circle? The equation of a circle is... x squared plus y squared equal to r squared. That is right. Let me just write it like this in simple two space or three space i'm just looking at two space for the time being you can do it three space it doesn't matter but let's be very very simple gx equation of the circle is x square plus y square is equal to some constant constant let me call this constant r well it is r squared but let me just say r squared right which is basically the die the radius of the circle right so what happens is that different circles will have different radius isn't it that's all it is all you're saying is all points which are at the same distance from the center define a circle they define this now let's do this let's look at this equation and take its derivatives so we will do now you'll notice that this is an equation in two dimensions. So how would you take derivatives? You can say that, hey, what happens to g if I just keep y constant and just change x? So if y is constant, the derivative of g, which is with respect only to x, holding y constant would be what? 2x? Derivative of g with respect to y would be? 2x? Derivative of g with respect to y would be 2y? Hold on, hold on guys. Just pay attention. Don't jump. Is this pretty obvious? Partial derivative means you keep the other variables constant and you take the derivative. Now the definition by depth by definition gradient right so take a point let's take a point some particular point i'll just take a point x y? Or maybe some special point, xa, ya. It doesn't matter. Actually, let me not clatter it with notation. So just assume that some point, p, is this. Are we together? What is a point? A point is a vector p would you that part is also obvious this is the p vector and this is the point p so a point and a vector are the same thing right this is one of the simple intuitions to remember and now so this point we can write it in a sort of notation point we can write it as p is equal to in high school notation you you would probably write it as xi plus yj. Isn't it? There's an x component to it and there's a y component to it. So far, so good, right? We could also write it, but in a more sort of a formal notation, we write it as a column vector. People typically write it as xy. people typically write it as X, Y. there's no difference between these two they just different styles of writing the same information in machine learning Community we tend to write. vectors as columns column vectors the first component, the second components we vertically stacked why is this a good idea. components, we vertically stack them. Why is this a good idea? See, if you use ijk, right, then what happens after you read z and you have more features, you run out of indices, isn't it? You have to cook up newer and newer indices. It's just simpler to use a simple notation. Forget about ijk, isn isn't it and then if there are too many of them you can put dot dot dot in between right so for example if it was n dimension where n was 30 you could say x 1 x 2 and then xn right assuming there's a column a stack of all those component numbers that's why in machine learning we use this notation now look at the gradient this gradient is defined gradient of g is here is defined by definition is gradient are we together now let me use use a color for gradient a different color. I will deliberately use gradient of g. It's a vector. It is this. It is the definition. It is that vector which is made up of the partial derivatives of that, right? So now in this particular example, let us go back and compute our g x y is equal to x square plus y square is equal to some constant square gradient of g x y is equal to what it is equal to 2x again i'll just repeat the whole thing from here. Gradient of g, x, and gradient of g with respect to y. It is this vector by definition, almost by definition here. And this is equal to what? 2x, 2y. Now, this is an interesting thing. In this special case, in this case case of it should be y by x right yeah y y y what do you mean by y x because the gradient is a slope right the slope here for the p would be y divided by x right the circle that you have no no no no see this is it this is the definition of gradient this is the definition of gradient okay so it is this are we together guys so you agree with this i have not done anything so far i have taken the definition and applied it to this particular circle as a constraint surface and now you notice now what is p p is just x y it is the point x y isn't it so is there a relationship between the gradient and the point for the circle yes yes for the circle Yes. Yes. For the circle, gradient G happens to be is equal to two times the X vector, the P vector itself. Does this make sense? In this particular case, if you look at the value and so it is equal to 2 times x y right which is equal to 2 times the p vector do you see the connection here guys so you say well in the case of a circle there is something interesting let me show what it means geometrically. If p vector is this, therefore, what is the vector for g? g is essentially the same vector at twice the length. This is grad G. Would you agree? Isn't it? And that is a crucial insight. What we are saying is, so let me summarize the point. So we find for this special case for i wouldn't use the word special for this specific case case grad g grad g points in along p right so let's not worry about a factor of two and so on and so forth right so magnitude we are going to ignore it points in that direction but more crucial than that and is perpendicular and is perpendicular to the tangent of the circle at p of the circle at P. Would you agree with this statement? This is a statement that perpendicular. Let's see if this is true. I just made a big statement. I'm saying that this blue line gradient is perpendicular to the circle. Perpendicular to the tangent of the circle what is the tangent line that will go across the p on the circle this is the tangent at p would you agree yes green line is the tangent and what is grad g doing it is perpendicular to the tangent is grad g doing it is perpendicular to the tangent so far so good and this will always be true for any other circle that you draw if you do another constraint surface the same reasoning will remain true right so now without showing, because it gets into calculus, I'm using this very simple example to make a statement, which I'll make in a moment. Now, notice that first it is perpendicular to the circle. What is the circle? Circle is the contour. Like, it is the point at which r squared, the constant remains the same. Isn't it? So it is perpendicular to that. Now, if I want to go in the direction of maximum increase of value of g. So suppose I take a point. Let's take a point. One more color is called for let us take a point this point point q at q if i go in this direction you know parallel to this contour surfaces. Let me draw one more contour surface here. So if I go along the curve, is G increasing or decreasing? Unchanged. It remains the same if you go tangential to the constraint surface. On the other hand, if I want to go in the direction where it increases most, what about going in this direction? Is that the best way that I can do? Go in any arbitrary path, but slightly move outwards, isn't it? You're increasing your G. But what is the way to increase G? So suppose you are allowed to make only two steps, right? You said, you know, in all of our stories, there's always in all mythologies, there's a story that somebody is asked to take only one step and whatever he can cover or she can cover in one step belongs to the person, right? So suppose you had to take one such step and you want to increase your g as much as possible. In which possible direction would you move your g? Perpendicular to the tangent. Exactly. So you would want to go in the direction, this direction at q. this direction at q grad g at the point q whatever it is that direction you already realize is perpendicular to the surface so the the path of greatest increase in g and that is the intuition the path of i'll write the point inside the path of, I'll write the point in sight, the path of fastest increase of g, x, y, is perpendicular, and we won't use the word perpendicular the word people often use is normal because perpendicular is with respect to line when you go perpendicular to the surface the wording it means exactly the same thing but people use the word normal to the to the to the const to the some R square some constant surface. Here it is a line, it's a curve, but imagine that it's in higher dimension, it would be a sphere, you know, perpendicular to the sphere. Where's my sphere gone? Okay, it's somewhere. Oh, there it is. So imagine it would be the line from the center of the earth going, penetrating straight up to the sky, isn't it? So this is a very interesting fact. And now I will generalize. I am going to say that whenever you get any kind of a complicated surface, what is true of a circle or a sphere is true of ellipse and ellipsoids, right? Because locally, ellipse or ellipsoid looks like a circle. You wouldn't know whether it's part of a circle or ellipse, isn't it? So it's true for ellipse, but it is actually, surprisingly, this statement is true right grad g will always point point orthonormal or like normal to the to the constraint surface are we together well that is very good and it is also the direction of most increase and represents the direction of greatest increase in G. This is our key result that is true. However complicated your surface is, like it will still be true. At this particular point, right? So if you have a lot of surfaces like this, I don't know. At this point, it will still be the gradient, A gradient would be the direction of maximum increase, and it will be perpendicular to the contoured surface, constrained surface. Now, with that digression in mind, now comes the key thought. So, so far, are you folks with me? I hope what i said was simple mathematics but this mathematics are far-reaching consequence now with this mathematics come here look at the red circle and one of the contour lines right of the contour line being whichever one it was that p the control line p so what can you say at the point at which the control line has some, the yellow lines are error lines, right? Error surfaces, curves of constant error, isn't it? The yellow lines, let us notice this. The yellow lines, well not lines, yellow curves are equi-error curves, right? B, red line, red curve, red circle is the constrained surface. So G, G of beta is this and this yellow one are the error associated with the beta at that particular point isn't it the error associated with that at this particular point we can tell what the error is because it's a point in the hypothesis space and we can also compute the value of g does that make sense guys right now what do you say when these two surfaces just tangentially touch each other think about it where is the gradient of at this point p where is the gradient of the red circle pointing this way so this is gradient of G right and in which direction is the gradient of the error surface pointing towards the opposite direction let me just mark it as big bold. So would you agree this is gradient of the error surface. This is the direction. I will ignore the magnitude. Let's not worry too much about how much big one is compared to the other. But the directions are opposite. Isn't it? if two vectors have opposite directions what can you say about them would you would it be would it be fair to say that the gradient of e e of the error surface at that point is equal to up to a proportionality constants because magnitudes will be whatnot up to a proportionality constant the same as gradient of g with a minus sign why minus sign because there if you put a minus sign then you can be sure that lambda is positive. You can give positive values to lambda. So would you agree that this equation captures our discovery? Because here, when we look at it, this is gradient of G and this is gradient of E. Isn't it? So this equation holds true. Would we say that, guys? Ask me questions, guys. Stop me here, because this is a very, very crucial point. Do we agree? I have a quick question. Yes. So in the inside, you mentioned like path the fastest increase what exactly does that mean in this context in this means if you want if you have a circle what is the con constraint it is r the radius right square of the radius what is the fastest way to increase uh this go outwards radially outwards that is the greatest increase of radius isn't it how do you increase the you are at a point you want to increase the radius just go out isn't it radially outwards yeah uh what is the point i mean why do we have a constraint surface okay just say that we don't want our beta as the parameters to become big so we say that we will stay tightly close to small values of beta so small values of beta we're saying I have a budget. None of the betas can be more than, let's say, some constant. Right? So what happens? It puts you in a circle, isn't it? Because the sum squared values of all the betas must be less than some constant. You're saying within the circle, you're close to the origin. So all the values of betas are small. And that's what we want. We don't want to go out. Always around origin so all the values of betas are small and that's what we want we don't want to go out always around the origin because you want to make it small right small means that's how yes so when when you say the constraint right like but the whole point is to use the gradient descent and reduce the errors so no no gradient descent has already found for you this nice parabola and it has found this con this the true optimal beta star and it has made this nice other equi error surfaces a b c d e f g so your learning is over now. Okay, so this is after the learning. After the learning, yes. There is an optimal point where you can say that. Yes, exactly, exactly. So you've already discovered your goal. You built it. You know where its absolute minima is. Now you're saying, well, you know what? We are not going to go that. We want to have, we want to be just close to it, not go to that best beta star, because we know that in reality is not really the best. It leads to wild oscillations. It doesn't work for test data, it only works with training data, because it has gone and fit to the noise. So Asif, how do we know that constant? So it's relative, right? So how big? Looking at the residual plot or something like that. Yeah, we will learn about that. Hold that thought in your mind. I haven't told you how that constant comes about. But at this moment, guys, what I have done is without this little mathematical digression has taught you not just this, what is very relevant to regularization has actually taught you one of the deepest or hardest themes typically people face in college calculus called constraint optimization what you have just learned is the calculus of constraint optimization this lambda in calculus has a name this is the famous lagrange lagrange oh my goodness I will massacre the name Lagrange I hope it is Lagrange multiplier it is the famous Lagrange multiplier and what this equation says I will rewrite this equation here the equation we came up to is that the gradient of some function e is equal to a minor in this particular case obviously i made it very real with respect to this but it is a general truth in mathematics when you have a constraint and you want to find the minimum minimize e e given this constraint so in other words find a point at which e is as small as it can be but it cannot violate the constraint you cannot go out of the circle this equation is I'll write it in a slightly different way. And now the rest of it is just a plus lambda gradient of g is equal to zero. Can I just rewrite it like this? Right? which now with both the with this from this we can conclude that gradient of e plus lambda g is equal to zero so what are we trying to do so we are trying to we are trying to minimize let me just call it L right you're trying to minimize not just that total error, but another function loss is called the word loss. And thus we get introduced to the big concept of loss in machine learning, the loss function. L is your loss function. What you're doing is you're minimizing not E, but the sum total of e plus lambda g are we together right so so hold your thoughts guys let me tell you some of the consequences and so these two have opposite effects if you find if you minimize just only, you'll go far from the origin. Then what will happen to G? You will have to have a pretty large constraint surface going all the way out to that point. Isn't it? So G will increase. So if you try to minimize E only, then your solutions that you'll get will be at large you don't want that so they have opposite effects so you have to be careful you want to keep g small and you want to keep e small and you have to bring them both down while both are competing they're pushing each other out right that is the constraint optimization now let me write it down, this loss. Once you understand, so now if you realize what I did is I use this problem of machine learning, loss surface, I mean, error surface and constraints to introduce you to actually a powerful idea of constrained optimization in calculus. of constrained optimization in calculus. So once you understand this idea, for the rest of all the discussions that we'll do in machine learning class, I'll assume that you understand constrained optimization, right, because we'll reuse this idea at many, many place. Here, we will write loss is equal to E plus lambda G, right? Then in the case of in of regression remember what this is is or let me just say regression. In the case of regression, regression, this is what? E is the sum squared error. Y I minus Y I hat squared, isn't it? You can take sum squared error. You can take mean squared error. It doesn't matter. You can take this or you can take 1 over N. It doesn't matter. Am I together, guys? It is the same thing error is error right you may or may not take this so i will skip this i will just skip this quantity plus lambda now what was g remember the g was not in the real space g was actually in the beta space. It was beta naught square plus beta one square. Because where was the circle being made? In the hypothesis space. Do you see this, guys? This is the hypothesis space. In the hypothesis space, equation is in terms of betas, right? So this is your, this is is true let's bring it here so this is equal to beta naught square plus beta 1 square right so people often write this equation as square plus lambda summation of beta j square right j being the number of parameters j is equal to one and suppose they are the model has p parameters right whereas this i is equal to n number of data points this summation is over a data set. This summation over parameter parameters of the model. And this is therefore the celebrated equation of the famous equation of L2 regularization or ridge it is called regularization or most often when people say they're regularizing, this is by default the regularization that people use. This is the famous equation. You must remember this and your book will talk about it. Now what happens is most of the books, in fact, don't explain how you arrive at this. They just give you this equation as a fact that regularization is to minimize this loss equation. And if you do this, then you won't have oscillations. Your parameters won't become large. They don't actually, I mean, sometimes they do, but most of the times the books that I've seen, they don't tend to explain. So I have taken you through an intellectual journey in which we have tried to understand how this equation, excuse me, I apologize. I need water guys, so we'll take a break after this. Yeah. So Asif, one quick question I have have so equation that you have you have beta j where j equal to 1 to p but the one that you have is a beta 0 2 beta 1 plus beta 2 so intercept is not the part of that all right let me make it zero better good so they are they are uh P plus one. You're right. That is good. So what it does is we have this famous equation now, and we went through the intellectual journey of understanding it. I will just recap this journey, and then we'll take a break. So this journey basically says that, see, if you do unregularized or now the word ordinary you must have seen this word in literature the regression linear regression that we learned is odd called ols ordinary least squares regression why ordinary least square regression because the last function is the same as the error function the sum squared error function it does not contain the other term when you do put the other term it is regularized regression what we were doing till now was ordinary least square regression namely unregularized regression are we together right this is an important point. And we have gone through the journey. It's a very simple journey. It basically says, hey, if you are not careful, your parameters will blow up. How do you prevent it from being blowing up? You put a circle around the origin and say, no matter what, I'm not going to go out of it. The learning must tell me the best value value the minimum error point within my circle so which of these points that i have within my little disk here a sphere here which of these has the least error and whatever that answer is i'll consider the right answer not the absolute sum squared error minima the bottom of the error surface. But you change the loss function itself. And this is where you stop. Right? So with that thought, let us now take a little break for about 10 minutes.