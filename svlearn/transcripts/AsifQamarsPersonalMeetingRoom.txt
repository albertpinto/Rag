 This is still for large language models. I've noticed that for people who aren't too fond of like so many knobs and turnings but are interested in constructing the LLMs, there's actually a flow-based system almost like system. Almost like yeah. So this was almost like I forgot the old one. But like Langchain had something similar also. Langflow. Langflow, yes. Langflow had so this acts a little bit like Langflow, but it also enables offline LLMs. So this one, I just had to mention this as in case you wanted something that was flow-based rather than a web UI, you could just follow flow-wise as one. So this is also one of those interesting this is very yeah this one is it's uh much uh better or i don't know this usage is better than lang flow right i don't know whether you use it i've tried landflow and we've gotten it to work but uh that was still back in the beginning stages so it's it's hard to judge uh the product now versus the product background but this but this has the general idea or maybe this is already a descendant of langflow maybe the people have have renamed or moved but this is this is another way to yeah to build to build with llms and this this one has yeah this one has uh agents and tools in mind so so if this works i'm not sure yet if this if this if i think we can use this with with uh the uber buger because uber buger has a way to expose an api and and and we can connect it as the LLM. So might be possible, but Flowwise also has a way to just test the chatter at the end here. So here you could see just connect everything, save, and then test your chat, and then it's gonna do a react or it's gonna use tools. So this is for LLM. So let's move on to image generation, generative. So image generation. So the favorite product for image generation is Automatic 1111. This brings a lot of the stable diffusion models together. So you'll have your diffusion, your basic unit autoencoders. You'll have the main big one, which would be your base model, and then some kind of refinement, and then even a variational autoencoder after to clean up faces, and upscalers and all of that. If you're interested in something that was particularly... Wait, I'm trying to remember the name. Oh, ComfyUI. I'll come for you. All right. So comfy, comfy UI is a flow based version of stable diffusion. So if you're interested in seeing what goes on underneath, there's also comfy UI. I also heard that this runs faster than automatic 1111, but automatic 1111 has been the most supported. So all the extensions and all these repositories that have so many capabilities for changing your image for generative image generation, it's all in automatic 11.11. So. so comfy UI this came out a few months maybe a month or a month and a half ago but it's it's slowly picking up so if if you're also interested in a flow based diagram you could see that so clip clip text encoders you can you can put in a pos so the way it works is you can put in a positive prompt or negative prompt. So those that would be included and those that would be maximizing maximizing the Yeah, this is really good. Oh, yeah, this is really good. Yeah, but under the hood, so if you're not interested in building a flow, they have complicated flows with that, that that have an output that shows you the base and then then it can go to it can go to a it can go to a refiner it will show you again an output and it can show so this can get as complicated as you want for come to ui this is the so this is so i'm also trying to explore this because i heard this runs more optimized. So if you already have a particular flow that you have in mind, or for, if you're building a startup and you already know the kind of flow and the kind of packages that you want, maybe it can help contribute to this project and yeah, enable end points for these flow-based. But so that's, this is the one that that is uh yeah this one is very manual but flow based but if you understand everything moving from point a to point b i would recommend this now for for the the current standard uh heavily the gold standard for support and image generation we have a stable diffusion web UI it's so popular that uh oops it's so popular that uber buga even said that they are the automatic 11 11 of text generation so you can tell that they're they can tell that they are there this is this is that popular so the the system will look a little bit it will look like this it will give you tabs on the top it will tell you uh what what the input and outputs are for the kinds of models uh if you have additional kinds of feed functions that you want to do even the most recent ones like segment anything from meta is already supported here. So, so many things that we could do with this one. And then all different kinds of sampling methods and you can do in painting and all these things. So, okay. But the way it works is, you can put in a positive prompt, negative prompt, click on generate, set how many steps of sampling, the size, the resolutions. And then if you want it to keep generating the same, have a seed ready, and then generate. You can also have batch size and batch counts and then generate. And then it's going to do it in maybe a couple of steps. You'll see from the base model moving and then a refiner. So you'll see the jumps. And then there are also additional there are additional extensions you can see where you can see the actual transferring of from latent space to your final output. So, so many interesting phase. Now, to get things started, I would also recommend doing the same thing, making sure you already know what systems are using, NVIDIA GPUs, Windows, or are you going to be using Linux? Linux is still highly preferred. And it also works the same way. You want to just get clone, and then you can just get clone and then install all the requirements. you can just git clone and then install all the requirements. But for here, the most important is that they're more particular Python 3.10.6 is the one that's recommended instead of 3.10.9 for the LLMs with Ugo Duga. So just keep that in mind. So if we were to make another... Okay, let's close this instance already. So let's just do boot camp 2. Oops. And then, so this time it will do Python 3.10.6. And then it's also the same, they tried to make it simple enough. You just have to get clone this repository. And then after that, just run the batch file. So just run the shelf. Okay. So let's do this. Cut down. Ugh. And then, okay. So git clone. All right. And then. Okay. And then just run. Run this, run the script. I know what, what it will do is it will install PyTorch 2. So you can see this could be a problem if your CUDA drivers aren't compatible. If you're using the latest CUDA, it might try to downgrade. If you're using CUDA 12, it will try to downgrade or try to install PyTorch 11.8. So your drivers might not be supporting this. So just be careful or use a Docker file to make sure that you can support to the 11.8. And yeah, so PyTorch 2.2.1 can run. After this gets installed, so I think I could just, yeah, when this gets installed, we can just, this might, this will take a little bit of time, but when this gets installed, it will, we can get access to the, yeah, we can get access to the web UI for the Gradio. And then we can start generating images or we could start adding some extensions. So I think while we're waiting for it to download, it might be good to discuss some of the extensions that are available. And I'm looking for the. There is another is a tutorial. So then to read me, you can go through the features by a picture, something they have been. Features. Yeah, features oh these one at a time oh okay yeah you cannot go the detailed feature showcase with images oh yeah i think i think it was um i think it's somewhere here if i'm not mistaken there is there's like a tutorial i'm not sure if it's not configs it's was it it's no uh i think it wasn't the read me oh it's the same thing but it's the same thing. Oh, it's the same thing. Oh, no, no, no. Here, they have... Never mind. Yeah. Okay, so let's just go through the review real quick. Okay, just... So in the prompting, it's able to recognize some different kinds of syntaxes. So if you're going to text to image, under text to image, you're able to do enhance the attention of specific parts. You can either put, so if you want to put a prompt that says a man in a tuxedo, You can either put, so if you want to put a prompt that says a man in a tuxedo, but you want the model to pay high attention to the tuxedo, you have to make sure it's an actual tuxedo. You can either add additional brackets or you can just assign, like put the colon, put it in parentheses and then assign how much more you know how much how much more increased so if one is the baseline for every every token you can enhance the attention score for this this also works with lauras so later you'll see when you want to attach a laura we can we can just do this we can just say Laura, and then we can put the attention score of the Laura. Now, other things. So it's able to do upscaling. It's able to do some color sketchings. In painting and out painting. So in painting, you can create a mask of your image. So let's say you created a mask on the face of the person and then have it generate again, it will just replace the mask. So that was in painting. Outpainting is outside what you had created a mask of or outside if you want to extend or if you want to extend your image, somehow like zooming out, almost like a zoom out effect, you can use outpainting. And then, yeah, there are ways to do some kind of depth mapping. Textual inversion, I think this has to do with the text embeddings. And you're able to, yeah, you're able to use different kinds of embeddings and you're able to use different kinds of embeddings. So it's almost like a prompt, but you're adding just additional embeddings already to help with the generation. So I'm not sure if it affects directly into the latent space or if it directs the input before going into the latent space. But then there are, yeah, there are neural networks. So there's a GAN that fixes the face. There's a restoration tool. There's upscaling tools, different kinds. So many, really so many. And then you can resize the aspect ratios, create samples. There are low RAM support if you need to reduce the inferencing to accommodate for smaller. And then there are so much more, a lot of extensions have to do with affecting the latent space. So while it's entering as how Asif had taught us, that when you go down to the hidden space, you're able to try to introduce much more information information uh and that's that's where a lot of these extensions come into play you can do this to to swap a face you can do this to yeah essentially create uh identify segments and so so much more so all this is because of clip. It starts with clip. Your contrastive language image pre-training. It helps from there. And yeah, a lot. You can create custom scripts. So one thing is for select, it says we can use the Xformers library. If you're running an NVIDIA card, that's pretty recent. So we could use this. And then there's also composable diffusion. So the one that adds weights. If you want to have multiple prompts working side by side, you can use the big word and. So if you want your image to have a cat and a dog and a penguin, and you're running this already over uh and you've already created a mask let's say you had you had three pictures of your friends and then you wanted to turn one into a cat one into a dog one into a penguin and you can just create an in-painting of those three friends send in a prompt like this and the first friend becomes a cat the first one second friend is a dog third friend's a penguin so that can happen and yeah a lot more a lot more uh trainings and all that so let's see if we finish oh oh it's downloading it's trying to download stable diffusion 1.5 as the base but but let me instead just get the base models already. So for stable diffusion web UI, it goes under models and under stable diffusion. This is where you put your models. Okay. So this is the main model. This is the base model, stable diffusion 1.5. But we're also including a stable diffusion excel so that uh so that we can yeah so that we can we can we can test the latest and greatest now i think um yeah stable definitions and i forgot we also need a variational autoencoder so let's copy this let me just So let's copy this. Let me just... So if you want to run SDXL, you'll need the variational autoencoder for SDXL. So this one, BAE. You can download this in Hugging Face. You also need the stable diffusion model for both the base and the refiner. And when you have that, you can run. So now if we try running this again, you'll be able to see that, okay. uh you'll be able to see that okay oh it doesn't run xformers by default so next time we can run this with xformers uh and this this will this will move faster model what happened my attention Patrick, this is your laptop with 128 GB RAM, isn't it? Yes, sir. And what does it have? It has a 3080 16 GB RAM. It's pretty powerful. Does it act as a as an informal heater in the winter? No. Luckily, it hasn't come down to that yet. Nice. It's amazing. I have a laptop envy seeing how fast your laptop runs. Yeah, this looks like a business laptop also. So, I seem to have encountered an installation error, but let me see what had happened. Oh, well, it ran. So this is, I just like to, let me just. Patrick, you have to create an elephant playing with the moon. Okay, sure. Hey, okay. I usually like the lighter thing but but okay. Let's see if this runs now. Because it's in my terminal, it seems to be... Oops. So it's not loading this particular model. Let's see. Okay. Okay, this one loaded properly, the SDX, the 1.5. Let's see if I can load my SDXL, the most recent one. Oh, no. So, we're having problems with the SDXL. We could try SD1.4, sdxl we could try sd one point stable diffusion just just a regular one for now oh model loaded okay did it look Oh, okay. So let's see if this can work. Elephant. It's just a refiner right? Second step. Oh yeah, sure. this is just a refiner right second step oh yeah sure right so let's the base is uploading so let's just do sdxl sd 1.5 so an elephant playing with the moon as the positive trump of course we don't want missing limbs we don't want anything we don't want deformed, deformed limbs. We don't want extra tusks. I learned my lesson already there, but okay. So yeah, these are just the sampling methods. There are so many, I don't, I have yet to figure out how those work. But you can also click on restore faces if you're actually creating a portrait. We're using stable diffusion 1.5. So by default, it starts with 512 by 512. We don't want to upscale. We're using a random seed. You can set your seed. seed you can pick a you can pick a generic you can set your seed uh but after you click you can just you can find out what the seat it was assigned to just by clicking or you can just go back to random you can add additional scripts but yes this is generally it so let's see let's generate it yeah this is quite fast on your oh yes the gpu if you have a nvidia gpu it will really run fast so how long did this take um so it took it took it for a couple well not even a second or anything a 1.4 second let's see the picture oh pretty good yeah so this is pretty good but this gets even much more interesting once you start adding extensions so if you click on extensions and then look um and then these are the ones that are added already by default. Like you can do you can add a Laura and all that. But there are so much more things. So let's just go by. So this is almost like a like a window shopping for what you want to add. Yeah. So this one has manipulation. If you want to, if you want them to have a specific pose, if you're generating just a face, you can actually get them to talk because if you have an audio generation, the audio generation one, or you're using a whisper or some text to speech like 11 Labs, you can actually get that stable diffusion image to start talking. You're also able to have, you know, if you want to connect it to your Photoshop and let stable diffusion be the one that paints over your masks in Photoshop, you can do this. There's some tile diffusion, I'm not too familiar with this, but I have to read up the paper on that. But here you can see the segment anything paper. So this one has already, I found out that, so grounding Dino is a zero shot object detection. So it's able to create a bounding box zero shot object detection. So it's able to create a bounding box and figure out from your text, what exactly you were looking for in your image. And then you can use segment anything on top of that to create a mask for that. So let's say we'll use this one. This was pretty interesting. just click on install it will take some time but it will install and then and yeah it will it will load this so that was installed and then you can look at so many other things but i can even connect to CompTIA UI, the flow version earlier. So this one was also good. But I think if you don't have something, if you don't have it available in that repository, you can also go to, what was that? What was that? So the same guy also has automatic 1111. He also has this prompt generation. So let's say you're not good at creating prompts for stable diffusion let me just copy this and then uh it's first one and then let me just install from URL it says put in the git repository so prompt gen so let's say let's install this one and what this does what this does is essentially it will add an additional tab oh sorry i didn't i didn't get to run a tab uh i didn't get to run i didn't get to teach everybody here So let me run through each tab first. So text to image is, put in some text, you can generate an image with some settings. Image to image is, given an image you already have, use this as your starting latent space and then convert it with some text converted eventually to to a new new image so almost like preload these are your preloaded weights and these are your text that you're adding extras uh there are some there are some like they want to upscale or you want to turn a black and white picture into colored they have those you can do batch processing also and you can do so stable diffusion uh is not multi-gpu friendly yet but what you can do is you can have you can have it process like have it process the same task across each GPU. So it will be just sending the same kinds of tasks if you want to create a whole list of images, or if you want to create images in sequence, you could send it to the different GPUs. But generating an image, essentially, you'd have to just, you're stuck with one GPU for now. Then you can't even use NVLink or like you can't merge multiple GPUs to increase. So that's the given limitation. Unless you have the H100 or the A100, 40 gig or 80 gig. Yeah, that's the limiting factor for stable diffusion. So these points are for, if you have some checkpoints. Checkpoints are essentially, so like, what I wanted to show is I can load my stable diffusion. I have more, I have more policy, but you can also train your own uh so hyper networks like a laura if i'm not mistaken it also it also has uh a way of affecting yeah affecting the different weights at the different points and initializations but but here you can you can train you can train your own lores as well. This one was the one I installed. So let's say you want an elephant hugging the moon. And then let's do generate. What it will do is it will already give you a list of do is it will already give you like a list of prompts that you can use oh this is interesting yeah so and then I want this prompt okay I want to do a text image because I don't have an image yet uh I click text image it sends it over there already so now so now we're all ready. We could do generate. And yeah, if we use the X formers, it will probably also move faster. But this was pretty fast already. And yeah, because we didn't put any negative prompt, the elephant only has three legs. So this is why you want to put missing loops and extra tasks. But that's, yeah, so this is the quick intro to stable diffusion. And oh, this one. Okay. Yeah, this one is just a quick answer to stable diffusion. The next thing is we can enable using the use of LORAS, and we can use multiple models, essentially fine-tuned models of the original stable diffusion. But I might have to I might have to I might have to maybe start a new video because I need some setup again. Nice. Yeah. So so yeah, so if yeah, if we could, we could pause the recording for a bit, I could or I'll just stop sharing for now. And then I will try to set up my original