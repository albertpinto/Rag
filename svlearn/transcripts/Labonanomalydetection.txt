 All right folks, today is Wednesday, December 16th. On Monday we covered anomaly detection. Anomaly detection is a vast topic that has a lot of applications. For example, you use it for fraud detection. We use it in the medical sciences for our discovery of diseases, tumors and things like that. People are even using it to find anomalies in the heart condition. So you can use it for astrophysical data. You can use it, I mean there's a wide range of places where anomaly detection comes in. It comes in for intrusion detection in networks when whether you're being hacked, banking systems, whether banks are being robbed. So the potential for using anomaly detection methods is vast. It's a huge field with wide applicability. As I mentioned, this field is just being an expert on this topic essentially gives it's a specialization in its own right. And usually there are lots of startups and big companies that are working only in this domain. It's a very fertile domain. From what I understand, there are not enough people working in this domain. There's always a scarcity of talent for this space. So therefore this topic is important. We covered the basics last time. It's a vast topic. I usually give, sometimes, in the past I've given twice a dedicated workshop on anomaly detection and those workshops were attended though the audience was slightly different they came from specific companies or groups who wanted to pick up this topic sort of like a private class and so forth. So today we will do one part of anomaly detection. If you remember last time I mentioned that anomaly detection has two parts. They're classical methods starting with very basic k-nearest neighbor or clustering and so forth, isolation forest, one class SVM and there's a lots of them. A lot of the algorithms you can repurpose to be anomaly detection methods. Now anomaly detection when you do it could be supervised, semi-supervised, unsupervised, the whole gamut of it. You use it and even reinforcement learning. Now we didn't cover reinforcement learning in this workshop, but they are quite actually effective in this space. And so today we'll do labs. Much of the lab is using pre-existing libraries. The emphasis will be on getting familiar with anomaly detection. Once we become familiar with anomaly detection, we will now repurpose it and we'll see how straightforward it is to repurpose it for anomaly detection but that will keep after we have done a lab in which we have become familiar with anomaly detection techniques so uh you folks are, most of you are coming here after doing ML100 and ML200. So this year's batch, the one that started in March. So at this moment, you don't have familiarity with anomalies and outliers and so forth in a practical way. So we'll start with the basics and see how it works and work through some examples. A lot of the examples that I'll show you are actually taken straight from the documentation or adapted from the documentation provided by the libraries that we'll use. We'll focus on two libraries. One is scikit-learn and the other is PIOD. OD stands for outlier detection. I would strongly encourage you that after this week as a homework, become familiar with it, apply to a few data sets, create your own examples, play with it. And it is important that you know how to use some of these libraries. In the next lab or in the subsequent lab, we will go, actually, this is a lab I need to do at some point. We'll do an extra session. We'll have to squeeze in a session coming to the end of the workshop here. So we will add extra sessions somewhere, extra labs somewhere, in which we'll build an anomaly detector using autoencoders and possibly GANs absolutely from scratch. But that is a little bit more advanced than first becoming familiar with the anomaly detector. So that is the context for today. Now, just to give you guys a sense of the roadmap of how we are moving forward, we are in the subsequent weeks, this is the 13th week, we have five weeks left. The last two weeks will go to projects. So I won't talk about projects at this moment. I think the feedback from last week was that people are just getting started and they are waiting for the holiday season to do much of the work on the project. the feedback from last week was that people are just getting started and they are waiting for the holiday season to do much of the work on the project. However, I will start, we'll cover the usual Saturday updates on projects if anybody has updates, some of you hopefully have a show and tells to do updates to show. So we will do that on Saturday. This Saturday, we'll also have our quiz review. This Sunday, we will have a paper, research paper. There's a review paper on the optimizers used in deep learning. A group of people have sat down and studied no less than 40 optimizers and how well they fare across a fairly representative real world situations and the findings are quite interesting. We will cover that this Sunday. this Sunday. We will give two weeks or at least one and a half week or two weeks, depending upon how much time it takes to time series analysis. I also need to, but then that leaves us with two important topics. One is CNN, object detection. This is how, for example, very much used when you have automated self-driven cars and things like that. So that topic is important to cover in a deep learning workshop. We haven't done that. So we'll keep some time for that. And we'll keep some time for interpretability of AI models, which is again a significant topic. So we will be covering a lot of topics pretty fast and sort of brace up for it gradually in the coming weeks. I will be throwing in a few sessions, extra sessions here and there, just to make sure that we cover all the material that we have. Now in the last two weeks when we do the project we'll give Monday, Wednesday, Saturday and pretty much all three days to workshop a project coverage and I'll be releasing solutions. I'll be hoping that you folks present your solutions. We will learn a lot from the projects. Now the learning from the projects I want it to be different from learning from the projects. Now, the learning from the projects, I want it to be different from learning from the labs. In the labs, these are guided labs. I walk you through the solution. I will, of course, release the solution to the project and walk through it, which is a slightly bigger code base than what you have been doing so far. But at the same time, one week when you folks, each of the teams will make presentation of their work finished or unfinished one way or the other you'll be presenting your work and showing how far you made and you'll be presenting it to your colleagues so do be mentally ready for that now furthermore there's one more announcement it is possible that I may not be here in the last eight, ten days of December, two weeks. So Dr. Chandler who is here with me as part of support vectors and he does of course attend these workshops he has been following. It is quite possible that if I'm not here, he will take over some of the sessions. It might be either with object detection or much more likely with time series. So he'll be doing the time series sessions till I get back onto this. I have some really vital work that has come up in the workplace. I'll have to go and take care of that. Any questions, folks, before we start? Who would that instructor be, please? Oh, Chhenda, you have listened to his, you have attended his talk. Most of you have. Remember the Sunday talk on word embeddings? Ah yes. Yeah. He's good. Yes. So you, he'll be taking over two of the sessions. He's part of our little support vectors faculty list, so, and I think he'll like it. He's, just to give you a bit of a background he's an extremely bright person he's those of you who come from India you know about the IIT JEE he's one of the top rank holders from that finished it did his thing from from IIT Chennai and after that came to us did a marvelous phd actually i was together with him in the phd program we were we happen to be roommates and of course we have worked together for many many years now so and of course there's a few as you said who have attended his talks now that he can be he is very good all right so with that uh one last bookkeeping thing. The quizzes, the completion rate of quizzes is quite disappointing. Last week, Saturday, we couldn't cover the quiz review because almost there were just four or so completions this week we have about 10 11 completions some of you are in the process of have started but not finished it many of you have not started can i please request you to finish the quizzes there is a lot of learning in the quizzes so if you're not doing if you're not taking 10 minutes out for the quiz, it probably means you're not getting time to even review your books or read, do more reading on homeworks and so forth. So it means that, see it can mean only one of two things. It means that we are moving much faster than you are able to keep pace with. So those are the opposing forces. While we want to cover a lot of topic, what I am seeing is a lot of you are being left behind. The completions on the quizzes is a bellwether. It's a clearest indication of whether I'm moving too fast. And it clearly has been the indication for the last few weeks that I may perhaps have been moving too fast right and it clearly it has been the indication for the last few weeks that i may perhaps have been moving too fast you guys are all busy and you're not able to catch up to it see at the end of this if you don't practice all that you have learned may gradually begin to fade out you have to go back and review learning is is cyclical. Please do take out time and learn and finish your quizzes. Otherwise, it's a very anomalous situation. Some of you are insisting that we cover a lot of material, which I am frankly willing to do. But when I look at how much amongst them there is a completion of quizzes or thoroughness in that, then the numbers are disappointing. And so I can imagine that even the labs you guys are behind, projects here, I know that in Saturday evaluations when we do the reviews, projects we are all far behind. So the reality is, guys, that we are moving pretty fast. And in fact, you guys are giving me feedback, I should go even faster and cover a lot of topics. I'm totally willing to, but if you want to get things out of it, do take the winter break. There's holidays coming up and do take time to finish off these things, finish off your projects, finish off your quizzes, all the quizzes and do the labs and so forth. Please, please do take care of that. So with that, I will get started now with the lab of anomaly detection. So as I said this lab will do in two parts. One part will be just using the libraries that exist and the second part would be building our own anomaly detectors. So let me know guys if you are seeing my screen. own anomaly detectors. So let me know guys if you are seeing my screen. You see it just fine. Is it too small, too big? Nice and visible. Nice. So far the lab is running fine on my machine. Okay you already ran the lab, thank you. So again guys all the lectures and labs are being continuously posted to the course webpage. For one thing, one more bookkeeping. The Sunday papers that we are doing, they are an integral part of this workshop. So the participation there is about two-thirds of you show up for the Sunday paper reading. Some of you are not able to make it. Understandable, we all have family responsibilities. But if you get a chance, do go and pick a few at least of the Sunday papers. Read the papers and then watch our code review, the videos of the, not the code review, the paper readings. Paper readings is an important part of developing maturity in a subject. I mean, when I go back and look at my PhD years, the one activity that was most fruitful that we used to do was paper reading. See, the subject matter is, compared to the research level, is fairly trivial and it is things that have happened way back in the past, you know, 10, 20, 30, 40, 100 year old knowledge. It is in the paper that you catch up to the latest and greatest always. And taking a tough paper as forming a little group, reading through it, talking about it, and really being thorough at it. Nothing gives you a more confidence and brings you up to the state of the art in the subject as paper readings, which is why I incorporated it as part of this workshop. So if you haven't attended the Sunday sessions, don't have to go and watch all the videos, but pick a few interesting ones and watch it. So last week, for example, we looked at how, it was a more theoretical paper, it was about how a kernel, I mean these deep neural networks can be reinterpreted or can be interpreted in terms of kernel machines or support vector machines, and that leads to interesting insights. This week we are going to cover optimizers and we are going to look at some researchers who have carefully paid attention to all the optimizers used in deep learning and seen how they compare against each other across a variety of real-world tasks as I mentioned. So do please do that. All the videos are there. All right, guys. So with those preface, let's get started. So we are talking about anomaly detection. Anomalies, well, are outliers, right? Or you can also call them novelty. If you have seen certain kinds of data, you have established a norm, a sense of normality, a representation of normality. Then you get a data point. All you need to decide is, does it conform to your representation of normality or does it deviate from that? If it deviates, it's sort of a novelty detection. People these days don't distinguish too much between the two words. They treat it as more or less synonymous. So people talk about supervised, semi-supervised, unsupervised anomaly detection. And of course, the one topic that I, at this moment, have avoided and won't be going into because we haven't done it is reinforcement learning. How does reinforcement learning contribute to anomaly detection? By the way, this is something I do cover in the anomalies workshop but that's a separate topic. So today we will focus on using other people's libraries. And so there are classical methods of anomaly detection, there are deep learning methods of anomaly detection, and so forth. So we'll focus on only two libraries, scikit-learn and PyOD. So remember that you can treat anomaly detection as a binary classification problem. Is a data point normal or is a data point represent, I mean, does it conform to some definition of normality or is it an anomaly? So you could write a classifier and one example is the, could write a classifier and one example is the comes from the credit card fraud data now credit card fraud data is something that i happened to cover last year in a in a boot camp that we took some participants through and we did it in the context of seeing how well can we use a classifier to work for essentially an anomaly detection situation. There the problem was fraud, credit card fraud and how much we could use to detect credit card fraud with that. Now, when you think of normal data and anomaly, by definition, to the extent that anomaly violates your normality, it's a rare event. So data comes terribly imbalanced. When data is so imbalanced, then the imbalance techniques, such as under sampling, sampling they quite underperform and then techniques like smote which try to interpolate and generate more data between the outliers they can get quite lost or misleading sometimes for reasons that we talked about in Monday's session, namely, if your outliers span a bridge over the normal data, then SMODE could quite likely create sort of a corridor of outliers that runs through normal data. So those are some of the concerns, not necessarily Arthur, but by somehow crossing it. So those are some of the concerns with SMOTE and so forth. Today, Harini is going to kindly show her notebook to look at that sort of an approach and how far it takes us. And then while that is a valid approach for anomaly detection and you know it's a question of judgment, when does class imbalance reach the level that you need to treat it as a problem more specifically as anomaly detection. So there are no clear-cut boundaries. So for example, let's say that five percent of the people or ten percent of the people at any given winter season, as we are going through now, have cold and flu and so forth. So you take their temperatures or you look at some biometrics like their runny nose and whatnot now the 10 of the people let's say that they are positive cases of fluid cold are there anomalies or it is just a classification problem in which data happens to be imbalanced at the rate of nine is to one right that line is blurred and it's often a judgment call and seeing what works. In the same way when you look at x-ray images of patients and you see some presence of some areas of interest Then, and from there you want to inquire if there are any anomalies in the lung x-ray. Is that a classification problem or is that a, should we apply direct anomaly detection techniques? So the lines are blurred. It's a judgment call. One slides from one to the other. Of course, if the data is 50-50 divided, then it wouldn't be an anomaly detection problem. But a significant class imbalance is a call for anomaly detection techniques. They may be worthwhile. So that is something to keep in mind. Now, when you learn about anomaly detection and things like that, one thing that I would strongly suggest at this moment I've started with very simple data, toy data cells, because it's important to get a feel for these algorithms. But perhaps in the next session or you may not get time in this particular workshop, there is a very good place this is this which is uh sorry it is a site of uh lots of where am i this website and there are many such websites you you can go to Kaggle dataset, AWS dataset, and so on and so forth. They all have a curated list of outlier detection datasets. The reason I gave this link in our lab is, all these datasets that you see, many of them are very well known and well studied. In fact, I myself have worked on and practiced on many of these datasets. So it is certainly worth playing with these datasets. When you look at it, you can see that the outliers or the anomalies, if you look at the percentages, It goes from, you know, 5, 6%, sometimes even 10%. Yeah, 15% of Wisconsin breast cancer data, I suppose that's what it is. MNIST with a 10% anomalies, and so forth. So there's a fairly big spectrum of how much and what proportion of data is anomalies right and what proportion is not so you'll get a pretty good sort of a exercise by playing around with this we have standard data the tabular data you have time series data which you can play with and then there's security related data. For example, this has a lot to do with, as I said, adversarial systems and so forth and quite a few, right. There's video data for anomaly detection and so forth. So pretty vast field with a lot of data sets to play better. So but today let's take baby steps. Now that's the publicly available data sets. Today I will start with two libraries. One of them is PIOD. So if you haven't yet, if you don't have it in your system, and I assume you don't have it, you want to run this, uncomment this line and run it, pip install pyod. Run that and it will install it on your machine and after that comment it out because you don't want to run it again and again. So we will take a few things from the PIOD just to illustrate. Now, this is a straightforward method. What we have done is you decide how much of outliers do you want. Let's say 10% anomalies, 0.1, 100 training example, 100 test example, or you can make it 1000 or whatever it is. It doesn't matter, play with these numbers. Number of features we took as two simply because, well, the screen is two dimensional, we want to be able to see that, but play with it with multiple dimensions and see how well it does. One of the things you'll realize is that in higher dimensions detecting anomalies it becomes harder, outliers and all these things, it becomes much harder to do that because of the sparsity of the data. So there is that. Now you remember that we said that some of the simpler techniques are clustering and KNN. So we start with that. And here is what it is. If you go to this website here, the PyOD website, let me go to the PyOD website. You will see that it's a pretty worthwhile website to visit. You'll see a lot of implementations. So do you see all of these names? These people have implemented all of these algorithms. And so there is code and examples for all of them. I would strongly recommend that you, like for example, you take this lab and all you have to do is change one model to another. The rest of the code remains the same. Import a different model and just feed the same data onto that. And one thing I find very useful is if you go to the API reference, you will get an idea. For example, how do you use the variational autoencoder? You can go and use it. This is it, VAE. The rest of the code that we have written will remain exactly the same, which is not going to change. and many many things you can use for example from very simple things like for example knn to obviously isolation forest to xg boost to whatever you know there's so many many algorithms that you can one class svm that you can repurpose for the purpose to apply it to your problems. And changing it is as simple as changing the constructor. Instead of using the K-NN constructor, you can use the VAE constructor, Variational Autoencoder constructor, right? And so forth. And that will serve your purpose. Just one simple line of change. So I haven't repeated those activities in the lab. I leave that as a homework because otherwise the whole notebook will begin to look repetitive. But all you need to do is go. So for example, if you want to pick, let's say just basic PCA, how would you do PCA? You would just literally put this constructor there. You would say PCA instead of KNN. Likewise, if you were to go and do XGBoost, what would you do? Well, you would just put this one line there. Literally that is it. If you look at the signatures with minor changes, they're all specificities. But if you just use a default constructor with nothing in it, that's good enough for most of these situations. So that is something to remember that you could use this library for that purpose. Now, in the beginning, once we generate this data set, it's generating X-train, X-test. So why are we doing X-train, X-test? You realize that quite often outlier detection is semi-supervised or unsupervised. It is just to validate. If you know the answer, you can cross-check. So I'll play with two datasets here. One are synthetic datasets generated by the libraries. The other that we will use is the SMILEY datasets. I don't know, do you guys remember the SMILEY dataset? Oh, yes. Yes, that's right, for our our clustering so if you remember in that data set deliberately embedded a lot of outliers and we'll see how well these tools do for those outliers so we take this k-node detection so quite literally the line four is the only line that would change in this library as you switch from one particular algorithm to another. So from that perspective, using this library makes it extremely convenient for you to use and practice for outlier for anomaly detection. This is why I introduced you to it. So we generated some data here. Let's see how well it does. And this is this, I hope this looks like, and it follows a convention identical to scikit-learn. So x train, y train, x test, y test, literally the notations and everything is exactly like that of scikit-learn, which you are all familiar with. So this should look, even the signatures are the same, that of scikit-learn, which you are all familiar with. So this should look, even the signatures are the same, dot fit and so forth. And then they give you some evaluation and visualization. When you visualize, you have two ground truths or rather, the training dataset ground truth and the test data set come to the left column is the ground group now let us see how well the predictions go so a very basic knn seems to have gotten the predictions like that which is pretty good right here it made some mistakes one two in the test data set, this was. Obviously, I don't know if you would interpret it as a mistake because this seems like an inlier data point. So it works, KNN. Now, the thing is, this looks rather flattering. It seems that KNN would work. This is not usually true. If you try with larger data sets or with more noise, KNN is perhaps not the best one to do, especially for higher dimensional data. The problem with KNN is all neighborhood based methods, they work very well in low dimensional spaces. We are looking at two dimensional spaces. In two dimensional spaces, the neighborhood based methods are very good. The trouble with them, of course, is that as you go to higher dimensional data, the space is extremely, the feature space is extremely sparse and your neighbors are very far off. So the word neighbor, its meaning becomes questionable. Are they really neighbors at that moment? So in lower dimensions, of course, you can do and for any of these, one fact to bring about, you can always build ensembles of your model. Whatever you do, you have hyper parameters, throw in that, run through that. And this example is literally, again, it is not something that I created. The library people have this example in their directory. It's a very good example. And I thought about it, if I could add something more to it, and I realized it is such a simple, straightforward thing, it's hard to add. So the first part of it is very simple. You generate the data, so on and so forth. But the only important thing is here, the only change that has happened is, now you're looking at many values of K, right? You take a whole set of values of K and go through that, so K list, 10 neighbors up to 200 neighbors. And then you just iterate through this. Now, we have done that in different aspects of our lab care when we did a care and lab you remember, we also did. We sort of did a hyper parameter search over the values of key, so that is exactly what's happening, I think the rest of the code remains more or less the same. Right now, they introduce a few notions with their explain like when you look at it, take the average of all the base detectors, like are you saying is it an outlier or not? Take the average of all the detectors. If the majority comes out that it is an outlier, mark it as an outlier. Or use that to create an anomaly score. Maximization simply means that if all of them take the biggest score, right, then so on and so forth. And so you can play around with these. Then you could also generate data in clusters. This is nothing new. Now, one of the basic algorithms is this LOF, local outlier factor. local outlier factor. What it basically says is that a simple way to see if something is an outlier is by doing a local examination of it. If a point is sitting in a spot, you know, if one region of the feature space is less dense or has less neighbors than your neighbors, then you have a problem. So for example, your distance to the nearest neighbor is two miles. But those neighbors that you reach, your nearest neighbor has other neighbors within 100 feet of them. Now, clearly you are an outlier and they are not. So that is a basic idea that local outlier factor looks for. It's a very simple argument that sparse regions contain outliers and further and dense regions contain inliers. contains in in layers right that is it so we say so again these are methods that work well in low dimensional spaces the moment you go into higher dimensional spaces are neighborhood based methods it is a neighborhood based method these arguments begin to get into trouble as if yes the data set that we're using right now for this lab what's the dimensionality of the data two dimensions remember x and y that's it x is only one x is a two-dimensional okay x1 x2 why is the label it's typical you know this psychic learn convention that y you reserve for output or the label x capital x is the vectors for so clearly if you see here this data is a little bit more on the trusted line this is the training data set and this is the prediction the predictions are fairly good except in the training there's actually flaws for example do you see this triangle sitting in the middle of inliners? There is no reason to consider them outliers. So one of the nice things is that the test data set is pretty smart. I mean, the predictions from this is good. It doesn't consider that those as outliers. So see, when you look at data in two dimensions, it can be pretty misleading. You may come to the conclusion that, oh my goodness, the simple methods work so well. But try it on MNIST or something like that, and then you realize that when you're looking at data at 786 dimensions and so forth, it begins to sort of degrade pretty much. Like you need more powerful methods. But that doesn't come true because at this moment, all the datasets have taken in two dimensions. So what is the purpose of doing this lab? The purpose is, guys, to become familiar with these libraries. You will use it in your real life quite a bit if you are in this space. These are easy to use. As you can imagine, this code looks like your typical scikit-learn code that you guys have taken with me before. It hardly even merits explanation. For example, do you think this sentence or this line of code merits explanation, guys? Straightforward. Very straightforward, right? It's just using the right thing. It's not like adding noise. they're very straightforward right it's just using the right thing it's not like adding noise that is right and all it does is if by the fact that it follows the psychic learn convention means the learning curve is close to zero the only meaningful line here is lof do you notice that they use lof right and now pay attention to do these three lines because it's using LOF here. But whereas here we used, where am I? We used KNN. Do you notice that we just changed one line between the two? The two algorithms, I mean, the two code bases are practically identical. The only change is this. That's all, right? There they use KNN. Here we use LLF and that is about it. Which is why I was just thinking, is there any value add if I were to rewrite the code in some different way? At the end of it, I realized that it is the same thing you end up rewriting. So I've left it as such. So this is the clustering part of it. Now what I would suggest is today in class play with this, change the parameters, change the number of data points, change the number of clusters and so forth and see how well it works. So your homework literally is to go to these examples and change the values, change the number of data points and play with that. And I would rather, because now that I know you guys are very busy and you're hardly even taking the quizzes, I want to give the last hour to your just doing it here before we end the day because otherwise you won't get time from taking me out now what does it mean if you use this library by the way they give you a comparison on their website of how it is if they used they have done a comparison of all these creators of pi actually creator i think it's a one-man effort of this, has created a comparison between various anomaly detectors and at the end of this comparison code, there's this beautiful picture. I want you folks to go and look at it and see how well different anomaly detectors have worked. They have taken a few methods and applied it to this. So you can see that each of these algorithms is approaching the same data in a slightly different way. Do you see how these pictures differ? So they all are taking a slightly different notion of what is anomaly and what is not and so forth, what is in line now. So orange areas are in line now and see how our different algorithms are, they sort of resonate with each other to quite an extent. The first one doesn't, right? But rest of them more or less resonate, but they seem to approach, if you look at the surface, if you look at the way the anomaly score propagates across the feature space, from the color map, you can see that they are all thinking in different ways about the data. The same data, different approaches that these algorithms take. And it again speaks to the variety and richness of algorithms in this space. And the fact that nothing is perfect goes back to No Free Lunch theorem. Namely, for any, is the data that decides which algorithm ultimately resonates best with it. The assumptions of which algorithm or approach of which algorithm resonates best with it. So that is something. And so do please go and pay attention to this particular diagram. Think over it for some time. You don't have to spend time looking at this code. It is very simple. It's the same code with now a for loop. They're going through many algorithms. That's all. Like here, going through many algorithms. Nothing special. I mean, you can read through this code, you'll realize that it's exactly the same thing. Wrapped up in a loop of many algorithms. That's all. So then, and I think to their credit, this Pi Odi, the person who created it, the developer think to their credit uh this pi od the person who created it the developer to his credit he has made it very simple that at least the algorithms you can quickly go and use how would you use a variational autoencoder now you would think that we will sit and build one which we will do in part two of the lab we will build these deep neural architectures from scratch will build these deep neural architectures from scratch right from the base up and then we'll train it to do anomaly detection but today just become familiar with it again the code so he uses an autoencoder the only change of course is using this library you don't even need to know deep learning at all all you're doing is for all you, you may not even know what an autoencoder is. You just change it to the name of this algorithm, autoencoder. Rest of the code, again, exactly the same. It's just repetition of what came above. And so, yeah, but once you run it, of course, you realize what happens. The model, you see, these are the things in your model, 300 layers. But you go from 300, 300, 300, 300. It's a pretty significant. Then to 64, I mean, dense, dense. 300, 300, followed by 64, followed by 32. You see the narrowing down the bottleneck? And from 32 back to 64, back to 300. And so there it is. And then you run the number of epochs, is running it for 30 epochs. You can see that the loss begins to fall. Now, what is the point of autoencoders? Just to remind you, you train the autoencoder only with normal data. So in the language of the outliers versus novelty, you will consider it more of a novelty detector. It creates a sense of normality. And then you feed it data and you see how well it does in capturing the outliers. Because the outlier data will cause it to have a huge loss in the output. The loss because it doesn't know, it hasn't seen that, anything like that. So it will try to match it to the closest thing it knows and so you will end up with significant loss. So this is a recap of the theory we did on Monday. And so again, as you see, see the reason I'm not going into this code is this code is absolutely no different from the previous paragraph code, which is no different from the other previous paragraph code. It's the same code, but the only difference is this. And the niceness of this library is that you get the benefits sort of out of the box. The next thing we'll do is apply it to the SMILI dataset and see where we are. So just to recall, this was the SMILI and this was the dataset that I created, if you remember. And if you remember, this gave you guys no end of trouble when you tried to use a k-means or hierarchical clustering. Do you remember that, guys? That k-means or hierarchical clustering. Do you remember that guys? That k-means and hierarchical were a disaster. Do you remember why it was a disaster? Because it wasn't as need of clusters, the different clusters. Yeah, for multiple reasons. First, this data has outliers. Secondly, the clusters are not convex. They are not globular. When clusters are not convex or non-globular, for example, the eyebrows and the lips, they're essentially non-globular. Then k-means and the easy clustering methods, k-means and hierarchical, they get into trouble. So you have to use, and one thing if you recall, we applied density-based clustering and that was quite effective. Let us go and apply that again. This is just bringing back the code from the previous slide. But for those of you who joined deep learning directly, I think it would be a benefit. Density based clustering looks for dense regions and then then everything else that doesn't fall in the dense regions, we covered or recap the theory on Monday. So this is the data that I had produced. This is so you may wonder what mathematical function I used to create the data. I didn't. I literally sat down and drew this picture and then extracted the data. So, so now let's see what we do when I run this dbScan. It marks the clusters, deliberately marks the clusters with colors and the outliers as gray areas. So you can see that it is pretty effective in finding the outliers. Would you say that guys? Yes, and very nice visualization. Yes, thank you. So that is that. Then comes another novelty detection method. And by the way, novelty detection, I'm using it here to be aligned with the psychic language. But generally in the field, as I said, people don't distinguish too much between outlier and novelty. Quite often, you'll see the word, especially in the deep learning literature. People say supervised, semi-supervised, unsupervised reinforcement learning. So this would be like semi-supervised. Why semi-supervised, unsupervised reinforcement learning. So this would be like semi-supervised. Why semi-supervised? You have to carefully feed it only the normal data so that it internally creates a representation of what is normal. So there we go. This is the date. So here is the part and I will do one process here. We'll first go generate the data. So this is the data we generate. So this is a little bit of a trick. What you have to do is go and pick two points and then around it, sort of go generate a lot of data. That's all. Nothing.