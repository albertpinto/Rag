 Today's topic will be Pandas. In data science, we use Pandas a lot and NumPy, you know, making arrays, etc. As we make progress with data, I'll explain some basic operations that you should do with data, clean it out and so on and so forth. There is also the concept of tidy data. Tidy data is a concept that says if you can put data in a long format then any kind of manipulation you want to do with the data becomes easy. There is a very close analogy between this and SQL. In SQL, if a data can be represented as a table, then writing and even there if it is in a good format, then doing joins, doing filters and picking up your columns and aggregates and this and that in group wise. You can achieve a lot with a very succinct syntax, which explains why SQL has been popular now almost for 40, more than 40 years. It's been, I think it's a history of that. I don't remember the exact amount, but it's almost four decades since it's been around. A similar thing is true for the data frame. When we create a data frame in pandas in python and pandas is obviously the default way we do it at scale we do it as a data frame in spark let us say or in a flank or something like that if we are dealing with streaming data but today we are not going to focus on streaming data or a large data set. We're going to focus on pandas. So in pandas, when you have a data frame, and by now we are all familiar with what a data frame looks like. It is rows of data. Each row has multiple attributes. Then we can actually do any kind of data manipulation with a very small what people call the grammar of data manipulation, tidy data and its vocabulary and its language for manipulating the data. It's a very small vocabulary it contains as you will see. If you understand that vocabulary, any data that you encounter, you can go through a systematic process of canonizing it, standardizing it, manipulating it, getting what you want out of it. Today we are going to focus on that. Now those of you who have taken ML 100, you will find that in the tabular data chapter there was a whole section on on this perhaps i should just start by referring you all to that chapter and then we'll go into the code and walk you today so explaining the concept of tabular data so if you go to where do we go to support vectors website. Okay, so now it's better. See there is this page those of you who took ML 100 with me recently. You remember that all our notes are here just to refresh here. This is the textbook for ML 100. All my lab works are here. We'll go through this again in a moment. It will remind you. Then we missed the first lecture. If you remember the first lecture was just getting set up an orientation lecture. So I didn't record it, but thereafter all the other lectures are here. the other lectures are here. This was in the pre-COVID world where we were holding the lecture in class and right after all the future lectures are as you can see we do it remotely. All of these lectures are here, there are about 17 of these. So in ML100 we had 17 lecture sessions. And I intend to create another page here today, maybe not today actually, sometime in the coming week for ML200 so that you'll have one page in which you can go and access all the lectures that we have had on the social media and all of that. But here I'll refer and this is obviously the fact that we are making regular data. So the three kinds of data we deal with are tabular data that is a data frame which is the context for today, network data, we test upon network data in ML 100 which is of going importance these LinkedIn for example is a professional networks Facebook is a social network and so on and so forth then this was the book that we those of you who were familiar with this does this look familiar guys yes it's looking familiar and so this is straight from your and this is shared with all of you here I will go in particular to this concept of tidy data. Remember, I'll just give you a review of tidy data. And those of you who did not attend the summer, it would still be useful. But before I do this, let me take you guys through something that you'll find very useful. In fact, since I'm talking about this course, let me go to this helpful resources. This contains tips, cheat sheets, documentation, glossary, bibliography. Glossary is somewhat sparse, but okay, let's go through this. It may be useful here. So first of all, you know basic tips on plotting, a principle that you should use the least in that you should use ggplot and how do you embed latex fonts or latex into your we covered this last time how can you have a plot in which you can do fancy have a production quality plot right so these are tips that you'll find in this book how do you create a multi-plot you know a grid of plots so there are sample code here to make the grid up plot it's very easy and oh by the way this should be in pasting it here the formatting this should be invented so that's that the next thing i do is our helpful cheat sheets see we and use a large ecosystem of life data science world is interdependent you will create that other people will use your code your analysis will be useful to somebody at the same time you take help from a lot of resources that are available so this is a collection of cheat sheets this is for the our library those of you who are doing it and I find that these t-shirts are actually very good for example gg plot if you click on this thing you will see you get this very detailed i find it very useful so one of the things i do is i traditionally would print it out on long format paper and paste it on the wall and just you know as when you're doing this, it is quite convenient actually just to be looking around and seeing it there. One trip that somebody used to do, which also is very good, what they would do is they would put it on the table, all these cheat sheets, different cheat sheets for all the things they use, and then they would do something interesting. They would put a glass on it, a large sheet of glass or a slab of glass so that they could work on their table. And literally they don't have to look far for hints or for the reference. The references are literally staring them under their nose. And I found that to be very useful. In fact, I'm meaning to do that found that to be very useful in fact I'm meaning to do that but it's very useful see you will know all these things but are they available on the tip of your finger that's what this cheat sheets do they make it available they're not ways to learn it but once you have learned it you can quickly refer to it right like for example this is data manipulation this is what we'll talk about today not in r but in python but you can clearly see from the picture what it is doing this is filtering the data looking for distinct values so on and so forth so these cheat sheets are very useful uh anyway my personal tip would be in them out or use whatever you have a younger generation maybe you never print things so do as you please so this is the pandas reference will pay some attention to it today that's a topic are you guys able to see this funders reference on the screen guys is it visible visible? Yes, it is visible. So one of the things we will deal with today is picking things that you find in this sheet. We will use most of it today. Importantly, we will talk about the reshaping of data. When data comes and it is in a shape that we don't like, how do we make it into a tidy data format? It is the foundation, as very well it says in this cheat sheet, a foundation for handling in pandas. Tidy data was created by Wickham in R actually, in a very influential paper he wrote on how data should be manipulated. Before that, people were achieving the results in all sorts of haphazard manner, manipulating it, hacking it and so forth. When he wrote his work on tidy data, it showed, and he came out with the dplyr library, which is hugely influential in our and influence ponders quite a bit then Vaidhyanathan Ramamurthy, Obviously, people you can see you can tell people who are used to tidy data and people who are not the people who are used to tidy data. data you can very clearly see the data analysis it it is very ordered very structured and very exactly what you would expect and we in a few steps they manage to massage or wrangle with the data into the form that they want to put it it's a very short journey so it is worth knowing so today from a vitamin finders perspective this is what we will. So how do we shape the data? Let us go through all these steps. Then there's data summarization we'll talk about. Basic summarization of data is what? Count the values in a given column. So for example, if you have one of the columns is cities, you would want to know how many cities are involved. It's very simple. You do values count. A question that keeps coming up is, what is the length of rows in a data frame? So quite often I see people do a data frame dot shape. And you pick the, then that shape is a Well, it's a list Like rows comma columns and then you pick the 0th row That is also true that that works but a shorter syntax is literally to ask for the length of the data frame It will tell you if you want to find how many distinct values are there in a column? Once again, you can do do that this is very common like what's the difference between unique and value found unique we just tell you how many cities are there value pump will tell you for each cities how many sort of rows are present right so you can say Chicago this 200 rows present and for San Francisco 50 rows are present and so forth. A summation, again, summation, min, max, mean, variance, standard deviation, median, quantity. These are standard descriptive statistics. So you can do that quite literally, as can see in a direct way it supports that now you have the apply method if you want to do a transformation of a row into something else or a value into something else apply is sort of your Swiss knife for doing it very very powerful it can do a lot of things for you then some of the things we have been encountering are a drop not available. So you want to remember that if you are rich with data you don't mind dropping a few rows where data is incomplete or some column values are not present. You can do drop a bit. Excuse me today, I woke up a bit late, I'm still yawning. Okay, so a fill NA will fill missing values with some fixed value, and then there is a whole technology of data imputation. If you don't want to just fill it with a constant value, what do you want to do? There's many imputation techniques, which are more powerful, to limit this value. How do you make a new column out of data? This is one of the first things people ask. Suppose you have data, in our case, let's say in the California housing data, latitude, longitude, and from latitude, longitude, somehow you want to find distance to San Francisco or distance to Los Angeles how would you do that and the answer to that is let's say that you have some computation that you can do right that would be you can do it and then you know you can create a new column or here is a simple example actually actually I like this example much better than what I said. Suppose you have a column called a length and a column called a height in your data frame. So you see this column and you want to add another column, this green column, which is the area. Area of, let's say, some, something. So wall, let us say, length height of a wall and then what do you do? You can use a syntax like this. You can create a new column, lambda d data frame, such that length times height for each element of the data frame you do this. You could do it in a different way, which is sort of an implicit way. You could just just say you could do it this way and you have seen me do it this way, the second way and The other thing you can do is you can bin data into n buckets. So What it so basically it will make it discretize the data. Next, there are many things min, max, the max of the row, and so on and so forth. I won't go through every one of them. Let's combine data sets. Suppose you have two data sets what can you do you can well you can merge let's say that ADF or data frame and be data frame are x1 x2 and this is x1 x3 one of the things you can do is you can join the the standard sequel join you can join based on the column the common column is x1 here if you observe carefully x1 is the common column you can join on that right when you do join there is a concept of inner join outer join and when you do outer join it can be a left outer join or right outer join. Those of you who are familiar with data manipulation probably know it but I'll just iterate it. The inner join will keep only those rows of the data which are present like if you take a key, a join column, a key these are called the join keys. so this x1 is common to both so you notice that ABC ABD so what are the two common rows a be the first in the second row so if you do a inner join here you notice that the first and the second row are kept if you do an outer join, full outer join, then it doesn't matter. It will keep rows from both the tables. So it would mean ABC, ABD put together would become ABCD. So you see ABCD. the problem is that this one doesn't have an x2 value for D and this table doesn't have a value x3 value for C so those would be replaced with nads not a number but you could choose to say keep the ones on the left must preserve the ones on the left and I will join ones on the left and outer join. So then you have ABC here, but D gets dropped because the D is not present. Likewise, you could do a join on the right. When you do a join on the right, let's look at it. It is ABD. So C won't be here. And here we go. We have ABD, but C is not here. Observe something interesting interesting whenever you do a left right or outer join do you notice the presence of nans not a number in other words number the value is not present because the other table doesn't have it and so you develop not a numbers inner joints if you start with tables that don't have uh n, then the inner join will also not have NANDs. Inner joins ends up with the least number of rows, full outer join would end up with the maximum number of rows, and then the left and the right outer joins would be somewhere in between in terms of the number of rows. Again guys if you are familiar with SQL this is a direct translation from that. Then you could you could do a further filtering joins which is a bit more sophisticated. You can say that given a data frame preserve only those rows where x1 is in the other guys x1 right so you're filtering the a data frame based on a criteria on the other one this is a bit more advanced so let me not uh go too much into it and you could do the opposite of that say exclude the ones that are common or keep the ones that are not common not present and so on and so forth this is it you could also do set like operations what are set like operations remember sets do not contain duplicates so if you do a merge of a Y So look at the Y and Z data from ABC, BCD. If you do a merge of the rows. You will end up keeping only the intersection. So the intersection between ABC and BCD is of course B and C. So B and C are preserved. So B and C are preserved. So the outer merge of course preserves from both as you can imagine. Now when you do this cyclic operation remember that the other column has to be the same. So these are anyway we can go and talk a bit more about it but let's come to group operations. Group by is very very common. So suppose you have a lot of data. Let's take the example that we are going to deal with. Suppose you have a bunch of cities, about 20 cities. For each of these cities, you have a lot of data, temperature data. There's a column called temperature. You don't want to compute the average temperature across all the cities. Maybe that is of some value. It will tell you something baseline. But you're much more interested perhaps what's the average temperature or the median temperature in a particular city by city. So how do you group the data by all the temperature data by city and then take the average within that group? So when you group by cities then for every city there will be a group of temperature data and then you take the average or median or whatever it is that you want to do and it would be specific to that particular group or that particular city so those are group by operations extremely powerful again coming directly from SQL. Then group by is here aggregate sometimes you can just aggregate directly by giving what do you want median mean this and that you don't need to do group by and then sum You don't need to do group by and then some or something like that. You can directly use the aggregate function. So this is the richness of these API, but we'll see it in action in a moment as a, as a Jupiter notebook. So then you can have all this value shifted by one. There are many, many things, cumulative maximum, cumulative minimum and so forth. We won't go into all of those, but this is it. And now, as you know, one of the big things that has happened with the latest Pandas is in the Pandas data frame, there is a very tight integration with matplotlib and and other plotting libraries so you can do you notice that these are things that come from matplotlib they are directly integrated so you don't have to separately call you can on the data frame itself say the dataframe.plot dot whatever kind of plot you want histogram scatter plot and this that. One more powerful thing that came into DataFrames, just as a reminder, under DataFrame is the fact that you can plug in the backend now. Some people find matplotlib to be rather vanilla looking, rather meh looking, then they want to put in fancier backends. For example, if you want interactivity, you don't directly get interactivity with a matplotlib, you can bring in interactivity with Bokeh and we did that if you remember in a previous lecture, we created a plot that was interactive. You could zoom in, zoom out, you could see the value of each point in the data set and so forth. So all you have to do is in the preamble of your data, you have to just specify that you're using the Bokeh backend. Do you remember guys that one line? I think the notebook that I have shared in slack has that example and we did that in the class anybody remembers that yeah yeah the bouquet is a library abstraction of on top of mat lip right or matplotlib yes it brings an interactivity actually what they do is they use JavaScript actually. So they don't produce static images. They do it in JavaScript. Another one is Plotly. Plotly is actually a commercial library, but they have made sort of a suite, a part of it available in open source. Usually, like for example, Plotly is sitting upon D3. D3 is perhaps the most powerful data visualization library in JavaScript. It's a huge, huge following. Plotly is based on that. The good news is that makes it very sweet and powerful. The bad news, I don't know if it is a bad news, but I'm usually very skeptical of companies that will sell you one version of the product free and then another version of the product they'll charge you for because sooner or later they'll start restricting what's available in the open source version. So I have been a bit skeptical, but maybe my skepticism is not justified. Anybody who has more experience about you can speak up. But here's the beautiful thing. All you have to change is the backend. The Pandas Plotting API remains the same. And just with one line of configuration change, you can flip from one to the other. And that is a powerful thing to do. When I'm back into the other. So this is a powerful thing to do. So this is a basic landscape that I'm going to cover today. Let And I will take the example of the weather data because there I think I have worked out deliberately a situation which required pretty much all of these things that you have seen here. We'll need to reshape the data and make it into the standard format. We'll have to do groups, we'll have to do group bys, we'll have to do some manipulation and adding of columns and so forth. So let's try that example. So There we go. There's a few who have read the book, the Exploratory Data Analysis book on the tabular data. You are familiar with it. But anyway, I was talking about the cheat sheet. Let me finish the discussion. These are the cheat sheets that I use. So I just put the ones that I use fairly extensively. And I am inspired by seeing my friend put it on the table and put glass on it I intend to do that I haven't quite done that but it's useful the R comes with a lot of documentation there's a website of documentation R is probably one of the most well documented languages and now Python is trying to follow that tradition you have very good quality articles explaining every library. And there's a journal. By the way, this journal I highly recommend that you guys read if you are doing a coding with R. If you look at this journal, it is not, so let's look at the current issue. The current issue talks about all of these new techniques and R package for bi-clustering, fitting tails or modeling regimes, R package for computing duration based quantiles from the Cox proportional hazard model. So people are announcing new work quite literally here. So here's the thing. And this article is worth reading, a landscape of R packages for automated data analysis. Let's go and look at it for a moment and see the quality of the article because on the internet, the quality of articles are pretty valid. So guys, when you look at it, do you realize that it speaks to a high quality? It's been well written. It has an abstract. It's written in tech, sort of exported to tech, most likely through Markdown. And they're talking about all of these libraries that some of you, some of it you may know, some of it you may not know and just by looking at it what do you contour the most popular libraries are data explorer this is the one that I had recommended and now summary to summary tools I did not know about so see today I learned something I could use this tool called the summary tools the number of downloads is very high so obviously a lot of other people have discovered it and I was a bit in the dark I still have to catch up on this but I've been using the data experiment so now you look at this article we are doing exploratory data analysis and is this useful in fact it is so useful I will immediately go and print it out for myself so you you see that each of these libraries they are explaining in detail and they are showing how to use this libraries what do they give they can begin and these are powerful libraries. They even doing your principal components analysis. So on and so forth. And all these like we look at expand and so forth. See what they do for you. A decision tree fitted using the explain tree. Certainly it's something we could have used in the R version. I'm yet to release the R version. So now what will I do? I'll probably go and incorporate this work into the solution that I'm going to use. So that is the fun of it, guys. You can keep on enriching your arsenal of tools that you use. Remember we did a missing example, yes, function, just these types of data. This is the missing value analysis. If you remember the missing no function in Python has it in black and white well this does one better you have it in color now isn't it and so well in general the our libraries tend to be a little bit more professional little bit more because the statisticians take it a little bit more strict Python is moving faster though in deep learning Python is the dominant king outside deep learning I would say still R has a superiority over a Python part so which is why I should explore a really solutions in that also so you see it here wonderful article literally worth reading guys this world of data science is so rich with other people's hard work and the results of their hard work that it's any hour you give to reading these articles and journals is really time very well spent so that explains for example this our journey what I have done in this notice is all these things that I talk about I put it literally on the left so then this is a curated list the seat and task views if you want to do something let's say exploratory data analysis or regression or clustering somebody has curated the best libraries available to do those tasks has written a review and accumulated the best one so quite often you can shorten your exploratory, I mean your web surfing time by just going here, picking up the ones from here. These are the respectable high quality libraries you can use for the task. Similarly for Python, first of all is there all of these tutorials. In the same tradition as R, Python Jay Shah, Dr. Anand Oswal, Dr. Anand Oswal, Dr. Anand Oswal, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. standard. A lot of effort is put in explaining them very very clearly. So that's the beauty of this subject. Anyway, I have put all the libraries that I use quite often and I put it just next to it so that you can quickly refer to it. So do that. Well glossary is rather sorely missing. The best definition that I have is for mathematics. I hope you'll agree that nothing beats this anyway I'm joking of course so anyway now I'll come back to this document is available for to us like the one that's your book right were you doing a 100 with this spring or you did you miss it not this is spring way back when you oh yes I have not written it I wrote it actually this thing itself okay I'm sending it for publication at this moment and if you guys I mean as I said actually I'm a bit behind I promise people who want to purchase a copy of this so you must have taken ml-100 sometime in your past to be yes by this and this book is going to publication hopefully in another couple of weeks when I get a bit more time but let me give you an idea and I'll give it to whatever the printer charges. Let's say that the printer will charge, I guess it's about 30 bucks. So whatever the print, the reason for that is a printing color is expensive and data science is about colored diagrams, you know, colored plots and graphs. So you can't avoid color in a data science book. Otherwise I can wait wait takes away half the charm of the book so yeah it gets expensive it's about 30 bucks 40 bucks whatever it is at price I'll make it a page of utility so this is the book anyway so I will go in particular to a chapter on so this is anyway so for those of you who haven't seen it, putting it here, it's exploratory data analysis is what it is. And that's where what we'll do today in Python. So what do we do in exploratory data analysis? There are different ways of doing it. Single purpose, you know, you write your pandas data frame dot history plot that histogram So one by one you within our you will do his or you use this automated data explore explorer tools So so far guys in the in the lab which automated data Explorer tool did we use? You guys remember which one did we use profiling yes and then we have the concept of a data frame which is fundamental once you have a data frame the our data frame gives you some basic very high value functions you can call summary on it string covariance covariance plots etc funders is not far behind we just saw so together I would like to say that between pandas and art there's a very very close similarity which is why knowing one learning the other is a child's play so a grammar of graphics let's think through it what does select do whenever you have data in a rectangular form rows and columns one thing you could do is keep some rows and throw away some other rows isn't it so that is the select we are selecting some columns sorry not throw away some rows the columns so keep some columns and throw away other columns so select very much like the selecting sequel does that the technical word that you use is data projection we are projecting it onto a subspace if you think of it as a real value n dimension space you're going to M dimension space which is smaller than the original space by throwing away some of the features. Select will do that. Filter, what does filter do? Filter very much like your SQL filter, you're saying don't keep all the data, throw away some rows or instances of the data based on some criteria. So for example, if the temperature, if you're looking at a rainfall or something like that throw away any data where rainfall seems to be greater than a certain amount because maybe they're they are bad readings our accidental readings for example a rain somebody has some data gather has gathered rainfall in a given day at something like 200 inches 200 inches of rainfall in a given day therefore you would say that those rules are bad rules that's good and collegiate that could be a reason to filter or if you have data and you say I'm just going to look at San Francisco I don't care for other cities and so you would say you filter down to cities equal to San Francisco. So that is a filtration of data. Mutation of data, you know you add things, features, columns, etc. That's a mutate. Arrange, this is your way of ordering sorting in on print data sorting is a classical example right you can arrange data by any other means that you choose but sorting is by far the most common you want to ascend or descend sort data and in sequel you realize that this is sort of something sort of something thing summarize the data is a way of getting descriptive statistics about it so that is summarize about it the distinct just tells you removes the duplicate rows so sometimes people gather data and two people may you know in the data set if other lots of people are gathering data, they'll have overlapping sets of data. So distinct will make sure that you have only unique rows of data. Count will count the number of rows. Then you can sample into the data, you know, you can dip into the data and get a sample. Group by, we talked about group by, you can group by some feature value, for example, city. Take all the temperatures of San Francisco and group them together. Inner join, left join, right join and full join. Exact SQL equipments, we just talked about it from the cheat sheet. Intersection, again the same thing, the common those is the set operation and the union and union all is again a ways of merging data frames it turns out that these basic operations which had there in are also exactly the operations that you have in partners so that brings us to the concept if you notice that you know I was talking about these methods in our deep liar but they are exactly the same as the methods and pandas do you see that guys when we reviewed pandas we were talking about exactly the same methods merge and so forth a mutated there it in our it's called mutate then pandas it's called merge so with minor name you know syntactic differences which is exactly the same so today's topic will be obviously since this is a Python class will keep it a pandas centric so what is tidy data the ID data says it's a philosophy about data it says that if you can put data into a standard format, which follows some rules, then you will have ease of data manipulation. You can use a very small grammar and things will, you know, you can manipulate it to handle with the data with ease so this is a code actually taken by a victim himself when he wrote his paper so this paper that he wrote well I didn't quote the paper it's and today I did so but this is his website let me click on that to your tidy data and irrespective of whether you work with R or Python you should read or be familiar with this. So this book guys I've gathered all the important references that are really worth understanding in a given topic. So see if you can go back and review this book. I've shared it with all of you as a PDF. So what is the basic and I love this quote actually. The first quote is coming from Tolstoy in his book Anna Karenina. Anna Karenina has a wonderful and rather heartbreaking story that Tolstoy wrote. Actually the interesting thing is if I remember right Tolstoy fell in love with a girl, with a sweetheart and got, and they married. And in his days of greatest happiness and honeymoon, pretty much around that time or during that time, he wrote one of the saddest books in human literature, the Anna Karenina. Very heartbreaking. And the book starts out with a sentence. It says, happy families are all alike. Every unhappy family is unhappy in its own way. It's an interesting way for a book to start. So then what Wickham is saying is tidy data sets are all alike. It means once you have data in a tidy form, it's right. But messy data sets are messy. Each messy data set is messy in its own way. And of course, the messy data sets give you no amount, I mean, endless amounts of heart headache. So the first job of a data scientist is to convert whatever data you get into the tidy format. It is surprising how many data scientists don't adhere to this principle. And I think this is because books, textbooks tend not to emphasize that. So they manipulate data in sort of a hacky way. So what we will do today is we will take the weather data set. This weather data set I've taken from the Kaggle website. You will notice that this data set comprises of many files. There is a file of cities which contains a city's country, which country the city belongs to, latitude, longitude. We'll see that in Jupiter notebooks in a moment but let me give you a walkthrough before we go into the Python code then all other CSVs are about one kind of weather observation so the kinds of weather observations are temperature humidity I believe precipitation wind speed and something else we'll find out a few such observations are there so when we look at this data so first thing is okay here we go humidity or pressure I missed pressure or precipitation is not there I take that back humidity pressure temperature wind speed, wind direction. These are the five attributes. How do you read it in Python guys? By now, this one is in R. So we haven't done it in Python, but I'll show it to you in Python in a moment. You read it like this. Did I give the Python version in this let me see I think in this book I may or may not have given yes the tidy tidying data with Python so let's look at the fighting would you guys agree that to read data by now this syntax looks a no-brainer figure 2. Would you agree guys that this is a no-brainer? We are all used to reading data like this. Yes. So this is simple. What is the first thing you should do when you read the data? You just go and check how it looks, you know, a few rows. So here if you look at the first few rows, you realize you're in for a surprise actually. This data looks a no-brainer. Vancouver first column is city, country, latitude, longitude. I believe they have about 34 cities involved here. Now, just as an observation, we talked about it. Pandas is a native support for styling and customization. We talked about it, how you can customize your support for styling and customization. We talked about it, how you can customize your pandas. You see, I've changed the header and so forth, made it look a little bit prettier. But then that looks normal, but then you go and look at the, look at that thing and you look at how the data is for cities and you realize that the data for city is in the wide format wide format means if you let's go and see what it looks like in the so here it is. When you go here, let me increase the font a little bit. Is this now legible guys from everybody's desktop? Looking here? Okay. So when you look at this data, you look at the raw temperature, you do a descriptive statistics you're surprised because what are the columns columns are the cities and let's look at the if you look at this data I don't think I did a preview of this data here if you look at this data and you do a preview you'll realize that it is in a, like columns are the cities and the temperature on a given day called a row is a given day. And it gives you the temperature for that city on that. So the first thing you need to do is you need to put it in a standard format. Let me, we are being rather cryptic. Let us go and check it out. Let's take the data. Let's look at humidity. Let's look at humidity. Do you see something odd here? Date time is the first column. Each of the city names is the second column after that. And every day you have, let's say, a given date and you have the values for it, humidity values for each of the cities. So now if you think about it, does it follow into your tidy data format, which says that data should ideally be what is the tidy data format saying data should ideally be in this format the column should be sort of the observables or the features it is not the data is very wide each of the city is in the columns so it violates this and it says that for a given city or a given day all the observation should be in one row that is a tidied data format whereas what we are looking at here where are we here is not in that format you see that columns are not observables they are instead dimension so you know when people talk of dimensions and measures in data warehousing there's a very close analogy here the dimensions are there as columns but the observations are there in in the rows so you want to change this data into a standard format so guys if this word wide format which into a standard format. So guys, if this word wide format versus long format is not familiar, I would suggest go read my book or go to the Tidy Data website, make yourself familiar. When you did ML 100 this spring, I covered it in a lot of detail, so I don't want to waste time repeating all of that but for if you have not it is really important that you go and do that so what do we do we take data and we want to put it in the right format how do we do that we take the data and we melt it melt is a way to pivot the data. In Pandas, you pivot it by saying rho remains this, the variable which you want to create is city and then the value of the city and the value name will be temperature. If you do that, the data becomes in this format. You see this is in a way a neater format. On a given day, in a given city, the observation was this. Does this make sense guys? On a given day, in a given city, this was the observation. So these two are the dimensions and this is the measure, the observation. And in data science, try to do it like that. Make sure that the dimensions are your column and then your measure is right here. So if you do this for temperature, you can look at a few temperatures. You can do the same thing for other files also so now what will you end up with you would end up with a data frame like this except that the next one will say humidity the next one will say a wind speed the next will see where a wind direction and so forth right so now that you have that it is time to do a join isn't it because what you want in reality you want that a data frame like this should have date time city temperature next column should be humidity next column should be wind speed the next column should be wind direction and next column should be pressure make things like that and how do you do that by joining on these two so let's go and once you have this you notice that we are doing a join. Is this making sense guys? So let's let's stare at this. This is a bit of a complex syntax. We are saying that the ID, the index of a row will be the date time from the original data the new column we are creating is city and a new value column that we are creating is the humidity or pressure of wind speed or so forth so it will create the data into the long format from the white format so once again please do review tidy data is important i wish i had time to spend more time on this but now what do we do this is a simple join operation so guys let's go back to the pandas data set this partner's thing tidy do you see that I use melt if you look at the cheat sheet do you see me using melt isn't it nice followed by once I have melt I do join of the data right so how do I join the data somewhere here is a way to join let's go back and say a combined data set right so there are there different syntaxes this is merge right you could i could have used merge or you could directly use the word dot join which also is there i think this one doesn't mention join but you can directly use join, which is more appropriate here. Join and then you have to just join on what? These two column attributes. When you do that, you will get a combined weather data which will look like this so do you notice that all the columns have been joined so in other words data from all of the files have now been joined to this this data we can do a cleanup if needed drop null values it turns out that you don't have null values so zero null values then you continue on so you drop the end so then you ask for what do you do with the null values remember one way is to drop it you don't have some column value is missing you drop the room sometimes you don't have to especially for weather you realize that the weather between two days are likely to be relatively similar temperature are likely to be similar if the temperature value is missing for a given city one easy thing that you could do is just interpolate between the two days are we together guys you can just interpolate the value of the day before and the day after the temperature if you guess that the temperature today is the average of the temperature yesterday and the temperature day after the temperature tomorrow you'd be fairly accurate and that's what interpolate does interpolate will fill up all the yeah all of the first two because how do you interpolate in the first and so then you can drop the this is the Python syntax that happens to be here then you can drop that row. This is the Python syntax that happens to be. Then you can rename here. I do some cosmetics. I noticed that temperature, these things have capital letters. And so I canonize the names so they all look a bit more uniform, date, time, city, temperature, etc, etc. And then here I'm just showing the top seven rows of data. Suppose I want to filter down just to San Francisco. What will I do? This is the filtering. When you take a data frame and in brackets you put the filtering criteria. What are you saying? weather.city The city column must be San Francisco. Then this data frame will limit itself to only San Francisco. And so for San Francisco, we have so many data points. What can we do with these data points now? We can do further things. For example, you can do what you want. For example, you can take the average years or whatnot. So here what did we do? We did a restriction on the rows. We only want the rows associated with San Francisco. Alternatively, what we could have done is we could have said I am not interested with all of these attributes, just give me city, temperature, humidity. Forget about wind speed and wind direction and pressure, right? Just give me these four columns. So how would you do that? you're all familiar with this remember this is how we would create the X the features vector the feature matrix and the Y target variable we have done this in our notebooks isn't it guys the river data set the California housing data set we have so this syntax is very common. You pick a few columns, you sample into a few rows. So now data is limited to these four. How do you find the average weather for each city? See how powerful the syntax is once the data is in tidy data format. You can say weather.groupByCity. So now you group by city and then you say for each of these attributes temperature pressure all of these find the median what is the median days german what is the median of a bunch of numbers average the center value you know you sort them Average the center value. You sort them. The center value is the median. So there are three things mean, median, mode. Just to refresh. Mean is take all the values and divide it by the number of data points. Average. Median is sort and pick the middle guy. What is mode? Frequent value. The most frequent value. That's good. That's mean median mode. And then there's standard deviation. So, Asif, just here we use like a bracket right around city and sometimes we use like that other bracket right I think there is a is that when you pass the argument then they use the square bracket break open this box so you have better as a data frame is a function on that data frame okay say goodbye city you're calling a function on that data frame. It's a group by city. You're calling a function. You're calling a function. So now what will we get? We will get a data frame in which for each city there will be many values. And then what we are doing, we are saying that in that a pic only these columns leave the other columns wrong because that thing will still contain the city column and other things right so it says just pick these columns this is very much like you're passing an argument yeah no this is square bracket is a list of remember the square bracket is always a list in Python square brackets produces a list yes are you're saying that from that data frame pick these columns right and from these columns find the median of these columns right now the thing is you got the, it is worth giving those medians some name, right? These medians that you found. So you say, okay, you know that the first one, second, what the each of them were, you give it the column names because otherwise they were missing column names. You just give it column names. And then this is just reset the index means forget about the index column, the row ID column, reset it. And then you sort it by media. Let's say that you want to find the, from the hottest place, sort values. You can sort it by values in a given column. So when you do that, see what do we get? So can you tell which is the coldest city and which is the hottest city? Coldest is Montreal, right? Or 281, what do you think the units of measurement are here? Anybody can guess? What are the units of measurement? In what is the scientific unit of measurement of temperature? It's Kelvin. Kelvin, exactly. So these are Kelvin. So if this number looks huge in fahrenheit you have to remember that these are kevin so 273 is zero degrees and zero degree zero degrees celsius or 32 degree fahrenheit so this basically in terms of centigrade it stands for median temperature being just 8 degrees centigrade throughout the year you would imagine that if you live in a degree centigrade medium temperature you would call it a rather cold place Minneapolis also in the US as you know is cold Toronto is cold and Seattle looks cold Vancouver all of these are up there Detroit these are up there now then cities they are on the other hand let's go and see what are the hottest places Miami Miami is close to 300 degrees like temperatures are pretty high isn't it 27 25 degrees. Houston, you all would agree, Phoenix, Arizona, San Antonio, these are all fairly hot places. Tel Aviv, Tel Aviv certainly and Jerusalem are in the Middle East. Those are really hot places. All these are hot places. So this is it. And the same thing if you want to just prettify it, I'll just show you how to prettify this. So, so guys, this is using Pandas in Python to do data manipulation. The first time you encounter this and I'm speaking to those of you for whom this is your first encounter with data manipulation, I would I would again remind you of the advice I gave in the orientation book. Pandas for everyone. That book should be your primary text. Read it. Those chapters are easily digestible. Each chapter you can finish in two, three hours. There are, I believe, 16 or 18 chapters or 20 chapters in that book. Every day if you do one chapter, in two, three weeks you would have finished the book. And just giving two hours or so to read one chapter in the book is not much. Do it, practice it. If you just read it, it's an hour. Practicing it will take you an extra hour. Correlation plot. How do you find, what is correlation? Correlation is how related are, you know, how co-varying the normalized values are. You know, once you Z value it, it gives you the covariance of two variables. As if question. Yes. In line 23, as if, when you set the index, is that sorted already? Is it already sorted? Yeah, it is at this moment, it is already sorted. And then, and then you just reset the index from 0 to 30. Yeah, you reset the index. The reset index on the city. I mean you make city itself the index. So do you notice that there is this index column here? Okay, let me explain what I, to give you the context. It's 21 probably that you are referring to once you have done all of this right data you have sorted the data remember where did we sort the data yeah you sorted by median temperature right so observe something you have this index column the funders data frames will always have this index column it is serving no purpose it's just a row ID what do you do about the row ID it is unnecessary because each you realize that a city sorry city is unique in this table city is the unique thing so you don't need this you could have made city the index so two ways to deal with it first is you can go and just say reset index. If you reset index, then you will see something like this. Zero, one, two, three, if you look at the data. But actually, these indexes are necessary. So the next thing we do, and I'm just showing what all the things you can do. You say that, forget about these numbers. let us declare that the city itself is the index so i make city the index right you say make city the index column and effectively get rid of the row number as the column index column every data frame has an index column so make city the index column you just got rid of one unnecessary column. And so you can then go. And now that it became the index, you don't want to have an index and a city as a column also present. Now city column is useless. So you can go and drop the city column and your data then. If you sample into the data, you will see a slightly different behavior. Actually, there is something odd. If you run this, actually, this should have been city names. Yeah, you should see city names. So, I have to run this now. Did you get that, Patrick? You remember that? You did this in the last course. Yes, Asif. My question is, Asif, if I sort and then I just put inside the sort ignore index equals true, will that be the same output? Yeah, it will do that. That will work too. So whether data, now merge, remember I talked about left join, right join? Well, those joins are here as I'm using merge left on right on right so you could do that city right on city your emoji i could have just used join i wanted to just show you the syntax for both left and right like you could use merge to do the join or you could use literally weather.join if you want. So you're joining another table called the means and the city table and so forth. So now all you are doing and anyway, these are minor details. All we are doing is we had the mean temperature of all mean etc etc of every city. Temperature is useless actually these things are not. So temperature you added the mean value what is the use of this? Now I can complain the temperature of a given day versus the mean temperature of Vancouver. So what does it mean on this day, which is 6th of March, 2014 are the temperatures below, actually it's not mean, it should be median. Are they below median or above median? They're below median. So it is a colder than normal day, isn't it? On that particular and so forth. You can go about the day like this. particular and so forth. So this is data manipulation with Pandas, guys. I've just given you a flavor of it. This notebook is available to you and of course this book is available online for all of you taking ML100. Soon you'll have a printed copy all of you if you are interested we go review that uh we are reaching a stage at which you know you know enough about the language you need to learn about the library so today was the day to do the pandas library pandas is very powerful you know this is a course in machine learning so obviously we are focusing on the algorithm but i thought we needed to give time to basic uh you know the vocabulary of data manipulation by the way approach is maintaining the list of people who are interested in purchasing a copy of the book uh give her your name if you haven't given at some point because if we group if we make a group order with the printer there is a discount they give if you order more than one more than five copies or something as an author I don't get any any discount I mean any extra discount so we will all collectively order this book I will keep a copy for myself you guys will all get a copy it's an unfinished book that's why i'm not fully publishing it so you will you're essentially buying a draft or printing out a draft copy for yourself so with that guys i don't have anything else i'll take questions so all right guys i'll summarize what we learned one thing is that i started out by saying in these notes that I've given you guys obviously apologize to people who haven't taken a 100 Just reminding you that there's a lot of references those references are very useful. use of the references. It is surprising even when you're reading email or doing something, if a reference card is just under your nose, you end up imbibing a lot of it, those cheat sheets and those documents. The R journal is very good. It comes out periodically and there are a lot of these articles that come out. One website that I like is Towards Data Science. I've mentioned that before. I have obviously no association with them I like you and just another subscriber I tend to like it but I assume there must be other websites also devoted to this but this one is pretty good actually so I think they charge me if I remember right they charge me a little bit of money to read all the articles, some two, three dollars a month or something. So somebody told me that first few articles are free and then after that you have to go into the incognito mode and you can still read it. I don't know. Personally, I like to support the site. So I actually like Like sort of patronizing all the people who contribute articles because apparently their model is that anytime you write an article on the site. And anybody can write and then based on the number of claps you get you get money, maybe each clap is a dollar or something like that. So people can actually make a bit of money by writing high quality data science articles and posting them. So this is another. So there are many such websites. Then some people like the analytics Vidya a lot. It is filled with some good articles. You can try reading all that. YouTube is of course absolutely chock full of good videos more than that the second thing we talked about was that data it is worth having data in a tidy form and for pandas because today we were doing python ponder's data frame if you follow the tidy data syntax, you will realize that manipulating data with the pandas is both elegant, powerful and easy. You can do a lot with just a few lines of code and you should always learn the right language or grammar of data manipulation. It helps you a lot going forward. People these days are in a hurry. They come into the field, the very first thing they want to do is how do I write my first deep neural network? And in a way, it's sort of you get onto the hot topic, it's exciting, it's exhilarating, but someday you should go back and fill in your basics. This is filling in the basics. Very good. Some data manipulation and that's a summary. Go ahead Patrick. I have two tables that share the same, let's say city column. What's the quickest way to compare if they have the same amount of cities listed or the data is exactly the same oh that's very easy so see if you want to see if the number the cities are the same all you have to do is do dot unique like you can get this set you know you can take the columns all right not unique as if like the exact same exact same values in the exact same order Oh why is it whether the two tables are exactly the same data so first what I would do is I would sort both the tables see it's basically you should go like this you've sorted columns because somebody may have written the same data in different order so you sort it in the same order after the two things are sorted in the same order then there there I think data frames may have a way of comparing that I don't remember it's a good question actually I don't know the exact answer to it but one off the top of my head answer that I can give you is i would sort both the data write it out to csv and just do a diff on the csv okay so as if what i did was um i pushed i pushed the values to a numpy array and then i sorted it and then i compared if the values of one table would equal the values of the other table yeah yeah that is that's just as good right see in data frame itself you can write a little bit of code that will compare it column by column right once you have sorted the data it will compare it column by column because each column of data is a series right and you can say on the series you can check if two series are equal. Two series is nothing but a wrapper on NumPy array. And you can check if those two areas are the same. So you can iterate over all the columns and just verify that. Okay, thank you. I thought there was a one line to do it. Maybe there is. This question that you asked was very interesting. It didn't fit in a very good fashion. Let's explore it differences quick tip there it is somebody here then if he is always works if this is okay here here's he has a data frame one data frame two you concatenate the two data frames we said the index then you group by list of the DF columns so you group by those columns one two three and now this is a more complicated I think compared to data frames and output the difference is side by side somebody else has yeah not even so this is it yeah this is really the shortest syntax what you do is you take the data frame and you take only those rows which differ right where something is different not equal right and then you produce it so yes so yes this is the way it's so nice i learned something let me paste it in there that should i yes this is an elegant answer I suppose to that. It does it in one line. Can't we use join, Asif? Join would not be the best way of comparing. Asif, if we go be the best way of comparing. Asif, if you go to the Pandas cheat sheet. On the second page, combine data set. The CA, so that is the difference. So if you combine data sheet, the last one. All rows that do not have a match. Yes, you are right. Yes. Thank you for pointing this right here. There's another way of doing it. You just say all the, but this is only on a column. The trouble with this is, Anil, this is only on a column basis. So you're comparing the first column to the second column yeah so yeah that is it so you'll have to write a sort of a for loop to iterate over the columns uh that that also does you see that is the approach i was thinking about but this one seems even more elegant it just if data frame not equal to data frame 2 which means that data frames bundles data frame is pretty smart internally it will do the comparisons for you. Isn't it? And so look at this syntax. Very, very elegant. I didn't know that. In one line you can find that. Like both approaches. Thank you Anil. Thank you Asif. Thank you, Asif. Yes. And people have added more and more to it. Very nice. All right, guys. So that is it for today.