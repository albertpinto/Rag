 NLP with Transformers Everyone, welcome to this workshop, NLP with Transformers. Today is Saturday, the 18th of March, 2023. It's a really beautiful morning here in California. Search using Elasticsearch, it is something called keyword search. The keyword search is not as effective as you know. As for example, you have gotten used to Google search. When you go to Google search, most likely you find what you're looking for and you find it in the first page. In the world of search, call it, you know the measures that people use is precision and recall. Precision is the search results being precisely about what you're looking for, having high relevancy. Roughly speaking, in this context you can think of it like that. If I produce search results, the top results, are they precisely about what you're looking for? Are they relevant? So that precision speaks to that. Recall speaks to the to a different aspect. Let us say that you are searching through a corpus of documents. So I'll keep using this word corpus. Think of it as the the big set of all the things you are searching from. So imagine Corpus as a library, and imagine that the answer to your question is there in one of the books in the library. Let us say that you ask a specific question, and there are, let us say there are 10 books that are absolutely relevant to what you're asking. A search engine that picks only three of those books and presents them to you in the search result and doesn't produce irrelevant results has very high precision. But it is still not a good search engine because it missed out the majority of the relevant books on that topic. Isn't it? So you would want to make sure that when you search for this topic, all the 10 books that delve with this topic, they show up in your search results. And that is measured by recall. How many out of those 10 actually showed up? Are we together? So recall says, did we pick all the right answers or did we miss some it's a measure of that and precision speaks to the relevancy aspect of it the search results that we got are they truly relevant or did i end up with junk like Like for example, if I'm a farmer and searching for apples, and let's say that I'm searching for ripe apples, or how to grow ripe apples. And I start seeing in the search result, also all sorts of entry about the Apple computer or hardware, that is a lack of precision because I didn't mean that. Now when you look at keyword-based search, one of the issues that we have always had with keyword-based search is precisely that it's hard to get precision and recall right because in natural language, we have synonyms for things. And I would like to now just open a notebook and I just, you can concoct all sorts of examples. You know that in enterprise search, you search for something and often the most relevant results don't show up. Then that is a, that is poor recall. And also some very irrelevant thing shows up. We use synonyms, English language, we use synonyms. And then we often, when we, human beings, when we enter keywords, we are not very good at putting keywords. we are much better at describing what we are looking for, describing our intent, because for millions of years, we have been communicating with each other and communicating our intent rather than keywords. So here is, let me share my screen and go to some examples. So let me get to so ideally when you search let's say that you're looking for breakfast places you're feeling hungry it's morning and you and you search for breakfast places it's a very natural phrase for you to enter but you realize that it has a word breakfast and it has the word places. What you mean when you say breakfast places is you mean restaurants, cafes, etc. All of those things that serve items appropriate for breakfast. Now, there may be a restaurant which does not describe itself. It may call itself a shop for espresso or for crap and so forth now you notice that the word espresso crap shop they are not in the search keyword breakfast places and that that in in a nutshell, illustrates why keyword searches don't work as well when humans search for things. which returns to you all of the shops that give you expressors and crepe and all other breakfast items, or all sorts of food items that it knows are meant for breakfast. And it should also understand that breakfast places refers to restaurants, cafes, food know, food trucks and whatnot. Everything is included in the semantic meaning of breakfast places. So today we will learn how to do semantic searches. So semantic search is therefore very, very different from a keyword based search. Keyword based search used to be the basic philosophy or the basic technology behind keyword based searches. It's something called the inverted index. It maps for every keyword. It keeps a list of documents that contains that keyword. Right. Or with certain weightages or relevance, how relevantly this keyword is present in this document. So when you say imagine that you have a hash table in which the keys are the keywords and the values are the document IDs along with their relevance. And so when somebody searches for and just a few keywords, all you're doing is finding the subset of keys that match the keywords and then looking up the documents associated with them and then you know looking up their relevance and returning the sorted results. If you get that basic idea you pretty much know how the keyword-based search engines like Elasticsearch, Sonar, UCN, etc work. Now I must say that that is the key idea. using such a word now i must say that that is the key idea these uh these uh search engines these keyword-based search engines linguistic search engines are actually very powerful they have developed a lot of strong capabilities which is beyond the simplified picture that i gave you nonetheless that is the foundational idea it is both its strength and also speaks to its limitation. Now, when we talk of semantic search, the fundamental problem is, how do you understand what the user means? And that's where learning, the learning comes in, the machine learning comes in. When you take a model, imagine Transformer as a baby, to whom, which has been exposed to lot of text, it has listened to human narratives, and it has processed through a lot of narratives, so that it has learned to learn to understand the semantics and to understand a sentence and its semantics, its context. So it knows that when you talk of ripe apples, inherently you're not talking about computers. Computers don't get ripe, isn't it? You are talking about fruits. computers, computers don't get ripe, isn't it? You are talking about fruits, right? Or when you talk about breakfast places, then that shop that sells crepe next door is very relevant. That is semantic search. And the power of AI, the power of these transformers are, they are very good at understanding context and semantics. They're much, much better. These are the best that we have today at understanding the semantics behind the words. And that is what explains why they're having such a runaway success. We are making bigger and bigger models. These are called large language models these days. And these big models understand the semantics of a sentence or text or questions being asked in a pretty coherent manner, in a pretty good way, almost with human-like ability. and then they are able to generate or synthesize answers to it. So let us go and see what we really want a semantic search engine to do that should feel intuitive to human beings. What we want is the user enters something, a search query, but not as keyword. The user can just describe, I'm looking for good ways to grow apples that will become ripe without being infected with pests or worms. So suppose I say a sentence like that. Implicit is that I am trying to farm apples, or I need some app I'm thinking of growing some apple trees. Right. Or maybe I have apple trees and my apples are getting infected. And I'm looking for ways to improve the situation so that my apples don't just get infected. They do actually become ripe and tasty, and then I can get to pluck them. So the descriptive narrative should go as input. And the second stage I just marked as magic happens, which is that the AI should do, and what it does is what we learned today. It should figure out what is the semantics behind the sentence or narrative that you have. And it should produce search results which absolutely has high precision and recall. Means everything that is relevant in the corpus, in the library should show up. And only those things that are absolutely relevant to the user's intended search, only those things should show up. So it should have high precision and high recall. Motion of search. Now, the problem is, how do we go and search? Think about it logically from an AI perspective. Now, I'll give you a sense. Machine learning has a limitation. Input to any machine learning algorithm has to be a bunch of vectors or you can call them tensors, but tensors are vectors. They have to be vectors of some form, right, or matrices or tensors. For simplicity, let's just say vectors because I don't want to keep saying vectors, tensors, matrices, and so forth. They have to be vectors. Now, if they have to be vectors, right off the bat, you have a fundamental problem. A text is not vector, isn't it? But text is well text. It's unstructured. So we need a process to convert it into vectors, given a bit of text. How do we do that? So NLP has the technology to do that, a process. In fact, that whole area is called the generation of sentence embeddings. So when you convert a sentence or a text to a vector, and these vectors are by the way not two or three dimensions they're typically very high dimensional vectors uh 300 dimensional uh seven eight hundred dimensional vectors these are high dimensional vectors so when you convert them now you may say how do i visualize a 800 dimensional space you can't but what i do is in my mind I always visualize it as a two dimensional space, because I can draw it on a piece of paper. I visualize in two dimension and pretend as though it is 100 or 800 dimensions, and most of the time it works. So there is a technology called sentence embeddings. What sentence embedding does is it takes a sentence and it converts it into a vector. A vector is a point in an n-dimensional space. Now, for simplicity, as I said, think of it as just two-dimensional space. Two-dimensional is what? A page. On a page, if you draw x and y-axis, you get two dimensions. So imagine that every sentence somehow using techniques, and in fact, that's where transform is coming, they understand that sentence, the semantics of the sentence, and they put it as a point in that page. But what it does is, two sentences which are semantically similar, it will place those two points near to each other. And points that are semantically different, so one is talking about, for example, politics, another is talking about, I don't know, maybe cats. They will be far apart. Those two sentences will be far apart. So that is where the crucial magic is. You put the neighbors to a point, any sentence, so when a sentence becomes a point, the neighbors to that point are other sentences, which are semantically similar. And when you can discover such an embedding, you say that you have a process to do sentence embeddings, convert a document. So now I'll start using a little bit of a jargon because we want to get into the hands-on part. Let's say that you have some document and text. In this world of energy, people interchangeably use the word text and document. Any piece of text, we can consider it a document. And so given a document d i, there is a corresponding vector x i in an appropriate d-dimensional space, and that x i is called the sentence embedding of the document gi right now the collection of all of these points all of these vectors it is your search index in the world of search a database of like database of your search documents or searchable documents is called a search index. So in the world of semantic search you can consider that if you take the sum total of all of these vectors, they form your search index. You store a collection of these tuples, document one and it's embedding, document two and it's embedding and so forth. Now, how would you search? Well, first of all, there's a caveat. Can you zoom in a bit? Is this better? Yeah, much better. Thanks. Okay. Now, there's a bit of a caveat and this is getting familiar with the language. See folks, in English, when we say a sentence, the boundaries of a sentence is, a sentence ends when there is something like a punctuation of the form of period, question mark, exclamation mark. These are punctuations that terminate a sentence, isn't it? So for example, you would call this, this line, one sentence, and you would call this paragraph as two sentences because there are two periods in this paragraph. But the way people use it in natural language processing, they have a more generalized definition of a sentence. This sentence is any piece of text. Any piece of text could be called a sentence. In other words, they use the word sentence and paragraphs also sentences. And implicitly documents also sentences. So remember that when we use the word sentence, we may mean an actual genuine grammatical sentence in english we may mean a paragraph of text or we may mean entire document of text like long passages of text any one of those things can be called a sentence in natural language processing so it's part of the one of those things that trips people over right now i told you that when you create search embeddings, sentence embeddings, then semantically relevant pieces of text are nearby a point, a sentence in the embedding space. So. That means that we implicitly have a search engine, isn't it a semantic search engine suppose i have a query query text right how do i grow ripe apples without it being infected with infected with worms let's say that is a sentence that sentence we can embed into a vector then we can go into the embedding space and find all the points near that vector. And if we do that, those points, whatever sentences they correspond to, those documents have to be the most relevant search results. Would you agree, guys? And implicitly they will have high precision and high recall. So long as my sentence embedding technology is good, the better my sentence embedding technology, the better the technology, of text the more likely is it that the points around my query vector submitting will be other documents or other pieces of text which are highly relevant and probably answers this question right and which is why these days you notice that the You notice that the boundaries between question answer and search is beginning to vanish. Today, these transformers, they can be used as direct search engines to get... You need not write your sentence as a search. You may just say ripe apples without worms. That's not a question. And it may still find all the relevant documents or you may pose it as a question. You may say, how do I grow apples to a ripe state without it being infected by worms? In which case it would again find a similar, more or less the same set of relevant documents in the library in the corpus. Do you see that? Now, those of you who are following the chat GPT Do you see that? Now, those of you who are following the ChatGPT and its ability to answer questions, you must have noticed that there is a big news these days that Bing, Microsoft's Bing search engine, has already incorporated ChatGPT into their search. And so you could actually ask questions and get answers and google is in the process of doing the same like everybody is doing that nowadays so we are going to learn something very very topical from that perspective so now the question comes how do you find the similarity between vectors so this is a little bit of a technicality i won't go too much into it. So intuitively, so suppose you have a query sentence and it is this orange dot, then the relevant document is for example, something nearby, right? Any document whose embedding is near it in the embedding space is essentially likely to be the, it is the most likely answer to that question, assuming that the library does contain an answer to this question or has something relevant to it. Would you folks agree, guys? Right, now a little bit of technicality, and we are just about to get hands-on now. When you look for similarity or proximity between vectors, it turns out that in high dimensional spaces, Euclidean distances are not so useful unless all the vectors are of unit length. So, for example, if all the vectors are of unit length, they all become points on a sphere. It's a technical mathematical thing. If you don't get it, don't worry about it. But generally in high dimensional spaces, most points are far away from each other. Unnormalized vectors are far away from each other. So Euclidean distances are not very useful. It turns out that distances, the notion of distance are similar and the converse of its similarity that makes sense are dot product and cosine similarity that product is you must be remembering given two vectors how aligned they are and you also look at their magnitude and cosine similarity is when you first normalize each of the vector right you you make each of these vectors into, you know, sort of normalize it. And then you look at the cosine, the cosine of the angle. So you basically take the cosine of the angle between them. Means you don't care for their magnitude. You just care for the direction. How aligned directionally are two vectors to each other that would give you the cosine similarity. So, all right, guys, similarity so all right guys that is all the theory that we need to know and now we'll get into the practice now some practical considerations when you actually do search engines using these technology with transformers there are two different use cases that people tend not to realize and i thought i'll mention it see these transformers are trained to answer to embed points when you train them in such a way that you expect the documents to be short or of the same length as the query query sentence then it is called symmetric search when you expect that the documents are usually much longer than the query text, then it is asymmetric search. Most of the time when you go to Google, we are doing asymmetric search. Just a minor technicality. If you can remember it, it's fine. If not, the only thing to know is it is far better to use the right model. When you use the right model, it helps. Use a model, and there is a whole list of models that you can go with, but pick a model that caters to your needs, to your use case. Now, by the way, in the distance measure, I should mention one more behavior, practical behavior that you see. Between dot product and cosine similarity, which one should we choose? It turns out that cosine similarity, if you use to train your transformer to understand the semantics of sentences, but then cosine similarity tends to favor shorter texts. And dot product tends to favor longer texts. So when you get the search results, you'll notice that if you use the cosine similarity train model, it has a preponderance of shorter documents, shorter texts in the response, in the search results. Whereas if you use the dot product based, you will see that a longer text tend to get favored. So that is something. So these are practical considerations. So now let us get into the do part. So we are going to do all of this together guys. At this moment, I'll just stop. I'll just give you an eye, but okay. I'll give you a general sense that we can do it by now. But you can search. We will do this. We'll start with a simple situation. First use case. We will take some documents, some sentences. I will take a toy sentence, toy situation, just a handful of sentences, and we will create their sentence summary. So that will become our search index. Then we will take a query sentence, right? And we will search for matches in the search index and see how good the search results are, right? Text-wise. So that will be the most intuitive thing that you can think that's the way you think of your search. But then we'll go one step further. We are going to take, oh, look at this, what is this? We will take this cute little animal, a golden retriever. And we will give this image to our search technology to our search engine to our transformer and we will say go search give me all the documents that are relevant all the text that is relevant to this picture right and and you realize guys that we are already beyond what a linguistic search can do already beyond what a linguistic search can do you cannot do a keyword-based search now it's it will be on that because there is no equivalent for that in the ordinary search a picture there is no relationship between a picture and the keyword you can generate a caption from a picture yes that's true but let's a picture is a picture. Let's take this. So we will search for a picture, we'll give it a picture as the query and get search results. Now we can do the opposite. We can give it a text, you can say you can have a your documents, by the way, could be lots of photos. Now those photos can be photos of dogs, the animals, they can be nature, forests, beaches, people, any number of things, buildings, whatnot. So take a very large corpus of photos. In this particular case, we'll take 25,000 photos taken from Unsplash, which is a sort of a public hub for good photos that people put in their documents and so forth. I mean, they write articles with and so forth. So Unsplash, most of you are familiar, I suppose,plash to get free for royalty free photos so we'll take 25 000 photos of arbitrary of a mix of things and now we will say we will give it a text and we will say show me some sunset some beautiful sunsets on the beach sunset, some beautiful sunsets on the beach. And let's see if it can search and give you those images that match those, right? That will be another example, right? And we will, it's interesting, we'll give it very non-standard of questions to answer from within the images, and we will see how well it works. So today is going to be an interesting journey. And then we can go one step further. And I leave that because it's a very small extension. I'll leave that as a project for you guys to do today. And I'll be guiding you through. Suppose you give it a picture and you say, show me all pictures that are semantically similar to this picture. Now the picture may be two dogs playing together, right? The picture may be a human being expressing affection for a dog, right? A picture may be a car driving down a highway. And you have to use search technology, And you have to use search technology. You have to use exactly this natural language processing technology to understand the semantics of that picture. And from the semantics, match it to all of the pictures whose semantic, whose meaning, whose context matches the context of this picture. In this file, it should look something like this. The first thing you do is you go to the top where it says not trusted. Click on this. Just click on this so that it becomes trusted. This is a step zero. We must do that, make it into a trusted notebook. Now, those of you who are on Windows, on Linux or Mac, you can run the command as it is. If you're not, then remove the dot, I mean, remove the dot slash thing, just run trust.sh or rename the file to trust. This step, do it only if you're on a Unix variant. If you're on Windows, don't worry. All you have to do is for every file, you have to click from not trusted to trusted as we do this notebooks. After that, run this. So I'm going to do this along with you. The way you run the notebook is you keep clicking on this. And then you go and click on this, which is to install all the important root libraries that we need to install to get our environment going. So what it will do is, it will make sure that your Anaconda is completely up to date. If it is not, at least for the libraries that we need, it will make sure that they get updated. So I'm going to run it again. So I'm going to run it again. And for some reason, it tends to run a little slower on Windows, I don't understand why. On Linux it runs much faster. And if you have not installed Jupyter notebook extensions, it's a good idea to install the extensions. It gives you a lot of useful features. You can run that one also. Now, I'll just wait for this to run on this. So you notice that all of these libraries are getting installed. And finally, this also I can run and it will get installed. Once we have successfully run this, and guys, at this moment, I'm going at a relatively slow pace, but it is possible that some of you may get stuck. If you get stuck, raise your hand, speak up, reach out, like interrupt me here or reach out to our teaching assistants who are there in the audience right harini is here thank you harini for being here we have kyle is here right chanda also is here they can help you so you can do all of this after this the second thing we'll do is in the course of this workshop, not very essential today, we are also going to need some visualization libraries. So open the second Jupyter Notebook, which is all about visualization. And once again, just go and run it. Do you see this button here, run all? Just go run everything. And it is going to install all the visualization libraries on your machine. So this is the process of setting up the environment. It will just make sure that all the libraries. And finally, we will install all the NLP libraries that we need, natural language processing libraries that we need. So once again, open 00C and run all of it. When you run this, you will notice that we are installing a lot of things and by the way, these soon you'll become familiar with it. Torch stands for PyTorch, or torch vision we need because in our natural language processing, we are also looking for textual descriptions for images. Torch vision, torch audio, torch text, torch NLP, these are all PyTorch is a bridge between PyTorch and scikit-learn. Captum, spaCy, HuggingFace, Transformers. HuggingFace will actually install the HuggingFace hub. Transformers is the HuggingFace project. Sentence Transformer is what we'll use today. And there are many libraries. In the course of this workshop, we'll go through all of them. But for now, we just go and run all of them just as I'm going to do. So you can see on my machine, it is going through the installation. iTorch is installed now. It will go through and by the way ignore the warnings. Yeah. So, I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. So we go through this install and if you run through this 005, your environment should be completely set up the next thing that we need to do and i'll wait i mean we are coming upon a break i want you to go and look into the search corpus the jupiter notebook called search corpus so you know this is just a toy corpus of your creating of search of documents that just says to so what i did is from many of you are familiar with al Winsley and Into the Litten Glass. So I just took a famous poem, Jabberwocky from there, and I took some famous, you know, just some text. As you can imagine, these are the reason I'm giving long text is I wanted to illustrate the fact that even though we say we are going to do sentence embedding, these are really not sentences. These are entire paragraphs, like a points and things like that. So I want you to read through this. That's the only thing we'll do. And after that, we'll take a 10 minutes break. So our goal is to do the setup and then to read through the search corpus and run this run this entire file right and it this file doesn't do anything significant it just creates certain sentences and makes an array out of it this is the main thing it just creates an area out of it right with that i'll take a 10 minutes break and we'll regroup at 11.55. Any questions, guys? So far, any questions before we take a break? Anybody who's stuck or anybody who needs help? Hopefully, this is all elementary, guys. We all seem good. Yeah. Kashish, are you doing well? Are you able to do the setup? are you able to do the setup you're on mute all right shalini how about you are you making progress dilip is it working on your machine? Dilip, is it working on your machine? Dilip, is it working on your machine? Dilip, is it working on your machine? Dilip, is it working on your machine? Dilip, is it working on your machine? Dilip, is it working on your machine? Try a microphone like I had said. How about you Laila? Are you able to make progress? I'm not getting responses from anyone. Kashish has responded. Go ahead, Kyle. She posted a comment. Kashish has responded. She's saying, yes, all is working on my end. Okay, everything is working. Excellent. Nice. Shloka, I hope things are working for you too. Okay, so I'll see you guys in 10 minutes. I'll pause the recording. And then we'll get into the semantic search. All right. I'll just repeat for the benefit of recording. We are now going to get into the actual project. We will start with the toy corpus of small documents that i created and i deliberately introduce a lot of variety in the documents now while we go through this your project will be to pick your own meaningful domain like your health care your energy your forestry whatever it is pick a genuine domain form study groups form teams a genuine domain, find a large corpus of text and images, pictures, and create a search engine built on top of it. And I'll give you the instructions in a little while. But let's look at the toy example. So we will take the principle of crawling, walking and then running. So let's start crawling and walking first. We'll take a small toy car. This is a poem that is very famous, Jabberwocky, you may be familiar with it, but if you're not, well, as you can see, none of the sentences make sense. And yet, overall, it sounds as though it makes sense. So I took something that was deliberately hard for a semantic search engine and as you can imagine that for a keyboard search engine this sort of text would be an absolute disaster right then i took another famous phrase this is the beginning paragraph of the tale of two cities by charles dickens it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness and so forth he's describing the times of the french revolution long ago well it could easily be the description of today could easily be the description of today. Another thing, another favorite quote for me from the tale of two cities is this one a wonderful factor reflect upon that every human creature is constituted to be that profound secret and mystery to every other. So forth. Another of that tale of two cities, I'll let you read it, then a little bit about dogs. A dog is a gentleman. I hope to go to this heaven, not man's. To his heaven, not man's. Mark Trace. Those of you who have dogs would resonate with that. So I'm trying to make them as different from each other as possible. This is another quote from Alice in Wonderland. Or no, I think it's from The Looking Glass, Twiddle Dee and Twiddle Dum. Then a few more quotes on dogs in particular. Some on golden retrievers. You can read this quote. Then a quote on beauty by Keats, a poem on beauty by Keats. So this is the corpus. Then for good measure, I did a technical thing. I took the abstract from the Attention is All You Need paper. This is the preface of the abstract of that research paper. Then another one which has to do with this is the Wikipedia description of back propagation. So as you can see, and then this is Worthworth's beautiful poem on Lucy. Some of you may know about it and another poem by Davis. So hopefully a pretty wide variety of texts and we are going to search through this. So let's now go into the real parts of it and go into the semantic search. The first lesson today. So we're going to do semantic search. Remember, let us recap it. What do we want semantic search to do? We want human beings to describe something, a paragraph that gives the search intention. What is it that they are searching for? They can describe it in words, not keywords. We are not good with keywords. Then when they give that sort of the AI magic should happen, the NLP magic should happen. And that's the technical aspect we'll go into so the results that come back we hope that it has two at least two good properties first of all there's a prior assumption the prior assumption is the corpus of documents the library of text that you have does actually contain an answer to the query does contain something generally relevant to the query? So that is, of course, relevant. You can't have a text, a corpus of poetry and then ask it to solve to give a query that says, explain to me Einstein's theory of relativity. That wouldn't make sense. So with a good faith assumption that the answer is there, now what we expect the search results to have are two key properties very good precision and very good recall precision means all the search results should be relevant and recall means the most relevant results should be present in the search the most relevant documents should be present in the search results that's what we want now again to recap that here the way we do that is we convert everything into sentence embeddings and then we do a search in the embedding space in the n-dimensional d-dimensional embedding space we use two similarity measures dot similarity and cosine dot similarity tends to return longer results cosine similarity tends to return smaller results so now for the code guys please do along with me because you would need to do so we will do the lab and the project the lab is the guided part the code is there for you the project is what you will do amongst yourself while you're still here. So we need a sentence transformer, a kind of language model, a kind of a transformer called the sentence transformer. And we will go into the theory of this later, but just do that. Now, remember I said that there's symmetric and asymmetric search. We will be doing asymmetric search because you notice that the texts are pretty long. These are pretty long bits of text. And we don't need. So in semantic search, I want you folks to all run this unit and make sure that it runs on the machine. Can we all please do that? Once you do that, now we will run the corpus, the search corpus. All it does is it creates this sentence array or a list of sentences. And the first thing we do is we take a model this is a model that we created the transformer model which is our ai model or nlp model and by the way these transformers are chat gpt and all of these are transformers so now you are getting into that world of transformers let us go create the sentence transformer. When you do that, you will use the sentence transformer to convert this list of text sentences into embeddings. So run this and when you look at the shape of the embedding system, you will see that, let me run all of this along with you. And converting our sentences into embedding is an influential process that may take a little bit of time based on how powerful the hardware you have or don't have. When you look at its shape, it says there are 16 rows and each of the row is a 768 dimensional vector. So the embedding space is 768 dimensional. Why are there 16 rows? Because we have, if you recall, we had 16 sentences or 16 pieces of text documents here there are 16 documents that we are indexing right so that explains that we can choose to just see what any one of the document looks like let's look at the first document so this is the first document. Let me just do it like this. The first document is this poem, Jabberwocky poem. And what does the embedding look like? Embedding is a tensor. It is a vector of 768 dimensions. As you can imagine, it's just a bunch of numbers. These are all fractional numbers. right? These are all fractions. So obviously, you can't read these numbers and make anything out of it. It's an abstract representation in a n-dimensional space, d-dimensional space, where d is 768, right? So this text has now gotten converted into vector. The moment it gets converted into vector, now life is easy. All you have to do is bring the rest of the machinery of machine learning, right, or AI. You know how to find nearest neighbors, etc, etc. And you can do that. So then let us search for something. I'm deliberately taking a sentence, which is hard to get in this word. I'm saying a friendship with animals. And one of the exercises you'll do is try out different queries. Let's do this. First thing we do is we take the query text and convert it into a vector in itself. This is the query vector. And then we search for the query vector. And we say, just give us the top three results. You know, we have 16 data items. So the results are not like, obviously, three is already overkill. We are getting one fourth of the results. Let's see what the three results look like. When we run this, you will notice that the three results look like when we run this you will notice that the three results it comes out with now look at this we deliberately took a word that is not there in the search results do you notice that the word friendship or animals, as far as I can make out, is not present in the text? Do please remark that these keywords, the friendship with animals, animal is in this result paragraph. And yet it is talking about companionship, and so on and so forth. The second search result is that if you are lucky, a golden retriever will come into your life, steal your heart and change everything. Would you agree that this too captures the spirit of friendship with animals? I don't know if you are, but I'm currently impressed how well semantic search works. And likewise, the next one actually does have the word animal. And so this is very interesting. The last one is from Einstein. It says if a man aspires towards a righteous life, his first act of abstinence is from injury to animals. His first act of abstinence is from injury to animals. So obviously injury to animals is the antithesis of friendship with animals, right? But it contains the word animals. Maybe in a just keyword search, this word would have showed up. This result would have showed up ahead of other results. Right? And so guys, I want you to do the following. Take a few, during the lunch break, try out different sentences, create your own query terms or query sentences, and see whether it's able to answer it or not, and how well it answers.. Remember of course, that we have only 16 pieces of text. So the other thing you could do during the break is add more pieces of text to the corpus. Or train the model, create a new search index and then search for things in it and see how well it is able to do that. Right? To reveal to you a secret. This is actually only half the semantic search. There is a way to make it more powerful. And that way is using a cross encoder. There's another transformer architecture which will apply to the search results to re-rank the search results and make them even more relevant. Right. So this is in a way what I'm taking you through is half the solution. And I hope that even at half the solution, it feels good. Let's try visual search now. So, guys guys I hope you are doing along with me otherwise you won't be able to do the project these are easy things anyone who can raise hand and say yes it's working on their machine I would appreciate it Thank You Albert anyone else Jaishankar thanks Jaishankar so it is working on your machines good now comes Naishani. So now let's do something really interesting. See images when we take photos, we take photos because they have semantic meaning. Most of us don't take photos of random noise. Right. The reason we take a picture of a dog because a dog means something to us, right? It is interesting. So we take this dog and we ask this question. Can we search by image in the same corpus of documents? We did have a corpus of documents and in our documents, in our text that we created, occasionally we did refer to dogs. There were a few references to golden retrievers and so forth. So we are now going to try and do that. So what do we need to do? So long as we can create a sentence embedding in Corset of this image, So, the distance embedding in Croisset of this image, we can do it. And that brings us to this very landmark work that this was a paper from Okuniai meaningful embedding vector out of an image that captures the meaning, the semantics of the image. So we take clip and we convert this, use the sentence transformer, exactly the same code, all we have done, guys. And you see how elegant it is, all I have done, if you look at this code, it is nothing but the previous code, no change. The other thing I'm doing is I'm not searching through the original texts, I'm searching actually through short text, the reason for that is, there's a technical reason for that, A clip was trained on very short texts, right? So at this moment, the current limitation of clip is you can't give it huge texts. It sort of fails, right? And there are ways to fix that, but we won't get into that for the time being. So we will just look into just some short texts. We'll take this picture and we will embed the text. Again, we'll create a search index out of the short text. Here, we have just taken four texts, obviously. That is too little. And I invite you to add more and more texts, but I hope there are enough for text, many texts. One, two, three, four, five, six, seven, eight, eight pieces of text. A smiling dog, a house five, six, seven, eight, eight pieces of text. A smiling dog, a house with a chimney, car on a highway, elephant in a field. Attention is all you need. Golden retrievers are little children, a cat on the windowsill for the love of these furry dogs. And we are now going to search exactly the same thing. We're going to take this image convert it into a query vector and search for the query vector against this text and let's get the top three results and what do you think guys looking at these results i will let you decide if these three results look relevant to this picture anyone would like to make a comment absolutely absolutely isn't it and guys if i may tell you none of these things were possible just a few years ago none of it was possible. This landmark things started and got really popularized. There were efforts before that, but it really took off after the coming of the CLIP paper, research paper. We are going to do the CLIP architecture in detail in this course. Now, there is a bit of information propagation asymmetry here. Most of the world has not woken up to it. Like I just told you that in Cornerstone, I used AI search and to a customer base of close to 100 million users and its 7000 clients, suddenly it meant a lot. They felt as though we have done some magic. You know, something very profound has been done. Well, if you know AI, you know that, well, it is profound and it is not. It is profound because it is profound. But I was simply standing on the shoulder of giants. The amount of code that I had to write to pull this off was fairly nominal. Obviously, the engineering aspects, it isn't as simple as this. I had to write to pull this off was fairly nominal. Obviously there are engineering aspects, it isn't as simple as this. I had to, there's more to it but the gist of it is this. Now we'll do the opposite. What if you have a lot of images, large corpus of images, which you'll get from Unsplash. Unsplash is a website from which you can get lots of images free. They are royalty free. And people post a lot of images there. So let's take 25,000 images of all sorts of things, right? The landscapes, the forest, the things, right? There are landscapes, there are forests, there are things, there are animals and whatnot. Everything is there. In that landscape, in that sort of thing, what we will do is we are going to index each of these photos. We are going to create sentence embedding of these photos, right, and create a search index out of it as simple as that so first part just downloads those images into a local directory this is where you need bandwidth so when i give you the zip file fortunately all the images are downloaded right so you should run through this pretty fast right instantly you should go through now we are going to create the search index. What do we do? We take a transformer that will create sentence embeddings out of each of those images. Again, using the Clip VIT transformer. By the way, all of these words, these are landmark transformers, Clip architecture, these words these are landmark transformers clip architecture a visual transformer the vit these are all great things in this field and we will break it down and study it properly now you have all of these images and what do we do we create image embedding out of all of these all of these images by the way do you notice that i tend to convert things to tensor convert to tensor could somebody hazard a guess why i do that would someone like to guess why we do that? Well, the reason we do that is because see tensors, we can put onto the GPUs, which have like you know, if you make it a pie torch, or TensorFlow, but in this particular case, PyTorch. PyTorch is pretty dominant in the research community. So converted to PyTorch, TensorFlow, PyTorch code can be moved from your laptop or your computer to the GPU, the graphic processing unit, which is a massively parallel processing engine that you may or may not have in your workstation, or you have it in the cloud instances and the servers there. And so, both your training and your inference will run much, much faster, thousands of times faster, or hundreds of times at least faster than they would run on your CPU. So much of the transformer work is done on video cards or tensor processing units and graphic processing units. So that is why I tend to do that. And one of the exercises I'll have is when you do a large corpus, I would encourage you to do it on the on the gpu on the qda enabled gpu so we are going to do an interesting experiment now we are going to give give the obviously remember in the previous one we said give the given this image find some text that is appropriate to this image and given this dog golden retrievers are little children for the love of these furry dogs a smiling dog these were the relevant results that came up now we'll do the opposite we will give it text and ask it to find the relevant images in a corpus of images so here it is image search with text. Very simple code, just the opposite. And I deliberately took something that is hard. You know, love is an emotion. I said to love a dog. Well, if you just say dog, it will find dog. If you say man and dog, it will find man and dog. But to throw an emotion that is semantically read you know it's hard to grasp we are talking about an emotion to love an animal i just took the first three results would you how many of you feel that the emotion of love is involved in these pictures they're all about dogs and there's a emotion love as an emotion is involved dog owners would immediately identify with it i suppose how many of you have dogs, by the way? Raise your hand. No, nobody has dogs. Maybe I should have taken different examples. No, it conveys the message, yes. I mean, it matches the search query, yes. It does match the search query yes it does match the search query exactly it's very very interesting so see i took a fairly difficult pretty difficult semantically difficult text or query and did that right and when you do that it comes up with this what happens if i i don't know if i run these elements or not let me run these elements to be sure right let's let's change it to something else let's uh put something else would somebody like to search suggest something else search for anyone else search for anyone give me a query query text just put a dog a dog or something just a dog but that is too easy right let's increase yeah do you see that dog and we can do more more, let's say 10. There are 10 dogs, hopefully. Would you agree that these are all dogs? Yes. But give a phrase that has more semantics to it. a phrase that has more semantics to it. If you do dog or dog and cat, how many of you are impressed that it can do things like this? Well, this isn't quite it, but I suppose it's somehow thought it's a dog and cat. It's also bringing in some of this. So do you notice that some of these results are not quite? And the next stage I will tell you is when we talk about the cross encoder, which will re-rank the results based on relevance much better. You'll see a better quality of results. But even with half a semantic search, it's still doing pretty well. Somebody would like to give me another text. How snare a lake? A dog with blue eyes. Oh, see, I said how snare a lake? Wow. Anyone impressed? Yes. I, I searched for an angry dog, and angry dog, and someone else said a blue eyed dog right let's do that. Angry dog. I don't know if they have pictures ofeyed dog. Let's do that. Angry dog. I don't know if there are pictures of angry dogs. Yes, it is. You can search that. There are three items. There you go, angry dog. Does it look like that? Yes. What happens if you say hungry dog? My dog is always hungry. No. Yeah, you can see the tongue coming out. Yeah. Or jumping dog. I don't know guys, do you see that? Asif sir, how it is able to find angry dog or jumping dog? it a metadata behind the scenes yeah so that is the clip architecture i would say that hold the detailed discussion uh in your mind because i will go over clip in detail but basically what people did how so the question is how did you train this transformer form are what the researchers for the clip paper did they happen to notice that on the internet and they basically call it unsupervised learning and so forth because these days are zero-shot learning very common for people to use such words but actually the idea was very simple they happen to notice that whenever you create an image on the internet, people tend to give captions, isn't it? Like, for example, if you notice my code here, see, I put this picture here. Right? If you go and look at the code, see what I have done. This is the HTML code. Figure. Right. Source, the image picture, picture image. And there is a caption. You see the caption here, a golden retriever image from Wikipedia. Right. So sometimes you have captions, sometimes you don't. captions sometimes you don't but what they did is they went and scraped the web and gathered hundreds of millions of pictures which did have captions once they got the captions they sort of streamlined the captions into a finite set of categories so that they could write a classifier and then what they did is they trained the transformer so when you do the transformer you will realize that what you can do is, especially with things like BERT, you give pairs of inputs. So one input is the picture encoded. The other is the text encoded. And you feed that into the transformer. And what you do is you give it positive and negative samples, right? So a dog with a caption that is actually meaningful should have a high probability output. And a dog with a caption that is junk should have a low probability output. So you train that transformer. That's how Clip was trained. And so that is the transfer. And therefore, now, when it sees an image, it knows which appropriate caption to pick up. Right. And the converse is also true. Now, if you can do, therefore, you have figured out a way. If you can pair an image with the most appropriate caption, all you do is you can you can therefore interpolate the sentence embedding for this particular image from its captions in effect. You see how it goes guys? Joshinka, does that answer your question? That explains the question sir uh the answer the question was from somebody else oh from somebody else okay i uh who who asked that question i don't know if you could say did it answer your question or not yeah yes yes thank you yes nice friend sorry i didn't recognize your voice so now we'll do the opposite. Yeah, so this is it. You can do, so guys play with this, you know, house near a lake and it comes out with 10 images of house near a lake. Now, as I said, I haven't done the whole thing. There are ways to make it better by using a cross encoder and we'll do it. I still call it a Harper solution. Jumping jaw. Now you can give me semantically relevant things instead of just saying forest, which would be easy. You're seeing pathways through the forest. And would you agree that these are pathways to the forest right so guys now i will there is a homework for you to do in the next two hours so here's the plan we'll take a lunch break it's going to be it's almost one o'clock it's 12 45 we'll take a lunch break till two o'clock and then what we do is actually till 2 30 so you have approximately a little less than two hours. I would like you to form little teams. So today is the first day, guys. Form your steady group. Form your project group. All right. And now two things you have to do. The first part is I want image to image search. Given a picture of a dog, you need to find all of the pictures of dogs that are like that. Semantically, whatever this picture is showing semantically, the other picture should also be showing. If I show you a path through a forest, then other pictures in the search user should be of pathways through the forest. And let me give you a hint. The answer is there in this notebook itself. You just have to patch things together and do it. So please do that. Right? That is homework number one. And homework number two, let me just mark it. Actually, I just marked this as a homework. The second homework is, which is implicit and I didn't state it, is, see, I created a toy corpus and searched through it. I want you to do a project. This is not a homework, but a project that you should do through the week. The project is this. Take a meaningful corpus in your field, whether these are images, whether they are audio files, audio clips, or whether they are audio files audio clips or whether they are text or their mixture of all of them right and create a little search engine first do it in jupyter and then use streamlit to create a simple application that does it this is streamlit i will put stream it has a zero learning curve. You can be going in 10 minutes. So as the demo is, my first app, hello, and right away it works. It is the data scientist dream tool to just prototype things. So create a Streamlit project with a Streamlit UI that gives a meaningful search engine to whatever domain that you guys choose as a team. your screen now. This one. So basically, like when we looked at it, we saw that here, it says first we encode the query, we can use the image or text. So we have that the search function takes both image or text. So then what we did was basically we added the path to the image. And then it comes up with the pictures. Very good. That's all we had to do. So easy homework. That's what we thought like it can be hard, it cannot be hard. Yeah. Junaid, is that the way you also did it um i'm modifying the search why don't you share yours with that share your screen yeah So guys, do you notice how easy it is with these transformers to do things that were not even possible a few years ago, just recently, not even possible. And you have to ask yourself, why are people still using the old technology? Right? 99.99% of the people are using old technology. So with this, you can go into your corp, you can say, hey, let me do it for you. Let me do a new generation search and make it more effective. Because search is something no one can live without. Right? So, Junaid, are you sharing your screen now yeah yeah so a lot of the search architects yeah what happens to them see what has happened is elastic search tried to put this into their database and so they like they have a problem because all of this search technology and then this so they added it as a feature and they still hope that people use the rest of their technology, but it has significantly diluted their value for sure. But the thing is, the problem with that is, and I'll talk about that problem in a little while in the next session, it doesn't perform well, because it does an order end search, because it compares the query against every single section. So, oh yes. So I just added this text file parameter called encoded, and based on that, here in search, I just pass that to true. Yes. And then that tells us it's an image it's already encoded yeah yeah perfect perfect so you're searching for dogs and you get more dogs yeah excellent use streamlit to create a ui that's a minimum bar let me just say that just as this homework was easy creating a ui using streamlit is also very easy then the input will be a query or a image make it either a query or an image and the output should be text images both because given a context there would be text that would match it, and there would be images that would match it. So produce both of the results. Are we together? So we use template as in for the UI and then what? And then use the code that you have modified and now get a domain data, pick a domain, your healthcare domain, Riddle just brought up about logs, wireless logs, Wi Fi logs, and I mean, so on and so forth, and create a search engine that helps you search through the logs in a semantically meaningful way. It can search through both images and text. And here is a bonus thing for you to do because this problem actually, once you get used to it, it will feel easy. The next thing to do is to search through audio data, the audio clips, right? Now I'll give you a hint of where this is very useful. Sometimes machinery failure can be detected semantically or classified or clustered by listening to the audio forms, you know, the sound of that machinery, the hums of the machinery. Right. Junaid, you have experience with CNC, you know what I'm saying? And so you can classify literally, groups of problems, or can you can say has this problem been solved you can look at a solution knowledge base by simply feeding it a waveform how do you like train it so let's say like if you're training for like security log data or healthcare how do you train it oh exactly that the way we trained it dot encode that is right and then there is more to it see if you want to if you have a vast amount of data for a particular domain, the sentence transformer, you can fine tune it. The part that I didn't explain is the fine tuning part. Now today, I gave only part one of semantic search, there is a part two that makes it even more relevant, which is to bring in the cross encoders. Right. And I will talk about it at a later date. And that is an optional topic. I will talk about it. We'll go over three concepts. One is attention. One is self-attention. And the other is transformer. Architecture. The original one. Transformer. So we learn about three things. So before I explain that, I will explain to you what used to exist before these things came out. So a classic case that is taken is that of a translation, translate something from English to French or Hindi or something like that. So how would you translate from one language to another? What would happen is, look, consider the translation problem. What happens is you have a text that is English. You want it to go into a box, translator, and you want, let's say, French or Hindi or something else to come out. Does that make sense? Right. Now, so suppose I say the cow jumped over the moon. The cow jumped over the moon, I say. This sentence, if you do word-to-word translation into French, it would not be appropriate, or to Hindi, it would not be appropriate, because there are multiple considerations. The same sentence or the same text in another language will have the order of the words would be different. Not only will the order of the words would be different. Not only will the order of the words be different, it may use fewer words. You often see that English is a very succinct language. When you translate it into French, sometimes the text is longer. And in some languages, the text is even short. Languages like German or Sanskrit, which allow for so-called conjunction of words, you can often say a lot with just one word because that word, it filters the whole sentence. So that can be problematic. So the way to do that is, and the way we think about it in architecture, is this. We say that, see, you take English, you first use two boxes. You use an encoder to create a hidden representation which you feed into a decoder. So what happens is that you take English text, goes into the encoder, and it becomes a mathematical representation of what was said. That mathematical text you decode into another language and a French or Hindi or whatever can then come out of the hidden representation. In fact, you can decode it into multiple languages if you so wish. Right. Now, this is the classic architecture and these things that can do that. These are often called language models, because what do they do? They have a way of converting a language text into a hidden representation, isn't it? And out of the hidden representation, they can do French or whatever. Now, while translation looks like an obvious thing, there can be more to it. So for example, if you're doing text summarization, pretty long, verbose piece of text. If you can create a good mathematical representation, you can decode it into much more succinct text, isn't it? Or you can do another way. You can rephrase it, or you could rewrite it as poetry, isn't it? As a haiku, as poetry, as sonnet, or whatever it is. there are multiple ways that you can decode it if you could generate a meaningful hidden representation. So what used to exist before this whole revolution of transformers is that the king of the hill used to be architectures called recurrent neural networks. So the way the recurrent neural networks would work is that they would, and recurrent neural networks and their more complex cousins used to be called LSTM, GRU, etc. They are still useful, but they were not as effective for language models what they would do is you would feed it a word word w1 it would produce a output output one and it would produce also a hidden representation then the next word that you gave Then the next word that you gave to the same, but you write it as a separate box, w2, it would produce output 2. And what would happen is it needed two inputs. It needed w1, w2, the second word, and the hidden representation of the previous word and so it would continue so that ultimately you would come up with a final hidden representation at the last word final which would maybe the full stop or something at which you know that you have to stop right it would do that now what happens is that this final hidden representation is what you would feed in. This would be your encoder. This would be your hidden representation. This would feed into the decoder. The way it would work is decoder, you would just give it a start token. You say, start decoding. take the hidden representation and emit out the first word right then you can either use the first word to feed into the or a hidden representation to feed into the second word representation to feed into the second word, then what happens is that you would take the output and you would take the hidden representation to generate the second word, right? And you would keep on going like this till the output was a stop or end of output token in which case you would stop so you would go on producing word after word after word and it seemed to work the problem with this was because you create a hidden representation and the hidden representation has a finite memory the bigger the text you feed into it the less it is able to represent with this what was meant. The other problem with RNNs was you're passing it one word at a time. So you cannot do parallel processing. The performance of this RNN could not benefit from all the video cards and things like that. Are we getting it, guys? The other main problem with thing is the words that came in the end. So suppose you say a cow jumped over the moon. You would feed it in the reverse order. The cow jumped over the moon. So this word would go last. These were the early words. Now, if the sentence was long, so suppose I said the cow, early words. Now if the sentence was long, so suppose I said the cow, which was really well fed, right, and of gray color, and had big horns, jumped happily, right, and with a mighty leap over the blue moon. So by the time you come to the blue moon, you forgot what exactly jumped, right? Because of the memory. Yeah, the memory. So that thing begins to fade out and the more recent things begin to overwhelm. They tend to be better represented in the encoded representation, in the hidden representation. And that was quite a nuisance. It would begin to fade out, right? That was the main representation. And that was quite a nuisance. It would begin to fade out. That was the main problem. So with these three problems we identified, one is that long passages, you begin to forget the early part of the passage. The second problem is you're not optimizing on any hardware accelerations. The third problem was that you have finite memory and from the finite memory you're trying to decode, right? So people for the longest time were thinking, what do we do about it? So then they came out with a combination. They said, if we attack, there was a discovery of a method called attention. Attention actually was discovered pretty much the same time when ResNet, et cetera, were making a revolution in computer vision. Attention was discovered in 2014 or invented, if you say, in 2014. The paper is an old one. And what they found is if you hybridize, if you add attention to RNNs or LSTMs, et cetera, then it solves some of the problems. And then people began to add attention to these networks. We'll talk about what attention is. And then came a bigger step, the 2017 landmark paper came from Google, Waswani et al., which said that actually, you just need the attention is not an additive that makes RNNs, LSTMs, etc. work better. Attention is all you need. You can forget about RNNs and LSTMs altogether. It is a thought that had not occurred to people because we were also sort of indoctrinated into the idea that if you want to do natural language processing, then you have to use this autoregressive RNN, LSTM, et cetera models. You cannot do without them. And attention just makes it better. But this paper said attention is all you need. That's why the title of the paper is attention is all you need and so we will now learn about what attention is but before i do that i want to ask you uh any questions so far right so these models are often often also called sequence to sequence models. Why are they called sequence to sequence? Because a sequence is an input and a sequence is an output, isn't it? So sequences could be words, they could be time series data and so on and so forth for prediction, whatnot. These are sequence to sequence models. Now, let's go thinking through it. And I will sort of explain it from first principles. I'll explain first what is attention. hidden representation no it is part of the decoder the decoder is basically made up of a unit in which you give it the hidden representation and you say start so it will emit out the first output now you take the first word that it produced in french along with the this unit's hidden state both of them as input and you say now emit out the second word it will do that and it will keep doing that till it emits out the word end end of sentence the moment it emits out end of sentence you you say okay i'm done yeah done. Yeah. So, transformer also has a given representation concept. Yes, it does have. And now that it also has RME also has a given representation. Yes. That's a little more sophisticated. Yeah. The sophistication is there is no size limitation effectively there and we'll come to that. Okay. And the whole problem that the recent words overwhelm the, or make the previous words being forgotten, all of those problems are not there. So I'm going to come to that systematically. So like let's say in the RLN, you increase the memory. So if you had, for example, like unlimited memory, then would it be okay or? Actually people have tried that, it doesn't work. So inherently and this there's a whole see it goes down to the foundations of computer science. What happens is that these things are not trying to remember what you said, they're trying to learn a representation because they're trying to learn a representation no matter what you trying to learn a representation, no matter what you do, they will be much more influenced by recent words than by previous words. And that is the main problem. So what you need to do is you need to prevent this sequentiality that from happening, forget images, suppose I were to give you this, i give you an arbitrary series of numbers i'm not giving them as images literally telling you this is what it is zero one three seven nine whatever and i ask you this problem tell me which is the biggest right and so i've given you label data given a sequence the output is the number which is the biggest in the sequence. But the machine doesn't understand the concept of the biggest. All it knows is given this sequence, this is the biggest. And now you have given it enough training data, enough random samples of numbers and pick the biggest. And now you want to test whether this machine has learned to pick the biggest in a sequence of numbers. And your sequence can be of arbitrary length, arbitrary token, just like language can be, a sentence can have arbitrary many words. Can you train a network to do that? Right? So you can try that actually. It's an interesting experiment you would you would you'll be surprised that a fully connected network layers gets a certain degree of accuracy maybe in the 90s or something it will get you there partly pretty well but the moment you use attention mechanism it becomes amazingly effective right of the bat for some such a simple problem as that because you have reformulated the problem as instead of pointing to and classifying into zero to nine or something you have said turn inwards and in the input tell me which number i should focus on like you're holding a torch and you're shining it on the biggest number you're saying this network needs to turn the torch or the attention onto the biggest number. And that's what it has to learn to do. And there is a way to do that. We will learn now how to do that. It becomes amazingly effective. Now complicate it a little bit more. You don't give it the answers. You don't give it the numbers. But you give it handwritten numbers. MNIST data, but you give it a set of arbitrary set of numbers, two, three, five, seven, 10, whatever, or six, eight numbers, images, and they are all inputs. And now you say, point, tell me which is the biggest. So what does it need to do? It needs to somehow shift through the images, or pay attention to the biggest. So what does it need to do? It needs to somehow shift through the images, pay attention to the biggest one, and then infer what it is, what digit it is, right? And with attention, once again, the leap in accuracy is enormous because you're not just doing an arbitrary classification problem. You're doing a much more focused problem, right? You're seeing the answer is right there. You're broken the problem into two steps. First, pay attention to the biggest, the image that represents the biggest digit. And then all you have to do is tell what digit it is. Am I making sense? Right? So that is attention. Now let's figure out how do we do attention, right? So context is the underpinning, the foundational root of attention, right? And differential scores or weights, and I'll use the word scores because the word weight is used in neural networks to mean the weights associated with the network parameters themselves so i'll just use the word differential scores to different items or tokens let's call them tokens right so when you say uh when you say and it I bought this printer it had this feature that feature model number was that I bought it on this day but when I put paper into the into the tray it did not fit all the things don't matter what matters is is this. If you're doing sentiment analysis, did not fit is where you want to pay attention to. Isn't it? So those words have far more score than all the other things that I bought it on this day and so on and so forth. Isn't it? So that is differential score and context, two big ideas. Let me name these things. Now, with that, let's make progress. So I'll start with some tokens. Let's say that you have certain tokens, X1, think of them as words, the cow jumped over the moon, or just numbers, so Xn. Or just numbers, so XN. Or, you know, just before we get into words, just think they could be just numbers and you're trying to find out the biggest number or something like that. A simple example, don't you? Now, we all know what a neural network is just a bunch of neurons all connected to each other. So layer after layer. And every neuron in each layer is connected to every neuron in the next layer, but not within the layers. Right. So this is your neural network. I'm just writing. You can throw in a few layers. Let's say that there are three layers or whatever number of layers you want. In each layer, you can throw in as many neurons or nodes as you want and then move forward. Now, neural networks in simple terms are, and I'll say what a neural network is for just a recap. Now, many of you have taken my neural network course or you have learned it somewhere else and so forth. And you hear about the implementation detail that you put neurons here then you connected that oh in layer one i had six neurons in layer two i had eight neurons here i had 16 neurons don't forget that neural networks conceptually especially connected networks they are what i call universal approximators universal approximators. These are universal approximators. What does that mean? You take any function, any arbitrary function, let us say y response output or output y is equal to some function of x like this this is y this is x and let's say that the value goes like this wow pretty complicated function right it certainly is not a straight line isn't it and god knows what it is not a straight line isn't it and god knows what it is but the thing with a neural network is that if you train it with enough data enough points with enough points data points x i y i means arbitrary x 1 x y 1 etc give it in a table of x and y's two columns sufficiently large table of x and y's you train it it will come up with a curve that will sort of be a pretty close approximation of this are we together it may not exactly get to it, but with sufficient data, it may even exactly get to it, right? But it will come closer and closer and closer the more data you feed into it. Assuming that you have a sufficiently rich network, it will pretty much figure it out, right? And when it figures it out, you're saying that it has approximated. See, And when it figures it out, you're saying that it has approximated. See, when data came in from X to Y, that data came from the real world. There was some underlying force producing it, some generative effect, some phenomenon that this data represents. Either a financial phenomenon, a biological phenomenon, a weather phenomenon, physical machinery phenomenon. Something is producing this data behind that phenomenon is a generative force some causation you don't need to know that but what a universal approximator will do is it will infer a relationship between input and response that is as good as it gets may not be perfect but it is as good as it gets and it will serve real world purposes so that is what uh your neural network is now what a neural network does and this is now i need you guys to have a slight shift of thinking think like this the way i'm going to now tell you, we think of a neural network producing inputs and producing output, isn't it? So from here, typically, I'm saying, think that it will produce a hidden state, H1, H2, H, let us say, H2, H, let us say, K dimensional. So they're K nodes, right? It will produce K outputs, right? Because this layer, I mean, at the end of it, there were K nodes. So what it has done is the last layer, whatever number of nodes you have, let's say you have k nodes in the last layer they will all light up they'll produce some voltages think of if you think of it in an electrical engineering way they're producing certain voltages or certain energies but people in this literature often call them certain energies or largest and so forth they're producing so each of them will produce its own h1 h2 hk output so you can look at this entire thing as a h vector which is made up of which is thinking of it as h2 hk this is the general way of thinking about it now if k is equal to 1 then the hidden state is just a number isn't it if you have only one neuron at the end is just a number right so that it? If you have only one neuron at the end, it's just a number, right? So that number you could use for regression. For example, predicting if you make a model between a temperature at a beach and the amount of ice cream that will sell, you can use response as the amount of ice cream that you project it will sell on the beach to children as a function of their temperature right that H would be your response and that the final year H you can possibly it can be the output you're looking for in a regression problem isn't it in a modeling problem but in general you should just think of it as a latent representation. Latent representation. Representation. But latent, whenever you go from a pretty complex representation to a hidden representation and the system has learned it to do whatever work it is, the underlying assumption is the hidden representation is the hidden representation or the latent representation and or hidden let me put the word hidden also before i forget hidden latent representation that it is somehow richer more meaningful representation are we together right so i'll give you an example suppose so here is one very physically intuitive example suppose in the center of this room i put a light bulb right and let's say it's dark Then the rest of the lights are off. Then I measure the light illumination at different points in the room. So at each point in the room, what do I have? The coordinates, X, Y, Z coordinates, isn't it? If I put a coordinate system, there's a X, Y, Z coordinate. So I'll have a X value, X, z value and then i would have the light intensity value the response suppose i were to model the output as a function of the input you would realize that actually these inputs x y z don't matter what matters is a straight line distance to the source of light, isn't it? So a better representation is somehow from these three, figure out how distant it is from some central hidden point that you have not been told about, right? And if it could extract that representation, it is well on its way to solving your problem. The next step would be maybe take the reciprocal square of that thing, one over d squared and so forth. It gets to the physics intuition. But you realize that the most crucial thing is distance to the source of light, isn't it? And that is a much better representation to solve this problem. And that is what neural networks do. When you put many layers together, they come up with a hidden representation that somehow is more meaningful to the problem that you're trying to solve. So let's go with that. So that is why hidden representations are important. And that is why deep learning is often called representation learning, because the system is trying to learn better representations of the reality than is given just by the input data. Go ahead, Albert. better representations of the reality than is given just by the input data right go ahead it could be less or more it doesn't matter usually it is less right so for example auto encoders they have a hidden representation that's smaller but sometimes you can have over specified representations also right you do that so so now so now what we do is we have made a journey. Let me again recap what we learned. We made a journey this far that you give it the inputs and a hidden representation comes out. And now comes the part that I'm going to bring. Suppose you attach another network whose job is to somehow. So let's bring our input here. Another network whose job is to somehow, so let's bring our input here. Let me use another color to summarize because we are changing. We are adding one more new technology to the mix. What color will suit? This could be a good one. So let me go back and say X1. No, it's the same color. Let me use a different color. Yeah. X1 to Xn. It went into the first network. It produced the hidden state. Hk, right? So actually let me write it out in its full form. H1, H2, H3, H4. It produced this. Right? Or just to be sure, okay, let's do one thing. Let's keep matters simple. Let us say that it literally produced N. Suppose there were N tokens, it produced N hidden representations. We keep it exactly the same. Let's keep it exactly the same for simplicity then what you do is you create another neural network right now this this is a universal approximator so we can write it like that that h1 is some function of x like h1 h2 they are all some functions of the x vector input right or in vector language you would say that h is a some function some some the network has approximated some function here we will create a scoring function that somehow takes your H and produces a score out of it. So this goes in and a score comes out, S1, SN, N scores. Are we together? And now comes an interesting thought. Suppose I were to convert see you know any arbitrary bunch of numbers you can apply a function you can represent it as between zero and one right and you can do it in such a way that the big numbers get really amplified exponentially bigger like let's take a so one such function is I'll give you an example what happens if you do e to the x so think about it e to the one is 2.7 what is e to the two right it is close to approximately seven right it is seven times bigger even though two is only just double of one so e to the power makes it it amplifies the bigger guys makes the big guys stand out now you take and one more thing it does it gets rid of negative values because what is e to the minus 100 close to 0 right so it brings the numbers from 0 to infinity but now suppose i have numbers and i do this i take this it is called the softmax function of h i any given h is equal to e to the h i divided by sum of all the so it is basically e to the h1 plus e to the h2 hn so now what happens you take the total of these outputs and each number is a proportion of that output. So its value can only be between zero and one, isn't it? It is positive. It's between zero and one. And the positive value, the big value has really been blown up. Those are the virtues of the softmax function. So softmax functions are used therefore heavily all over machine learning right so now we will do that we will softmax it pass all of these through a soft max function so it will produce for each of these these things it will produce certain softmax value right alpha right let's call it this thing of alpha that it produces we will call this alpha and for mysterious reasons i chose the word alpha because i'm claiming that this is a pretty good definition of attention, right? Attention, right? So this is the attention coefficient. Basically, you're saying how much attention to pay to the X1 input, right? Is the softmax of this score? By the way, not H1. I'm so sorry. Because H1 disappeared. of this score. Oh, sorry. By the way, not H1. I'm so sorry. Because H1 disappeared. It is score. Score. Oh, I'm so sorry. S1. My bad. Yeah, go ahead. One question that I have is, what's the criteria for the scoring function? That is the thing. We haven't answered that. So see, that I have is what's the criteria for the scoring function? That is the thing. We haven't answered that, right? So see, I could just do, now the question is how do I score? And that goes back to the part that I mentioned, context. We should score because that score is the weightage. That is what I'm saying. Attention is score and context. So therefore, any scoring, good scoring function, should use the context to score. Isn't it? From the hidden state, how much weightage to give to a hidden state, that has to come from the context. So how do we create the context now? That's the remaining problem. So what people do is, and we'll take a break after this, let me finish this chain of thought. The easiest context people thought is, see, they are all of these hidden state vectors, right? Presumably they are all talking about the same thing, right? The cow jumped over the moon or something like that. So what happens if I take a context vector, context is the, let me just call it the average vector, I'll put a tilde here to average, is equal to h1 plus h2 plus hn divided by n. All of these vectors, I take the average of them. Then you say, well, that's a relatively lame definition of context. But hey, it works. It's good enough that it works. It works. It's good enough that it works. Now you say, if this is the context, how do I score it? Well, the simplest way that you can score it is you can say how much is every word or every token, the hidden state, how much aligned it is with the context, like how related is it to the main context of the sentence? Are we together? That would be the dot product right and so you say that the scoring function so there are many scoring functions actually but the simplest scoring function you can create do you see guys how logically you can reason about all this in a constructive process you can practically invent this whole theory with just a little bit of thinking on your own. So you can say, and by the way, there is an excellent book that sort of follows this chain of reasoning that I like very much. I encourage you to buy this book. I like the way that this book explained actually. Very few books explain the mathematics of it. This one I like, Inside Deep Learning. Get it if you can. So the scoring function is, we can think of it as scoring function. The score of i is equal to the dot product between the hidden state, the i-th words hidden state, token's hidden state, and the tilde state the i th words hidden state tokens hidden state and the tilde the average yeah this is a dot product symbol like or you or you you can write it as hi dot h tilde right in in the other notation but dots are a little hard to see so in machine learning literature you'll often see this angle brackets put to signify dot products in fact in my notes also in lab it was go ahead but instead of mean then you go with mode or median or all the contrasts see you don't want to take the mode because it is the most occurring and in real numbers, like because you're talking about fractions, there will be no mode, each number will be equally frequent. When you talk of the median, can you take the median of these numbers? You could, I suppose, take it. I'm just trying to understand why the mean instead of the definition of... See, I can give you one answer. I suppose median could be. Off the top of my head, I cannot think of a reason why median couldn't be taken. Generally, what happens with these hidden states, if you actually represent it, you will notice that the hidden states have a Gaussian distribution. So the median and the mean will not be far off. They are far apart when you have a non-Gaussian distribution, non-symmetric distribution. Skewed distribution will have a median different from the average. But typically typically in these situations, generally if you look at the distribution, Gaussian distributions emerge, maybe with sufficiently many hidden states and so forth. So mean works, it's cheap to compute. See, median is very expensive to compute, right? Think of a number, like you're trying to find the median, you have to sort it. Why bother? Just add them up and divide it cheaper, right? Now, so this is the scoring function. This is called the dot product scoring function. Then somebody said, now, wait a minute, this is getting, this is good, but I can be better. I can be smarter right so instead of it's called the general scoring function somebody said what about this fun thing i take hi right sandwich it first with a with some another weight matrix, which I first apply to this hidden state, right? I write it like this, but basically what it means is, H I transpose W H tilde, right? Or if you want, you can write it the other way around, H tilde transpose H I. It doesn't matter which you can write it the other way around h tilde transpose hi it doesn't matter which way you write it now you have sandwiching a w also then the question comes oh you're not you're first applying a matrix operation to one of these and then doing the dot product now how in the world why in the world you do it you say say, well, you know, it gives us extra flexibility. But then the question comes, what are the values of the W? You say, what's the big deal? Back propagation will learn that also, right? The optimal W is it will learn. Because when you assume that there is no W, just do direct dot product, you're implicitly saying that the W, the optimal W is just a diagonal matrix of ones, mathematically, right? Optimal W is just a diagonal matrix of ones mathematically. But let's leave the possibility open that that also can be there. So there are different scoring functions that come in. And now we have the whole story. So did we put all the pieces together? We used the context. We used it to differentially score the inputs. So different tokens in the input will get different weightages. We softmax all of it together and we are done. So for example, if we are predicting which of these numbers are bigger, we want to train our neural networks so that when it goes through this entire two networks, when it goes through this whole thing, like the feed forward and the scoring network, and then the softmax, the biggest numbers score that should light up basically now there is one extra bit you do you say right that the net output you don't want to say that one three like for example when you when you have words and so forth what you don't want to say is, it is this guy. What you do want to say, okay, so first of all, by the way, there's a little bit of a cheat thing. There is a technicality, which doesn't matter, but I'll just mention it to you because these things are there in the literature and I'm just being exact. Suppose these vectors are D dimensional. You scale it down by D, square root of D. Why do do we do that it's just to prevent those numbers from blowing up dot products can become huge by scaling it down by the square root you sort of you're doing a defensive mechanism right so in the literature or in the original paper of attention you'll find it there right this sort of a weightage is all uh it is all there now that you have this what do we do now what are we doing what you are saying is you have a way to do attention isn't it how much attention to put so what is your effectively what is your output in effect you can say you can say that your main output is this attention alpha i or the score softmax code uh the softmax part of it is alpha i is soft max si given all the other scores right right exponentiated this value so now uh softmax of all of this now you say that your final output the output of this network is one more thing extra you do you multiply the scores the importances with the actual value right so what will happen is suppose you're taking numbers two five and eight right so what will happen is suppose you're taking numbers two five and eight so eight gets a huge amount of attention you multiply it by the hidden representation of eight whatever it is so the output will look eight ish right and the second biggest number which may be let's say five a tiny bit of that remains and the smallest number is practically so what you do is it is alpha i times the hidden state i there's a dot you multiply you take those things together are we together right and you add up across right you just this is your weight vector, and you're doing it across each of the values of i. i is equal to 1, 2. This is your total output attention. Are we together? Final output attention is this. And that output attention will look very 8-ish in some sense, or the hidden representation of 8-ish, because the alphas are moving all the importances or the scores towards let's say the big number right so that is how attention is done that is self-attention right now see the moment you put context and this thing together are you beginning to see now i'll start connecting the dots because we still have to go all the way to the transformer but i want to take a break and tie it back to our reality what did we do today we gave a search text did you notice that now now let's bring it to attention what is the context of a sentence why is that matter the context is the semantics of the sentence isn't it and so you have a mechanism right off the bat that inherently thinks through or begins to think through something that is the beginnings of semantics, isn't it? All of the words put together. Do you see that? So it is the total information captured by all of them that is baked into the whole process. And now you're beginning to understand why this is the way to move forward if you're creating a search engine, the right way, semantic search engine, isn't it? So I think like in the numbers, you can say this is the highest and the lowest number. Yeah. But in the words, like how does it come up in the context? Yeah. So the same thing applies. A word is represented into a hidden vector, and it is the average of those vectors. Right? So suppose I say, I am very, very, very angry. Three various, right? Guess what it is going to do? Right? And also, angry has importance, like somehow it should pick up more importance. Right? So the thing that should light up is that I bought this thing printer on this day, I did this, I did that, and blah, blah, blah. But finally, when you look at the sentence, the thing that should be brightly lit, neon lit, is very, very very very angry no not necessarily it is see based on the problem that you are solving it tries to establish what is the context what where should i pay more attention to get the answer if it is sentiment then it is emotion isn't it in the case of sentiment analysis the appropriate context is sentiment so weightages should be given to things that have emotional bearings automatically so there it applies but in different situations it may not be so right so the scoring function is different in case of sentiment now yeah so if you'll graduate, see at this moment, I'm doing a hand-waving argument because I haven't gotten to self-attention and transformer. I'm just trying to give you the core concept. But you're right. The scoring function is different in case of sentiments, how it scores the word, right? So if you were to just use pure attention, obviously the emotional word should blow up in score right whereas a word like model number pqr right who cares that that's the idea so guys at this moment i'll take a break because now i'm going to introduce another powerful thing called self-attention right because that paper the transformers don't just use attention they use self-attention and why they use i will talk about it in another few minutes but let's take is 3 40 close to it should we take a how much of a break 20 minutes break okay so let's regroup at four o'clock by the way uh on the remote i'm not getting any response are you guys getting the idea ask any questions guys if you have to yeah sure is this explanation clear yeah yes yes nice i'll pause the recording recording all right folks i'll give you the. Like look at this journey from the input tokens, right? Think words or numbers or whatever it is. We got the hidden representation. One neural network is smart to come up with the best hidden representation for it. The next stage is there is a scoring function and a scoring network, especially if you put a W in there, which somehow finds the best scoring thing. And how does it do it? It also uses a context, right? It uses a context to differentially score. You get the attention coefficients, and then you multiply each attention coefficient after soft maxing it with the actual hidden state to get the output state. And the output state should automatically be pointing towards that token to which you should pay the most attention. Right? So this is it. Now we will learn about self-attention. Self-attention to frame the context. See I framed the context initially is that if I give you a bunch of numbers or images of handwriting, images of numbers numbers find out which is the biggest so the answer was right in there now and it was sort of a problem of which of these but now let's change the context i'm not saying which of these i'm saying here is a sentence and you need to jump you need to translate it into French, let's say, right? And I make the sentence complicated. The well-fed and happy cow, right? Jumped over the high fence, or no, jumped happily or whatever it is, over the blue moon, right? While it was, well, doing something else too right while it was going right now the problem is what does it refer to does it refer to the moon does it refer to the cow it's not very clear isn't it so what you want the it to do somehow is you want the it to point to the cow and less to the moon. During the translation process. So what we need to do is for each word, you can almost say that when I'm translating the sentence into French, then suppose the sentence had 10 words. Then I'm actually solving 10 different attention problems. I'm asking the first word, how should it distribute its attention across the other words so that it falls in context. Isn't it? Right? The second word also needs to. So what you're doing, in effect, is you're creating an attention model, one attention vector per word, per token. Isn't it? One separate attention vector per token. You're doing this exercise 10 times over when you do that that is self-attention now this is a lot of abstract thinking suppose there are 10 words so each word i i want to find which which of the rest of the center words how much attention it should pay to the rest of the words so that I'm properly able to translate this word into French. Because I can't do literal translation, that wouldn't make sense. I have to contextualize this. So for example, the ripe apple, the farmer grew the ripe apple. If I just take apple, I might be meaning computers? But I don't. Why? Because the ripe word and the farmer would immediately is a giveaway that it's about fruits, isn't it? So Apple needs to pay attention to the farmer and to the ripe. So the thing is, for each word to properly translate it, I need to know, what is the attention map to different words? Are we getting it? And so what we are trying to do, let me put it this way. Let me write the sentence. The farmer grew some nice, some delicious ripe apples. Deli, deli deli she is ripe apples now let's think through this I'll write the sentence again the farmer grill some delicious ripe apples so now think about it if you think of apples Apple should pay attention to which words we yeah it probably should pay attention to this this and also to the farmer, right? Because the farmers grow or something like that. Maybe to some extent to grew, some and the take very little attention perhaps, right? But farmer and apple makes it very, very clear that we are talking about the fruit, isn't it? Now, on the other hand, if you you think of delicious delicious should pay attention to ripe it doesn't care about ripe apples it should pay attention to apples isn't it and delicious well farmers are not delicious thankfully right you don't want to pay too much attention to farmers etc so it's attention to farmers and to other words would be uh lesser right so you can always see that each word has its different attention map right its own attention vector to the words in the same sentence am i making sense so that is why you call it and so each word is both the target of attention from other words and has its own attention map to the other words. That is why this mechanism we call self-attention. And it is at the heart of being able to do the right translation, isn't it? Because the context is in the self-attention here. In fact, you don't need a context vector here because the moment you do this self-attention you can basically say that this self-attention that you're passing this different attention is your context isn't it if these attention weights are properly scores are properly computed the whole goal is to compute them well right so now let's think how would we do that so what you say is that now now here's the thing each of these words let's say that this is delicious at any given moment you're you're trying to ask what will delicious pay attention to, right? So to create an attention map at this particular moment to put your scores or your alphas, alpha 1, alpha 2, what their value will be, alpha 3, alpha 4, alpha 5, alpha 6, alpha 7, and of course, alpha 8, which is the end of sentence, right? Delicious is the end of sentence right uh delicious is the context let's say it is the context word would you agree right so whatever vector it is suppose you create a hidden representation for it whatever you do it becomes your q it's your con think of it, this is your context vector. And we'll also call it a query vector because you're querying the attention for this particular word, isn't it? And now you are getting the hidden states. So in the self-attention, there is a very interesting formalism that came up. You don't create for each word one hidden state. For reasons that will become clear, you create two hidden states. In fact, you create the key one and value one. But because the query itself is a vector and it will be the context vector for that word there is also the query one q1 so it's a triplet when when the word for this word you need to find out how much attention to pay to different words it is the context when it is the target then the key key k and v of that word, because other words are paying attention to it. Isn't it? So, for example, for apple, you would say Q. So here's the thing. This line is a Q. What is this? Q5. That is the delicious apple. This is paying attention to this is the context the this alpha 7 you can say that here the alpha 7 is like sorry the hidden state of 7 what would have been hidden is now burst out into two different hidden states k7 and value 7 why I'll tell you why why you produce two hidden states rather than one so now these together k7 value 7 right so now what happens is remember that context you dot product with the hidden vector right now here is a way of thinking why you did key and value Now, here is a way of thinking why you did key and value. Think of it like this. Suppose you had a hash table, a dictionary, a map. So think of a dictionary. When you look for a word in the dictionary, you look and you find an exact match. And so the value is the text, the meaning of that word in the dictionary. Isn't it? You get the answer. But now, suppose I told you that the word is not there, let us say. Then what would you like? right then what would you like or the word is there but exactly that word doesn't exist in in this language but there are two words this word is closer to the word in let's say some particular word let's say that you have a word i don't know x word in english it has no french equivalent hypothetically let's say and but it has but they're two different words this is closer to this but a little bit of this also so what would you like to do you would like to partially take the value of this word and partially take the value of this word isn't it to different degrees and put it together value of this word isn't it to different degrees and put it together right um in a way of ranking it differentially variant so in information retrieval theory or in search engines etc one of the things that comes up is you think of it every word so for example this particular case word embedding. We just did sentence transfer. What did you do? You put it as a point. Every sentence is a point, but imagine that every keyword was a point. Then what happens is, suppose you have a key vector. Think of the key vector, key i, as the location or... key i as the location or location of the token in some space some d-dimensional space. So key is there, key is here. The value of that, sorry, value of I is actually what it is. When you think of the value, what you think of as the hidden state, it is more like the value, much more like the value, the actual value so now think about it this way if i take a query vector q j right take a query vector q j in the same space if i dot product it with k i what will happen what does the dot product measure how close how close qj is to ki right and the closer it is that is your attention that is your context or the score function scoring scoring function, isn't it? Up to the normalization constant. Remember, I put the square root of d there, right? So if I look at the attention, and so this is how much attention it should pay to the value vi, the value sitting at that location, isn't it? So if this query is pointing this way you better not pay too much attention to this value if the query vector is like this literally pointing in the direction of this particular value right you better pay more attention to it the dot product will be bigger and so you pay more attention to this point right and so this times v i this is literally your you know what is this this is your alpha i j in the previous language that we were talking about no how much attention to give how much attention attention token token j pays to i pays is this right to this point but the attention is the sum total of all of this i let me just write it this way this is how much it pays attention to vi this particular one but you have 10 tokens right there's this there's this there's this so to come up with the full attention vector you have to do the dot product with all of them and you know multiply it by that you have to find the vector that results by taking the weighted attention of all the value vectors are we together yeah yeah we'll come to that but just say imagine that a magical fairy has come pulled her wand and lo and behold, you know the perfect query vector, you know the perfect location of each value vector, words, token value, and you have a value vector. Let us leave that to the fairy for the time being. But should that problem be solved, then you agree that the attention that j will have will be the sum total over all the i means averaged over all the i so in other words it would be a qj dot product with key one times v1 right plus dot product of the same sorry the query the query vector j with k2 times the v2 plus qj dot product with k, let's say, n vn. And all of it, of course, you can go and normalize by square root of d, by square root of D and you can do this. So this is your attention vector where this term is paying attention. So you agree with this, right? This equation, right? This is literally the weighted sum of all of those value vectors. Now the question is that is there that you asked is, it would be so lovely if magically we knew, right? All of these values, but who tells the value? Isn't the whole problem that we don't know this value? We have to learn these values, right? How can it magically do that? So, because if it can do that, then in effect, for example, it means that the word delicious should pay much more attention to apples, much less attention to some of the other words, right? But how do you come to know these values? That question is answered by, it is almost, you know, sometimes the answer to a profound question is, it is already a solved problem. Why? Because in neural networks, you can learn it by feeding it a lot of data, putting a context of a problem, some classification problem or something, and letting the network make mistakes and back-propagating the error. So in the beginning, you just assign random values to Q, K, V for each of these tokens. And then what happens is it will create some random values. Then the predictions that it will make will be terrible, right? So suppose it has to tell, is this sentence a positive sentiment or negative sentiment? It will just come up with random prediction. It will produce errors. That error, and there's a mathematical machinery for that. This is a cross entropy loss. That cross entropy loss will back propagate. So there is a back propagation algorithm and a gradient descent. There's basically a mathematical machinery that will go and say, let me change the values of KQVs of all these things. I'll change the weight. So you don't change the values of KQVs. What you change is that the neural network, the way it computes the KQVs, you change its matrices so that the next time the same input goes in, it produces different KQVs, and therefore a better prediction. And you keep on doing it, feeding it lots of things, looking at the mistakes and back-propagating the mistakes, doing gradient descent, back-propagating the gradients, forward pass, let it make a prediction, backward pass, the errors, back-propagate the error gradients, and keep changing the network weights. And eventually you'll stabilize at a situation that it is making lesser and lesser errors. In fact, the definition of learning, the word in learning, machine learning has the word learning. Learning is this, gradient descent and back, in the case of neural networks, it's back propagation. And it will automatically shake up the network in such a way that for this particular task, it will, given a word, it will produce the ideal or near ideal Kqv vectors isn't that magical and it does that so basically like when you read it it gives you the results and if the results are wrong you back propagate the errors so you correct the results you give back again and so you look at the gap between the correct answer and the answer it produced, that is called the error. And there's a mathematical way of quantifying it. For example, in the case of classifications, cross entropy loss, it's called a loss. Now how much you lost in this prediction process, you can do, there's a mathematical jargon called gradient descent, you take the gradient, and gradients, what it will do is it will tell you how to best slightly change the weights, so that the next time you feed the same question, it will come up with a slightly better answer. That's it, right. So the gradient of the error helps you fix all of turn the knobs, and a neural network is nothing but a million gazillions of knobs. So it will help you help you change the knobs a little bit. So the next time around you give it the same input, it will produce better vectors. And that is it. So this mechanism guys, this is self-attention. So you're changing the alpha basically. Yeah, alpha values change as a consequence of changing the network weights. And attentions change because of that. So you gradually force it to pay attention like for example delicious shouldn't be paying attention to the word duh. Duh is irrelevant to delicious. It should pay gradually more and more attention to apples. And you'll keep on doing that till it gets it right. And you train it on vast quantities of data. And so what happens is, as it soaks in more and more text of a given language, it learns to pay attention the right way to produce the right KQV vectors. And how do we apply that like let's just take that as an example of delicious or apple and say that like there are there are k and b7 k7 b7 values like the region that you wrote yeah yeah so let us write that down so if you realize let me just number it one two three four oh no this is five yeah k five six seven so what it means is let's make it very real actually thank you for saying that because i should tie this back in um so what it means is that look at this expression that i said attention what is the attention vector for five that is what we are trying to do delicious is the word what is the attention vector that it has? And we are saying that up to a proportionality constant d, forget about the scaling factor d, it is key 5 dot product with query 5. Because for delicious here is the query vector right is the query q5 let me with k2 vector which will be a number times v2 so this is your alpha 5 1 this is alpha 5 2 in some sense right and and so forth right you can keep going all the way to q what is it seven q5 sorry q5 uh key seven times the value seven how much attention it's giving and so you will end up with what you will end up with this is see these are just numbers right this could be 0.01 etc etc and this apples because it's the seventh one hopefully it is 0.85 lot of attention here right and so it is times v1 plus something something v2 plus plus this times v7 right so most of the attention is going to which vector the apples vector right so attention vector for app for uh delicious is more or less pointed in the direction of the apples vector it's literally staring at it. That is attention. That is self-attention. Once you get this idea through, now let's tie this back to the transformers. Now, if you get this, you pretty much have gotten the big innovation. So what are the, now you look at it and say, this is all very nice, but what have I gained from this? Right? Let's think through it. The big benefits that you gain from it is, first, did you notice that you did not feed it tokens one at a time? In an RNN, what did you do? You first say the, then you give it farmer, then you give it grew, some, apples, some delicious apples. By the time you get to apples, you forgot who grew. So you don't create one hidden representation for the entire thing. You instead do it in parallel. In fact, you have to do it in parallel because every word is both the key, both a query vector as well has its own key value, hidden representation. It is both the context as well as the hidden state, right? So you have to compute all of it in parallel. Also, because these things are computed by direct matrix multiplication with W, massively you can parallelize, right? In one fell swoop, you get all of it. And so while you have to remember, by the way, one thing that we missed, we still have to remember the location of words because you cannot reverse the words. I'll give you sometimes a hilarious example. There was a book, the book was titled Eats, Shoots, and Leaves. Right? Now, based on, I will put two is here and the other is no no karma at all so one is eats shoots and leaves what are we talking about here the panda but the pandas eat shoots and leaves for example or giraffe or something like that. Let's say, panda. The first one eats, shoots, and then leaves. That sounds very James Bond-ish. Isn't it? So you have to be suppose i were to put a karma here for whatever reason shoots and something else and leaves based on where the location of the karma is the meaning is entirely different all right so position is another part that you have to remember and now we come to the idea of transformers so so far so good guys self-attention is the idea that we all see this attention maps drawn and etc etc uh heat maps but usually it's a little hard to get what is meant right i hope now the meaning is clear to you this is self-attention and finally finally we come to the transformer right the transformer is a realization now let's go back to our language model encoder decoder model encoder um the self-attention is the AJ this The self-attention is the AJ. Each word's attention map to other words. But you call it self because the sentence is paying attention to itself, not to something else. And those keys are not weights. K vector, Q vector, and V vector. These are three vectors that spill out of every token. Is that value of attention? No, no, no no no let's go back let's go back um no no it's good that we really the more data in federal firms yeah the more data the better it gets see what happens is what I'm saying, Albert, is that you remember that for every token, a token is its own context. So the token vector itself is the query vector, Q vector. I represent the context as a, for this word, delicious, it is a Q5, is the fifth one, Q5. One vector, Q5 is for that. And now what happens is the hidden representation, the fifth one q5 one vector q5 is for that and now what happens is the hidden representation the hidden state representation i don't have one they bring in google because it was google and they were into search and all that they brought in the machinery of information retrieval now look up key values right dictionary so they said that the hidden state should be represented as a token's location in the abstract space embedding space and its actual value both so they produced hidden state got burst into two parts the k and the v and that is why the same equation see if you look at this equation what is this it is the hidden state multiplied by the context that is exactly what we are doing if you look at it uh context being q multiplied by the hidden state but here we are taking key. And then finally multiplying it again by the hidden state, but now we take the value instead. Whereas in just an attention map, you would have said that the hidden state context, which would be a global context, not a per word context, times the hidden state i times h i. This would be the attention, total global attention vector. Do you see the close similarity between this? Except that now we do not have a global attention vector. We have one attention vector per word. So, therefore, this thing gets replaced by the query. And now we are talking about attention vector on a per word basis. So Q5, right? HI, now that same HI is both here and here. But what they do is they call it the coordinates of it, they make it KI, and they call this the VI. And the moment you do this substitution, pretty much what you get is the famous equation in the research paper, attention is all you need. In fact, it's the only equation in that paper, right? Except that it sort of is, it bursts upon you and you're like, where did it come from? Right, all that we talked about is one attention head. And they found, the first thing they found is, let us make both encoder and decoder out of attention heads, not just feed forward networks or forget RNNs, right? So let's sandwich some attention heads. And obviously the dense feed forward is just a given. It's like the glue. You always keep sort of pouring it everywhere, right? Because you need the non-linearity. Otherwise, just multiplying by matrices won't do. You won't get the universal approximator behavior. So take attention heads and put that. But then they found that instead of one head, if you put many heads, then a very interesting behavior comes out, multi-attention head. Multi-attention head comes from the fact that see one let's say the word delicious one attention head may focus on its relationship to apple based on the semantic meaning another may be more sensitive to the grammatical structure the grammatical relationship between the words right right? So what you can do is, because there are different underlying contexts, you can have one attention head for each of the implicit underlying contexts, right? So I believe that the first architecture that started out used six attention heads or so. Nowadays, there's many more. So you pass them in parallel. So given a word, delicious, you don't just produce one query vector, one key vector, one value vector, but let's say six sets of them. And they all have different values because one is looking at the grammatical structure, one is looking at some semantics aspects, something or the other they're looking at right so this is the mechanical aspect that it just increases the power of it are we together right and so what you do is you pass it through all these attention heads and what comes out some feed forward network i won't go into all the technical details let's stay with the big ideas what comes out these attention maps keys values come out from the top and you know you take the output of one layer put it through another attention layer through another attention layer so what you do is now that you know attention works you you put them in parallel you put them in series you make a whole circuit out of it kind of electrical circuit sort of a situation out of it you go absolutely bonkers with it right and you notice that the more you add them the better it gets right and that's that and so what you produce is a hidden state but notice that the hidden state has been produced completely in parallel in one stroke there is none of the things that some words are forgotten before others but there is one little subtlety now from the hidden state, you can go into the decoder part of it. But the decoder is the only part which is still autoregressive. In other words, what you do in the decoder is you still produce the hidden state, but it's pretty big hidden state. And then so the hidden state, encoder produces the hidden state. You pass the hidden state into the decoder, into that, and you give it a trigger. Let's say start. It produces the first word, the French word one, first French word. And what you do is you take that French word and you pass it back next time. You don't give it the start token now, you give it instead the word that you just produced, along with the hidden state. The hidden state remains the same, that along with the first word. So at every stage, what happens is the hidden state remains the same, but the next word keeps changing isn't it the next word that you feed in it's the previous word and so it will produce fr2 and it will keep producing till it says end of sentence end of sentence word is produced and so you get a French translation are we together so basically it is an encoder words go in, go in, and it produces a hidden state which goes into the decoder. Right. And that's that. So now there's a bit more. See see I have glassed over certain technical details one of the things is hey what about the position where the where each word is matters even where where even a comma period is matters in a sentence as we just saw so what you do is when you convert these words into vectors these vectors are called actually embeddings e1 e2 e n so words w1 becomes its embedding vector right and there has been techniques to generate those embedding vectors you can learn that transformer learns that also and so on then one more thing you do is you add another vector which is the position of each word in the sentence are Are we together? Now, for a set of technical reasons, you also pass in P1, P2, P3, Pn. Another set of vectors, you first put the embedding vectors, then you put the position vectors. And together, you feed it into the whole architecture. So position is also preserved. Because see, the moment you pass words in parallel the notion of position would be lost unless you encode and feed it in isn't it so you encode the position thing also and feed it in so that's a bit of a technique technicality here's multi-head multi-heads so multi-heads guys the way to understand it is to first understand attention and self-attention and then realize that the whole architecture is just making encoder and decoder out of just going bonkers with it are we together putting them in parallel putting them in sequence one after the other doing everything that you possibly can do right and there is also some residual residual circuitry and so forth i won't go into all the full details of a transformer but that was the original transformer then came so that was attention is all you need paper right so what i have done guys on the theory side is I've explained to you the original transformer. Now, I will, on the Slack, I'll mention some weekly reading. I will post the original transformer paper. See if you can read it. It's a technical paper. But I hope after this introduction, you should be able to make some head and tail out of it. Even if you understood 50 of it right you're coming out way ahead right because it is a very technical paper but with some luck you'll you'll get a sense that you get the big ideas now don't get hung up on small details because anyway that original architecture is not what you use now the new things have significantly changed. Go ahead. Yeah, have been evolving. So there has been evolution. And in subsequent lectures today, I introduced three main concepts, attention, self-attention, transformer with this encoder decoder, right? And while we are at it, let me just bring up the paper. And let's do it for a couple of minutes together, just to make sure that we don't we don't feel intimidated by it uh if we google up the word let's google up the word let me remove this let us say where where am I? Where's my mouse gone? Just about the right time for it to lose power. Yeah, okay. Attention is all you need so if you look at this paper i'm going to go through this paper and you will find that it's actually relatively straightforward to understand right tell me if it is so or not oh oh goodness uh So or not? Oh, goodness. I have to go back and look at my password. Why am I getting into that stage? What happened? Sorry, excuse me, guys. I need to figure out what happened. Attention. Attention. And. Just give me a second. We're almost done for today, but i would like us to glance at this paper yes So, guys, look at this paper. I'm going to read out the abstract and see if it makes sense. The dominant sequence transduction models, transduction means one sequence goes and another sequence comes out. Transduction models are based on, that's a trans part, are based on complex recurrent or convolutional neural networks that include an encoder and decoder so that is what used to happen the best performance models also connect the encoder and decoder through an attention mechanism which is the part i mentioned that people started adding attention to it we propose a new simple network architecture the transformer based solely on attention mechanism. Did you notice that there was no RNNs anywhere? Dispensing with recurrence and convulations entirely. This is it. This is the gist of the revolution, the breakthrough. The experiments on two machine translation task show these models to be superior in quality while being more parallelizable and more parallelizable in fact very parallelizable and requiring significantly less time to train and then they give you the data so is the abstract making sense guys now right now let's look at this formula. You go into the background and we will talk about it. Now look at this. Is this, look at this formula and look at this part. What is it? Multihead attention, a few feet forward networks thrown in. Right? mentioned a few feed forward networks thrown in right this is input goes in but there is more you know from the side you also Indict do you notice that positional embedding is thrown in into the mix you need to do that so what I have highlighted in yellow is your position is your encoder part output of the encoder is the hidden state where does it go it goes into the decoder part decoder it has an interesting thing you notice that the output of the decoder right it keeps going in going in going in and then this is the output embedding and what you do have here is you keep producing the output embedding and you pre like whatever the soft max is of course in the end which were to emit out and so on and so forth right so this is your decoder part on the right is the decoder in the left is the encoder part right and you take the output embedding right shifted right you keep on feeding it in and you get away with it now this is what we are saying now the scaled dot product attention is it making sense to you q dot k remember query the context dot you know query the the hidden state. You scale it. What does it produce? It just, so up to here, it produces, as you can imagine, this produces the scores. Then you normalize the scores to the softmax, isn't it? And then you multiply it by the V, isn't it? Because this is your coefficient. This tells you how much weight is to put on the V. That's it, right? And why multihead attention? It turns out if one attention is good, many attentions is better. Go at it. So this is the formula. Is this exactly what we were talking? It is written in matrix notation, but it's exactly the same right and that is it the rest of it is just you know explanation of it where for a head i the i mean there isn't the rest of it is just explaining things right and why self-attention right because it also gives you enormous performance benefit. So if you look at this paper, it focuses a lot on the optimization, performance considerations, et cetera.