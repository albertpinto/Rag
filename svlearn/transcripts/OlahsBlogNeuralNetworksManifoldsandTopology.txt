 So, we are discussing this very nice blog by Christopher Ola. It is about networks, neural networks, manifolds and topology. Sort of shows the link between these three things. Now, neural network is something we have been introduced to in the first two weeks. You probably don't know what a manifold is and what topology is. Anybody here with a background in manifolds and topology? So I'll just give you a way. So as if I think it is the study of the shapes, right? And how like it deforms and if it like even if it changes the shape, if it does not geometry does not get deformed, like you have to poke hole to actually understand the behavior. Yes, that is a pretty, at least a pretty good understanding of it. Close to it. What you're saying is the homeomorphisms and that is one of the, or diffeomorphisms more like it. Those are the things that we will talk about today. You will see me introduce a few words that are at the heart of machine learning. See in machine learning and this paper sort of alludes to it, but doesn't get too much into it, there is a fundamental hypothesis. All of machine learning is predicated on a hypothesis. It is called the manifold hypothesis. If this hypothesis is not true, machine learning would essentially not be possible. So what is this hypothesis? We are going to understand it in a little bit, but before we do that let's try to understand a few things. What is manifold A? What is the topology? What is the topology of it? When we say topology of a manifold, what do we mean? Manifold and then see, we'll talk about things like homeomorphism and if your mom is and if your mom is and is something this bug did not mention the free free oh this is actually more so what are these things even networks actually they're not just homeomorphisms so what are these things we will understand that and what is the relationship i suppose machine learning to manifold center quality so imagine that you have an elastic sheet. I'll just take a square sheet, but it can be any shape. And in that sheet, you have, let's say a shape, round shape. This is let's say a shaded red and this is white. Suppose I say it is stretchable sheet, rubber sheet, elastic sheet, some form. Think of it like this circle to become an ellipse? So how would you do that? You can stretch it. You can sort of stretch it and you can say, there we go. It has become this. Are we together? So now it has become an ellipse. What you have just done is a transformation. You would call it both a homeomorphism and a diffeomorphism. Diffeomorphism is a smooth transition. So let me just call it a diffeomorphism into this particular shape. Because you can think of, in some sense, that this state represents a state in which the ellipse. So let me parameterize it with a time that goes from 0 to 1. At 0, it is this red circle. At 1, it is the ellipse. So this is t is equal to zero state, t is equal to one state. So you can say that in some sense, you could say that equation of this is one minus t, the circle, if you say that the circle manifold, the initial manifold, plus t times the ellipse manifold. Let's see if this equation makes sense. This is a mapping from zero cross one times. This is what a 2B space going to a two dimension space. Well, I wrote some fancy mathematics here, but what i'm seeing is very simple this r square is the the sheet this um rubber sheet and this r square is of course the target uh stretched out rubber sheet isn't it and now we are saying that actually this is a mapping in which you have a continuous deformation of this into this parameterized by a factor t so now in this equation try putting t is equal to zero what will what do you get at t is equal to zero you will get the circle at t is equal to one when you put t is equal to one it will become the ellipse does this make sense guys? Think of this equation and it's a very simple equation. I'm saying that as you increase T from zero to one, it initially to look like a circle and gradually it will start looking like an ellipse. Does that make sense guys yes it does make sense right so this is a classic example of a saying that we have performed a homeomorphism and usually when it is done in a very smooth way it is not just a homeomorphism it is a if you're more present right so you can be a little bit more exact now generally smooth transitions are preferable if we can get away with smooth transition it is really very good now if you understand this much then let us add some of wordings to it this is I it. This is, I did it as a 2D surface, but suppose it was curved. This is in a higher dimensional space. This was just the plane here. So here was your circle. I'm just doing the same thing. Gradually I'm taking you there. So this is your red. It's the same thing gradually I'm taking you there so this is your red it's the same thing but now there is a third axis this is X 2 X 3 right now suppose I say that you can deform it into like take this and make this circle sort of bulge out can you do that can you just sort of pull start pulling here pull up and create this red bulge here can we do that all right yes we can do that? Yes. We can do that. All you have to do is take this and just put something underneath it and gradually start pushing it up. And now you have ended up with a surface that has gone from this to a much higher dimensional space. The surface is occupying a three-dimensional space now, this manifold. And yet, even though it's occupying a three-dimensional space, you would say that in itself it is two-dimensional. The property of that rubber sheet has not changed. It is just occupying a higher ambient space. So the ambient space, the outer space is, ambient space was a three-dimensional. But in the beginning it wasn't occupying or it had no presence in the third dimension. But by making a little bit of a push here or bulge here onto the red part, now you have occupied the whole ambient dimension, all three things are there. But you would still say that the same essential quality of this elastic sheet has not changed. So when you say that the essential quality has not changed, you would say that the topology has not changed topology is the overall quality that remains invariant to stretching and bending and all of that are we together so you can smoothly stretch and bend and do whatever like for example you bent it out do whatever you want but it remains what it is just don't tear. You can't bring a knife to it. Are we together? So long as you don't bring a knife to this sheet, everything that you do preserves the essential quality of the sheet because it's a reversible change. If you can go from here, you can go back also. Would you agree? So this can go here, but it can also go back so these are reversible changes yeah so is it like a way of like can we say that is it like a 2d manifold uh embedded in a 3d space but this rubber sheet is a 2D manifold that was initially embedded in a 3D space. And this is now occupying all 3D space or a significant part of all the, you know, it's gone on and occupied the third axis. Sajeevan G. But it is still embedded in 3D space. It is a 2D manifold embedded in 3D space. So let's ask the question. Sajeevan G the question yes i wanted to ask someone so how is the topology related to geometry how we'll come to that okay i see these fields i i am giving you a geometric way of looking at topology actually if you answer that when you look at topology the initial version or the way it is taught it starts with something called point set topology. So people start very abstractly with algebra. And then they develop it from there. Whereas what I'm doing is I'm giving you a geometric interpretation of topology. In a very geometric way, I'm showing you what topology in a very geometric way i'm showing you what topology means really correct correct let this be a set of points and blah blah blah and then it is uh you know these properties hold true and so forth so my question is like uh even though i mean the geometry won't change out the object even though we can bend and do all sort of things with an object right yes the geometry not the geometry the essence the geometry is changing look at the shape the shape has changed but you see the psychological property hasn't changed topologically still the same rubber sheet it has just been stretched you have introduced a bulge with your hand so what is topological property sir like when we say that what is the property so let us take an example of a topological property right see suppose i take a rubber sheet which is solid like this right so let me take this example consider a this is a solid rubber sheet and now think of another rubber sheet which is which has a hole in it now is there any way that you could deform one into the other any kind of stretching that would cause this to happen you know it can't I mean the only way you can do it is you can stretch it to the limit so that it perforates in the center that's a catastrophic event it's not a continuous deformation there is no way you can deform this into this so you say that this is the topology of this, A is inherently different from topology of B. People use some fancy language homotopy and so on and so forth. Let's not go into that binding number. I'll give you an example of that. So see, you take this rubber sheet, right? You fold this edge, you have this edge, you wrap it up. What will it become? It will become this. Cylinder. Yes. So this cylinder is still the same as this, isn't it? Right. Correct. as this, isn't it? Now comes the funny thing. So you say that these two are equivalent more or less, even though this has been maybe stretched, maybe bent and so on and so forth. But this is not equivalent to this guy. They have different topologies. Or suppose you take this, topologies. Or suppose you take this, imagine a surface which has two donut holes, a two-hole donut. In India you would call it jalebi. Everybody remember that? There is no way you can take a jalebi, a two-hole doughnut, and deform it into a sheet of paper, into a continuous flat sheet. It is just not possible, isn't it? So you would say that this topology C, in a very intuitive say, is fundamentally different from the topology of A and the topology of B. The people use more words but let us use some very rough words here to get the intuition. Are we together? Yes. So that is the meaning of saying that things have different topologies fundamentally. Now there is something very interesting that you will realize, and it is a joke about mathematicians, that if you give a mathematician a donut and a cup of coffee, he will sip the donut and eat the coffee mug. So what does that mean? So you have a coffee mug here. So that means 3D and 4d it was it it means like a three-dimensional and four-dimensional manifold embedded in four-dimensional manifold right see think about this i'm just approximating uh coffee mug right And think of a donut. This is your donut. This is the hole. I'll just mark the hole with a solid. This is the hole. And what you can do is imagine that you took a little bit of the mass here, solid mass here, is there a solid 3D donut? And you start bending it a little bit. And then it became like this, right? Then you stretched a little bit, took more material from here and you stretched it a little bit to this. And then a little bit, you keep on deforming it and then you take some of the material here and you make it. So now what does it look like? Same. It begins to look like a cup but you can do it this is again a hole so you realize that you can do a homeomorphism a smooth deformation from a donut to a cup and this is so popular I just realized that Wikipedia actually put it in its web page defining what a homeomorphism is so I wanted to show you that I was very interested in morning I was looking for it I suppose the joke has become so popular that I will now show you this screen here so look at this visualization do you see how a cup a doughnut becomes a cup look at this. Are you seeing the point guys? Yes. So this is called, this is the heart of homeomorphism. See what is interesting is when two things are homeomorphic and when two things are not. When two things are not homeomorphic, it means that they are fundamentally different. So for example, and why do mathematicians play with such silly ideas? You may wonder. They seem to have too much idle time it seems. But it turns out that this is actually a bunch of what we do. Raja Ayyanar?nilam?rua?on?h? comes down at heart to these basic ideas. If you want to say, you know, you can you can go about fields like machine learning, etc. Raja Ayyanar?nilam?rua?on?h? In a in a cookie cutter way just this library this R library and pick up gazillions of things. So you can sort of go straight to the root, the foundation, I guess. I'll give you an example. So imagine that you have a ring that intersects each other. You know, people often give in the US and in the Western hemisphere, it's common for young couples, I don't know if it is still common, to be often given a gift of a ring. This is supposed to be person a and this is supposed to be the spouse and these rings they are locked in like this they're locked in you realize what I'm saying you can't unentangle the rings they're looped are we getting that now is there any deformation that will open the rings? Think about it. Is there any smooth movement that will unlock the rings from each other, unlink the loops no sir ah yes there is no such deformation isn't it and in fact that is the that is the symbolism of giving young couple i believe this sort of thing they are trying to uh i i'm not very sure what somebody here can clarify for me but i suppose it symbolifies that the two people are joined forever they should stick together or something like that. That's the symbolism. So anyway, so that's that you can't uncouple it forever. It's forever coupled. So you would see that these two things have completely different topology, different topology. You can't go from here to this. It sort of actually one of the things that I worked on during research in theoretical physics is my research group, it wasn't my idea, it was my professor's idea. They had hypothesized that the reason the proton is so stable and it doesn't decay at all is because it is actually something like this or not i mean it's sort of a in a very rough sense it is a topological solid on uh basically but think in a very rough way you can think of it as a knot that you cannot unknot right you cannot go from here to here so for for a proton to decay, in other words, become a lesser thing or disintegrate, it needs to be unknotted. And it takes tremendous energy to do that. You can't just do it, which is why protons are so stable. I think that we know that protons live at least 10 to the 38 years, the best of our knowledge is greater than its lifetime of a proton is greater than 10 to the 38 years. Nothing has that lifetime. So it's very, very stable. So that goes again back to the idea that these certain geometries are very stable and you can't go from that to this would be considered a trivial geometry relatively speaking trivial geom simple geom compared to this one so you can't go back to this anyway those are the ideas another example is suppose you take a paper so you have a sheet of paper and you align these two roles you will get a cylinder you will get a ring actually agree let me just take a very long bit of paper so to illustrate the point just imagine a long strip of paper and if you align this ring with this what will you get You'll get this nice structure. Isn't it guys? If you tape this edge to this edge, let me call this A, B, C, D. So if A and C are taped and B and D are together, they're joined, what will you get? You'll get this structure. But now let's do something slightly different and do the opposite. We align and this surface, if you look at it, it will have an outside and an inside. It has a and an inside. Isn't it? This is basically a can of soda or your can of food there is an inside surface and there's an outside surface but now we do a slightly different experiment suppose you do the opposite you connect a to b and b to c in other words before you connect these two, you give it a twist in the page. Maybe I can do this and show it to you guys. And you will know that then the concept of geometry, it is topologies, different things. So see guys, I have the sheet of paper. Can you see it? If I do, what do I get I get a mobius strip mobius strip right now it is a cylinder and this is the outer surface so it has an inside and outside but if I give it one twist then it becomes a mobius chip and a mobius chip only has surprisingly one surface. It doesn't have an inside and outside. It has only one surface. So if you start roaming around the Mobius strip, you will realize that you'll come back to the original place after having traversed all the surface. So now if I keep this thing, is there any way you could deform this just by without cutting this paper is there anything you could do to make it into you can try all sorts of things but will it become the simple cylinder again no no you know right so you would say that a mobius strip cannot be cannot become a slend cylinder through any homeomorphism there is no homeomorphism that will take you there simple guys yeah so that is the idea the core idea i will talk about here so now that we understand these words homeomorphisms and if you're more if you're more just smoother ones and remember that inherent in this is the invertibility of it you could always go back that is why i gave the sliding scale 1 minus t times the original thing plus t times the final thing so it's a little bit of the original little bit of the final at t is equal to 0 it's only the original at t is equal to 1 it is only the final diagram that you are making now how is that relevant to us? But before we go there, let us start putting tick marks to it. So now what is a manifold? So far we understand. Now what is a, with all this background, what is a manifold? So think of a manifold as a sheet of elastic sheet which need not be plain which can be crumpled up so for example a donut is a manifold a sphere is a manifold any surface that you can think of let's move I mean so long as it is smooth so the one condition we expect is smooth. You can't just iron a sheet with a very sharp edge and call it a manifold. When a sheet has sharp edges, it is called an orbifold and we won't deal with orbifold. Machine learning, the gist of it is, is manifold, smooth surfaces. And the surfaces can be in higher dimensions also. So in three dimension, we think of two dimensional surfaces. We live in three dimensional, in-game space. So I'll use the word in-game. In-game space is the space in which data lives. So our eyes see data in three dimensional space. In this three dimensional space, surfaces, when the natural thing we think of surfaces, the surface of a teapot, the surface of a, you know, just a bed sheet on the bed, right? Or my writing tablet or anything like that. These are all surfaces and they all come as two dimensional surfaces. But when mathematicians think of surfaces and spaces in machine learning, the first generalization is the data comes in arbitrary many dimensions. So let us say that the data comes in a P dimension space, initial space of P dimensions. So you generalize from three dimensions. See a human mind cannot visualize higher dimension, but logically we can sort of imagine that data is existing in p dimension space so any surface there would be of one less dimension just as in three dimension a surface is two dimensional so in a p dimensional space a surface would be p minus one dimension are we together any hyper and you would call that a hyper surface so that brings us to the concept of a hyper surface and i see one quick question on so you you made a statement about in generally in machine learning we deal with smooth surfaces yes now would there be a rider that you'll add on to that to say the smooth surface requirement is within an interval a bounding so you know yeah that is there so what you say is see machine learning generally in real life when you solve a problem the the input space is not unbounded. It can be, but usually is not unbounded. There is some region into which you are trying to model. Occasionally it is possible to model for the infinite extent also. It depends on a situation by situation. So one example that I would give is, let's say you have a sine wave. Now a sine x goes from minus infinity to plus infinity, you can define sine x. So if you're trying to model sine x on the entire interval, the function that you will that will approximate that will be entirely different from a function that just looks like one period. Look at this one interval, one period. So let me call this a limited box b and this is the whole thing. So in other words, when you do X going from zero to two PI, the model that you would build that you will need to build, let me call it GX would be different from the model that is here. So for example, if you did the ML 100, 200, do you guys remember what did we approximate this with? How many bends does it have? Two bends? And so we can approximate it as a polynomial of degree? Three. Three, right? So we could do it as a third degree polynomial means. And now instead of is weights in machine learning deep learning w1 x plus w2 x squared plus w3 x cube so this will suffice to describe it in b but it will not suffice to describe it in this interval right because in this interval there are infinitely many banks infinitely many bends. And of course, you would realize that if I want to do this fx, I will have to write a polynomial in infinite number of terms. In fact, that's what it is. When you expand a Taylor expansion of sine x has infinitely many terms. And many of you would remember that x minus x cube over 3 factorial plus x five over five factorial. And it goes on and on and on, like minus x seven over seven factorial. There's no end to it. So the question in machine learning always is, you're building a model for some practical purpose. So what is the domain of relevance of your model? Is it the entire extent or is it a limited extent? All right, clean it. Yeah. Now, so with all that being said, invariant to all that, you can imagine a surface which is infinite extent. For example, you can think of a plane that has infinite extent, right? It's just an infinite bed sheet in that sense so a hyper surface may or may not be limited or bounded all right so given a hyper surface if i add one more condition to it which is interesting roughly speaking and this is a very rough way of putting it roughly if your rough way of putting it roughly if your hyper surface does not have sharp creases did i spell creases correctly or is it c-r-e-E-S-E-S? I don't know. Creases. That's correct. Then it is manifold. What does that mean? So one example of that is, let's say that you take a sphere. Let's take a sphere let's take a sphere so you can take any region of the sphere tiny region and imagine there is an ant here oh by the way do you guys know that in fremont at least there is a massive um ant problem that has suddenly come up. Terminix informed me that they are getting 1400 calls a day about ants. So unexpected result of climate change because the earth has become so hot and dry that this poor ant is moving into people's home in search of better ambient conditions and some moisture. And they have become, in a very real sense, climate change refugees. So anyway, so imagine that you're, well that's a digression. I don't know, is anybody else noticing that? Yeah, in San Ramon also same thing. Same thing, yeah. So imagine that you're a little ant. If you're a little ant, to you, this sphere, the little bit of surface that you see, and imagine that this is a giant sphere, what will it look to you like? It will look to you, simple flat surface, right? Like a Cartesian surface. X, Y. You can draw your XY coordinates. Does that make sense guys? So this is the ant sitting here and to it the world looks flat. So you would say that to the end her world is world is her world is equivalent to an r square cartesian flat surface would you agree i a little flat plane yes so this little world the little world that an aunt sees the little world that an aunt sees is called a map. Because it's the map that this ant has of her world. Now what about the next ant? There is another ant here with its own little map. There's other ant. But then suppose you have another little ant who has a world like this the yellow ant now look at the yellow ant it has its own little map right which is r-squared but there is a problem to it you know that this the sphere is curved so now see the in reality whether the ants can see it or not there is a little deformation of their worlds right actually let me make it in white and yellow let's say that the world of ant yellow ant is this and the world of the white ant is something like this so now there is a problem this ant the white ant has a coordinates and has a coordinates X and y and the yellow ant has its own coordinates for the same point P yellow ant would say as x prime y prime isn't it so far so good guys. And you realize that the yellow ant is sitting here and the white ant is sitting here. And see the yellow ant and the white ant are not different because these two maps, one can be diffuomorph to the other, right? Morph to the other. Means you can continuously change this world to this one how make the ant go go this path if ant white ant goes to the yellow and to have a conversation what will happen then the world of white and will become the same as the world of the yellow and would you agree guys so if white and goes and meets the yellow and so it is at the location of the yellow and what has happened to the white and snap it is identical the location of the yellow ant. What has happened to the white ant's map? It is identical to the yellow ant's map. Would you agree? Yes. It's like this, you know, the white ant is sitting with its map and it is going and meeting the yellow ant, right? So the thing is gradually this map will change into this map as the white and goes and then white and comes back, it will get back its own Jay Shah, Dr. V. S. Now there is something really interesting this point P, which is x, y. Jay Shah, Dr. V. S. Has a different interpretation in this other guy in the yellow and swirl, which is explained by crying. in this other guy in the yellow ants world which is x prime y prime so you would agree that once again there's a homeomorphism between these two points there is some function some way to tell that x prime y prime is some function of x y right it is some function of x, y. You can do that. And so long as this condition is fulfilled, you see that this surface in which you can smoothly move back and forth is you have a manifold. Roughly speaking, it's a compact, well, there's something housed off compact and many, many things here. Technical definitions are there but I'll keep it to basics. It just means that they're smooth, you can go smoothly from one guy's perspective to another guy's perspective. And this is very real guys, you know we all were little ants thinking that the world is flat, right? And now we don't think so, we know that we are on a sphere so you would say that sphere is a manifold manifold because local patches look like partition spaces, right? Or whatever here it was. So I'll just say Rn some way. So with that understanding of manifold, now we understand all the concepts and now we are ready to read this paper. Do we understand what is a manifold, guys? This one? Guys, if you don't understand, stop me and I'll repeat it. Ask if you can repeat it again. Okay, so I'm saying that a manifold is something whose local patches, so on which if you put tiny ants on, to each of the ant, her world will look like, for example, on the sphere, is true for a sphere, isn't it? To the and, if you want to say, to the and, look flat. So any kind of a hyper surface broadly where this is true and there is a way it may be curved which is perfectly fine but if two answers separated from each other but they have a way to reconcile their perspectives so the same point P I represent as XY you represent as X prime Y prime and we have a rule to translate back and forth then we are in good shape right we are looking at what we are looking at is a manifold am I making sense now guys that is all the manifold is locally Locally it should look flat. So for example, you crumple up your bed sheet and to an ant, the little bit of space that the ant can see looks flat, isn't it? But to us human beings for the longest time, the earth looked flat because we didn't travel far. So we imagine that if you went too far, you'll fall off the edge. that clarify and yes anybody else guys okay so you know these are preparations to this beautiful blog at this moment so this is a manifold what is the topology of the manifold is the structure think of it roughly as the structure right whether it looks like a plate a flat whether it has a hole in it or whatever it is that is a homeomorphism and diffeomorphisms are smooth transitions from one looking one way to another while still preserving the topology right you don't change the topology of the manifold you just bend it a little bit and stretch it out and bend it and so forth so for example a cup and a donut they are homeomorphic to each other because you can smoothly transition a donut into a cup and vice versa i said one question so the manifold locally it will look flat but non-locally like how it will be it it could be flat uh you know globally also right a flat or a deformed surface. A hyper surface a hyper plane is globally flat. Right. Right. So but it need not be it could have been. It could be deformed surface in any shape, any hyperplane basically, a hyper surface. That is right. In fact, just as a small digression, what Einstein said in his general theory of relativity, once you understand manifolds, what he said was something very simple. He said that if you think of space-time itself as a fabric as a money think of it as a fabric of space time it's a four-dimensional manifold but then what happens is that just as you take a surface and you put a heavy object on a rubber sheet what will happen it will develop a rubber sheet what will happen it will develop a bump isn't it if you take a let's say and this is again a digression from this block but i thought i'll mention it to you you take a plane this is two dimensional but imagine it's four dimensional if you put a heavy object here you expect that a deformation will form on the fabric on the elastic fabric correct this is it and so now imagine so with this deformation there and i'll just exaggerate the deformation here yeah what will happen to a point that happens to be traveling in a straight line. It will come and it will get bent because the surface here is bent. Do you see that? A particle or a body that is traveling in this direction will get bent. Isn't it? It's spinning by by but then you see there's this deformation and it bends you can see you know in the mall you go there this little this surface is kept there for children and they'll roll a coin and the coin will spin many many many times and then fall at the bottom do you guys remember that seeing this in malls so this is it so imagine that unknown to you the space-time itself has developed a curvature Because here is a big object heavy object. Let's say the Sun and this happens to be Asteroid or something like that And so what happens is when the asteroid comes within the solar system or sort of comes within the orbit it gets bent sort of comes within the orbit it gets bent automatically and gravitation the explanation of gravity is that the reason gravity exists is because the masses i mean the explanation is whenever you have a mass in space time it deforms the space time manifold and the degree of deformation the curvature the curvature here is directly proposed the degree of curvature is directly proportional to the amount of mass that you have here or the energy that you have here in fact roughly speaking people talk about gene I won't go into the detailed subject that is all that general theory of relativity says that the curvature is energy and the amount of energy at a place is directly the mass controlled by the mass there in the space time and that's why one thing gets pulled into the orbit of another thing that gets attracted to another thing and so on and so forth so this is it guys you know when you when you understand understand these concepts of manifolds and homeomorphism, etc. A lot of these deep ideas that begin to become quite simple after that. So now, but let us go back and make progress on that. So, we understand manifolds and topology. Now the fundamental hypothesis, what is the manifold hypothesis? Sir, just a second. What is the difference between the normal and the typological? Let me write this sentence down. What is the manifold? And this is fundamental. Is someone asking a question? What's the difference between normal and vertical? I can't hear you Harini. I think you are speaking but I can't hear you clearly. What is the difference between normal and vertical? Anybody can repeat that what she asked? Ask if your third question in your bullet point if you go to the top. Homeomorphism and diffuomorphism. Yeah, the difference between those two. Just think of it as very smooth transitions are diffeomorphisms. That's all. When you gradually transition one thing into the other, if you do it very smoothly using the mapping, if the mapping function is differentiable, it's a diffeomorphism, right? So in other words, if X goes to Y and there is a function fX, if you can take the derivative of f, if it exists, it's a diffeomorphism. And what it means is, geometrically, what does it mean for a function to have a derivative everywhere? It is just a fancy way of thinking that the function is smooth are we together that's all it means not all functions can be smooth it can be continuous but it need not be smooth for example this function is it smooth no there is a non-differentiable point at the bottom which is why i said this is a crease this is a wrinkle right this is a very strong crease here so long as you don't have creases so you so long as you don't have these edges in your function your function is a diffeomorphism simple enough guys okay so now we will go to this manifold hypothesis. So the best way to do that is, I'll go back to the foundations that you learned. See, regression, the classic regression is there is an X point and there is a Y, which is a target variable. You do things like this. you have points like this. When you have a point like this, you become hopeful and you say that a good prediction line is this, Y is equal to 1X. This is how you learn in elementary machine learning books. Isn't it? Up editor am i making sense guys does this look extremely familiar now yes so this is but what is a line now just ponder over it and think back we went from data is in R square right data is in R square but the the so Indian space this is the Indian space where data is presented to us. In common sense terms, what it means is we will be given a table, X, Y, a table of data. So we are given a two dimensional data. But the only hope you have of solving this is that you find a one-dimensional line. Yellow is a one-dimensional line. This is R1, i.e. R. It's a one-dimensional line. Now, look at an alternative data set. If the data set is like this. So let me call this data set A, data set A, data set a data set b now in which of these two cases will a regression model be successful a and why is that there is pattern mapped by length see there is a relationship y does have a relationship to x and it shows itself by the fact a linear relationship that all the data points sort of reflect that relationship here all you can see is that if anything you can say y and x are completely unrelated seem to be seen to have no relationship isn't it would you agree guys so you wouldn't try to build a model here in fact your model will be a null hypothesis there is no relationship between the two now observe the situation where there is a well visible relationship in A and lack of a relationship in B. What can you tell about the data points in A versus data points in B? Geometrically, what do you observe? Anybody Anybody would like to venture a guess? Geometrically, what's the distinction between A and B? So is A is like a stretch surface? Sorry A is like a? Like a stretch surface? No, no, I'm saying data, two dimensional data x y data okay cases in one the data looks like that in the other look at the white points and the data looks in the second case b like what it is in the lower graph so geometrically what is it what what are some of the differences that you see when you just visualize the data so So space B is more dispersed. Space A is more concentrated. But the distribution of the data is different. Line and a space, line and a surface. Yeah. So, you know, you guys are coming to it. See, if you look at B, the data seems to occupy all of the two-dimensional surface, the two-dimensional space, input space, right? It's scattered everywhere. But what do you see about A? Most of the data is close to a lower dimensional surface, namely a line. Do you see that almost all the data points are either sitting on the line or close to it? Isn't it right we can say that all the points are proximal to the line isn't it so now let's try to generalize from that what about this I could have the points here it need not be a line. Suppose I have a situation like this. X, Y. You would still say that there is a relationship between X and Y and you can do a regression model here. You have a one dimensional curve. One dim curve. Right? That the data is proximal to that. one dim curve, right? That the data is proximal to that. The data is proximal to. So generalization of this is that if you have a relationship in a plane, you can build a successful regression model, which is a relationship between y and x if and only if the data are close to some function some curve right and what i'm saying is very common sense i'm saying that that if you're looking for y is equal to fx epsilon this f is nothing but a curve 2d do you agree with that guys this function is a curve and so most of the data has to be close to the curve right up to up to this error term to this error term. Okay. Feedback that this looks obvious. Does this look obvious guys? Simple right? Some function. Now what you say now observe something in higher dimension. So suppose it was 3d 3d your y would be some function of x1, x2 and this would look like a surface. All the data will be proximal to the surface in 3D and in higher dimension. So now generalize it to higher dimension and y would be the sum function of? So let us say that dimensionality of this is our end. So the vector space. Let me make it one plus one because it is x the dimensionality of x and y, you would say that y would be some function of x Isn't it, and this would be n dimensional hypersurface. Does this make sense guys? Is this generalization making sense? In two dimension, the function lives in one dimension, namely is a curve. In three dimensions, the function lives in two dimension namely is a curve in three dimensions the function lives in two dimensions in other words some surface and you can generalize to higher dimension as simple as that so when you say that this is true you're saying you're making the hypothesis that all machine learning is possible see you you can find a relationship only when a relationship exists. And the existence of a relationship means there is a hyper surface in the ambient space, which is of a lower dimension and the data is close to that. Most of the data is sitting on it or is very, very close to it up to an editor. Otherwise you wouldn't be able to do machine learning. Geometrically, it's a very obvious statement, isn't it? So now we understand this manifold hypothesis. Manifold hypothesis just says that This is just says the date data given in ambient space of D dimension, let's say of capital live closer proximal to a lower dimensional embedded manifold. Embedded means this line, this curve is sitting inside that higher dimensional space. lower dimension embedded manifold manifold is just your hyper surface so this is the otherwise you cannot build a model otherwise you can't build a model. So this is the manifold hypothesis. So far so good guys. I say what is that like a space RD will actually live after that what is the world will actually live proximal to a lower the rocks the world means close to approximately close just use the word close with that search close to a lower dimension and better than anything otherwise you can so guys do we understand it I hope this was something by now fairly self evident would you say for deep are you getting it yes sir okay it's good yeah all right all of you are good with that right okay so then we are ready to read this beautiful thing block let's go from the beginning and try to read this beautiful blog. Let's go from the beginning and try to read this. So what it says is that imagine that all the data points that you have, by the way, this picture in the printing because I made an image out of it to put it into this notepad. So it is actually like this it is one big blue line a blue line like this and a red line like this if you read the original blog it's like this so data is here or here so suppose you're trying to write a classifier you know that you need a decision boundary and how should you should your decision boundary go it has to go like this in this space, isn't it? So if this is your x1 and this is your x2 and these are your data points, it turns out the data is most of the data points are here. He has made a solid line. Realistically, you can make it a little bit more rough line in the sense that data will be proximal to this surface suppose this is the reality you want to write a classifier you need to build a decision boundary that looks like the dotted green line that makes sense isn't it a Speaker, guys, please. Yes. That makes sense. Right. Okay. So now how does it do that? What it will do is when you take a neural network, so when you say that I have a neural net, ultimately, right? So suppose this is the input X1, X2 are the input that go in and then there's some hidden layer, right? So There's hidden and there's hidden and so at the end of it What comes out is some a transformed variable? Let me call it Xa prime Xb prime that are coming out of these nodes These are the responses of business and then it finally goes at the last node right the logistic unit at the end so you know that the last thing what does it draw what does a logistic unit draw if you recall a logistic if you recall a logistic if you recall, a logistic unit, logistic regressor, builds a what? Builds a linear decision boundary. So what does it mean? If this guy has to successfully be able to classify, what you need to do is you need to take this space and somehow do a homeomorphism of this formula of this to some place where a straight line, a straight decision boundary, let me mark it in green, a straight decision boundary would suffice. So this is the decision boundary that this line is, this is the last unit is building. So what for that to happen, somehow the data has to become, the blue has to somehow be here, and the red curve somehow has to come here, completely in this region and this region. Isn't it? You need to take the original manifold and you need to deform it into another manifold in which a straight line goes through because that's what the last unit is doing and but then for a straight line to be the correct classifier all the blue points should be here and all the red points should be here make sense right so that is what it does and you see that happen you look at this so you may have all sorts of hidden layers not just in the hidden layer not just two elements you think with more but at the end of it for you to draw this decision boundary that is here we need some representation that divides the data and you notice this is a green line where is a green line this is a green line. So all the blue points are up and all the red points are below. And so what has happened? Do you notice that this curve has been deformed? This curve, let me use some other marker. Look at the blue curve. Blue curve has become this curve there's a homeomorphism of this curve into this curve and there is a homeomorphism of the red curve into this isn't it gradually it has been deformed from one to the other. Isn't it? And so this is your, like I said that the new axes are XA, XB. This is your XA axis. This is your XB axis. Right? And the initial axis, if you think of them as X, what is it? X1. What did I initially say? X x2 right this is your x1 and this is your x2 axis right so initially this manifold that you have here it is gradually deformed to this other manifold where the curves gone on the manifold their shape has changed isn't it but when the shape changes the beautiful thing that happens is a straight line can cut and separate the two curves easily the green line now straight green line which is the logistic layer the last logistic layer decision boundary of the last output layer. Isn't it right? Does that make sense? Yes. This is it. And this is the beautiful idea that he's trying to say and then he gives this visualization. If you see how it does it, how these functions do it, it is approvable and the thing is the proof of this is quite simple. If you think about a point, let us look at the proof of it. Suppose you about a point let us look at the proof of it suppose you have a point x and you translate it to x plus omega naught do you think that this is a homeomorphism all you have done is you have taken a point and you have moved it with Omega naught right this becomes X becomes X plus Omega naught is this a homeomorphism if you take a plane and every point you shift will extend smoothly transition to the other plane yes yes a translation therefore is a homeomorphism this is easy translation is homeo and is it invertible how do you invert back to the original point you just subtract w naught right likewise the next factor suppose i take X and I multiply it by W1X. Is this a homeomorphism? It is a stretching, right? You're just stretching it by an amount W1. Make sense? Correct. So is this invertible? Of course, you can go and divide it by w1 and you'll get back the original point. equal to w naught plus w1 x1 and then let's go to two dimensions x1 plus w2 x2. You would agree that this is a homeomorphic transform. This is a smooth transition. All you have done is stretch the plane a little bit and moved it around a bit. And now comes the interesting thing. After that, suppose you apply a tanh function, a sigmoid function of some sort. Suppose you apply a tanh z. What is tanh z? e to the z plus e to the minus z, E to the Z minus E to the minus Z. I hope I got it right. Correct me if I got my pluses and minuses in a twist. But this is it. This is your tanh. Now, this function, is it, what about this this does it lead to does it take z to a homeomorphism of z when i go from this tan x z is it a homeo and the answer to that is yes because this function is differentiable i can do because i can do uh let me write it, because. So as if you're, I think you change the sign for tanh. It's the e to the power z minus e to the power minus z. And then positive on the bottom. Positive. Okay, there we go. Because I can differentiate tanh z is differentiable with respect to Z. So in other words, it's a smooth function of Z. So I can gradually deform Z into tanh Z. Therefore, it also is a diffeomorphism. And so what it means is that in my layers, when I take this network, let's say any one of these nodes into which X1, X2 goes, what is it doing? It is doing a tan H of Z, where Z is this transform. So you're doing a set of homeomorphic transformations and the accumulated thing is still a homeomorphic so your points x1 x2 ultimately they just smoothly moving to some other point in space which is what we see if we go here every point every point here is smoothly moving to some point here right do you see this nice square grid? And what has happened? The square grid has been deformed. Do you see that the same grid is now stretched out? You can see a bulge here, you can see a squeezing here, but it is a smooth transition from one to the other. And that is what the neural nets do. What they do is they deform the space continuously, the input manifold continuously, until the two classes become linearly separable. And when they are linearly separable, the last output layer just draws a straight decision boundary for a classifier. So that is what he's trying to say and much of the paper actually talks about that then he goes on and says you can do more than that if you think about this one this picture so so far we understood here so far guys right did you ask them a question so when we say the like the transformation so is it like a linear transformation or is it a fine transformation no it is fine sir see it is a smooth homeomorphism it's a non-linear it is a smooth okay it's a deformation see imagine that you have an elastic sheet right and then you put your fist into it you you realize that it's a non-linear deformation but it is still a smooth gradual deformation isn't it but it's non-linear inherently the moment you bring in tanh tanh is non-linear up to z it's a fine transformation z is w W naught plus W1 X1 sector. Okay got it. Got it. Yeah. And it inherently introduces a non-linearity. Then comes a beautiful thought. So you can sometimes look at the data and tell. Remember this was a point I was telling you about. You can look at the data and tell how many notes you need in your neural network so suppose you solve this problem you have the red in the center and blue outside right so you if you take a neural network with two nodes let me take that in some color suppose you build a neural network with only two nodes, X1, X2, and what do they produce? Then these are the two hidden layers, right? What do they produce? They have the different weights. So how many weights will be there? There'll be four weights here and a bias term and a bias term here, right? So they will produce, let's say xa xb and let's say that xa and xb go in and feed into the output layer right so now think about it xc xp whatever it is it's a homeomorphism smooth deformation of this this. Let me call it X1, X2. It would be a smooth deformation of this to this, isn't it? Now you can convince yourself that there is no, whatever stretching you do, the red circle will stay inside the blue circle. If you stretch it a little bit, you might get a red circle here and you might end up with a blue circle here. Isn't it? But you won't be able to take the red out of the blue. Right? So there is no straight line that you can draw. There is no straight line that you can draw that will separate the red from the blue. Actually I should have used. the red from the blue actually i should have used it sorry let me make the blue into a blue property first blue so there is nothing that you can do will convert and you can in your mind try to stretch and do whatever you want to do with this plane there is no way you can go from this two-dimensional surface to this two-dimensional surface. Give it a moment in your mind and see whether what I'm saying makes sense or not. Harini, you need to fix your microphone. I can't hear you at all. your microphone i can't hear you at all what happens when we pull the red up try dialing hurry please do one thing take your cell phone you can use your cell phone to dial into this zoom try that because your laptop is not working i think she is asking what if we pull the red part up? Up, exactly. But that is, you're now going to three dimensions. Right. Harini, you have just the right idea. Your idea is correct. You are now lifting the thing up. So to lift it, you need one more dimension, isn't it? Right? Yes. What does that mean? You mean that this thing here, you need one more dimension, basically. You need to go to a three-dimensional space. You need one more node coming here, which will produce could give me a three-dimensional space all I need to do is make the red in the third dimension make the red sort of stand out here and in this plane again the same thing about this plane and the blue part to be a circle here and therefore I can easily make a decision boundary like this that's the point you're making harini isn't it okay excellent point that's exactly the right point and there we go so in other words what have you proved you need at least three hidden nodes in the hidden layer to be able to solve the problem and that is the point he is making that see sometimes by just looking at the topology of a problem you can tell what neural network you need and that was essentially the point i was making in the class also remember i i took a slightly different example i took the example that if your data set is like this uh what was it this is all blueberry kind of thing. And this is some red colored berry. Oh, what happened here? This is all red berry. You need at least one hidden layer with how many nodes in it for this problem? Two. Two. You need two nodes in this. And let's reason why. So I gave the explanation that you need two hyperplanes to solve this, two lines to solve this, one line per logistic unit. So you need two of those magic boxes to solve it now let's reason the way ola reasoned it he's saying that see suppose you look at it as a manifold can i somehow stretch it in such a way that this begins to look like a solvable thing yeah i can right if i stretch it really really hard what i can do is gradually i can make it with if i stretch it really hard i can make all the blue points sit up and all the red points sit here effectively and then i can draw a thing like this so So for something like this, though, this would be quite a stretch, maybe a better state. You guys can think of a better deformation. Maybe this is not the right one. By the way, we can find that out. Why don't we do it as a lab homework? Let us see that if you put two layers here, you give it X one X two, I'll give take this data. And and let's see what XA, XB turns out to be. And let's plot XA, XB. We will also learn whether it looks like this or looks like something else. So let's take this as a homework and say, what does XA, I'll write it here. XA, XB space. Play around with it. It's very simple. You take a data, something like this, and you just feed it to a hidden layer with two nodes, and then you get your answer. So So that is the gist of what he is saying guys. Then he goes on to other things. So he says that the moment you take a third layer, the red lifts up here, separates up and you see it happening here. You see that the red has lifted itself out from the blue plane right the blue is here and the red is here it has deformed obviously it doesn't look like a circle here uh i over simplified it it begins it gets stretched out but in a three-dimensional surface now you can do a separation right you can simplify this even more suppose your data data is like this. Is there a green line that I can draw that will separate the blue from the red? I cannot do that. Isn't it guys? Look at this data. But if I lift it, if I could somehow do this, and then this is blue let's say that the blue is here this is blue now I can draw a decision boundary that goes like this and that is that that's a basic idea it becomes even simpler that you need one more dimension to do this. So in other words, to solve this problem, you need again, how many nodes do you need? You need two nodes to do this because you need to produce XA, XA, XB. You need a two dimensional plane to solve this. you need a two-dimensional plane to solve this so that's how you reason about solving problems looking at the data and solving problems now he goes on to something else he says that see there is a problem of knots suppose you're given data like this people in not theory talk of knots and unknots so when you tie your shoelace it is not a knot we think we are tying a knot but mathematicians would disagree and they would say actually you're tying an unknot because it can be you it is a unknot because you can actually unravel it you can pull the strings and then it will open itself out and when you pull this thing gradually so what you have done is that what you think of as a knot has gradually deformed into the straight string do you see that guys and therefore it's not a real knot whereas what is a real knot a real knot is what you see in the picture above this is a real knot why because now you have these two rings this is the the ring that I was talking about it's popular in the US I suppose the young couples are given this kind of ring this thing is there any deformation that will classify this data think about it is there anything you can do to these rings says that the red separates from the blue uh you can go to the fourth dimension and then separate it you can go to the yeah yeah so what you can do is you can keep going to higher and higher dimensions you can separate but suppose i say no you have to stay in three dimensions then what you can do is you can keep going to higher and higher dimensions. You can separate it. But suppose I say, no, you have to stay in three dimensions. Then what do you do? Then you can't separate it. Yeah, three dimension, you can't separate. And so what happens is that, see, it is a fact that all knots are unknotted in higher dimensions. So it's a general statement. I think that statement is true. Maybe there's some peculiar notes that cannot be. It can be. All knots can be opened up in higher dimension. So that's that. So where does this all lead us to? It's just a way of thinking about what neural networks do. So this week, we gave to the fundamentals of neural network, understanding what do networks do so you know the foot this week we gave to the fundamentals of neural network understanding what do they do yeah and having a strong geometric intuition at least I find helps a lot that's that then then the rest of it I won't go into then there's some speculations and so forth but this was the idea that I wanted us to cover in this talk. And that is it guys. So I hope it gives you some intuition of what exactly those networks are doing. And why do they work? Yes, if this was a good paper, good paper and the fact that it picked it out for us to read kind of gives an opportunity to read a very focused piece to understand something fundamental thank you you can also line bottle example is good example a Klein bottle is a great example theoretical physicists love it great that you brought it up you know something very interesting about klein bottle you think that klein bottles can only exist in four dimensions and yet people doing image processing showed that local patches of images can be mapped like there's a homeomorphism of a patch of an image because of the color gradient, you know, the gradients and so forth, to, of all things, a Klein bottle. If you guys are interested, we can discuss that paper. Well, actually, take it as a self-reading because next week I have a different topic too. We will discuss in a new paper that we will discuss next time. But it's something to know that these beautiful geometric shapes are all around us. It's a reality that we don't see. And it's there. In fact, images and patches of images are mapped to the most natural mapping of a natural image is to a Klein bottle. So can you hear me? Now I can hear you clearly. Okay. I have like when you say when now we found the linear, we found the decision boundary in a higher dimension space, right? The knot and all. So as we used in kernels, so that result is used in the lower dimension space by the system to solve it or how does it come? Do we have to come back? The idea is the same. So in many ways you can see that it is the same kernel idea. What you do when you apply all this hidden layer with many neurons is you're lifting data to a higher dimensional space where the data becomes separable. So there is a very deep connection between all of this and what we discussed previously. See all of these fields, all of these ideas are very interconnected. Okay, yes. And the manifold, is it mandatory? It need to be a flat surface always or only the Cartesian space it need to be flat? No, no, Harini, the whole point is manifold is a general hyper surface. So a crumpled up bed sheet in your house is not flat. Yeah. So here's the thing, you're a mom, you'll know this. In every morning, you make a mom, you will know this. Every morning you make it a flat surface. Correct. And your child sleeps in the bed and next morning what do you find that surface to be? Crumpled up. That's your manifold. So it can change. The only condition of a manifold is that if you are an ant sitting on that locally it looks to you flat okay and also manifolds not only have curvatures in the mv space they also have stretching right so for example uh it is almost like you know the bed sheetet has some amount of spandex or something in it and your child just finds it and You know takes a big ball and wraps around it and so on and so forth So there is a bulge in the bed sheet because there's some amount of elasticity to it or spandex in it I don't think anybody makes bed sheets with spandex just hypothetically Yeah, okay. Okay, so it can have stretch it can have stretch deformations anything the whole point of manifold is it's a generalization if it is absolutely flat it's a trivial manifold that trivial mind is it's a hyperplane but you want a sphere the surface of a sphere for example is a manifold it is stretched right but locally if you sit anywhere as an antenna sphere it looks flat isn't it in fact for the longest time we all lived on this big sphere that we call the earth well not a sphere but somewhat like a sphere like an apple and we all thought that the earth was flat because why we are like ants we see only a little bit and wherever we look our eyes were telling us that the world is flat so therefore the earth is a manifold surface of the earth is a manifold okay so so just to make it clear so when we see a small surface like an ant that needs to be flat right for it to be a manifold right it is always flat in the nature yeah Dr. Sanjiv Moinish- Yeah. So the idea is you give you. Why are we putting the emphasis on the flatness see flatness local flatness any function that is differentiable. Dr. Sanjiv Moinish- Says that you can find the derivative of it locally. If you look, it will look flat. Do you see locally. It looks like a line. if you look it will look flat do you see locally it looks like a line right and so in higher dimension this curve becomes a surface so locally it looks flat so when you say that locally it looks flat all you're saying is in many ways you're seeing that it's a smooth differentiable it's a smooth surface okay that's all and one very rough way of carrying these thoughts around it think of a manifold as just a smooth surface okay thank you sir the opposite of that would be like you know absolutely like you know edgy bend like this right or even a bend like this or discontinuities right gap that isap. That is not a manifold. If you have a tear in it or gaps in it, or if you have non-differentiable, non-smooth points in it, then it's not a manifold. All of the things, generally speaking, the sort of data space machine learning people look at, they are all manifolds. Right? Okay, sir. Okay. That's all. All right, sir. Okay. That's all. All right, guys.