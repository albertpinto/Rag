 So the topic today is boosting. Remember we did not do boosting in ensemble methods. So we are going to do a boosting today. Are you guys able to see my screen? Yes, I see the boosting and other than that it's fine. Yes. So let us recap what we did in ensemble methods. What are ensembles? Ensemble is a good word for saying crowd, right, or a collection of things. Now the root idea in ensemble methods was, it was sort of the wisdom of crowds if you take learners weak learners even then you can take an ensemble of weak learners and create a committee of votes or some process out of them such that you get a strong learner we we did a quiz on all of that I hope you recap what we did. The weak learner is defined as a learner which can make a prediction at least right 50% of the time in a binary classification problem. So between a cow and duck, it's able to distinguish or get to the right answer classify with an accuracy of 50% or greater if you are able to do that then by taking a committee or a crowd or an ensemble or a jury and many other words people use in textbooks you can get a strong learner but we are used a lot of words that I didn't use before, committee or jury and so forth. So a lot of textbooks you will see, they use these words, they call it committee methods in machine learning. They mean the same thing, they mean ensemble. Some people call it the jury method and so forth. So taking the majority and jury method and so forth so taking the majority and so on and so forth so these are all ways of taking a bunch of weak learners but out of them extracting a strong learner a decision from them that's a strong learner and that's sort of the wisdom of crowds approach when we do this thing then to approach one approach that we talked about was bagging. Bagging was bootstrap aggregation. What it meant is that to each learner give a subsample, a random subsample of the training data, and this subsample is with replacement. In other words, if just because you gave a data point to one learner doesn't mean you can't give it to the other one. There may be common data points between two learners that they're learning from. So you take sub-samples without caring that they should be mutually disjoint. They don't, in fact. They have overlapping sets and that's bootstrapping. and that's bootstrapping. So you bootstrap and then you aggregate in some way. If you're dealing with classification, then the predicted label or the predicted class is that class which gets the highest number of votes. The highest number of votes may not be majority. For example, if it is a three class problem, a cow, duck, and horse, then majority would be 50%. It doesn't mean that 50% of the people have to say it's a cow. But if the greatest number of people say that it's a cow, which may be as little as, let us say that 36 people said it's a cow, or 37 people said it's a cow, well, then it's a cow well then it's a cow right so it is the largest the candidate that gets the most vote that is the aggregation for classification problems on the other hand for regression what do we do we take the predicted value from each of the learners, we asked the learner, okay, tell us, for example, if you're trying to tell the temperature or the amount of ice cream that would be sold on the beach by an entrepreneur, young entrepreneur. We ask each of the week learners to make a prediction of how much ice cream would be sold by this entrepreneur on the beach. Then you take an average of all of them and a simple average is your prediction. So that is called bagging, bootstrap aggregation. It's a form of aggregation and you can do, I mean you can, I don't know if there these are the two common ways of doing aggregations. Are we together? Each tree is independent or each sort of learner is independent of each other. When people talk of bagging, sometimes they think that bagging is true only for trees. No, it is true of any form of weak learners. You can use weak learners of any kind and you can even have heterogeneous collections of weak learners, different algorithms thrown into the mix and so forth. The question often arises why not if weak learners can do so much better why not take strong learners, why not take like really intelligent guys and take their vote. Experience teaches us actually that if you take strong very very strong learners they tend to get correlated in other words let's put it this way you get a four professors to opine on a topic let's say economics it would turn out that they have already exactly the same economics textbooks gone through exactly the same academic program and quite often they will be aligned in what they say doesn't mean that they may be right quite often the all the even when you have a unanimous especially when you often they will be aligned in what they say. Doesn't mean that they may be right. Quite often, all the, even when you have a unanimous, especially when you have a unanimous vote amongst the experts, you should be particularly cautious. You should ask, are their opinions correlated? Are they all drinking from the same fountain of knowledge? And therefore, could they all be wrong? Time and time again, we see that it happens. It is not as rare as you think. It doesn't mean that they are always wrong. So for example today all the experts believe that you should wear a mask. It not only protects others, it protects you as the scientific data is emerging. It doesn't mean that they are all wrong because they are all correlated. It is just that the scientific evidence here speaks for itself. So just putting it in perspective. So that is bagging. In contrast to bagging today, what we will learn is something called boosting. Boosting is not very different from bagging, but it takes a slightly different approach. In a very intuitive way, if I have to say, what is boosting? I can put it in a more jocular way. It is to say that learn from the mistakes your parents made. Well, big into the grain of salt, of course, what do I mean? What it means is that, let us say that every, suppose you have a sequence of people who are trying to solve a particularly complicated problem, but at some point in the solving the problem, you trip, you make a mistake. Now, let us say that next comes the next guy's turn to try it and the next guy is turned to try it have you noticed that when you do things like that the guys who come later they tend to go further before they trip right because what have they done they have learned by observing the early players trying to solve the problem and they know all the ways that it can't be solved. So a little bit of the spirit of that is captured when we do boosting. What you do is you train a learner in the data, a base learner, a weak learner. The weak learner will do some things right. Some areas it will make predictions well. And some areas it will just completely founder and some areas it will just completely founder so then what do you do you you march out another learner in you you gradually build an army one by one of these learners so the first learner you you ask where did this learner make mistakes large mistakes and the job of the second learner is to try not to make mistakes in the area where the first learner made mistakes are we understanding that right so he's the second learner is being trained not on the target variable itself but it is being trained actually on the mistakes of the first learner then you have two learners put together now you take the two learners combined and you make predictions again you have a you have a crowd of two learners and sample of two learners this collective group will make mistakes the mistakes that it will make or in more particularly, let's talk regression, those mistakes, those errors we call residuals. Now you look at the residuals that are made by this collection of these two learners. Then you say, well, the problem is not yet solved. We will take another bite at it. For that, you bring about another learner whose job is to now try to fix the mistake of the previous two learners the the the two ancestors that have gone ahead you try to fix the mistakes of that right and so each successive learner tries to improve upon or learn from the mistakes made by the collection of ancestors ancestor learners ahead of it am i making sense so you have a whole sequence of learners if you consider the first learner to be the ultimate grandfather great-grandfather then the second the second the next the next the next and by the time let's say that you are standing in the queue as a learner as a weak learner your job would be to learn not to you know that you your ancestors have done a tremendous amount of work learning there's a whole body of predictions that they are getting right you need to only work on this residual amount of mistakes the residuals that remain from the predictions of all your ancestors put together you get that intuition guys as if are you sharing the screen or anything writing on the board I am sharing the screen but not writing yet I'm just explaining. Okay. Okay, thanks. Is it as if that these learners are correlated? No, they are not correlated. Here's the interesting thing. Even though you grow them, each of them is observing the ancestors and the predictions made by the ancestors. These learners are not correlated. They're quite orthogonal. And you'll see in a moment when we go into the nitty gritty detail, but I just wanted to bring out the general idea before we go into the details so one very rough or cheesy analogy for this would be that if you look at the human knowledge tradition it's like that you know somebody creates let's say Newton comes up with some law Galileo comes up with the law of inertia the Newton builds upon it it, he adds something more. Then people use Newtonian law. By the time you come to theory of relativity, it is not reinventing all of physics, it's trying to look at areas where it is wrong. Newtonian law is not quite accurate. They are still making errors and predictions. And so it has a theory, it has a model that sort of fixes those errors and so science progressions progresses each of us as researchers we we learn from a whole body of knowledge we benefit from all that has been learned but then our work is with the residual areas that are unsolved the things that we that remains unexplained so there's a bit of resonance between that if i have to use it as a motivating example and a boosting so with that sort of illustration or the general idea let's now see it in actual practice what do i mean by that right so let me take the example of regression and people often use when people do ensembles they talk of trees because trees are sort of intuitive I will actually avoid the analogy of tree or maybe I'll use it at some point but I just because it has become annoyingly common for boosting is all about trees but it is actually more general it has to do with any kind of weak learners I will not use the word tree unless I need to. So I will just use the word. Okay, so weak learners. Do we all know what weak learners are by now? We know what weak learners are. More than 50% barely. In a binary classification problem, right? Or something that learns just a little bit. For example, in the case of random forest we deliberately created weak learners by what by at each split of the decision tree that we gave the learner. We gave the split decision to be made along only a few of the axes or along only a Sajeevan G. Or what or mathematically you say you let it search through only a feature subspace. or mathematically you say you let it search through only a feature subspace you know not the entire feature space and so you deliberately crippled the learner the decisions of the learner and you've made it weak weak learners are good they become de correlated that is even more true with boosting in boosting we actually use a word and in ensemb, we use a word. What is that? Learning slowly, you will hear the word learning, fast or hard, hard fitting, and learning slowly, slowly, and gradually fitting, Slowly. Gradually fitting So, working out the details. See, the trouble with decision tree, if you remember, was that it was one learner that tried to be extraordinarily smart. It went and fit hard to the data. It would go search through the feature space, find the right split points, and then go splitting, create a big tree, and it would fit hard to the data that led to overfitting. In a way we improved upon that situation in random forest by taking trees which were deliberately weak learners so that no one tree could fit to the data hard. It sort of only saw a partial picture picture or got some areas right in boosting it turns out that the learners that you pick you find really the point that weak learners are better than strong learners you over and over quite often you see that if you take very weak learners and you very slowly learn sequentially about the data you actually end up building very good models. So let us look at the sequential process. Let us say that it's a problem of regression. So when you have a regression, let me just take it, you have some predictors x1, x2, x3, xp. Let us say that you have P predictors. These are your predictors, right? And these are your values. The first value would be something. And then there is a label, Y. This is the reality, right? So let's say that you're predicting the amount of sale of ice cream, how many pounds of ice cream you would sell, let us say that it is 157 pounds based on maybe temperature, pressure, wind speed, right, and so forth, distance from beach and so forth. All sorts of predictors are there and there is a certain amount of ice cream you sell and there's another data point something something and you have data so so far are we clear guys this is your standard data frame let us say right yes so first of all you create the most biased or simplest model that you can create. Remember that a model that has no variance, zero variance, so a simple, very very simple. What you do is you go and you can take, just take the y bar, the y bar. What is y bar? Average. Average of all the y's, which is one over, suppose there are n data points, one over n of all the y i, right? So this would be your y bar. And your first prediction would be, let me call it, your first model would be for every data point, you make a prediction y bar, right? It's your baseline model, isn't't it you can't be any simpler than that and still be legitimate you're ignoring all the predictors you're just taking the output and you're making that so let me call this the y hat zero the first learner the zeroth learner makes a prediction like that so it will be always y bar y bar everywhere right y bar and so for every y i the answer is still y bar so what will happen this learner will make mistakes what will be the mistakes the residual will be equal to what residual here Residual here will be R1 would be equal to Y minus Y naught bar, Y naught hat, which is essentially Y bar. Isn't it guys? Y bar. And this is also equal to Y hat, but in this particular case, why not? So you end up with some residual, which is the gap between these two columns. Let us say that you have a certain residual and I will write this as y1. Let me just write down a few rows to illustrate this point. y1 minus y2 minus. Actually, let me use, because I'm using the level of the learners, let me use a slightly different language. I will put a superscript of that. And this also, because we use subscript for the data point, let me just use zero at the top. Y1 minus the ith data point is Y bar. As simple as that. So do you realize guys, I'm not doing any magic here. I'm just writing y3 minus y bar, y i minus y bar, y n minus y bar. This is the residual. Now when you create a regression model, the first regression model actually you're trained on nothing. You just got the bars and you picked up the residuals. So you can say that these are the mistakes made by the great, great, great grandfather algorithm, the root, the primordial algorithm or the first classifier or the first sign or classifier, the first learner regressor the mate predicted y bar and these are the residuals here. You then do something else. You create a model. Why? Let me call it a y model. The first degree model that you make is a function of, let me call it F1. So F0 was what F0 which is which is just F0 was just y bar. Right. Therefore, so suppose you write the equation. Remember we used to write the equation that the prediction f uh fx plus error right remember this is your equation of regression isn't it y hat the prediction y hat is some function of input space up plus some errors error terms do we agree this was the notation we We have been carrying forward. So I'm saying that the first model that you build was just the average simple model. Now the F1 that you do you don't train it you train it on the Y one or the residual one you treat this as your Let me just call it this way. You say that residual one is, observe something very strange what I did. This looks very simple, right? You're building a predictive model on X up to an error term. It will predict what? It will predict the errors not the actual y it is trying to because the actual y already part of the prediction is being made by the first model the f0 model it is making a prediction y bar so the next model just needs to try and predict the error so let me make this simple for you let us say that you have a temperature measuring device, a function that measures temperature, an instrument. You let it take measurements, but you notice that, let us say in a very simplistic situation, it always gets the answer wrong by three degrees right so suppose it is hundred degrees it will claim hundred and three degrees if it is 90 degrees it will claim 93 degrees and so on and so forth so for any input it predicts the answer but gets it wrong by three so what do you do now? The next learner, what should it do? It should be able to just predict the three. Are we together? Because then I can take the first instrument and the other instrument whose job is to just keep saying three. Learn the mistakes of the first one. Does this make sense, guys? Yes. You see that, eh? So you just learn from the mistake. Hang on. I don't know if you guys are seeing my expression, my hand here. So the first one is telling you the temperature, but it's a little bit off. The second one's job is if it could tell how much mistake the first guy will make, then you can put these two together and get a much better correct answer and so the job of the second learner is to predict the mistakes that the first one will make not predict the initial value but just predict the mistakes right so that is why the second model that you build you you train it on the first model, you train it on the mistakes of the first model. Right. So you will end up with a second level of prediction. Y hat one. This is the prediction from the model F1. Right. This is equal to F. Right? This thing, this model, you will make a prediction, which is from, let me just write Y hat. And how did you get Y hat prediction? You learn from this. So what is the total model, the effective model? Let me call it capital F is the initial thermometer, the F0 plus F1, isn't it? Between between these two the first one will make some prediction and the second one will sort of compensate for the errors that the first one is making are we getting the idea guys yes the paper was helpful too yeah so, so this is it. And so now what we do is then we can keep going this. So this will make some mistakes, Y1, and so you can come up with the second level residual. What is this? Let me call this the collective model F1, right? And then you can imagine that F2 will be some, so then you get the second level residual. You can then start making, second level residual will be the gap between this residual and the predictions, right? Once again, you can keep on looking at the residuals of the residuals that remain. But once you have composed a model and keep on increasing it, so it should be F0 plus f1 by the way this is not strictly correct i'll add one extra bit of information to this but this is the general idea so you go to f2 and now you can keep going to fm so you can say that the fm the model fm is whatever the the collective model that you have built so far a plus the mth model that you built fitted to the residues of this model fitted to the residuals of the f m minus one the whole composite Remember I'm using capital F to signify the collective model, right? This is it. So there's this strict look, something that will work. Common sense says that this should work. Isn't it guys? Yes, sir. yes sir right and that's that's what you do there is only one extra so this is it actually this is the gist of a boosting there is one caveat to it though remember that if you continue this go too far after a little while the residual also contains what errors noise and so you will start overfitting to the data right sooner or later if you if you keep on growing these learners you will go and really start overfitting to the data so then we need some vehicle some mechanism to dampen it down right and so the way you dampen down is you always attach a lambda factor there. You say is actually you add a factor lambda. Let me call it the Right. Or damping factor. People use different terms damping factor to this. Does this make sense, guys. So then in fact, suppose you take it zero point. Let me just take this 0.5 or something like that. So what you're doing is you're seeing that the previous learners, whatever they have learned, and then when I add the new learner, I will not listen to it perfectly, but I'll push it down a little bit, mute it down a little bit, its predictions, and I'll take, let's say half its, give it half the relevance as the previous, the previously built model. And you continue this process. So what happens after a little while is you grow a sufficient number of trees or learners, actually let's use learners that you're used to. So then it doesn't matter. You don't, you don't start overfitting to the data. When you ask this question, are we not creating correlated learners? No, actually. Because remember, what you do, the same arguments apply. Because we are using weak learners, they are inherently de-correlated. So the thing that you shouldn't do with boosting is once again, take a really complex and big tree. Because if you start fitting a very, very big complex tree to the residuals what will happen? Very soon. You'll be listening to the noise You'll be overfitting to the data You don't want to do that, right? So in fact the experience shows that if you just use stumps remember what was stumps a Decision tree a tree stump Like when we deal with decision trees, which is very common to use in this situation, decision tree as a weak learner. And again, I must emphasize that boosting is more general than just using it with decision trees, but I'll use this as a motivating example. And it's quite common to use trees is by far the most common approach, which is why a lot of people confuse this by saying all the cleaners are just trees but if it is trees you could just take a stump what is a stump that has just one split one feature one feature split are we together one feature split. Are we together? So you would just split on this, this simple tree. And you would imagine that if you split, if you take a more complicated tree, it would be better to learn, but actually experience shows something very interesting. People tend not to go beyond eight to maybe 16 leaves. Well, that's wrong spelling leaves. Here, the leaves are the terminal leaves. You don't go beyond that, but actually, so it should be less than equal to these number of leaves. You don't build very complex trees do you remember for the river data set the decision tree that got built was rather bizarrely complicated the same true with california data set but the idea with gradient boosting is or this approach of boosting is that you don't actually build very complicated trees you stop and this is the depth. This is therefore becomes a hyper parameter of the model depth of the tree. It's a hyper parameter of your model. Nonetheless, it turns out that quite often, and this is what I would suggest, start with D is equal to one, stumps. You'll be quite surprised actually that stumps work. See what happens is a stump creates the weakest legitimate learner you can create. You need a legitimate learner. It is the weakest legitimate learner you would have. Do you agree guys? Yes. Yes. It would be the weakest legitimate learner. Now with that weak learner, but growing it, gradually you would see that the accuracy or the performance, in this case, the total residual error, sum squared error of the model MSE and so forth, very slowly decreases as you grow more and more trees, one after the other. So the learning is slow. With very weak learners, the learning is slow. But here is the beautiful thing. You don't overfit to the data. The variance problem that is a huge, the variance problem the overfitting problem that is endemic indecision trees that random forest solves to quite an extent but not perfectly actually this the gradient boosting seems to do in many situations a much better job by learning very slowly you actually you know between the bias weoff, you're always on the side of the bias side, right? Avoiding variance errors. But you're growing it, you're growing the tree very gradually, not the tree, the model very gradually by adding little stumps to it, or very simple trees to it. And then you end up with a very good model today that this particular algorithm that I taught you it is called gradient boosting why is it called gradient there's a reason for it but there's a mathematical reason which we will do in engine math but here I'll just mention to you see this is the Delta residuals are the Delta whenever you have the Delta which is RI I is essentially the Delta between Y minus Y hat right Delta it is the it is Delta Y right so it is Y I minus Y hat and you can think of it as sort of this so now what happens is when you train your, this thing, people often think of it as this residue. Actually, here's the thing. Let's not keep it because I'll have to bring in the gradient operator and so forth of the residuals. We won't do that. Let's just avoid that because that's what the engine might. But okay, there's a reason for it. It tries to learn in a process that's very similar to gradient descent that you did for regression. A similar argument is applicable to the way we gradually decrease the errors here, one by one, the learning process. And so this approach that I taught you is the gradient descent. process and so this approach that I taught you is the gradient descent gradient outside not gradient descent a gradient this is the gradient boosting now this is by far the most popular boosting these days it's very popular implementations are things like XG boost right, a CAD boost, etc. It's very popular. There is an older form, there's another form of boosting, which used to be called ADA boost. I won't go into that very much because things like XGBoost just use the idea that I gave you, but just I'll give you some basic pointers to it. Adaboost stands for adaptive boosting. What adaptive boosting does is somewhat very similar. It grows the tree sequentially, and Adaboost deals with the trees. But what it does is it looks at the mistakes that you make, let's say for classification, the mistakes that you make and it amplifies the mistakes. It exaggerates the mistake. It gives exam weight boosted weight boosted weight to the data instances data instances that that a learner got wrong that the that let's say FM minus one got wrong so far. So what happens? The next time that you try to learn, if those mistakes have been amplified, the next model will try to learn from those mistakes a little bit better. So that is AdaBoost. It's typically the one that quite often sometimes the textbooks start with. But I would like to stay with the gradient boosting. It's straightforward. typically the one that quite often sometimes the textbook start with but I would like to stay with the gradient boosting it's straightforward and it is a I would like to say that much of the things that we use today largely are gradient boosting based so guys are do we understand this idea and the similar argument can be made for classification I won't go there with regression it's a little easier to explain go ahead hey so if G law is just you know you're the why mean right yeah why mean is this just learner the weakest possible learner you can have right if one what is f1 f1 is it are you adding one more predictor to it or yes yes so f1 is uh is a predictor look at this equation f1 of the first one mistakes of the first one right are you are you picking any predictors from that table? No, you don't do the feature subspace notion. You don't do it. But you do it in a slightly different way. The way it works, Abhijit, your question is in random forest, we did this random subspaces we would pick and split. You don't do that here because it is implicit because if each tree that you're growing is of limited depth you are effectively picking only so suppose i say don't go more than depth three so how many possible predictors can you have right so look at this i can if i if i can only go to depth one, two, three, you realize that I can have one, two, three, four, five, at most predictors. Even if I have a hundred features, I can just pick six. And so in effect, you're choosing a feature subspace, but you're scanning through, you're scanning pretty far and doing it. But usually when you take stumps, then you're just picking one feature, right? And quite often decision tree, I mean the gradient boosting, you should try to run it just with stumps. It learns faster, very quickly. It leads to what people call an additive model. It has some interpretability to it, right? It will basically tell you that the most important feature is this and then up to lambda dampening the second most important is this and the third most is this and so on and so forth there's a certain degree of interpretability to boost boosting with stumps right remember with by the time you get to boosting these are very opaque models like black box models and you want to preserve any level of or any semblance of interpretability that you can so by taking very simple learners like stumps to some degree you retain a level of interpretability you still retain an additive model you go and do do level five, you know, the depth eight to 16 leaf or D3, D4, D is equal to four depth. It's a hyperparameter of the model. See if they really give you an improvement of a D is equal to one. If they don't give you, you're lucky because now it means that very simple weak learners, but sequentially grown lead to a remarkably accurate model or remarkably predictive model, which is what you want to have. So guys, that is it. That is gradient boosting. Asif? Yes. So during, I think one of the dataset classes, you mentioned something about like the Sanyam Bhutani. Like October, November, December timeframe. Sanyam Bhutani. About Gann or adversarial network. Is this the intuition kind of behind that because it's a cop and the cop and the thief right like so you have one model which is creating something another one is trying to catch up so is the catching up is like trying to work on the subspace or the uh delta or the residual uh to catch up no no it is actually cooperative model see there it is adversarial cooperative model see there it is adversarial okay so adversarial is not this it's not similar to like boosting or no no no it's opposite here okay your ancestors are not against you in boosting right all these ancestral trees you take the collective wisdom of them and then you only look for the little bit of mistakes that they made and And you learn from that mistake. And this, like if you treat every tree, the sequential sequence of trees as ancestors and then the great grandfather, grandfather, father, and then the tree that you're trying to build. So what you're looking at is the collective wisdom of great-grandfather, grandfather, and father, right? Or I shouldn't use that. That's a very, I suppose, sexist way of putting it. Let's say great-grandmother, grandmother, and mother, or whatever, I apologize. Or parents. Not being gendered. Yes, that's better. Yes, parent. So a great grandparent, parent and grandparent, the collective wisdom of all of them put together still leaves something behind, you know, something yet to be fixed. And your job is not to subvert them. Your job is to improve upon what they have done by fixing the mistakes that remain. by fixing the mistakes that remain okay that is boosting that's that is a core intuition behind boosting right especially uh the way we did it now other boost follows the same thing gradient boosting is what i taught explain to you in detail with the residuals and so forth this is this is it guys a simple idea see the reason it feels simple now is because we spend so much time on ensembles and on bagging and all of those concepts so by the time you come to this boosting uh it remains simple i wish i could have covered in that original session itself but we had really run out of time so what i'm going to do is if you all understood this we'll do this in the lab but the second half of the class i want to give to recommender systems how does amazon and netflix make recommendations ask a question can you hear me yes so in this case you said that if you take an example of this term and works better if your works better if your data has multiple features then would you have sort of a set of these learners no no you do right so imagine that the feature data has 100 features now let us say that you want to build a tree of depth only one. So what happens? The first tree will be depth one. It will pick one feature, perhaps the most important feature, right? Then that will make some mistakes. Then the second tree again gets to pick only one feature. It will pick up some other feature and then one by one you'll keep on picking up features. So suppose you build a tree of depth, let's say, I mean the gradient boosting of depth, let's say 200. Gradually each of the features may come in a couple of times, on the average twice, some may come up more often than less often and so forth. Or if you take a tree that is depth three or four or depth three, let us say, then up to six features at a time may show up in a tree. Then the next tree will pick up another six features, another six features, and there'll be some overlap. Are we getting that Sanjay? Sanjay Gupta Yeah. So that is gradient boosting. That's boosting, guys. It's a very simple algorithm once you understand ensembles. The point here is the trees, you don't grow them independent of each other. But you use, you grow trees one at a time, sequentially. does that bring up it's slow isn't it random voice will go lightning fast it will on these machines with six you know these days we have machines with what 20 processors 40 threads but total hardware threads you can do a lot of parallelism but the traditional one moment reason but the traditional boosting the way I explained to you it seems to be as though you can grow only one tree at a time. So you're not using the parallelism that's inherent in machine. The implementation XG boost, extreme gradient boosting, that is what itGBoost is. It solves that problem from an implementation perspective. It says, let's see guys, we live in a world of massively parallel computers, right? Distributed computing. So can we find a way to do this boosting business in a parallelized way, right? As a parallel computing or distributed computing manner. The fact that you can do that, the clever tricks to do that, implementation tricks to do that, and that is XGBoost. It runs much faster than normal boosting, which slowly grows a tree. Okay, I think somebody Patrick was asking a question. Go ahead. So as if running the boosted model, this is just one instance or one epoch. This is not, this is different from stacking? Or is it a subject of stacking? Yes, so yeah, that reminds me, why don't I talk about stacking also? Before the break. So guys, we have learned two ways of putting learners together. Right. Let me call it learner L1, L2, L, M. M learners. So if you just parallelize it and aggregate it, this is bagging. Right? into this and then you these are weak learners li data together to produce, to produce what? To produce like here, the residuals from this, you produce L2. So there's a sequentiality to this. You notice that this, and you can't build L2 till you have built L1 and L3 again data helps in and so what you have is a sequence L1 to L2 to L3 to LM this is boosting backing and boosting are the common things that people know of there is a third way to do ensembles which I would like to mention now and I would suggest that you use it as much as you can. It's really under exploited but very effective. It is called stacking. You create multiple layers of learners. See this is all one layer learning right. All of them form sort of one layer of learners and then you aggregate them or here You have a sequential model. There is a third way of creating learners. What you do is suppose you create learners Let's just take bagging. It's sort of think of it as a way to improve upon bagging So let me take L1 by the way in stacking you don't have to take weak learners You can take any form of learners that you want l2 lm the weak learners do get the job done actually quite well what you do is they will all make predictions why one hat um actually let me because this learners are I use subscript for data so let me use this and to apologize I'm mixing up my subscripts here so this is your first but the first one is making predictions like this the Emmet learner will make predictions like this isn't it it? Individually on the data, each of them will make predictions. Are we together? These learners are independently trained. They're not trained sequentially. So let me just put it this way. Independent learners, very much like backing independent learners. And then you do something. You create another learner here. Let me just call this learner, capital L learner. What it does is it takes the output of each of these learners as though it is the X vector for this learner right it is a m-dimensional input vector to this learner and that one will make the final prediction guys I'm hearing some sound if you could please mute yourself. Guys, do we understand it? Let each learner make a prediction, y hat. All of those predictions are actually the input vector, they form collectively the input vector to another learner, which what is it learning from? It's not learning from the data actually it is learning from the behavior of each of the learners do you see something very interesting right it is the saying these learners are making some data this input some predictions and what is the weight ageage? If you really think logistic regression or linear regression, it would be as simple as how much weightage should I give to each learner? Maybe some learners are better than others, right? So how much weightage should I give to the predictions of each of the learners? And how do I sort of put it all together to create the final prediction so a very simple way would be a linear combination let us say w what w1 right times y hat 1 as w2 y hat 2 so this would be a real is it could be something like this. Do you realize that, right? You could do a sort of weighted combination for regression, something like that. This would be a simple example, a linear learner at the end, right? But, or for classification, you could do the same thing, but a sigmoid of it for class for regression sigmoid Y hat L could be nothing but the sigmoid remember the sigmoid function the logistic function something you could do logistic learner for classification classification you could try but in general what you do is or more or more use a more complex use a more complex right is coming from you please yourself yes okay I'm more general more generally we could use a more more complex learner at the end, it need not be a linear, right? So what people do these days is something, for example, what you do is the L, for example, this L commonly quite often is a support vector machine or a deep neural network, a deep feed forward network, you know, dense feed forward network, something that we will learn about in the bootcamp feed forward network. What is it? A feed forward network is basically you take the inputs and then you feed it through a few layers of let's say three you pass it through a hidden layer and maybe another hidden layer and then finally this sort of a situation a dense combination of these am I getting it right yes something like this and then into this to get your white hat finally from the learner it's very common to put a feed-forward network that learns from the predictions of each of the learners before it right and to use this technique is called stacking now if you remember some of you guys, I mentioned that bagging you can do with any sort of classifiers. You don't have to have all of them being trees or something like that. In fact, the scikit-learn has the voting classifier in which you could give it a classifier C1, C2, let's say, and likewise the voting regressor, CN, and it will take this and it will do that committee, the votes of each of these learners to do this. That was the voting classifier. Now, beyond voting class, so that was more bagging along the idea of bagging stacking to my knowledge psychic doesn't directly out of the box come with um stacking built-in or maybe people have done that you guys can explore but stacking you have to build on your own so you'll have to create a pipeline in which you take all the white hats from each of the learner and feed it into a final learner are we together you can feed it into a final learner here and get the prediction so that is stacking and stacking is amazingly effective in some sense stacking is a generalization of the voting classifier isn't it what you're saying is i'm not going to directly take the predictions and average it but even if you do something very basic as the weighted average or a sigmoid on the weighted, weighted, the weighted combination of them, it still is maybe a generalization or an improvement upon just directly summing them up and taking the average. And you can get more complex when you put it through a feed forward network, even a couple of layers. It can lead to very good models. Stacking, I would say, I haven't seen it very prevalent actually Which is surprising. Because why would you not do stacking if you're using if you have a lot of algorithms that you have tried at the end of it. Why not stack them. at the end of it, why not stack them and have one giant pipeline in which every single algorithm is contributing. Think about the no free lunch theorem. Every algorithm has strengths and weaknesses. And so create a pipeline in which you do this. So in my case, what I do is I actually have a couple of, you always reuse the work that you have created. So I built a few models, pre-created models in which I have quite a few learners and they all feed into a stacking layer, the, you know, a feed forward layer, right? And so given a problem, right, what I would do is I would first try some simple classifiers and then I would see what is the performance of each of the different kinds of classifiers and then you would do their hyper parameterization and hyper parameter tuning and so on and so forth. And once you have done that, at the end of it, right, what would happen is there would be a competing set of models with very close good performances. The next thing that I would do is I would just stack them, put it into my stacking pipeline, those models. And generally I see some improvement. Yes. So you mentioned that we can use strong learners, right? What if we, what if our strong learner actually overfit the model, let's say the borders of The problem with very strong learners is that, if especially if you see, if you use exactly the same algorithm, right, decision tree, then there is no reason to believe that learner three and learner seven will be very different. Isn't it? Because you're making very complex trees, they will begin to get correlated. If you use different algorithms, let's say that one is using the best support vector machine that you could build, another is making the best random forest that you can build. Remember, those learners need not be individual, they themselves can be ensembles. So let's say one learner is a random forest learner, another is a support vector machine learner, another is something and so on and so forth. You have a whole bunch of learners lined up. Their predictions here are now feeding into a feed forward layer. You're creating a pretty sophisticated learning architecture here. Right? And try it out guys. Quite often it beats just using one. Even though you may say, well, my random forest is already an ensemble and why should I go further and do that? You'll be surprised that sometimes it does give you a benefit, though it is more common for people to put SVMs and this and that and so forth together and make it work. So when you use heterogeneous learner, I would strongly suggest, try stacking them. It is one of the things, I suppose, for reasons that I don't understand, it is rarely mentioned in books. For example, your book doesn't talk about stacking, the textbook. Not many textbooks talk about it, but from a practical perspective, right, what should I call it? I wouldn't call it a best-kept secret, but it's certainly one of the things under emphasized because it is really quite effective it brings a lot of benefits so I have question about the overall accuracy will it not reduce no it doesn't it actually gets better because see that what will the neural net at the end do the feed forward network do? It looks like neural net. Yeah, it is. So it is just trying to figure out what is the relationship between the predictions so that I can massage out a squeeze out an even better prediction. So certainly it won't give you worse results. Depends. See, there's always a tendency to overfit the more complex the model you make. So do these things only if you have sufficient data. Yes, sir. Right, and regularization and all of these things are important. So a boosting obviously comes with the regularization parameter, the lambda, the learning rate sort of thing, with damping factor. It is essentially the regularization of boosting. You remember that in decision trees, what was the regularization parameter? The size of the tree. You had to prune the tree to reduce its size. So regularization is an important topic. Last time we learned about it, I hope by now it's clear. And you see that anything complex you build, you have to have some degree of regularization. In the same way, when you do your feed forward network, obviously put in L1, L2, the LSO or the regularization, put it there. Asif, is this stacking more interpretable than a deep neural network? How was it different? By the time you reach a color, it's the difference between pitch black darkness and degrees of pitch black darkness. That's okay. Okay, so that's what I- So I remember that when I came to the Bay Area, I was driving over the San material bridge with a colleague and because I was new I asked the colleague how deep is the bay here right now my colleague didn't know swimming so he gave a simple answer he said is it deep enough and that pretty much sums it you know if you don't know swimming what's the difference between 15 feet deep and like, you know, 30, 40 feet deep or 50 feet deep. So by the time you reach here, you know that you're making fairly complex models. Interpretability is, you know, very marginally there if at all at this level. So I was thinking, sir, we pick up different pipelines and we pick up the best model and then when we do stacking, we actually trying to introduce noise into our best model? No, we are not introducing noise. We are saying, see, I'll give you an example. In some areas of the feature space, one model may be very good. Another area of the feature space, another model may be very good. Right? Or one model may always be coming out with a slightly optimistic answer of how much ice cream you'll sell. Another model may always be coming out with pessimistic answer. Or one model for high temperatures makes a wrong guess. Another model may always be coming out with pessimistic answer or one model for high temperatures makes a wrong guess. Another model for high temperatures makes a better guess, hotter days makes a better guess. A third model makes better guess for is the best when it is windy and so forth. So all of the models have their strengths and their weaknesses. What the neural network at the end of it is trying to do or the support vector whatever the last layer the stack the the the the algorithm at the end the learner at the end is trying to do is it's trying to learn the strength and weaknesses of the learners themselves yes now that is what we're trying to do so each model, based on the feature set, they have their strengths. Last one pick up the, you know. It observes where the models are getting it right and wrong. And therefore it builds a model to compensate. It just basically says that for this area, let me listen more to this and that area, let me listen more to that. It's sort of doing that, mass massaging that and you do it all the time you know if you if you want to uh learn about the economy you what do you do you don't go and listen to one guy you know that when the times are good some economists will give you a better understanding of the dynamics of the economy when When the economies are bad, some other people will be better. So let me put it this way. I don't know if this thing is appropriate. It's wildly off. But anyway, I'll just say, suppose there's a recession, massive recession. You realize that the kind of people who are... There are certain people who are very very good at bringing an economy out of recession right they are just the right people and you bring them but at the same time they're because they understand risk and dangers and they know how to take it out they may not be the best people to run the economy during the best of times. Maybe, maybe not. There are other people who know how to take accelerate, create accelerations in a good economy or whatnot. I'm taking as an example. So different experts have different strengths in some sense, and you switch between them based on what times it is. If I have to take as a motivating example from real life. In a similar way, think of the last learner in the stacking learner what it is doing is it's tuning into different learners in different areas to different degrees right that's what it is doing so asif so this is this like something like distilling the last bit of intelligence which is left in the model? In some sense, yes. You're basically saying that can I do better by not listening to these individual learners, but by looking at some nonlinear combinations or complicated function of what each of them is saying and will that be a better output better prediction is it stacking let's say for the river data set and the flag data set the three the three sets three bins if we do three binary classifications one for the top one for the river and one for the bottom a flag data set flag like this yeah if we do three binaries like one that predicts one model that predicts the top one model that predicts the river one model that predicts the bottom can we stack those and then in a way in a very informal way that is what one of the people here did it and Dennis you're here Dennis is here yeah I am you in effect did that isn't you're basically built one for the top one for the bottom and then you squash the two together to get you so in a very informal way you did stacking yeah so guys these ideas as you can imagine are very obvious ideas sometimes when you struggle with problems used you come upon these ideas in a natural way you rediscover those ideas it's as simple as that just like Dennis rediscovered stacking in effect and the way he's called the flag data set. All right guys, I hope it's been fun. This is the end of ensembles. So bagging, boosting, stacking three main things. Weak learners, the magic of weak learners. How do you squeeze out a strong learner from weak learners it's been magical isn't it the whole idea that you can do that and you can ultimately get a far stronger learner than just one learner which is super smart you see the wisdom of crowds sort of phenomenon happening here that is all there is to this topic and this is it guys beyond that you can start reading papers or books and so forth uh to deepen your understanding do read the book the book doesn't talk of stacking uh and it has a bagging and boosting random forest and boosting sir i have a question yes so in Yes. So in boosting, the meaning of word boosting, it relates to it eventually you boost the the performance by combining a lot of learners that is a given right so maybe that's the history of the word boosting there's another word if you look at some of the older like for example the ada boost what they were doing is one learner would learn it would make mistakes then the second learner when you give it data you would boost the weight or relevance of the mistake data points more so that it noticed them it paid attention to the mistakes more so it is possible that the word boosting came from there i don't know which of the two is the origin but for our practical purposes if your intuition is that we are boosting the overall performance by growing a whole set of weak learners sequentially, that would functionally be a right approach. Okay. And then when we say gradient boosting, then how does that associate? Like if you in one or two lines, I know that you have explained it in the past but even one two line if you could just try to associate these two words and give some yeah so uh gradient boosting see the gradient word comes from the fact that it's a bit bit of a mathematical term you are in some way looking at the gradient of the residuals right how let's not go into that because that will need us to do the math of data science the word comes from there but in very simple terms what it means is that we are we are building a model see if you are building a model like this r i right the residuals from the previous model is letting us create the model the next model right the residuals that came from the previous model is letting us create the model, the next model, right? The residuals that came from the, you know, Y minus Y I minus one hat, which is this residual plus error. See what you're doing. You're fitting a model to the mistakes of all the previous models put together you know the composite model so far right and how do you do it you do it by using gradient descent right that's right you do gradient thankfully and in a rough way that is where the word gradient comes from. You're doing gradient descent using the residuals. Is there any inherent value in stacking? Cause sometimes like a single algorithm just returns better results, it's unlike the voting results. No freelance theorem always holds, Dennis. No one technique is necessarily always a good idea. You have to play with it and see where it is of value and where it is not of value. And that's true of all algorithms and all techniques. For example, these days it's become very fashionable to say that we can solve any problem much better with a deep neural network. Is that really so? The answer to that is very interesting actually. So we'll deal with it, give you a flavor of the deep learning. So there is a universal approximation theorem for neural networks, which says that any function can be approximated using a neural net like especially even a single layer neural net and so forth can we do that yes we can do that the is that the most efficient way to do that that is where the catch is you can approximate it but is that the best way to do that not sometimes a linear regression is transparent interpretable and gets you as good or better predictions and so forth so we should never let go there two things that people keep pointing out in the new way of doing things they'll keep saying that it can do everything but if you see how they do it they'll point to some expert who did it right who proved it but those guys are very clever at setting up deep neural architectures for you to be able to set up just the right architecture to achieve state-of-the-art accuracy or performance it takes experience and time and quite often you in the time that you invest, it may not have been worth it because another algorithm would have just achieved that. Any algorithm, you have to try different algorithms out, but some other simpler algorithm may have just done it out of the box with very little hyperparameter tuning, giving you the state of the art performance. And that is the spirit. And that's where, you know, these two statements come together. The no free lunch theorem on the one hand, and the other are these universal approximation theorems, which basically say that certain algorithms are universal approximators. You can use them to solve any class of problems and so forth. So we'll come to that. It's a very interesting game. My approach has always been that use the simplest model that will give you a state of the art performance. Because the simpler the model, see these are practical terms. I deal with it in my data science team that I run, the machine learning team that I run. These things don't come free. Suppose you use a very complicated model which runs on a GPU and takes a lot of time. We are raking up a big cloud bill. Obviously the cloud providers, the AWSs and the Amazons and the Googles of the world will be delighted to see the bills that you're raking up. But it's your responsibility, a fiscal responsibility that you find the simplest model that will get the job done with state-of-the-art accuracy why because the training time of that is less the inference is faster the sheer amount of cloud resources that you will use will be less so often it comes down to a very practical decisions like that comes down to very practical decisions like that. Okay, thank you. So when we say the word recommender, typically in your mind the picture that comes is Netflix recommendation for movies, right? Or I don't know, Amazon Prime recommendation for movies or so forth Spotify song recommendations you go to Amazon and they do product recommendations what are what are these recommended systems and how do they do did do that what they are recommending is something very personalized they know you they seem to know you they seem to have anticipated your needs. And they are proposing that you may want to buy this. They're suggesting that these things are of relevance to you. They appear as service, for example, Spotify, you pay a subscription. They try to bring forward the songs that you would be interested in, I suppose, or for Netflix and for these things. If you're at Amazon or one of these e-commerce sites, Walmart, for example, I was just turned Walmart and very pleasantly surprised how far along they have come in their e-portal. So they give you all of these people make recommendations. How do they do that it seems very personalized it is very effective much of the sale that Amazon and these guys make is actually not from people searching for what they want you know suppose you say I am looking for this book on boosting specifically and you go and search for that and get it yes that accounts for part of the sale but you'll be surprised an astonishingly large amount of this sale comes from the recommender system things that they recommend to you and then you end up buying it not now but maybe next time and so forth but by putting it in front of you things that are relevant to you they sort of gradually soften you up and you end up buying it sooner or later that's the value or there's a power of recommender system much of the revenue comes actually from there it tells a large part of the revenue so how can they do that we are are surrounded by personalized ads. These days you notice that these targeted ads, they track our behavior, etc. And then they. Sometimes they can understand what we want much more than we would have anticipated. In one case study someone told me, I haven't read the article in the original myself, but somebody told me that if you use one of the loyalty cards in a grocery store then the data is there sufficiently and that when you enter a store in general the systems have a much better idea what you will walk out with what you'll end up putting in your card then you do you walk in with a list of things to buy, you know, a purchase list, but generally you end up buying a whole lot of different things also. And the machines are able to predict that with pretty good accuracy. So how do these machines figure us out? That's sort of the way we will talk about. We will go back to the original algorithm in this space we'll start from there see initially when people did recommend it in the previous century if you think about it the recommendations used to be made based on either user user similarity it's like you know people of a certain age or a certain cohort tend to buy something. You know, there are stereotypes that the baby boomers are into this and the millennials are into that and so on and so forth. Those are true in broad strokes. They say something about a class of people, a cohort of people, cohort. But they're not individually shaped. Then you can do item item similarity for example you can say that oh this is this person is searching for a computer this computer he looked at so maybe he may buy that or let me suggest other computers that are like it or in the same price range or whatever and maybe you looked at you see some of that happening when you go to purchase a car the salesperson very quickly establishes what your price horizon is right at what price point approximately you would feel comfortable buying things or quite often they'll trap you by saying how much are you willing to pay every month what your monthly payment goal is. Once they have established that, then they would now recommend you vehicles within that range, right? And you get a lot of vehicles of a given type from different brands maybe and so forth, right? Or when you rent a car, it happens quite often. So these are item-item item similarities so for the longest time people were doing user similarity based recommendations or item similarity based recommendations and people were creating context so for example the the joke was that you shouldn't recommend a lawnmower to somebody who lives on the 15th floor. That's exactly the mistake you shouldn't make. So what is it? Once you establish that people, you should recommend lawnmowers only to users who demonstrably live on the ground floor or who own lawns, who are homeowners or something like that. So that's one way of looking at it. You're looking at the users themselves and figuring out the characteristics and doing it. Then came, and so people were doing that, then came an idea that said that, see, maybe when users buy things or watch a movie or listen to a song, when a user interacts with an item, underneath that, there are certain tastes of the user. And items have certain traits, certain characteristics that makes the user like those traits. So the thesis is that these tastes are not written. Users don't hold the taste on their head and say, these are my tastes because those tastes are very hard to quantify. And those traits of the item that makes people that align with these tastes are also hard to quantify. This was observed for example in movies by Netflix. They observed that see movies were traditionally broken down by genre. You have the action genre, you have the you know comedy genre, you have the I don't know murder mysteries and then what is, the drama, the family movies, and so on and so forth. And there are specific genres meant for teenagers and so on and so forth. And I don't know all the genres of movies, but whatever they are, horror and so forth, that seems to be a staple diet with the young crowd. So you could make a box or classify movies based on that. And then the initial traditional idea used to be that users will go for that, right? But it wasn't quite working as effectively as one would wish. And people began to study more the interaction between the user and item, whether it's a movie, it's a song, it's a book, it's a thing that you purchase, whatever it is. They began to study the interaction for clues and they then started a movement called the collaborative filtering which was the study of these signatures to find something deeper and make very personalized recommendations. If you think about it today, for example, when you compare a brick-and-mortar shop and a place like walmart online or amazon online amazon or so forth think of the books a small bookstore will keep the top the best sellers because logically most people will buy the bookseller bestsellers right a bigger shop will keep more of the popular books but then the top end books they tend to have let's say that you have something like this if you look at the popularity curve the items many items are here and their purchase or popularity is like this. It sort of decays, but then you realize that these things have a very, very long tail. It's called the long tail distribution. So there's a vast number of items that have few buyers, but those buyers are constant. They sort of keep coming up. If you observe yourself, each one of you may have an unusual taste of a genre of music or movies. For example, the Indian diaspora here in the Bay Area tends to watch a lot of the Hindi and Tamil and Telugu and so forth movies. If you look at it as a percentage of the United States, that is a minuscule, vanishingly small quantity. Somebody told me that the Indian diaspora doesn't even count as minorities apparently, because to qualify as a minority, you need to be at least a certain percentage of the US population I don't know if that is true or not but so it is so you can imagine that's a population group that is a cohort that is so small that it may not even meet the technical definition of being a minority, its tastes are not likely to be very popular or in the bestseller list. But nonetheless, they are constant consumers of those items. They may not be extremely popular, but they're certainly, they have a loyal following. One of the authors, it reminds me of a science fiction author that I like very much is Ursula Le Guin. I believe she, I don't know if she's still alive, but her books are just absolutely wonderful. I like that way much. If some of you want to read it, read The Left Hand of Darkness and so forth. So to my knowledge, she was never in the bestseller list, or maybe for a very short time. But nonetheless, after so many years, she and her writing still commands a faithful following. It's a small group of people who like that science fiction, but they are still there. So what happens is that if you look at this long tail that is there this tail is very very long right and it is also heavy what does that mean it's a heavy tail very very long heavy tail what does that mean means if you look at the total area under the curve under this curve and if you take the total area to area under the curve under this curve and if you take the total area to realize that most of the weight of the tail most proportion of the weight is actually most of the long tail. And therein is actually the business model of a lot of these e-commerce sites. Why? Because in an e-commerce place, you can have very large inventories. If you think of books, even Barnes and Noble or Borders that used to be, they could have, even with their big shops, they could have only finitely many books. But an online books seller could keep quite literally millions and millions of books in the inventory, or tens of millions of books in the inventory. Today, I'm told that a place like Walmart or Amazon, their inventory, I don't know a Walmart anybody from Walmart could tell me what the inventory size is but somebody told me that Amazon's inventory size the last time they check was close to a hundred million or items yeah almost the same thing by the way, Jai, congratulations to you guys. You guys are doing a very good job. I just looked at the Walmart e-site and I was very impressed with how nice it is now. It's really catching up, isn't it? Jai Mathaiyaadu Das, Yeah, I mean, yeah, the online studies. It's far behind, but still catching up. Sajeevan Gapura, Yeah yeah still catching up and doing a pretty good job i like it is like the old prediction was that it will fade away but contrary to it it's doing well so um anyway so the mass under the long tail is quite heavy and so these e-commerce places they can stock everything and most people actually end up buying things from here, this region. But there is a problem with these things. There's so many things when your inventory is 100 million large, you can't show a list that is 100 million long to a user, isn't it? In any category, there will be way too many items and therefore therefore, the importance of recommender systems. What do you want to do? From this vast ocean of inventory, you want to pick those items that are genuinely relevant and align to the taste of this particular user and show those to the user. Because that's the one sensible way you could have a winning situation the other is of course have a search bar let the user search for it and have us efficient search search is the easy one but once you realize that there is a huge amount of revenue to be made by recommending the relevant products to the user you you see the importance of relevant recommendations, right? Or personalized recommendations. So how are these personalized recommendations made? I will take a historic route. The state of the art is, of course, we do a lot of things using restricted bolts and machine and many, many techniques. The whole field of recommender systems has exploded. There are graph techniques for making recommendations. There are all sorts of techniques. It has become a vast subfield of machine learning in itself. So we won't go too deep or we won't even survey it. I'll just limit myself to one particular algorithm, which was the initial algorithm in some sense, the primordial algorithm. Netflix once announced a prize of a million dollars. It says anybody who can beat us, I believe by 10%, they would give a million dollars to. Because they suspected that the in-house team had done a very good job, but perhaps there was still margin for improvement. house team had done a very good job, but perhaps there was still margin for improvement. So a million dollars is an insane amount to dangle before researchers and they all got together to the challenge and it was more for the prestige of it than for actually the money. Because when you take a lot of very, very high caliber researchers who are themselves being paid insane amount. And they sit down and focus on this problem. The total expenditure in research far exceeds the price money, but it was certainly a grand challenge that people took up, researchers took up. Many teams competed and then they formed ensembles of those teams coming together, techniques coming together. And so, by the way, ensemble was in one of the leaderboard. I think the second or the third or something. At the end of it, there was one idea that sort of stood out and ultimately won. And we are going to talk to the core of that idea, which, and that is what I'll explain. So what happens is, let's look at this dimension. It, out of that idea came an algorithm called the alternating least square. So I'll talk about this and singular value decomposition. So imagine that you have items, I1, I, M, right? Where M is equal to a huge number, let's say a million, just to keep the problem simple, which is equal to 10 to the power six. Then you have users U1 to U, again again n which is a vast number of users where it could be again close to 100 million or something like that 10 to the power 8. actually the number of items these days is also large 10 to the 8. so you realize now suppose you ask this question this question how would how would a user UI rate an item IJ right this is a combination of UI item J what would be the rating of this rating IJ of user UI for item ij. Now by rating, it may not be explicit rating. Let us say that you're just trying to see rating is a proxy for how aligned or how relevant or how preferred is this item j to user i. So now if you you look one way to establish that preference is by looking at explicit things like let's say that uh in amazon in netflix for example it was customary for users to rate the movies they saw so they rate the movies and so somewhere you may see a five star somewhere you may see a two star so forth. Right. Up and down. Do you have the ratings. So that tells you whether the user like to dislike this movie or to what extent they liked it. But, and then the whole question is quite often we watch movies, but we don't read them. name. That is by far the more common thing. So, or songs, a book, purchase things and we don't rate it. Most of us don't rate, don't get the time to rate, for example. So then you assume an implicit rating. You take the average of all the ratings for an item and you assign the default value or you take the average rating of a user and so forth. There are techniques to impute a default value, or can do implicit recommend implicit data. You just say, did the user buy or not buy this thing that is implicit, right? So you can create a matrix. This you want to think of it as a rating or interaction matrix, depending upon whether there is a rating or just the fact that the user bought this item implicitly is a vote for it. When you look at the size of this matrix, you realize that suppose you're talking of a giant e-commerce site with 100 million users and 100 million items. How many possible values there are? There are 10 to the 16 values, right? In this matrix. So for each user, for each of the item, you can ask potentially how much does the user like this item? Or how relevant is this item or aligned is this item whichever what you want to use or to this particular user isn't it so for each user you can give hundred million ratings potential ratings to that users and of course for each item you can have another huge number of a day but let me just make the problem simpler for the time being. Let me just say that there are a million items and there are 100 million users. Just to simplify this. So this problem. So we agree that the rating matrix M, I'll just use M, very would you agree to that it's a very large matrix ideally you you would want data, UI item J, the rating IJ. You would want to know the rating at the intersection of a user I and the item J. For every J, wanted it for all user I and item J, for all user. That means that the matrix is no blanks, but every user has bought every item. Isn't it? Which is not likely to be true. Even with 20 years of using Amazon, most of you would have bought what? About a thousand items, different kinds of items. And that's pushing it. More like a couple of hundred different kinds of items, and that's pushing it more like a couple of hundred different kinds of items. So this matrix is very sparse. It's mostly empty because most users have not bought most 99% 99.99% of the things they have not bought. Are we together guys? Are we making sense? Yes. And the same is true for movies and for songs and so forth. So then it begs the question that in this matrix, what is there? You know that people don't arbitrarily buy things. They buy things for a reason. They watch movies because of a certain taste in songs or movies and so forth. The question is, this matrix is data, is the over data, is the manifest data, right? The matrix is the observed data. Behind it is some explanatory, behind this data, data it we seek because if we can do that then we can predict what things this user should watch or listen to or purchase isn't it what things are resonating with this particular user? So it becomes a machine learning problem. If we can give an explanation or build a model to that. So here is how we do that. Let me sort of give it like this. So what you hypothesize is a space of low dimension. People often use the low rank space, low rank space, which means a lower dimension space. So I'll illustrate it with an example by trivializing it down to two dimensions. Let us say an absurdly low two-dimensional space. So, suppose there are only two factors that matter and the fact that those traits are hidden traits that are not written explicitly on the items and users don't wear billboards or don't wear t-shirts saying I love this. These are something that you that is hidden and that you discover. Those are hidden traits of items. Right. And in the case of movies, let us say that we take the case of a movie, movies, we can say that, let me take some absurd things like this is humor. Let's say that this is how much humor it has. Right. And let's say that how much some people like action, right? I can just take two things. How much action thriller it is. It is, right? And maybe a humor and a lack of humor or the not action thriller, maybe serious movies or something like that. So let's say that some documentary is here right well michael moore can be pretty funny here let's say something like this there's some michael moore movie right do we know who michael moore is who Michael Moore is anyone yes in spawn yeah he's a very documentary maker on the with a liberal bent of mind right or Michael Moore you haven't heard of who should I put okay let's not put that let's put popular names so have you seen the movie dumb and dumber right It's a total farce. So it would rate somewhere here. And let's take one of those Terminator movies. They do have some amount of maybe humor on the positive side, but they are completely action packed, isn't it? So this would be Terminator. And let's take another movie. Let's say, let's take a movie like Sleepless in Seattle. you wouldn't exactly call it action thriller, isn't it? So it's a romantic movie. So let's say that we put it somewhere. It is certainly has a degree of humor. So let's say that we have sleepless in sleep. By the way, am I getting that right? Isn't that a, it's a rom-com. Yeah. Yeah. Yeah. Sleep. Where do you put surely, Yeah, rom-com. Where do you put Sholay? Sholay. Sholay is certainly with the Terminator. Something like that. And then think of an action movie that is absolutely tasteless or humorless. There are quite a few these days being made which are filled with senseless violence and absolutely no humor. So you know, something like that. High violence. In which the only thing that happens is either a building collapses or somebody is killed. Anything by Michael Bay. What's that? Anything by? Michael Bay. Michael Bay. Okay. So let's say that. Something like that. So you get the idea, Kaiser, that we can put a lot of movies like that. And then of course, imagine that there is a absolutely tasteless documentary. I once saw a fairly tasteless documentary that said that there is no global warming. The whole global warming is a conspiracy. I don't know if that is humor or not, but I found it rather humorless and certainly very non-thrilling. So let's say the hoax of global warming. I'm just concocting a movie, huh? The hoax. Or flat earth, sir. just concocting a movie huh the hooks or something like that right a doc documentary something like that I'm just giving illustrative examples. Now think about yourself. Suppose you are a person, you certainly realize that you belong somewhere in this landscape. Now people's tastes change, you know, people have a variety of tastes, but let's say on a given evening when you sit down to watch movies,'re in a certain mood you would fall your taste your preference at that moment your taste would fall somewhere here isn't it so for example you might be very interested in a romantic comedy or something like that so your your taste, like you may say, I want this much. I want something that's quite romantic or something like that and quite humorous. This is where I am. Let's say person one. Another person says, I absolutely like, I just can't stand all this romantic stuff and what you want is just some good action funny action like you know what is that Jackie Chan kind of a movie yeah so that would be somewhere here I suppose Jackie Chan is it still acting I don't know so let us say that you you are somewhere like here i remember that he is negotiating between india and china now but i don't know if he's acting oh really interesting he's become a diplomat yeah like to do the damage control ah there's tension between the two okay so so here so you know you could be like that. I remember when I was a kid, a young boy and you know how boys are they have these games and competitions and who wins and who loses and all of that. When I would watch a movie to me it had to be filled with interesting things you know somebody taking a motorcycle and jumping from one building to another and it was just so exciting to imagine him flying through the air or jumping from an airplane with a parachute and so on and so forth so people have different tastes obviously I'm not 10 year old anymore and tastes have evolved but so here we go we all can be put in this landscape somewhere based on our taste are we together so if you look at this very simple low dimensional space we can say this we can say that each item can be represented let's say I actually let me use a different color to illustrate this. Each item, let's say this item, or maybe this one, each item. So suppose this movie, if I think of it as a vector, would you agree that it has a certain degree of thriller or absence of thriller and a certain degree of humor? Isn't it? It has this much humor and this much negative action, action thriller, right? It's like saying less than average action thriller stuff way less than average so would it be right to say that any item IJ I could write as some amount let me just call it alpha J one that is along the first axis. This is the first axis. Alpha one and it has this much along and let's say that I use the IJK and then alpha j To I can write it as a two-dimensional vector does it make sense right the coordinates of this point is basically this man is alpha J so suppose this is item J alpha J 1 and this much is alpha J 2 does the notation make sense great it's quite simple I hope that what I just wrote. So alpha j1, alpha j2. These are the coordinates of this point, the item j, in this space. Are we together, guys? Does it make sense? Yes. So this is that. So every point, therefore, we can say, let us say, what are we saying here? What we are saying is, is every item has a certain amount of the specified traits humor and action thriller action thriller humor and action thriller isn't it and how much they have each of these I can just write it this is the humor amount and this is actually is the other way around it is the action thriller amount the x-axis and this is the humor amount isn't it guys as simple as that now what about a person let's go and see what is happening to a person let's look at this person this person I are side person let me call this person user you user I right so what do you see just intuitively looking at this person would you say that the user eye is likely to enjoy Sleepless in Seattle? Yes. So the taste of this person is pretty close to the traits of the movie Sleepless in Seattle, but this person might not quite enjoy the documentary the hoax of global warming isn't it it's pretty far away in the in this space in this hidden space or this abstract space of traits the person is quite far from the liking the characteristics of the ho hook so because it says it doesn't have enough humor it might have the action thriller access agrees but the humor access doesn't agree right and this person is very unlikely to like a movie which is senseless violence maybe give me a name guys of a senseless violence movie of a senseless violence movie? The senseless what? There is this meaningless violence, humorless violence. Extraction. Extraction. Okay. No disrespect to that movie, but let's say that it's there. Right? So you realize that this person, UIi is quite unlikely to like this movie they seem to be at the other ends of the spectrum um i said my question here so we are dealing with like over here being a rom-com which is kind of interesting and the documentaries which we consider them to be quite serious but uh nowadays the documentaries are made in some kind of funny way so as to connect with the audience and like you know and we'll come to that so hold that thought in your mind we'll gradually come to that so so it is possible yes your documentary could be humorous and it could be see we have just a two-dimensional space at this moment rate which is a very low dimensional space we will talk a little bit more about it. But you're right. We will come to that. So now user I, I'm sorry, user I, user I can also be represented as, actually, let me use alpha and beta. Let me just, because people tend to do that, let me reverse the notation. Let me call it beta, beta, for no reason, because in the literature, when you look at it, you might find that people have used beta. So I hope this might just arbitrarily changing the Greek letter. Hopefully that makes no difference. This is beta J, beta J2 because it's more conventional people put alpha for this. So suppose you have this alpha. So the alpha I1, alpha I2. this is what the user would be isn't it in other words if we go back here user is given by user i1 user i2 the coordinates of this user right this and this? This is user I1, the x-axis, right? Are both in this particular case in the negative direction. So now you would say, looking at this space, a very intuitive definition would be to look at suppose there is a handle theta between them would you think that would be a good measure? See to what proportion? So you realize the dot product between these two points would be pretty high. They are more or less aligned. Two vectors which are more or less aligned have a vectors which are more or less aligned have a high dot product. This will not have a good dot product with this guy. Right, they're almost perpendicular. And this will be close to zero. And with this guy here, this extraction movie, if they're almost diametrically opposite, so the dot product, if you look at the cosine distance between the two cosine theta it would be practically minus one right in fact it could be what the user does not like minus one and likewise with this so you realize that something like a cosine theta could be a good measure. Theta looks sort of like that. Do we agree, guys? Does it make sense? The inner product or something like that makes a lot of sense. And so that is it actually. What you say, the hypothesis that you make in this space is you say that the rating IJ, sorry, rating that the user gives in the matrix, rating that the user will give, which is a proxy for affinity or alignment or something like that is you hypothesize that if this user was given this item and bought it, how would it watch this movie? How would they rate the movie or another way of buying in in purchase history? You say, if this user bought this item, then what would be, let me use a different color. What would be the rating or the affinity between the two? You say that it is user i dot essentially item j, the inner product between these two. That's one way of looking at it. Are we together? Which I suppose, I I hope I get the details of that paper right so it would be alpha I 1 beta J 1 plus alpha I 2 beta J 2 in this particular space so now this was a two dimension so this space is a it's called the word that people use is latent space because this space is not visible you don't know you discover it through machine learning through a mathematical technique that we will do in a little bit in more practical terms if you look at movies and recommendations you can think of it as your taste or trade space. Those are the traits of the items or the taste of the user. Does it make sense? Now the question is, how do we discover that? That is an interesting bit of mathematics. There is a mathematical way that you can discover it, and you can do it using these techniques of alternating least square and so forth. We won't quite go into that because that is the context of a much larger discussion, but let me just give you an idea. So what happens in reality is that this space is approximately 100 to 200 dimension, 100 to 200 dimension. In other words, items or movies or things like that, or sometimes you can get away with just 10 to 20 dimensions, 10 to 20 dimensions, depending upon your problem, how many dimensions you need, but you realize that even a hundred dimensional space is much smaller than if you look at the user vector you know how many items are there what is the rating of a user for each of the items those are like 100 million dimensional vectors those are huge to describe the user and what they would have done in other other words, that matrix was huge, but this matrix is much simpler. We are saying that there is a small space of 100 dimensions, well 100 looks like huge, but in the world of recommender systems, it's actually a very small, that's why people use the word low-rank matrix. It's a small dimension space. Think 10 dimensions. So let us say that we were talking 10 dimensions. What it means is that movies have 10 kinds of traits and users have 10 different degrees of taste for those traits. You can quantify. And if you could get those right, you can pretty much predict how much a user will like this movie. Now, the dimensionality is more than 10, of course, it's closer to 100 to 200. But that is the basic intuition. And now one of you raised the question that these days people have changed documentaries of humor and so on and so forth. Yes, it's good. Certainly, obviously, you don't want your documentary to just be boring. You try to make it interesting. But there is also another shift. One of the things that has been happening is people are now trying to make movies and Netflix is trying to assign genres, the new value of genres or things like that, based on actually some of these hidden dimensions or these traits in the movies. And so you notice that after these algorithms, the new genre, the new set of movies, they break genres. Have you noticed that? For example, think of a movie like maybe Hunger Games, right? It is a movie in which the young people are taught a lot of things, bravery and loyalty and so on and so forth. So there's a educational aspect for the young adult protagonists in these movies. At the same time, they do have a lot of action and even violence, right? And yet the protagonist, if I am right, is a female character and it totally breaks the stereotypes of the previous generation. If you look back at the 20th century, the old stereotype would have been that what they would like are romantic movies or something like that. And if you make action movies, traditionally you would embed action and violent movies with usually male protagonists. And those norms are being broken. Those stereotypes are now being shattered in many ways because we are realizing that they were assumed stereotypes. And actually, people have much wider, richer taste in things. And so this spills out from a kind of machine learning that is quite interesting. And in fact, in this context. So this is actually the way you do it. It's not that different from regression. It's a form of regression is called alternating least square alter making least squares we will cover that whenever we do the math thing and another way to look at it a singular value decomposition what it means is that given a matrix remember that from the covariance matrix we did the eigenvalue decomposition and we found the principal axes right and a few principal axes were enough to describe the data it It led to dimensionality reduction. The argument is very similar. What is singular, what is eigenvalue decomposition when generalized to rectangular matrices is pretty much the same idea. Instead of eigenvectors, you have something called singular values or singular vectors. And you can therefore, and only some of the dimensions matter, not a 100 million dimensions. Very soon, they cease to matter. And so you can look at it as a fact that the intuition that we gain for eigenvalue decomposition very broadly translates to this problem of items in the recommendation. And they help you find that hidden space. Think of it like that, that if it was eigenvalue problem, which it isn't, it's actually singular value decomposition-like problem. Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam In a very similar way, you get a massive dimensionality reduction in explaining a user or an item in terms of traits or taste, which is representable with a smaller dimension space. And by the way, this is a vast reduction in dimensionality. Imagine that a user vector in the original matrix is a hundred million dimensions, but when you represent the user in this a taste space, it is a much smaller vector. It's a vector in just maybe 100 to 100 dimensions. So if your problem is simpler, 10, 20 dimensions and so forth, right. So it leads to a massive oversimplification. Not only that, it gives you insight into why this user possibly, it gives you a hypothesis on why this user bought this item in terms of traits of items and users taste for them. So that folks was a taste of the recommended systems. We are at 10 o'clock. I suppose I should start summarizing. I would like to summarize not just what we did today, but the whole workshop. So today we did boosting, which is a form of ensemble methods. We did stacking, which is how you can take a lot of learners and then extract the last use out of it by stacking them together and seeing if it helps. Then we moved on to a new topic, which is the recommender systems and try to understand how the initial versions of it work. The state of the art has moved forward. We still use this quite a bit, but more the field has exploded there's more to it now these days that is that in this workshop guys we started and covered a lot of topic if you look at that we covered we started with the no freelance theorem which says that no one algorithm is always better than the other algorithm always like they all have a place under the sun. Then we did feature engineering. After feature engineering, we went onto ensemble methods. First we did decision trees, then we did random forest, then we did gradient boosting, then we did stacking. We did kernel methods. We talked about a nearest neighbor with kernels. That's a distance kernels. And then we did, so those kernels are not the kind of kernels we're talking about. The word is sort of slightly varying meanings in different subdomains. Then we did the other kernel as an dot product in the other space. Remember that you went from, suppose you have a vector X and Y. So, or I tend to use X and X prime, two points, X prime. So the kernel of this is actually the dot product. If X goes to, by some function, becomes phi of X in some other space from x and x prime goes to let's say the same function phi takes it to some other place phi x prime then the dot product in this space is the dot product in that space is the kernel now couldn't say quite interesting because it turns out there's a kernel trigger there's a mechanism by which you can actually come back with the dot product without necessarily fully specifying or discovering what phi is. This leads to a lot of very interesting problems getting solved. We use the concept of support vector machines, which are the whole concept behind support vector machines is can you, what is the widest river that you can flow through the data that would separate the two classes as in the data now when the decision boundary straight well of course you can get away with just flowing a straight river you can allow for some mistakes so if you don't allow for mistakes it's a hard margin classifier. If you allow for some mistakes, it's a soft margin classifier. And you can have a cost or a budget for how many mistakes you allow. Now the points that are on the banks of the river, these light houses that you have that help you navigate, or even the mistakes that are in the river itself, the trees and the houses, for example, we took as the two classes, so the ones that are floating there. They all form the support vectors because that's all you need to specify your reverse banks and through the center of the river is obviously the decision boundary. That was the intuition of the support vectors is a lighthouses to help the boatsman navigate. Now when the decision boundary is highly nonlinear then comes to the rescue the fact that every nonlinear surface can be represented or let's say curve can be represented as a linear surface in a higher dimensional space how high arbitrarily high sometimes infinite dimension but not always or one of the misconceptions people have is that it's always infinite dimensional space and here there's a bit of a terminology will do it in math people say it's the hill version trick go to infinite dimensions well many things are odd about that statement it's not like that but just remember it the way I explained it to you it is going to a higher dimension though the high dimension is something called a Hilbert space. What is a Hilbert space? A Hilbert space is a generalization of a vector space where dot product is preserved, a space where dot product is still preserved. The Hilbert space more formally applies not just to real vectors, real valued vectors, but to complex vectors. And so the dot product, we start getting into complex conjugate and so forth. But for real values, just think of it as an inner product or the dot product. And we are basically saying is a space that preserves the notion of a dot product, roughly speaking, is your Hilbert space. It's a generalization of the notion of vectors, the ordinary vector space as we think of it as in the euclidean sort of vector space right and the hilbert space may may be finite dimension maybe infinite dimensional right so people often think that hilbert space is always infinite dimensional unless i'm mistaken that's not true so there's that anyway we won't go into all of that so you have the ability to take data into a higher dimension where it is straight. Once it is straight, now all you have done is you have gone to another space where once again you can flow a straight river through the data. And therefore, you can create the old theory of maximal margin classifier is applicable. margin classifier is applicable. The journey from here to there has an interesting bit of mathematics. It's the use of constrained optimization and dual spaces, the dual problems and dual spaces. That was actually one of the things I was looking forward to explaining to you in the math of machine learning. But whenever we do it, you will get to that. It's a very interesting and beautiful explanation so that was support vector machines then we did regularization regularization is the way that you prevent overfitting to data there are two ways of addressing overfitting to data one is just get lots of data then even a very supple very flexible or complex model will nonetheless behave it won't oscillate it will fit to the data in a meaningful way it won't have high variance errors overfitting errors because lots of data will tame it generally lots of data is hard to get especially in high dimensional spaces these days we talk about models which have quite literally 100 million parameters. I mean it's just mind-boggling to even think about it. So any amount of data that you bring is probably not enough. So what's the alternative? When you do have a situation where data is finite and you have a lot of complexity in the model, you use something called regularization or what statisticians also call shrinkage. The chapter in your book is called shrinkage on regularization. Though in the machine learning community, the word regularization is far more common. So to review that, obviously go read the shrinkage chapter. In shrinkage, we started with the concept of to motivate it. There are many ways of doing regularization. For example, in decision trees, pruning is regularization. In boosting, this damping factor learning rate is regularization. But more broadly, when you look at linear, for example, linear models as an example, or even non-linear models, one of the ways to look at it is example linear models as an example or even nonlinear models one of the ways to look at it is you say that parameters will explode if you don't regularize it there they will the error surface will achieve its minima far from the origin in the hypothesis space so each of the parameters will be huge and you'll have massive oscillations in your model high variance overfitting so how do you solve that problem? You create a safe zone, a safe sphere around the origin. You decide how much area or how much volume you want to consider safe. And now you say that I will only search for the minima of the error function in this region, not outside it. So you have constrained, you're saying I'm willing to go only so far from the origin in the hypothesis space. In other words, more practically, I'm going to let any of the parameters grow only to a certain extent and no more. When you do that, this process is the regularization. Now, you typically take a unit circle whose generalization is a hypersphere, sphere, hypersphere, around the origin. Now, the shape of the sphere, it depends on how you define the distance. When you think of Euclidean distance, you obviously agree with the intuition of a nice round sphere or circle or disc, solid disc. On the other hand, there are other norms and that brought up the concept of the Minkowski distance. Minkowski distance is a generalization of Euclidean distance. One example of that was the Manhattan distance, the taxi driver's distance. If you use the taxi driver's distance to draw the unit circle, it will actually look like a diamond. And then other shapes ensue. And those are called the norms. L is equal to one. The first norm is the Manhattan distance. the second norm is the Euclidean distance. And they can take fractional values. You can take L is equal to half norm and so forth. And you can take L is equal to infinity norm. It's a generalization of distance notions, the Minkowski, Minkowski's thing. And they all of course respect all the properties of the distance measure, namely the positive definite and they honor the Schwartz inequality and so forth. In other words, the shortest path between two points is the shortest is shorter than any detour path that you take through some other point Z. So between X and Y the straight path is shorter than the path from X to Z and Z to Y. And that sort of thing we all respect that so uh using minkowski norm if you use the first norm you get lasso regularization if you use the second norm to draw your unit in your circle you get the ridge regression and so forth you can mix the two up or take the benefits of both and it becomes elastic net lesson gives you the benefit that it can lead to potentially dimensionality reduction which doesn't which will dampen the factors that don't matter but won't eliminate them but less so does eliminate them the downside is of course if you're not careful it may it may dampen or get rid of too many features features that even mattered so you have to do a good hyper parameter search with cross validation. At the same time, Elastic net idea is that let's take the best of both and work with that. And then it tries to do that. And there are many variants of it and so on and so forth. A regularization is something, especially the L1, L2 regularization, is something that is not just of all models. So as we go into deep neural networks, deep learning, you will see that we simply always use regularization because the parameter space is essentially ginormous. Even with the basic linear architecture, we'll come to all of that you'll realize that we are talking about things that you regularize things with or any debts if you create a dense layer let's say of hundred notes hundred notes in the first layer hundred in the second and in this that you are already looking at hundred times hundred that is 10 000 plus 10 000 parameters so massive number of parameters so the parameter space is quite literally ginormous the regularization comes to the rescue it's very useful so all right guys so then finally we did a bit of about the recommender system i give you you a taste of it. Well, just as our way to use, we use the taste space to explain it. It was just a teaser just to get you motivated and interested. Whenever we do the math of machine learning, we are going to cover that in great detail. We'll do bi-shell learning, we'll do all of that and back propagation and so forth. So guys, our road forward is this. We still have a couple of labs we will do. The theory ends today. I will elaborate these things into the labs. We'll still have our Saturday quiz. So I will still do all of that. So we have three more sessions at least. And of course, I'll continue to help you guys with the project. For the project, I would say that use this month till the deep learning workshop starts. Try to do the project. By the way, the workshop doesn't end. I will still hold periodic project clinics in which you can come and show me the progress on the project. So do try to finish the project guys. Don't give up on it as a team and I will still be guiding you for another month or so. Or longer if needed, but do the project. So with that, guys, that's the end of our theory. I hope you had fun in this long, pretty long, I would say, intellectual journey that we covered in a brief six weeks let us do the lab on these topics uh the ones that we haven't done lab on and let's do the quizzes in the coming days anything else guys any questions no sir thank you so much thank you questions so is that also true not by the same technique maybe but also true for this example let's say there's some producer movie producer and he wants to gauge which genre would be the one that the majority of the audience would like. Maybe this may be a question of seasonality or this may be a question of what's happening in the environment or in the country, something like that. See if you project all the users into the taste space, right? So then you can take a certain volumetric subset and ask, how many users do I have? What is the concentration of users there? So you get a sense of the market that you have there. Right. And yes, you do that. I mean, see, since these machine learning algorithms came, think about it. Since in the 21st century, you do that? I mean, see, since these machine learning algorithms came, think about it, since in the 21st century, have you noticed that the movies are dramatically different from the sort of movies that used to be made in the 20th century? And it isn't just that people's approach to movie making, I mean, see, it hasn't organically happened just from that. It has also been quite a bit influenced by all of these things that have come about all these   .  .