 We covered gradient descent last time. We realize that when you do gradient descent, it gradually takes you to a minimum. There are a couple of interesting questions that come up. First of all, why is it that you don't use, for example, just take the derivative and set it to zero and so forth. It's not feasible from a computational perspective. In low dimensions it's possible when you have a huge, huge data sets and high dimension spaces, it's not practical. Gradient descent and the whole family of gradient descent are the practical choices. Then we will discuss today. So the topics that we'll cover today is I'll assume that you understand gradient descent. I'll remind you what it is. We'll talk about the loss landscape. Then we'll talk about something called regularization, which is an important topic. We have covered it in the previous workshops, those of you who attended it. But nonetheless, we'll do a refresher. And for those of you who are new to this topic, it is an important topic worth paying attention to. Once we have done that we'll focus on, we'll see how the loss surface responds to these things, to regularizations, various techniques of regularization, how it responds to, like how does it behave, what does it look like. So there are a few videos that we will look at and lastly we'll do back propagation or maybe yeah we'll do back propagation. So we have quite a few topics to cover today. So I'll start with regularization. Regularization refers to something quite interesting. See, oh by the way I don't know if you are seeing my screen yet. Probably not. Not yet. How about now? Yes, regular. So we'll talk about regularization and what it is and how it can help us. See, what happens is, suppose you have two data points. I give you something like two points, and I ask you to fit some function to it. You could fit all sorts of things to it. You could say, this is one function, this is another function, and this another function that I could put it is. You realize now between A, B, and C, you would notice that there's a progressive increase of complexity. Now, which of these is the right function? So it is always a question, which of these should we take? Now, this question is hard to determine when you have just two data points. But imagine that you have lots of data points. You have data points like this. Something like this. So now, or maybe I'll decrease the number of points here. Suppose you have data points like this. So you could say that I could fit a line like this. Intuitively you would like to believe that that is the best line. So why would you say if you think about it why would you say this is the best line, best function and you would say that is superior to let's, another function that goes like this. Right, and let me move this white point here just to illustrate this point. Or illustrate this. Now you would say that intuitively you feel that the simple simple blue line is the best and the reason you think it is the best or you can check that it is the best is by a simple measure you take this is your training data data but suppose you have data too. So suppose this is your test data. Let me just mark it with crosses. Some crosses here, some cross here, cross here, cross here. So So these crosses are test or validation data. Now, what would happen if you look at the total error, the sum squared error? Or average sum squared error, whatever you want to look at. But let's start with sum squared error, whatever you want to look at, but let's start with sum squared error. You would realize that the white, the blue points has smaller residuals, you know, this, this, this, this, this. Do we see that guys? That the blue points, I'll just circle the test data so that it's evident to you. The blue points have relatively small residuals. What about the yellow points? Let's look at the yellow points. This yellow is going, let's say, like so the yellow residual would be this much right the next residual of this would be do you see how big the residuals are with respect to the test data this residual would be well this hardly looks like a straight line but imagine it is and these residuals would be this much and at this point residual would be I'm cheating here a little bit I can't draw the straight lines exactly but what do you notice on test data the residuals of yellow line, yellow curve are cumulatively much bigger in size, right? Isn't it much bigger than the blue? So ultimately you say that that model is best, which makes the least error on test data. So from that perspective, you would say that we should have stuck with the blue line. But that is a little bit counter intuitive because as far as training data is concerned and let me actually let me mark the training data with a different color so that we make a point quite clearly this is your hang on this is your training data um where was the training data points? If you look at it, which of these two curves fits the training data better? Let me leave this question here. which of these two blue versus yellow curves fit the training data best? Guys, if you just visually observe which of these is fitting the training data more accurately? The yellow one. The yellow one, right? So this is a paradox, yellow. training loss training laws least or yellow but s loss which is what really matters these four blue so what happened if you're training and you're not careful, I mean, if you, what you can do accidentally is you can fit a very complicated model and it will go and fit exactly to your data. Are we together? When it fits exactly to your data it not only has fit to the content the information content in your data the generative force behind the data it has a fit to a large extent to the noise in the data it's very susceptible to noise so this is called the problem of overfitting because what you really wanted was the simpler model, the blue model, the blue, you want that curve and therefore that model which gives you the least error on test data. So this brings us to this, a bias-variance trade-off that we talked about in the previous class, the bias variance trade-off was that as you dial up the complexity, so for example if you're talking of polynomial degrees, which is which makes it intuitive, you know, degrees, how many degrees of polynomial you're taking. So what happens is as you increase the degree, so increase the complexity of your model, then your loss, if you look at the loss, it's really interesting. The loss for, total loss for training data is, it goes like this. The more complicated your model is, the more it tends towards zero and it will achieve zero. Quite literally achieve zero. On the other hand, if you look at your test, I mean the test loss and let me use a color, maybe I'll use red for test. Test loss, you will observe something very interesting. It tends to go like this. And let me give these names, a test loss. And the green one is, as a recap, this is the training loss. So what it means is that there is an optimal level of complexity at which you should have stopped. This is the place where you should have stopped. And we always search for this, this point. So how do we find it? It's very difficult because if you only listen to the training data then the more complex you make as you can see the green line the better your model fits your last function keeps decreasing so you need some way to tell it that you know what you're going to you're making models that are unnecessarily complicated you can get into trouble now in this context people talk of two kind of forces at work. There are forces that come, errors that happen because of oversimplified models. So let me write those things. A bias errors. This is just a review of what we did I believe early on. Bias errors caused by bias errors caused by overtly overly simple model compared to the ground truth are we together on the other hand, the variance errors, they arise caused by overly complex models. So bias errors are because your model is not flexible enough. Suppose the data is truly like this. Suppose your data is like this and you build a model which is a straight line. Then the best straight line would be this if you remember in data set 2 we did this exercise in ML Henry you you get a line like this your best fit line but this model is under fitting because it is It is simpler than the ground truth. So it suffers from high, overly complex, it overfits, more complex than the ground truth. Truth. And so this one suffers from variance errors. Variance errors. So it raises the question, but how do we know what is the complexity of the ground truth? How would you know that? And the answer to that obviously is a priori. That is, before doing the experiment, you don't know. Before doing the experiment, you don't know. Before investigating the data, you don't know. A priori, you can just make some hypothesis about the complexity of the data. So then the question arises, does it mean that I need to keep dialing up the complexity and keep track of the errors? If you look at the test error, do I need to keep building model after model after model and keep looking at the test loss and stop here once I notice that the test loss is beginning to climb back up, then I stop right here and say, all right, this is the right complexity of the data. So that is one approach. There is another approach that people use, and that is the approach that is the topic of today. It is interesting, actually. It's almost like having the cake and eating it too. What it says is, suppose we could, and let me take this example and repeat it a little bit below. Hang on, where's my mouse gone. I will reproduce this thing here. Once again, I'll put the data. The data is like this. Now you know that an underfitting line would be, just to reproduce what we said, an underfitting line is this. Well, it's not looking like a straight line. This is your underfit. Maybe I could do better. There is a shape thing somewhere here. Yeah. Line. So your best fit line would be like this. Then your overfit line was when we talked about overfitting, I said that it was something like this. And this is an, I'm deliberately exaggerating it, but actually this is what, no, maybe I'm not actually it gets worse than this or if you over fit so this is your over fit line so you ask yourself what if we could take a complex model by the way you could count if you're doing polynomials how many degrees of polynomial it is how many bends does it have by the way why did it become yellow? Yes, so I'll make it, give me a moment guys, I will make it blue. So this blue, this line was supposed to be blue. So this is simple. So this is simple. Simple. And this is complex. I'm sorry. This is, let me use the right color for the sake of numbers, complex. Now, you asked this question that this is 1, 2, 3, 4, 5, 6, 7, 8. So this is a polynomial of degree. Do you remember, guys? Polynomial of degree. 8 or 9. Yeah, polynomial of 9. It is because of polynomial of degree. At least. Anything less than that won't do. Yeah, eight bends. Right, eight bends. So it is a polynomial of degree nine. So the question that you ask is what if we could have eight bends but those bends did not have oscillations? That's the basic question you ask. Can we have, let me, this is too hard, can we have a model that is, it is already to have eight bends so long as those bends are not harming us. So can we have all those bends, but, but we dampen the oscillations. This is the question. So suppose I made a model that was like this. Let me build a model and I'll still give it eight bends uh one two three four five six oh no not this one sorry i take this back this was a terrible mistake all right this is not the line that i want to follow let me let me get back i made a mistake guys in my thinking so let me draw the old model I made a mistake guys in my thinking. So let me draw the old model here and the yellow line. I'll do that. I should be following the data, not this. So suppose you could have. Egg bank. Suppose your model looked like this. What about this? Do you notice that there are quite a few bends in this? Maybe eight bends. I tried to get it. Let's see. One, two, three, four, five, six, seven, eight, nine. Well, I overdid it by one. But all right. Suppose you got a model like this what can you say about it do you think that it suffers from high or errors variance errors that on test data it will do poorly yes why would it do poorly on testing if you have to compare the the blue model the yellow model and the pink model, which of these is the best? Which model? Which model on validation or test will do best? And so somebody said pink. is there a contrary opinion yes because it's over yeah but if you look but it seems to be let's look at the residual remember the proof of the pudding is in the heating residue think about in your mind which of these has big residuals and which of them has the smallest residuals? Yellow has the big residuals. Yeah. And what about blue? Blue also has big residuals, isn't it? So to illustrate that, if you look at the blue, let's say that this is a test data. So blue has residuals like this. Yellow has residuals like at the same point, yellow has residuals like this. And what about the pink one? The pink one has residuals only like this tiny residuals. Take any other point. Let's take this point here. So here, this point, yellow has residuals like this. Blue seems to have even bigger residuals. And what about pink? A pink line seems to have residuals like this. So in general, which tends to have the smaller residuals? the smaller residues. Guys, which of them? Pink. So therefore you expect that the pink model would perform best. Right? So therefore you, so let me write the conclusion down from this. Looking at this picture, Looking at this picture, diagram, we see that the pink does best. Except that if you look at the pink, it is pretty complex. Isn't it? Pink is complex, but we have dampened the oscillations in pink. So we learn a lesson from that. We learn that if somehow we take a complex model, but we can dampen the oscillations, we actually might end up with the best model. Isn't it? It doesn't matter. We can start with a complex model, but we just need to dampen the oscillations. So far so good, guys. That's the lesson to learn. So with a relatively complex actually, so long as we know we can dampen the oscillations. That's the lesson we learned from this particular example. Now I want you guys to think a little bit and make sure that we got this point. This is an important point, guys. Any questions before I continue? As of these white dots, we're training or test? Oh, just assume that some are training and some are test. Okay. So the white dots are the ground truth. So some proportion of them are training data and some are test. So the white dots are the ground truth so some proportion of them are training data and some are test data. Infusions guys. So this regularization, so this process of dampening the oscillation and not overfitting is called regularization. This dampening of oscillation, this dampening. And in the statistical community, they also call a shrinkage. What are we shrinking? The oscillations. We are shrinking the oscillations. Isn't it? So that's it. So now how do we do that? So there is a, there are quite a few interesting techniques and in the next hour we are going to learn about these techniques one by one. But before I do that, let us, let us ask this question. Do we really need it when we deal with deep learning? Are deep learning models complex? Who would like to answer that? Yes, sir. It is. They are very complex, isn't it? You have a lot of, you have layers. So what do you have in deep neural networks? Suppose you have an input in deep learning. In deep learning, what you have is how many layers do you want to take? Let's say that you even take three layers, right? The example code that I gave you had three layers. So suppose you have input x1 and x2 or whatever, and then it goes into the first hidden layer of 128, let us say, and then the second hidden layer of 64, and then the third hidden layer, which is smaller, let me make this a bit smaller, a little hidden layer. This was the example that we took. And I believe the second, third hidden layer, we took off 16. And then finally, I suppose we are doing some sort of, I think I took a single variable. Let's take this. And there was some output. So between these, these are dense layers layers right 128 times 6 64 and then 64 times 16 and then 16 times 1 these are just the number of weights and then here you start out with 128 weights right here so you realize that you have hundreds of weights I think it all adds up to approximately 500, if I remember right, or more than that, some thousands. If anybody has your lab notebook open, you can check how many parameters it comes to, not to mention that each of them have biases which are also effectively parameters of the model. So what are the parameters of the model of a neural network? Parameters are weights and biases. And have a large space of weights and biases. So one thing you can be sure of is that you are looking at a very, very complex model. You definitely run a huge risk of overfitting. See, overfitting, one way of thinking of overfitting is your data has memorized the trick. I mean, your model has essentially memorized the training data. It one analogy I can give you is in, I think this is a little bit off track analogy, but it might work. So in, when I was in college, in the IIT, then I was studying for some subject, right? And I studied the book back and forth and so on and so forth. And then the exam happened at the end of the term, exam happened. And then I realized that, oops, the guy who got the highest was obviously not me. It was a person who I had never thought really had mastered the subject. really had mastered the subject. So I asked him that how is the world I saw you having fun the whole time and how is it that you managed to get do so well in this class and he gave an interesting answer he says you know it is pointless this professor tends to repeat his questions year over year and if you know the questions of the last two, three years, if you have the question bank, you can answer it. So he said that in the beginning of this semester, he just got the question bank. Then he went to everybody that who he thought was a guru of the topic from the previous matches. He got the perfect answers. Then he wrote it out neatly and memorized it. And sure enough in the exam questions came, the questions that came were from that and he got a perfect score. So let's think a little bit about that. Does he know for each question, did he answer perfectly? If you think about it, yes, he did answer perfectly. It is very much like a model, the yellow model. It goes perfectly through all the training points. All those, if you look at this curve here, the yellow line goes through, the yellow curve goes through all the training points. So the, you have memorized, it somehow sort of has memorized the data. And so it will always make perfect predictions on the training data. On the other hand, has it learned anything? To learn is the ability to generalize from specifics to other instances. Generalization requires implicitly that you have understood the concepts. What our classmate had not done is actually understood the concept and he didn't like the topic. I think it was a perfectly legitimate way of getting around and finishing a subject he had absolutely no interest in. But while his method was logical, learning was not his goal. If learning is your goal, then the only way you can test whether somebody has learned or not is to give new data, new questions that have never been given before. And then you would have an entirely different set of results. So in neural networks, you have the same issue because there is so much flexibility. If you give it a finite amount of data, small amount of data, it will just go and completely fit to the data, huge risk of that. And so it has not generalized. It will make perfect predictions on tests, on the training data, and it will make terrible predictions on the test data or validation data. Asif, I have a question. So one assumption with regularization is if we start with a relatively complex model, what if we start with a simple model? Do we still need regularization if we don't see overfitting? No, actually, if you start with an overtly simple model, you're essentially screwed. You can't improve upon that. But if the simple model is accurate enough for the application. If it is a simple, and that is a good question. See, if this line that you see here, the blue line, if it is good enough for your purposes, then you have solved the problem. Remember the old statement, if I may repeat Boxer's statement, which was, this is an appropriate moment to say that all models are wrong but some are useful right box the great data scientist as you would say today statistician the box made this famous statement. Let's see, whatever model you make ultimately is wrong. Asim Kadavig, Dean of Students & But if it solves the problem. So, for example, in practical real life if it is useful. The blue model is useful, then who are you to argue with that go with it. Asim Kadavig, Dean of Students & Are we together. with it are we together yeah I was thinking from a point of view of you know especially with deep learning models as more and more resources are required to run you know deeper and deeper neural networks yes and if you know relatively simpler neural network can solve the problem with slightly lower accuracy, would we still want to have regularization in that case? So I'm guessing as long as... Yeah. Nisarg, you have hit an important point actually. So today we live in the world of deep neural networks. One of the things that I always tell people to do is before, suppose it's a classification problem. I asked them, did you get a baseline accuracy? Remember, how do we get in accuracy? A baseline is just counting the majority class. Now you have to look as a lift from that and I always say that go from simple to complex. Hawkins raised a principle, the simplest effective answer is the definition of correct. In science there is no such thing as correct. We do not know whether Einstein's theory of relativity is correct. That question doesn't make sense. In science we can only falsify or disprove a theory. We can never prove a theory. So what are we doing in science? We are searching for what is the definition of correct. The definition of correct is that which is simple amongst all competing, which explains the data. Are we together? So in the end, I believe the accuracy of the model will still be based on how good is your test accuracy compared to your- Absolutely, yeah. So the test results, the validation results and test results are king right remember that training is suspect because you sus because overfitting will take place and with complex models like deep neural network you will never quite get rid of overfitting unless you bring in a vast amount of data. So this is perhaps a good moment for me to tell you something in practice that happens. See what happens is because deep neural networks are complex models, if you look at the accuracy, let's look at this, a data set size, data set size. If you plot it along this axis and you plot some model performance right so model performance can be precision recall accuracy some squared error whatever you measure performances so i'll just use the general word performance of a model of algos and I'll just use the word algos what you notice actually is that classical algorithms the simpler algorithms they actually achieve a situation like this they all rise up on any one algorithm that you take will achieve a situation very similar to this that they reach a point of saturation. Do you see that this is a saturation point and beyond this some point of data set size, d-size, at which, beyond which, performance of your model does not go up. Right? Now what we observe with deep neural network and the reason we are so fascinated by it these days is that deep neural networks go like this. They actually underperform in small data ranges, but at some point, and they keep going like this so what do you what do you notice the growth slows down but it still keeps growing the performance keeps getting better and better for deep deep learning models so if you are blessed with a lot of data deep learning models tend to overperform tend to outperform other models why simply because they are tremendously complex right there's a lot of parameters that weights and biases that you can fiddle around with but in this performance is it against the testing data or it's a it against the testing data or is it training? Against the testing data. So let me put all those caveats. What I'm saying is, so we observe performance of algos against test data. So that is a very good point, Hannah, that you brought up. So we observe in practice that with growing data, deep learning models tends to outperform other less complex models. This is an observation, but you can observe something else. This story doesn't come with its own warnings. What about this region? Let me highlight this region. What about this region? Should you be bringing out your latest and greatest deep learning toys when the data sets are small? No. No. You can't blindly assume that deep learning will outperform other models. And you don't know what this cutoff point is. So you should always start with simpler models and gradually dial up. So let me give you a practical thing that I do. Suppose I have a classification problem and this is something I taught you. What you do is start like this process. A, the baseline classifier or the zero R classifier, baseline R zero R classifier. What will this do? It will give you a performance that you get even without learning data has imbalance the target class will have imbalance of the classes right and one example is what if you have a test an algorithm that tells whether somebody if you have a test, an algorithm that tells whether somebody does or does not have a lung cancer. So if you just declare most people to not have lung cancer, you'll be pretty accurate, right? Because most people do not have it. Let's say that the incidence of lung cancer in population is one in 10,000. So what does it mean? Your baseline classifier, which doesn't look at the data but declares everybody to be healthy, will still have 99.99% accuracy. Isn't it? And it will fail for those cases that really matter. The recall will be terrible as a measure, but it is there next what you do is build a simple model like the one-hour model you know one-hour model which means that take only one best predictor and see, see what happens. And the corollary to this, I often do. Let me just call it B prime. I would build a random forest of stumps depth is equal to one trees. So what happens is that you create a random forest in which one particular tree will randomly pick a feature and it will make predictions based on that. So it's a little bit better than one R because it gives every tree a chance. It's the next level of sophistication. Once I do that, generally, I will try out basic algorithms like naive bias or the second step would be I would try out logistic regression. Naive bias with Gaussian of course, naive bias etc. I would try out something simple, right, log just taken that subsumes a soft max I mean if it is a multi-class as you so you would do that and you would write down the the performance so you know performance of a performance of B B prime performance of logistic. And now you have a benchmark. Go and do your neural network. And if nothing else, you have a guided path. You know what you're competing against. Are we together? So if your data set is not sufficient what will happen if you're in this yellow zone what do you think will happen anyone could guess the simpler model will be better yes what you notice is that the deep learning models are not outperforming the simpler models. So that gives you a hint that maybe this is not the situation for deep learning. We don't have enough data. And so you could try other things. You can keep dialing up. Now would be the time to go to SVM or something like that. Or XGBoost. Or boosting. I mean, I use XGBoost in particular, CADBoost. I don't want to go into implementation, but some boosting algorithm, boosting, et cetera. You could try all the things that you learned in ML 100 and 200, and you might have a successful story there, because you're in the yellow zone. And that is something to remember that even if you regularize and dampen and do things, if the data is a simple parabola, the last thing you want to do is have a deep learning model come out with something like this. Even though it has fit, it has fit less well than let's say simple polynomial regression. That is a lesson to learn. Now, the other thing is, these things don't come without computational costs. Typically a deep learning model to train takes minutes to hours, to days. A traditional algorithm these days on SVM stick can take hours or days, but almost all other things, they converge pretty fast. You can get a baseline understanding by just taking a sample of the data and running with it. Now, there is this other region at the very bottom. When the data is really sparse and entirely different set of things shine, actually there's a class of algorithms called the Bayesian methods. Bayesian learning. Bayesian learning is in general a good idea always. In other words, whether you have less data or more data, you should take a Bayesian approach. It's a topic in its own right. And we won't get into that topic. But one of the things is, Bayesian approach certainly works very well when the data set is sparse. The reason for that is Bayesian includes your experience, the domain experts experience. It is called the prior. You start with an informative prior hypothesis and then from there you start learning. It is like, let's say that you lost your keys, which seems to happen to me practically every day. So I speak from experience. So not using a Bayesian approach would be to scan the house from one corner to the other, up and down, and look for the keys. A Bayesian approach would be saying, let's take a prior, which says that the keys are probably in my office, for example, where I'm sitting at this moment let's start searching from here around this table now common sense would say that's a good approach isn't it so that is including a domain expert since I'm an expert at losing my keys you're including my experience into the prior and starting from here. Do these scale well beyond sparse data? The Bayesians, they do extreme. The only thing with Bayesian is that see deep learning is computationally intensive. Bayesian can also be a little more intensive than other models, but Bayesian, see we live in the world of computational plenty so to do markov chain monte carlo and all of this things that we do and it's variants that we do today is not a big deal we can easily do it in italy right and if you go and look at the kegel you'll you'll see that there are literally data sets which warn you that they'll be overfitting. And I suppose they're just waiting to see how many people know Bioscience. So maybe it's a hiding strategy. The people who use Bioscience, they will reach out to them and try to hide them or whatever. I don't know. But certainly the winning leaderboard is filled with the Bioshoots. Asif? Yes? Can you recommend some algorithms in Bioshoots? Oh, Bioshoots. Okay, guys, see, here's the thing. I can. See, the Bioshoots is, they're all variants of the, you start with MCMC, Markov Chain Monte Carlo, and go from there. Let's put it simply like that. And there's a pretty rich literature on that. I offered a course purely on Bayesian. Some of you have been through a few workshops or sessions on Bayesian. Those of you who did engineering math, you remember we did quite a few classes on the Bayesian or sessions on the Bayesian. Anybody of you who did engineering math. You remember we did quite a few classes in the by Shin or sessions in the by Shin anybody here was taken engineering math and being through the by Shin stuff. Yeah. Srinivasan Parthasarathy, Good fun. Srinivasan Parthasarathy, This one day. So, is a different thing. But anyway, this was a big digression from our Srinivasan Parthasarathy, Regularization topic. What I Srinivasan a regularization topic what i have a question regarding regularization so you're saying we have to decide on the application of regularization based on the performance uh matrix right no no no i'm not saying that i'm saying that if you use a complex method method must irregularize it see one so there is one practical way to know, which is quite simple. See, look at this curve here. Look at this. When you have a complex model, do you observe that your training data, when you have made a model that is too complex, what do you observe? Your training loss is much smaller than your test loss. And to build a model and you notice that your training there is a significant gap between your training loss and your test loss or the test performance. The performance of your training data is very good. The performance on the test data is much less. Then conclude, you are looking at overfitting, you have made a model that's too complex. That is a clear sign. So remember this gap. This gap tells you that you are looking at bigger gaps between test errors and training errors is a classic signature that you have over fit. Okay. So that's a very practical way of looking at it. Alright guys, so let me continue for another 10-15 minutes. Let me summarize what we talked. Regularization is asking this question. Asif, one quick question. So does that mean that, you know, whenever we see there is, you know, we are overfitting. So can we generalize that whenever we are overfitting, we are regularizing the data? are regularizing the data? No, no, no. Whenever you're overfitting, you have forgotten to regularize, dampen the data. Regularization is the cure for the disease of overfitting. So regularization, is it part of our implementation? So whether it is simple or a complex we have to include regularization yes yes so it's like you wake up in the morning and you brush your teeth okay so a data scientist wakes up in the morning looks at his model and if it is not regularized regularizes it in simple terms okay so which means that we have to regularize any model we build generally you should unless you have a tremendous amount of data and you don't need to regularize but how will we know that you know we don't need to regularize generally you don't it's very easy like i said right you look at the error on training and you look at the error on test right so let me give it values, names to that. So suppose the error in test and error in training, training and test, if E test exceeds by quite a margin, the E training, that's how you would know that you really screwed up by not doing regularization. Are we together? This is a clear sign. This is a sign that we have overfitting. Are we together? And let me say that statement that I said a little while ago, that regularization the cure is the effort to cure well it's never a perfect cure effort to cure the disease of overfitting right so what we are saying is it is okay to take a complex model but if we can regularize it, if we can remove the damping in it, then all is well. Because if you can regularize successfully, you actually are doing well. Let's go back to this picture and let us look at this picture here. The blue is under fit. The yellow is over fit and but you agree that pink which is probably more complex than even the yellow one is the best fit because we have dampened it down we have regularized it are we getting the ideas across guys anybody who wants to ask any more questions yes so i'll say if you know the training errors are greater than the test errors so is that the indication that we need not uh you know regularize see when the training errors really are uh in excess of test errors by a wide margin but if they will be close to each other occasionally you'll see training errors actually are less than test errors usually the training errors are more like sorry the training errors are more than test errors when that happens that is usually an anomaly of sampling data sampling your test errors will always exceed by and large on average the training errors but the question is by how much if it exceeds by a significant margin you know that you're over if it doesn't exceed if they are more or less the same or occasionally you know you may have be having a situation in which you notice that your training and test errors are pretty much matched then the only thing you can say is that you have not overfit you might have underfit but you have not overfit it may you might have underfit or you might have just hit the nail on its head. You might have made just the perfect model. So in that scenario, we need not regularize, right? In that scenario, if you are sure that you don't have under fitting, you're fine. But if it is under fitting, regularization is not going to help underfitting but the question that you have to ask is have you missed an opportunity to build a better model right could you have gotten a more performing model by dialing up the complexity and regularizing it right so in terms of this picture that we have, it would be asking that if I have the simple blue model, there your test error and your training errors would be matched more or less. But have I missed the opportunity to draw the pink line? That is a question you have to ask because the pink curve certainly performs much better is a much better model isn't it so that question still remains which is why the preferred approach is to take a reasonably complex model and to regularize it somehow dampen the oscillations in that model Somehow dampen the oscillations in that model. Asif? Yes, Dennis? Isn't a minibatch gradient descent a way of regularizing? We'll come to that. Hold onto that thought. I haven't got methods of regularization. We'll come to that one by one, guys. Asif? Is this clear? Yeah, alright, go ahead. So, Asif, a little similar to what Dennis is saying, if we implement like early stopping, does that, does this, in fact, does regularization influence early stopping also? Yes. So, you're right. See, early stopping is another regularization technique. It is in the class of regularizers. Now the question is, so we will go into the techniques, that's what today is about. In the next two hours we'll do those techniques. But before I do any of the techniques, it is important for us to grasp the concept of regularization are we clear guys i hope we are if you have questions ask me otherwise we'll take a 10 minute break and then come back and do regularization techniques specific techniques um asaf then in short i'm just extending what sudhir asked you so that means that like for every model we have to regularize because we might miss making the model perfect generally see generally there there are ways of looking at it so you know i just uh alluded to bias and learning one of the beautiful things with bias and learning is it auto regularizes the regularization term automatically gets baked into the theory so it comes out you don't have to do a separate regularization it is always regularized there is never enough wild oscillations with Bayesian so that's one positive but since we are not going to do Bayesian theory here I'll leave that aside we are not going to do Bayesian theory here, I'll leave that aside. Generally speaking, there are two choices. You start with very simple models like I do in practical terms, right? What do I do? I start with very simple models, but I know that with very simple models, it is likely that I have underfit. So I do go through the process of dialing up the complexity. And then at the end of it when I take complex models that regularize it and once I have regularized it I ask myself this question did the regularization lead to that pink picture here or not so in other words did a complex regularized model give me better answers did it become the pink line versus the simple blue line? If it does become the and you don't know data is king remember in this world don't forget you know free lunch theorem. You don't know ultimately data will decide which of these approaches will succeed and that again comes to the thought that in machine learning, you never try one algorithm and say it's good. You try a whole set of algorithms and you have different complexities and see where is it that you're getting your sweet spot. Does that answer your question? Yeah. All right guys, so any other questions before we get into the regularization techniques? If not, let's take a, it's 8.14. Should we take a 15 minutes of dinner break and come back after dinner? I'd like to have two hours straight. So we might extend today. We have a lot of carry. If it is okay with you guys, we might extend till 1030. We need two solid hours to cover regularization and to cover back propagation. All right guys. So it's 815., let's meet at 8.30 and let's have a quick dinner in 15 minutes. All right guys, so in the first hour, the summary of what we did is we covered the topic of regularization. We understood its purpose. Roughly, it is to take a more complex model and regularize and dampen the oscillations and hopefully get a really good model out of it. When you regularize, you don't quite cure overfitting, but you mitigate it to quite a bit. That's the goal. So now comes the question, what are the techniques to regularize? Now there are quite a few techniques and I will give you guys the short descriptions of those in the next one hour. Now, those of you, and only those of you who have taken ML 100, 200, you may be, or independently from somewhere, from reading the textbooks and so forth, you must be familiar with L1 and L2 regularization, namely LASSO and RIDGE and the combination which is ElasticNet. I will quickly review that for those people who have not gone through that process, not become familiar with those regularization techniques and will introduce regularization techniques that are specific to deep learning but before I do that I'll just itemize the things that I will cover so our topic now is regularization techniques in deep learning techniques. This is a big topic. So let me start by item1 terms in the loss function. So what we do is we add some penalty terms, regularization or penalty terms in the loss function before we do the gradient descent of it. The second technique is something called dropout dropout of nodes in a layer but this is usually referred to so i'll just highlight the words that people use for these techniques so when people say l12, they invariably are talking about regularization by adding those terms to the loss function. Likewise, dropout is the adding, dropout is a thing that we will talk about which is very specific to our deep neural networks. The other is is early stop early stop and that helps and then there's more to it actually you could do for it turns out that something which isn't really meant for that but but does end up having the effect. It is called batch normalization. Now batch normalization is something very interesting. It was invented in 2015. People started using it. They thought the reason it works, or the problem that it solves, which is something called covariance shift, mitigates covariance shift, and I'll explain what that means. It was supposed to be doing that. Later on, research found out that it does no such things. So the whole thing has become a little bit murkier. What exactly does it do? but one thing we do know that batch normalization it does lead to regularization it's one of its benefits and we'll talk about why it regularizes and so on and so forth and what batch normalization really is so anyway if you're reading your books uh any of the books you'll often say that we believe that, I mean, you know, they'll give an example and we'll talk about covariance shift. But I just thought I'll put the word out. The evidence says actually not that it does that. It doesn't do that. Now we have a different understanding of the whole thing now we had early stop batch normalization would somebody like to add something else to this anybody has heard another buzzword uh adding noise to the data come again adding noise to the data adding noise is what dropout will do in effect but you don't want to know deliberately add gaussian noise to the data i mean in a way you're saying it can can we augment the data perturb the data and bring so actually let me put the other part that is more interesting data augmentation can we use data augmentation to create a regularization that is more interesting data augmentation can we use data augmentation to create a regularization that is a technique so when we do data augmentation traditionally we don't do through perturbing the data with noise actually maybe do a little bit what you really do is you look at, for example, images by taking small sections, flipping the image upside down, left or right, lateral flips, or taking sections of the image and working with that, rotating the image and things like that. You do data augmentation. And the last thing that helps is creating an ensemble. Remember, a random forest is, for example, an ensemble of trees. And because it's an ensemble of trees, it leads to far better regularization, much less overfitting compared to a decision tree. And the same thing applies to neural networks. Whenever you build neural networks, at the end of the day, it often helps to use ensemble techniques. Remember the ensemble techniques of bagging, boosting, and stacking? So I find from experience that using a small ensemble with bagging or taking a few models and then stacking them together invariably leads to better results you should always do that in the end now always is a rather strong word it all means and I must qualify it see guys what happens is that when we model it is for a purpose there's some business goal some practical goal if you are working for weather, if you're working with weather data in a scientific laboratory, your goal is to be able to predict the weather with accuracy that from a business perspective makes sense. So for example, if you can predict the temperature tomorrow at noon, within one degree, so let's say 70 plus minus one, but that would be considered very good. You don't have to be, you don't have to improve your model so that it predicts down to 70 plus minus 0.0001. That is unnecessary. So one of the questions that always is there is have you already like when do you stop improving and usually the answer is you if you have infinite time and infinite budget of course you can go on improving it it becomes in any topic becomes an area of research and you can keep on improving upon it but in more practical terms so long as the goal is satisfied the prediction goals or modeling goals are satisfied, the performances are within the parameters that you're trying to achieve, you sort of stop there. So for example, if a single neural network is giving you the performance on test data that is within the parameters of what you were hoping for, that was your goal, but there may not be any reason to go out, all out and build large ensembles and train them because they take effort, a computation time and effort. So anyway, we'll start with six and maybe something else will occur to me. So as you can see, this is a pretty long list of regularization techniques. I will explain what the L1 and L2, I will explain the last because they have some interesting geometry associated with them. Let me start with the sixth from the bottom up. What happens when you ensemble a lot of them, when you average? To ensemble is to average the results from individual learners. So let's take the example of, this is the wisdom of Kranz, isn't it? So you ask people, what is the weight of this object? And people will guess, but if you take the average of it, you will realize that it tends to be a pretty accurate number. Averaging also reduces overfitting, as you can imagine. It reduces oscillations. It smooths things out, right? For example, if you look at the stock market and you look at moving averages, you'll notice that they're pretty much smoother than the wild oscillations. So ensembles have that effect. When you take a random forest of lots of trees, decision trees, you tend to get a lot of learners, each tree being a learner, each with a different opinion, but when you average them all out, that's your bagging thing, you take the aggregate value or the majority vote for classification, it tends to do really well. Pretty much similar ideas are applicable to neural networks. Now, one of the fallacies people have is they say, well, isn't a neural network itself an ensemble of nodes? lots and lots of nodes. So why do we need to create ensemble? The answer to that of course is you have different neural architectures that you're putting together. When you put different neural architectures together or you're playing them with different sets of data, different input features, there's all the more reason to treat each of the neural networks as a weak learner and then to ensemble their results, either bagging it or stacking it and so forth. The stacking is one thing, if you remember in ML 200 I mentioned, it is the most overlooked because most textbooks talk of bagging and boosting. They tend to mess out on stacking though in practical terms stacking gives you a lot of benefits so that is ensemble ensemble means crowd it's just a fancy word for a crowd crowd of models right crowd of models so you get the wisdom of the crowds essentially. And I won't go into it because there's a lot about it. If a model can get its prediction right, it's a binary classification. If it gets its prediction right, even just half, just a tiny bit more than half the time, then a crowd of such models are guaranteed to give you much better results. So, ensembling. And those results get better because they also have an averaging or dampening effect on the wild oscillations. That is one. The other way to regularize is just more data. So what do I mean by that? Suppose you have a model. So let's talk about the data augmentation. The multiple ways of data augmentation, where is to literally go out in the field and get more data. But if you can't get data, somehow generate more data. Let's say that you have an image. You have an image of a person, let's say this is a person with his hands raised. So what you can do is you can create one more data point by having him stand, this guy stand on his head. Or you could do the data augmentation this way. So this is a rotation. You can do various rotations. You can flip left to right. So you could do, for example, right to left, left, flip, and so on and so forth by various rotations and flips. The thing is, you're training. So suppose this is a cat. You can get a lot more images of cat simply by doing these things. The other thing you could do it, is you could just look at little patches of the cat. This patch, this patch, this region, this region. And so by looking at sub regions of an image, you can get more images.