 So welcome back guys. I hope you had a good lunch or perhaps on the East Coast dinner. We now move on to the lab part of this session. Today, we are going to look at one dataset, which is I've deliberately crafted because it's hard for some algorithms. We will first analyze this dataset using K-means clustering, and then we'll use dbScan. Now, the typical way one starts is that one gets some easy data sets, which you can easily use K-means clustering to bring out its strength. I'm actually taking a different approach. I want to show that I will assume and I'll give you some problems, homework problems in which you can validate that. But I wanted to take a tough case to start with. So I created a data set. There is a data set in the cloud. You'll find it. It's called Smiley. And you will see shortly why it's called Smiley. Now, usually, when you have the data, you don't know what. Oh, yeah. Oh, yeah. Data comes without labels. The whole point of unsupervised learning is you don't know labels, but here I'm giving you the answer that it will tell you which cluster, there are seven clusters, which of the cluster each point belongs to. But remember, you don't on p that is sort of our our secret it's not something that the algorithm should see the algorithm should see just x and y so from this data set you strip out x and y and you look at your description and you then plot it out ah and what do you notice And what do you notice? It looks like a smiling face. So I created this data. Now what we are going to do is how many clusters do you see? Six. Six clusters. And we also have outlier points, isn't it? All these poke marks on the face, pimples on the face. So we're going to see how well the algorithm is going to do. So what I do is we will run k-means for different values of k. Remember what I told you, you have to keep running k if you don't know how many clusters really are there in the data, you have to keep running for for different values of k you have to make a screen plot and look for the elbow if you remember the steps because k is the hyper parameter of the model so here it is what i do is for k and i just chose an upper bound of 16 Assuming that there are no more than 16 clusters possibly. And of course you don't know, you have to be careful. You have to build the screen plots and iterate over it and see where the bend is. So k-means clustering is very easy. You just say, find these many clusters. How many clusters I find? Between one and 16. Find the clusters, fit, and then you can predict the labels, and then you can say, you can sort of append. The inertia is your WSS. Then you plot it out. So look at the first one. When k is equal to 1, is the clustering good? Of course, it's good. The whole thing is one cluster. But it doesn't look like one cluster, clustering good? Of course it's good, the whole thing is one cluster, but it doesn't look like one cluster, isn't it? It looks like, well, six clusters have been merged into one cluster. Now, when K is equal to two, do you notice how it has built the cluster subguides? Would you agree that it's good clustering? Not for the client. It almost feels as though, well, in the COVID days, it looks as though the person is wearing a yellow mask. Isn't it? Well, it doesn't look like a good cluster. Half the top half and bottom half. What about K is equal to three? A little bit better maybe, but nose has a bit of a yellow.'s a little bit better but not really the eyes and the eyebrow don't shouldn't they should be separate clusters hopefully well that didn't quite work but this looks reasonable three what about k is equal to four keys is the same at eight equal to three except that your lips have gotten parted isn't it do you see the split in the lip right so would you say this was a great job maybe not what about k is equal to five now what happened where are the five points it is not visible this is one cluster this is third, fourth. So here you have k is equal to five. In k equal to four, why did it part to a split in the middle and why? It's just the way k means ran. It's partly sensitive to initialization, in this case not. It just so happens that it's looking for round clusters for whatever reason it decided that this is the best way to get a nice round yellow you see that yellow is the problematic area right it ended up picking that as a cluster and then it took a little bit of the nose and the other side of the lip as one cluster right and you can quite clearly see the limitations of k means, right? Isn't it? You go to K is equal to six, it's beginning to get iffy. Lip is wrong, but other things look right. Isn't it? It is, for example, not confusing the nose with the lip. Even K is equal to five is reasonable, at least everything looks reasonable and thereafter you start noticing something interesting k is equal to seven now for the first time the eyebrow is separated from the eye on the left isn't it not much has changed k is equal to eight interestingly both the eyebrows are separated from the eyes right so everything would have been great except for this troublesome lip the smile is getting the k means is wiping the smile out of kv's clustering in some sense right not quite good well what about this you would agree that from k is equal to 9 onwards, it gets progressively worse. It's splitting clusters, isn't it? Until you reach k is equal to 15, and it is forced to find 15 clusters, see what it looks like. Where is the screen plot? What we did is we saved the WSS. So let's plot it out for different values. So here is the screen plot. You look at this. Now, where exactly is the elbow in your view yeah three or five right so let us see how does the cluster three and cluster five look cluster three is this one eye is one cluster with this eyebrow, second eye is another cluster and then the nose and the lips, the smile are together. Maybe reasonably good. If you think about it, K is equal to five has split the lip, but other things have got right. It's a K is equal to three. These are, I mean, if you really look at it, arguably these are the better ones in this clustering isn't it which one would you guys consider the best one at five five is yeah except for the split lip right uh an outlier is being dragged in. Three is this. Okay. So this is it, guys. K-means clustering. Now, explain, why do you think K-means didn't work well on this? The shape of the data. It's not roundish. In this phase, eyebrows and lips are not roundish, right? And that gets it into trouble. You notice that it always seems to get the eyes right. You notice that never does it split the eye. It seems to get the eye right. So things that are roundish, it gets right. But things that are not roundish, it does not get right. It's a limitation of the k-means. But if you were to try it on globular data, it would work just well. Let me get back to that. Let's try the same thing with dbScan. Let's try the same thing with dbScan. Same data. This is the original data. I hope you recall that. Oh, what am I doing? Okay. You remember the face and then this by the way db scan the way the algorithm has been written it's a little clumsy to come to know which are the real points and this but the api is a little bit clumsier to get the cluster labels out but you'll get it uh this code they have given the example code on their scikit-learn website and most of us pretty much write code almost or near identical to that for the purposes so this is it you you get the labels and then you look at each of you you do the same thing you say all right now that i found clusters let me tell so you ask this db scan how many clusters are there you know this is not a question you could have asked k means but db scan you can ask for clusters what does it say there are six clusters it says there are six uh does that look correct looking at this picture would you say there are six clusters? There are six clusters. So it seems to be getting right. And let's see what it marked as clusters. Now, see, this is how it calls the clusters. All the gray points are the outliers, and all the clusters are colored. Would you agree that it seems to have gotten it right. So that is the advantage of density-based clustering, that it works for non-globular shapes also. And it's a lesson here. Now, today, I kept the lab to something actually very, very simple for clustering. Clustering is simple. It is as simple as that, k-means and that. Now, one lab is missing, which is the hierarch for clustering clustering is simple it is as simple as that k means in that now one lab is missing which is the hierarchical clustering i'll also put on hierarchical clustering and i have only put on the tough ones actually i could put on some easy clustering labs also like for example if you take data let's say that you create hand create data that looks like this let's look at this data. Do you think clustering will work? K-means will work? Yes. Yeah, and why will it work? Because globular nature, right? It will get at this. It will make it correct. So I'm going to post this data as homework for you. Try to do K-means clustering, et cetera. See that it works right obviously will db scan work on it surely it will work and well of course this is a the smiley data set so this is it clustering is very simple when you do the labs with it you'll realize that it's a very simple algorithm hierarchical clustering, I wanted to bring the lab, but I don't know what I did with the lab. But it had a few linkage functions, the different kinds of distances between them. And to show you the difference that it makes, I'll post that lab and maybe next time we'll walk through it or Tuesday we'll walk through it or something. Or you walk through it or something or you can read it on your own i believe that this is very simple code right all of these algorithms like say for example k means algorithm you would agree that this code is one liner to create like look at the inner loop if you set your value of k a cluster is the same fit and predict that we are used to isn't it now this something is odd about it it is unsupervised learning so why is it using the word predict it is a it's an abusive term what it is it should be more like assign each point in the data assign it to a label to a cluster that. That's how it is. But since people are used from regression and classification to fit and predict, I suppose that's why the creators of the scikit-learn library chose to keep the word predict. But remember clustering is not a prediction algorithm. So that is that to the lab guys. I wanted to keep the lab simple. Go ahead, honey. Yeah, I'll show you. Where is the db scan part gone? Yeah, epsilon and minimum number of samples. So actually I I deliberately was hoping nobody would ask that question because that's exactly your homework the homework is a play around with different values of epsilon and. Raja Ayyanar? number of points and see here in this line of code, where is the db scan God. got db scan do you notice this line you have to choose epsilon and minimum number of points endpoints in the you know this too so these are the two hyper parameters in the model how do you choose them that's a problem remember we mentioned that's a problem you have to guess you have to guess and choose and see which of them gives you the best clustering so for example if you make epsilon is equal to 10 see what it does to your clustering right every time yeah i mean there is a metric you can for each of the clustering so you can observe the wss and things like that the same thing that we did for k means strings you can observe the wss and things like that the same thing that we did for k means right remember we use k means you could build the same plot and see at what value you get good wss of epsilon and a number of points right so see it's a and this is one of the part of machine learning people often ask why do i have to keep on doing experiments to find this business you can i automated building a loop and so on and so forth so there is this emerging field and we'll have one of the sessions called automated machine learning guess what it does it not only finds the best parameters it finds the best hyper parameters. Right, so if you use automated machine learning, which is all the rage, I mean, all this cloud providers are very big and will give you automated this and that. So the idea is that it will keep on trying different values of the hyper parameter and see which is the best and tell you that as the answer. Oh, yes, lots of computation. So if you're in the world of machine learning, the first thing you need is you need good hardware. So one of the experiences for people in this class is if you are sitting on old laptop, three, four, four, five year old laptop, the first thing people end up doing after some time is doing what? Upgrading the laptops because things don't run on the older laptops it's a lot of computation in general but see today's hardware has become very powerful like if you ask for the smiley data set it is too small and the math is too powerful it will run just fine on any laptop and you could like you could try out hundreds of combinations of epsilon and number of points, and it will barely take a minute. But when the data set sizes are huge, then it becomes a concern. And so that brings up one of the interesting problems. Why do I have to randomly try all these possibilities. Is there a way, just like we do gradient descent in parameter, to find the best parameters in linear regression or classification? Remember, we did a gradient descent on the loss function. So can we not apply a method like that for the hyperparameters and quickly go from one bad set of hyperparameters, quickly find the shortest path to a good hyperparameters of the model instead of randomly trying many things. And that is the whole field. That's a core area of automated machine learning. It is called Bayesian inference or Bayesian optimization. You will learn about it in the next workshop. Because it's a bit math heavy topic. So we'll learn about it in the next workshop because we'll have to know things like a KL divergence and things like that so we'll do it along with the engineering math so any other questions guys anybody so this is this looks easy isn't it clustering is easy guys it's very easy right the theory is easy and at least the standard some of the simpler algorithms very straightforward and so is the practice the doing of it right um yeah go ahead oh yes I have to post it on the course page. I'll do that. Just like all of you, I have also been busy having fun with Diwali. Now, time to get serious.