 Machine learning. This is a word that you hear everywhere now. You see it in the newspapers, artificial intelligence, data science, statistics, data mining, statistical learning. These data mining, statistical learning, these are words that to different degrees are overlapping and somewhat synonymous. Yet there are different types of people sort of belonging or behind some of the different meanings. It has a long history, quite a long history to the subject so we will today start with this word machine learning what we all know what a machine is and we will learn about what is learning what is a machine What is a machine? It is essentially an artifact, something that we create, right? A device or something like that. The old meaning would be something with mechanical parts and so forth. The old picture used to be with gears and pulleys and whatnot and conveyor belts and things like that. Today it could be electronics, it can semiconductors and so forth. So all sorts of things are machines. We know machines do something. And now, what is learning? Machine is easy to understand. Learning is something that is inherent to our species. We say that human beings learn right well it turns out that all living things learn right if you ever had a dog you would know that a dog learns as well almost as us right in many ways all all living creatures have the capacity to learn and they are learning all the time what is learning though for example if i were to ask you this question these stones do they learn what would be your natural reaction Do they learn? What would be your natural reaction? No. You would say a stone does not learn. Why does it not learn? How do you know that it doesn't learn? Does your clock learn? Your marvelous clock, does it learn? It doesn't learn. Does your chair learn? It doesn't. Right? You don't really ascribe learning to that. When you take a computer, does your computer learn? It doesn't. It is a mechanical computing engine. Isn't it? But we ascribe learning uniquely to living things. isn't it but we ascribe learning uniquely to living things isn't it in fact is the distinguishing a feature of living things is that it learns other than that there's a whole variety of living things some some live in water some live in some breathe air right some live underground some need no oxygen right some are anaerobic bacteria, single cellular, some are multicellular, some have vertebrae, some don't. There's a wide variety of things that you call living, trees, right? But we somehow feel that for all sentient beings there is a quality of learning that is there. So what is learning? If we were to ask, what does the word learning really mean? How would you quantify it? Any one of you, what is learning? What is it to learn? Why do you say, for example, that a computer does not learn? Anyone? That is one way of it. Sanjeev says that you gather, that learning is being able to create an understanding out of knowledge. That is one way. Raj, what do you say? There is a feedback. The feedback, it could be a negative feedback, which could help you take the next step. Along the right steps, yes. You're getting close to that. But, you know, there's feedback in the ocean. A wave comes and then it pushes back. A tide comes in and a tide goes out sure that's there but the ocean doesn't learn from it that so it doesn't remember oh last time the wave came back these things happen so right the goal is to go to place B and that's positive feedback, then you will increment your That's right. Your behavior changes based on observation. That is a quality of that. So any other answers somebody would like to give? Improve on performance. Somehow show improvement in performance by some measure. Also a good part. What would you like to say? What is learning? I think it's registering what you see and processing it the way... Registering and processing information in some sense. Observations. How about you? To know something which is not known. To know something which is not known. Yeah, this is the philosophically that may be a bit of a contradiction. How can you know something that is not known? Yeah, by giving somebody information. But then computers in the hard disk, there was some information that was not there. And now you just saved a file with that information. So has the computer learned? No. So think again, what is learning? So I'll give you guys a story, guys. Imagine a meadow, grass field or something. In the meadow, it's a simplified meadow. There is a pond there in the meadow and there's nice grass growing there are cows there are only two animals in the meadow the cows are there and the ducks are there you as a parent or as a teacher take a bunch of children to the meadow and those children are two-year-, hopefully they have they don't know what a cow is and they don't know what a duck is, they're very young. So you point to this little feathery creature with webbed feet and a beak and you say, see that's a duck. Do you see the beak? Do you see the feathers and you see the wet feet that's a duck so all the children say yes ma'am or yes sir whatever and then you move forward and then you come across this big giant animal and you say do you see this big animal with this sw. And so the children process that information and you move around. You're trying to teach them cows and ducks. How would you know whether they have learned about cows and ducks? You could say, for example, you could go back to one of the ducks you showed and say, what is this? of the ducks you showed and say what is this and if the children just let's say that you ask a child take one particular child and the the child says cow clearly the child hasn't learned so what the child has done is made an error has made an error when the child makes an error you say child has not learned now you show but then suppose the child does say error if the child says cow but if the child says duck then the child do you think that the child has learned you would say well maybe it's not quite certain because children can have excellent memories. They may not know that it's a duck. What the child may remember is that the name of this animal is duck. Just like a child is introduced to Henry and to Mary and to so on and so forth. It may not know the concept of a duck. It may think the name of this animal is duck and so forth. So it may have a perfect memory. It may have memorized this. And this is true for your exams also. What do you do when you are in college or school? You don't like, if you like a subject, you understand it. You learn. If you don't like a subject, you don't learn. What you do is you memorize the key facts and you regurgitate it in the exam, in the test, and then you are through it. So how do you know that the child is not regurgitating the information and therefore saying, done? The way to do that would be to take a completely different duck that the child has not seen and ask now what is this right so you take a duck and a new piece of information Xi that the let me just call it a test sample and you say is this a cow or a duck if the child answers this correctly you you may suspect that the child is beginning to learn but even then you're not sure because the only two possibilities the child may be guessing. Isn't it. And children guess. Right. So, they will give you an answer So, they will give you an answer. It is your job to find out whether the answers. So what do you do, you show them, lots of cows and ducks. And you look at how many times they got it right right and how many times they made a mistake you can add up all the mistakes the number of times they make a mistake are we together that is your error rate let's say that they make mistake uh in the beginning they made mistake let's say 50 of the time at 50 of the time what what can you conclude child hasn't learned but after a little while you keep explaining that oh remember if it is small if it has feathers if it has webbed feet and a beak then it's a duck right and if And if it is big, if it has a swishy tail and horns, then it's a cow. Right? But it's a two year old child. They don't fully understand adult explanations, just as adults don't fully understand a child's mind, that a child's mind cannot grasp so many facts. But at some point, learning happens because each child will pick up, one child will pick up on the fact that just look at the beak, is there a beak? If there is a beak, it's a duck. Right? And if it is big, it's a cow. Another may pick on to the fact they may like the horns of the cow very amusing and they may look for horns. If they don't find horns, they'll say it's a duck. So each child may pick up different attributes or start learning different things. But at the end of it, if the error rate goes down, especially when you show animals that they have not seen in the meadow, that is a clear sign of what? Of learning. That inarguably is a clear sign of learning. Now, in science, we like to think everything has to be quantitative. If it can't be measured, it's not science. You must have heard a famous saying like that. It goes to Lord Kevin, a thermodynamics genius, It goes to Lord Kevin, a thermodynamics genius, whose quote is much longer. It says that nothing rises to the level of science unless it deals with measurables and so forth. So if we want to quantify learning, would error be a good measure of learning? Reduction in error, could that be a definition of learning it is and in fact machine learning the learning in machine learning is always quantified as learning is the reduction in error in fact this almost becomes the definition of learning. Now notice that this is something while it may look like a very simple definition, there is a lot of profound things happening here in the field. We are not talking about thinking machines. When we talk of artificial intelligence, when we think what is intelligence, people will often say intelligence is the ability to think, isn't it? But none of us have ever been able to quantitatively state in a measurable way what is thinking? The weight of stones is there in grams and kilograms. Your height and weight is known. And every time I stand on the scale, it gives me a depression. That's something different. But how much did you think? There is no measurable that today I thought 70 units of thought, but yesterday it was only 52 units of thought. We don't have a measurable units of thought. We don't even know what thinking is to the best of our knowledge. Now philosophers have quantified many, many theories on what thinking is over the centuries or over the thousands of years. But frankly, we still don't know. We know what thinking is, but we can't articulate it in a measurable way, which is one reason when artificial intelligence as a subject started, people thought they will create thinking machines. In fact, the history of Silicon Valley is right here. Those of you who are here with me will remember, or may not remember, because it's a little bit before your generation. There used to be a very, very big company, a legendary company called Thinking Machines, where many Nobel laureates came and worked in the dream of creating a thinking machine. Artificial intelligence was supposed to be, the goal of artificial intelligence was to create a thinking machine. Artificial intelligence was supposed to be, the goal of artificial intelligence was to create a thinking machine. That thinking machine company actually never succeeded, despite the contributions of legendary thinkers and Nobel laureates. Today we don't dream that. We say when we talk of artificial intelligence, it is actually, the heart of it is machine learning, because learning can be quantified. Thinking cannot be. In fact, thinking machine broke up into parts. One part, the machine learning part of it actually went to Oracle, and the hardware part of it went, from what I understand or remember remember it went to a company that used to exist another high flying company called Sun Microsystems where the word sun was supposed to be an acronym for the Stanford University Network because it was filled with Stanford graduates that sun eventually also got taken over by oracle eventually so well that's a that's a history of it but the point is that that company didn't quite achieve the goal that it set out to. But machine learning is a runaway success today. Everywhere, and we will in the afternoon, look at some of the amazing things it can do, and learning machines can do. It can do things that we can't imagine that it can do. In fact, there is a paradoxical law, sort of a statement that somebody made, I forget the name, said that what is easy for humans is hard for the machines. And what is hard for machines is easy for us. Walk across a street in a busy traffic, We can do that. Machines have still not figured out how to do that. It's hard. Self-driven cars. Most kids learn to drive in their teenage, isn't it? Children learn to drive. They may not drive on the street, but they drive their little toy things on the streets by the time they are five or six. Despite the efforts of some of the best companies and frankly, the exaggerated claims, we are far, far away from self-driven cars. In fact, not only far, far away, we have not even a clue how to start making self-driven cars. In fact, not only far, far away, we have not even a clue how to start making self-driven cars. If you wipe out the marketing that people do, you'll realize that we don't even have a clue how to start or go about it. It's that bad. So some problems are extraordinarily hard for machines, but some problems are very easy for human beings, those problems. And the converse is true. And yet, machine learning has made great strides today. Such great strides today that in this room itself, there are things that are endowed with machine learning ability. You drive a car, most likely there are machine learning, there are things that are learning right your uh roomba your vacuum cleaner has a ai chip in it right you you're all of your holding cell phones the apps in them are ai driven you write code and you compile code today gcc compiler and all of these they are now ai driven to create optimized binaries out of your code. Just about everything you look around you, machine learning is there. So we will come to that in due course of time. But remember, I was unpacking the word learning and why it is important. In this field, intelligence is learning. Evidence of intelligence is learning. And learning is quantified as a reduction in error. So today, guys, I'm setting the big picture. It's a very general lecture. For a reason, I want you to think straight in this field. Think right about this field. So error is most important. What we do is we... So let us go and summarize what we do. For any problem, step one, quantify error. So in our meadow with cows and ducks, error is easy to quantify. How many mistakes did you make isn't it b a two rather two re reduce the error errors this is learning are we together now when you think about this definition of learning, you'll realize that nowhere did we say sentient beings or human beings, isn't it? Learning is something orthogonal to whether the thing is alive or dead, isn't it? And that is crucial machines can learn the only question is how do they learn and that is the subject that we are going to do deal with how do you make machines learn so the that part there's a bit of history to it see if you look at this the the creation of beings that can machines that can learn or things that have intelligence is pretty, pretty ancient. Throughout history, we have tried to create our likes. People have stories. In the Arabian Nights, you have stories of Aladdin and his magic lamp, and you rub the magic lamp and out comes a jinn, right? The ghost or whatever you call it, genie, out comes a genie and the genie can do whatever you want at lightning speed. Isn't it? It is endowed with intelligence, it understands you, it does it much faster. Many mythologies, for example, in the Hebrew mythology, there's a concept of Golub, I believe is the word. You can create a doll in the human shape, and you could breathe intelligence and life into it, then it would do whatever you want it to do. But it comes fraught with great risk, because it can do a lot of damage. You have to know how to manage it. Are we together? And it's interesting that even in ancient times, people were already aware that if you create intelligence, it is with its attendant risks. And today we are facing it. The artificial intelligence systems that we create, they're not necessarily only a force for the good. It's a tool. They're also being greatly used as forces for the evil. Throughout the world, democracies are disappearing. Authoritarian regimes are coming about. There is a swing back to the dark age thinking, conservative thinking. Throughout the world, it's happening. There is a swing back to the dark age thinking, conservative thinking. Throughout the world it's happening. There is an extraordinary disparity of wealth and the aggregation of wealth into the hands of very, very few people. And artificial intelligence is squarely to blame because it is affordable only for the rich who use it to get even richer. And we don't have checks and balances to prevent that. So what has happened is technology has galloped forward, but the social structures, the political, the law framework, legal frameworks and norms have not caught up as fast. There's a lot of catching up to do to prevent this abuse. You all know about fake news right so um and in the afternoon when we do the lab i will uh you will marvel at some of the things artificial intelligence or machine learning can do we'll talk about it so remember that so for the longest time people have tried to create or had the dream of this if you go back and look at charles babbage the creator of the first computer, he designed the computer, those were the days without like, it was all gears, very, very lots and lots of gears, right? He conceived of a machine that could compute, right? And when he did that, obviously the technology didn't exist to make such very high precision gears. Today, that machine has been realized in high precision gears. And miraculously, it does work. I believe it's in the San Jose Museum. You can go and see that it actually works, computes. But was Charles Babbage just trying to create a very fast calculator? People used to calculate by hand and using the abacus. Was he just dreaming of creating the computer? No, he was dreaming of creating a learning machine. So this dream is very old, and its history is quite even older, even older than that. so and there are many different people who will tell you different things so for example if you ask people in the computer science field they said of course artificial intelligence its algorithms its foundations started after computers came into being which must have been in the 1950s right if you ask statisticians they will tell well, statistics is an old subject. All that machine learning is statistics wearing a marketing hat. Right. So, for example, one of the authors of your textbook, or in fact, the authors of your textbook, Tipshi Rani and Hasty, they used to have a very good cartoon in which on one side there would be this person who would try to, a cartoon or maybe a joke, goes and does a conference in statistics and the budget is $10,000. And now the same conference or the same content announcement in machine learning and all of a sudden there's like a million dollar budget or something like that for that conference. So the joke is that it's the same subject, but you're doing that. That is a statistician's point of view, right? They sort of, well, it's not pure statistics in my view. There's a lot going on in machine learning. So if you look at the foundational ideas, the foundational idea is this, quantify the error and reduce the error. If you look at it from this perspective and its history, the history actually goes remarkably to a person who was the son of a poor coal miner in Britain in 17 something. He was not supposed to be a learned person, but as a child, he was a math math he was very good at math so his teacher noticed him and plucked him out of the fate of being another coal miner to the great anger of his father who said do something practical learn to go into the mines and get coal right because that's the way they would they used to survive but the teacher pulled him out got him into education used to survive but the teacher pulled him out got him into education he grew up to be quite a famous mathematician and you will learn about his name so he was once given a problem so the legend goes i don't know which part of this is true there was a problem thrown so this part is history if i remember right there was a problem that in the sky some planetoid or something was going to show up again you know these comets and this planet they show up once in a while they're visible because they're travelers into our solar system it wasn't hilly it was something else so was about to show up and i suppose it was at that time blinded by the sun. It was on the other side or whatever. And the idea is where in the sky will it show up and when will it show up? Now, if you were to lie down and look up at the sky at night, you'll be surprised how vast it is. Big starry sky. Or in the daytime, how vast it is, right? You have to tell where it will show up and when it will show up, which month, which year, when it will show up. Everything was in it. So in Europe, science and mathematics used to be, STEM fields used to be patronized by the royalty, by the kings, the feudal lords and all of that, by throwing these grand challenges. A grand challenge was thrown to all mathematicians and scientists to predict when and where it will show up. So they all made their submissions. And then there came a submission from this particular person, which was literally on the dot, fairly on the dot, while others were wildly off. the second close prediction was like way way off completely wrong and others were hopeless kind of thing right so so that so it is so then people said how did you do it and then he wrote a paper in 1773 explaining how the the part of it, right? So the paper was in 1773. He used a technique which he didn't quite talk about. So explicitly, he was coy about it. In 1805, remember, I'm talking about 200, 300 years ago now. A mathematician named Legendre. So this mathematician who found that planet, predicted the location of the planet was named Gauss. You may have heard the name Gauss, Carl Frederick Gauss in the world of mathematics and stem fields. He's a celebrated person. He's called the prince of mathematics. He was prolific till old age in the kinds of things he did in his life. So then what happened is in 1805, Legendre wrote a paper whose title was Least Squares, Principle of Least Squares. And we will start that as our first thing in machine learning today. Principle of Least Squares came about. He wrote a paper and then Gauss came out of the woods and says, now, wait a minute. I discovered this first. People say, where? He says, go back and look at my 1773 paper. How do you think I did that? Right. I used this to do that. So then started the debate that actually went for 200 years because this concept is so profound and so important that is so profound and so important that who did it was a big deal. And in research, who did it is always a big deal. It's called the prior debate. You must have heard who discovered radio, Marconi, JC Bose, many, many such people, there was debates about it, right? In the same way, who discovered this concept? It turns out after 200 years, person who studies scientists as a tribe, anthropologically sort of discovered that conclusively, that it was independently discovered by at least three people. One was Gauss, one was Legendre, and one was a lesser known mathematician in the United States. So it has a long history to it. So what is that concept that we are going to talk about? We'll use that as a foundation today. And that will also represent a moment, a demarcation between a very general discussion of machine learning and now getting down to brass tacks and doing something. So the principle that I will talk about is the principle least method of least, the paper itself was called method of least squares. We will start now understanding the method of least squares. So remember the core method. Some of the code mathematics of this field were created long ago. By the way, even before the first statisticians came about, this was a mathematical physicist. These people who discovered it are all mathematical physicists. Gauss, Li, Chandri, etc. These are the people. Now you may say I'm biased because I belong to that tribe of people. If a statistician was giving this session, he would reinterpret the whole history differently. I'll leave that to you. But okay, here we go. What is least square? To pose this problem, I will take a different case. I'll take the case of data. Now, another So we have a story of a young entrepreneur that we will use. This story goes as follows. There's a young entrepreneur who wants to open an ice cream shack on the beach. Now beach, what are beaches made up of? Generally, besides sand and water, sand and tides, they contain young children, isn't it? Young children are there, parents are there and so forth. And what do young children do after they have been playing off in the beach for some time? Well, they like to snack. They like to go to the ice cream shack and get ice creams. So, well, it's an opportunity. Young entrepreneur wants to open a good ice cream shack. Trouble with ice cream is it's a perishable good, isn't it? So how do you make ice cream? If you, what you would do is you would go to a wholesaler, buy ice cream, bring it to your shack, and then sell it for the rest of the day. Early in the morning, you would bring it. Now, when you sell ice cream on the beach there is a fundamental constraint if you buy too much ice cream from the wholesaler and wholesalers don't take refunds of ice cream right what will happen to your ice cream if you get too much of it and you can't sell all of it it will get spoiled and you lose money. If on the other hand, you get too little ice cream, then what happens? Yes, you wouldn't be able to sell enough. The children will just come and get disappointed. And then they will go over to the next ice cream shack and buy ice cream there. And then you'll be disappointed. Isn't it? You lost an opportunity. So the young entrepreneur's goal is to get just the right amount of ice cream, purchase the right amount of ice cream. How would you do that? So the young entrepreneur begins to scratch his head and say, okay, how do I determine how much ice cream. How would you do that? So the young entrepreneur begins to scratch his head and say, Okay, how do I determine how much ice cream actually sells? Let us say that being of scientific bent of mind realizes that the temperature matters. So suppose you take x-axis as temperature and y as ice cream sold. Ice cream sold. Then let us say, and I'm oversimplifying it, that the data points show up like this. get them that the data points show up like this this is not likely to be exactly true but uh i'm taking a lot of liberties with the actual physics or of things but let us say the data is something like this right and the entrepreneur observes that the amount of ice cream sold is somehow related to the temperature on the beach. On warm days, more children show up. Certainly true of California. I suppose in tropics it may be the opposite. But here, on warm days, more children show up, more parents show up with children. In cold, windy days, fewer of them show up more parents show up with children in cold cold windy days fewer of them show up so lesser ice cream is sold on cold days and more ice cream is sold on hot days hot days you feel thirstier you feel like having ice cream and there are more children in general on hot days in californian beaches suppose you were to look at this data what does does your eye tell you? So eyes, actually, the trouble with human eyes is that eyes are probably the world's most marvellous and the region of the brain that processes optical information together form perhaps the most marvellous machine learning engine, a learning engine. The human eye and the eyes are there in all mammals. They are there in fishes. To the best of our knowledge, they have gone through at least a hundred million years of learning. They have learned a lot. So what happens is they look at this data and they immediately see a relationship like. They can tell the human eye can immediately spot and say, hey, this is easy. All you need to do is. I see a relationship like this. Approximately. Would you agree? Right. Right? This seems to be a relationship that the human eye will say that y is approximately equal to now what is the equation of a straight line? m x plus c or b or whatever you say, right, like this equation of a line. Equation of a line. It discerns a line like this. But for a moment, assume that you don't have the benefit of the human eye. You just have data. So imagine that you're blind. A machine is blind. How would it discover the blue line for example why is the blue line better than this line or this red line or this green line or this let me take another line up let me just say this what color would we say indigo purple purple line what's the difference between these lines i'll color these lines i'll call this a a b c and d these are four lines so looking at this which line best fits the data So looking at this, which line best fits the data? A best fits the data. But how would a machine know? So let's go back to our fundamentals. We said we need to measure error, quantify the error, and find a way to reduce the error. It turns out that if you just look at this problem, much of the core of machine learning you can understand just from this example. So guys, give me a moment. I'm going to put on my reading glasses. Much better. Now, what we do is we say let's measure error. How would we measure error? Let's take this. What we do is, let's take any given temperature. Let me just take a temperature x star, some temperature, x. I'll just call it x. At this temperature, what is the green line predicting? This value, isn't it? What is the reality? this value, isn't it? What is the reality? This is the reality. What is the blue line predicting? This value. Actually, let me cheat a little bit. I don't want these to be so close to each other. Let me say the blue line is predicting this value. Let's say that the reality is this point. Reality. This is the actual Y. This was the actual amount of ice cream sold. Actual. On that day. Blue. What is the red line predicting? This. And what is the red line predicting? This, and what is the, this, oh boy, this line will be predicting pretty high amount of ice cream. You really hope that the entrepreneur doesn't go with the D as the, now what do lines A, B, C, D stand for? These are different hypotheses of the relationship between the temperature and the ice cream sold would you agree there are four different hypotheses and so i will use this word often hypothesis hypothesis there are four different hypotheses a b c d each hypothesis predicts a different thing and for each of the hypothesis there is a gap between prediction and reality this is this this one is y so there is a rule in machine learning whenever you make a prediction any hypothesis makes a prediction you make it wear a hat right just to distinguish it from so this is y a do you notice that i made it wear a hat right so i'll write these things there the predictions of hypothesis where facts this is a prediction. Are we together? So it wears a hat. This is YA. This is YC, Y hat C. This is Y hat B. And this is Y hat D. So now you look at it and tell me which of the hypothesis has made the least error. How much is the error? This is error from A. This is, how much is the error from B? This much. This much is the error from B. How much is the error from C? Oh, sorry. This much is the error from C. Would you agree? This much is the error from C, right? And D, D seems to have made a pretty substantial error. This much is the error from B. Right. Now, which of them looks to be the least? Well, I'm not so sure. Maybe A, maybe, yeah, A seems to be the nearest one in this case. But now look at this point. yeah a seems to be the nearest one in this case but now look at this point uh suppose i take another point you notice that the data is here and the prediction that the d line makes for this one let me just call it X prime. In this particular case for X prime, which line seems to be the best? Which of hypothesis, the D seems to be the best. So it depends upon which point you took. Different hypothesis will look better or worse. Would you agree? And yet your intuition tells you that the entrepreneur better not be optimistic and pick the line B. He is not likely to sell so much ice cream, right, given a temperature. So how would you argue that A is the best answer? How would you argue with that? . Say that again. . One way, yes. Anything else? See, you would say that don't look at only one point. Look at, we have this data. Let us look at the error made across all the data points. How wrong are the predictions? Something tells you that on average, somehow A is making far less mistakes than B or C or D, isn't it? So you can say one way, so intuition is add up the errors. Intuition, aggregate the errors to get a total. So you can say for each of them, could do for example error from the first data point the total error let's say for the a hypothesis is error that a makes for the first data point plus error that a makes to the second data point all the way to the error that, suppose the end data points that the A hypothesis makes, isn't it? Likewise for B. But there is a problem. If you just add up the errors, errors could be positive or negative, isn't it? If you define error as Y minus Y hat, YI, I stands for any point, let's say XI. So I'm using now now introducing mathematical notation you know if at the tail end or the bottom end bottom south what should we say south east corner of a symbol if i put a little i as a subscript it's called it means that and that i stands for either first second third fourth whichever data point you're talking about, right? So this is the definition. So I'll just put it by definition of an error. Definition of error. And geometrically, it's obvious, isn't it? That's what the error is. You predicted five buckets of ice cream would be sold, but only two buckets of ice cream got sold. How much of a mistake did you make? You predicted five buckets of ice cream would be sold, but only two buckets of ice cream got sold. How much of a mistake did you make? Well, you take the difference, minus three. A two minus five is minus three. So you lost the money worth three buckets of ice cream. Because you bought it and it didn't get sold. So if you do this, the problem is the error could be positive or negative. Do you see that, guys? Suppose you have two points. This error, so this is y1 and this is y1 hat. This is y2 and this is y2 hat, right, if you look at it. If error is defined as y1 minus y1 hat, do you notice that this would be greater than zero? Look at this example. Would you agree? This is the first situation. this is the first situation this is the second situation for number one error is positive because the reality is here above the prediction more than the prediction right on the other hand for the second case reality is less than the prediction, right? So in the second case, y2 minus y2 hat is, I mean, sorry, less than zero, I should say, greater than zero, less than zero. So the trouble is error one, error two. So if you add up, it may so happen that positive and negative can cancel each other out, but you're not interested in that. You're interested in the size of the error, right? The magnitude of the error. You want to add up the magnitudes of the errors, the sizes. Is the intuition clear? And so therefore, what you really want is you can say, well, let me put the error. Let's look at the sizes of the error. Now, this looks better, isn't it? You're looking at the sizes. You say sizes of the error is good, but people are, for reasons we'll talk to, you can do the size, but you can say, well, you know what instead of size i could do any power k of the sizes if i do the square for example that also is good i take the magnitude and square it i take the cube i take the fourth power i take whatever it is that two will work isn't it right and so? And so, see, other thing is that there are n number of points. It is customary, not necessary. Customary often to sometimes average it. So we are on the right track. This works. This is a milestone. This works. would you agree that this will work i can quantify the error by adding up all the sizes of errors then what do i need to do i need to find that hypothesis that minimizes that has the minimum amount of error in in this case it would be a a would have the least amount of error so far we are making sense right now comes the whole question then what is K it turns out that nobody knows what we should do with this K there are no perfect answers there are answers that work well in different situations? When k, let me make it a bit smaller. When, by the way, is my scribble understandable and writing understandable? When k is equal, well, if k is equal to zero, will that work? That would not be very useful, right? Not useful. That would not be very useful, right? Not useful. When k is equal to 1, this is called total. e is the total absolute error. Where does the word absolute come from? Absolute comes from the fact that whenever you take epsilon, you put two bars around it. By the way, you look at only the value, not the sign. And mathematicians say you're taking the absolute value of it. equal to total sum of absolute errors. That is the word. I forgot to put this thing. Total sum of abs. Actually, forget the word total because it becomes redundant. Let me just say sum of literally saying it out in words. Would you agree that this is what it would be? And the second one would be sum of squared errors, right? In other words, epsilon one squared plus epsilon two squared plus epsilon three squared. K is equal to two. Thank you for correcting me. K is equal to two. epsilon two square plus epsilon three square. K is equal to two. Thank you for correcting me. K is equal to two. Then it becomes this. And remember when you're squaring, you don't need to take the absolute first operation because a square takes care of that. Sum of squared errors. It is the famous SSE that your textbook talks about. Sum of squared errors. And is the famous SSE that your textbook talks about, sum of squared errors. And then you can go to higher powers if you wish. Now, the whole question is, which of them is better? And we'll talk a little bit about that. But this thing, this is your error. Now, your book will write it as SSE. People use different symbols in different literatures right i like to just use the word e e for error quite simple now there is another variant of it what sometimes people do is they they take they divide it by the number of data points if i divide it by the number of data points i will get average isn't it so if we divide I will get average, isn't it? So if we divide by n number of training samples, then what we get is another form that is often called E, some absolute error, becomes mean absolute error. MSE, ME, 1 over I is equal to 1 to N. This is epsilon I. Can I write it as this symbol this this weird sigma is called sigma right sigma at all Add all values, which is just a fancy way of saying plus this. Sum of all values. Add all values, right? Are we together? So you notice that we are slightly making small steps to mathematical notation. And then likewise, E of a mean squared error is mean error uh oh i'm screwing up my spelling today error one over n i is equal to one to n epsilon i squared and that is equal to epsilon one squared we are not doing anything fancy we just rewriting it in a more condensed notation. So this is it. These are two notations of error. Then the whole question is, which one should be used? And there is a lot of trade-offs to do it. Simple answer is, there are no correct answers. Based on your data, sometimes one error is better to find the best prediction model, the best hypothesis. Sometimes the other one is better, right? And what those things are, I won't introduce today because it's one of your homeworks to discover when one is better than the other on your own by doing the same. But there's an interesting literature to that. There is a theorem theorem it's called the blue theorem goss and another mathematician markov is often called the goss-markov theorem and it is called best linear unbiased estimator fancy mathematical word a blue theorem and blue or gauss word, a blue theorem. And blue or gauss I believe it was as early as 1812 that this was discovered. It said that for a variety of reasons that we won't get into and forget about some of the jargon here, unbiased and all of that. It turns out that by default, you're better off taking this one for most situations. Raja Ayyanar? Are we together. Raja Ayyanar? For most normal situations you're better off taking this, which is why this is the default whenever you do regression and this process has the finding the best fit line or the best fit hypothesis for the data you call it regression a word regression now why is the word so let us introduce that word and by the way we just covered a fairly important topic we covered something called which goes with a very negative word regression it is the converse of progression. Progress. Regress is the converse of progress, isn't it? Right? Opposite. Now, what does it mean? See let us capture what we did. We give it, we want that x goes in, x i, temperature goes in to a magic box. Out of the magic box comes out a y hat which is ice cream, how much ice cream to buy. And you want this magic box to be perfect. You want it to have the perfect predictions. You want the magic box. So this is a summary of it. You want it to have minimum whatever your measure of E is, error, right? And the data that you know. If you can fit it to the data and you get the least amount of errors, that is the magic box you want. That magic box internally has a hypothesis, and hopefully you want it to have the best hypothesis. Right? Now, it turns out that what we just did, now let's put something to it. This is what, it is a number, ice cream quantity, quantity is a measurable, it's a number, and it quantity is a measurable it's a number and it happens to be a one-dimensional so in mathematicians like to use fancy notation so you write r and then for good measure you add an extra line to it right it's called the blackboard R symbol. This is this is a symbol for real number. Real number is any number that you can think of, fraction, positive, negative, fraction, decimal, irrational, whatever you want. Which is the way you can have your ice cream quantities. So if this is an actually there's a generalization of it, there is a D, you can have a D dimensional, but we won't go there. We'll just, for simplicity's sake, in this case, it is just a number. When the target is a number, what you're predicting is a number, it is called regression, right? So in the story of the entrepreneur trying to sell ice cream on the beach, you did a number. But when the box, you give it some data, let's say that you tell a child that here is an animal with these features, big or small, beak or no bheek, et cetera. And what the child is predicting, Y hat, belongs to cow or duck. Now, is the child predicting a number? No, it is predicting which of the two classes, the formal word you use is pick a class from a category. That's rather formal language category category g class is called c belonging to a category g of cow and duck that's a sort of a formal way of picking a class means a class of animals ducks come in no two ducks look alike right but they all would agree that they are sort of together so the thinking is the class of animals right and duck likewise cows are a class of animals and the two categories the two types of animals that you're considering are cows and ducks right when you have this you say that you are writing a claw this is a classification task isn't it you classified an animal as either a cow or a duck are we together you're not allowed to say none of the above or tiger. Right? You have to only say cow or duck. So this is classification because cows and ducks are classific classes. This model and so you can have your own hypothesis to do that something and these the now there is some bit of language that you use the model is called classifier the model that is namely the box is called the regressor in regret regressor regression model. So there is a bit of an abuse of term. You can say regressor or regression model. Here, the model is called, the model is called the classifier. Now, regression is a little bit in trouble. People use all sorts of words, by the way. Xi, the input, is often sometimes simply called the input. Y is simply called the output. Right? In some communities, some tribes of people. Other tribes of people will call the output as the response. So same here. So I will just write it, the XIs. Let me just say that input, right, and output. Output is also called the response. Right. Sometimes people call it the prediction. Input is often called the feature features. The feature vector vector or something like that. People often use a lot of jargon. People will also call it the, and this is for regression, they'll also call the word regressor, which is why I was a bit hesitant calling the model regressor, because in some literature they mean xi as the regressor. So there's a lot of different terminology that is used, but I will limit myself to simple terms input and for output i might use only one of these two things output or response or prediction output or prediction is a common thing i'll use are we together now in the literature you'll find for example example, for regression, people will talk about the regressors, which is the inputs, and the regressand. Please don't use jargon, unless it's absolutely necessary. No point, just say input-output, input prediction. Prediction is probably the most intuitive thing to use when you're talking about uh models right and your model is a hypothesis so now what did we learn let's summarize because i think it's time for a little break we're coming to an hour we learned that there are two kinds of algorithms two kinds of tasks in one you have to predict a number two kinds of algorithms two kinds of tasks in one you have to predict a number in the other you have to identify a class the former is called regression process the second is called classification the first is your entrepreneur trying to sell ice cream on the beach wants to have the best prediction of how much ice cream will sell given this temperature so they it can do a smart purchase from the wholesaler the second one is a child trying to figure out is it a cow or a duck right that's classification now in both these there is inputs base you make this prediction based on some input something goes in and based on those inputs you do make an output in now, how do you quantify errors in the case of a classifier very easy just to count the number of mistakes. R. Vijay Mohanaraman – UKRI BASAVATARAMAN – Number. R. Vijay Mohanaraman – UKRI BASAVATARAMAN – Of mistakes. number of mistakes, right? And then try to minimize the hypothesis that makes the least mistakes. But it turns out that we will refine the idea of an error in a classifier, right? So what really happens is when you train these machines, they will never say it's a cow or a duck. They'll say, you know what i'm 90 sure it's a cow but 10 it might be a duck then how wrong is it well it is suppose it's really a cow that model is better than a model that says i'm 90 sure it's a duck, isn't it? And a little bit possibly still a cow. So it will always hedge just like you would hedge. It's like you ask anybody a difficult question, A or B, they'll say, maybe 50% A, 50% B. Say, no, you can't say that maybe 60 percent a 40 percent b right and that's what these machines can these models tend to do as just like we do so classification we need a more rigorous definition of error which we will come to but for regression we have come to this definition of error which is pretty good for now now comes the interesting question how do we minimize the error you can say hey and in this you can go about making all sorts of hypotheses in a plane given data how many hypothesis lines can you build each line is a hypothesis how many hypotheses can you have infinitely many So one way would be to just go drawing lots and lots of lines and finding out which one gives you the least error. Right? And that, by the way, is very common for programmers. If you don't know the answer, just randomly do a lot of them, find the least amongst them. Now, there is a flaw with that argument. The flaw is you may find the least amongst them now there is a flaw with that argument the flaw is you may find the least amongst the trials the ones the hypothesis that you created the trial hypothesis that you created the lines that you drew but you'll never be sure that it is the best isn't it So is there a question, is there a way to find the best hypothesis, the best line quickly? And it turns out, and this brings us to one of the crown jewels of this field. The most magical answer is, and that's what makes us feel possible it turns out that we have many techniques and one of them in particular called gradient descent that is the workhorse of the industry that makes learning lightning fast are we together it can find the shortest path from a wrong answer to a right answer or the best. There's no such thing as a right answer, a best answer. Are we together? Right? It will give you the shortest path. So I use the word right, I sort of eschewed the word, why did the word right answer? So since I did that, I'll end it today with a break. But before that, I'll explain why do we not use the word right answer in the suite? You never use the word. If you ever see somebody say, oh, I found the right model or the right hypothesis or the right box to make predictions, that person probably is not well grounded in the subject. You never do that. The reason for that is, I'll give you an example. See, suppose you have data. I give you this data in this region. And you may say that a straight line makes sense. But you don't know is that this data after a little while it bends which you know if the temperature goes too high nobody will be at the beach isn't it so you don't know all you know is within this region this model works this hypothesis works so there is a famous saying by a great, great data scientist, statistician, mathematician, whose name was, since we drew so many boxes, ironically, his name is Box. Right? So Box, and I'm not kidding, he's really a great guy in the field. He said, hmm? And Cox. Yeah. And by the way, his brother-in-law was Cox while he was at it. And there's a very interesting story about Box and Cox, and they discovered something which will actually use, right? Which is the Box-Cox method method so box said a famous thing all models are wrong but some are useful this is a very famous saying uh and a truism in the field. See, when data comes, you don't know what forces generated the data. You'll never know. Because if you knew, there was no point in building machine learning models, isn't it? You don't know. But what you can do is you can build a model that is useful for your practical purposes. And by the way, this is more broadly true of all of science. Science can never be proved right. I don't know. I would like to relate it to one of the fundamental tenets of science. What is the difference between scientific knowledge and, for example, a mythology or any other kind of knowledge? You go and talk – and forgive me for being rather rude or obnoxious, you go and talk to any religious tradition, they have the answer to everything and they are absolutely sure. Right? And there are many systems of knowledge which are complete and which can answer everything. In fact, Freudian psychology belonged to that framework in my view. Everything, every problem had an answer, fixed answer. And they claimed that it is true because of this. The science actually differs in scientific knowledge. If apples fall and Newton sits down and creates a theory of gravity, it says force is mass the force of gravitation is proportional to the masses of the objects and inversely proportional to the square of distance between them remember it's a hypothesis it's a model of the data you got the data you're making a model is that model right you can never prove it right but you can prove it wrong easily so for example if you notice that suppose somebody makes a hypothesis that the force of gravity is proportional to just one over just the distance there or something else maybe proportional to r squared or something like that or maybe it doesn't depend on distance between the objects at all you can easily gather data you can create a table you can measure the force and you can say this is wrong this is wrong isn't it what you can say is one over r squared proportional to one over r squared seems to work but you can never show that this is the right answer there are no right answers in science so you never say unless you're school kids that the theory of gravity as propounded by newton is right all it says is it works and of course we know that it's not right because einstein came along and then he made subtle corrections to it and now we have the einstein's theory of gravity right then we know that well that too cannot be right because that is in direct contradiction or there is a huge problem between gravity and yet another remarkable theory that works that is called quantum field quantum field And in the world of physics, for example, there are these two grand traditions, the theory of relativity, gravity theories, and then there are the quantum field theories. And nobody has ever figured out how the two will work together. And there are many situations they have to work together. One of them, for example, when a person in our today's audience is gender my. Raja Ayyanar? roommate during graduate school years is not in this room, but he's a remote he literally pondered over the mysteries of what happens if you were to sitting there if you were sitting there on the edge of a black hole. Raja Ayyanar? Right and dangling your feet don't you can do it, it's a thought experiment. But if you were doing it, that's when gravity and quantum field theories meet. And you get into all sorts of trouble, right? So, well, you know, there are lots and lots of grand theories. So what do we do with these theories? Are they right? None of them are right, right? Science is never about right. It's called falsifiability. Theories can be falsified. They can never be proved right, but they can be proved useful, right. Surely because we have all these theories that we can send people to the moon, to Mars, right, and do all sorts of, you know, even the very fact that we are talking remotely using this digital device is semiconductor physics, which is quantum physics right so practical things useful things are possible because we can make useful models useful theories but they are not necessarily right so that is why I make a distinction between useful or effective theories let's use the word effective theories or effective theories let's use the word effective theories models theories and models are synonymous in my mind versus right let's leave right for the religion makers right so if you feel like founding a new religion go create some bunch of facts and call them right sorry for being obnoxious but anyway let's take a break for 10 minutes Sorry for being obnoxious. But anyway, let's take a break for 10 minutes. So first of all, guys, I apologize for taking off on traditional belief systems. I have no right to do that. But I was joking. I take it in a lighter sense. All right. So remember, we are in search of effective models. One question, why the word regress, a negative word? It has an interesting history to it. Again, it goes to the history of the subject. And this is, in fact, your first paper reading. There is a statistic, there is a data, today we would call a data scientist called Galton. Galton observed something amongst peas and is true for human beings also and for other everything. He observed something remarkable he he would take peas and grow them and then he would look at how tall they are and so on and so forth and what the pea plants are and then i suppose he would do that with children with human beings uh families and so forth and what he observed actually after a little while with people, I believe, and the paper is about people, he noticed that the height of children of extraordinarily tall people, tall parents, is less than the height of the parents. height of the parents. The height of the extraordinarily short people, you know, children of extraordinarily short parents is more than the parents. So what happens is that if you look at the height distribution of human beings, you will find like, see what happened actually in reality there are two high distributions slightly shifted men and women men tend to be slightly taller so if this is the height in most genetic pools if you have so this would be women and this would be men but suppose i blur this so for the first approximation assume that you're looking at only one curve right for simplicity's argument so let's say that you're looking at pea plants or something like that so he found that if you have a parent whose height is here parent then children's are heights tend to be less than the parents' heights. If your parents' height is here, then children's heights are towards. So this is the mean, you know, average. And you tend to have a bell curve distribution. That's where the word bell curve comes from. And you tend to have a bell curve distribution. That's where the word bell curve comes from. We all are familiar with bell curve and heights tend to have, or do have, more or less a bell curve distribution. Right? A Gaussian distribution. Actually that itself, one mathematician has questioned in a very interesting way. But anyway, we'll leave that aside. is it so what is found is the extraordinary ness gets reduced the outlier behavior gets mitigated in the next generation are we together and the whole question is why it happens so we will come to the why it turns out that the why is not as hard to understand as we thought in view of the theory that we are learning. Today itself we learn. So he said that regress is opposite of progress. So it isn't that the children of tall parents will be even taller. That would be progression towards great tallness, isn't it? More tallness. The children of very tall parents would be less tall. And I believe this is true in many areas, many things. Now, why it happens is quite interesting. And I would like to just talk a little bit about that. So suppose that height is based on some input factors, let's say genetic factors, genetic and genetic factors, x is the input, height is some function of the input. Because so this is the because part. So this is the because part. Because then there is always an adventitious part. Let me just call it epsilon part. It is often called the irreducible error. See, the height of children is not only because of the genetics, it is also because of the prenatal conditions. How well was the child taken care of in the womb? What were the conditions? What was the mother eating? Isn't it? It is also because of the food and nutrition and various environmental conditions after the birth of the child and the growth into maturity, isn't it? So there are many factors which you don't quite understand. If you understand only the because of part of the genetics, let us say, this captures what you don't know. Factors that are what for or in your model? Are we together? There will always be things. So let's take this example of selling ice cream on the beach. The entrepreneur says, well, you can predict the ice cream based on the temperature. But if you notice that in any temperature zone, let's look at this, look at this. You see temperature is more or less the same. Temperature is more or less the same here, whatever the temperature is, but how many different dots there are, data points there? Someday someday there is more ice cream someday there is less ice cream sold isn't it so there is a error band of this is your band of errors right let me use a slightly different symbol because i use this is the error that your model cannot explain even the best model cannot explain isn't it. This will remain and which this now you can think about it. Can you think why given the same temperature on a given day on the beach the sale of ice cream would vary depending upon it's a holiday or not let's say sunday or monday exactly very good point uh harry you know on weekends on holidays more children come more parents are free they'll take their children to the beach on work days parents have no time're working. So day of the week is a factor, but your model didn't take that into account. They could be wind, how windy it is, right? Whether there is a tsunami warning, right? Whether there's a shark warning, right? There are many, many factors. What is the economics in a depression? When everybody is just, the market has crashed and people don't have money, they're less likely to take their kids to the beach or more likely, I don't know, whatever it is. Beaches are rather inexpensive outing for children, maybe more kids on the beach. One way or the other, there are many, many factors that your model has not taken care of. And so the epsilon, this little thing, we introduced the concept of irreducible error. So I will write this statement. Irreducible error, irreducible error comes from that which the model does not account for. It is that which you don't know that is causing it. And from basic things like for example measurement errors. Most thermometers, unless they are medical grade, will take the temperature with a certain error range to begin with. And those of you who are fighting your weights would agree that you never can trust the scales. You stand on it and then stand on it 10 minutes later and your weight has changed. Anybody shares that experience? Right, I certainly have that experience. So if I feel bad that I've gained weight, I just wait 10 minutes. No, I'm kidding. So that is your irreducible error, epsilon. So now let's relate that. There are factors that are beyond your control. So let us say that you have is some function of the height of the grandparent grandparent right plus an irreducible error so some is because this part is the because part explainable part right but what about the irreducible error? This error is because of all sorts of circumstantial things you don't know. Now errors tend to have a themselves a bell curve distribution. There will always be some people whose height will be less, you know, the environmental factors will push it down a little bit, push it up a little bit, and in rare cases it will push it up a huge amount. So these will become a bit of a freaks right or outliers people whose heights are so much that they can they immediately qualify for a basketball team or something like that right So there we go now this factor is the random variable, right? It's the random stuff. So when you look at that, of a child, of this parent, this will be of this parent, the because part, it is very unlikely that in the mistake the random mistake that happened for the parent will also happen for the child it's very unlikely what is far more likely it won't happen that mistake will not happen so but this value is less this value is less than that of a the the actual causative value is less so the the error that will be here, let me call it E1, E2, E2 is likely to be less than E1, right? Because only the causative factor is being preserved. The error part is randomly moving around, right? So if you have an outlier amount of error, much more likely that the error will be less in the next time around. So what will you notice? Children of outliers are less outliers. Does it make sense? And do you see how easily you can explain it if you reason through it? So this is the classic paper of Galton. It is literally called regression towards mediocrity. By mediocrity, not with negative connotation, mediocrity means the mean. Medio, mean, mediocrity. So we are going to use that as a first paper reading exercise. Please read that paper. And in fact, one of your labs will be on the Galton paper. So that is it. Isn't it remarkable that when you see outliers, don't expect the next generation also to be outliers. So that's that. So this is true for plants, for height, height for human beings for pick an attribute that has a bell curve that has some random variation built in and you realize that the outliers are produced by causative factors see the child of a very tall parent is not likely to be a dwarf will still be very tall but perhaps not as tall as the extraordinarily tall parent i think that is that so that is that well of course uh this was paper this is a paper of galton now those of you who are biologists may point out that there is such a thing as a flynn effect how many of you know what a flynn effect is right okay please go ahead progress nutrition over time yes have gotten taller and have gotten more intelligent that's right so yeah excellent point so i'll just mention an aside it's called the flinn effect so uh ignore this for the timing of this discussion what flinn effect says that because science and technology are progressing and because subsequent generations are getting better food better education better nutrition better of everything better environment they are genetically developing or genetic epigenetic and all factors put together environmental factors put together they're just generally smarter better taller than us. But that is the average moving up, not the individual. Remember, the individual extraordinary child will be less tall. But as a population, the younger generations are smarter. Now, we can all identify with the fact, those of you who are parents, you all know in your hearts that we all feel like idiots before our kids isn't it they run circles around us at least in my case right so that is the flan effect they are taller you can see what used to be the olympic record 20 years ago high school kids do that in any anything high jump long jump this that whatever used whatever used to be the Olympic record of 20 years ago, 25, one generation ago. If you look today, you will find that that's sort of the high school or the state records in different places. And that's true of, so that is the Flynn effect, not to be confused with regression towards the mean, which is for individuals. This is for population. All right. So that's a summary of that regression towards the mean. Go ahead, Sanjeev. The meaning of regression is not about predicting. I would think that progression means you're predicting something in the future sorry you know what i gathered is that you know it's regression it's called regression because it's addressing to the mean no yes good question i didn't really answer sanjeev's point is why the word regression regression is the opposite of you're regressing towards the mean that's where it comes from but where did this algorithm such a big algorithm and the most one of the most used algorithms of machine learning why is it called regression good question i didn't finish the loop at all it turns out galton galton's disciple was a yet another great mathematician and data scientist named Pearson. A Pearson discovered the concept called correlation, which we'll talk about soon. Pearson correlation, two things are correlated. And then people realize that his correlation and what Gauss had discovered, and Gauss and these people had discovered as the best fit line to make predictions, there was a deep connection between the two. And so, because for a whole lot of historic reasons, the regression paper of Galton was so famous after some time that retrospectively, that whole class of activity in which you predict a number began to be called regression. It's just a colloquial use of term stuck and we called it regression. There is no reason to call regression. When you predict a number, there's no reason to call it regression. But it just got the name because of the deeper connection in a single variable between correlation and the correlation and regression and it's very so the whole history of this field is quite uh odd i'll give you a fact i was at a of all things one of my nieces wedding in sedona arizona and her professor was from ohio state university or something like that university of ohio or something and he was there and her professor was from Ohio State University or something like that, University of Ohio or something. And he was there and he was a biologist, geneticist, and he was writing of all things, the history of statistics or data science from their perspective. And the amazing thing is he rooted it all the way back to Galton. He's literally writing a book and he was in that, all the way back to Galton. He's literally writing a book. And he was in that he's put it to Galton. And I was just talking to him casually. And I happen to mention that it predates Galton, it goes back to Gauss. And he was like, wow, I didn't know that. It's sort of the history of these fields are very interesting. And because the word Galton's paper regression towards the mean became so famous. So that word regression has stuck and has gotten more broadly applied to this entire process of this. And which is why this negative word regression is a negative word. It has stuck to one of the most powerful class of algorithms in machine learning. Also maybe the real number that we got there. No, nothing to do with that. That is nothing to do with this. It's Galton's fault, got it. Was that? It's Galton's fault, got it. Galton's fault, yes. Right. It's probably British, I think. Oh yeah, of course it's a Brit. If something is wrong with the world, you can be sure the Brits did it. Americans and Indians will agree on that. Yes, yes. And since we're all Americans and Indians here, we can blame the Brits. All right, so now comes the second part of the story, guys, the more interesting part. All right, we learned to quantify the error. We need to reduce the error. So let me reframe the question this way you would agree that suppose i have data like this and again i'll just i won't make as many points because if you randomly draw as many lines as you wish you might still miss this line what may be the best given this data right so the brute force way of just randomly drawing lots of lines and then picking the best amongst them only gives you the best amongst them it gives you the best amongst them. It doesn't necessarily prove there is no mathematical evidence that it is truly the best hypothesis given this data, right? So how do you do that? And that brings us to a very interesting idea. So let's work our way through it because it will bring us to the process of gradient descent. And many other methods will come to it and generally optimization theory. But I won't go into optimization theory. We'll stick with the gradient descent. The way to think about gradient descent is this so let's look at the error you have the equation of a straight line so suppose you do once again you you pick this thing you randomly pick a line any line let's say this line this line has an equation what what did you guys say mx plus c or b c right whatever so now we will change our notation a little bit it is a convention in higher mathematics especially in machine learning in machine learning in particular that we often get mixed up between what is observable what is an observation and what is a construct in our mind? What is a part of our theory? Are we together? So when we draw this line, it's a hypothesis, but what is the observation? Observation is temperature was certain, this much ice cream sold. Those are the actual observations, isn't it? So observations were X, Y, temperature, ice cream sold. These are actual measurements. You can go measure it. That which is a measurement is called data. Now, the convention is you use Roman letters for them, X, Y. But the moment you talk about things that are constructs of your hypothesis, of your theory, you tend to use the Greek letters for them, right? So Greeks, so this is Roman letters, Roman alphabet. And the Greeks, they use alpha, beta, gamma, delta, etc. The Greeks tend to have a letter system, alphabet that's different. And so what you do is you tend not to use Romans with that. So then suppose I did that and I wrote it. So, and so this equation is more written as c plus mx. I can write it like that. Suppose this is the zeroth order x. Suppose you write it as beta naught and beta 1. What is the 0 and 1 stands for? You could write this equation. Let's go back to the equation. You can say mx plus c is the same as c x to the power 0, x to the power 0, plus m x to the power 1. Isn't it? if this could if i replace this with beta naught zero stands for this power which power of x and beta one why beta beta because it's greek right line the intercept and slope is not something you can go up take it out of your pocket it's not real right it's it's not there real, right? It's not there. It is something that is a constructor. Those are pieces of a hypothesis that there is a line, a model between them. So beta naught. So in this book, in machine learning, you'll often find that the equation of what is called linear regression. So what we just did, just did line as a hypothesis for regression what we just did is simple linear regression simple i will keep under bracket what we just learned is something called linear regression. And this equation we will write as beta naught plus beta 1x, right? Where, again, there is no magic here. This is your intercept. We're just changing our language a little bit. This one is, what is beta 1? Slow of a line, right? Beta naught and beta 1. And now let me introduce you to a different space from reality, a dual space. This dual space is the hypothesis space to the reality, to the data. there is a dual space the hypothesis the hypothesis or parameter space parameter space so let me let me say what the The observed data is what? Observed data is this. Any one point has an x and a y, isn't it? So you say that if this point now, what we say is this data point belongs to R squared. So this is a two dimensional plane, belongs to a 2D plane. So this is a two-dimensional plane, belongs to a 2D plane. Now imagine another world which is made up of where the axes are beta naught and beta 1. Remember, you can't see this. This is an abstract space. Now you realize that if I make any line here, suppose I make this this line has a unique beta naught beta 1 right the equation of this line will be beta naught plus beta 1 x so it has a unique intercept and slope for example this is a beta naught and the slope is example, this is beta naught, and the slope is beta one. Are we together? So what it means is corresponding to this hypothesis, there is a point, well, okay, let me be a little bit more precise, because I believe this point beta naught was negative, and beta one is definitely positive. was negative and beta 1 is definitely positive. So something like this. So let me take this. Would you agree that this line in real, this hypothesis that you put in the real space actually corresponds to a point in the hypothesis or the parameter space, isn't it? Likewise, if I draw another line, which is, let's say, like this. This is a positive intercept and a positive slope. So let me put it in the slope is not so much. So this is it. This line corresponds to this it in the slope is not so much so this is it this line corresponds to this point in the hypothesis space are we together guys right and and i can go on now what are you doing this is the hypothesis so this is their high power their high power parameter space. People in the books call it the parameter space. I tend to call it the hypothesis space. I believe it's unique to me because it conveys that intuition. And this is the real space, data real or a data space, observed space, the space in which you actually observe the facts, right? Now comes an interesting observation. You know that if your hypothesis, so first is you hypothesize that a straight line fits the data, isn't it? You could have hypothesized that a sine wave fits the data. You're welcome to make that hypothesis. But if you did that, your job now becomes find the best sine wave, right? But here, right off the bat, you make an assumption. Assumption. Let me pick another color. Assumption. A straight line line a line best fits the data this is your assumption becomes the task isn't it because each line is a hypothesis that is wrong to different degrees right now how do we do that how do we solve this problem so now we realize one more thing for every point for every beta naught beta one line there is a certain error right every hypothesis h there is an error associated with this hypothesis with this line right now let's work out so now we'll grind through a bit of very simple mathematics suppose you say that your error is some squared error, right? Or mean squared error, whatever it is. Let's take this now. And I won't write the SSE over and over again. It is yi minus yi hat squared, sum squared error, right? Now, yi hat, the prediction is your y i hat is beta naught plus beta 1 x. This is the hypothesis, isn't it? Let me forget the i's here. Forget it. This is your hypothesis of a straight line, isn't it? So when you plug that in, you realize that this is the sum over all the points y i minus beta naught minus beta 1 whatever point all the points sum and square them find the difference between them remember this is your y hat i so far so good guys i haven't done so this is the most crucial thing. I have not done any magic. All I've done is plugged in the equation of the hypothesis here. Right now, notice this X and Y are the data you're learning from. You're trying to tell the line and see which line fits best. So when you're trying to find the line that fits best, do you think X and Y can change? Reality cannot change. change observations are there each of the points is there you can't move the points around that would be cheating right so you can't do this that take any line and then move the points to fit the line that wouldn't serve the purpose would it instead what you want to do is keep the points fixed data fixed you say this is the data now let me move the line around to fit the data so when you move the line around to fit the data what are the things in this equation that can change this and this can change these are the only things that can change isn't it and therefore you say that your error is actually a function of beta naught and beta one, right? And now going back to our hypothesis space, see, these are all vectors in the language of mathematics. You consider this to be the beta vector. Beta vector is beta naught, beta one, right? Or in the high school language, you would write it as a beta naught i plus a beta naught j remember those notation of vector notations you would write it like that but modern notation is you just write it like this one on top of the other it's called a column vector right as a matrix like that right because why because if you're talking about 30 dimensions you may run out of alphabets i j k l m n o p what happens after you get to z right you may exhaust all of them so instead but if you just stack them together you can keep going on so you write it like that so this is really a function of this when you work it, you realize that it is quadratic. Look at this equation, i. It will be, if you just work it out, and I invite you to work it out, it will be the actual value squared plus beta naught squared plus beta 1 squared x i squared minus, you know, the two y i beta naught minus i need not write the rest of the terms you can figure it out right you know how to square but observe one thing e is therefore it is a function is a quadratic function of beta naught and beta 1. In other words, the highest degree of betas is square, isn't it? How do quadratic functions look like? So they will look like this. Now, interestingly, you notice that error. Can error be negative? It is a square. It is a square, right? Can squares be negative? So you would agree that error is greater than equal to zero, right? But it isn't even equal to zero because when you factor in the irreducible error, remember the irreducible error. Where is the irreducible error? Look at this. You have irreducible error where is the irreducible error look at this you have irreducible error always isn't it even your best fit line gets this prediction somewhat wrong right so in reality what would happen is that your error is actually in theory in practice e is of course greater than zero of course, greater than zero. Right? It is greater than zero. Now, think of a quadratic function that's always positive, that can go off to infinity. Can you make infinitely huge errors? Of course. So the data goes like this, and you make your hypothesis that the data actually goes, I don't know, the opposite of this. You'll be making huge errors vastly greater errors so you you can convince yourself that the shape of the quadratic functions they look like a parabola parabolic right they look like a parabola do you remember this is your high school coordinate geometry so now think about it this way. There are three axes. Error, E. This is your beta naught. And we are again looking into the parameter space, beta 1. Every point, every line that you make in the real space is a point in the hypothesis in the plane. So imagine a floor. If every hypothesis were a point on this floor, floor of this room, right, of this, this floor, let me just call it the floor. You agree that every hypothesis is a point, or rather every point on the floor is a line in the real data space. Associated with everything, there will be an error, right? There'll be an error vector, error beta naught, beta one. There'll be an error. When you look at the shape of the error, the error is, looks like this. There is some hypothesis, some preferential best hypothesis. Beta naught star, beta one star, which leads to the least error. E star, the least amount of error, E star, the least amount of error, isn't it? The error surface looks like this bowl rising above the floor of the hypothesis, the parameter space, isn't it? And this bowl has a minima, the point you agree with that and whatever lowest point the the projection of that lowest point onto the floor represents a point that is a best hypothesis with the least error right that line is the best we will do it is demonstrably with this data the best hypothesis can we find it so think of this bowl if you were at anywhere in the bowl imagine that you're a little guy somewhere on the bowl right in the inside of the bowl and how would you go down to the bottom of the bowl let's try this i'll take this grape i'll put it somewhere here right and i let it go what happened it takes the it takes a path that is straight down, isn't it? So another way to take it is, if you, or another way is imagine that you are hiking in the hills, and there's a valley down there. One second, this seems to be not mine. Suppose you're here. Do you think you should go around something like this? This is your best way? to be not mine they seem suppose you're here do you think you should go around something like this this is your best way down or is the best way down just to tumble down tumble down just come down so uh when you are up let's say on mission peak or somewhere near nearest hill the the one part that you don't want to take is the steepest part down. So what you do is you come slowly down back and forth right, but when you are in the hypothesis space you're in a hurry to come down to the best, so what is the path you would want to take the part that as a human being you wouldn't actually take in the hills you want to tumble down right straight down that you you want to take the steepest path down are we together that steepest path down path you want to take the path of steepest descent. And if you take a path of steepest descent, right, you will ultimately tumble up here to the optimal point the optimal hypothesis in the parameter space or the hypothesis space do you see that guys now how in the world do you find so at any given point you want to find which direction is the steepest so you're in a hill and you look around and you say okay which path is the steepest and you go boom right you take a swan dive on that path make sense and you're on the way to the valley with some broken bones right that's what we are going to do in the hypothesis space so now we need to grind through a little bit of mathematics of finding the steepest part. So far so good guys. Now I'll take a breather and ask you, so far is there anything unclear? I do have. So why do you need to take a path you already know the minima? You don't know the minima. So, okay, so very good question. Raj asked, you know where the minima is. The point is you don't. In this particular case, because it's a quadratic equation, you can just find the minima by setting the derivative to zero, right? It's a very good question. Why not say dy dx to zero or de? So the question is, why not so this so the question is why not partial derivatives to zero and wherever it is it has to be this point you're done yeah yeah you can do that this is what you do in high school calculus the answer to that is it turns out the reason we use gradient descent is quite interesting for this very simple problem actually the the error surface so now you this bowl, the formal term for this is error surface. Error surface is, the formal term is, it looks nice parabolic. The mathematical term is it is convex. When it is convex and there is a global minima you can of course find it and in fact for linear regression this problem some of these libraries they don't do gradient descent they just go straight find dy dx set it to zero you do it using matrices it's very easy to do it you get the answer in close form the problem that happens is I am just getting started with machine learning. For most real world problems, the error surface looks like, in fact, I'll show you, why don't, why don't, that's a very good point. And we'll stop with that. We'll do a little bit more, but let me show you what a real loss surface looks like. Right? And then you will realize the whole world of trouble that we get into in machine learning and what makes the subject so very fascinating. Now, guys, I have to continuously search between my reading glasses and the bane of getting old. Let me. So not for linear regression, but in real life. Gallery Explorer. Yeah, are you guys seeing my screen? In real life, a lost landscape, the error surface is called a lost landscape for reasons. But in most complicated algorithms or situations, a lost landscape looks like this. when you look at this do you see a lot of ridges and valleys right so this surface has many many minimas many many places where the derivative vanishes but you want to go to the deepest valley you want to go here to the bottom here where my mouse is can you see my mouse on the screen yeah you want to go here how do you go here if you said dy dx to zero you will be in any one of the infinitely many places no that's what i'm saying quadratic is a very simple situation for straight straight lines and planes as we make progress with this course and with the theory, it turns out that life is rarely as simple as this. Actually, one of the reason linear regression is so popular is because of the fact that the surface is quadratic and solutions are easy. And so, but when we are building the whole machinery of machine learning, I'm deliberately taking you towards things that generalizes to horribly complicated situations. So gradient descent is a method that generalizes and works for very complicated situations. Are we together? But you have a point for just linear regression. Why not just say dy dx to zero? Just say the rather derivative of the error surface to zero. And then you have the answer. It would work, but don't stay with that intuition, because that doesn't generalize forward later on. Do you see how complicated it is? Just think about it. Right. And if you were a mountaineer hiking, you would continuously keep getting stuck all over the place, which would be good news because you don't want to take a dive, swan dive straight from the top of a hill to the valley. Isn't it? You want to gradually come down. So now this is an intuition that you should keep in mind. And this is beautiful. So as you can see, we're just getting started. Let's put it this way, into this field. But we are building our intuition. We are just building our muscles at this particular moment. I said you need to do the path of steepest descent. And then somehow the word gradient descent comes in. Now what in the world is this gradient? You hear the word gradient all over the place. You go to a hill and people will say, oh, it's a steep gradient. What do this word mean? It's pretty much the intuition, but we'll quantify this a little bit better, this word gradient. So I will take an example. Suppose you have, so look at the equation of a circle. By the way, as I said, I'll take simple examples and generalize. And a little bit, you have to take me on faith that when you generalize to more complicated mathematics the thing remains true still what is the equation of a circle that you know of x square plus y square equal to r square is equal to r square so in other words this is suppose i write it like this if it is x y is equal to this and a circle is just a constant surface like if i say f x y is some constant c square or r squared then r is the radius isn't it i take another r1 r1 i could take another r2, right? And so forth. So you say, well, that is good. Every constant value of the function produces a circle. Would you every? They are all concentric circles. Right? So now we will do something. Take a point. Where have my colors vanished? And yes, here are my colors give me a moment guys and my glasses again and so this argument please pay close attention to is the last big argument we'll make today let's take an arbitrary point x y right so let's say that this is your x axis this is your y axis right now this vector is the x vector x vector is the x y point that's right x y right or in the language of machine learning okay let me just leave it as x y for now x and y i'm writing it as a column vector because now we you're familiar with this notation is this now look at this function what is the partial derivative of this function with respect to x which means a treat y as a constant what is this equal to 2x 2x likewise partial derivative of f with respect to y is 2y right and so what people do is well they do something that's a, they have a thing. Let me introduce you to something. Well, you know, pyramids look like this. Inward, turn the pyramid upside down. It becomes this symbol. Mathematicians use the symbol, which turns out to be a little bit more useful in the pyramid. This of f, x, y, it turns out is a vector, which you can write simply by stacking this thing together. If I stack these together, in other words, is equal to this. In other words, df dx in high school language, if we write this, do you agree that this is a vector? No. What is df dx is this. Do you agree? Yeah. Do you agree with this? So suppose I construct a vector which is called this. Literally, I'm constructing a vector with these two partial pieces. I can construct this vector because I feel like it. Now, it turns out that when you were to conceive this vector, it turns out to be profoundly useful. In fact, it's one of the workhorses of mathematics and in particular of machine learning. The moment you do this, this will be equal to two in this for our case. This is 2xi plus 2yj, isn't it? In high school language language you would write it like this which is the same as saying 2x 2y in a different notation so far so good raj making sense now you say what use is this so we'll figure out what use it is a little bit later but for now we notice something very interesting this is a vector and this is equal to two times x and y in this particular case, right, is equal to two times the x vector itself, because that's what the x vector was. So now, suppose I write, or I draw this vector with the, let me, which color would you guys like me to choose? Green it is. Let's say green. At this point, at this point x, if I were to ask you this vector, which direction is it pointing to? It is also pointing in the same direction as x, isn't it? It is just twice as large, 2x. Gradient of f is this, twice the x vector. Is it making sense, guys? So gradient of f is just 2 times x vector. So they both vectors are pointing in the same direction and i'll let you ponder over it any no mysteries here guys it's simple like that isn't it true now what is this direction suppose you are at this point and you want to go to a direction of fastest increase in f which direction would that be at any given point on the circle if you want to go in the direction of maximum increase of f you would go radially outwards isn't it in other words you would go in the direction of the gradient right to maximally increase f you go along the gradient of f on the other hand if you move the count of that point is if you move along the tangential to this circle then what happens if you go tangential does f increase at all no you stay at the same radius right so these are the two opposites if you move along the surface i mean if you suppose you are in a circle you move little bit along the arc you're not increasing the radius but if you want to increase the radius the function you go outwards common sense right it turns out that this is a general statement very general statement to maximally increase f in this particular what is true for this very simple statement is true for all of mathematics so let me write that profound statement here at any point x and now i'll just use x as a vector at any point x right x if a function fx exists any function it could be not necessarily x squared plus y squared, right? Any function of x exists, of that vector exists, right? Exists and you want to go one step let's say one step in the direction point off the gradient take that step you allow if you if you have to take one step and you want to maximally increase f you see the greatest growth in f that step you should take is along the gradient of f at that point at that point x are we making sense guys right this is a this is a universal thing of calculus right if you go along the gradient you you increase fastest but now let's relate it to our error surface so suppose look at this what happens is that somewhere there is a minima around it are concentric shadows of the lost surface right suppose you are here right what do you want to do? The gradient is pointing outwards. Gradient of the error is pointing outwards. At any given point, there's an error. The gradient of the error is pointing outwards, right? If you want to find that best point, beta naught, beta 1 star, I'll put a star here optimal point should you follow the gradient or should you go against the gradient against the gradient you want to take a step against the gradient you want to take a path that looks like this this this and finally you'll come here right start taking steps against the gradient. Raja Ayyanar? Right, so, in other words, suppose you are at a point now remember this is beta space right so yeah you would say that the beta. beta, you're at this beta. And then you want to go to the beta prime. Beta prime, the best beta prime that you want to go to is wherever the beta was, this point, beta, minus. So gradient is there. What is minus of the gradient? A step away from the direction. The negative of the gradient points, therefore, in this direction, minus gradient t, right? Isn't it? If gradient points this way, minus of gradient will point the opposite way. Take a small step in the direction opposite to the gradient. And when you do that, right, you're going home. You're going to the best point. Am I making sense, guys? And this is the celebrated equation, the great equation behind most of machine learning this is the gradient descent equation descent equation and it says that once you can quantify the error on the error surface just find the gradients and keep taking steps in the hypothesis space in the direction opposite to the gradient. And it will take you home. Because you know that at the bottom, gradient will vanish. Slope is zero, right? It will disappear. Now, this is the famous gradient equation. Now, I'll try to bring the same intuition before we end and we are about to end with a for and break for lunch with a very simple like this is all this gradient and hypothesis and so forth so let me bring it down to earth using simple one-dimensional one-dimensional calculus take any function. Let x one take. A single variate calculus. Single variate. Suppose there is a function fx. Now, take a point a. Take a point A. Take a point B. Suppose you are at XA, far so good this is the f x b so far now think of slope in single dimension one variable uh this gradient is nothing but the slope now what is the definition of a slope? Slope is the amount of increase of F if we take a unit, if we take a unit very small unit unit positive step we are tiny tiny ants so suppose you are a tiny tiny ant and you are here and you take one unit positive step. Right. So that you are at XA prime, one unit step. How much did F increase in this particular case? F will. Will it increase or decrease? So this is it, f, x, a, right? So the slope is negative or positive? Negative. This is negative. So negative. So let me color it differently. f, x decreased. So negative slope. So if you want to go to the best point, which I will mark as x star, what do you want to do? You want to move this way. you want to move this way would you agree that if slope is negative it is better that your next step should be and if you're at a the best thing that you could do would be the next would be x next is equal to whatever your x a is and because slope is negative you want to take a positive or negative step positive right you want to go in increasing x so you will take negative of slope at a at the point a you want to move in this direction right negative and you can take a small step multiply it by a tiny step take a tiny step. Take a tiny step, alpha. But you would do this. Alpha is like 0.01, because we are ants. We have tiny little feet. This is a parameter state, right? No, no. At this moment, I'm just talking calculus. I'm forgetting everything, just going back to calculus. So you're just trying to go to the minima of f. You would do this, isn't it? Yeah. The answer is yes it is parameter but let's not bring that in yes this is it now what about b at b if i do i take a tiny if i do a positive unit step xp prime what would happen to it will f increase or decrease did f increase or decrease increased because f increased the slope is positive slope that's literally the definition of slope positive slope the unit increase the increase in function for unit step is the slope now at this point you would say that do you want to go is x prime a better value or should you do that or should you be what are you trying to do if you're trying to go home you actually don't want to go to x b prime right you want to take a step actually in opposite to that would you agree so at b here x let me use this and be used the same thing X next here should be the X B now slope is what positive so what do you need to do D F D X B let me just use just use b here like say xb this is this is negative to smaller than zero and this is positive to zero so if you want to go home go towards the green dot what should you do your x next should be in the opposite direction to the slope right right? Minus alpha. Does the intuition make sense? You take a tiny step against the slope here also. So now there is something common. Both here and here, both here and here, you took a step opposite to the slope. Do you see that? Right? So alpha is the slope or the step? No, no. Alpha is the step. d of dx is the slope. Derivative is the slope of the step no no alpha is the step d of dx is the slope derivative is the slope so you always take a step against that now if you notice this equation is the same as this equation so in other words irrespective of where you are x next your next position should be or is guided by whatever your position is minus a tiny step against the derivative against the slope dx you agree with that right whether it's a or b it doesn't matter and this is gradient descent in one dimension from this if you want to generalize, it's a straightforward generalization. The only difference is we are not in the X space, we are in the beta space, hypothesis space, and so forth. So gradient descent is quite interesting. It basically says that imagine that you are a genus of a citizen of a hypothesis space. Function space, every function. See, people use all sorts of words. They use field and this and that, right? And the moment you use field, a tribe of people, mathematical physicists, they get very excited. Roughly speaking, a field is, if for every value of x position, so imagine some space, you can find a function. Generally, if the function is continuous and differentiable, even better because that is well-behaved function, smooth, continuous functions, there are actually surfaces rising above it, just like this error surface rising above it. Those fields are the error surface or some surfaces rising above it. Those fields are the error surface or some surfaces rising above the actual plane. That's sort of a very rough idea of what a field is. Now in such fields, the beautiful thing is there's always a way home, a way to the minima. What do you need to do? Just start doing gradient descent. It's almost like before Google came up with Google Maps, the mathematicians already had a sort of navigator built into their function spaces, right? To go to the minima, you just had to keep going against the gradient. So this process of finding the minima of the error is called optimization, the minima of the error is called optimization. There is a vast amount of literature on optimization. It's a whole field. People do enormous number of PhDs in optimization theory. Gradient descent is only one of the approaches. Machine learning in particular uses gradient descent heavily. It finds that it is the only thing that sort of at this moment we know works well other methods don't work so well except for simple situations like linear regression etc they do work now there's a lot of research to use quadratic and other sophisticated methods but broadly the what i've taught you is the simplest form of gradient descent. Now there are some more downtown cousins of gradient descents, more upscale cousins of it. We'll learn those over time, but we will end our theory session today with asthma. So let's review what we learned and then we'll take an hour long lunch break what did we learn today we learned hopefully something useful let's see did we learn anything useful yeah right we learned about learn first of all that let's focus on understanding right understanding is more important we let's focus on understanding. Understanding is more important. We'll focus more on understanding. If you don't understand, stop me. Stop me again and again. We need to make sure that you understand. In this course, there are certain things that are core. Core are lectures and labs and homeworks and quizzes and social discovery. These things are necessary. The optional things are paper readings projects and presentations if you do all of them on poverty you'll do very well and when i say this i mean it people who have done all of these things seriously they have sailed through the interviews with some of the biggest, best companies and done extraordinarily well. They are also doing extraordinarily well in their current jobs, applying it. They're doing extraordinarily, some of them have gone on to do PhDs in graduate schools here in top tier places like MIT, Urbana-Champaign, etc, etc. So we have a pretty good tradition. 1200 engineers have been to this place and they are all doing quite well actually. So in case that is a factor to keep you motivated, do that. Now machine learning, we learned what is learning. Learning is easily quantified as a reduction in error. So we need to do two things. first to quantify the error and then we'll have all sorts of means for regression we can look at mean the absolute error or the square of errors and then other things can come in between right and, by the way, people sometimes. mix and match the two, for example, there is something called the Huber error, which in one circumstances using absolute, in another circumstance of points uses the squared and so forth. A lot of literature there, we won't go into that. We will take the least square, method of least square, which was done, created by the likes of Gauss, Legendre. By the way, these are the giants in the field of Gauss, Legendre. By the way, these are the giants in the field of mathematical physics. So then comes the question of reducing the error. Reducing the error, it turns out, instead of randomly making all sorts of hypothesis and picking the best of the trials, there is a lovely process, gradient descent. It's a workhorse of machine learning, and it helps you reduce the error. We sort of motivated the example with two stories, one of children in a simplified meadow with cows and ducks, the other with a young entrepreneur trying to sell ice cream based on the temperature of the day. So we use that to quantify error. We talked about two class of algorithms, regression and classification. Regression, you're predicting something measurable, like how much ice cream would get sold. Classification is an identification task. You look at this thing, this animal, you look at it all around and you say, oh, it's a cow or it's a duck. The ability to identify is classified right that comes from the class cow and duck are classes right and you're only allowed a category of two classes the input to these models are called input literally and the outputs are and people also call it the feature vector the and many, many things. The output is called, well, output, response, predictions. I'll often call the output just predictions. You'll see me use the word prediction. I've circled it. Input, I've circled, and prediction, I've circled. But I tend to use these two words quite often. We talked about where the word regression comes from. It has a very interesting and peculiar history for the negative word regression to be applied to it. We talked about Galton and his regression towards the mean. We learned about irreducible error, things that the model knows and things that the model doesn't know. And irreducible error is the acknowledgement of all that it doesn't know out of its control in modeling. Right. Then we learned of the powerful concept of gradient descent and how for any function, it can help find the minima of the function. Right. Even if the function is not necessarily convex. We did it applied. We applied it to our situation. We learned that for linear regression, the error surface is like a bowl with one unique minima. Now, even though we could find that by taking derivatives, we chose not to because it turns out that gradient descent is a more generalized method applicable to all machine learning. So we took that approach instead. Right. And so we found that gradient points to the direction of maximum increase of a function, if it is the error function, we want to go against that. To get to the point with the least error right, and so we apply gradient descent to get to the point of least error, and this is a celebrated gradient descent equation. We follow that and I again motivated this by saying in one dimension it looks a little bit more obvious perhaps. Right. And so that ends our theory session for today. We are going to start with labs.