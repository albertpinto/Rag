 The first thing, if you have seen the website carefully, you will notice that I have converted this document, cleaned it out and put it there, AI in the Cloud with Java. It's the same notes, but some of the clarifications that you were asking for during the class are here now. So just to recap, we have gathered here to look into considerations of deploying an application to the cloud. In particular, we took the Kubernetes engine of Google, which is extremely resilient actually, very, very scalable and resilient. Google brags that it gives you, you can create planet scale clusters and so forth and applications, scale outs and so forth. And I do believe that they are justified in their bragging rights because internally that is what they use for everything. So it's quite nice to see this open sourced, such a robust framework and through experience in my teams that work with me, almost all of them who use Kubernetes, I mean all of them, the ones who do use Kubernetes, they all swear by it. So it's a worthwhile technology to get into. It is something you can deploy locally on your own data center in your own servers. And in cloud, of course, it comes as a service, much easier deployment. You just click a few buttons and you have a Kubernetes cluster. Of course, the price that you pay is that the cost of running it in the cloud is quite high. For example, a modest cluster, as you saw last time when we configured it, easily came to four or $500 a month. And that was a puny little cluster with hardly, you know, 16 gigs of RAM and four cores and some 30 GB of storage and so forth, creating three instances of that. So the the that is always been the trade off in the cloud. I always see it as essentially 10x more expensive, or 20x more expensive in terms of sheer hardware cost. But where people believe that it pays for itself is one is the standardization. Now, the other is with, you don't have to maintain an IT staff. When you maintain an IT staff that can try to do all of these scripts, first of all, they might produce substandard results. Whereas here is one company whose entire mission is to give you good IT services. Cloud is nothing but their data centers and their IT people providing you services. So they give you more robust services. The other aspect is within the US, and I suppose if you look at the money aspect of it, as I have to, if you look at a engineer, even the engineers who do your deployment and IT, by the time you get their salary, the bonus, the health insurance, and all the incidental costs and everything, the outer financial envelope always comes to 250k plus. Which means, and you never would trust having a team of less than four, you need a quad, you need a proper pod of people to maintain any kind of services that you deploy to production and so forth. So by the time you have a reasonable staff of let's say 10 people you're looking at 2.5 million dollars in um in basic overheads in i mean in sort of a salary overheads and i suppose what you end up saving is you say that all right for 2.5 million i will get something which i don't know how good these people may be but I could instead keep just one or two DevOps I mean one or two IT operations guys to manage the cloud operations and let the cloud people do the rest of it so cloud gives you that simplicity once you factor that in the cloud providers would have you believe that it even south or is just as expensive though I think would have you believe that even South is just as expensive, though I think it depends upon. Like if you are not into core technology group, you're better off with the cloud because you will have hard time building that core competency in-house. On the other hand, if you are a tech shop, of course it's far less expensive to have an in-house data center, which you use for most things. As an experience in my team, we have, or I have instituted a rule that a large part of the development we do on the in-house data center, testing, et cetera, because it's a bit expensive taking, doing it in the cloud. And then we go to production in the cloud by and large. Deploy to the cloud by and large. But some people don't do this. Some teams, they will only develop in the cloud itself, but we stay within the budget. Some people, even now, just go to production in the data center itself, though we are moving them away to the cloud. So it's a mixed bag at this moment. And as you probably know, but there are, it is not a jury's out on which is the better option. There are many, many companies which have saved, literally saved tens of millions or hundreds of millions in times, I think for some big use famous cases. Simply by getting out of the cloud and back into the data center. Because they realized that with sustained operation it was too expensive to be in the cloud. So they did a hybrid solution. I believe Walmart also has a hybrid situation at this moment. Anybody from Walmart here? Oh, this batch is nobody from Walmart. Okay. So many houses are doing things like that. So that forms a backdrop of what we are doing. Now, what is the context of our lab? In this particular project, a lab, what we are doing is, we are going to definitely use the Google Cloud because we have been using it for the entire workshop in Jupyter Notebooks and Collabs and whatnot in the cloud. We are going to use the Kubernetes engine there. Now what do we do? We need to create a Docker image and push it to the cloud. Now the Docker image of what? Our application, our microservice, our backend microservice. So the UI will interact with the back-end microservice. Now, when you were doing your projects, you didn't need to. For example, if you do it with Streamlit, then your Python file itself is the application. And you take that to the cloud. Oh, by the way, I must apologize. I'm not sharing my video because there is no video on this Linux machine. Apologize for that. So anyway, that is a one single application as it is. But in real life, when you do a high-scale application, enterprise grade application, the typical structure is that you have microservices exposed which are scaled up, typically at Kubernetes, and your microservices are in Docker containers or containers, there are other containers also, different containers, which are managed by Kubernetes, and you have these parts and scale outs and so on and so forth. You said elastic limits when the load is low, the number of cluster nodes decreases when it is high, it increases. You have vertical and horizontal scaling and so forth. And you have a ton of monitoring, ton of health check and dashboarding and so forth because the worst thing that can happen is when you are getting millions of hits you go down because customers remember that everybody remembers when google has an outage but they sort of assume it should be working all the time same applies to enterprise applications so that's that but now the question so the front end so you you have microservices. Microservices these days, it is popular to write it in three frameworks. One is Node.js. Node.js is a JavaScript framework. It came about after, remember I was saying that Google created a very high performance JavaScript rendering engine, JavaScript compiling, and so forth. So using that and saying we can run it standalone on the server side, it becomes like an application container. And that is your Node.js, starting with that idea, you build out the Node.js framework. Very popular amongst the JavaScript crowd. Its advantages are if you're very familiar with the front end, you can just write your back end with that. And many, many companies are adopting that. The second framework is you can write your back end microservices in Python itself. If you remember some of the early labs that you did, I encourage you to write it in Flask or Fast.ai. What is the advantage of that, especially for data scientists, the advantage is that, you know, all data scientists are familiar these days with Python. So and microservices are straightforward, just little routes or endpoints you put in your Python class, and you are up and running. So that is the Python way of doing your microservices. And then there is a third way, which is using a JVM. And one of the languages that the JVM supports, Kotlin, Scala, Java, Groovy, and so on and so forth. So there are trade-offs or advantages and disadvantages to each of them. The advantage of creating these things in Node.js is, you can quickly, today you have tons of people knowing JavaScript. It has a very low learning curve. You can do it. I suppose the downside that I see is that debugging Node.js is horrendous. Stack traces are not as elegant as what you find in Java, for example, where it is very easy to pinpoint the source of problems. In Node, it's a little harder. And by the way, voice your opinions, guys. If you are using these alternative frameworks in your uh wordpress you can voice a contrasting opinion because none of these are written in stone i think there's a there's a perspective at most that i can say i'm giving not giving out the truth with a perspective so node.js has that limitation but barrier to entry is very low. Quickly, any JavaScript developer can be writing a microservice and getting the job done. And then obviously you don't get the robustness of concurrency and multi-threading and all of those lovely things that you get in Java. Python, the advantages and disadvantages are, Python gives you, especially for REST, etc., a fairly elegant syntax, clean syntax. Of course, the learning curve is you have to learn Python, which for data scientists is a no-brainer. Now you can take it to production. Flask is the default thing that most people use, but when you use flask, actually performances are compared to enterprise grade things like a Java enterprise server. The performances can be anywhere between 20 to 100 times slower, right? Because those things have been optimized over 25 years down to the metal now. So that's a trade off with Python. But Python again has a low learning curve. For the data scientists is very, very appealing. So for example, when you use Streamlit to just write your application, you will testify to the fact that it was far easier than even creating a flask application and a ReactJS front end and so forth. Then comes Java, those houses that are very performance and scalability or reliability focused. Java has turned out to be in the backend, nearly indestructible. I mean, I can attest to it from the fact that we have 100% of time in 2020 for such a massive node system. So it's fairly indestructible. The ecosystem has been maturing now for 25 years and there's a lot of open source libraries that people have contributed. So the big demerit is it turns out that the younger crowd and a lot of you guys don't know Java. It is a slightly harder language to pick up compared to python which is a one-day learning curve or javascript which is a one-day learning curve but you know let's say one week you become reasonably fluent in those languages but with python with java it takes a while to know how to write a good design pattern in a code that is structured well, says Steve Klinger. But the positive side that you get is that you get absolutely battle-tested libraries which have matured over the years. Apache Commons, Google Guava, and many, many libraries which are just absolutely battle tested and application containers to run them the engines to run this code which are also seen about 20-25 years of maturity so the performance is blazing and for server-side work a java performance matches that or almost matches that of c or c plus plus performance which is really admirable. In some places it exceeds that. And now of course there's the additional fact that you can compile Java not to bytecode but to just pure native code. When you compile it to a pure native code, it is like writing C code and compiling it to pure native. And the experience is, is frankly i find it very very good very fast now what does fast mean so why should we get excited about fast see when you're talking about cars fast means more expensive right you go and get a car with more horsepower it can go faster have better acceleration zero to sixty in whatever number of seconds, 5, 6, 7, whatever seconds. And now you're paying through your nose because the car gets twice as expensive or thrice as expensive, or maybe almost sometimes five times as expensive. On the other hand, in the computing world, the story is different. If you write your code, and that code is going to run day after day after day, 24 by 7, for the next and typical lifetime of this code is, well it varies, depends upon how well the code is written. I'm somewhat proud of the fact that, I suppose we all have our bragging rights. My bragging rights is that the code that I wrote for NASA and CSA is still going strong. The code that I wrote for Oracle is still going strong and so forth. So it depends upon where you write. And it's over 20 years now. But most code at least has a shelf life of about four to five years. of about four to five years. So imagine a code that is running for five years on metal, on some sort of hardware and the same code and it's taking a certain amount of traffic load and now in cloud you're literally being metered by how much hardware you use. And now here comes another code which does exactly the same thing, but is 20 times faster or 100 times faster. The implications are immediate. So for example, if the first, the same code needs a million dollars worth of hardware. And let us say in the most fortuitous of all circumstances you can you can scale you can write code that is 100x faster right you realize that you have gone from 1 million to 10 000 okay a 10 000 is the cost probably of a you know a large departmental lunch or an outing. A million is a budget line item that you have to justify. The business has to justify. So for example, what is the inflection point for profitability? At 10,000, you just need to have enough customers to break your... Let's say that you get a million dollars and let's say that there were a couple of developers involved and so on and so forth and you're doing a small business uh if you're doing it in a language that's not very performing if million dollars is just your cloud cost plus there is a salary cost and setup cost and operational cost you are still underwater on the other hand if you're paying $10,000, and then it is quite possible that with the high performance system, we have already gone beyond the inflection point into profitability. So all of these considerations, they become real very, very quickly when you look at the financial aspect of it. And I'm just speaking from experience because we sometimes acquire companies in which there's good IP, but very specific IP, but their deployment thing would be some, you know, Node.js or Python and so on and so forth. Then it could crash and could be unstable and whatnot. And we would have to sit down and make it much more robust and scale it well. So you learn a lot of lessons by that, which is why in this particular lab, I took the unusual way of instead of teaching you the simplest way to go to cloud production, teaching you a way that is actually going to help you in real life gives you a path to write something that you can go to business with and do well i'm not saying that people don't go to business with node.js and python matrices they do a lot of groups actually have rebased on that but i try but there are also a lot of mission actually have rebased on that, but I try, but there are also a lot of mission critical work done on these, for example, much of NASA even today runs off Java ecosystem. So with that, then the cloud, this document talks about the question that we have is quite interesting. See, we are as data scientists in this particular deep learning workshop, we standardized on PyTorch. We could have used a TensorFlow or we could have done both but already the course load was heavy enough so we chose one and I preferred PyTorch. If you remember the prior versions of this workshop I used to give in TensorFlow. PyTorch in my view is far more elegant at this moment, gives you more visibility and what's happening inside. Now the question is once your model is built, how do you take it to production? For development, for training, Python is absolutely great. It has the language to use at this moment for deep learning. But when it comes to going to production, you need something that is rock solid. So can we, for example, deploy it in one of the Java containers at scale? And so that brings us to this open source project. In the context today, we will talk about how to deploy a Python model, whether it is TensorFlow or PyTorch or MXNet of Amazon or whatever, Azure's, Microsoft's thing, all of those, can you deploy it within a Java microservices and scale it out rapidly with Kubernetes and all that? That is the path that I've shown. And by the way, this sort of, if I may say, not many, you won't find this kind of a construction widely discussed on the web. To my knowledge, at least I didn't find that widely discussed, but at least here we are. I set up the instruction because I thought I'll write instructions based on what we do, which for us has been very successful. So now, when we do this, the question is, what are the pieces that I'm going to do? Okay, before that, let me just review what we did last time. So we did Kubernetes setup, almost all of you got it right. This is your cluster, you create a cluster, you're careful, you make three nodes, or you try to take older generation cheaper node or something like that, and then so on and so forth. I added a paragraph here for the Google shell, like if you want to not do local setup, you want to just run it off the cloud, then just use these resources to pick it up from the cloud and people do that or people do that and as far as i can say that they are successful with it but the limitation of your instructor is i myself personally are heavily right from the beginning have been a heavy user of doing it from the unix command line, local setup. So I'm showing you the way I do it and we do it in my team. And we have been, like as I said, for us it has worked out very successfully. See, getting to be proud of and say that we are 100% up-to-date in the world of, engineering is a big statement, especially when you talk about a large enterprise deployment. So here we go. So I set it up locally. You all went through the steps. So I trust all of you have gone through the steps successfully. Let's do a pulse check. Are we all here oh so i was successful till the sanity check when i came to 1.37 docker support i have an error and what is the error you get docker not in system path docker and docker credential g cloud need to be in the same path in order to work properly oh goodness okay those things uh there seems to be permissions issues that were like default as um editor and that we should be switched to storage access or yes and stuff like that there aren't instructions on that let me talk a little bit about that suppose you go to let me go to this new table google cloud console Google Cloud Console. We are here. When you are here, you pick your project. Where is my project? First of all, I need to become myself. My support vector's identity guys. Wait a minute. I need to change my identity. Once you do that, go to this tab. You see this IAM and admin tab. Go here. And when you go here, you will see somewhere here, that this load, you see these members, right? So what happens is that if you notice that your account google service api computing is quite small, it's hard to see. Loud and clear, I can't hear you. Could you increase the size a little? That's very small even on my larger screen. That's better, thank you. All right, so when you go here, what you can do, I think, let me remember which of these two. So what you do is you just go to these and just escalate their privileges see what privileges they have so in this case this particular thing has a editor privilege right so if you wish you can just go and add a storage let's say you can type type a storage storage bucket so go and app engine admin somewhere in here there'll be a cloud asset a compute storage admin. Yeah that's it compute storage admin. Yes exactly. You have to add a few of these and I'm forgetting the exact one so or just give yourself a Kubernetes engine admin right we'll have access to everything kubernetes or engine cluster admin or something like that by the time you give yourself these privileges all of those little bugs they tend to disappear now what happens is in production zone of course your system administrator can be very strict about what privileges they'll actually give you right but for development purpose it's fine you can make yourself pretty, you can escalate your privileges quite easily. So but remember that that's not what you would do in production. Then you have to be super, super careful. And so you can do the same thing. Just go escalate your thing. The particular error you were seeing, Kate, it is on permission setter. header so go and fix it here yeah in fact it pointed me to uh another another um link to deal with that okay and what did you do with that link um i'm on it right now it's um It's container registry access, docs access control. It was right in the... That is right. You need to add those privileges. So this is in the IAM and admin. Go and boost up your privileges and those problems will go away. Harini, you got that? Sorry, sir? Did you understand what i'm saying yeah i did that but i think mine is a different issue which is docker not in system path though i set the environment and all that yeah so i had the google sdk command from last time we installed all that. But I can see the nodes, but I'm not able to, you know. OK, what I'll do is I will have one of the people who are participating in this workshop help you. OK. I'll have him reach out to you to just sit with you and spend some time and figure out what is happening with it. Okay. A person with more Windows experience. Okay sir, thank you. I'll have that person sit with you. Let's do it right after this. Okay sir. So that your problems are gone. And Kate, let me know if you still have problems. I can couple you also to make sure your issues are not so bad. I'm trying to debug it right now. OK. So all right, guys. So assuming that you have all of those support, and if you find out that there's some instructions we need to add to this, some steps in this instruction list that I may have missed, let me know. See, with me, what happens is my environment is fully configured, right? And it's hard to know what steps were necessary fully or I may have missed to mention in creating the environment from scratch. So let me know. As if there's one thing to add there. So I installed Maven from the Apache website, but that doesn't bring the Maven wrapper, right? So in your command, you're actually saying MVNW, that's the Maven wrapper. That is right. That will be created by the Quarkus program. That never comes from Maven itself. It is a script that just wraps around the Maven command to make it run locally, I mean, on your machine. So if you go into your Quarkus project directory, you'll see that. Oh, OK. So it's not on the normal path. It will be only in the Quarkus directory. Yeah, it is a Quarkus-specific thing. Got it, OK. Yeah, let me look for it there. Yeah, So let me show what I mean. And again, let me increase the font size here. Suppose I go to 400. And if you go to the deep learning workshop and you go to well not here or maybe no no this is not here this is in and i'm forgetting all the where exactly did i create this code okay Okay, I'll have to remember. Let me see that. Where did I create it? Eclipse authored app. Workshop coordinates. Let me see where I created the workspace for this project. ML400 Cloud, let's see where it is located. Okay, this is the location in the GitHub. ML400, ML400 Cloud. So why do I not see that CD? Yeah, here it is. I can see the obvious cloud. This is the project. So if you look at this project, you will see this MVNW. It is created by a Quarkus. It's very Quarkus specific. So for example, if you do MVNW, then you'll see that maps to this. And again, I'll expand this. It is nothing but a shell script that wraps around Maven and it takes care of the fact that people are on all such a like for example this is for Windows this is for your Mac you see it should have come with a maven install that's a mistake yeah let me look for it in the project file let's see yeah that is it it doesn't do anything beyond it's basically a sanity check some extra sort of a helper thing to make it adapt to different environments that's all that is that so All right. So we are all together so far, isn't it, guys? So where we are is that we are going to then install JDK. Have we all installed the Java JDK, guys? Yes, sir. Okay, that's good. See, guys, I do realize taking you and just saying, okay, let's take this Python and wrap it up in a flask and take it to production it's a very very easy set of instructions that hardly needs instructions right deploying it to kubernetes is very trivial i'm deliberately giving you an instruction for something that is hard but actually will benefit you when you take things to product in In terms of cost, in terms of reliability and so forth. So worth doing. Alright, here we go. So you install Java, you install Eclipse. If you have not, now is a good time to become familiar with Eclipse guys. There are tons of tutorials on Eclipse. And Java and Eclipse, they go together. The Eclipse Foundation is very, very influential in the Java community. They own the Java Enterprise Standard, pretty much the Jakarta EE. And there's a massive amount of synergy between these guys. So you create this project, we created a Quarkus project in which we included these things. So have we reached this far guys? Have we all been able to reach this far? And when you do a local dev, are we able to see this application come up on our individual machines? Yes, I see. We are all able to see that, excellent. And you do that. And of course, if you do hello, then you will see the hello application slash hello. You see the results. So now how do you deploy to the cloud? So the first thing to know, if you are not familiar with YAML, these days there is a new way people write configuration files. People used to write property files and it used to get very messy. Lots of properties and half a z into place so there are multiple ways of writing uh configuration java world properties files properties files are very popular then uh other file format that's popular is that dot any format ini format which is again a standard i believe there there's RFC or something behind it. And then there are some other formats in which you can have XML format, you can have JSON format. But very interestingly, this thing that seems to be where everybody has converged is YAML. If you ask people why, they'll say, oh yeah, YAML sucks. Yet, well, YAML is very convenient. Let's only use YAML. It's because it's so easy to read. It gives you a hierarchy, which I suppose XML would have given you or JSON would have given you. But the syntax is so clean. You don't have a lot of tags and this and that. Try to imagine this in JSON and you'll realize what a mess it would be. So here, very much like Python, spaces matter, indentation matters, right? And it is key value. So when you say service type colon load balancer, what it means is quarkus.kubernetes.serviceType is load balancer. Are we together? Right? That is important. Now, when you create your application YAML, when you say it is load balancer, what you're saying is that there will be multiple instances of this app. You have to, there has to be a load balancer routing the traffic to the different instances. This is your registry. The standard registry is USGCRIO. You can play around with it and so forth. So we have stuck to the simplest possible things. So you'll have to use your own project name, your own stuff. Now once you do this, what happens is in your uh quarkus application you are saying that this is how i'll deploy to the cloud when you deploy to the cloud you try to deploy what it happens is it will go and check your local um system to make sure that your cloud keys are there. In other words, the secret keys and all of those things, they must have been taken care of here. Where are they? At this level. Right? This command, you must have run it on your, and then you must, if you go to the .kube.config directory, you should see that, yes, your project has been properly set up. So by default, when you run the kube control get nodes, you should see that yes, your project has been properly set up. So by default, when you run the kube control, get nodes, you should see your nodes running successfully, which means that it could get the authorization and it could do that. If that is done, then the rest of the from that machine deploying to cloud is very easy. By the way, this is something that again, I don't know if you're using Maven, but you're not using local Google SDK, but just using the Cloud Shell, I would imagine it could be a bit troublesome because you'll have to copy this code over this project over to the Google Shell. That is a Linux instance in the cloud and then do a MVN deploy. So I'm imagining because I haven't done that before. So here we go. If you do this, then right away, you will see that your application is running, right? Now, once your application comes up and then some of you may have to do this, which you should do only in developer mode because there's a security. So there's a little bit of imperfections here in the cloud, everything is not perfect. So sometimes you can get away with it, but because we work on different environments, Mac, Windows, this, that, so sometimes you do hit glitches. And again, as I said, if you're doing these things from Ubuntu, you seem to have a very smooth sailing actually. So a lot of these issues that you all are having to me comes as a surprise because when I created these instructions, I thought I'd done a pretty good job. Every single thing I'd itemized, right? I had another person QA this, like actually take an Ubuntu and go through all the steps. And the person came back and reported to me that it was all successful, to the dot. And then, but that's not your experience. There is some bit of struggle going on. So, okay, so how do you visit your application? So guys, how many of you have been able to reach this part? Been able to deploy to the cloud? As if you go up, you know, when you add that content to the YAML file, one of the section, the container image. Yes. That's where I get an error that the container image is not allowed. Oh, then again, go and fix your permissions here. Go boost permissions of these accounts. These accounts, these two, right? These two are the accounts that get created to deploy to your kubernetes things right so these are going to be a problem boost their privileges so we need to add specifically storage access and yeah what i do is yeah go go here and add, like add roles or add this. So if you search for this, like storage. Yeah, specifically which ones? Or we add every single one? I mean, like to just add a few. Storage admin and these are Kubernetes. Actually, what I do, if I may just tell you, Cloud Storage, H have not MS services, GCS storage object resources. So what I do is, which I would suggest you do, because you're in Dev mode, these clusters are not there, but just go and full management of the Kubernetes API objects. Just go give yourself this. Kubernetes, just Kubernetes engine admin will cover all of them or do we have to? It will cover all of them. Okay. Because I'm trying to see if that helps. Just experiment. Okay, add that to the compute and. Yeah, that is it. Yes, added those roles. And that should take care of it. If it doesn't and anyway I'll have somebody right after this thing, reach out to you, those of you who are? I'm at the end of 1.4, section 1.4. It's up, that comes up when the web page comes up. Locally. 1.5? Locally, yeah, locally. I can't. So we are talking about 1.5 now, guys. Try it. Let's see how many of you succeed. Do you? So once we add the permission do we need to start or stop something no you don't need to so long as your kubernetes cluster is running make sure it's running you cannot do this without a kubernetes cluster running and these things should match to that you know your group id your application ideas etc et cetera. They should match the entries there. I also get messages about storage buckets. User does not have permission storage.buckets.get. And it keeps referring to me to that page about setting stuff up and add storage privilege also in the storage okay but you know i'll give you the exact instructions or i have somebody who is doing it on a day-to-day basis or help you out if you just Google storage, then App Engine Cloud Asset, compute storage admin full control of computes engine storage. that see if that compute compute storage admin compute okay looking for compute storage admin yeah try it out see what what it does for you try it out see what what it does for you i think um i'm getting it just go and do a storage admin like you know a full control of gcs uh yeah go go ahead perfect yeah i was saying i'm getting an error where it says um starting the container image build and then base image, fabric aid, Java, Alpine, does not use a specific image digest. And then after that, it says build failure. Why don't you share your screen? Share your screen, let's see what's going on. Yeah, right now the roles I have is Kubernetes engine admin, owner and storage admin. So you're saying I need to add something else too? Oh yes, so let me do it one by one. In your case, the base image requires or try again, starting base image does not use a specific image digest. It will be reproducible. that's a warning that you can ignore the the base image trying again so let's look at this failure build format java unable to create container image so this is a local problem application and did you include go down go down scroll down message caused by registry okay credential problem cannot run program docker credential yes permission denied oh okay yeah so another thing is that uh it was giving error some other error so i figured that docker credential gcr was not installed on my machine so i had to manually install it yes uh and then uh i think since it is installed in slash user slash bin uh it's actually sudo so i think i will have to change the permission to run as myself not a sudoer okay guys let me do one thing let's change the session to one-on-one help to each of you and i'll pull a couple of guys and to help you guys individually so we can yeah a lot of permission settings we need. Yeah, let me do that. But let me just walk you through the whole flow. Then we'll convert it into a one-on-one help sessions for each of you. Okay. We'll get you through this. But for the time being, let's pay attention to the flow. The idea is that assuming that you have done the see uh make sure that you followed this step configure docker did this succeed for you yeah for me it didn't succeed yeah for you it didn't succeed highly but for others for rafiq yes yes i had already gone through these steps all the way where it says deploy. Okay. Yeah, the deploy step. Okay, all right. So I guess you need specific help. There's something peculiar in your machines which we need to help fix. Here in this 1.5, the group and the name, there are two properties there, right? So one is your project name, which is group, yes, right? Yeah. And then the name. Application name. It's application name. Yeah, or pod name, it also maps to your pod name. I see for 1.5, do we expect this file to be already there or we are creating this file from scratch? No, if you go to see look at this now open SRC main resources yeah I don't find a file called application dot yaml but if you don't have it then create one yeah so that's what I've confirmed by the way it should have gotten created if you have not you know what it means it means that here you forgot to include yaml configuration that's why so then these steps now you have to meticulously follow if you had included yaml configuration but then it must have been created yeah it is created as it for me yes it is created so you have skipped that frame jitha frame jitin okay yeah i was kind of falling to the dot maybe by mistake yeah okay yeah if you don't follow these instructions they have implications and it could be that some of the errors we have seen is because they're little uh you know small because you're doing it for the first time little things you guys are missing so read the instruction line by line and then do it awesome i'm noticing in the google cloud platform the iam the permissions for my project that um there's not just besides me at my email i also have a project um at developer.g service account.com and cloud service um so there seems to be two other yes those accounts are created those are your real see what happens is you never run see whenever you run an application a process in a linux okay there is a user associated with it sure okay now basic security rule says that a real user should never be firing applications not the root not the not gate not this what you do is you create specific these are called service accounts that's why the word service account oh okay then those still only have the role editor. Should I be adding? Yes, I think you need to add. So hold on. Okay, I think that's- Step by step. So that's that. I see this block, quite a few syntax error it throws. Now that earlier error where you cannot add a container. So the syntax error you're getting is because of the spaces. You have to be very precise if you don't get your, this is the downside of YAML and Python. It is space sensitive, indentation sensitive, a very subtle mistakes and indentation, and it will show errors. So you have to reach a state in which your application YAML has no errors. And once again, I think you know what is happening is there are small, small mistakes that seems to be happening because everything is very new. You guys need individualized help at this moment in a one-by-one basis. So let's convert this thing. Let me go through this entire thing and convert it into a, after I'm through this whole, into just a help session. All right. Are we together guys so far? So assume that this worked, this particular command in 1.5 worked if that works then what would happen is you can visit your kubernetes engine and you will see it run you see this ip address here 34.105.40.63 it will be something different in your case just note it down this is your public ip address to your microservice and now anybody in the world can hit you on this ip address are we together guys is this visible on the screen for you guys Are we together guys? Is this visible on the screen for you guys? Yeah. Yeah. So at this moment, obviously it seems that not many of you have reached this far. What about you Anil? Did you succeed with all these steps? No, I say same step as Arne. Okay. All right. So let's just all solve that problem and that maybe there's something i did with my machine which i skipped or in the dark i forgot what it is so the same you can also get the same ip address by running this command get service so you'll get this command this value yeah so now you make a note of the external IP values, this is it. And then you can go and if you say this, you'll get hello. And if you don't put hello, you'll get the same application page that you've got locally. So at this moment, what have we done? We have a Java application running within our Kubernetes cluster. So we have put together two pieces of the puzzle, Quarkus application, a microservice, a microservice running in a Kubernetes cluster. So what you have achieved is something that will be really rock solid, assuming that your code is not completely screwed up. We'll scale well, we'll be rock solid, we'll perform well and all those nice things will happen. But where is the AI? It is just a Java application that's running in the Google Cloud. We need to now plug in the AI. So that is step three. Now there is this, let me take you to this library, Deep Learning for Java. I should have put a link here to the website. Let us go there. Deep Learning for Java. So it is worth actually reading this. This is open source library to build and deploy deep learning in Java. Now the good thing is that it's agnostic. You can do TensorFlow, PyTorch, whatever it is. And as you can see, the testimonials are from all the big guys and they're all pretty upbeat about it. So it is not a trivial, you know, somebody's GitHub project. It's a project with a lot of energy behind it. So we are going to do that now uh what do we do uh so this is similar to onyx right onnx no onyx is a data export format see onyx started out as a model export format sorry as a portable or an open standard for exporting models, ONNX models. And ONNX runtime, yes, gives you the ability to run those models, assuming that it is exported in the ONNX format. What DJL does is that it supports ONNX, so it will use Onyx or MXNet or TensorFlow or Azure, I mean, or PyTorch. Based on your model, it will figure out which runtime it needs to do and under the covers, it will automatically do it. And that is the power of it. Can you imagine that? You don't have to worry about it. It does it for you. So this is it, beginners. And you can look into the examples, but that is the main point. And you should watch this 10 minute video. Spend some time guys becoming familiar with it, because I think that this will give you a good way because in your team, if somebody is doing some work with tensorflow somebody is doing with pytorch you're not creating two three different servers to deploy their code you have a common code path that will run all of them together right you can literally load one model in tensorflow one model in pytorch in the same java app and one model as a onyx export you could all run all of them together that is its strength so uh uh pravin now you see the value of this right uh yes that is its main thing uh it's very very good so you you see this Apache MXNet, Onyx runtime, PyTorch, TensorFlow. And you can go write your own extensions. All of these. So imagine a single framework supporting all of this. So you don't have to do framework specific runtimes. run times. So for example, you spoke of ONIX. Let's go to ONIX and you see this. All you have to do is just add this in your maven dependencies, this particular library. And that is it. How do you load the code? Here it is, iris model. Code, here it is, iris model, iris prepare, sklearn, cfit, preparing this, blah blah blah, create a translator. So you have to translate it into the ONNX format of course. You do it, satona blah blah blah, process input. So you do all of these little things. At the end of it you get a model and then you run the model. This code will look very similar to the code that we have done in a little while, but let's go back to our document. So deep learning for Java, in other words, gives you an agnostic platform for running different kinds of model within Java, within a fast runtime. Now it is an initiative. There are some other initiatives also, guys. And this is not the only one. So for example, I could have said, if you stick only with PyTorch, so there is the Torch serve. They give you a built-in sort of a rest endpoint for your models, inference serving models right so that is one initiative uh tensorflow of course has its own runtime and so on and so forth so there are multiple efforts um that you could use this is one that we use because for us it seems to work quite well so once you have done these dependencies in your maven project then what do you do you let's take an example code and so this code guy is by the way in some sense i would say if you absolutely don't want to learn java then these instructions are not for you huh you can just continue to use your flask or fast API within Python and survive with that. Or you can, for even simpler, just use Streamlit and survive with that as you have seen. But if you do have familiarity with Java and you are familiar with enterprise grade deployments, large-scale deployments, then this code will read very straightforward to you. This is the use of the builder pattern. Do you see that this is the builder criteria, builder criteria, builder and so forth. So very straightforward way of doing it. In this example, because for simplicity, this is a REST service guys, the path slash detect, except that you don't give it an image to keep it simple they have hardwired the url of an image in this code so the way to understand this code is you go and open the image image factory get instance from image you create the image runtime object and then what do you do your criteria is that image classifications do you see how it reads my criteria for a builder is i'm building an image classifications thing criteria if tensorflow which in your case is not so you'll go straight to the else the concept is a little more complicated this is uh pytorch is more straightforward criteria builder set type image and classification is the task the set type image and classification is the task right you can give it a id artifact id artifact id says which particular a pre-built model or stored model to use if you give it one of the standard names like resnet what are you doing here? You're using transfer learning. Do you remember that it's transfer learning? And the first time you encounter ResNet, of course, it may not be on your local machines. It will go downloaded. So this is an optional progress bar. And then you do a build. At this moment, you have basically what they call a criteria, but to the first approximation, you can say that is a description of the model you will use to do this task. And after that, all you have to do, if you look at this line of code, this is a specification of what you want to do. So one way to read it is to say, use ResNet to do image classification. That's all it says, image classification using ResNet. Right? And if you're familiar with Java syntax, you'll actually consider the syntax to be very elegant. It's called the builder pattern in Java. Asit, quick question. Like, how do you get the ImageNet? It's not allowing to download for your general thing right so it needs organization you mean no i didn't get that image net this is rest net it's one of those pre-trained models not image net rest net okay now if you need image net data set uh like you can download right oh yes you don't know dodge dot a dodge vision dot data sets okay it comes built in don't try to do it because there's some licensing issues you'll have to apply for it and so forth yes yeah that's what i was saying i cannot go directly to image net and uh download it it needs organization name but why about actually in our python code in the labs you'll see that we have used that torch torch vision or data sets and so forth let's go through okay so it's available through a torch vision data set yeah yeah it's already torch vision library is there so what are the data sets you get free freely without thinking? All of these. You see, ImageNet is one of them. It's a pretty extensive list, pretty good bridge. And for all learning purposes, these are great. Celebrities, CIFAR you know, CIFAR 10 for 10 c for 100 and so forth those are subsets of image so that's that so we do that and then so what do you do here so line 74 to 70 uh 7 does what guys it gets you a predictor isn't it now when you go to the models you and load model what are you doing you're going to the zoo and picking up a pre-trained model right if the next example that i'll give you once you guys are successful here i will post it maybe next week, will be of how to, like for example the models that we have built, I will give you the Java code for loading that and running it. It's just two, three lines different, that's all. It's pretty much the same code. And so now you have, you can use the model to predict, predict and so forth. So the thing is that you don't do all this in reality in every get call. What you do is you do have your predictor saved. This entire thing you don't do. This is just an example code in real life and you save it and then at runtime all you do is predict or predict given an image that's all so it will contain these three lines and the last line that's all and you return the results that is it and this is how the results get back to you right so ask it for where is the you convert python model to here right oh so when you export the python model the python save model it is just a see it is basically a hash map yeah and they recognize the pytorch format okay now i mean it's there in this example what you are showing or it's a different? Oh yes, yes. So for example, when you load ResNet, right? The model ResNet, it is all the model itself has been is what it is a bunch of weights. Okay. It will just load it and it knows it's ResNet. It knows the ResNet architecture. So it will immediately instantiate a neural network that is Resnet and apply those weights okay that's it so you don't have to worry about it it will instantiate it under the cover c all you need are the weights the model after that you can always create recreate the same architecture in your code and apply those weights pre-trained for our own model how do you do that same thing so let me release the code for that so i wanted to keep it simple because it's two three more lines that's all i wanted to keep it as simple as possible because i can clearly see that a lot of people are quiet because they are not very comfortable with java i think i've lost you on this description as if um like i'm not able to distinguish between what the sample code or what is like the the template code and what is the specific additions to the template no this is just a directly a template code it just shows you end to end flow in reality code is simpler than that for example this criteria and all of that you don't do it all the time at every request you do it once and then the only line that matters is predicted or predict think of your python code what do you do you build a model right classifier clf Which is like a logistic regression dot fit or your neural networks dot fit to the data. After that, what do you do? You just call the predict method on your, if you're doing scikit-learn, you do a CLF dot predict. Isn't it? Do you remember your scikit-learn? Yeah. That's it. So what do you do at the end of when your model is built you just do predicted or predict but before that what do you do you need to load the model this is just loading the model that's all okay so guys to get this you have to refresh your java otherwise this will actually if you know java this is very elegant obvious simple code if you don't know java then it looks like i don't know it looks very cryptic and that is the one thing with java like to the people who are the inside of the java world java looks extremely simple and elegant. To the people standing outside, it looks like a complicated mess. So you have to brush up on the Java. Anyway, so if you do this, it would be running. I give you guys a complicated set of instructions, but I wanted to give you guys a taste of the real world, you know, thing, which will actually scale if you can get this far and succeed with this it gives you a blueprint for using it in your startup or in your workplace and getting a high performance runtime environment for inferences right you're doing inferences given an image you're telling it's a cat for example look at this prediction it says it's a cat right all of these first three things are all cats are you is anybody who followed this or understood what we did in this lab or was it too hard uh yes so to the extent stop me from going further yeah but see you may be struggling with the lab but you understood the instructions like what it is doing yeah yeah good one piece of advice guys pick up java. You can't. Enterprise world is dominated by C, C++ Java. You expect people to pick up C, C++ is a tall order. It's a steep learning curve. Java is not so steep, but don't limit yourself to just you know javascript and python because soon you'll hit a wall at some point go ahead somebody had a question no you uh so we deployed it to a kubernetes and we got this uh api to detect so it's running on Kubernetes and we get this result. Exactly. Exactly. Right. And now you can, once you deploy this, it will, the whole thing will be running inside Kubernetes. So for example, if you give the deploy command now with this, uh, this command, this command, it will have it running in the cloud so what have you achieved you have an inference engine ai inference engine running in the cloud in a kubernetes cluster right and that inference engine is fairly high performance robust robust. So, Asif, for the models that need to be trained on a sort of a recurrent basis, then do you just automate the process by saving a save point and pushing it through? Yes, what we do, my team is, we literally have a model repository and versioning and so on and so forth. So every time a model gets, some of it you get through the weights and biases and so forth, you know, you do experiments and you keep track of which is which. But at the end of it, if you want to push a model to production, you follow the same pipeline, you version it, it's your next version. A build process will add the next version number to your model, right, and it will add metadata, etc, etc, to the manifest file that goes along with the model. Then we have it, then we'll have it in a specific model registry, right, which is our internal thing. You just keep it there and then what we do is when we push it to Kubernetes, and that is the next example I'll give you. See here, I just took the, from the model zoo, you took ResNet, which is prebuilt, you know, just download a pre-trained model. But if you have trained your own model, how do you load it? It's just two more lines. But so what you do is you'll go to the model registry, the latest model, you bundle it up and push it to your communities as part of deployment so you deploy two things you deploy your code and you deploy the model yeah that's good now even in case of pre pre trained right if you have a layer on top of it then you would have to still deploy your own model yes yes so if you do fine-tuning tuning you still ultimately you'll save it in a file that becomes your model the only difference between this and a hello world normal java application right or i don't know something like a travel application in java or pet store in java the only difference is that in a pet store the database is separate and you just ship the code to a machine and it goes live here the difference is you package the code and the model to pick it up and some people don't even do that what they do is they store the model in a database and then the just like a normal pet store application will go to the database for data in their case they will go to the database to fetch the model as a binary block so in this case uh are you really by database are you referring to what database here anything oracle mysql postgres pikachu you're saying that because um your model is just you know some numbers right matrix that's why you're just a file with which contains some weights and biases that's all okay okay okay now that that's very interesting uh i said because if you have a model that you want to apply on the data in the database then you really don't need to pull the data out in in mid-tier right so that's right so yes sql query right correct you can you can essentially execute it in database itself because your model resides in database no no no you don't want to execute it in the database so you're getting into oracle okay that's a separate discussion I was engaged with that team, the advanced analytics team and database. No, no. Here you just retrieve it back to your Java application and run it. That's what the possibility is there. See, Oracle stack was that if the models are built and trained, you can store it. It becomes sort of like a stored procedure. You can execute it in the process space of the database itself. That is how the Oracle Data Mining Group works. It's a valid strategy, but it is not commonly practiced. The reason it's not commonly practiced is the database servers are not put on machines which have GPU. And now, almost all inferences we like to do it on machines that have gpu maybe we can have a separate discussion ask if i have a few few more questions on that but may not be in the interest of a broader audience here by the way there's a typo i've noticed there's a sentence there. It shouldn't be there. So guys, this is it. I'll take a break and let's convert this session into one-on-one after the break. Let's get you guys through your problems. Okay. How long is the break? Break, let's make it, say 30. Let's make it for 20 minutes. Okay. How long is the break? Break, let's make it, say 30, let's make it for 20 minutes. Okay. So let's do that. One more thing is, I've taken you on a long journey. Typically, people always use a simpler route. They'll just take your Python to co-production or your Node.js or something. Data science communities are very tempted to just put their module into Flask or into FastAPI. My team, one of my teams does that. Get away with it so long as your traffic is low. When the trouble happens, when you scale out and you start paying a large bill on the cloud. So what I have showed you is a path that is more sort of how should i say harder but more sensitive in the long run do you have any further tips on this container email not being allowed to be added there on that because the yeah one on one let's fix everybody's environment see some some of you may have messed us up in some of, we need to give some permission. So let's do one by one help. OK. All right, guys. I'll stop the recording, because after this, we'll do one on one. Thank you.