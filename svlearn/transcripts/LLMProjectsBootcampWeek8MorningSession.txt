 One second, please. We are waiting for the last folks to join. All right. Good morning, folks, and good afternoon for those of you who are on the East Coast. I know some of you are there. I don't believe today we have any participants joining from India. And Thomas, how are you joining from Malaysia today? So today is the week eight or the week nine who remembers? Eight yes it's the week eight and we are going to do now people have requested that we oh my goodness yes indeed one second I'll turn on the one second One second. I'll start the screen sharing. Okay, what just happened. Yes. I was requested last time by many of you that the projects are coming. These are big projects, complicated projects, and we are all behind. Many of you are on project one, many are on project two, on three and four. And here we are. We have quite a few projects that have been released, if you recall, we started with creating a vector database. Then indexing doing a search engine then doing multimodal search then doing RAC then doing active RAC and then doing image generation and about writing an essay based on the search results an essay that an eighth grader could understand now some of you asked me for tips on last week i'm going to give you those steps. But today is a very interesting topic. While we have been throughout this course, I hope suitably impressed as well as frustrated with the powers of large language model. So these large language models, they do seem to deliver on the promise. After a lot of careful preparation, they don't just sort of work out of the box or their performance isn't that good. But for the particular problem that you want to solve, if you try hard enough, if you do good preparation of data, so on and so forth, fine tuning, prompt engineering, etc, etc, instructions, you do get to make them work. That's a state of the art in 2023. It isn't yet a level at which you can say, this is what I want and just do it right the first time or something like that. I don't know if that will be feasible someday, most likely will be, but today we are not there. So part of the lesson or the learning in these bootcamps was to realize the struggle part of AI, that it's a lot of experimentation, it's a lot of hacking, it's a lot of of there are many, many ways of doing a problem. So, for example, I introduce you to ImageBind and many of you used ImageBind. How many of you were happy with the results? Reasonably happy with the results of ImageBind? Of ImageBind. And did anybody try an alternate approach to ImageMind? What did you try? So we've used specialized models for different things like text, video, image. And how would you then search for it? So, for example, in the image, we use the embeddings and stuff. So if you technically do a search, what we do is we map it into a regular thing like lava providing a description of the image. Great. You use the image description and you index that. So basically all the video, image, text, what we do is they all have something similar. Like here's a text explaining what you what this is about even the videos even the timestamps and everything right so we transcribe that so we always have a formalized text text so you actually you generate using lava and using video llama you are generating the textual descriptions. Yeah. So yeah, this was the second approach I wanted. This is great. And what did you find? I'm impressed with what I'm getting. Yes. In fact, what you will find is most machine learning projects are done in stages. If you try image bind, sometimes it will work quite well. But quite often you get much better if you do just like we did for taxi for text what did we do if if you just take the whole text and just vector encode it you get pretty poor performance in search you're much better off to chunk it but chunking shouldn't be done at random places so you need to do a semantic chunking to do semantic chunking what did you do do a semantic chunking. To do semantic chunking, what did you do? You used an AI model to do semantic chunking. You use quite a bit of mathematics to get it right. And many of you, different teams have tried different approaches. But at the end of it, you used an AI model just to get your chunking right, even before you would put it into a vector database, use a sentence encoder after that. Right? So in the same way, when you come to images, you have to think a lot about it. What you should do is you should ask yourself that given this image, right? What can I do? You could, one way is that you feed these images into a direct multimodal encoder like ImageBond. Whenever you do that, so keep it there. The other alternative is you could use Lava, you could use VideoLama, you could use Whisper for text, for audio and so forth. And you could convert each of these systematically into text and then ask, can I use the text to do that so when you convert that realize that you need to do quite a bit of instruction and prompting you need to write good prompts now these things come with prompts but you can do sometimes better you can do more of it then when you index it together with other other chunk size text for one thing your text chunks and your image descriptions would look pretty much textually of the same size or video descriptions would be of the same size right and when you do that you get another search results given that you get these search results what do you do now right two ways to Given that you get these search results, what do you do now? Right? Two ways to do that. You can hybridize the two results together, isn't it? Just as in search, we hybridize the results coming from keyword search, traditional search, and AI search, and pass it through a cross encoder to see what is more relevant. Now, it turns out that there is no multimodal cross encoder, but what we can do is take, yeah, take a mixture of both the results coming from two different approaches or multiple different approaches. One lesson, if you remember that we learned in the ML 200, the comprehensive internal data science was, ensembles are a good idea. Do we remember that, guys? Try to use ensembles. And so what I'm suggesting to you is, like whenever you have multiple approaches to a problem, and your hardware and your budget can sustain it, unless one model is significantly better than the rest, in which case just use that one model, or one model is all you can afford on your hardware, or one model gives you accuracy that is good enough for your business problem. In all of those cases, the problem is solved. You don't need to use an ensemble. But on the other hand if that is not the case then and you you you're unhappy with the results you can always boost the results by using a committee algorithm an ensemble approach right always use or use a mixture of these sort of experts to get your job done. But this image mind, that's what's supposed to do. Combining all the text and image and give the hybrids. Sort of, sort of. But it doesn't do as, if you look at the text that comes out of the images, clip based, clip like approaching, whereas Vlama gives you, see, if you look at Vlama, how does it work? It is itself a multi-stage approach. First, it does image segmentation. That segmentation detects things in the image. From that, you feed it with instructions to a large language model to get a fairly good textual description of what's happening in the image, right? Whereas just saying picture of a sunset, it will be much more precise. There is a setting sun, there's waves, there's birds in the sky, there's a kid playing, right? And there's a runner running and so on and so forth. So it would be because each of the objects is identified, you realize that you would you would get a lot more out of it. And actually, that's the reason I covered that paper in the last course. The deep the neural architectures because I was hoping that you guys would use it here right and the same thing applies to video llama there what you're doing you're benefiting for the full power of audio video and all of that going through the q format and so forth and in a way you can argue that image mind is also doing the same thing right but you will realize that just do it and see what happens just do that and see what happens Just do it and see what happens. Just do that and see what happens. You can do that same text description that we get from LAMA or people in LAMA. Exactly. And so what you can do, and so one easy way to hybridize it is this. When, see here's the thing, an image, create a vector embedding, or feed it into ImageBind. From the image, take a circuitous route through Vlamma, generate a detailed text, and use that also into the embedding, ImageBind. And now what happens? When the search result comes, it may pick either the image or this, let us say it picks this, but both of them are pointing to the same record. Isn't it? And so in your search results, you'll just have one entry, namely that image. So, as it happens, both in 760 increments, you just add the text embed whenever you average it you sort of. lose some resolution. Just literally have them as two vectors either of the two can qualify as a search result, but they both point to the same object. But relative to the other the average of its if its name were in search results the text either the text caption versus the image embedding there might be something that yes you could try that actually because you're doing the image like you're passing it through image bind you may consider before you store it or index it, average the two. I don't know whether it will give you a good rate. See, this is it. It's all empirical, you have to experiment and see. But try that maybe averaging is a good idea. Can I share a find? Yeah. So there's a very, so there's obviously, I mean, all these days, there's a bunch of people writing on LinkedIn and all that kind of stuff, blog posts. We came across a blog post and it made a lot of sense. Basically it says, one is the fact that you've taken the content itself and you have indexed that, right? But even for like text or whatever, right? When you look at a PDF or something, you know that the audience is going to come and ask questions about this thing. So in the ingestion pipeline, if you run it through another LLM, give it a piece of text and say, what questions does this text answer? And let's say it returns 10 questions to you. You can actually take those 10 questions and embed and put a vector embedding and it's very possible that the user when they ask a question it'll hit the question itself but it points to the same record so the the fundamental concept was that content itself is not just the index you can run other things around on the content on how you perceive the content is going to get viewed by using llm or whatever like describe this picture that's a feedback loop rlm that's what you ask yeah that you can uh add these questions as a feedback to it right in addition to this let's say let's say you have a speech versus a song right the way the elements will ask the questions, one could be like what is the content of the speech, therefore you'll need some embedding from a different time, like from whisper, essentially. And then you can have one from the content that was said versus the one that's happening in the background. We can just use one LLM to generate all of that. Exactly. So that's exactly the point answering ImageMind, that yes, ImageMind will do a good job, but that's only on the content. But then if you enrich it by saying, what else can I say about this content and insert that into image mind or any other election feature extraction and then yes exactly feature brings back to the old message we have been doing all over the months feature engineering can you like Can you like that? Yeah. Sure. Feature engine. See, guys, so anyway, for those of you who are remote, Satyam pointed to a blog, it said that amongst other things, when you get a text, for example, you can index the text, but what you can also index or vectorize is give the text to a LLM and say, what questions can I ask about this text? What questions does this text answer? Right. And so LLM will produce some questions. You can do that. You can try this experiment out with chat GPT or something. It will produce a list of questions and then you can index those questions also those questions also pointing to the same obviously content record in the search results eventually. So it makes for sometimes you'll get a much more direct hit in that. And in this context, since you guys brought up this discussion, or one other way, and that is a segue into graphs and semantic web, you can actually use LLMs to do extraction of semantic web content like triplets, the ontology content. And what used to happen is ontology extraction was an extremely manual process. You would take a bunch of domain experts, lock them in a room. The vision, mental picture is keep them hungry and thirsty and starving the whole day and say, unless you come out with a thousand triplets, you're not going anywhere right today. So it was a very, very painful process for any domain to become a knowledge graph and which is why good mature knowledge graphs are hard to find and they're relatively incomplete they're more abandoned knowledge graphs than they are actually by orders of magnitude than their actual completed knowledge graphs but one of the things that actually here at support vectors we are experimenting is, can we use LLMs to extract those triplets? And we had pretty good success with it actually. And once you do that, then you can bring the power of graph theory, complex network analysis and graph theory to rationalize those triplets and see which ones are much more important by looking at the centrality measures in the graph and many, many things you can do. And then, of course, subsequently follow it with a graph neural network embedding. And then even the graph becomes those ontological elements become embeddings at the end of it. So. That's a long journey and we go there anyway. So this is we just discussed what are the lessons we learned from the previous thing. So the big lesson or the one lesson throughout machine learning, if you remember, I've been telling you that you can squeeze a lot out long, long ago when you did the first course on machine learning. We did linear regression. And then literally your first lab was there was one data set that was a straight line, linear. What was the next data set? It looked like sinusoidal. And a straight line equation couldn't solve it. A linear model couldn't solve it. What did we do? We did some feature engineering we just expanded the basis to polynomials and the moment we expanded it to polynomials a linear regression equation a linear regression model began to work you just had to expand the basis set and we have been seeing this over and over again. The river data set, if you remember, was our, I hope, memorable event where you learn the power of solving complex problems with simple models. Now in this LLM world, obviously there is no such thing as simple. We are looking at very, very complicated models, huge models, and actually the next year's models will make this year's models look tiny. But there is no denying that if you want a good product, think of all sorts of ways to improve. And so I was hoping that you guys all would have some level of dissatisfaction with the results of it what you have done, because you would see that it leaves a huge scope for improvement in in the models that you use. Anyway, so today we will move on to my usual thing guys remember we are shippers. We are here to help you. Right? You are by now. You must have realized that doing this boot camp is a little bit like climbing Mount Everest. Right? So we are your cleansing shipper carrying your luggage to the top. Okay, So this is who we are. Just as a quick reminder guys that we not only give training support practices also into consulting and guiding corporate things. We also provide corporate training wherever you work. If your team members are talking about, oh, which is a good place to get training, you can tell them that training can come to you. We can come to your places and give you training. We can also give seminars, lunch and learn sessions, whatever you want. And we'll be happy to do a few sessions free. This course is about, I keep reminding you over and over again, this course is about the gap. In all the previous courses, you learned the theory of machine learning. You did some labs, some simple toy examples, and proof of concepts. This entire workshop, a boot camp, is about the vast journey you have to take when you go from a proof of concept to an enterprise-scale, robust, and high-performing architecture. And you realize that that is a different it's an entirely different kettle of fish so it takes a lot of things this is again a very very quick reminder many things go into a proper ai centric enterprise there is the people the data infrastructure infrastructure, the models, the ML platform, ML Ops security and compliance analytics and visualization ethics, many, many aspects go into an AI system. And its processes, the whole ML Ops stack is pretty complicated. You have the code can you go back to the previous lecture one second um one element which i don't see here is you have security and compliance privacy and security are becoming paramount that is right so usually yeah you're absolutely right i put the privacy under the compliance because of all the unitary and so on and so forth absolutely it's becoming very paramount uh and then we are what's the cycle of work so people are trapped based on their skill set in one of these three cycles usually they are either the data science cycle building models science cycle, building models, they are in the data engineering cycle, generally involving a lot of work, and then you're in the MLOps cycle. And quite often when you hear of a great breakthrough or somebody coming out with a remarkable platform that can do magic, you often think that there must be a couple of really good algorithms sitting there. What people forget is algorithm was one part of it. To get to a remarkable usable practical product, it takes an army of engineers to make it happen with very different skill sets, with infrastructure, hardware, networking, MLOps, like DevOps, data engineering, systems engineering, microservices, UI, all of those things go, not just algorithms to support such a pipeline. So again, it's a pretty complex world in which you ingest data, clean, validate, transform, label. Then you do the actual training, which you'll notice is one part of it. So there is a huge part, the data stage that comes before the training stage. Then you do the training stage, experiment, train, tune, and then you have the deployment stage. You need to properly manage it. Now, today, I would like to emphasize the use of a feature store, guys. As you are creating all of this data, get into the mindset of a feature store. When you do the mindset of a feature store, you will realize that you will be more aware of feature engineering. From an image, for example, how many features can be extracted? You can obviously convert the entire image into a vector. What else can you do? You can take, you can pass it through VLAMA, and then extract a textual description, a proper good textual description, or you can pass it through Blipto, whatever it is, and get good descriptions out of it. Those are features. What else can you do about that image? Obviously, they can be metadata somewhere in the .xf files. One obvious feature is when was the picture taken itself, if you have that. But then more than that, if you are familiar with computer vision, you can do feature point extractions. How many of you know what feature point extraction is? Only one. It's a praveen. So see, there was a world of computer vision way before dpn networks came about and people were doing a lot of feature extraction from images so those things are still believe it or not a lot of those methods are still the same old methods that and it's a pity that most people don't know about them right so learn about feature point extractions. It's very important to know these things that you can extract those. You can do then image segmentation. And one of the features you can extract is literally the labels. What are the things in this particular image? isn't it? That itself is one hot encoding in some sense. You can ask dogs, are the dogs there in the image or not? So you can do a lot of feature engineering. It's worth doing. Monitoring and training, of course, you should keep doing that. And one good thing with Ray platform that we are standardized on for this bootcamp is it gives you pretty good monitoring capability. I would say for for a young project new project is pretty good. Yes. Question. Which engine and almost always is going to be associated with. Data sets. No, no, no, no. See features go into in a way, see when you're doing clustering and so on and so forth. If you extract a whole lot of meaningful features, you could be doing clustering in a much larger space of those features. So if you look at the traditional models versus the LLMs, then for us, like you would skip the training stage? No, training will become fine tuning. And in fact, you would fine tuning and featuring like a prompt engine, instruction set. So that will become a large part of what you're doing. OK, guys. Yeah, go ahead. That is just sort of a problem. Right. Right. Yeah, it comes under the family of a pump. Yes. And that gets better results. You can use the results of the competition for that much better oh yes so when people use the words even these alliances form and they use words like training and this they generally include all of those techniques that you use along in your journey creating your architecture and so forth and And yes, guys, there's things like, let me give you guys a hint. Your last week's lab was actually a 20-minute lab. You have a whole week. And I'm giving this week for you to catch up. But believe it or not, the entire thing that I gave you, the coding part of it is barely happening for 20 minutes. But the reason I gave that is because I wanted it to be the lab where you'll realize the importance of, can somebody guess what? Prompt engineering? Exactly. Prompt engineering and instruction tuning. tuning exactly that's what it is i give very specific example you need to write something an explanation or a description or an essay for an eighth grader implicit to that is what are some of the things implicit to that the things implicit to that. You have to use a child's language. Isn't it? It should not contain, and now I don't know how many of you thought about it, it should not only be easy to understand, how would you know that it matches an eighth grader's language? What's a metric? Vocabulary is one, but believe it or not, there are actually algorithms that measure the readability of a text and they quantify it with what age group can understand that text. I wanted you guys to explore that. It goes back to old methods like the Fleischer score and so on and so forth, which have been used for decades to much more modern methods which use transformers to evaluate whether how readable or comprehensible a text is so think about it this way guys when you how many of you use things like grammarly right most of you do does it tell you at some point how readable the tech comprehensible or easy to read the text is? It's one of the measures you get, isn't it? How do you think that's happening? So there are means to do that. And so implicit in my requirement is that your essay should be understandable by an eighth grader. If you really do it well that is what should affect you right how do i know that it is understandable by an eighth grader objectively and there is a whole world that looks into that of transformers and so on and so forth so if if you take that angle, of course you'll be doing much better. Now, the second part of it is just image gen, which I'm sure that after the diffusion models, you found it quite easy on your machines today, so long as you have a 4090. And now I believe Sukpal has been having a rolling business of making machines for everyone. Is there any team that still doesn't have one of Sukhpal's great machines? Every team has that, right? So there you go. But with the JAN 1490, you need like things. Like I am not able to run so many things. You need to have like 96. Like I have 32 GB RAM. And all the time I am running into OM. Oh my goodness. If you have 32 GB RAM, you are in the begging category. Yeah, you know I ordered like one yesterday because i'm not able to run in parallel like anything see guys if you're doing machine learning start with a machine that has at least 200 gigs of ram i don't think people have that i have 96 96 96 yeah it's a good starter but see these models but on 96 i can't run ingestion and influence together yeah you should have see guys here's the thing ram is cheap these days when you're building a new machine start if you can start out with 256 or better still go with 512 or so forth. RAM. RAM. I ordered yesterday. Some more. Okay. But generally, a stable diffusion should be able to produce an image in under about 30 seconds. Just enough to be a banner to the picture, low resolution image. Are you guys getting that? I'm in three seconds to seven seconds but the problem is it's the longest pole in everything that we do yeah and i hope you're putting it behind a messaging queue these jobs image generation jobs yeah yeah doesn't get your response quicker it doesn't yeah it doesn't it's right obviously you could do better with a it doesn't yeah it doesn't it's right obviously you could do better with a h100 no diffuser diffusers don't do particularly better for the project so far you are fine one 40 90s you know yeah but like you can't run that injection and inference you can't do together like if I run there's just no way I can run text video image everything together while running inference that's never gonna happen yeah so guys we're coming to today oh by the way now you guys will start using that big inference server we we have completely cleaned it you will get one we we have completely cleaned it you will get one one docker image you have to set up your stuff in that one docker image you will not get access to the root machine because yesterday uh ravine myself hamad we are looking into the machine and we found just about everybody's fingerprint on it there were multiple operating systems running so you reinstalled everything yeah we wiped everything and now there is a rocky linux and podman which is dockers and you guys will just get a docker to run your load on now thanks to sukpal the whole machine set itself on fire quite literally but pretty much i think podman and other podman docker all of these guys will put other artificial limits on it in the execution it doesn't allow it to like no no when i created those docker things i gave blanket blanket thing no memory limit no cpu limit nothing but i thought that depends on how you run the containers. That is right. So from the cockpit, when you fire up the container, it basically comes out with no limit. No limits. I'm sure. Check the dock are running, unable to run the GPU. Yes. Yeah. Because you cannot by default run uh docker see gpu support is there in the host machine right and therefore the dockers because they're so on fedora we are putting just a fedora docker it inherits everything it's a very thin very small it's not even one gig run you need to say a GP oh yeah you have to enable the device support yeah so all right we'll we'll all learn it together maybe I'm missing some nuances I'll learn it So there we go. So guys, today is many of you requested catch up time guys. So this week I was going to release a very interesting project. But I'll wait it out till next week but I'll give you an idea of what that project will be. And the projects that are going to come will be. November will be a fun month. Now you guys are, I think, into the flow of things. But today I've deliberately kept it no project, just finish your existing projects. But we'll do a few paper readings. See, these large language models, impressive as they are, they also have weaknesses. They can be very easily attacked and poisoned. There are countless stories in which, you know, people have released chatbots which were soon made into completely hate spewing racist entities. People have learned the hard way that these things can be manipulated. But that can only happen if you use reinforcement learning or you're trying to do something with it. That is right. so some of the early attempts of relation app to indiscriminately take the user imported adapted but you must be storing the data somewhere in your pipeline like what like you wouldn't just change the model on yourself no the model keeps going through some of the older models right they were going through continuous learning I see so like as you ask prompt questions it'll try to yeah that's right is that how some of the new ones like Mistril and stuff work? No, they didn't do that. Mistril is a standard open source model. Yeah because you load a new model right I mean the minute I kill the instance and I bring up a new one basically it's going to revert back to you that is right that is right unless you save the weights but the thing is you can choose to continually update the weight see what happens is on what imagine that you have one production instance input is coming in some engagement and some user says oh no this was the wrong answer this is the right answer and you capture it and in parallel you're running another model which on which you are doing reinforcement learning what will this model learn after some time right so you can't really trust the user they're they're having fun with guiding you okay so things like that so today is about adversarial attacks. We'll read two papers on adversarial attacks, and I will make, oh, you like the topic. Okay. Yeah. So how can you do adversarial attack? Mathematically, actually, it's beautiful. The reasoning behind it is quite beautiful. first do one of the early papers of 2014. This whole adversarial attacks were independently discovered by two different research groups in 2013, multiple research groups in 2013. By 2014, the giants like Ian Goodfellow started thinking about it and coming up with systematic approaches to attacking neural networks. And we will, in in fact I won't utter those words because they are literally part of your paper reading silent paper reading today please do take the paper reading very seriously it's an important topic guys you cannot you're playing with fire you'll burn yourself in your projects if you don't know what risks there are and how it can be attacked and what differences you can have and so forth. It's a topic that you should go through once. So I'll do the paper reading. I'll make the implementation of this as an optional project. If you have time, do it. If you have interest, do it. Otherwise, you will not do it. In the implementation, all I'm saying is reproduce, play with the... There are very good libraries like Clever Hans and so forth. Use that to craft attacks and defenses. What's the library again? We'll come to it in the paper after the paper reading we'll do it. It's a so in the library one library one good library is Clever Hans. Clever Hans. So I'll tell you a story of where the word Clever Hans comes from. Anybody knows the story of Clever Hans? No, I haven't told you the story. Okay, so there was once So there was once a horse and the owner of the horse was convinced that this horse understands human language and can do it. So you would ask him, clever Hans, what is five plus seven? And the horse would go, duck, duck, duck, duck, duck, twelve times. And people would be absolutely amazed and there was no trickery like it was all very honest and he would do it on stage and it became quite a sensation this clever hands this horse that could do arithmetic and could basically give yes no answers to things and people are puzzled and they said how is it able to do it finally the resolution came when a psychologist a behavioral psychologist did he also tried for a long time and the horse always came through and then finally the he resolved how it is he created a completely dark room in which the horse couldn't see. And then he would ask, what is five plus seven? So let's say two plus three. And the horse would go and it would just go on. So that's where you get the clue that what the horse was doing is it was looking into people's faces. And it picked up the side behavioral or psychological cues that told it when to stop. And in many ways, you can think of this, I don't know. So that's a clever Hans thing. Now if you think of these deep neural networks with all their reinforcement learning, they all seem to be clever, right? They seem to think, but we know that they don't think. Right? So in many ways, they are the clever Hans. So today we'll do paper reading on these adversarial attacks and what is the beautiful way of interpreting why it happens, how it happens, how to avoid it and so on and so forth. Guys, I am really hoping against hope that today you guys brought some new finding to share with the community. Cambrian Explosion. If you haven't, there are still five hours. Please go find something of value that you can share and enrich the learning community with. So remember guys, we are a learning community. Bring that. Again, there is a team presentations of the work. Which team would like to present? Last time, Satya, you said next week. We can present something. Some things, okay. Which other team? How about you guys? We have a lot of pieces but we have to bring it together. Bring it together. Praveen, you have presented. No, we have a lot but i can't i have to get ready on a public server is i mean i don't have a machine that's nice okay i got my machine yeah how much memory did you put in that 96. Yeah. How much memory did you put in that? 96. I'm running into the same issue. I'm not sure. I mean, yeah, I'm running into inference and injection. Double it anyway. It's not just suffering. Double it. It's not hitting your head against the wall. And which other team? How about you, Albert, your team? I think they're not ready. Not ready. and which which other team how about you albert your team they're not ready not ready all right and uh prakash how about your team yeah they're not preliminary yet okay all right so there will be one from sakim's team all right guys so that's the plan and and what's the journey we have done so far? Are you noticing there are lots of milestones we have crossed so far? We learned text extraction and we realized that even text extraction is an AI process. It's not that easy to convert from multiple formats and take it out and make it into a proper chunk. We learned about vector databases, ANN indexes. Can somebody give me an example of an ANN indexer? Oh, FAS is, yeah, indexer. Internally, what are the pieces that it could use. IVF, HNSW, flat, and so forth. Excellent. Hybrid AI search. What is this hybrid AI search? Anyone? Elasticsearch plus. Yes. I'm putting it through a cross encoder. Semantic plus. So you pick up the strength of both. And what is multimodal search example of that giving a giving an image for example and getting lots of descriptions and other images and videos or giving a text and giving images like we did that and retrieval augmented generation rad we know now it is putting it so the more technical way of saying it is that you benefit from or you augment the knowledge base of a LLM. See, LLM, when it produces an answer, as a next word predictor, it's predicting the most statistically most likely word. likely would. Even if you sample over it, it produces a text. In a way, you can say that the text that it produces in some effective way is being generated from the weights and biases, the parameters that are stored in the model. Right? Somewhere in there, in ways that we don't fully understand, the knowledge gets encoded. And it is using that that so that is parametric knowledge but then you can you have access to a vast amount of non-parametric knowledge when you take an llm it is dated you trade for example you may have trained the llm in 2021 but a lot has happened since 2021 wars have happened right and new wars are mushrooming all over the landscape these days, unfortunately, right? New discoveries are happening, good things are happening, right, good and bad. So if you search for it, search for answers, you would realize that you don't get the best answers from my LLM, if its knowledge is dated. But we just did RAG. I mean, just we did hybrid search. And the question is hybrid search, or search engine was one way of getting answers. Asking an LLM is another way of getting answers. Why not combine the two and smarten up the LLM? And that idea is RAG. It to further, and it also is a cure cure not just of stillness of data problem, but also of hallucination. You can further reduce the hallucination by doing active RAG player. Then we went into things like memory efficient fine tuning. Qlora and so forth, quantization, etc. Then we talked about diffusion models. Beautiful theory. But by the way, guys, you realize that behind, have you noticed that behind all of these models, while the models are fun and we can go get them from hugging places and we can fine tune it with data and do a lot of coding there's also beautiful mathematics there's beautiful ideas there diffusion again and as you will see today too there's good like diffusion models are based on a key idea from non-equilibrium thermodynamics literally the diffusion process and its thing and how would you reverse it and so forth and and they build on each other you realize that diffusion models wouldn't be possible if we didn't have the world of encoders and decoder like thinking the unit didn't exist or if we didn't have clip or guided a text transformers right or something that would create a vector out of text and we could look at the alignment between this image and that vector and so forth so everything that you're doing builds upon it the next generation algorithm is built upon it and today when you look at stable if you i mean any diffusion model i mean now we are haggling over this guy seems to have six fingers, that sort of minor details. But all of this would have looked miraculous in pre-pandemic days, isn't it? How many of you thought that this was possible pre-pandemic? Very, very tough, right? Transformers were just taking off. They were doing good things, amazing things. They're beginning to do amazing things in 2019. But we are in a completely different world now. There's a Cambrian explosion of things that you can do with Transformers. I am not able to catch up. I sometimes feel I learn more from the news that praveen and patrick bring to me and others bring to me i often miss crucial papers and then i have to run and catch up on them so this is this is the face at which things are moving so to go back to things are moving so to go back to your time so request if you can uh the paper reading ourselves index or roadmap all the papers yes i'll do that all the papers will be on the course portal not only of this but from the deep learning course also so i've over the years over the months i've forgotten all the papers i've taken you through. I sort of visually, mentally remember what are all the key milestone papers I must cover. But then I forget which ones I did and didn't cover. If one of you can just give me a reminder or a list. Atira, would you like to take that task? Because you must have noted down all the papers that I've covered in the last six months. Please give me that list and I will then add it to the course one more it's not just a list of papers no no why is it it's the description like click click like how one or the other like the image text audio video you could have it categorized how needs paper will fit into like what they do some kind of tutorial i'll do that i'll do that i'll put little descriptions of things like that onto the cards definitely it's a good value and guys are you finding the course portal useful no no something else i haven't used it you haven't used it okay use it you'll find many things by the way jenkins instructions gender gave uploaded to it so some of you are struggling with jenkins from the rocket linux so you can now follow the instruction but i'll also upload the instruction some of you want to know how to load wikipedia into your project so i'll release the code today for the wikipedia part i'm uh doing a github action so once i complete i can answer oh yes please do so cicd both now it builds and deploys to my server so but it's still in the working really oh i would love to learn that i mean if you can create a good tutorial for that let's share it yeah get a batch so that's how it should be he wants you every time you are told it is checking it does the continuous and whenever we release it github action if you use um the default server it takes a long time so you probably want to use your own servers or something like that otherwise it takes endless you probably want to use your own servers or something like that otherwise it takes endless oh interesting for example anything that's free unless you buy a yeah they all have the ketchup models. Everything is usable is available for free. Okay guys, so today's topic is very interesting. What do you see on the left? A cute little panda, right? And what do you see on the right? A cute little panda. But what the machine sees, what your neural network sees is, one second. Sorry, guys. So I'm almost done. What you see on the right hand side is also a panda. But what the machine sees, have you noticed something interesting? What is the machine saying at the bottom? It's absolutely sure it's a given. What's a given though? Let's find out what's a given. I don't know. I don't know. What does a given look like? Let's let's see. I have known about this paper and curiously, I've never found never looked at a given. Oh, yes. Images are forgiven. Surely you would agree that this thing is or this thing, let's take this one. This is not looking like a panda at all, isn't it guys? Right? But this is not looking like a panda at all. If I just put it on the left and the right hand side. Do you notice that a panda and a gibbon, they look quite different. And yet, the AI with great confidence says, I'm looking at a given. And what did you do? You took the image and you put in some hidden noise that's not even perceptible. It is just below the perception threshold, the human eye perception threshold. So you can't see it, but the machine can, and it has completely altered its inference. And so your image classifier is misguided now. That's an example of an adversarial attack on your model. Adversarial attacks, it turns out, are common. Now, it's a very active field of research why they happen we don't fully understand they are different hypotheses the hypothesis that i find most plausible and i like the most are two hypotheses one of them is called the dimpled manifold hypothesis and i've given a paper here the other one i didn't give it to you read because you know these are both relatively dense papers you'll have to pay some attention to read it so two paper readings are enough the third one is a curvature manifold curvature hypothesis it says that mistakes tend to happen when manifolds develop huge curvatures or they become highly not like complicated right in a way they tend to sort of try their best to fit to the data and they go around points to just fit so during the learning process they also are learning mistakes so read about there is a good survey paper, a survey article. Somebody wrote very nice, and then there's a clever hands. But there are many resources if you are interested in all those resources. We can share with you a bibliography of all the resources on this topic. So this is it, guys. With that I'll let you go and enjoy your silent reading till two o'clock these are two dense papers guys uh do it please do study these papers and then of course continue to do your paper uh your projects you all requested that we don't do any new projects so this is it i'll give you an idea of what is coming next we haven't dealt with tabular data right and so we'll do it in stages first we'll give you this problem suppose there are lots of forms that have been scanned create a pipeline that extracts data from those forms meaningful data knows where to do it this will get you into all of the some details of ocr it's a necessary topic in uh like in and the llms do i mean sort of these transformers do a pretty good job of it these days much better than the old methods could right so we will do one project on ocrs the other project that you will do and we have only four weeks left The other project that you will do, and we have only four weeks left, the other project that you'll do is, you will have to create, you have two choices. One thing is, take all my videos on YouTube, they're only 100 odd videos, small data sets, and create a virtual instructor out of it ai instructor so you can chat to it and ask questions and you should be able to answer your questions and be a virtual teaching assistant put a facade on top of chaldea good one put a facade on top of charging yeah and of course don't use the open ai open ai bard or cloud models see guys with those things yes it is easy but trust me at this moment they are very cheap it won't be like that because it's costing them a lot the moment they make sure that you have forgotten how to do all of this yourself that's when the prices will go up all right think of think about it this way guys how many of you remember suppose i were to ask you 5163 times 218976. How many of you can just do it? Did you know that a generation ago, people could? Yeah, and it's very easy. Even if I say don't do it exactly, I'll make it easier. I say, just do it approximately, you should be able to do it approximately. And if you use common sense, it is nothing but approximate means rough approximation. Just look, put it in scientific notation. You're multiplying, you know, five point six with something round off, round off. You can do it in your head and have approximate answer. Even better. If you remember your log and antilog table, just a few key values, certain milestone values, you'll be able to do that easily. You could do all of this mathematics. It used to be the trick before electronic calculators came about. What did electronic calculators do? It made arithmetic idiots out of followers right i was shocked when i was giving a lecture lecture at syracuse university i remember many many years ago and that was 20 30 years ago when i asked what's 77 right divided by 11 and i saw one kid bring up the calculator and start doing it right so shouldn't be this bad, so you lose something guys today. you're using large language models, if I noticed that most people don't trust the text they write they will write the facts given to chat GPT or whatever and say write it. Right, and then they will generate lettersG or whatever, and say, write it. And then they will generate letters and so forth. And even, I mean, practically everybody seems to be doing that. So nowadays when I get an email or a letter, I have to scratch my head and say, this looks pretty sincere, who wrote it? That was not a question we used to ask okay so you lose the ability guys soon what will happen is we'll lose our literacy completely and that is a broader thing but computer science you guys are ai experts soon you won't have any AI expertise left if you just keep relying on these APIs. All that will happen is some of these companies easily, easily will become $100 trillion companies. Their wealth will be more than all the nations combined. And the rest of us will all be a nuisance that they have to tolerate on earth. Anyway, that's a political state. I shouldn't make that. There'll be a lot of William Shakespeare. Lots of William Shakespeare. Go ahead. Somebody is saying something? No. Okay. All okay all right guys i have a question um this is actually relate in relation to using the apis or whatever llms and prompt engineering you know like especially tad gpt and bard or any llms they are heavily used in documentation. So in relation to an earlier discussion, right, if I just give some code, right, a terraform script or some long script, and I ask you to document a document, and I can even say, you know, create 10 FAQs out of this, or we can basically create any if any level of granularity in this faqs right and i um also came across a term called um ai generated prompt engineering is that even real or what does it mean no it is real actually it's a very recent breakthrough it's very you're asking about the ai generated prompting recently in fact the paper i read but its name is escaping me the gist of that paper that came out quite interesting is that you can create prompts for llms by using llms sort of a bootstrap in process because the prompts that the transformers generate are actually more systematic and better than the ad hoc techniques that we have developed. Go ahead. I want to share an experience. Recently we had a headphone last week. One of the things that we did is we were supposed to do a We were supposed to do a research. We have this thing called Yeah. And we wanted to create a chat that you could ask in in natural language, a project, that it could generate the Yeah. What we realized very quickly is So what we did is we created a knowledge base on the search in that and we picked that up and used that to create a chrome for the lm and then put it to the llm and gave it the source of the public capacity you could use the lln to generate a problem with the rack so we got the data out from our that we saved it in a record db we indexed it yeah knowledge search and use that to generate a prompt and then take that from to text so usually uh so uh by the way you should talk to srina he did a project exactly like that the The basic knowledge is this field of text to SQL generation, natural language to SQL is a very active area of research. But our state of the art today is exactly what you discover, that if you give it just natural language and ask it to generate a SQL, it doesn't do... It doesn't do SQL but it transforms. It transforms. It doesn't. So within the same class of course then it doesn't do as good a job as when you back it up it's again right you rag it but usually the what you're seeing is something slightly beyond that what i'm familiar with is we have a way a template a instruction that given this knowledge given this user input not generate a script or a SQL script or a Terraform script, whatever it is, that's the way I did it. But what you're saying is given this knowledge, given this user input, generate a prompt that you will then feed to yourself to generate to us. That is it. And that is actually, it's remarkable that you did it because the paper in which said, I think the paper is auto prompting isn't it? I did not know about this paper. Oh, you just discovered it on your own? Yeah, brainstormed it and came up with it. I read that paper, I forgot the name. Is it auto prompting? DSP. DSP. Demonstrate search predictors. Demonstrate search predictors, yeah, but that's been there for a while. Recently there has been a paper that basically is using LLMs to systematically create good instructions and prompts. Autogen, exactly. That's the paper. Yeah, that's the paper. So you should look into it. And by the way, if enough of you ask for it, two important ideas were there. Patrick mentioned demonstrate. I keep forgetting the whole thing. Demonstrate search predict. Demonstrate search predict, DSP. Yeah, that is a very crucial idea. In fact, React and other things are all interrelated. I haven't covered the React paper. I haven't covered the DSP and i haven't covered the autogen if you guys are interested i will do it because they don't directly impact upon this boot camp but i can do them separately as paper as part of the boot camp you're already doing the prompt augmentation and actually using that right and then that's it you are doing it you're doing it so yeah these are just see the prompt engine has become a pretty involved field in excel and there are many, many techniques by which you can generate. The example she gave makes perfect because they're insert they're generating sequel or. Your like which. is going to be our terraform script could be I don't know anything yeah. Oh, it could be Terraforms. Terraforms, CREB, it could be, I don't know, anything. Yeah. It could be. And the SQL one I follow very actively. It's very much work in progress. I want to know. OK, Anuresh just volunteered to give a session on the latest and greatest news in natural language to SQL. Anuresh, would you like to take that? Sure. Take your time, like in a week or a couple of weeks. Go ahead. I shared aluminum-plopped-in attacks on the channel. Is there a detect for it? Would you like to present that? Oh, definitely. That would also refine my understanding. Absolutely. I mean, I would consider that a classic you know cambrian explosion event uh please we can present those facts would you like to do it today no i'm not ready for today how about next time next next next week that will give me time to also think about it sure whenever you're ready next week two weeks yeah because that will also help me understand understand the topic a little bit yeah i like to do that it is the texture yeah this is great guys finally the cambrian explosion series is taking off so one from anirish one from marsupial energy see great examples follow it who is the next volunteer patrick which topic which topic oh that is awesome in fact uh are you afraid today okay so then that could be a cambrian explosion uh albert so line change does all of this no knowing it at the knowing why it works knowing it at the level of the paper adds value. Like for example, I think you should give an example of dynamic problem. You create one problem, pass it again, you get a different dynamic problem, and you can chain it up. The land chain does that too, doesn't it? No, no, yeah, it is that. So Albert's question is land chain does a lot of these things. See, there are two i have one is fundamental research people are doing a lot of this uh in the in the space of prompt engineering there's a flurry of creative activity land chain in a way is an aggregator of ideas whenever something new comes up projects like land chain they try to absorb it and augment so you notice that lanche is a fast evolving open source project and it tries to absorb it but at the end of it right how many of you by the way initially thought oh why are we doing the search or that it is just four lines in lanchean right the trouble is lanchean doesn't go to production it doesn't scale it's a popular it's very it's very popular to feel, yay, I did it. Isn't it? They're trying hard to commercialize it. Demo purpose. Demo purpose is great. Or the presentation. Yeah, it's great. See, ultimately they're an aggregator of ideas. They bring tremendous value because once you know that you can do it as a proof of concept and you're capable of taking this journey of ai engineering both the pieces are in place whatever new thing has been done you have the engineering infrastructure to take it to prod you prototype on transformers yeah you deploy on on like pine cone and stuff then you look at your bill and then you come back to the other exactly then you start building things on your own right it's the same thing guys why do you why do you plant your own trees in the backyard right or lay you know make your own patio because try getting a plant expert to come plant trees in your backyard that's what happened with the cloud. Yeah, with the cloud the bills are killing people. Now we're back to on-prem and moving workloads. In fact, I would imagine Walmart, anybody from Walmart here with present or past experience from Walmart, somebody was there, right? Did you have any background with Walmart? You do have, right? I heard a statement that Walmart ultimately has a hybrid cloud because they realize that you should have many things on-premise to keep the cost low and some things in the cloud. The biggest example is Netflix. Netflix? Well, they have gone hybrid now. They have always been. They always have. The video content, if you start paying the egress bill on any cloud provider yeah it would be way more than the revenue oh the content has always been in there so what is the Netflix use AWS for the what you see the menus and everything the AWS is used for syncing the time at which people have been looking at things and stuff like that. So what they do is if the regular CDN is not able to access it, then they have a backup CDN to which they send the request. Seamlessly, they do the failover. So their thing with the internet providers is that we'll put 50% of the bill and we'll give you the hardware boxes that you use as the cdn at the periphery so that's this model the aws is all for internal basically just management of the accounts and those kind of okay aws is for amazon prime okay yes because they don't need they don't charge themselves yeah right, guys. So there you go. So see prompt aging is exploding, and there are many, many things coming. Lanchen is a great target. I mean, my first thing is go to his land. I go there, see what's happening. Oh, it is doable. Then I dig deeper, go under the covers and see what makes it work. And then with that, you know, my indexes or my indexes. I should be the right line. There's a lot of yes, they have an open source. Can you check if I have added my index to a postcode. Oh, it's a one year toddler. So the founder, he didn't ask me anything. I've read it. Everyone's complaining about the documentation. I think, are you having the course portal in front of you? Right. So just remember that I need to add again in this Lama index to other resources. Okay, guys, that's it till one o'clock go read the papers. Guys, please don't start coding on your project. Do that afterwards. Read the paper paper these are just two papers you can get to the papers yes right here in the presentation in the pdf it's not working it is working oh it is working the links don't work it does work. Control and then click. And yeah, so the PDF is in the course portal as well as in Slack. So just to give you, so guys I want to give you, take one moment to give you a little bit of the orientation of the course portal in case you haven't been using it i have sort of so guys um you have all of these resources the zoom links etc etc etc then and you have you have more than that oh yeah so you have guys i put a thing called solutions and instructions a section here this one contains just to give you an idea it contains your project spec doc by the way that doc i'll revise i'll add everything that we did afterwards the sample solution up to v4 and today i'll also release the wikipedia loader to that how to load the wikipedia into your database instructions for deploying the sample solution elastic search you have py spark you have mysql maria db instruction we have and this is the instructions for compute pipeline using jenkins okay so is there any and then uh pravin you're contributing one instruction what is that i forgot the cicd cicd yeah one cicd pipeline instruction so nice guys and any resource that you guys can collectively contribute i'll appreciate it anything that you feel is benefit all of us. yeah we'll do that and so that's what we were just taking notes on and then. So everybody's aware of our mind that. It is. yeah yeah. The amount of the link and updates from them is like, once you follow it and Jeremy, you guys. No, I remember the day Lama Index when it first came out, people said that, okay, so what value do you bring? And so quickly they have moved to a stage at which you don't question it anymore. So it's like, landion is trying to be in red and lambda index is focused vertically yeah right no lavendix is great it's totally i totally am and this other thing which is this uh chroma db and there's some overlap between chroma db and lama index right has anybody caught up with chroma db the latest in chroma db and llama index has anybody caught up with chroma db the latest in chroma db but they are also moving pretty fast but there are other degrees i had a list of degrees now everybody has a post-grants yeah all the external vendors have moved into yeah yeah elastic search has its own but elastic says it does a brute force direct order and scan of the whole table to find the nearest neighbors so it will do the vectorization but at this moment the last i checked they didn't have the ann integration like really quadrant everybody has hybrid student is a quadrant is one of the early guys it's where there's an exit and then there quite a few of what is it called nebulous or something will be will be will be is also very good. Actually guys would you like to do a comparative like see one of the things I was hoping did anybody compare your own search engine implementation against these things I was hoping at least you would do it against chroma unrest yeah by the way did you guys know how many of you know one of your projects is your the last project is rather rusty. What do I mean by that? You'll have to do your last project. You will be using Hugging Face and you'll have to do load your model and run your inferences in Rust. Sorry, what? Instead of Python, use Rust? Okay. Yeah, you'll have to use us that's your last project satyam is unhappy he would have said why not go no actually i was about to share that you know people have gone crazy over this so obviously the golan purists are talking there are things which is outperforming python significantly so all that they have done is taking just some of these vector implementations and all of this loading and unloading and all that kind of stuff it apparently loads into the gpu much quicker oh yes yes definitely i mean see guys here's the thing today you can come you can do all of your ai in c plus plus but nobody wants to. Exactly. Like PyTorch and TensorFlow, both are C++ codebases. So you have to find something that is a little bit more modern or recent. But see, I don't know, this is very, I mean, I find it hard. Has anybody coded? When was the last time people coded in C++? Like who is the, who coded last year? Grad school. Grad school. Oh my God. coded in c++ like who is the who quoted last year grad school oh my god i thought that was java yeah here's news for you c++ 21 is amazing absolutely amazing and very different to whatever notion of c++ you have it is so there's a reason why companies like google swear by c++ right reason why companies like Google swear by C++. C++ and Rust that has come now, if you want to go platform level performance, they're amazing languages. Go is amazing. Go, Rust, C++ are very, very fast languages. In machine learning, the two big languages that are upcoming are Julia. Actually actually three big languages Julia Julia is very good but it's getting a bit old now the next is uh rust people are taking very seriously and the third one is Mojo Mojo is extending python but making it native you want to no by the time they have a release yeah we will at this moment it's beta right invite maybe it's only supports You can go to site. Now you can. They claim 50,000 faster than Python. I don't know how is that. Actually, being 50,000 faster than Python is very easy because Python is interpreted. So that is, if they say 50,000 times faster faster than Fortran or C then I would be listening. So I said, okay uses dotnet anymore. Dotnet, oh blast from the past. Is Microsoft actually using it? Even Microsoft isn't. So, well, okay, C sharp I know is still active in the gamer community. Isn't that a beautiful language, C sharp or the is still active in the gamer community. Isn't that a beautiful language? C sharp is like much better version. You used the right word, it was a beautiful language. I'm not using many as well. Copy for Java syntax make it look to make it nicer. So, all right guys, I'll just. I see, I have one more question also, sorry to keep extending the discussion, but you know, I'll just. I see. I have one more question. I'm sorry to keep extending the discussion. But, you know, I hear one of the advantages of vector databases, you can actually selectively remove any bias from the data, right? Although I have some intuitive understanding of how it can be done semantically, it'll be good to hear your thoughts on that and maybe, know have some practical uh hands-on and doing that using the database that's a very good thing i know that there are statements like that i haven't researched it let me research it sure but i what you said is reminds me i've heard