 All right, folks, welcome back. Just to recap what I was saying a moment ago. Last time, we spent our day in three parts. In the first part, we took a case study. In the case study, we looked at one particular thing, how to convert text into vectors. And if we could convert text into vectors, then amongst other things, we could search for text. We could create a little search engine. That was vector search. Then in the second phase, I gave a small homework, which was to extend the search with images as input and images output now in the third part I went into the theory of a transformer attention is all you need the root paper that started this whole field so I will recap by today by giving the context for those of you who are new. See natural language processing as a recap. What is natural language? Language is what human beings speak or whales speak or birds speak or dogs speak. Inherently it's unstructured. It doesn't show up as, for example, tabular data or spreadsheet data, but it's very nature. It's a stream of words, a stream of sounds. So language is a stream of spoken language is a stream of sounds. We encode it as a stream of words, isn't it? But language can be pictorial. You might say, you convey a lot by just showing a picture, right? So for example, we, in fact, in moments of danger, you don't want to see a word, like quite often that in case there is a fire start moving in this direction direction what happens is imprinted in our mind are some colors like red and we look for exit not as a word that we actually read but like that or sometimes there's just a symbol saying out right the way out so in a way you would consider that too is a form of language and many many languages are like that for example chinese is a pictorial language right so natural language when we use the word natural language it is to be distinguished from computer languages computer languages follow a very strict grammar you cannot make a mistake for example if you write C code or Python code, just one indentation being wrong and the meaning of your code changes or it becomes wrong. It just doesn't work. It just throws a compilation error, isn't it? Or execution error when you try to run that code in many languages. So computer languages, in other words, are not permissive of errors. However, human languages are very permissive, even though there is a grammatical structure. That grammatical structure almost always is practiced in the breach. Most of the time when we speak, we speak half sentences or we concatenate a lot of sentences. You must have seen people using clutch words. And they'll say, I went to the beach and I ate some food and I went swimming and then I drove back, right? So you see, and somewhere in the end is our conjunctive words like eh, and um, right? So it's certainly not following a strict grammatical structure. So that's the nature of natural language. Now, what is the processing in the natural language processing? Computers inherently deal with, well, of course, in machine learning, the foundational idea is, if you, for example, build an inference model, the just point either to a number, a vector, or to a cat or a dog. It will say this picture that you are showing me is a cat or it's a dog. That's an example of a classification problem. Now, the question that comes, therefore, is if you're used to the basics of machine learning, the first thing that strikes you is text is not a vector. learning, the first thing that strikes you is text is not a vector. And so the first task that we have to address is how do we convert text into vectors? There are many ways that people have done. You could do it at the level, at many, many levels, actually. You could do it at the level of a word. Convert each word into a vector. And those are called word embeddings whenever you convert it into a vector what does that mean you imagine an abstract a latent space n-dimensional space and for every word there is a corresponding point in that cartesian space n-dimensional cartesian space or d-dimensional Cartesian space and so there is a to the extent that you can connect the origin to that point you get a vector and you say the way you want your machine to learn the AI to do things is you wanted to pick up the semantic content of words, right? You want to make sure that the word dog and canine are proximal to each other in that space. But the word dog and truck are far apart relatively because, well, dogs chase trucks, but let's say dog and checkbook. Well, that too may not work. Dogs eat checkbooks, but okay. Something dog and star or well that too may not work dogs eat checkbooks but okay something dog and star or something like that they should be far off semantically they shouldn't be related right that's a basic idea now the question comes how does how do we embed Take the words of a language, let's say English language, and embed it into a vector space, a latent space, in such a way that semantics is preserved. In other words, words that mean similar things are proximal to each other right that is the job of machine learning or artificial intelligence or neural networks or so on and so forth that is what you're saying you're asking the an algorithm to somehow read a lot of text and then in the beginning it might start by throwing all the unique words randomly in the vector space. But then as it reads more and more words and it sees that certain words go together. Right. For example, king and queen tend to be together. Right. Or male and female tend to be together. Then move those points a little bit closer to each other, that words that tend to be together, right? And so the thing is, as the algorithm encounters more and more text, it sort of shifts those word vectors, those points here and there gradually, such that words that are similar in meaning will be pretty close together. And when you can do that, you can play games like you can say a king minus male plus female is the moment you do the vector, king vector, and you subtract from it the male vector, and then you add a female vector one of the lovely things that people realize that if you get your embeddings right your embedding vectors right you can actually do a bit of vector calculus you could do this kind of algebra and you will find that if you take king minus male plus female you will end up pretty much next to the queen vector. Right? And so you can say queen is approximately equal to king minus male plus female. And it actually happens. It was one of the first successes of using deep neural networks for natural language processing. That is when people really felt that maybe these deep neural networks understand things at a level that is beginning to make semantic sense right until then the field had been doing things it's not that the field wasn't making progress but it was sort of unsatisfactory levels of progress or partial progress this This was one of the great milestones. So you can therefore embed a word into a latent space. Then people realize that maybe word is not where you want to do. For example, if you want to do, I don't know, a composite word. Let's just say ripe apricots. And you tend to notice that ripe apricots always comes with a hyphen and they always go together. What you could do is you could break it up and have an embedding for ripe and embedding for apricots, right? Or, for example, the word running and so forth. So you may take run and then ing part, you can just consider it as a extension of run because ing is attached to many things, running, jumping, swimming, and so on and so forth, right? And so you could take a run vector and attach ing, and you will get sort of the process of doing it, just as swim vector and the ing would be the process of swimming, right, and so on and so forth. So people began to do word pieces. And they got quite a bit of success. So one of the things that people began to realize is embeddings are foundational to natural language processing and which is why I started this course, this workshop with embeddings. But amongst all the embeddings, there is a problem with word embedding. Words have meanings contextually, the way we use in languages. For example, words like apple. So in linguistics, we talk about synonyms and homonyms. What are synonyms? Two words with the same meaning. Great. So would somebody like to offer a synonym? What's it? You could, but the homonyms are less well known. We all know it, but we don't know the term. Homonyms are when the same word has multiple meanings. For example, the word belay is to see or to cheat or to deceive, right? Or to lie and so forth. So there are many such words. Now belay is a word that you may not be familiar with, but let's take the word apple. To folks here, the moment you think apple, immediately you begin to think of a lovely little laptop, right? MacBook or something like that, or iPhone or something, hardware. But to the farmer, it's a fruit, right? To the poet, if you say, she's the apple of my eye, you think of her sweetheart, right? right um so you're thinking probably of a pretty damsel or something like that so well there we go the word apple is a homonym it has multiple meanings it is the context tells which of the meanings to pick up right so when you say looking for ways to grow ripe apples You certainly are not going to create ripe laptops, isn't it? You usually don't put your laptops into the oven and bake it, right? Or put it in the ground and wait for it to ripen up. You're very specific. It has a farming context. So things like that. So one of the problems is when you create static word embeddings, you say, let me read a lot of documents, all that. Let me read the whole of Wikipedia and all the free novels in the world and free texts and stories in the world and generate word embedding. So there are word embedding algorithms, word2vec and GloVe and so forth. And in a previous iteration of this course, I went extensively into how to create those embeddings. I don't usually in this course, I decided not to do that for a reason exactly that static word embeddings are not as powerful as contextualized word embeddings. And that contextualization comes from, if you remember, we talked about attention. What was the point of attention? It's context aware. It creates understanding that is context aware. And so the embeddings that in the process of things are generated by transformers are far more sort of useful, isn't it? They're far more powerful. In particular, we took the example last week of taking entire text embeddings. They're usually called sentence embeddings, but in linguistics, in natural language processing, the word sentence is used in a very loose form. It may just be a phrase to love a dog. It's not a sentence, it's a phrase, but you consider that a sentence. It could genuinely be a sentence in the English sense, that it has a period or an exclamation mark or a question mark at the end of it. It may be just a clause with a semicolon at the end of it, or it could literally be a paragraph. So the meaning of the word sentence is not what grammarians would call a sentence. It doesn't necessarily mean that it ends with a period. So we'll take it more loosely. But generally, when we say sentence, we don't mean arbitrarily long sentence. We mean a reasonable piece of text. Like, for example, to call a war and peace by Tolstoy, a sentence would be perhaps stretching it a bit too far. right? So you don't usually do that. It's a relatively short paragraph. There's a sentence. So what we observed is that you could take a sentence and create an embedding of the sentence itself. And the amazing thing is because transformers are so context aware, they tend to generate pretty good embeddings for sentences. Now, how we do it, we didn't go into the sentence transformer. Yesterday, last time, in the previous session, we started with the basics. Remember, we took the approach of crawl, walk, run. So, we were still crawling. We got the gist of what attention is. But as we make progress, remember, afternoons will give to building up the theoretical foundations, but the mornings we give to practicals, to use cases. So we did that. But magically, somehow, we were able to create the right contextual embedding of a sentence into a latent space so that you could find similar sentences. And then came a very powerful generalization of it. We asked this question, why just sentences? Could we not embed pictures? So if we could look at a picture and if a transformer can extract the semantics of the picture, you see a man loving a dog, right, and could caption it out and say man loving a dog. Then it's a very short journey to go from a picture to searching into a corpus of text sentences. So now we have generalized the concept of sentence a little bit further to images. And that is what we did is and now this comes this beautiful idea that will be the sort of the leitmotif through the course that you piece pieces, powerful technologies together into a chain. So you have one transformer that can take a textual text and create a vector embedding. But if you have another transformer architecture, which can convert a picture, render a picture into a semantic sentence describing it, right, that can caption the sentence effectively. And that is the clip architecture we'll talk about. We'll cover all of these theoretically in detail. So now by chaining these two things together, what could we do the last time? We could show a picture to us, to a search, to a repository, a box of sentences, and we could find those sentences that would match this sentence in a corpus, in a search index. Remember, and that is a very powerful use of it. Right? Now, we could go, we could go another way, you have a sound, somebody is speaking, what can you do to the sound and this is yet another transformer we'll use, we'll use, as an example, we'll use the whisper, another neural architecture, to convert that sound to text. But the moment we can do that, what can we do? We can search a vast repository, isn't it? And find similar sentences here. Right? So we could do that. Now, we could now just with these ideas, see what else can we do? One of the things we did is we did a text to image search. How could we do that? We took a large repository of images and then we do the semantic extraction from it using clip. Using that transformer, we then created a search index. Our search index was very naive. We literally created a list, and we did a nearest neighbor. We literally did a cosine distance to each of these pictures pretty brute force right but we solved the problem and so what we could do is you could say to a friendship with dogs as a phrase and certainly out popped if you remember the pictures that represented friendship with dogs i don't know if you guys were impressed, but I find that very impressive that today the AI can do that. And believe it or not, all of this power has come about only in the last three, four years, quite literally. Trillion pieces of text or trillion pictures or trillion videos or a mixture of all of them, pictures, text, videos, audio, soundbite. And I asked you, search for it. Forget that you know about transformers. Initially, just five years ago, how would you search? You would do image-to-image comparison would be horrendous. Isn't it? Pixel-by-pixel comparison would be be horrendous and to be able to compare that to a trillion pictures of the hopeless endeavor isn't it or yeah i throw sound bites into the mix or x also it doesn't make sense uh guys you'll have to mute this is for harm yeah that's okay. Now, it's one of those Zoom things one has to get used to. And yet I'm going to, today we'll focus on taking the ideas of last week and scaling it out. And when we scale it out today, you literally reach a place that you can take and use it in your workplace. So what is the main problem? The problem is we do realize that we can convert whether it's an image or whether it is a soundbite, whether it's a text, whatever it is, and whether it is genuinely a vector, a data. You got a data, right, as a vector. All of those, you can do vector embedding of. Once you do vector embedding of, for each item, you get a vector. But if there is a ginormous number of vectors, now we will solve the problem. How in the world do you search through it? Right. Now, see, going back from vector to whatever it was, like returning a sound byte or returning an image is easy. We did that last time. In fact, your homework was to do image to image search, for which I'll show you the solution. Actually, the trick was, the solution was staring you in the face. You just had to see it, isn't it? But anyway, we will do that. But the big question is, how do you search? Because I told you that when you compare two vectors, you use either cosine distance or dot product distance. Do you remember that? And why do we not tend to use Euclidean distance? Anyone remembers why we tend not to use Euclidean distances in these situations? Anyone? Guys, have you noticed why all through our school we learn about Euclid and his distance? Delta x square plus delta y square square root, and so forth. And yet you come to machine learning, and especially these recent papers, and wherever you are, you find all sorts of notions of distances. But you can poor Euclid seems to be forgotten quite often. Why? Curse of dimensionality. What it basically means is, and without going into it, maybe I'll make it an extra thing we can talk about in the theory part in the afternoon. after in the theory part in the afternoon, the curse of dimensionality is really, if you remember the Harry Potter stories, there used to be this very, very dangerous curses that could be put on you. And in the machine learning world, the most dangerous curse is the curse of dimensionality because data comes as very high dimensional, like you do embedding in high dimensional space. In high dimensional space, the problem is there is just way too much space and all the points run away from each other, right? So when you just, so think about it this way. If you just think of a unit distance and you put 10 points together, they will randomly, they will be fairly proximal. Now I make it two dimensional. You agree that if I randomly throw those same 10 points in two dimensions, they will be slightly further apart, right, in general. Now you make it a unit cube. They'll be even further apart. So what happens? There is just way too much space in three dimensions for those 10 little points and they will be significantly further apart from each other so imagine what happens when you go to a thousand dimension there is just way too much space and they are all lost in interstellar space effectively so the idea the point is that to your neighbors are pretty far off in fact you have to go really far to get your neighbor now we can we can quantify it and more. There's a very lovely illustration we can do at some point. But the main idea is that if you look in higher dimensional spaces, compared to you, all the other points are as far as they can be. So there are no neighbors, basically. so one example that i give is when you mean neighbors you mean the person living next door to whom you can go you know if you have cooked something nice you go and give some to them right on the other hand neighbor doesn't mean like for example if you're living in wyoming or montana then a neighbor can easily mean driving one hour right to meet your neighbor so higher dimensional spaces are like living in Montana and so it defeats the purpose of finding similar sentences you know searching for because you're searching for neighbors you're not searching for that so Euclidean distance as in not searching for that. So Euclidean distance as in as the bird flies distance doesn't help you. So when it doesn't help what works? The degree of alignment. What you say is in the semantic space what if we were reasonably sure that two points that are semantically similar, they are in the same region of the sky, right? Now, sometimes you see two stars in the sky. You know that one is dimmer than the other. One is probably hundreds of millions of light years away compared to the other, right? And yet they are along the same direction. They're more or less aligned. So what if you could train your network to learn things in such a way that the alignment captures semantics, then you could take alignment based measures like cosine distance, and you could take dot products and so forth, which is why those two tend to be favored now as a practice some people what they do is so which is preferable cosine distance tends to favor shorter it's just an empirical thing you can check it out tends to favor texts that are shorter. Why? Because we will write short queries. And so if you have another query that's somewhat shorter, in your index, you'll have lots of short data points. They'll be much easier to be near it. What will dot product do? There's a big piece of text way out there with a big vector. This is an embedding vector but this will get aligned very well with that right so it will tend to bring in a lot of big vectors also for you right so a dot product seems to slightly favor longer results search results text now comes another thing sometimes what people do is they normalize the vector deliberately they say before i index it i will take every vector and divide it by its magnitude so what is a vector it's made up of a magnitude and a direction so if you throw away the magnitude and make the magnitude one the only thing that remains is a direction. So it becomes a point on a hypersphere. If you make it a point on a hypersphere, then obviously cosine and dot product, they have identical meanings. And if they are all on a unit sphere, then you don't, then in effect, right, you could even try your Euclidean distance or something like that, right, because they are all in, they are all great arcs on that sphere. You're looking at the arc distance. So there'll be minor variation in the way that you that you look at it. So this is it. So that's something to know why you see that you encounter machine learning, especially NLP books. And by default, I mean, by default, most people will end up picking cosine distance and why they tend to pick cosine. That's the explanation for that. So, and now, we were at a place that, so whichever distance measure you take or similarity measure you take, you realize that if I have a query and whatever the query is, whether it's a text or whether it's an image or it's a sound, and I can convert it into a vector or do the embedding to use the formal language, I can create an embedding or the latent vector, vector representation, vectorizer. Then that vector, I need to compare with every single billion vectors or trillion vectors in my database. To do so, would you call that an effective methodology? You wouldn't, right? So here is a bit of a history, guys. If you look at the search world commercially commercially let's bring it down to real life these days when you try to do if you are searching for something in the web google has become synonymous with searching though a bing is trying to dethrone google these days with chat gpt but we'll see right you google it out. Then comes, but in enterprise search, what do you do? Most of you are developers of some sort. In enterprise environments, what do you use for searching through whatever documents that you have or texts that you have? Elastic search. Elastic search, I told you, is linguistic search, keywords-based search. Remember? Elasticsearch, I told you, is linguistic search, keywords-based search. Remember? So would somebody like to tell me what's the difference between this kind of vector search that we are talking about and keyword-based search? Keyword is just an inverted index. For every keyword, you remember all the documents that contain it, right? And you look it up. To the first approximation, there's more technology to it. It has evolved. But in its simplest conceptual form, that's what a linguistic search is, or keyword search is, it's an inverted index. Right? Now, given that Elasticsearch was the king of the hill, and then Solr was doing well, both of these are based upon Lucene, the open source project Apache Lucene, at the core of it, which gives you the underlying library of that. So suddenly this transformer-based vector search comes about, right? And you can imagine that it's eating their lunch, so what they did what elastic search did is they they now have given extension you can take all the text and they themselves can do the same thing that we did they can create vector indices and they call it the vector search ability right so for every document they search they shred it out and do all the normal linguistic. But at the same time, they'll create a vector out of it. Now that they do the vector embedding, they can do the search, right, your query term, it can search against the vectors also if you want. But the problem with that is and this is a problem that is still today, I as of the last version that i saw it is still there the problem is you can do that so long as the number of documents you're comparing it to is in the order of i don't know less than a million even at a million the whole thing just feels slow isn't it because you lose the benefit of an inverted index in an inverted index. In an inverted index, you look at the keyword, you have a lookup table. You have effectively a dictionary, you look at that. Now you have to go and compare it to every vector, right? So it does feel slow. How do we have a cake and eat it too? That's a question, right? So there it turns out, and that is the context for today. And that would be a place at which I will take you guys. The reason I wanted to take you here is I wanted to take you to a place where you could actually use it in your workplace in a massive manner. So that brings us to the topic of ANS, the approximate nearest neighbor searches. So when you do K nearest neighbors, we'll still look for the K nearest neighbors, K being a 10 nearest neighbor, 5 nearest neighbor, 20 nearest neighbor. But can we make it fast by being willing to give up a little bit on the accuracy. So in what way will we give up on the accuracy? See, remember I told you that in search, the two crucial measures are precision and recall. I mean, there are many ways of looking at it, but I took the framework that will care about precision and recall. What is precision in a search? Who would like to tell me what was precision in search? A search result is precise if what is true. Yet most of the search, the more the search results are actually answer to the query, the right responses to the query, the more precise it is. The more it contains things that are irrelevant, the less precise it is. So you're searching for ripe apples and it shows you lovely pictures of MacBooks. That is not a good search result, is not a good search result isn't it so that is one way the other is recall what was recall it's like if you have 10 items how many show up like you have like 10 and nine yeah so suppose i knew because you're the magic fairy with global knowledge you know exactly all the billions of documents that you have indexed or gazillions of documents that you have indexed, or gazillions of documents that you have indexed. You know that for this query, which ones really should show up. Amongst other search results, irrelevant results can show up, recall doesn't care. But it says that I know that there are 10 of these that must show up. And yet, when you get the result only three of them shows up that is a poor recall right by the way is this real it is to the topic of approximate nearest neighbor so obviously you have to give give up on something so if you could give up a little bit on precision and recall very little bit you would have the cake and eat it too but how much do we give up well theoretically you can give up an arbitrary amount in practice though what happens is there are always other tricks up your sleeve right so by the way now i'm beginning to reveal the sort of things usually like that makes for a kind of a product whose secret sauce you don't reveal, right, usually. So here is what you do. Suppose I make my search a slightly imprecise somehow, it gets a lot of documents. Instead of getting the 10 nearest neighbors, suppose you want 10 search results, but I go and grab 100 search results, right, or 50 search results. Now, I have a problem. Because it was slightly imprecise, I know I can improve recall by making sure that I grab more. Because in 10, my all 10, the things that I wanted may not exactly be there. Only seven may have showed up. But if I go and grab 50, very high likelihood that all 10 would be there. So I solve the recall problem. Now we have the other problem, precision. This ranting contains irrelevant results, isn't it? So how do i solve that so what i did is i broke the problem into three stages you search imprecisely and then you put something so first you index it you vectorize it then you search it with with a little with the understanding that there'll be some noise in the search results. And you can solve the recall problem by getting more. But you don't mind that because you know, at the level of memory server middle tier memory, the difference between 10 results and 50 results is not much. These are transient memory, like you use it and freight up so you're willing to do that right then you have a problem you need to re-rank in such a way that the top 10 are the most precise you need to improve the precision you solve the recall problem by getting more but you have to solve the precision problem by re-ranking am i together guys and if i could do that what have i achieved i have done lightning fast retrievals but while i was scooping into that big pile i managed to get some dust also right then but i know that my gems are there. Then all I need to do is shift through it. Right? It is almost like you're shifting through that and letting the dust fall off down, and at the top are the search results that you want to serve. And if you do that, do you see guys what we have done? You have to vectorize it, put it in a big pile. There's a problem with that pile. And now you have to intelligently design that pile so that you can do very fast retrievals. But when you retrieve, you will get your gems, but you'll get some dust also. In other words, you will get some not so relevant results. but then don't suppose you want 10 results no no we'll come to that it will come to that that's the whole technology of approximate nearest neighbor search algorithm right and i'll give you ideas on how to do it but suppose imagine at this moment at the hand waving level first you could scoop in but the problem is you is you may miss some and you may get some irrelevant ones. It's approximate. Well, the two axes that you want to improve upon are precision and recall. Recall you can address by getting more. So you are willing to burn some CPU cycles know some data retrieval and bandwidth and so forth but getting more now you got all your gems you're getting that albert right you get more so instead of 10 you get 50 you probably i mean by empirically you can establish what is a good number to get from right given the fact that you need. Now you do have a real problem though. You have also retrieved a lot of junk. Right? So you need to seep through that and pick the gems. And can we do that? So how would we rescue this problem? Right? So where is the knight in shining armor that can solve this problem for us? Make a guess. You can brute force search again within that 21. You can brute force search again. Yes, because you have only 50 to compare to. Excellent answer, Shishinde. Because you're not comparing against a trillion. You got 50. So long as to the trillion, you can come back lightning fast, your query vector can now be precisely and exactly compared to re-rank against just this 50. But actually what you do is to do this even in a better way, not to have to do even this, you use the knight in shining armor that comes to your rescue is yet another transformer. So one lesson you'll learn in this world soon is it's transformer, transformer all the way. is it's transformer, transformer all the way. It is yet another transformer architecture that will help you re-rank. It's a cross encoder, the transformer architecture. It's called a cross encoder because it is looking literally at the distance between two different pieces of text precisely right and coming out with an answer simultaneously right so what happens is that see the reason you use so you ask this question why can't i just use the cosine distance i have just 50 of these right i have the embedding vectors the problem with embedding vector at this level is, see, when you generated embedding vector, you assume that perfect embeddings were generated. But it is not perfect. What you could do is you lost actually a little bit of performance, a goodness of fit, because every text was individually embedded but there is a technology which is even better which will take a reference sentence and it will do it will cross find the similarity not just at the cosine level but at a much deeper neural network semantic level between this and every single other text that you give it right to make it more real imagine that it has a very complicated way of assigning probabilities of two vectors being similar and when it is doing that it is not using something as basic as cosine because it's saying, give me a series of tokens and give me another series of tokens or any stream of things, vectors. And I will tell you how similar they are and how will I do it? I have this big magical neural network inside. It has its own attention heads and so forth. But it will tell you much more precisely than a cosine can, the probabilistic similarity between them. And that is yet another cross-encoder transformer. So now let's look at the journey that we are taking. What we are saying is, so that's why, do you remember me using a sentence. I've taught you semantic search, the half of it. Did I say that last time? That half of it is actually pretty much what Elasticsearch as of today uses. But the best part is not that. The worst part is what we are going to talk about today. Because you want to do it at genuine industrial scale. That is just shocking fast. And you don't want to lose. You want to have your cake and eat it too. You want to get back your precision and recall. And that's when you have a full solution. Are we together? All right. Go ahead, Albert. Vector cosine search across a large corpus of documents in the vector space. And of course, doing what the history of this sentence bird that we use, that you used in the first lab was, before that, people were trying to use what you would today call a cross encoder to do search, given the text across all the other texts in their corpus or images in the corpus and they knew that it just is very very good right this transform that thing produces wonderfully good results such results problem is that is simply just not scalable right performance is horrendous why because even one query is expensive. That query gets hugely expensive and the bigger your corpus, and it gets even more expensive when you take in vast, you know, traffic of queries. So you cannot take it to web scale at all or enterprise scale at all. So if you start from that point, see how well the problem has been solved. Such a big problem is solved. First, you take the intermediate step of embedding everything into a vector space. Now, you know, vector comparisons are cheap. Cosine comparisons are cheap. But even not cheap enough, because if your corpus is big and you're getting a lot of web traffic, that expensive you bring in another piece of technology you do approximate nearest neighbor search you get back the results you realize that you you may have lost a bit too much in terms of your precision and recall then you bring yet another piece of technology to improve upon that. So that is your intellectual journey for search. Right. And we are as as an NLP exercise, our first case study was to create effective search engines. We'll do that. So with that, let's get to the lab. By the way, guys, is all of this looking interesting to you or not i i did this is a hand-created call right these are some of my favorite poems or sentences and i took it um if you are in the you know people who are mathematical physicists or strength theorists and so forth uh you have to absolutely swear by Alice in Wonderland. Everybody loves Alice in Wonderland, right? And through the looking glass and so forth. We named our theories after Alice, Alice strengths and whatnot. So anyway, Jabberwocky the poem. Tale of Two Cities was one of my, I don't know how many of you like Dickens. I absolutely love Dickens. I absolutely love Dickens. So a couple of passages from that. Then Mark Twain, some things. Twiddle Dee and Twiddle Dum again from Lewis Carroll. A few things, a few good statements and Golden Retriever from that. Can you guess which dog I have? How many of you have met my dog, by the way? None of you. I'll bring her next time. Oh, yes, Kate, you have. I'll bring her next time. Oh, this is one of my favorite poems, Keats. I think of beauty as a joy forever. Attention is all you need. Literally the abstract of the paper that I explained last time. And back prop, the Wikipedia explanation of back prop. And a poem by Wordsworth and full of cares. So guys, do you realize that I took a variety of texts and we are going to search through them and I made them hard. Then we went to the search. Let's do semantic search. And I'll quickly recap now. I'll make it brief. By the way, how many of you at least reviewed the notebooks once you went home? You did, right? Was it useful? Did you find any errors in that? It was useful. And so this is it. The gist of it is embed it into a vector space, just use vector distance measures to get your answer. We used cosine similarity and dot similarity. Now you know why. Yeah, one distinguishing point was symmetric versus asymmetric searches. It turns out that it matters whether you have trained the embedding, the creation of embedding, to search for short answers, you know, symmetric, like query and this are more or less of the same length. So, for example, question answer, much better to do perhaps with a symmetric way i think we'll have to check you know and asymmetric is better for you know our search we'll put a short sentence and expect quite a bit of results so i used a asymmetric model for that reason and guys this is one of the straight outs that people don't, the SBIRT website mentions it clearly, but most examples that you see on the internet, you actually see them use not the right model for the use case they're doing. It is just doing due diligence and knowing how the model was trained so you use the right thing. So we create the embeddings. How do we create the embeddings? We read the sentence and a list of sentences and we create embeddings. And it is as simple as that, .encoder. Well, we had 16 sentences. Each sentence has become a vector of what length? What's the length of the vector looking at this? 768, yeah. Go ahead, albert so how did you find this model oh it's in the documentation on the website i'll walk you through it no no suppose you have a problem like you said before you don't need the right model oh how do you do that see what happens is that you have two choices either to take a sentence transformer from scratch and train it yourself. But that is prohibitively expensive. You don't do that. What you instead do is you take a transformer that has already been trained by somebody, but for a specific purpose and released. So when you release it, it is in the list of available pre-trained models. So a model is nothing but exactly the same transformer, sentence transformer, but trained for a specific purpose, a specific data set. So you go to people or you go to Google? No, you go to the website of that particular architecture and they'll give you, right? Or you go to Hugging Faces and you look up the list. And the trouble with Hugging Face, though, slightly be careful, when you go, for example, when you go to Sentence Belt and you see a catalog of the models, you know that these are the best models. When you go to Hugging Face, then random Joe has decided to do its own and put it there, and you don't know which one is the best. So you have to watch out for that also. Now read the documentation and it always helps to talk to the community. So there we go. You get this and of course then comes the third phase. You get the model but your problem is not exactly what this model was solving, or trained to solve. So then you do domain adaptation. So for example, Patrick, you and Sukhbar, you release that medical domain, right? So how will I do search now for that data? You would do domain adaptation, means you would fine tune, further train, already trained transformer to be smart enough and more precise for this domain, right? So that is domain adaptation. That's fine tuning, essentially. And the fine tuning, you don't need very powerful hardware. Just make sure that you have a $20,000 workstation and it's fine. I'm kidding. You can do it on good workstation. But I don't know, below 4090, I don't know, but at least with the 4090 plus, you should be fine. You should be able to fine tune it. I'm told that it can even be done on Studio M1. your m1 now they release we can do it most of the even after last recently released the framework oh okay awesome yeah so there you go now you can do it very nicely so that's that so it so we we saw that example a sentence a poem like so remember when you use the word sentence is this a sentence no it's many sentences but in the computational linguistic space or natural language processing space sentence even though the grammarians would be very angry it's still a sentence it It becomes a vector. As you can imagine, this vector is indecipherable. How many of you see something very valuable or informative staring at the vector? I can't, right? So there we go. Then we did a query. Now I deliberately took a relatively hard query. I said a friendship with animals. Do you realize that I did not say dog? That is too easy. I put an emotion there, some subject, some quality there, relationship, it's a friendship with animals. It's a positive relationship, it's an emotion there. And yet, it was able to come up with answers, right, and observe something and there's deliberately a crafted it, you know, it has the word animals, what would and if you go back to the corpus, if you did elastic search, guess what would be your first search result, that text, which has the word animal in it? Isn't it? And I don't think I use the word friendship anywhere in the text and yet those of you who have a golden retriever would swear by it that the first result is the most relevant anybody has a golden retriever oh good grief i used. I used to. You used to. Yes. Would you agree that the golden retrievers are the angels and the lights of friendship? Oh, yeah. She was very sweet. Yes. They're absolute things. So there we go. And look at this. Do you notice that in this sentence, look at the search result number two. If you're lucky, a golden retriever will come into your life, steal your heart and change everything. Is any of the keywords present? Is friendship present? Is animal present? No. And that is AI search, guys. And that is the power of the concept that I was teaching you last time. is AI search, guys. And that is the power of the concept that I was teaching you last time, attention. You see where it is all coming from. You can't connect all the dots yet, but you see why contextuality, the whole concept of attention is context helps distribute, differentially put a weightage on different things. That was attention. That is at play. Then you could give it a visual search and this is it. This search leads to what? It can search through text and look at the search results. It is searching only through my 16 documents, 16 texts. one of the text was golden retrievers are little children it came out as search result number one do you think it's appropriate this happens to be the golden retriever species right picture it latched onto it the other one for the love of these furry animals. Is this picture furry? Right. And the third one is a smiling dog. Now, those of you, if you knew Golden Retriever, you would know. This is a happy face. Right. So anyway, this is a recap. Text as query. You could give query text and you could search images. Why? So now comes the interesting thing. You just figured out a way to convert text to vectors. But how do you convert image or sound to vectors? Well, as I said, transformers all the way. There's another transformer architecture called ClIP that was created by OpenAI. One of the big works, in fact CLIP and DALI are opposites. CLIP goes from picture to text and DALI and stable diffusion, they go from text to picture. So you have that. everything is a transformer right as i said transformer transformer all the way right ai has become nlp and uh multimodal learning is now dominated by transformers who uses oh yeah yeah they use they all use these things underneath it yeah and by the way clip is something we'll cover in detail in this class both dally and clip their actual architectures will cover whisper dally clip will cover oh that brings me to one important point guys i realize that the pace at which i'm teaching i have two choices either accelerate it quite a bit to finish it in six weeks or extend it by a couple of weeks i'm leaning towards extending it by a couple of weeks of course right i hope that is okay or would you prefer that i just accelerate accelerate okay some people i accelerate some i extend okay i'll do both i'll accelerate a little bit i'm going deliberately slow in the first i will accelerate and as needed i'll extend it so you create a search index you do the prompts oh uh well in this particular case because the pictures are not on my machine i just copied it over onto this machine uh from the other machine but you know that those pictures come out and those pictures were uncannily like uh relevant in fact one of them we said a path to the forest right and the path to the forest was also uncannily. It only found very specific things, paths to the forest. So you can do that house on a lake, it would find house on a lake. So with that being there, now we go to today. Let's come to this. How do we do approximate nearest neighbor search? So one of the so there are many algorithms and we'll do that. But in terms of practical implementations, there are 2 leading. There are 3 or 4 leading contenders. Let me give the name. One of the first ones that was very good. But I'll give you a homework. The state of the art today is scan. Scan has been done by Google. It's also an open source project. So your homework is to translate this lab all you have to do is instead of using fares use scan so you become familiar with scan so please take this down as your homework right now with these things because these are all computationally big things like i said it's you know, transformers all the way. This is one thing that's not a transformer. This uses a completely different theory of approximate nearest never search, but we'll use this in between for doing it. You'll know when I explain the theory. So we will take the same thing. What I'm doing is let's go through this code a little bit. Oh, no module sentence transformer. I think I tried to run it here, but okay. Guys, this first notebook, start running it on your machine at least as we go through it. So what am I doing here? I'm loading the, in this, I'm loading that toy corpus of 16 documents, doing here? I'm loading the in this I'm loading that toy corpus of 16 documents, 16 pieces of text. Next is let's look at this slide from sentence transformers import sentence transformer. Is that easy? Yes, doing it now comes fast. Fast is a library. Now one of the things is what it does is it also embeds data into, think of it as a latent space. But how does it do it and how does it find distances between points? That is decided by this index flat IP. What it means, I was initially going to explain it to you, but then I decided it would be far better. I'll make it as a homework for you to read. So read it and then towards the end of the day, I'll explain these things. But think of it as a way of putting it there and then finding neighbors. So what are we doing? I created a simple class to make it easy. This class needs a transformer. Why does it need a transformer? A search index why because you need to convert sentences into vectors right text into vectors but right so in in the beginning the simple uh fast index i i just made it only for text just to get the ideas across so you need an embedder or the sentence transformer what else do? Well, you need to know the dimension of the vector it produces. Right? Then what I do is set position. Actually, this position is equal to zero is irrelevant. You can forget about it. I also keep track of which documents. Because if I'm going to index a document, in the index is going to become a vector. But what I have to return is not vector. I have to return in documents. So I have to return is not vector I have to return in documents so I need to create a list of documents available there right I take out the self.position is irrelevant now add to index I've given you oh by the way I've given some of you may ask why what is all this listed it is called type hinting yeah people like me who come from c c++ java background we like to declare the type static statically type language we like to declare the type of each variable so uh that's what why what i'm doing it's optional you don't need to do that a lot of people don't do that though in my team if you were a developer in my team not doing so would be a fireable offense so team if you were a developer in my team not doing so would be a fireable offense so you what you do is you you keep you add this document so when you are getting some documents to index the first thing you want to do is add it to your list of documents and then you create embeddings from it and then you go to the first thing the ann and ask it to index it internally by some magic do keep it so does the add to index look easy to understand guys and then report what is the total number of documents you now have in the index what's the size of the repository now let's look at search and see if it is easy to understand. I'm saying search, you give it a query text. Remember, it's very specific to text at this moment. I'm saying how many search results do you want? For simplicity, I've said that if you don't tell how many, we'll assume three. Python has this lovely thing, which I wish Java had, or C++ had. You can give default values right so you can query the the you can take a text make it a query vector and go into the index and find the approximate neighbors for it right when you do the approximate neighbors and then what do i do what am i doing here can somebody guess what does this line do i'm just putting it prettily in a in a panda's data frame right and when you do so uh you will get something you oh yeah yeah that is right you are saying that the return value is a panda's data frame the arrow what comes after the arrow is the return type this is also part of the type hinting type hinting yes type hinting it's very useful guys i I wish I could convince everyone to use it. Right? The people who come from see there's a reason that all enterprise software production grade are written in C C++ Java. They're strongly type languages. You read anybody's method, your immediate function, you immediately know what goes in, what comes out. And sometimes you may not bother what what mischief goes in inside the method so long as it passes a unit test it's fine in ds code as if you can you can get python to do strict python yeah you can yeah they can force it to yes and there are tools that can take a normal python code and uh convert it to type hinted code. Pydantic and so forth can do that. So it's very useful, but do that. Yeah. So here we go. I just put it in a data frame. By the way, in my case, I'm not seeing it here. The reason is I ran it. Maybe I put it on Slack. Let me copy it over from Slack. Oh, somebody has put a nice, oh, scan link. Thank you. Who did that? Kate, thank you for doing that. Semantic search, 0102. Let me take this, download it again, and give me a second, guys. I want to show you the results. And give me a second, guys. I want to show you the results. And I go to C drive. Also, I see you posted eight files. Yeah. Is that all required or just the semantic? No, no, no, just the semantic. And the first one has the solution to the homework I gave last time. That's all the 0, 1, the lesson, 0, 1. The rest of them are exactly the same. let's go and get this guys and i'll run only this okay guys so here we go. What happens? I create this simple search index. And then in the search, I've created, say, this is one of the things in a good object-oriented programming means you have to make it absolutely dead simple to the user. things to the index and search for things. So there are only two methods, add to index and search. So this is it. It tells there are 16 vectors in the index. And then when you search for it, a friendship with animals, it returns a nice data frame. And would you agree that the results are good? Right? Yes. Yeah. So, so I said that approximate nearest neighbor is approximate, but in actual practice, it's very good. Like, it can be better. We'll use a cross encoder to be better, but it's in general very good. Which is why actually, you notice that most people, so here's what I see, I was just looking on the web, examples of semantic search. Most people just mention up to what we taught you last time on the internet. Some people go and when you, they go into FAS and ANN. They usually, most people haven't realized that FAS is old now, you should use CAN. haven't realized that FAST is old now, you should use CAN. Very few people, very few people realize that you can easily, cheaply improve your results by at the tail end of it putting a cross encoder. So that is the whole journey that you have to do. So here we go. Guys, would you like, you guys are not not doing do it as a project, should I create a more structured project in which you do all of it? Or do you want me to release a solution in which all of it is stitched together? You do wonder, okay, I'll do that. So now, can we just do we just search through this or can we search through images also, image and text at the same time? So I'm saying that doesn't matter whether you give it an image or a text. The result should contain a mixture of text and images both. Can we do that? The answer to that is, of course, why not? We have done it the last time. And the only thing is, we do it now with fares with image there's one limitation at this moment the clip version uh you cannot search with long text your search query has to be relatively short so i mean or let's put it this way the the what you can index both the search query as well as the text that you can index has to be limited so here is what people do in reality or actually what i did in reality what i did is i use clip only for images right and i used a very powerful the macro the other one for text and i would do a merge i would i would take a search query right and i would shoot i would i would sort of scatter it to two different things then the search results would come i would gather it back right so i would do a scatter gather approach because then i would have the best because i can then sustain pretty long texts text pieces of text and so i mean so those are the tricks but obviously this one doesn't here i put all of it together so this is very easy if you're adding some text you would do this if you're adding images, you would do this. Right. And rest, when you search, you can what do you need to do now? Not only do you have to save the text, but if it is an image, you have to remember that that text is actually not text, but it is the location, the file name of the image, right? And you have to also know, how would you know that? You have to have a Boolean flag saying that it's an image. So to do that, I created, what I'm creating a list of is not the text itself, but something called a document item, which tells whether the text is actual text or it's the location to an image. Make sense, guys? So now I'm going to give you a piece of, and you can search. And when you do search, you can do it. Now, guys, I'll give you one thing. Run this. When you run it, you'll realize that there's one piece I didn't finish, which is a very easy one. In the column, the search results column, it will tell you the image file name. When it is an image, it will tell you the image file name. Not very good, because what you would like to see in that in that pandas column is the actual image. Right. So I leave that as an exercise for you guys to figure out how to do and show me one of you that you have pulled it off. to figure out how to do and show me one of you that you have pulled it off are we together guys this is the last one is the last one yeah and um let me just tell you that indexing these images now uh will like go to lunch after starting the indexing it time. It takes about on my machine. It took the better part of 7, 8 min. I don't know how much it will take on your machine. Your mileage may vary. So this is it. Now, first, I encourage you to read it, and your other homework is redo it with scan. So there are three algorithms that you should know. One is FESS, one is scan, and one uses graph theory, uses a concept called hierarchical small worlds. It represents the fact that in graphs, complicated graphs, you see substructures, you see sub-clusters. So it uses hierarchical navigable small worlds map. It is called HNSW maps. It's an approach to creating approximate nearest neighbor. And I encourage you to use that also as a bonus. Now, guys, what I'll do is today I will stop here. The cross encoder I'll come to later. I want us to try and do it as groups. We'll do that here in the afternoon. Please right away form your teams, your groups, and get busy with this. You can do it on large corpus, or you can do only with this corpus. Play with scan. Play with HNSW. I'll let you find the library for that. Great. And also improve upon the results. see how you can do that so basically the lesson all those algorithms right and make this all work make sure it all works on your machine and is it the same copper so you get the new quality up to you totally up to you which which corpus you use this is like for the lens do from now and i'll be with you guys i'll come and see what you guys are doing and then in between you can take a lunch break i will consider it a lab till 2 30 in between take out time for lunch but come back and do the lab and so i want to convert this course to a lot of doing things at this moment and then in the afternoon I will do the theory part and at some point I will bring in the cross encoder which is very straightforward and the other homework is guys clustering and let me do that let's go to s bird website to S-Bird website. So read about cross encoders. At this moment, you won't understand the architecture. I can explain you the architecture later. Now, basic things like textual similarity, now basic things like textual similarity how similar are two sentences you know that that is exactly how search is done but take these things guys even if you copy paste into your notebook create your own notebook understand it as a group see what else you can do with it then do this uh finding duplicates and all of these little examples that are there actually do it and then there is a where is my clustering gone somewhere in here is clustering clustering here do the clustering right it's important that you do that the other important use case guys and and and i would really love you to do it guys i'm leaving a lot of time see suppose you have english sentences and you have let's say chinese sentences right or french sentences or hindi you know that the translation of this sentence is here. But you know that not only the word orders change in languages, even the sentences are written in different orders, right? Because it's not a literal translation. There is an equivalent sentence here. I give you this sentence in English and I say, which is the equivalent sentence in Hindi or whatever language that you prefer, right? A Korean, right? Pick your language in that language, which is the equivalent sentence. It's a very powerful use case, the fact that you can do it. Do you realize that what you can do after that? You can ask a question in English and say that I know that in this corpus in other language, there is an answer there. Find me the answer there, right? what lovely things you can do with that so please today make yourself gurus of this sentence transformer thing because it's the last day we'll deal with this after this we'll move on to other important so next week i'll start with yet another big use case so this is it guys and please go do this do all of this paraphrase data duplicates etc etc do all of this clustering and semantic modeling are very very important very semantic modeling it should be sorry it should be somewhere here uh somewhere in this list you'll find I've forgotten I haven't visited this for a while where's clustering gone yeah clustering it should be somewhere near here topic modeling it's in the clustering page itself yeah because the two are related so do it guys please do it and one of you show me topic modeling on the the standard the hello world there is the 20 news groups data right do the the topic modeling on the 20 new scripts data or create a new notebook what is this journey into transformers if you recall we said that we are in the age of transformers in ai there is no doubt about it when you deal with unstructured data transformers are dominant right they are used in natural language processing they are used in computer vision they're used for all sorts of multimodal learning. The foundation of what is now being called generative AI, quite a bit. Generative AI is not just transformers, but it is increasingly transformers having a dominant influence. If you remember from the deep learning workshop, generative adversarial networks were also generative ai right or we could use some form of auto encoders also for that purpose and so on and so forth creation auto encoders but uh for a lot of things in the natural language processing world transformers do a very good job at being for the generator pi purposes and we are going to cover all of those things in due course of time. Having said that, what are transformers? We realized that at the foundation of transformers is the key idea of attention and attention heads. So we wanted to know what attention is, what self-attention is, and what the transformer architecture is. To understand that, we took a simple translation model, a translation problem as a model to learn. Suppose you want to translate text from English to French, what would you do? You imagine that you feed English into a box, and at the other end of the box, French is coming out or Hindi is coming out or some other language translation is coming out. Now, how is that possible? So one way that we looked at it is to say there is an encoder and decoder architecture. The encoder takes in the text and creates an abstract representation for it, a hidden representation that takes some space, some memory, and then the decoder decodes that abstract representation. And that abstract representation, when it gets decoded, it can get decoded into French, it can get decoded into Hindi, and so on and so forth are different languages. Now when you decode it has multiple meanings by the way. You could take a very long paragraph of text and say give me condensed version of it right that could be or just paraphrase it or those things like that. Many many things can be done as variant exercises from encoder decoder architecture. Having said that, we'll just take this simple language translation as a mind. Initially, people tried recurrent neural networks, LSTMs, GRUs, with different degrees of success. They're not all of, but then came this idea of attention. We said attention is putting different weights on the inputs based on a context that's the way we ended up formulating we gave the example that if you have when you go to a restaurant or to a cafe with your friend even though a microphone would record a cacophony of voices you are still able to listen to your friend. Why? Because your mind is able to focus much more on the sound. Your ears are able to listen preferentially to the sound from your friend, what your friend is speaking, and to some extent, de-underway the sound coming from other sources, sounds coming from other sources. So that is an example of attention. There's a context. The context is you want to listen to your friend, even though multiple inputs are coming, multiple sounds are coming. And how it does that, how you weigh your friend's sound more, voice more than all other sounds, that processes attention. voice more than all other sounds, that processes attention. How do we do that? In the simplest form, the way to think about attention is, suppose you get n tokens of input, right? Well, it could be n words coming through, or numbers there, you have to find the biggest of the number, or these are voices coming from different sources or whatever you can think of ways in the abstract, you can think of all sorts of ways. The first thing you do is you create a hidden network. I mean, a neural network whose job is to first translate it into an efficient representation and internal representation, the hidden representation or the latent representation. Once you do that, they all become the vectors H1, H2, Hk. Now, we do know that once you do that, you need a context. The context, the easiest context you can take from a group, a bunch of tokens, or all the words in a sentence, is just to take the average of the hidden states because that should say something about what is being talked about right it's a common thing but then with the hidden vector now you can start taking the so-called scoring function how much weight to put to each of the inputs or its hidden state you need a scoring function and that scoring function can start out as just the dot product of each hidden state, each token's hidden state, and the context vector, scaled by, with a square root of d, that's sort of a technical implementation detail, to prevent things from blowing up. Or you could sandwich yet another matrix in between to learn, or with matrix W, to generalize it a little bit more. But for intuitive purposes, this thing that you're looking at the alignment between each of the hidden states and the context vector to see contextually, which of the hidden states is most relevant, right? When you do that, you then can come up with an attention, how much attention the score gives you a measure of attention to pay to each of the tokens, and therefore you can create the attention vector. Now let us say that there is some one token that needs to be heavily paid attention to. So the overall attention vector will point more or less in that direction. to. So the overall attention vector will point more or less in that direction, even though it is a composite of all the other hidden states, right? Because all the other hidden states have gotten slightly de-emphasized. So that is all there is to attention. From there, we also realize that in attention, position, like, like attention not only has to be for words for every token, commerce, etc, etc, they all matter. And the position of the tokens matter. Right. For example, the classic one example is often quoted as it shoots and leaves where you put the karma determines whether you're talking about a murderer or a cute panda. So that's an example. Now what happens is in RNN you feed one word at a time. The notion of which word comes before and which comes after is well established because you feed it one word at a time. On the other hand, in attention models, you're feeding everything at one go. When you're feeding everything in one go, one question that remains is, how do you remember the position of things? So what you do is you feed in the position also using some encoding, some form of a position vector, and we won't get too much into it. You give it a position vector that goes into it. so every token becomes a word embedding its own embedding vector plus a position embedding right so both of these they go into the encoder which are made up of attention heads they break it out into key value and query pairs right query is each word is asking each word is a vector and asking which of the other words in the sentence i should pay more attention to so there's a self-attention part pay more attention to and so the the query now each every other word including this query word itself has a location in the latent space in the hidden space that location is more or less given, one intuitive way to think of the location is the key. The key is the location. So to find out, and then there is a value associated with that word. But if you want to find out how much attention to pay to this word, you see how much your query vector aligns with the key vector. You take the dot product between the query vector and the key vector so suppose you want to say the cow jumped over the moon the cow vector the cow's query vector how much attention it has to pay to the moon vector it will do a query of cow dot product with the key of the moon right and hopefully that should be the bigger value because cow and moon are the contextually relevant things here. So that is it. And then that is your attention. That's your scoring function effectively. And therefore you can create the attention vector, which will predominantly weigh the moon. The cow's attention vector will predominantly point towards the moon. That's the whole idea of the attention, KQV, in the attention is all you need paper. Now again, we have our encoder-decoder architecture. You go through the encoder and obviously in all of these, if one head is good, one attention head is good, many are even better. And then you throw in the usual feet, a few feet forward layers as good glue for good measure right so you keep doing all of those things right just to make it into a more powerful and that becomes all you know hacking figure out what is more what what other thing you can throw in to make it better and better and better you in one in the deep learning workshop if you did attend we talked about residual connections for good measure you throw that in you throw everything that you have into it because if you're going to make the perfect transformer why not right you throw all the good ideas that you have into the mix but the basic ideas are that if you get the idea of one attention head how it is done the rest of it is detailed so use that to create the hidden state hidden Hidden state you pass into the decoder. You trigger the decoder to produce the first word. You take that word and feed it back into the back end to go along with the hidden state from the encoder to trigger the next word and the next word. And you keep doing this autoregressive behavior till the translation comes out. So that's the encoder decoder architecture. That is the classic transformer mentioned in the famous paper, Attention is All You Need, from December 2017. Today, we will talk of the next big landmark that we need. I mean, there are many things that I'm sort of flying over. In came ELMO, in came GPT, and GPT has become big. We are in GPT-4 now, but we'll do the GPT architecture in a subsequent conversation and Elmo also in a subsequent talk. But today we'll focus on BERT. BERT is a neural architecture. It's a transformer architecture that gained overwhelming popularity and influence. It is amazing what a vast variety of problems today we solve using this specific transformer architecture, this particular variant called BERT. And BERT itself has its dialects. So there are Excel BERT, there is Roberta, there is Distil BERT, there is, I mean, there's a whole Bertology of it there, right? Literally a zoo of birds. But we'll try to understand the foundational paper. The foundational paper of bird, it is actually surprisingly easy to understand after attention is all you need. You will find that it is not that hard hard and in case you got somewhat traumatized with the first attention is all you need paper this one will be an easy relief it's it's very very powerful and it's amazing how they are built upon simple ideas and achieved pulled off so much with that what we said the foundational ideas that we talked about were that transformers are encoder decoder architecture. The encoder encodes the input into a latent or a hidden representation, and the decoder decodes it. That was the original transformer design in the Attention is all you need research paper of 2017. now in the previous week we talked about attention what is attention the intuitive picture of attention is again to recap i'll take a different example i took the example of friends in a coffee shop. The other example is you're walking down hiking trail, and you're enjoying the sunrise. You're looking at the trees and the flowers right your attention is going to to all that is beautiful, because that's the context you're hiking in a in the early morning but let us say you hear the sound and that sound is enough to completely change the context now survival right fight or flight is your is the context and you forget all the trees and the beautiful sunrise and you're looking everywhere for where is that what is that isn't it and it is human nature of course we are all as primates we are primed that if we notice for example a predator if you were to find a tiger or a leopard peeping through the trees we don't infer that there is a tiger and we need to worry in the blink of an eye, right? We see it and our mind responds to it. So attention is contextual. And based on the problem, at that particular moment, when you see the tiger, the sky, the flowers, everything has disappeared, isn isn't it so now let's capture this mathematically what we are saying is once we have a context once we have a context vector every input and its hidden representation let's say ith input has a hidden representation hi how much emphasis that is given essentially comes from the context and you can say the amount of attention it gets is the dot product between the hidden vector and the and and the context the hidden representation of an input and the context right and so you would presume that the tiger gets a lot of in a lot of a very high score. And at that particular moment, the scores for sky and everything else disappears. So that is the gist of attention, the two key ideas. A context determines how much attention goes to each of many inputs you give a differential amount of emphasis or attention to different inputs some some inputs dominate in terms of gathering their attention and that's why the word attention now attention is all you need is a 2017 paper again 2018 people came back from the holidays. As I said, the world had changed. Almost everyone realized that the world had changed. But we are going through those times. The world is changing so rapidly, it's hard to catch one's breath these days. We will go back and look at one of the first architectures that came, one of the first, not necessarily the first. I have skipped over Elmo and GPT. GPT and Elmo are architectures. We are going to cover those papers, but we'll do it in a sequence. Today, I'm taking a relatively simpler paper, but profoundly influential. This paper brought about the BERT architecture. It's a transformer architecture, but using it in an interesting way. And when they did it, right off the bat, historically, this paper is 2019, May 2019. Just off the bat, it beat the state of the art performance, the SOTA, for not just one problem, but practically a bag of NLP problems, it began to beat the state-of-the-art, which is a very remarkable feat. So the BERT led to many variants. There is from B well excel bird right alberta and roberta not alberta alberta is probably not there roberta uh distal bird and so on and so forth there's a whole zoo of birds bird variants right they are essentially the same architecture, but with minor variance. It's the same idea, but how many nodes, how many, you know, how big, how many of the parameters are, and so on and so forth. And obviously these are getting bigger and bigger. The first paper itself talks about a bird base, something that you could run on your machine, or and sorry guys guys you have to mute yourself um and something that is bigger right so there's bird base and bird large we are going to talk about so what does the word bird stand for it stands for bi-directional encoder representation for transformers right so we will pay attention to this word bi-directional. See what happens is that if you can pay attention, suppose you're writing a sentence. Let me try to concord a sentence like that. The big cow, which was happy after munching on, well, let's say something delicious, jumped high over the full moon. Something like this so now you realize that the happy cow which was right which had just eaten something if you just pause at that sentence and you ask where should it be paying attention to or how should it get translated there's a directionality to that sometimes what happens is people only tend to focus on the words that have been produced right and not so much for translation and the words that are coming but the reality is that language is not linear it's sort of it's a very non-linear structure sometimes the main part comes at the end and that part determines how the whole thing should have been said so one of the one of the important parts of BERT as a transformer architecture is it tends to take bi-directionality into context in the world of RNNs bi-directionality people try to create by having one RNN try to understand it, a representation, the sentence in the right sequence and the other with the words reversed. Those were other techniques that were, in hindsight, they look a little bit hacky, of course. But, you know, with transformers, the good thing is all the words in a sentence can go at the same time, right? The whole sequence can go in at the same time. And so you can do that. So doing bidirectional thinking is much easier. Now it brought in an interesting concept, which is an important word you'll hear again and again in transformer architectures, MLM, masked language model. So let's unpack that phrase. What is a language model? We already talked about a language. Your encoder-decoder is a language model. It helps you translate. We took the simple example of translating from English to Hindi or to French. How does it do it? It creates a hidden representation. That's the model. Give it a sentence that has a hidden representation. And from there, it decodes into whatever you want it to be. So what is a masked language model? It was also called Clojure, the Z, in some previous literature. They mention it. But I'll go over it. mention it but i'll go i'll go over it but what it did is when it came out in 2019 it made such a significant improvement upon the state of the art on benchmark tests that everybody woke up and said oh what is it let's understand it and we are going to do that today okay so are you all able to see my screen it stands for bi-directional so each of these words i hope make sense now bird tends to emphasize the encoder part of the transformer more than the decoder part so bi-directional i explain encoder representations what do what do what what do transformers produce encoders produce they produce a latent representation right uh representation from transformers transformers say that again please make the font bigger so can you zoom it absolutely let's zoom in more better a little more little bit it's fine okay let me zoom a little bit more for you how about now honey is it better yeah it's pretty good yeah that's pretty good now okay yeah of course one thing One thing, one nice thing about training this transformer is you trained it on unlabeled data. What do we mean by unlabeled data? You don't have to say that this sentence, let's take an example. If you want to decide, if you give it a fill in the blank, and you say fill in the blank, here's a sentence, couple of words are missing, fill in the blank, and you have to tell it the answer. Then giving the answer in the language of machine learning, you call them the labels. For historical reasons, answers are called labels, correct answers are called labels. labels for historical reasons answers are called labels correct answers are called labels so it says that you don't have to give question and their answers it can work on unlabeled data now how in the world can it act that is where the masking comes in you can take wikipedia let's say what is it it's text or the book corpus and there are many corpuses available lots of texts available you can take any sentence and then hide one of the words or a few of the words so for example you say the suppose it was the happy cow right jumped over the full moon in delight so you could erase the word delight you could erase the word happy what and jumped over water something a few of the words you can judiciously mask but of course if you mask too many words the semantics will be lost altogether it will be impossible to infer but let's say that you mask a few of the words, and then because you masked it, you already know it, know what it is. So therefore, you have in a sense created a question and answer by just masking some of the words. To do so is to use the masked language model technique. That's all it means. The masked language model is a word that keeps coming up in this space. And it looks like quite a mouthful, like scary lingo, isn't it? But that's all it means. The technique of masking some and then asking, oh, giving it to the transformer and say, tell me what was it that I'm hiding? And we do that. That's how we teach children quite often. Right? So to do that. So what we what we do is think about it this way. The basic idea is this. You take a transformer, this transformer, and just let's focus on this. You give it. So this is the intuition. You give it the sentence, the tokens, randomly go and hide a couple of the tokens. You feed it. And now you ask, what was that? What was that? So now it will say the way this as all algorithms do, what do they produce? You are classifying. Now let's map it to normal machine learning. produce? You are classifying. Now, let's map it to normal machine learning. You have a corpus of English words, let's say. Let's say that your corpus will take a kindergarten vocabulary, 100 words. Let's say you have a 100-word vocabulary because we are talking about cows jumping over the moon, right? And you ask this machine, which of these 100 words it is? What do classifiers do? They will say, we will ascribe a probability to each of the words. What is the probability that it is this word? What is the probability that it is that word? And therefore, you pick the word with the highest probability as its guess. Typical classifier. So in a sense, you're classifying you're training it to be a classifier of the master word right and that is the gist of this idea and once you train this thing so this is the training part the amazing thing is once you train it like this, it becomes capable of quite a variety of tasks. Not only can it infer mass words, but think about it. How is it inferring the mass words? It cannot unless it begins to understand the structure of the language and its semantics using the attention mechanism, which was the entire goal. And because it has understood it, now you could use it theoretically, and we can get into the details of how it does it. We can use it to do many things. We can use it to tell, for example, sentiment analysis, because if I'm giving a sentence and it gets the semantics, it can tell that classifier. i can give it two sentences and say does this sentence follow that or not entailment question right uh for example i can say it's a beautiful weather outside so i went for a walk one follows the other you say yes it follows the other and suppose i say it's a beautiful weather the the screwdriver is six inches long or whatever it is right or the hammer is three pounds heavy does one follow from the other or have any relationship you would want to give it a very low probability that the hammer has anything to do with your weather sentence. That is a problem of entailment. Does the second sentence entail or have any contextual overlap with the first sentence? You could give it as a question-answer problem. If there's a question and there's an answer, You could give it as a question-answer problem. If there's a question and there's an answer, does the answer, the second sentence, in any way reflect? Could it potentially be the answer to the first question, first sentence, assuming that it is a question? You see that, right? Yeah. So remember, so here is an interesting thing. What you do is you train it. Typically when you train a BERT model, you train it with generally at least two so-called sentences, but you remember what is a sentence in our language? Is it what the linguists call sentence? No. For a sentence is any sequence of words. Full stops are included, right? So a whole poem can be with its many real English sentences may still be called a sentence in the language of natural, in the vocabulary of natural language processing, this jargon. But basically what it means is two logical units, one sentence, another sentence. You put a separator between them. You need to put some separator. So in English, You put a separator between them. You need to put some separator. So in English, you put a separator like a full stop, exclamation mark, question mark. But you just erase that. You have an arbitrary notion of a sentence. You still need a separator. So you create a special token called the separator token. You put it there. One more thing you do is at the beginning of the sequence, you put a special token, which mysteriously is called CLS. Separate token is called SEP. Looks obvious why. But the first token is called CLS. What in the world could they be thinking about? So one of the common things you use a transformer for is for classification. And if you think about it a masking is again a classification thing so what you do actually is something very very interesting you put you put a start token for mysterious reasons let's just call it CLS and let's unravel this mystery what happens is it all all of these tokens they go through the transformer they have an encoded representation R. Vijay Mohanaraman, Ph.D.: Now what you're writing on a board. R. Vijay Mohanaraman, Ph.D.: No, i'm not writing in it actually as well, let me write it, why not. R. Vijay Mohanaraman, Ph.D.: Can I decrease the font a little bit, so I will writing space. R. Vijay Mohanaraman, Ph.D.: Of course. R. Vijay Mohanaraman, Ph.D.: Yes, okay. So here we are saying, so suppose there's a token, there's a sentence, the cow, let me make it much thinner. The, so let's look at this. The cow jumped over the moon right she moved in well i i need some little bit more space here so let me do this she moved in joy so let's convert this right so the first thing you do is you bring in two imaginary things first is you put one more guy here place. And suppose you want to consider them really different sentences. Your full stop won't do, you need to arbitrarily decide that this is the break of my sentence. And this is the separator between the two sentence. Then what you do, you add some one more thing. So this will produce its own encoding. What will it do? Again, everything, I have to make it very thin now. This will have word embedding. Remember we talked about a word embedding, token embedding. So this will have a token embedding T1. This is a CLS, right? This will have a token embedding see let's say that this this will have a token embedding a t2 this is token three actually let me not let me use the vocabulary that this paper uses i believe it calls it e1 e2 e3 e4 e5 and we'll verify whether it uses the same notation or not this this one we call it the scp right this is the cls thing then it it has a this has e six, well, E7, I suppose. This is E6, E8, whatever. It doesn't matter. E9, E10, E11, right? And of course, you can put an end of tokens, end of sentence or whatever it is. But there is a problem. We need to know each token belongs to which sentence, the first or the second, isn't it? So we add one more embedding here, which tells that this is sentence one, sentence one, sentence one, sentence one, sentence one, all the way, sentence one, sentence one. And sentence two, sentence two, sentence two, sentence two, sentence two. So far, so good, right? So this is the word. Actually, I'm calling it word embedding, but this is a little bit more. Remember I told you that running, you make it run and ing, word piece, embedding. It's a bit of a technical detail it's good to first time around gloss over the technical detail and be slightly wrong so you get the intuition first this is the sentence sentence identifying identifying and then each word has a position. Position is also important, right? So you need to remember the position embedding also. P0, P1, P2, all the way to P5 and so on and so forth. P11. So to each token are three embeddings, the word embedding or word piece embedding, the sentence embedding telling which sentence does it belong to, and the position embedding. And so the total of these three embeddings is the actual token. Are we together? You do this come again each token is a list of embeddings yeah what you do is you concatenate them you literally put them adjacent to each other to create a single embedding literally you concatenate the three vectors and to create a single embedding. Literally. Concatenate the three vectors, and you create a ginormous vector with those embeddings. And that becomes your embedding. And now we play the game. So let's play this game, right? And we'll see how it goes. So the PIPA is very straightforward. We'll understand it soon. straightforward we'll understand it soon and this is the thing that is you don't get to say that quite often in research see it says that it is a simple and powerful and the very next sentence it says something even more remarkable it obtains new new state of the art results on 11 NLP tasks. Means it beats the state of the art performance benchmarks on 11 separate NLP tasks. Pretty much just about anything you could think of, it was beating the state of the art when it came out. just about anything you could think of it's it was beating the state of the art when it came out no so attention is all you need was the classic transformer after that people went lots of things came let's just say that bert was one of the the reason i picked bert is because a good place to pick gpt i want to do afterwards GPT came before it actually and but I'll do GPT later but I picked because of its sheer simplicity it's simple and amazing right so this is it then it talks about in how many ways it did and what the prior artists see every paper is written like that introduction method results conclusion introduction one of the necessary steps in introduction is you tell what has been done and so forth and then you talk about what has happened before right so that you you contextualize your work with respect to this now the the important statement that is here is this statement. In this paper, we improve the fine-tuned base approach by proposing BERT. BERT alleviates. So remember the unidirectional problem that I mentioned? So you take it away by using a mass language model. What is mass? Exactly what I taught just now. We talked about, right? It used to also be called the close. But I think today, after Bert's paper, everybody calls it M11. Right? And it goes on. We can skip the related work and all of that. So this is it. Let's come closer to this. Now one of the lovely things about BERT is, how do we use transformers? Let's go to the two phase approach. architectures are ginormous right they contain like if you make a linear regression model with three columns three features you will have four parameters intercept and yeah the slope along the partials a derivative along each of the directions but these transform and by the time you reach here, it's a sea change. Remember ML100 we dealt with two, three, four parameters. But, off the bat, it just starts out with hundreds of millions of parameters in its model. 540 is, I believe, the number we'll encounter in a moment. There's a massive number of parameters that you tune when you have that many parameters how do you prevent overfitting to prevent overfitting you need to do a massive amount of input data to train because see here is how it goes guys just as a recollection from our deep learning course is if you look at this the model performance and the data set size of data model is trained on the classical algorithms typically let's take, will exhibit a behavior somewhat like this. They will start out with a model performance. Now, why is this so weird? Okay, now, let me take this. A typical one will exhibit a model performance like, it for yeah improve improve improve saturated right model performance saturates after a certain size why why does it saturate think about a straight line through some data once the data has figured out the best fit straight line, throwing more and more data at it is not going to create any change except very tiny perturbations of the line. Isn't it? So that's an example of a very simple model which quickly stabilizes. Now you can complicate it, you can take a polynomial model, It's a normal model, but in the beginning, the more data you get, the less overfitting will take place, because more data acts as a regularizer. It prevents overfitting because complicated models tend to overfit, but it will saturate because there is only that much it can fit to the data. After that is minor wiggles, minor perturbations of your curve. Now, you would agree that the more parameters there are in the model, the more inherently, the more data you need to give it in order to prevent overfitting. Otherwise, you have overfitting errors, right? And simpler models, their overfitting is cured with lots of data. And the error that remains, the two different kinds of errors that remain, are the irreducible error, which of course you can't get rid of, given the information, and the bias errors. suffer continue to suffer from biases now the more flexible your model is then the more you can regularize it with more and more data overfitting problem can be solved but bias problems cannot be solved right with that so what happens is that these models saturate and the model performance doesn't improve beyond a certain point but the way the neural network large models especially these transformer models do is they actually underperform let me pick a different color they actually underperform they tend to underperform and their performance well this is not a very smooth curve but imagine that this were a smooth upward moving curve what happens is that the more data you throw at it the more it keeps on improving so you say well how is that possible whatever the number of parameters is ultimately it will saturate right the the problem is at this moment we are in a world in which the neural networks are so large that we don't seem to be hitting their saturation point right you can keep throwing data at it and they keep getting better and better and well this might be a little bit of an exaggeration let me just say it like this they slow down the the improvement but they still continue to improve. Are we together? In fact, one of the things that you know, let me contextualize it with today. Oh, my goodness, you are not able to see this. Would you like to come sit here, then you can see it? Yeah, please do. Yeah, or sit next to Patrick. Thank you so much for listening. I should have seen this, but I'm wearing my reading glass. So all of you look like blurry figures at this moment. So let me contextualize it. You know that the big word these days is a chat gpt 3.5 or gpt3 and gpt4 and open ai says that oh my god it's so big we can never give it to you it wouldn't anyway fit on your machine and obviously these big models are much better than smaller models right so we know what language models are we are dealing with language models the buzzword these days is large language models implicit to large language model says only large companies can have it isn't it the googles and the facebooks and the open eis can have it in the microsoft they only can train the large language models and obviously it means big money to them because if they hold on to that then they can expose the api and we are all we are all serfs we have to pay pay the taxes just to use them yeah i see a useful no no no it is not it is not and i is not. And I'll come to that. I'm coming to that, how to use that. You can use large, the whole point is that you can just use this large language model. Yeah, the reason for that is, you never train a model from scratch. So I'm coming to that, it's transfer learning. Right? So, so when you create this large language model, and they say that to get reasonable performance, you need to feed it a lot of data. And when you feed it a lot of data, it gets smart. Right. And of course, we can only give it to you through APIs and we'll charge you for it. Right. And there's a whole problem actually. Look up the word industrial capture. What is happening is we might be, that people are speculating that we might be entering the next feudal era in which the wealth and power is getting inordinately concentrated into the hands of very few technological giants or industrial giants, and all of us will be beholden to them. Right? Well, recently there was another paper, another work that came out from Facebook. Right. And it's a much smaller model. What it showed it, it was based on this idea that you don't need such a big model that only fits into the server, into the memory or be trained on a server farm you can actually create still a large model but something that will fit on the consumer hardware right and facebook released llama right the cute animal llama and then somebody thought it would be a good joke to create a little script or a website github around it called the dalai lama to install it right so that's a npm package to install it so anyway what llama does is it still is a language model but instead of feeding it making it ginormous and then feeding data through it it's taking it just taking more data which is more readily available public data and instead of training it with just some 200 or 400 billion tokens it's trading it with a trillion tokens and showing that even small models have not saturated. The accuracy is still going up. They stopped at a trillion, and they released it. I mean, you can download Lama. And this has been quite a shake up, actually. We all are putting Lamas. And Praveen, if you are there, you already put it on your Mac Studio? Yes, I see. Yeah. It's working just fine. And I believe somebody has put it come up with the C++ version and put it on a raspberry pi right and so it all goes to show that I mean obviously first it's great innovation it was needed because everybody was getting worried what's happening right but this is the curve the performance keep improving on it again and again. So that is the thing about deep learning. What you want to do is give it lots and lots of data because we haven't reached a saturation point. Ultimately, I mean, theoretically, given a model size, if you give it, keep on scaling up the data, you will reach a point at which it has learned all it has. and the only errors that will remain is because the model is too simplistic compared to the reality and it may have some bias errors residual and of course the irreducible errors will be there right but mostly irreducible errors and so forth but we haven't reached that in fact we despite the buzzword of big data and so forth we actually are actually are a little bit starved for data in training these large language models. We want more. So that's where we are. Anyway, so that's the pretext to it. So now what is the pre-training and fine-tuning? These two words keep coming in. And it goes to the heart of the question that Mosme asked. So Mosme asked this question, I'll repeat it for people who are remote. She asked this question, I'm a small startup, I have 3,000 rows of data, not even three, maybe 100 rows of data. Or take a realistic problem, even if I'm a giant enterprise, here is a rare disease. It shows up on the MRI like this, right? But it's a rare disease. In the whole world, they're like 85 people or maybe only 12 people or 15 people. I want to train a neural net, let's say a language model to detect. Now, before we go, you say language and images, are they not opposites? By now, you must be realizing that everything can become languages. So what you do is you can actually write a very good detector for that rare disease or whatever, a rare data form still. But how do you do that? Therein comes the magic of transfer learning. So let's talk about transfer learning. Transfer learning. Transfer learning. So I will take this example, which was a real example in our class, in the last version of deep learning workshop, which is about to start, guys. This is ML 500. Before that, there's ML 400, which will start next month. After this course is over, one of the projects was to um we called it i believe the eloquent transform not eloquent what was the project kate that we said distinguish between whipping willows and pepper tree um the tree pilgrim the tree pilgrims we created a project called that tree i called it lovely trees but that was me okay so the project was and i i spent a lot of time sitting under trees actually most of my professional life i've been hiking and sitting under trees doing my work so i'm fond of trees so the problem was classify when you look at a, whether it's a pepper tree or it's a weeping willow. Now, if you have looked at them, the leaves look somewhat similar, isn't it? So how would you do that? And suppose I gave you no more than. Let's say I ask people to use their original pictures that automatically puts an upper bound, because most people, unlike patient photographers, they will take what 10 pictures of pepper trees and 10 pictures of weeping willow, right? And then say enough, isn't it? So how do you write a first class classifier that can tell the two apart? Here's what you do. You take one of these large transformers, which has been trained on all sorts of objects. It has recognized, it has a way to recognize a tree, as opposed to a cat or a truck or a house. It does a pretty good job of telling it apart. And you have trained it with the whole image net data set which is gazillions of images and it has figured out what a tree so what is it doing is semantically understanding oh this is the essence of a tree this is the essence of a cat look for the eyes right and this is what it is fish and whatnot whatnot. So a certain number of objects. And then what you do, so that is you take a model that has been trained for that problem. And then you say, I will fine tune it to my specific problem because my problem is that problem plus a delta, plus some extra learning. Are we together? Because that model is already smart. It understands what trees are, isn't it? All I need to do is teach it the subtle differences within these two species of trees. And so you can take that model, which is already trained to this. It's almost like you take a botanist or you take a person who is fully grown knows that that is a house and that's a tree but is not a botanist but you take that person through the forest and you carefully show this is this species of mushroom don't eat it it's poison and that is that species of mushroom delicious you see that right but you don't have to show millions of examples anymore because the person knows what a mushroom is you're just doing fine you know it's the subtleties that it's learning so whenever you can craft a problem as some other solved problem, plus picking up the subtleties, these transformers are extraordinarily good. And what has happened is AI has been a very open community so far. The bad news is what came last week, but so far it has been a very open community. Google, Facebook, et cetera. They have been, whenever they would train a transformer, they would contribute that transformer. They would train it on general purpose tasks, which says that there would be a vast constellation of other tasks that could be done as just refinements or fine tuning of this task, just the last mile from this task. And they would make those transformers available. Now training those transformers on that much data is a ginormous task. You have to take a server farm, lots of GPUs, train it for months, thousands and thousands of epochs. And then your network gets trained and then they would donate it right and somebody once told me that to train a transformer at full it takes the same amount of electricity while it is learning as a small city or small town or a neighbor whatever is town or smallish city, right? So just think about it. It's just that much it takes. It's, I mean, in many ways, it's an ecological disaster. So the one thing you do want to make sure is everybody is not sitting at home trading their own transformers, right? So then you give it to people and they fine tune it. Then obviously, so that's the idea. So that's the difference between, so that's the idea. So that's the difference between pre, so that the, that's why you say that the full problem is solved by pre-training. Pre-training is on the general class of problems. You train the transformer so that a lot of problems look like last mile problems. You just fine tune it. So that explains the picture that you're seeing on the screen. And if you look in the picture between pre-training and post and fine tuning, do you notice that they look uncannily similar? And that is one virtue of the BERT architecture. It is exactly the same task. And we'll see how. See, you are sending in two sentences. When you are pre-training, you're sending two sentences together and in both of them you're nuking out some of the words you're masking it and then you're saying predict what was it what did i miss right it it learns to it learns that it figures it out semantically trained now suppose you want to give it the question is this paragraph the answer to this question yes no what can you do you're again sending these two sentences in exactly the same process and all you're doing is you're asking at the end that what is it yes no true false right probability that it's the answer to this question or question answering or entailment does the second sentence have anything to do with that or am i saying the weather is beautiful and the hammer is three pounds right that sort of thing so you can do that they look similar now notice this fact that the the two special that I talked about, do you see the separator token here? The separator token here? Exactly the same. This mysterious CLS token and the CLS token, right? They all go through the system. So let's take this question. Question and the second paragraph, whether it's the answer to this or not. You have to answer yes no so what happens is that when you are training it fine tuning what will you do you'll take a lot of questions with the right answers in the fine tuning process you would feed it in and then you have to predict the probability but the probability of what of it probability has to come out of something and how does it come out in a classifier it has to go through a lot you know the final layer has to be a logistic unit or a softmax but there are so many tokens that have been produced you don't want to send all of them through the softmax so what you do is you take the this special token which is derived from what from the cls that special token that you put in the beginning. Do you notice this, guys? I'll highlight this again. Look at this line. And that is why, because there is no reason to pick any particular word and feed that through the soft token. Right? So you always reserve a special token that you add to your entire payload and you say that whatever that token becomes right after training will feed that into the softmax and ask for the probability that the answer the paragraph is the answer to the question softmax softmax is Paragraph is the answer to the question. Softmax. Softmax is, softmax, okay, you know what a logistic regression is? Logistic unit is? Think of it as that. It's just generalized to multi-class. So that is it. So you feed that in, right? Just think logistic unit, that will do. So far so good, guys? So that's the purpose of the CLS token. And now you're just classifying, is this answer or not answer? What would be a good name for a token that helps you classify? How about CLS? Classify. Right? That's why it's called the cls token right so you see the architecture of this right simple right and how do we do that this part i already talked about what does it go the input for each is let's say that you have an input classic so first input is well i i drew it bottoms up these people have drawn it top down so don't get confused this is the words these are the words where is my separator token remember two special tokens are this and this right and of course at the very end you put a separator why to say i'm done right so leave that aside okay now token embedding is just your word embedding right or rather word piece embedding to be more precise but will be sloppy and just call it word embedding right then segment embedding says what which of the two sentences it belongs to our segments it belongs first or the second a or b and the third is position embedding position embedding you remember we did we talked about it in attention is our last session you need to know this token comes where in the sentence right so all of it put together add it to get your final to get your final token embedding, right? Your final embedding. So that is what it says somewhere here. It will take, it says that it is input representation, but okay, so yeah, something about the sizes of it. By the way, this you'll find to be very easy, hopefully after this discussion. And these numbers, if you are not used to this transformers they even by neural network standards they come across as a bit shocking so look at this bird base the base model that you can easily have 110 million parameters right massive network right it has 12 attention heads right or try layer so l as a hidden the hidden size as h and the number of attention oh sorry a 12 attention heads it has so many layers transformer blocks it has so many you know hidden layers and so on and so forth so So these are big. But then when you go to Bert Large, 340. And today Bert Large would be called Kiddush, like too small. We are in billions and we are going to trillions now pretty soon by end of the year. Do we really need to go there? Well, we'll see. Now, yeah. need to go there well we'll see now yeah so this sentence is crucial we use word piece embedding so what piece means why ing running run and ing that's it colloquially think word embeddings the vocabulary they pick is actually pretty small in the beginning when they trained it just 30 000 tokens right and there's a special classifier token cls because that's what you'll use to answer whether this follows that whatever problem that you are going to solve the rest of it is and then follows that one, whatever problem that you are going to solve. The rest of it is, and then what you do is, there are certain technical details, like for example, if you use mask to train, but when you feed in the actual two sentences or two segments and ask, does this follow the other, you may say, well, you know what? You're trained on mask sentences, but you're using it for different, is it really fair or something like that? So one of the things they did, and I don't know if it was, is that sometimes what they do is they random, they will just replace it with random tokens or something like that, and ask what was it. Sometimes they leave the token unchanged and you just leave it there, dog is dog. And then you ask, okay, what was it? And it has to remember that it was a dog. Things like that. So they do all of these games. Suppose you want to do entailment. You get lots of data in which you have, how do you create an entailment problem of fine tuning? Entailment means this sentence makes sense after this sentence. How can you do it? You just take text, find lots of pairs, one followed by the other. Then for every pair that you create, you also take one sentence and take another sentence from some arbitrary place. That simply hopefully doesn't make sense. Who knows, might, but hopefully it doesn't make sense next to this sentence from the corpus, hopefully from another document. So you have to hope that this is about animals and that is about, I don't know, finance, right? So that sentence won't make sense here. And so you have positive and negative samples. You want to make sure during fine tuning that the positive samples come out high probability negative samples come out low probability how would you do it the normal classifier training cross entropy loss right you could do that so that is it right and if we get that i suppose the rest is just bragging rights. It's amazing. Look at these tasks, and at each of these tasks look at the performance of bird base compared to what existed. So at that time Gpt. Three, four were not there. The opening, the whole zoo has much better birds now and GPT itself is evolving. So you look at it. These are the different tasks. And what do you notice on these different tasks? If you look at the last line, they all represent like these two lines, compared to what was there, the differences are pretty, pretty stark. Isn't it? And in those days, people like love GPT, even in those days, and they thought GPT is really awesome. And then it went and off the bat, beated by five to 7%. Right in most tasks, which in this world this world you know most improvements are like one percent half percent little bits and here comes something that uh blows it literally blows blows it out of the water no it's the corpus it's the target space if you do a classification right so let's say you're classifying which animal it is you have to decide how many animals there are that you will, it can predict into cats, dog, horses, three, right? So in the same way, vocabulary is simply the number of the target space of words that it will predict into. So the 30,000 corpus that was tested in... You don't call it corpus. You call it the vocabulary. Corpus is each text so so it was to train with more than just so the vocabulary so what do you use you use the vocabulary by making sure that when you're feeding in sentences your sentences are from the using words from the vocabulary and not something weird like for example if you look at 30 000 vocabulary probably doesn't contain the word uh myocardial infraction so you don't use that word any sentence that contains it don't train it on it which brings us to the whole point right how do you do domain or adaptation we were talking about it the other day isn't it how do you adapt it to the medical domain, which has its own vocabulary? But the point is, if a transformer has already picked the essence of this, so then to fine tune on the medical domain, you already have a huge leg up, right? It may not be the last mile problem, but it is the last few miles problem. And that's how you would create, for example, you have the bio bird and so forth. So bio bird has the vocabulary that's more inclined to bio than- Yes, biomedical terms. That's right. So there we go, guys. The rest of it is mechanical details. You can look into it. And general thing you'll find is that in all of these papers, they follow this methodology, introduction method, right? Results, right? And results is where you do a lot of comparison, a lot of benchmarking. And then comes one important thing about this paper that I like is the ablation studies. What are ablation studies? What are ablations? See what happens is ablation means observation on removal. observation on removal right so for example if you do ablation study on people with respect to their heart usually is catastrophic right but if you do ablation study on people without the appendix ah okay are they torn cells. Right? So which is more relevant? The heart. If your goal is to keep a subject alive, you don't want to, you come to the conclusion that heart is far more relevant to being alive than an appendix is. That's an example of an ablation study, right? But of course, don't actually do it on people, but on neural networks you can do. You said that BERT is made up of all of these features. We did masking, we did this, we did so many heads. Now, the question is, what is the relative importance of each of these facets? And how would you do it? Turn off a facet and see how does it degrade your performance? Isn't it? performance, isn't it? So to do so is to do ablation studies. Now, a best practice when you're writing a paper, whether it's a research paper or your internal work, whenever you build a compound, whenever you build something that is made up of parts, the gold standard is you must have a section for ablation studies. What if I remove this feature? What will it do? What if I remove this feature? What will it do? So, for example, let's say security. You have the firewall. You have IP address whitelisting, or detection of which geographical region the traffic is coming from. Many, many means that you apply. At some point you must have a differential study that how much impact does each of the features have right and what is the cost how much is the impact if i removed it because ultimately it's a cost benefit thing you need to know the benefit because maybe to do that is very expensive and some things which benefits are low you might say okay forget about it right let's say ablation study so you have ablation studies and so forth if effect of model size will clearly bigger is better it was clear but bigger is better it also puts the burden of getting more data and more hardware to train it on and that has been the the big provide the big company's absolute dream. They couldn't be more delighted saying bigger is better. Because who can train bigger models? Only they can. So well, in this case, it was true. So this is it, guys. So now you know BERT. We can use it for classification. So suppose I ask you this question. Let's take some very simple examples. You have to tell, I give you a sentence. you have to tell whether it's a positive or negative emotion now i told you that bird has two things two segments going in two sentences going in what can you do you can null out the second sentence feed it in the the cls token put it through the class to softmax or a large it and ask what's the probability that it's a positive sentiment, right? You can do that isn't it so it will come out with the answer. So you can do sentiment analysis of it. You can do question answering. Is this the answer to this question? You can do entailments. You can do prediction of what which words are missing and so on and so forth you can think of a whole slew of tasks and i encourage you to read this paper guys now this paper should be easy reading i leave you with the homework of please read this paper right it's the beauty of this paper is its utter simplicity. Nonetheless, it uses researchy technical jargon, which I hope I have explained to you now, and you should read through it. And when it comes to, see, sometimes the hardest thing is to read the prior work, the related work. You can skim over it because, you know, that will use its own jargon and so on and so forth. How do you know the ablation study on this model? Do you know the answer to the question? No, no, no. So let's actually, let me remember. It's been a while since I read this. But ablation, so what happens is that no NSP without the next sentence prediction task. So see, you train this model, right, with a lot of data. So suppose you had not given it the task of predicting the next sentence. It means this other thing is the next sentence or not. If you had not done that data training, how much impact would be? So no NSP, it tells you the impact, right? Likewise, it tells you the impact. LTR and no LSP is training as a left-to-right language model impact, right? Likewise, it tells you the impact. LTR and no LSP is training as a left to right language model only, right? Means you do one directional training. You don't do bi-directional, right? OpenGPT. So, so on and so forth. So, you can see the effect of trying out or training it in only a particular way rather than comprehensively training it. way rather than comprehensively training it. Any questions, guys? Was it fun, by the way, to understand how BERT works? But this is the beginning, guys. We have a lot of lovely and deep architectures to do. Now, every single session, we'll do at least one architecture, transformer architecture sometimes two now i have not we have been using sentence bird we finished the topic of sentence bird we did search engines by the way i'll release this i will give you the notebook for clustering and all other things since you guys didn't take the time to do it i'll do i'll give it to you guys now but the sentence bird you are ready to understand you need to understand one more thing called siamese network right so you know what siamese twins are what are they i do they're they're attached that's right they're they're attached. That's right. They're attached and co-joined. Yeah, co-joined. So we will learn about neural networks that are crafted as Siamese string, and we'll see why. And sentence bird actually uses Siamese network. We learn sort of Siamese network-like architecture. Let me put it more like that. Now. So the biobot has been like basically trained on- Fine-tuned, yeah. Fine-tuned on purely medical corpus. And you should do that. Lawyers are training it on law corpuses and so on and so forth. It is very, very productive. And quite often you don't need like server farm class hardware just pick your little workstation worth five ten thousand dollars with a couple of 4090s or whatever it is sometimes not even that you know if like for example this uh class that we did distinguishing between pepper tree and uh weeping Willow I don't think people had to do anything beyond that couldn't be done on their very very small laptops every single laptop you know 10 data points how long will it take to fine-tune it right or 20 days to chat gpt as well or is it just inference good question so i for those of you who are remote patrick asked a good question he says when we use chat gpt and we send it prompts are we fine tuning it or is it just inference the answer to that is it is just inference chat gpt gives you the ability to fine tune it i I think you need the plus subscription. It's straightforward. You can say, here is my corpus. Learn from this also. And then answer it. So it's a very controlled way, very limited fine tuning they give you at this moment. And I'm sure that if you gave them a ton of money, you'll get more fine tuning ability. But it is what it is at this moment it's predominantly inference from the already trained model now there's a lot of backstory i'm told that um i think this praveen told me that what see there's a human in the loop it's not a single transformer. So what they do is that transformer can return something, but it can be politically charged or biased. So they have other transformers chained, which look for politically sensitive questions and they automatically get it out. Right, they get it out and they put a neutral statement out there. And then there is also a thing. I remember that the moment chat gpt came out early days i asked why is seven not a prime number and it said confidently seven is prime numbers are numbers that are divisible only by themselves and one seven is not a prime number because it's divisible by three and nine right but today you try it it doesn't it doesn fail. So what is happening is there's a human in the loop. And what I'm being told is that all the mistake of these problems, they are being shipped out to Nigerians. And the Nigerians are busy doing the further tuning and training of the chat GPT, so it's smartening up. At this moment, it's a bit of a pickle, actually, I feel bad for Google. They're amazingly progressive and open company when it comes to AI. So I'm a bit disappointed that they're barred from what I hear. Today's news was that those people who I haven't gotten beta access yet, I'm waiting for it but those guys who got it they say slightly underwhelming they need to improve upon it all right guys so we finished the topic let's take a 10 minutes break now we will go to approximate nearest neighbors okay so yeah so don't run away I know it's it's time now and it's time to go home, but I need half an hour to cover the topic of approximate nearest neighbors. Oh, yes. Okay. Can we please pause it? The topic now is approximate nearest neighbors. Instead of going into any one specific algorithm like FAS or scan or a hierarchical navigable small worlds map approach, What I will instead do is give you broad intuitions into it. And different implementations use different sort of sets of ideas from this. Are we together? So one idea is look at our problem. We have a vector space of high dimensions. What was the dimensionality we are looking at? 768 dimensions, pretty high dimension, right? And even for image embedding, we were using 512 dimensions, pretty large embedding spaces. Within these spaces, you're trying to find the nearest neighbors. You can brute force it using cosine distances and so on and so forth. Or you could say, isn't there a way to shrink the problem, simplify the problem and still get the right answers? So one story that I often tell is a kid's joke. So how do you catch an elephant? So the way is very simple. Take a lens, magnifying lens kind of thing, and look at the elephant from the wrong side of it. So now the elephant will look small. When it looks small, take a tweezer, pick it up and put it in your pocket. Right. So therein is actually a bit of wisdom. There are ways by changing your perspective, by differentiating the essence from what is what is intrinsic to what is not intrinsic in the data. You can often get most of the benefit. So one example is dimensionality reduction. When you have data in very high dimensional spaces and you could reduce it down to lower dimensions, you lose something. You realize that if you remember when we talk of principal component analysis and so forth, we talk of proportion of variance explained. But that is a big term. Simply put, it means how much did I lose? How much information did I lose by projecting data down to lower dimension? It depends upon the technique you used. For example, if you use principal component analysis, usually the easiest, but obviously not very effective many times, because what you're saying is data exists in high dimensions on a lower dimensional plane. That's a big word. What does it mean? It means, let's say that you're looking at three dimensions. And the only thing that matters is, let's say data that you're looking at is divided by a river in a plane, right? On a two dimension, like we live in a-dimensional world. Imagine there's some river flowing. Now, what is the dimensionality of the river? Forgetting the depth of the river, as far as a separator of the land, and on one side is the city, on another side is agricultural land. If your job is to tell whether you are in the agricultural land or in cities, to tell whether you are in the agricultural land or in cities then you can easily project data down to two dimensions from three dimension isn't it because the decision boundary is the river right so that's an example of dimensionality reduction right one one dimension was inherently irrelevant right But it could be more than that. Suppose in this room there is a plane so that that area and this area are separated each other out and there may be some angle to the plane. So then you can say that the decision boundary is a hyperplane. Right? may be some angle to the plane so then you can say that the decision boundary is a hyperplane right or another way to look at it is just look at a galaxy we live we live on a our solar system is a star within a spiral galaxy now we know that the spiral galaxy is three-dimensional right it has a certain well it's more a spindle shape it's like this right it's spindle shaped uh it has a certain thickness I'm not seeing the drawing that you're doing oh no I'm holding this uh okay this in my hand this is my this so there's a disc which has a certain thickness right and on this are all the all the stars including our solar system now you can say that to the first approximation i can just ignore the thickness part i can project it down onto our planes is there something else i can help with sorry let me take this out before it gets more so i could just say it's irrelevant for our problem it's irrelevant when you do that you lose something why because let us say that they're points. They are proximal to each other on this plate. They are really close, two dots. They are very close to each other on this. But let us call them A and B. And let us consider two more points, a point C, which is not proximal to it, not so close to it, but it is vertically aligned with it more or less with a so when you squish it down onto a plane now which will look to be the closest neighbor b or c c will look but c was actually further off right so by doing this reduction you made it into a smaller problem you know you're looking at the elephant through the lens and it looks smaller, but you lose something in the process. That is where the approximation comes from. So whenever you reduce dimensionality using any technique, there is a possibility of being somewhat wrong. But why? It may not matter because suppose you take enough number of neighbor neighbors 50 neighbors all it will mean is the ranking between the neighbors will be slightly off isn't it and you can then re-rank it remember the cross encoder we talked about that's something will that architecture will talk and next time we'll do it yeah that will fix the problem anyway later on so it's okay yeah so so long as you have a way to tolerate from the search you can do approximate nearest neighbor another way of thinking about it is so is to ask this question that suppose I could do this. Let's look at this neighborhood. Oh, sorry. Suppose you have data is in this plane. And they all exist as these are data points. Yes. I'm just taking an example of data points here. And what I do is, and it looks randomly distributed, so let me give it a little bit more structure somewhere so it doesn't look so randomly distributed so let me give it a little bit more structure somewhere so it doesn't look so randomly distributed so what i could do is a process of partitioning the feature space into regions so let's say that i arbitrarily say, this is my, actually, let me use another color. Let's say that this is my first partition. I have two regions. Then this partition, I further subdivide into this, and I further subdivide into this. And I can go on a few more subdivisions into this and i further subdivide into this and i can go on a few more subdivisions this and i go and do let's say this and this suppose i get a point i say i have a query and find its nearest neighbors let us say say that your query vector is here. Query point. Would it be reasonable to say that I need to search only within the region that query falls in? I may miss some points. I may miss some good points. For example, I may miss this point because i'm just searching for things within that region first if i don't get enough then i'll pop out of the region right this part by the way this approach of dividing people one common approach is called kd tree this This is a KD tree. But leaving that aside, I can look into only the sub-region the query belongs to. What have I done? I've quantized the data into regions, bended, and I only look into the sub-region where the query vector falls. I may miss some. That's all right. Am I making sense? Or if I want to be smarter I may also look into the two adjoining regions two three adjoining regions but that's about it but I don't need to look into the far off regions regions that have no boundary with this region isn't it so you have reduced the problem you have accelerated your search for the neighbors this is another way of doing nearest neighbor search there's a third technique that you could use what you do is let us say and so to to sort of motivate that let me take another structure of the whiteboard right and yes a couple of things here over the whiteboard right and a couple of things here and something like this and i ask you find the nearest neighbor what you could do is you could instead first say hey you know what let me take this data and store it as cluster one which has a centroid c1 this has a centroid C1. This has a centroid C2. It's a cluster two. Here's a cluster three, right? And then there can be many, many more clusters. I've done three, but imagine there are 100 clusters in the data. And I use clusters as a logical way to bin it. And then these outlier points, add it wherever you like. Then comes a query vector, and the query vector happens to fall here so then what can i do i can search for neighbors only within that cluster intuitively geometrically that makes sense isn't it just go find enough neighbors within that cluster and hope that the cluster is dense enough that if you're looking for five neighbors or 10 neighbors, the cluster at least has 10 members, isn't it? So that is another technique for doing it, right? Another technique that you could use is the angle technique. Let's say that, imagine this. Suppose you have, I mean, and there are many, many techniques, but we'll just take this, and I'll deliberately take just a two-dimensional plane. Suppose, imagine that it's many-dimensional, but I'll just deliberately make this. I say there is this vector, there is this vector, there is this vector, there is this vector, and maybe there is this vector, this vector, this vector, this vector, right? There are eight possible directions. So if I get a query vector, I just need to find the nearest vector. And whatever it is, I'll rotate and say it's that. So suppose you get a query vector this as a query vector right so let me name it a b c d e f g h now there are two things you can do you can just say query vector q is closest to a right so it is then you say, well, what's the point of these vectors? Because when I take data, it will spread itself everywhere. But no, you don't do that. What you do is replace every data point simply by the vector, the closest vector, right? And so suppose you get a query is close to A, you will only compare it to those points whose associated vector is a. Do you see that? And that is another dimensionality. I mean, that's another way of doing fast neighborhood. What ultimately what is your end game? You want to somehow search in a smaller pile, right? You want to make lots of piles and then search in a smaller pile right you want to make lots of piles and then search in a smaller pile it's another way of saying it is suppose I tell you that there are lots of coins there is a bad bad a quarter in there do you want to search through all the coins or would you rather that somebody already gave you the pile of quarters and you only search the pile of quarters isn't it that's what it all come boils down to right yet another technique is see points are scattered all over the place right so imagine that you have points and and you know i can go on and on and on ideas that you can have. And obviously, there's a vast amount of literature on how to do all of these things. Let's say that these are points. Here's what you do. And the points go up too far off. What you can do is deliberately take a unit circle and project all the points back into it. So this point connects, this is the point, this, this is the point, this is the point, this is the point, this is the point, this is the point, this is the point. And you can take a unit sphere of a lower dimension if you so choose. Right? There's this, there's this. Do you see what I'm doing? So just the opposite of a planetarium. In a planetarium what happens? You have a sphere and it projects light onto the ceiling right but we are doing the opposite we are saying project all the stars back onto the planetarium the big machine and they're all little dots they're little holes on the planet on the machine do we agree with Right. And then once you do that, finding nearest neighbors should be relatively easy, right? You're just looking for neighboring points. You can even divide it into little patches and just, yeah. Will be best. See, the thing is, let me answer the question without the caveat your data one question that is often asked in machine learning which is the most powerful of these methods which one should we use and therein comes the no freelance theorem no freelance theorem is a very very deep result in machine learning with a very frivolous name no freelance theorems but what it says is that no one algorithm in general will ever outperform another algorithm in fact your biggest transformer will not outperform linear regression always there will be situations where this will do better where that will do better so that applies everywhere when you have it is the data that speaks right and now what happens is it is the data that speaks, right? And now what happens is that certain domain data tend to be a certain way. And so for them, it tends to be that certain algorithms or approaches tend to work better. And the same applies for ANN approaches also. Based on which domain you are in and what's the nature of the data, different approaches to ANN will work better and it's just a corollary from the no freelance theorem and we're together and which is why you should always experiment with these things experimentation is very important and you will notice that all of these libraries they give you a lot of knobs to experiment swap one approach with another and things like that internally. If you just, they will give you a default, like for example, the way we used FES, we just didn't do any tuning. We just used it out of the box. That is just to, you know, give people a foot in the door, make it easy so that without getting lost, they can come in. But as they better understand the theoretical foundations of these things they should realize that there's a huge there are many degrees of freedom there are many knobs or many choices to make and those the choices that you make can make a vast difference on the performance are we together guys so there it is that. Those are the approaches that you can use. So one homework I'll give you is, put all of these ideas together, go check out which of these ideas and maybe I missed a few, you can share with each other on Slack, other ways of doing. And there are by the way, survey papers on ANN approaches. More and more ideas we can generate. Figure out which one Faiz uses, which one Scan uses. Scan has the word anisotropic. Hint, hint. What do they mean by that? Go check it out. Yeah. Can you also use kernel methods? Yes, very much so. Very much so. Very much so. So that is basically projecting it into a different space where things are linear, much easier to find neighbors, and then projecting it down. So you could use, for example, kernel. And by the way, UMAP, any good dimensional reduction technique like UMAP, right? And finding your neighbors from the UMAP rather than that, and kernel methods and so forth, so the variety of techniques that you can use is very high and you can imagine very easily, you can imagine new techniques and implemented it's a very fertile field. So that is how, in fact, once you get to that, then you ask, why are people, for example, today, Elasticsearch still does exact search, right? Why is it taking them so long? So anyway, this is the state of the art, guys. So today we learned where we ended up is we did two parts of the full search. We said we can do exact vector search. It's very good, comes up with good results. we can do it in a multimodal way we can cross search across images and text i left it as a homework for you to add audio guys take more and to do it right this week was a bit disappointing in the output that you guys came up with put in you'll enjoy all this then uh the second part we did is we realized that when you have a massive collection trillions of images billions of images it is not quite our documents it's not feasible to just do brute force linear search linear cosine distances or whatever is. Then we learned about approximate nearest neighbors, just bring in a few. Then the third thing I told you is, you can then use another transformer architecture called a cross encoder, and use that to further re-rank the results. Because if you get sufficiently many results, you can re-rank, and the things that were irrelevant, you can push it back and pick the best ones. It's saving, what do you do when you get, I don't know if it is still true in India, if you went,