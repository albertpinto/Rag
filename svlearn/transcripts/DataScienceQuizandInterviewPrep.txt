 All right guys, so we are going to review the solutions to the quiz. This quiz is about care nearest neighbor approach to classification and regression. So the first question was the curse of dimensionality is which of the following? The correct answer is the curse of the dimensionality. It's a curse that adversely affects neighborhood based methods. In high dimensional spaces, the neighborhood grows to too large a proportion of the feature space when searching for K nearest neighbors. So the big idea is this guys, just to recap. Suppose you have a thousand points on a one-dimensional line unit line if you put thousand points they are they look rather close to each other now suppose you go to two dimensions you realize that now you have a square unit square and they have they are not as near each other some of them if you randomly spread them uniformly spread them sort of but they would be not as close to each other isn't it now you go to third dimension a cube you realize that a thousand points are beginning to look rather comfortably settled they're not crowded in the unit cube and so as you keep on adding dimensionality, your volumetric space becomes, or your hypercube becomes rather sparse. So that in 100 dimensions, your thousand points don't look too much. They look, they have gone from being dense data in one dimensions to being very sparse data. Sparse data means what? That when it's hard to find neighbors, the neighbors are far away. Another way to argue is the way we argued, if you take a unit hypersphere from the center, how far do you have to go to capture half your neighbors or even one third of your neighbors or something? What happens is in high dimension, or even one, let's say 1% of your neighbors, if you say you'll take one percent of the neighbors to do care nearest neighbor how far do you have to go it turns out that in high dimension spaces you have to practically go almost to the edge almost to the periphery to capture enough neighbors to do your care nearest neighbor when you go that far from the point the question of neighbor the word neighbor begins to lose its meaning it gets diluted well the whole k nearest neighbor approach or neighborhood methods are based on the premise that your close neighbors are a good proxy for you they are representative of who you are right good proxy for you. They are representative of who you are. In other words, you are defined by the company you keep. So when you can't, when your neighbors are very far off, for example, if I have to find your neighbor and your neighbor is like 50 miles away or across the state border, it is not so clear that his life is representative or her life is representative of your life, right? Those neighbors can be living entirely different lives. So that is the main point. The point of neighborhood is that in the neighborhood certain characteristics you expect to remain the same and so you expect the target values to all be the same target variable value to be the same more or less and that condition no more holds true that's why you can't do that question is possible a question on that is if the data becomes sparsely on a height in a high dimensional space Does it only impact the distance methods? Does it not actually, no impact on other methods, other models? No, because if you don't use distance or the concept of neighborhood, then you're not impacted. Absolutely not impacted, okay. So, yeah, it depends on which algorithm we choose. But in general, to see generally very high dimensional data and just having enough data causes problems. It's a classic problem, for example, in genomics, etc. You have so many genes and yet you have data from like 30 patients sometimes 100 patients or so so that's a different class of problems there we have techniques to deal with them but but whatever other method may or may not work okay the neighborhood methods begin to fail okay the second question is, when, suppose you have a data set, a training set, where there are M instances of the data. And let's say you pick a K that is almost that, suppose you have 200 data instances, and you take K to be 200 or something like that, 190 or something like that, approximately equal to k right with this value of k the k and an algorithm is likely to create a predictor that exhibits what what will it have see what will happen is almost all the instances will say will be whichever point in the feature space you take most of the instances in the training set will be there point in the feature space you take, most of the instances in the training set will be there. So whatever the majority classes, if you're doing classification between cows and ducks, let's say that you have 150 cows, or not even 125 cows or 150 cows, let's say, and 50 ducks, data is slightly asymmetric. Now suppose you take K is equal to 190 irrespective of whatever the weight of the animal is weight and size of the animal is what is the majority prediction likely to be it will always be cows you know because they're just so many cows in the feature space. If you take k is equal to 190, you'll have to go far and count a lot of the cows. And those data points will all be shouting cow, cow, cow, right? And so your answer from the classifier will look like, will always be cow or almost always be cow, right? In fact, always be cow. That's the problem. That's the problem with high taking very large numbers. It comes down to the baseline classifier. Do you remember the baseline classifier or the zero hour classifier is what? It basically says the majority class wins. Isn't it't it and for regression it just begins to look like the average of the target value so how much ice cream will this person sell on the beach well it turns out just take the average of all the data points whatever average you get, that seems to be the answer always. So don't take too many neighbours, that has a bad effect. You have high bias error. Your model is very bad, the variance is very low, your answer is not changing at all much. That's that. The K in the acronym, K nearest neighbour refers to, what does it refer to? So there were some quite interesting choices, a parameter and so forth. By the way, because I'm sharing my screen, I shouldn't look at the statistics because you'll see. I think some of you tripped on this question. It's a hyperparameter of the model guys, not a parameter. KNN is not a parametric model. There are no parameters. There's nothing to learn. You just hold on to the training set, and then in the training set, you find the nearest neighbors. So parametric models are parameters to learn. This is lazy learning. There's no learning actually taking place in KNN. So it's a hyperpar hyper parameter of the model that helps you decide, I mean that you have to tune to get the best model. Next question is KNN is considered a nonparametric model because, the correct answer is, it does not build any model at training time at all but simply saves the training data. At inference time it finds the KN care nearest neighbors and their target values to draw an inference So that is that so I said that is the lazy evaluation, right? It is also a lazy evaluation. In fact, that is one of the questions I ask here. Yeah. Yeah the optimal value of K should be what? It should always be n, the total number of data points. Well, that is a bad idea, isn't it? It will lead you to the baseline classifier. It is always one, so that also is wrong. You might think that the nearest neighbor is the most representative. No, that leads to high variance overfitting and the correct answer is you can only find K like any hyper parameter in machine learning you can only establish its value by doing what a search for the best value by building models again and again for different values of K and you have to test it out in the validation set right you have to do cross validation to establish the best value for that is this clear guys any questions so far this is i'm taking it quite literally from the notes that we went through or something like that similar to that i read you it so here the decision boundary looks to you like what is it very complex very simple what is it it's very complex very complex when do you get complex overfitted decision boundaries when k is small or large small small right then you small k. So this is really listening to very small number of direct neighbors. So it is very sensitive to noise. Do you notice how it has detected all sorts of islands in the data? So that is that. Was this an approximation? Because I literally counted the neighbors. Yeah, it's just an approximation. It literally counted the neighbors yeah it's just an approximation it's a small number I mean this curve is not exactly correct it's just logically I'm trying to convey the concept I'm sure that if we take this data and we actually draw the decision boundary using the right mathematics in using code the decision boundary will look right mathematics, using code, the decision boundary will look slightly different. But yes, it is just to convey the idea. It's not not representative. Yeah, so don't don't read too much into it. It's it isn't actually, I just do a complicated decision boundary. And then I started putting blue and yellow points all around it so this is it the other question is given a training data set of about 200 points this particular suffers from what variance errors or a bias error so what is it obviously it is high variance errors right you take another sample of the data, your decision boundary will change. And it exhibits significant overfitting to the data. I hope most of you got this one right. On the other extreme, this decision boundary, let us say that this is the same data. Actually, I couldn't, I wasn't patient enough to, I should have been smart. I should have first drawn this figure, made two copies of it and then drawn two decision boundaries, but I didn't. But suppose you take this data. Here, the decision boundary, it seems to have the opposite problem. It is too straight. Maybe it needed a few bends. For example, maybe it could have bent a little bit here. Isn't it? it there quite a few blue points out here but this decision boundary did not bend at all it's more or less a straight line not a good thing so it represents it can't be K is equal to 1 is not between 1 and 3 is not 200 because if it was 200 decision boundary would be far away right all the points would be marked blue because here blue is the majority so it's probably a large number you don't know how large a number so some number greater than 10 then this problem says this kind of a model what is it more likely to have what kinds of errors is it like bias bias it is right and it is it has probably under fit the data you probably need a little bit more flexibility in the decision boundary it has under fit the data next question is in the KNN algorithm, K is equal to 1 is likely to produce what? Well, obviously by now it should be obvious that it will create a decision boundary that is somewhat like this, isn't it? It is likely to overfit the data and exhibit high variance, it will draw excessively complicated decision boundary. Next question, different values of K lead to different degrees of bias and variance. Of course, K is the hyper parameter that controls your bias variance trade-offs. straight offs. Isn't it? Now, by the way, this question was actually a very easy one. I hope most of you got right. That K in KNN doesn't stand for the number of noisy neighbors. That noisy was cute. You didn't like my high-dimensional prisoner of Azkaban? That was even better. I was trying to introduce some humor into this. Anyway, a canon is possibly, is possible only in those feature spaces that support a notion of distance. Obviously, neighborhood is ill-defined unless you have a notion of distance. Likewise, KNN is a lazy learning algorithm. No, of course, there is no learning. You just save all the data. It is instance-based learning, of course. It is a parametric model that builds a linear algorithm that builds a linear model in k parameters. So I say, what do you mean by instance-based? It means you just remember the instances, memorize and hold on to the instances. So all you do is you take the training data and save all of it. Then when you have to make a prediction for a point, you go search amongst your instances and find the nearest neighbors. Oh okay. As if in the question about distance, it doesn't necessarily mean it should be discrete values, right? As long as your data is expressed in distance. Yeah, remember that I mentioned to you those fundamental criteria of water distances. between X and X prime is always positive definite it's a positive value right zero or positive it is symmetric the distance from X to X prime is the same as distance from X prime to X and it follows the basic Schwarz inequality based condition that the district distance the geodesic from X to X prime is always shorter than any detour you take through a point C I'm just curious how that can be expressed in tabular data as if see one thing you do is once you have scaled the data quite often people try their luck with just applying Euclidean distance. Assume the data exists in RP, it's a P-dimensional space, and you just apply Euclidean distance and try your luck, see if it works. If it doesn't work, try other Minkowski measures. If they don't work, then you try to build a more complicated distance measure. But so long as you can construct a distance measure between two points. It's straightforward. And people go people go pretty fancy actually and how they define distances. If you look at the literature of distance definitions and machine learning, you'll be quite surprised at how rich the literature is you'll be quite surprised at how rich the literature is. Because as if let's say you have data that are clustered in dates, specific dates, like when a patient goes to the ER, can you express that in terms of distance? Data that's clustered, so there's a panel data kind of thing, last year's data, this year's data and so forth. The answer to that is see it's a to be able to define the distance when one is not obvious is tricky because if you can infer a good distance measure that itself is learning isn't it you have learned something from the data yes so sometimes the sometimes the purpose of machine learning is to come up with a good distance measure itself that is a machine learning task in itself what is a good distance measure and you know we we go to all of these lengths for example in uh nlp when in the next workshop that you do what is the distance between two words what defines it you can say that you can use how many characters I need to change to go from one word to another so for example you can say cat and hat are close because you change C to H and it becomes a hat well that's a very silly way you know that isn't semantically it doesn't make sense or typically things sentences that contain the word cat don't contain the word hat isn't it? So unless it's a mad cat wearing a hat. So then what is a good meaning? How do you define distance between words? So one of the nicer things you can do is look at how often they co-occur in documents and close to each other. So things like word to weck and glove, they create embeddings. These embeddings are, you create a latent space a hidden space the embedding space in which you project these words you learn to project these words as vectors and when you do that but then there's somehow the semantic richness of the relationship between words is preserved. For example, one of the surprising and pleasant surprises that emerged when people did these embeddings was that they could write an equation that said that if you take a king, subtract man from it, the vector man from it, add the vector for woman to it, it will be surprisingly close or be approximately the vector for queen. So you could write beautiful identities like King minus woman plus man is queen or people began to notice something very interesting. They began to notice that if you take literature, you say United States and Washington DC. If you look at the distance between us.S. and Washington, D.C. words, I mean, this sort of phrases, and you look at the distance between, let's say, Philippines and Manila and or India and Delhi, you'll be very pleasantly surprised the distances are almost exactly the same, approximately the same. So it means that suppose I want to find the capital of a new country that I don't know the capital of let's say I want to find the capital of Germany all I need to do what do I need to do I can say that Germany minus its capital C is equal to let's say a US minus Washington DC and therefore the capital that see the capital of Germany is equal to the capital of Germany plus Washington DC minus US and hopefully Berlin pops up is Berlin the capital of Germany I hope it is it is it is right I don't know after the reunification first we were taught when I was young we were taught that world Berlin is the capital of West Germany and born is the capital of East Germany so now Berlin is the capital of both okay so things like that and so so those are word embeddings and you have a notion of distance now. So that is the beautiful world, you know, this literature of what is distance embeds itself into questions like what is the real space in which you want to submerge this data, you know, you want to put the embed this data. What's the real vector space and things like that. It gets quite interesting actually So the couple of things I can do guys, KNN it turns out in kernel KNN are just the tip of the iceberg. I didn't teach you a lot of other neighborhood methods. For example, you can do a lot of it. For example, TSN, UMAP, and so on and so forth, manifold learning and so forth. There is a rich field that I didn't teach you and I won't get time probably to teach you in the deep learning workshops either. There are sort of things out of the scope of any of the formal workshops. I'll probably hold the extra sessions, the seminar sessions or things like that. And in that we'll cover those. So keep attending the seminars on the Sundays, keep watching out. Or sometimes I may just create an ad hoc workshop, one day workshop to cover just a small bag of topics if you're interested, join in. We'll nominally price it or make it free whatever it is all right guys the rest of the questions have to do with regularization it was just supposed to be a review I'll stop sharing my screen and see what this Субтитры подогнал «Симон»!