 So as a reminder on this class portal, the workshop portal, you'll find a wealth of material and I'm continuously updating it. A few things to pay attention to, quizzes, when the quizzes are released, please do take those quizzes. They are an integral part of learning. Then we have the lab code, you know, the zip files, the download those archives and use them or upload it to your cloud, Google Drive or wherever it is that you want to do. That is another thing to do. The third is the surveys. Do please take the surveys. Surveys is how I know whether you have understood something or not and what topics to repeat whatever people have not understood i'll be happy to repeat in the weekend those specific topics and come to that so for example at this moment the month the tuesday session is here if you were to click on this, it will take you to the YouTube recording. Likewise, the quiz and their discussions are here. Today, I uploaded the week two lab and homework. So that's all I have for the administrative stuff. Now, I would like to start with a bit of good news. Most of you have heard of Geeta Alluri, she was a participant with us last year. And she got first into Walmart research group, Walmart labs as a senior scientist. And now she's gotten an offer from Amazon. scientist and now she's gotten an offer from Amazon and one of the things she mentioned which I feel quite proud of is that she said that the Amazon interview had a lot of theoretical questions deep in-depth questions and because of what she did in the workshops she could do very well so i suppose um that is good for me something is working and for you it also means that try to review stuff it will actually come in handy the theory part also not just the lab part it isn't enough just to know how to code it is important to know the foundations whenever you go and interview with some of the big guys, they'll always grill you on the foundations. So it is important to know the foundations. And so we are ready to start. Any questions guys before we start? Anybody has a thought? Asit? Yes. The homework, the tarfoil that you're uploading, is it cumulative? Does it include last? Oh yes, of course. It's cumulative. I see that it creates the same folder set again. It will always be cumulative. So that project will keep growing and I will be, now I'll be giving this week some tangible things to do in coding and so next week I'll release the solutions to those any other questions guys alright guys if you don't have any other questions then I would like to share my writing board and let's start here. Last week, sorry on Monday, just to recap, we talked about the fact that neural networks are universal approximators. networks are universal approximators. They can adapt to any function or any reasonable function, one would say, very effectively and they approximate it well. It doesn't have to be exact but they are good approximations to it. You can in fact do that with just a single layer in the neural network. One single hidden layer. That's a remarkable truth. Now the things that improved in that theorem, the versions of the theorem, one proved, the earliest version said you need the logistic function. It said you need the sigmoid function. The work after that showed that it is not a property of the activation function, but it's a property of the network architecture itself if you have a hidden layer then your neural network will be able to approximate any function and lastly then we live in the world of deep neural networks you'll see that we don't have one hidden layers we have many hidden layers in, we have nowadays a shocking amount of hidden layers sitting in neural networks. These neural network models are extremely large. So there is a version of the universal approximation theory that says that it works with neural networks also, which is sort of the people think of it as the transpose one. So you have a choice. Either you have one hidden layer which has a huge number of nodes or you flatten it out and you have many, many layers with lesser number of nodes per layer. It still works. So then the question comes, how does a neural network solve a problem what are the core ideas behind solving so towards that and just today i'm not focused too much on the theory i'll give you guys the big ideas because we want to do the lab today so this just as a review So go back to linear regression. If you remember in linear regression, we had, let's say that you have points. Let's take some points. Are these points visible? So suppose you have points like this and a few. And you try to fit a line to this. Within some coordinate system. So this is x and this is Y. This is a recapitulation regression is remember in this particular case you're going from R to R. Right? So this is your line line that you're searching for. Now, whether it's a line or whatever it is one way to quantify how good your model is is to look how well this line fits to the data and the way you do that is you ask if at a given value of x let's say i'll make the x y axis at a given value of x let's say this value the reality was this And what was the prediction? This was the prediction. So this is the y, this is the y hat for this particular value of x. So the mistake that you made, the gap or the error for this point, if we call it xi, so this is yi and YI hat. So you said that the error that you have that is often written as a epsilon I and it is also written as a RI. So you can use whichever notation you like. It is equal to YI, the true value, minus YI hat. So this is a recap, guys. I hope this is obvious that this is the amount of mistakes. This line, if it is the model at this value of Xi, it will say this is the value of Y. So this is your Y hat. But the reality is this is the data, training data. And so you know the mistake. Now, how do we learn when the machine learning learns it learns by reducing error. We talked about the process of learning. Learning is reduction of error. More formally, what you what you talk about is the loss function loss function which contains the sum total of all errors and you throw in a few extras that you throw in things like regularization and so forth and almost all of you have done those things with me in your prior labs in your ml 100 and 200 so you're familiar with it so without further ado i'll just mention that the loss function for regression what is it would somebody like to say it is made up of it is essentially the sum of all these errors, epsilon i's, squared. Some squared error. That is why people also say some squared errors. So this is one. Now what people do, and depending upon the literature you're reading, what people do is sometimes they say, let's take the average over the number of points so suppose the total number of data points are n you take the average now you get a mean square data right so whether you take the average or not doesn't matter because after all this is a this particular thing one over n is a constant given the data it's a constant so if you try to minimize the loss, you will end up focusing on this part. Now, what is this part made up of? This is a recap. This part is made up of yi minus yi hat squared, and that is equal to, so now the question, now comes the specific thing for regression what is your model right you may have some equation for the model and that will determine what your gap is some parametric model so i'll just write write this word some parametric model and i won't i will sort of avoid the temptation of writing the equation for linear regression because today we are going to get into deep neural networks and the number of parameters are huge. So we don't want to fall into the trap that we are thinking linear regression when it is a bit more complicated. But this is not enough. While this particular is the sum squared term, there is more to it. There is a regularization term. Would somebody like to give a quick, a ten-second summary of what is regularization? Any volunteers? a 10 second summary of what is regularization any volunteers it's how you dampen the dampen the effects that can cause um that's right excessive variance excessive variance that's right so what happens is when you over fit to a model that your variance goes up in other words you start seeing a lot of undulations or bumps or oscillations in the in the function that you create and so one way to tame it one way to improve the situation is that you add a regularization term for example the l1 regularization is uh adds a term it basically dampens the weight so suppose the weights are w right i will just write the weight vector the set of weights as w it will do it by saying there's a penalty term if you make the weights too big then your loss will increase you need to keep your weight a weight small enough the l2 version of regularization and i'll talk about this l1 and l2 are in case you have forgotten uh i think when i look at this audience all of you know i can see almost everyone knows what l1 and l2 are in case you have forgotten uh i think when i look at this audience all of you know i can see almost everyone knows what l1 and l2 is could somebody tell me the more popular names for l1 and l2 l1 is classic one is lasso other is rich and then there's a combination of both yes which is that uh elastic net excellent lasso bridge and elastic it's a combination of l1 and l2 and so its effect is essentially that now there's a bit of beautiful geometry associated with it which has to do with min korsky distance and so forth we won't get into that today so we will take this as a milestone result for today let's say that I'll just write it in bold here this is one of our milestone terms so now what do we do with the loss function? We try to reduce the loss. And the more we reduce the loss, the more our line or hypothesis surface, now here it's a line in two dimensions, three dimensions, it would be a plane if it was a linear regression, but you can have a regression in which it is not necessarily a line, It's some curve or some surface. Last time we saw that the universal approximator property was fitting to all sorts of data, all sorts of functions. So each of these functions is a curve and in higher dimensions, it's a hyper surface. So now what are you trying to do? You're trying to essentially your goal is this. Goal is to, let me take a little blunter. Goal is find, is happening find the hyper surface the surface that is best fit or closest to the data to the data set so here we see that the white line is closest to at closest to the green points. Considering that the green points are sort of scattered, there are error terms here, noise here built in. So we can never have a perfect fit to all the points unless you have a very, very complex model and which intuitively we know is wrong because that is overfitting. So this is our goal. This is what we try to do. Find the hypothesis surface that is best fit to the dataset. And that goal is the same as finding this goal is the same those weights. And so that is why these are called W star. Find those weights or the parameters of the model that will minimize the loss, the regression loss here. Are we together? And you think of the regression loss as built upon these parameters. Remember we talked of the rheostat model last time. At the end of it, what can we control? Just move the slider of the rheostat back and forth. Data is given. We can't change the data. So when we build a model, either we can change our hypothesis, but if you have a hypothesis, let's say a linear hypothesis of a surface, then all we can do is tilt the surface, right? One way or the other. And that is the same as changing the parameters or the slopes of that hyperplane, isn't it? So that is it. So you say that W star, the optimal optimal and so you'll see me use this notation whenever you see star associated with parameters by the way uh just about a parameter we used to in the prior courses use beta as parameters but in the neural network world it is much more common to use use the word weights interchangeably with parameters and w is much more used little w and then capital w for the matrix and so the optimal value we will represent with a star and this is our problem we want to find that value of the parameter vector of the weight vector essentially that minimizes the loss the question then comes how do we minimize it and so we will go into that part right away now so this is part one of the journey right so let me mention this this is one part one part you are, right? So let me mention this. This is one part. One part. You are trying to minimize the loss. So guys, this notation looks cryptic. I hope it is, you're understanding it. You're saying that arc means, suppose this is the input. This is the output. Loss is the output. If you look at this equation based on the values of the parameter, you'll get a certain loss value right loss. Vaidhyanathan Ramamurthy, Which is a function of the parameters. So you can keep changing your hyper planes or your surfaces and for each of these, they would be a different loss value loss amount amount, you are searching for that argument of weights. In other words, that hypersurface that minimizes the loss. That is all that this succinct notation says. It says arg min. Arg min here means W. So it says those values of the weights that minimize the loss function are the weights we are searching for. W star are the weights that minimize the loss. That's all that this statement says. It's a mathematical notation and we'll use that. So now the question comes, that is fine. We know what our goal is, but how do we get to that goal? We still haven't found a way to get to the goal. Now towards this, I will give you guys a review of what we have covered in the past but this is uh for those just in case you forgot there is the process that we use is called gradient descent and why it is called gradient descent you will see in a moment So, you realize that your hypothesis leads to weights, each of them has weights, and it leads to a loss function. Let's see what this loss function looks like in a moment in a higher dimension space. And on our website, we have a wonderful section, we have have a section there are people who have studied this loss surface the loss function very deeply and created beautiful visualizations for it we'll cover we'll go into that time permitting otherwise we'll do it on monday today is a lab section so i will bring about just enough knowledge to get us there so what is gradient descent imagine that you have a function something like this this is your x this is your fx y y or there's some function of x are we together now there is a concept of slope just as a basic review of what a slope is. What is the slope of a function? What is the slope of x? Would somebody remember what is slope? Like the derivative? Incline. What is the incline or slope at a given point x let's say that i have this particular point right let me just call this point x a and this is the x-axis yeah what is the slope would somebody say or the incline so the rate at which x is changing as you move along the x-axis exactly so the unit and the rate is that the unit change the unit increase in Y when we move unit distance so you move a unit distance along x axis. So you go from here to a unit distance would be, let's say Delta x and you are here. So what happens is you reach here. Now what this was your y at a this is your y at a plus delta x little bit it's a function of a different point here this point is x a a or sort of x well i a plus delta x this point right and so at this point you have this now how much increase did we have of y oh look at this this is the jump in y isn't it and it is therefore slope here here is and so let us say Delta X is equal to some some form of one unit you're a little ant and you walk one unit along the x-axis slope here is simply why a plus Delta x at the new location and let me just say the unit one if you want to think of it as one here minus y so if delta so for that for delta x is equal to one unit it's the unit increase in your how much did you increase so not the unit increase in your, how much did you increase? So not the unit increase, it is the increase in Y that you have. So this is your delta Y, the increase in Y for just a little bit of moving forward with it. And now what happens is that people have different notions of units and this delta X, which we think of it as a unit movement is not what you would say let's say that this is given in miles so you don't think of delta x to be one mile that is too much those of you who have been hiking in the hills you know that the slope changes vastly over a mile. So when you talk of unit distance, you mean a very small amount. So if you're measuring in terms of mile, you would say, well, 0.001 mile or something like that. So people in this space, they say, make sure that your notion of unit distance is that of an ant, a very small amount. And so if you're using a much bigger yardstick, you have to declare slope to be delta y, the change that happened. And in terms of your unit, your yardstick what was one unit movement you know we don't want to walk a mile we want to walk a tiny amount it's a 0.0001 mile and we want to see how much we moved up maybe one yard or maybe we just move a little bit up a hill and we see how much uh how much altitude we gained with that. That's your slope. So that's the basic notion of slope. It's the increase. Now, this is obviously a refresher. But I'll just ask this question. At A, is the slope positive or negative? Positive. Positive, right? So the way that people say, so what happens is that there's a notation for slope i said this delta y delta x so people's notions of deltas can be big and small some people may think of delta as a mile and that's unacceptable so mathematicians have come up with the notion of delta they say that slope is not any unit of Delta X, but it is really, so this is approximately, let's say, the exact definition of slope is that you take this limit of Delta X tends to zero. In other words, what is the tiniest little increase that you can make? The tiniest possible step that you Anshul Akhtar Wadhwa The smallest possible step that you can take and you go and measure how much did you gain in altitude and they refer to it by a notation dy or dx where dx the d of x says the smallest possible increase of x which is greater than zero right people often use the word I don't want to use the word epsilon because epsilon in machine learning we often use for error so epsilon mathematicians have an interesting concept and they define it in a sort of a very odd way they say epsilon is a number greater than zero well Well, that is fine. So what is it? Well, it's also a number smaller than any number that you can think of, positive number you can think of. Now that becomes problematic. Suppose you thought 0.000000001. Well, they will say, well, epsilon is smaller than that. And you can keep going that way, but it is still greater than zero that's how we think about it it's sort of a logical concept not something that exists in real world but it sort of clarifies the definition of a slope in an exact way so that two people don't disagree with each other so that is the slope now slope here will say'll say that dy dx is at a is greater than zero. Isn't it, guys? This not B, this is not B. At point B is here, B. And if you move at B a delta x step so that you go here and again here, delta x so which is bigger have you increased in height in altitude or decreased in altitude nice decrease decrease and so the slope you say is negative because you did not gain. Slope is always the gain in altitude. So well, you gained a negative amount in altitude. So you would say at B, this is true. Make sense? And now we ask this question, suppose you have to find the minimum point on this function, of this function, and you are a little ant and quite literally imagine, so by the way, think of this as something very real. Think of it as a peculiar children's slide. They slide from here and they can go all the way there and the other way around or you can slide a marble and it will roll back and forth. So now think about it this way. Suppose your goal is to find the lowest point of this function, of this slide. Now we ask ourselves, how do we get there? you can say wait quickly that well it's very easy to get there because we all know that at the bottom it's flat if you see it is flat the slope is you're neither gaining altitude nor losing altitude you are basically in the bottom of a bowl are we making sense guys Are we making sense guys? So you can say that at minima and I'll write it here. Some things at minima. Slope is dy dx is equal to zero. And given life's imperfections and data data you may say you may modify this and say it's approximately zero and that will give you the minimum right so now the thing is well why if you have to find this you could just solve for this equation and do that but it turns out that you can't actually in real life because this curve that I showed and now I can make it into a surface by spinning it around on its axis, then it will look like a bowl, a parabolic bowl. But so, and then you can generalize to higher dimensions from there. But look at this. This is something people call a convex surface what's a convex surface um something that has like so first let's define something convex convex surface in this case a convex curve. You say that a curve is convex if you take any two points, any two arbitrary points, let's take this and this. Let me call this P and Q. Then the line joining PQ is such, it's called the segment, that the curve that is there between P and Q will always fall on one side of it. So you can take any two P and Q and you will find that the curve never intersected. The function never intersects any of its segments. So, I see this property is only for a curve with one bend. Yes, or surfaces with just one bend. Yes, that is true. And that is what, intuitively intuitively that's what convex means right that achieves the same result as this i pick any two arbitrary points on the function so i picked p and q then the curve the part of the function that is between p and q so which part is there it is this part this part isn't it and then which part is the segment joining the two this part is the segment of sorry yellow this part is the something fishy here is my okay I'll just take a different color this part is there is the segment isn't it and this is the part of the curve. Between points P and Q. So do you notice that the curve, the pink does not go, I wish I'd chosen a different color, pink does not intersect with the red. Is that clear? Let me illustrate this with a counter example. So, look at this curve now. Let us ask whether this statement is always true. If I take these two points, A, B, then it is true. This curve is not intersecting A, B, isn't it? But likewise, I can say between this C and D, it is not intersecting. But the moment I look at this, so C and D is fine. But what about, what happens if I take a different point. I take, sorry, you guys give me a moment. Yes. So suppose I joined these two. CB. Now look at the segment CB. You are able to find one segment and you'll be in fact able to find many segments. Now, do you see the curve intersecting the segment? Guys? Do you see the curve crossing over the segment? Yes, sir. Yes. Yes, yes, it is in fact oops oops. Two points. So this is not convex. So I'll just put it like that, that this is not a convex Curve. Right. So the intuitive idea for convex curve is it has like a pretty good I believe said that it has, like Premjit, I believe, said that it has essentially one noticeable bend. So when you have one minima, one bend, only one bend, it is easy to find a minima, especially if it is looking like this. A trouble is when you have a non-convex surface life gets a little bit harder because there are so many points at which derivatives are zero uh this is bad this is bad this is here there are many things and not only that computing computing this derivatives directly and setting it to zero is computationally quite brutal. It sort of involves, this is a numerical aspect, it involves matrix inversions and they are bad for many reasons. They are quadratic in the number of instance and the feature sets and so forth. They're slow, they're prone to failing and many other things. so people in numerical computing they do sort of what is called pseudo inverses they try their best not to get into situations where they invert matrices and in practical terms when you have vast amounts of data it's not it's not generally a good idea so there is a better idea of how to find this point, this minimum point, how do you find that? And that trick is something quite interesting. I will ask you to note one thing. Suppose you are here, you are an ant here. So how does an ant look? Is that an ant in a flake? So suppose this ant is here at B and wants to go home. This is home. So ant is saying the same thing that Dorothy says in the Wizard of the Forest. No place like home. So she has to go home. So how does she go? From this point, in which direction should she go? In the direction? In this direction, right? She should make a journey like this. Does this make sense, guys? Yes. The only way you can come home is this. And what it means is you make little steps along this direction, this so that you end up here. You want to move in this direction. So now let us see what it is. But you don't want to keep moving. You know that at some point you have to stop when you come to a point where the slope vanishes right so you want to take a step based on how much the slope is if it is steep you tumble down more if it is not so steep you begin to now go slower and slower because you are closing getting closer to home very much like your car you don't drive at full speed towards your home, you slow down. So then if you look at that, what you're doing is you're saying that my new position is the slope, take us based on this load is proportional to the slope. And you take a unit, you take a little step, Delta X. So i will just simplify the step delta x in our context i will call it and there's a fancy word people use in this literature they call it the learning rate how big a step you are willing to take learning rate why it is called the learning rate we will learn in a while but this is your delta x and it has it has usually it is denoted by the symbol alpha so i'll write it up learning rate delta x here is learning rate usually given as alpha this is the notation that people often use in this space. So alpha. So we know that if you take the slope and the product of these two, and this is proportionate to here, then the product of this would be a good way to move, decide how, like you keep taking alpha step, but you multiply by the slope because when the slope is high, you want to take a bigger step. When you start coming close to home, the slope is small. You want to take a smaller step. Otherwise, what will happen is you will know how to stop. You'll keep moving forward in the other direction. So by multiplying it by slope, you're modulating yourself down so that you slow down and you reach home. But now the question is is I am here at B to go to this point I am how do I go so what should be my B next B let me call this B next B next is equal to what now think about it it is your previous B you are here B now what do we do let's look at this we know that we want to make a step like this of size this but slope though here is negative slope slope is negative so we don't want to subtract this quantity from this in other words we don't want to move in this direction so how do i go in the forward direction if the slope times alpha alpha assume is always positive and one easy way of0 like alpha is 0.001 tiny steps that you that an ant is taking in terms of your yardstick and it is always fixed let's say that for now it's always fixed so when you multiply by slope you end up with a negative number but you want to go b next relative to b is it in the is it greater than b or less than b in this situation guys this is b and b next look at this is it greater than b or less than b on the x-axis greater is greater so to go to b next surely I have to add a positive quantity. But I have a problem. Slope times alpha is negative. So how do I make a negative quantity negative? Well, it's very easy. You multiply it by minus one, isn't it? So suppose I did this slope times alpha. What about this now? Let's look at this equation. The next value of B is B minus the slope of times alpha, this factor, modulated step that I take. Now, minus will do what? What will this minus do? It will, would you agree that it will make the, make you move forward in the positive direction? Isn't it, guys? Yeah. Yes, sir. Yeah. you agree that it will make the make you move forward in the positive direction isn't it guys yeah yes sir yeah and now likewise let us look at what a life looks like if we started out in uh at this point okay so suppose you are at a l color a right is this green coming across or you should use some other color oh sorry i will just be a little bit more careful suppose this is a right and you you want to go actually let me just do it this way let me leave a here Let me leave A here. Suppose you are here. Which direction do you want to make a step in? You want to go to go home. You want to go like this, right? Do you agree that you want to go like this? You want to tumble down this hill. So in order to tumble down this hill, you need to make your next A. let me just call it A next. Let's think about it. A next is, let me take a thicker one, A next. What do you expect it to be? Your A next here, do you expect it to be bigger or smaller than A? Smaller. So let's say what do we do? A. But now look here. What is the slope at this point? Positive. Positive. So what will be positive so you want to subtract slope times alpha isn't it if at a you want to do this because slope is positive your modulated step to make it backward you again want to subtract from it right sir i have a question slope you said is dy dx and then the learning rate is dx so when we are learning rate is yeah learning rate is the tiny alpha yes go ahead isn't it dx uh see when you say dy dx no you think of uh well i think okay all right let us say that it is see dx when i see delta is different from d d remember is very very small one very easy way of thinking very small in fact this notation this is an abuse of notation what you mean is if you it is basically limit it is not dy divided by dx so it is not that this is actually a quantity and this is actually a quantity. People think of it like that. And there is a guy mathematician named Leibniz and he said, if you do that, well, it turns out that it's legal. You don't get into trouble. Things do make sense. But remember delta and d are different. Delta is a reasonable step, but d is the tiniest possible step you can imagine. So our alpha is not D because D is so small that it is billion times smaller than alpha. Got it. So they will not cancel out each other. They will not cancel each other. See, think of it, slope is the tilt, right? Is the incline and based on the incline, if the incline is high, you want to take a bigger step. Like basically your total step, suppose your legs, so think of alpha as the length of your legs. So you know that when you are really steep and you take a step forward, what happens? You end up taking a much bigger step in a hill, downhill, than when you're walking on a flat bigger step in a hill downhill but then when you're walking on a flat surface think of it like that the slope times alpha becomes what you actually come down when you are on a hill so now guys notice that both yellow and green these equations do they look alike whether it is in terms of a or B do these equations look alike guys yes sir yes and so you make a rule from that you generalize and you realize that the rule is that on this curve so let us write down the big rule we discovered i'll write it in one new color here rule that we discovered, rule. For the ant to go home, home, all they has to do that wherever the ant is, let's say that the ant is at X, right? Any arbitrary point x, the x next has to be x minus the current value of x minus alpha, well, slope times alpha. Let me write it this way, because times alpha. Does this make sense, guys? So this rule, what does it help us do? It helps us come down a hill. And it has a very simple intuition. Now think of a two-dimensional surface. Now this will look like the hills around your house. In Bay Area, we have a lot of hills and we go hiking. So if you want to come down the hill fastest, what do you do? You take the path of the So, if you want to come down the hill fastest, what do you do? You take the path of the steepest gradient. Wherever the slope is maximum, you take that path. And that is what this rule is also saying. It is saying that in a two-dimensional, in a simple curve, there is only one direction that you can come back, tumble down. But when you have more dimensions a surface you take that part the steepest part think of the slope being the place where you come down most and the generalization of the word slope to higher dimension is gradient so remember so gradient the word gradient and I won't go into the mathematics we have covered it many times in a past workshop, and I'll again cover it on a Saturday. Gradient. The gradient of X is generalization. generalized to multivariate higher dimensions. Higher. Well, that is a long discussion, but now let me come to the meat of the matter so that we can get started with that. So now what happens is, how does it relate, all of this discussion relate to the loss function? Let's get back to our loss function. Our loss function is a function of the weights. So it exists in this hypothesis of the parameter space of W0, W1, et cetera. Let me just say and assume that there's W2, et cetera. And this is the loss on the weight. And what happens is that in this surface, if it was very simple, then you could have a surface like this. But in general, convexity is not preserved. In neural networks, you don't have convexity. The surfaces are more like this, curvy, with many, many minimas and maximas, local minimas and maximas. But in any case, a path of gradient descent helps you find the best value. Your W star is sitting right here right and if you look at these projected down you see these contour maps and we'll play with that in a subsequent job that will be an extra session because we need to move forward and I'll show you how you can draw of this contour maps and see a gradient descent happening. So this thing we do, we do a gradient descent on the loss function and that essentially summarizes it guys. If you ask yourself in predictive modeling that is regression and classification what is learning learning for predictive models is great is nothing but a gradient descent on the loss surface that's a very powerful statement guys and I want you to think over it if you don't understand it we'll cover it again on Saturday and we'll clarify it at this moment i hope you get some flavor of it we do that now this leaves open one question that when you find this alpha dot gradient how do we find the gradient how many points do we take to find the gradient of this uh this of this loss function because remember loss function here says let us recap what the loss function said the loss function said or and what we are searching for is our men w star basically. That's your W star. Vaidhyanathan Ramamurthy, And I'm bringing back memories of what we did a while ago argument loss of them view, which is the same as this. Remember this is equal to zero to the number of points. Vaidhyanathan Ramamurthy, And this is your y minus y i minus y i hat in whatever form you write it plus regularization terms so now pay attention to this guy n how do we this you realize that if you if you find the gradient of l this n is implicit you can find the gradient but it depends upon how many points you have taken into this loss function. The choices are there many choices actually and that those are the three choices we'll talk about. Suppose you have. Okay, sorry. Let me try it out a little bit. So let us say that your data set is I'll put it linearly there like this is your data set and it is made up of samples think of it as a nice bread that we have sliced into slices and for some of you it is morning time zone so you must be getting hungry for bread and breakfast so it it's like that. Now, each point is a datum, is one instance of data. So while learning, when I'm doing gradient descent, I can take my n to be one, just one data point. I'm saying, find the gradient, find the slope with respect to just one data point and then take a step What is a step when we do this part? W next Is equal to the the current value of W minus alpha times and now I will write the slope in a more You know this this peculiar inverted triangle notation of loss. If you have difficulty remembering the inverted triangle, I often say that think of the pyramid and you can make the pyramid more useful by turning it on its head. You get the gradient. I'm joking, of course. So here we go. This is your step, one step. So in that one step, you can take n is equal to one. That is when you do that, it is called stochastic gradient descent. It's a name at this moment, just we won't get into all the details of it, but just say think of it as just a name of this fancy way of doing things. If you take n is equal to t it is called batch gradient descent and the name is sort of unfortunate. It basically says that I suppose it may at some point have referred to the fact that you're batching all the data into a single batch, you're clubbing them all into a single batch and then computing your gradient. This is your batch gradient descent. So that is taking all of it, all t of this. Let me just say t instances, all t instances. And then there is a third possibility that you have, which is that you take a number that is somewhere in between. Let me call this little n, and it is between one and t usually n is much less than t in other words much smaller than t but greater than one T but greater than one. Because if n is equal to one, you get back to stochastic gradient descent. So when this n is equal to this n and values of n are typically 2, 4, people tend to just by convention pick in powers of 8, 16, 32, 64, 128 and so forth. Pick one. Something like this. So then what happens is one step is learning from one N. So it is learning from a one. And so it is learning from a batch, one batch. One, let me write it. N items, N items makes, the technical word is actually mini batch. It's a mini batch because you know batches unfortunately got reserved for the entire thing. And so you'll see very confusing terminology in the library. So I want to clarify it. It's a mini batch and items makes a mini batch. So how many mini batches, how many total number of mini batches there will be p by n yes t over n and so how many steps you will take by the time you run through the data running through the data through let me just use the shorter form of this, through the data. Data once, while learning, you say that you have completed, you use a word, you say you have completing an epoch. Are we together? So now, one epoch in mini batch will have a key over n. So steps, because you're going to, there's so many mini batches there. Sorry, I want to change this. I'll keep the scrolling. Since there are, so for mini batch gradient descent, batch gradient descent number of steps is equal to the number for mini batches and that is equal to P over N. In the case of stochastic gradient descent, what is the number of steps per epoch? T. T itself because every data point is a step. So this is the distinction between stochastic gradient descent, batch gradient descent and the word for this is mini batch gradient descent. Now there's more technical aspect to it. It turns out that if you just do any one of these, there are different implications. I'll give you an idea, but we'll cover this in the theory section later. If you do stochastic gradient descent, you tend to be wandering around in search of home. You get to home, but you sort of, it's almost like imagine that there is a breeze, a strong breeze or something. And so as you find your way home, you go, well, not a breeze, it's not a good example. Imagine that your way home involves coming down a rocky hill. And so you take a path because on a bumpy hill with many, many different gradients, the path that you will take to get to home will not be the straight path. You will wander around and even when you get to home in stochastic gradient descent, it's sort of you wander around the home. You wander around the minima. On the other hand, with batch gradient descent, you have a more sort of a direct path home, but the trouble with with batch gradient descent, you have a more sort of a direct path home, but the trouble with batch gradient descent, this one is, it is very hard to fit that much data into the memory. Neural networks work best when you're talking of millions of billions of data points. And even with millions of data points, each data point occupying quite a bit of bytes. It doesn't fit into the memory. So batch gradient descent often turns out to be impractical. With small datasets you can do that, but for large datasets is impractical. So what people do is between the fact that the stochastic gradient descent is a bit slow and you sort of meander around the home or you gradually get to the home, you don't take a very direct path home and batch is not feasible. What people do is an in-between thing, mini batch gradient descent. That's what you do in practice. In a mini batch gradient descent, what you have is you take smaller steps. You pick n of batch of two data points, four data points, eight data points, and so on and so forth. So this is it. So guys, this is the foundation we needed to do our lab. Now I'll just summarize what we learned and then I'll take questions. It's important we understand that to understand the code. Today, importantly, we'll learn to write our first neural networks and do something of value. So the rules is regression, let's say, is like learning in regression. Basically, let's take the regression problem. Classification is not very different, but we do this. First you have the last function. When you have a loss function here, what did we did? We took the basically one over and why I minus why I had squared summed over And One over Some Where n is not the total number of data, but the size of your batch mini batch right with the size of your batch, mini batch. Now, there is a word for it. Do you notice that you are squaring the errors? This is the error term. So you have the error terms. What do you do? You square it, you take the errors, you square the errors, square errors, and then you sum it. Well, you sum it and then you divide it by n so when you take a bunch of items and you add them up and divide by the number of them that you have what are you computing you're computing the mean isn't it mean of square errors are we does it make sense guys you're squaring the error and one over n over all of those errors is this. So that is the average, right? Average or mean, average squared errors. And so the name for this is MSE. MSE, mean squared error. That is your loss function that you have done. At this moment, even though there's a regularization term, people tend not to mention it. It's assumed that there will be regularization terms there. So they look at the first term, the main term, and they say that for regression, an effective loss function is the mean squared error loss function. That is not the only loss function. There are other loss functions, but let's discuss that in the theory section when we do it more in detail. So loss function, and then you have to take steps. You've got, you talk of mini batches, you talk of epoch, right, running through the data once, right, and so your learning is you go step by step, you do gradient descent. These are the core ideas that we will use now in machine learning in AI. And in fact, these are the main ideas that will carry you through foundationally throughout the workshop. If you have grasped this much, you have actually grasped a large part of what we are going to do. And so from that point, it was a little bit conceptually heavy. But if you have questions, ask me. I'll just review it. So I'm saying that when you try to fit, for example, in the case of a linear regression, it's easy to understand, align to data. Your reality and predictions will differ. The gap is the error. One way of capturing the mistakes you make, amount of mistakes you make is called the loss function is you square those amounts for multiple reasons when you're squaring makes it positive but there is a more reason to it uh there are some good amount of mathematics there is a theorem that actually called blue theorem it says that it leads to the best unbiased linear estimator that it leads to the best unbiased linear estimator. But we'll leave all that there. So squaring is a good idea. You could have gone to the fourth power, for example, or you could have just taken the absolute value of the error. That also is valid. And people sometimes use that as a loss function, but we'll stay with the familiar one, some squared error. So you do that. So how do you minimize the error? The mechanics is the gradient descent. And I explained the concept of gradient descent. How do you come down a hill? You go against the slope. So one way sort of a way of thinking about it is that in the space of functions, in mathematical world in many ways you can think of it this way that there is always a google map to take you home and the google map is the derivative if at any given point you can take the slope you can determine what slope there is locally how tilted the floor is locally you can always find your way home. That's the basic intuition of gradient descent. Just go against the slope. In other words, go in the direction where slope would increase. And so of course you'll start coming down and down and down. It's a very obvious intuition and ultimately you'll hit home, home being the minimum. So that is all there is to gradient descent. And we do it in small batches. So I would like to take any questions and then we'll take a 10 minute break. Any questions guys? Are we following this much? Those of you who did ML 100, 200, of course, this should look like a very familiar review i hope oh so i said so and uh so then there will be multiple nodes in a layer right so this uh optimization or the uh minimum rates will be found for which uh using this method like for individual node and then remember we are not talking of neural net per se we are saying that if you are doing regression and imagine a box that somehow you want to do whatever the parameters are of that model w it may be simple linear regression it may be neural net and neural net has lots of weights but what we want to do is we want to do, we want to find the W star, the best weights. So we have to do a process of a gradient descent. We have to first find the gradients everywhere, right? Of the loss function everywhere, and then take the step. Now the question is how do we find the loss, the gradient of the loss function everywhere? It's then there comes, there's more trick to it. Let's make it come a little bit harder because for reasons we'll do that then you use the chain rule of calculus. If a is a function like f is a function of x and then g is a function like y is some function of f then it is implicitly a function of x but indirectly so that is how layers behave layers are just looking forward one step at the at the layer ahead of them and they're looking one step backward at the layer behind them so we have to figure out that if a layer in some sense is only locally aware right in the forward direction can do multiplication it can take back gradients then how did how do we do a global step forward, learning step forward? So that is a topic of back propagation and we'll deal with the back propagation in a better way. But at this moment, forget that. For the sake of today's lab, because I want to give the majority of the time to solving problems, just think of it this way that gradient descent is the step you must take to improve your w your weights with a bunch of data you take with the help of a bunch of data you take one step forward in the right direction. Are we getting that Abhishek? Abhishek Ballikrishnan Yes, so this is happening for each node, right? Is each node in one layer? Yes, it will be happening for each node in fact. It will happen for each node. But how it happens, there's a little bit of a complexity there. But think of it like that. Every every single weight see weights are associated with those nodes and if you have layers and layers of nodes and each layer has thousands of hundreds of nodes you're really looking at huge number of weights and we'll see that immediately we'll make some drawings and we'll see that so actually why don't i draw and show it to you guys right now? So suppose it will take one input Okay, it goes to even let's say three Nodes and those three nodes go to three more nodes and those three nodes go to an output node Let's see how many interconnects we can have This can go here. This can go here. This can go here. This can go here. This can go here, this can go here, this can go here, this can go here, this can go here, this can go here, this can go here, here, here. So what happens? How many combinations are there? Three cross three, three possible sources and three possible destinations. So there are nine weights in this region between the two layers. Then here, there are three weights. And then there are three weights here. Then we add a bias term to each of these. Those are also parameters that we want to learn. And so what you end up with is 9 plus 3 plus 3, you know, 15, 16, 17, 18, 19, 20, 21, 22. You're looking at some 22 weights of parameters. So you have a 22 dimensional space in which you somehow try to do a big gradient descent. Are we getting that? Dr. Sanjiv Moin. we getting that I see can you do a quick recap on what you have mentioned when you said lasso and ridge forget that let's do it on Saturday okay that's not connected right yes we don't need it Yeah, it's not going, we don't need it. Okay. For today's lesson. So how do we know the number of mini batches? Oh, you never know. It's a hyper parameter of the model. Oh, number of mini batches, you know, the total data size, suppose he has a thing in reality, right? Let's say you have 10,000 data points and you set your mini batch to 32 or 16. Now all you do is divide 10,000 by 16 and whatever number comes through is the number of mini batches. The last mini batch will be slightly shorter than the other. You realize, right? So think of it this way. Suppose you have 10 data points and you take a mini batch size of three. So what will happen? you'll get one mini batch of three another mini batch of three another mini batch of three and the last mini batch will have how many elements one you see that right that's it so of this like that this is the amount of data this is the min batch size and this is the number about the number of steps all you know is that number of steps increases the smaller your mini batch and they decrease the smaller the the bigger your mini batch that's all you can do in gradient descent yes so the number of uh batches to minimize we should give it or it is it will do it like it should be on its own all you have to tell is the mini batch size and mini batch size becomes a hyper parameter of your model you have to think see it's not really affecting the performance that much i would say it's more a practicality question how good hardware what quality of hardware do you have and how much can you excuse squeeze into each mini batch right have and how much can you excuse squeeze into each mini batch right so if you have a gorilla hardware then sure maybe you can take a slightly bigger mini batch usually people will start with a mini batch of eight of 16 or something like that in terms of a conceptual understanding of the mini batch is it right to assume that when you have a small mini batch, you're going to take a long time to complete the process? Yes. When mini batch size increases, is there a chance of missing a local minima? You don't miss a local minima, but you certainly run the chance of basically crashing the whole program. Very realistically, you'll face these things. Okay. Your laptop, right? It will just barf. And the last thing you want is a program that's been running for three days to barf. Okay. I think we have to play with it to experience it. Thank you. Practical life considerations are much more important to that mini-batch size. Little bit maybe it affects the overall performance, but I don't think it's a significant contribution. So see, you have two things, right? How many epochs, how many times do you want to run through the data? And how many, what's the size of your mini-batch? Both of which you have to team and you're realizing now that when you do deep learning there you will have all of these decision points hyper parameters and they keep growing more and more how many layers do you have what is the size how many nodes you have in each layer we'll think about that and before you know it you realize that there are too many choices or decision making to do. It's overwhelming. That is why the new subject of automated AI or automated machine learning, which in a mathematical process searches for the optimal, it looks into the search space of hyperparameters and finds the best combination of all of these considerations, all these decisions. So that's a separate topic we'd come to that so guys this is it i hope for any other questions before we take a break uh yeah as if uh i do have a question so uh what i'm saying is like gradient descent is a iterative algorithm and in the neural network most of the time the function we are solving the cost function is non-convex and there is high chances that my function like i will stuck into local minima and their advances like adam and rms pro which help us but my question is like apart from gradient descent we have new turns or sm or other algorithms then why we don't use it and like what's the scope and why we are most of the time go with the gradient descent that's an excellent question that youaveen, that you asked. See, the thing is, we intuitively feel that in a high-dimensional space and in problem spaces, we'll have many minima, right? It will be teeming with minimas, local minimas, isn't it? But two things come to our rescue. One is the momentum that you identify. If you have a momentum like physical momentum you bounce out of a local shallow local the other thing we are discovering and only now beginning to understand the mathematics of actually the deep learning spaces these worlds are not teeming with dangerous minima where we get trapped. We thought so. People did a lot of simulated annealing and this and that and many other things they thought of. Today actually we don't worry about it so much because it turns out, and in the theory part I'll give you a reasoning why. It turns out that higher dimensional spaces with complex surfaces are full of saddle points and they don't have that many, relatively speaking, that many absolute local minima and maxima or local minima and maxima are relatively sparse. And there's a very elegant argument to show that. It took people many, many years to realize that that is the case. And that's why just having a little bit of momentum takes you through. Most of the time you get to the true solution. You don't get trapped in the criminal. At one time, people used to believe that it's a lost cause. So people used to always, see for the longest time, they used to always look for those methods whose loss function was convex and people used to actually dismiss the neural network for that reason because it doesn't guarantee your convex loss surface in fact it is not convex we will do why don't we do this right after the break i'll show you a video where you can see simulations of the large surface. It's not complex, it's highly complicated. But the amazing thing is they are filled with canyons, or with, you know, canyons and creeks and, I don't know, ridges, but not local minimas and maximas. In one direction, if it is a maxima, in another direction, it is a maxima, in another direction it is happily coming down. So those are saddle points. And higher dimensional spaces or higher dimensional surfaces, they have a huge amount of saddle points which are harmless and rather sparse number of minima. Okay, thank you Aziz. Alright, anyone else have a questions before we take a break? I have a last one question about the epoch. So epoch is basically you run one pass through the entire data. But when you say I want to run two epochs or three epochs, what do you change between one epoch to another? Because if you run one pass and the second pass through the same data wouldn't the results be the same no no no it doesn't work like that see that's a good question you asked so guys see uh okay maybe i'll draw this curve this curve here has become rather flooded so let me redraw this curve here your question is can we not reset ourselves at every airport the answer is no you don't see look at this you have an error surface like this from the in the first epoch let's say that you your total number of steps are four you took a small let's look at this data set size 10 data each batch size is three so one two three four you're here right this makes one epoch epoch number one you haven't reached so when you start your epoch number two you're not starting from a you're starting from B right so next epoch will take you here right you see that four steps here so this is epoch number two and so one two three four uh with this data set that you're looking at and that makes your epoch number three and maybe the fourth epoch takes you home epoch uh four four or five or whatever etc you can go on in one of these epochs you'll reach four, four or five or whatever, et cetera. You can go on in one of these epochs you'll reach. So the thing is, if you don't run your algorithm for sufficiently many epochs, you're still hanging in the end on a hill. I think, are you getting that? Yes, yes, got it. So basically when we say one pass through the entire data means we are referring to the entire data of a mini batch. Exactly. The entire, all the data set divide, but each step is a mini batch. Remember look here, this step is 3, 3, 3, 1. So I made four steps here in this diagram. If you can notice that this is a step one, step two, step three, and this one is step four and it repeats every in each of those four steps will repeat and one more thing you do just just for fun is before you run an app or you just shuffle the data you just make sure that the data is not somehow pathologically ordered that will screw up your learning. You shake it up a bit. You're saying in every epoch, you're going through the entire data set. That is true. One, yeah, let me say that statement again. you touch every instance of data once and only once and no less than once I once I hope this removes all ambiguity now and epoch is whichever way you do it you do one data instance at a time you learn you learn in small batches or you learn from all the data at one single go but an epoch is finished when you have visited or visited or touched or visited every instance of the data and only once no more than once and no less than once so that is the definition of an epoch are we together now right yes any other questions guys before we take a break? So today it will be a bit longer session guys. Is it okay if we stretch out till 10.30 because we have a substantial nap to do. Sounds good. All right. See you guys. It's 8.36 by my clock. So let's meet at 850 in 15 minutes. Let's go have our dinner and come back. When you finish, you need to make sure that you record and I'll have to drop off around 10. I'm sorry, you said something? Yeah, I was saying that if you go beyond 10, then make sure that that is recorded because I would be dropping off on that. Yeah, you can then catch it up on the recording. So guys, I'm going to pause the recording for a bit and we'll meet again in 15. So guys, we are almost ready to get started with the lab. There's one more thing I wanted to do. Today, we are going to deal with one neural led architecture it's very interesting architecture it is called a feed forward network which is also called like dense densely connected network, or simply dense net. These are words that people often use for it. And it is in some sense the simplest notion that we have of a neural net what it comprises of is we have some inputs coming in x1 let's say that they are a k predict the k predictors you know the it is r to the k the data comes in from here you have so you call this the input layer this is just your data coming in and then to the to the first layer basically this is your input you have the first hidden layer and each data each suppose you have X1 to Xk, right? Let me just simplify it for the time being and make it just X1, X2. So what will happen is both X1 and X2 will go to each of the nodes. And this, this, and you get the idea. You notice that both the signals, input signals, X1 and X2, which are the two components of the X vector, and let me make it R square here, for this case, will go to every node in here. Now these nodes, what are these? These are the magic boxes. Just as a reminder, each node is our magic box or simply put is a model. It's a model in its own right. It's a tiny model of data. It was a logistic unit if you remember, for one thing, is a tiny model. So if you remember the way we envisaged it is that the weights come in, the inputs go in, the weights are there, and then finally there's a distortion function, some sort of activation function is here. This is what we learned the last time. Right? So that is, that is all each of these node is made up of those two things. Right? Then suppose you have another layer. Then what will happen is all these five with the outputs of all of them will go to each of these nodes. So what happens is, so look at this. And I'm deliberately use a different color to signify things going to the next node. Second node because otherwise it gets so sorry uh going to this second node uh just do this second node and uh this goes to the second node you you probably get the idea of what i'm going at the third everything is going to the third node by now you realize that there are way too many many connections out here. One, two, three, four, five. Can you see how dense, intuitively, how dense it feels? The number of interconnects. If you think of these as, you know, threads going from here to there, you pretty much are weaving a fabric in between. It's a pretty complicated set of connections. And it sort of continues. So there'll be many layers. So this is layer one, the second layer, and then there could be the nth layer. And this whole bunch of interconnections will go. And typically, in the case of regression, the simple regression that you're looking at you will have one output layer right because it would be connected to this like this now this output layer is very interesting in the sense that it is whatever signal comes here let's say that a signal i would call it y i minus one you know the these is one two three come here and there are certain weights here here the activation function is activation function S. R. Activation function. S. R. Is just what it doesn't do anything to the signal. So in other words, it just takes a linear this thing. S. R. Does S. R. To S. R. Linear and obviously there's a biased and bringing regression. S. R. In the traditional sense that you have learned from me the linear regression it just is doing a simple linear regression w dot x w dot the x coming from the previous coming from this not the original x let me just call it u let me just call the output of this layer as x to the n so w dot xl of the previous layer that's all and that becomes the output the final y is nothing but this so you have to ask yourself what does this give us intuitively so what it says is that all of these layers and this refers to the talk that we're going to have on sunday they somehow distort the input feature space in such a way that afterwards the solution becomes a straight line in that deformed space a very simple straight line in the default space so sort of the conceptual background to it i won't talk more about it i'll just leave it because that's for the theory part of the class but today what we will do is we are going to build a feed forward network from using pytorch and we are going to train it on some data right using using pytorch and it is by far you and train it on data on data why is this useful if you can do that guys what it means is that you can immediately apply it to every single problem that you have done with me in a 100 200 and in realistically speaking wherever you see tabular data, you know data existing in columns and tables, you will be absolutely ready more or less to, well, maybe not in one day, but maybe in a couple of labs, which certainly will make quite a bit of headway today. You'll be able to solve those problems now using a deep neural network, especially knowing that these deep neural networks are so versatile and powerful you can use it for all sorts of tabular data are we together guys so that's the power of it so having said that now i'll go and look at the lab so i'll share a different screen so far so good guys any questions do we understand what a feed forward network is? It's a network with lots of hidden layers and each layer is connected to every node in each layer is connected to every node in the layer ahead of him and every node in the layer behind it. So that's the definition of a feed forward network. Now I'll share something else. Let's go back to sharing a different screen. So guys when you unzip that file and bring it into your, you fire it up with Jupyter, into your you fire it up with jupiter this is what you will see i have added a few things one of them is of course the universal approximator that you guys were talking that i demonstrated on monday so everything that i showed is here here your homework and i'll write it down is to invent more peculiar functions of your own and check if the approximator is able to approximate it and how well it is able to approximate it. That's your homework for the universal approximator. I'll write down all that homework and email it to you and put it on Slack. But let's go back here. Today we are in the business of taking our first steps with linear, we'll solve one problem, linear regression. Well, I call it linear regression. It is unfortunate. I should not call it linear regression. It is just regression. Okay. I'll change the name later on. It's regression. So how do you solve regression problems now here i'll go slowly guys very slowly so because this is a lab that is important and we need to understand it so guys this cell one can can can you guys remember what this is about why do we do this yes so we can see the SV learn library between now we are what is the cell number to do you notice that it's importing the torch library the the PyTorch. It is also bringing in matplotlib, which gives us a clue that we must be plotting something sooner or later. And it also brings in numpy, you know, the arrays. So those are the three imports. Besides that, you notice that I have imported some classes, two things. Now here is a convention guys that people follow in the Python world. And I highly recommend you follow that. If you see something start with small letters, you notice this, this starts with small letters, little c. Generally things that start with small letters are functions when you import them. And things that start with capital letters like this, it starts with a capital letter and seems to have a camel case notation. These things are classes in an object oriented sense. So it should give you a clue that we are going to sooner or later instantiate these classes. Now let us look at these names and see how frightening they look. One of them is says simple feed forward network. Promises to be simple. Let's see if it truly is simple. The other is simple NumPy data. If you are familiar with Numpy from the previous things or with pandas, this will look quite straightforward to you. This create regression data is just a helper function that will generate a regression data for us. Single variable X and one output variable, one input variable, one output variable. It will create some function and you have to train the neural network to that function. Are we together guys? And we will see whether this is able to do it or not. Now, let me introduce you to all sorts of animals in this zoo. Do you notice that we are talking from torch, we are importing a package called NN. So that is the package where most of the neural network modules are. And do you also see the import tensor. Tenser is a generalization of an array to higher dimensions from array you go to matrices and then imagine a stack of matrices one behind the other. You're looking at a tensor and now increase it to more dimensions. Imagine a stack of tensors and that too is a tensor so that is what a tensor is it's a multi-dimensional array of some sort now autograd actually i don't need this let me remove this i should have this is actually part of the old api that has now uh more or less it's a legacy api and i was going to use it then I realize I shouldn't okay then what is this MSE loss in terms of the discussion that we had does this could this be something that reminds us something we studied a moment ago what does this remind you means squared error loss the loss function that computes that means square in fact that's exactly the one we deal with in regression isn't it when you do regression by default we choose msc laws unless there's a reason to choose something else then comes these adam and sgd so without going too much into the details because optimizer is a topic in its own right, in the theory side, think of this SGD stands for stochastic gradient descent and ADAM. But take these words lightly because when they do SGD, you can still give it a batch size and so forth. It isn't what we studied in a literal sense, where batch size is one. So it's a little bit of an abuse of name, but it's a pretty common abuse of name that's been going on for the last 10 years. People create libraries and call them SGD in a very loose sense. We'll leave it as that. Adam actually is the, at this moment, it's one of the best optimizers we have. Think of it as doing the gradient descent. What is an optimizer? It does the process of gradient descent on the loss function. And then comes data. With data, we have a few more imports. There is something called data loader, a data set and a tensor data. So let me now dwell on these two things, data loader, data set, tensor data and tensor. We know what tensor is. All of computations that you do in PyTorch are done on tensors. Just as all the computations that scikit-learn does, it does a NumPy arrays. It works on NumPy arrays, which is why NumPy is at the foundation. I apologize guys. It's the foundational data structure. We use it indirectly because we use Pandas quite often. And the Pandas library helps you makes it convenient for you, very convenient to deal with those NumPy arrays. And think of it in a more logical way, at a higher level of abstraction. So in the PyTorch world, it is tensor and it is very easy to go back and forth between a numpy array or multidimensional array in a tensor. We'll see that using direct examples in a little bit. So that is why. so when you have a bunch of tensors you then need to store them somewhere in particular when we do with regression we have the training data you know it's like the input data and the output data in scikit-learn you usually identify it as capital X for input data, little y for labels. So here I'll refer to them as either inputs or data and the output as labels or target variable or y or y hat or something like that. All the conventions that are common in the data science field. So that is what a data set would be. It would be the feature space and the target space, the labels. Now when you have a data set, you don't give it to the compute unit, whether the CPU or the GPU, just like that, because those things can be ginormous. So you have an intermediate agency which gives it in batches, mini batches, and that guy is the data loader. You can tell the data loader what is the size of your mini batch and it will then properly go to the data set and ask for items only as many items as needed. So it will sort of lazily keep pulling data out of the data set based on every time that your learning algorithm takes a step or asks for new mini batch of data from the data loader, it will give it. So remember guys, this is an interesting indirection. You don't go to the dataset and ask it for a mini batch. Dataset has no notion of mini batches. Dataset is just dataset. It knows the feature data, the input space, the input data and the output tables. That's all it knows. But data loader is the one that will slice it up into those mini batches and serve it to you on demand. And the idea is that what do you do with that slice of data? You take that slice of data and you feed it into your network. Are we together? So how should I say it? What would be a good analogy for that? See, imagine that you have a pile of something, a pile of sand, right? And there is a machine that is laying down in your backyard, just laying down some stones and putting paving stones and putting sand over it now that machine or maybe it's just a couple of people you and your family are doing it so you don't want all the sand at one go that would be useless you you take a couple of paving stones you put it on the ground then you need to add some sand to it right so you are asking for sands one bucket at a time it would be useless to get all the sand at once so the guy who takes a shovel and just basically takes the sand and puts it in the bucket or maybe a shovel or whatever it is and gives it to you one unit at a time one batch at at a time, is the data loader. That is the basic intuition you need to carry about the data loader. Are we together guys? So think of the data loader as the guy with the shovel and your data as the pile of sand and your computation, your neural network as basically the actual worker, the thing that is doing something, that's paving stones or doing something. That could be one very, I suppose, real-world analogy, if you want to use a rough analogy, to think about the roles of these three things. Are we together? Right? And each grain of sand there is your tensor. Make sense, guys? right and each grain of sand there is your tensor make sense guys now the tensor data set thing I don't I don't think I use it just it's just a helper that helps you quickly create the data sets will go with that so guys any questions before we move forward with the info? At this moment, we must understand it guys, because this is the beginning of deep dive. Now in a survey. Asaf here by tensor, you mean an array, a vector? Yes. See a tensor could be a number, just one one or it could be one dimensional or it could be a number is just zero dimensional. You know, it doesn't reach out in any direction. It could be a vector which is one dimensional, right? Or it could be a 2D array, a matrix or it could be a 3D array or it could be a 4D array and so forth. So a tensor is a general term that can refer to anything. Just as in NumPy, in NumPy at the end of it, when we talk of an array, it could be a number, it could be a real one dimensional array, it could be a two dimensional matrix matrix and it goes on the only difference between tensor and numpy is num actually is that a tensor contains two properties that numpy doesn't first tensors can be loaded onto gpus and tpus a dedicated hardware cuda hardware the second is that tensors come with this amazing property called automatic differentiation. The derivatives, you write a function and the derivative is taken care of on its own. You feed it data, it will basically know, it will sort of hypothesize a function and it will also take its derivative. That's the amazing thing with PyTorch and that is why you use libraries. Otherwise if you understand the theory, we could have written things from scratch, but that's it. And you'll see that, Nisarg in a moment, you'll see that part. And I said, what quality of tensor allows it to be automatically differentiated or loaded onto? Nisarg, I would like to answer that, but you know, a little bit here, can we take that for Saturday? Because that is an open interval and we can have a long discussion then. But it's really powerful. It's an amazing thing. And I'll show you how all of this magic happens. Sanyam Bhutani So one last question, Asif here, dodge.nn.functional as f, can you repeat what that was? Oh, I didn't talk about this one. Yes. Good. So I actually have to explain that. It is just a library which is filled with functions. For example, the activation function, remember that distortion function we talked about a little while ago. Traditionally it used to be in the F that particular functional module. And now it has been moved to the Torch based Torch module itself. So things are moving guys. Things are fluid as you know, you create the next version of the library things move around. But as the name suggests functional, it is just built out of a lot of useful functions and here from dodge import and then tensor the tensor is the numpy thing like np or is it something no no it is all from dodge you see that yeah but is it like does it work like numpy or is it something exactly the api is uncannily similar to numpy for deliberate reason they have imitated. They have made sure that it is NumPy compatible so that the learning curve that people have is very, very small. Got it. Thanks. Thanks, Arun. And that is the reason I took it guys. See what happens is we used to do this workshop using TensorFlow and Keras. Some of you have even attended a precursor to this workshop. So what happens is the things that Keras can do is it looks very easy. Quickly you build up layers and you do. It is in some sense even easier than PyTorch. But the moment you try to do anything non-trivial, you try to design your own network or anything like that now it is out of the limits of keras you have to go to the tensorflow api and that api is very non-intuitive it's a steep learning curve you understand it ultimately you can do it but it has a very steep learning curve so i see one quick question one second let me finish this quarter so white torch is interesting because you it follows the logical the theory it is not as high level as Keras, but it is certainly very intuitive once you get used to it. And you can create, that's why people, researchers use it heavily because you can design very, very complicated neural architectures in PyTorch in a very intuitive manner. Go ahead, Priyanka. intuitive manner. Go ahead. Sanyam Bhutaniyya, So one trivial question. So in all those imports that you're doing there in some, like if you go scroll up a little on the visual, yeah. So there's import torch that you have, and then there's import torch dot nn dot functional as if, if you could take a minute to say why import dot torch import torch didn't get all the necessary libraries, why do you have to import certain things specifically are you is there a reason why you want to leave something out yes yes see languages like python and java what they do is these modules roughly follow a directory structure and in both in java and python to the first approximation and so what happens is if you include a directory the subdirectories are not automatically included right because you don't want to recursively load everything the moment you do these imports what it means is a lot of these things are being searched for in the python part and potentially loaded into memory so you don't want the whole everything to get loaded in memory which is why one of the bad practices you see sometimes is people will do import star from you know from dot import star which means important not only that and this and that you try not to that you try to import a thing but not its subdirectory thing that's a separate module it just happens to be logically there but it's it has different functionality okay all right so now let's look so guys if you don't understand today you must stop it's important so we first let us create some data. So data will create very easily. Remember that PyTorch only talks in terms of tensors. So we need input tensor, output tensor. is based of many rows and only one column. Why? Because why one column guys? Because we're looking at one dimensional data X and Y is a function of X. For today, we'll keep it one dimension. Generalization, by the way, to higher dimensions is a triviality. In five minutes we can do it. But let's understand things in one dimension. so it will produce a tensor one and suppose the data has in this particular case i think i have a thousand a hundred instances a thousand two hundred instances of data whatever i forget so those many rows and one column and why target variable what is the space of the target variable guys in regression most often numerical yeah just one number right it is one dimensional so this again will be the the number of data instances is the number of rows but column is still one right this is the this is what you will get if you now till now i've been telling you to only read the jupiter notebook not necessarily to go back and look at the code of how it was created this one i will this week's assignment your only homework is to understand every line of code that does things you don't see for example here create regression data you need to read that code and make sure you understand it. Now, I'll make life easier. I will walk through that code, explaining it, but you need to review it and make sure you understand it. Because if you get that right, you are on solid foundations. We'll move pretty fast after that. So this is it. Now I'm creating a simple feedforward network. And what we will do is spend a lot of time looking into how we build that network so when i take that network and i print it out and let's uh do that you print it out so what you will notice is that it produces something like this you know that simple feed forward network these are the layers so each layer that hidden layer or feature layer each layer has a name fc0 fc1 fc2 fc3 by the way using the word fc is the fc as a symbol for it is very very common most people use this variable name so that it becomes easy to read other people's work or other people's code if you follow the same symbol just like in scikit-learn if you are doing a matplotlib you will say import matplotlib.pyplot as plt and numpy as np if you import numpy or something else that's a duck, then everybody get confused in the middle of the code. What is duck. Vaidhyanathan Ramamurthy, So in the same way, just use this convention. Vaidhyanathan Ramamurthy, So this is it. We have only one feature goes in. I find it out. So the first hidden layer, how many nodes does it have? 128 128 nodes. bias is just the same thing. If you think of it as a logistic unit, it has a w naught or a beta naught biased. So let us when we look at this network, this is what it is. And what I have done is typically when you write your own neural net, it will produce this much. But I wanted us to understand more, a little bit more. So I have written a helper, a table, it generates a table, right? And some of you who come from Keras world there also they have code to generate a table nice looking table so I wrote some code which that sort of brings the goodies of Keras in case you're used to Keras and you don't miss it so what it does is it says each of the layer how many features go in how many features come out then how many weights are there you know how many weights so weights will be associated here with the output or input where is it going if the number of nodes are 128 and from every this input fans out to each of the 128 nodes how many weights will we have attached to the edges one weight per edge so makes 128 weights isn't it going into the first into the first hidden layer so you're looking at 128 of those resistors or your stats but then each of them also have a bias term each of these nodes have a bias term so one easy way to know how many nodes there are in each layer is simply to go and see how many bias terms are there bias parameters are there so there are 128 bias parameters and so in each layer how many total parameters trainable parameters there are this is the number of trainable parameters this is is what you learn. The neural network will go learn. Already it looks amazing. 256 dimensions, right? Then that 128 nodes connect to 64 nodes in the second layer. Do you see 64 here? They connect to 64 nodes in the second layer. So how many weights will be there? 128 times 64, right? It will be a massive number 8,192 and 64 biased turns because there are 64 nodes in the second. And then, so this is the number of parameters, huge number of parameters. Now the third layer is smaller 16 and how many 16 so once again you're coming from a feature space of 64 so 64 times 16 will take you here right or 1 0 2 4 and then when you add the bias terms you have 1040 parameters now those of us who have been doing linear regression and so forth used to using three, four, five parameters. These numbers look astounding, isn't it? Massive number of parameters. And then finally the output node has only one parameter, you know, just the w.x we talked about. And but because 16 nodes feed into the output node, you have 16 plus one biased term 17 parameters so this is how you understand the neural network and i'll show you the code is this looking easy to understand now guys yeah right and when we see the code of the neural net it will now observe one thing do you see that these numbers are in decreasing order Now observe one thing. Do you see that these numbers are in decreasing order? It is quite common for people to do that. For most situations, you may start out with lots of tiny features, which help create slightly more coarse-grained, higher-level features, and higher-level features, and finally the output. So that process, and we'll learn a little bit about it. It's called representation learning. It's at the heart of deep neural networks. All of that in the theory subject today is the learning. So now what do we do? Let's go back to this simple net. We understand these two lines. Are we together guys? Now comes the activation function. There are many activation functions. At this moment, we won't go into it. But the two common ones that you should play around with, first of all is not to activate at all. That is a unit identity activation function. If you don't apply a distortion, no distortion happens. Then the second one is the people call it sigmoid but sigmoid is the in the word it's an abusive term but sigmoid in the deep neural network has for better or for worse always refer to the logistic regret activation function logistic function logistic function I see almost no reason to use because tanh another activation function is tanh if you just look at the on the Google web what tanh looks like and let me show it to you guys and what does tanh looks look like looks like. Do you see tanh? It goes from minus 1 to plus 1. Isn't it guys? Do you see this picture here? I want some feedback. I hope you're understanding this, but the tanh goes from minus one to plus one. Those are the two asymptotes. Yes. Yes. On the other hand, the logistic function, if you remember logic function, it goes from 0 to 1. So they both look alike, roughly, quite a bit alike. But damage has more sort of an area to climb. And it turns out that that has a wonderful effect on learning. The neural network learns much faster with damage so if you want to use a distortion function use tanh if you want to use on the other hand something very simple right and fast cheap and fast and get gets the work done use something called relo the word relo stands for rectified rectified linear unit well that's quite a mouthful now what in the world does it mean we'll learn about it later but just think of it as a distortion function that sort of distorts the data in a linear manner. Are we together? For the output node, can we ever use TANH or it needs to be signaled? Like it needs to be either VLU or something which doesn't become negative see you typically you could use for example like tanh or if you're doing a binary classification people tend to use a sigmoid right uh logic or that actually most often see what happens is we get so used to using a softmax that by and large and we'll learn about that softmax that by and large, and we'll learn about that softmax, you just by default use a softmax. But yes, you can use a tanh for binary or a sigmoid for binary. And people do sometimes. I have seen sigmoid being used, I don't remember tanh being used, but certainly they should prefer tanh frankly. Because it will give like negative probabilities, right? Yeah, I mean, but then you can scale it up, take care of that. Generally, if you're looking at probabilities, yes, you're right. What it will produce is a number between 0 and 1. So sigmoid looks more natural because it's a probability. Softmax also looks like a probability. Tannage, you'll have to struggle a little bit because it produces numbers in minus one and one. So you have to handle it accordingly. That's that. All right, so we are at this, but at this moment we are going to use activation of ReLU internally because it is cheap and fast. By the way, if you don't give it an activation internally i have used the lu i think by default we'll look at the code or i might have used time as we will see um one more question here um can you explain why we don't use the closed brackets uh after torch.gpu like what is that why do we not use what those Those closed brackets. Oh, because it's a function. You don't, you're not calling relu. It's a, it's a, it's a PyTorch thing. It's a Python thing. If you refer to a function, right? You don't you, the function becomes the object in Python functions of first class objects. So you don't need to put brackets there. So see when you invoke relu function, then it might start asking you to give it some input. But if you want to refer and save the function itself somewhere as a variable, then you don't give closing brackets. Oh, I see. So you will use network.activation somewhere else? and save the function itself somewhere as a variable then you don't give closing oh i see so you will use network.activation somewhere else yes in fact not far not far off you will see that internally when we look into the code you will see me invoking the network.activation and now let's look at the next line guys loss function is MSC loss. Does this look familiar to us? Yes. And optimizer. We need an optimizer to do a gradient descent on the loss function. So Adam is a pretty good one unless you have a reason to use otherwise you could stick to Adam. Generally you should play around. Start with Adam and then as needed with you. And there are many parameters to queue. Learning rate, remember the alpha, how big steps you take. This is the learning rate. See, in terms of the notation that you used, like torch.relu, I think that's coming from PyTorch and .relu is referring to that function, right? That's what you're setting as the network activation yes next line you have loss function which also I'm assuming by just the name it's a function but you're given MSC loss with brackets exactly so it's very interesting the they created MSC loss as actually a class it is not not a function, it's a class. When you refer to classes, notice the capital letter there, capital M. Capital M, beginning with that, should immediately give you a clue that this is a class. And it is indeed a class. So you have to instantiate a class you you instantiate a class to create an object the sort of the you know object oriented thing okay and it returns a function no it returns an object okay which is a function in this case right it returns a function object it's basically a function object think of it like that see these are minor distinctions see idiosyncrasies of pie torch you'll get used to it like you say why do the designers make it like this and not the other way around right why not or make it all like you know torch dot msc loss they could have maybe in the next version they'll do that. Okay, who knows? So it's all the fun with open source design. So now let's look at this. What we are going to do is we are going to pick, oh goodness, see how many epochs am I going to run through? First what I do is I do, this is batch learning, batch gradient descent. So in batch gradient descent, what is my mini batch size? Remember the word batch is a beautiful, batch means all of the data is the batch. Oh, it's not mini batch. Yes, so basically suppose you have a thousand data, or 300 data points, I don't remember how many I kept. All 300 points make a single batch. So one epoch is one step is only one like one epoch has how many steps? In balance. Come again. One step. One, exactly right. Because you treat the entire data set as one batch, therefore you have visited it at once. And so if you want to take, you know, when you're learning, you do want to take a few thousand steps because you're taking tiny steps so give yourself a lot of epochs in these situations i said you don't specify the layers here come again in the output there you you said there were multiple layers but in your parameters oh where do we specify the number of here we We'll come to that. I have not made it open in the simple feed forward network. I have not left that choice because this is the class I created. Remember simple feed forward network is import from SVLearn, our library. So we, I deliberately took three hidden layers, hardwired to three hidden layers hardwired to create less. The idea is your homework is to extend it. And how to know, sir, that how many parts to that. So see guys, here's a, there's a basics, which you count not in a box, but in number of steps you should take steps in thousands otherwise you won't converge properly and you will see this in a moment you'll see what I'm talking about to achieve convergence to reach the minima you need to take thousands of steps so if you're running batch then the number of epochs itself becomes thousands because one if one step is one whole epoch correct so that is it but hold that question in your mind pradeep there'll be a more careful answer to that you will see it on your own in the graph so you choose the epoch what is dropout ignore this dropout is something that helps you. It's a form of regularization. And there's a technicality to it, what it does and so forth. But pretend that you don't see this word here. this for every epoch we need to run through the data batch by batch mini batch by mini batch isn't it guys so this is how you do it you start out you ask the optimizer to do zero grad means it it has to reset itself those optimizers they start remembering the things, those derivatives, you know, they remember from the previous run. You don't want it to remember because it can get really bad. So you tell the optimizer to go reset itself, just as in C programming, you would create an array and reset everything to zero, very much like that. Optimizing reset. So this code looks new to you you but soon will become boilerplate this particular pattern of code will literally become boilerplate in your life this this will begin to look the easiest part of the code which is a no-brainer because it's the loop you'll run over and over again everywhere so you take the optimizer and you reset it. Now what do you do? Remember you have your neural network. You ask the network and here's the beautiful thing about PyTorch. The network is an object, but it is also a function object. So you can invoke the network on the on the input data extensor and when you do that where did we create the extensor guys do you remember we created this data here so you ask the network to compute on x and what will it produce your y hats isn't it your results of the prediction right please tell me that you understand up to this line anyway yes simple you took a network you know that the network at this moment is useless it hasn't learned anything so all the weights are random weights but nonetheless you ask the network you make to make a prediction on the data the moment it makes the prediction on the data you know that those answers will be wrong but what can you do you can find out how wrong it is that is done by the loss function it will tell you it is law it is so very wrong right so you can pick from can you define the back size by increasing the back size. So in this one here, in this particular loop of code, we are not defining the batch size. The batch size is the entire dataset. Remember the name of this approach is batch gradient descent. In batch gradient descent, whatever is the total size of the data, that's the batch size. Are we understanding that? Mention weights like if they're random, so they don't need to be mentioned in the code. Come again? If the weights are random, like I don't see like the weights in the code. So are they not to be required to be mentioned in the code itself? No, you don't have to. So you know, the beautiful thing is that it doesn't matter how wrong you are. The network, you just initialize it with random weights and it's fine. Actually, there's a subtlety to it and the theory I'll tell you. It must be random. You cannot make sure that all the weights are zero or all the weights are one or something like that. Right? It is called a symmetric initialization and people joke about it. In theoretical physics, there is such a thing as symmetry breaking. You need it for the world to come into existence, apparently. All the forces, you know, electromagnetic force and nuclear force to come into existence. So the forces you know electromagnetic force and nuclear force to come into existence so a similar thing happens in neural net if you set all the weights to the same thing initialize it to the same thing like zero or whatever number you want then your network will never learn so you initialize your network with random weights and the the package pi torch takes care of it it will automatically initialize it with some some random weights you don't have to worry what dropout is no that sentence about dropout entirely ignore that you are seeing it so um one more question like this optimizer dotzerogrand. One second, let me finish that Lars. So dropout is something that helps you regularize your network, prevent it from overfitting to the data. Think of it as another form of regularization, but beyond that, till I explain it to you in a future lab. So, our learning is a bit circular. We are ahead in the lab than the theory which is why we are having all these issues soon we'll get ahead in the theory than the labs and you'll be more comfortable but at this moment we need to make progress with the lab so i'm bringing those things in wherever you see dropouts just ignore the fact okay now uh then all right somebody was saying something uh yeah so this uh optimizer dot zero grad and then the comment is like it reset the parameter gradient. So are we resetting it for every epoch or we are not? Remember, these are the gradients, not the parameters. You're not resetting the parameters, but the gradients. Derivatives of those, derivatives of those parameters. Derivative of those derivatives of those parameters. Derivative of the loss function with respect to the parameters, right? D L D W D L by D W. Those things are being reset back to zero. The parameters will learn step by step, but not this. So maybe I should have taken the word parameter out. Let me do that. The gradient. So let's just, those gradients are associated with the parameters. So that is technically correct, but let me not use the word. So we reset the gradient. So remember remember not the parameter now we make predictions with this network you would agree that those predictions would be absolutely bogus isn't it guys the first time you enter in your first epoch right away your predictions will be bogus they'll be hopelessly wrong so then you compute how wrong they are then that the fact that you can come once you compute the loss that is called a forward pass you move forward you took the input what was the input the results the the extensive and you forward computed the results now you take the results and you compute it with actually what the data is what the training data is saying that why value should have been Vaidhyanathan Ramamurthy, So there is a gap between y and y hat right that will help you compute the loss. Vaidhyanathan Ramamurthy, So far so good guys we compute the loss. And now we do the one thing that a pie torch is all these TensorFlow give you, they give you automatic differentiation. They have already computed the gradients for back computation. So in other words, what you do is this step just computes internally the gradients, back propagates the gradients, but then gradients are there computed everywhere, but you still need to take that step forward means all the parameters from their random value they need to improve a little bit in this step you update the per parameters actually update the parameters so this is your gradient descent step am i making sense guys this is your gradient descent step. Here you compute the loss. Here you take the derivative of the loss. You find the gradients and here you use the gradients to do gradient descent. So if you understood the part that I explained, does it feel very natural now guys, this code? You make a prediction, you compute the errors, you take the gradient of the errors, and then you use that to gradient descent, improve. Anybody would like to confirm that this looks understandable, please? As if the zero grad is happening at start of every epoch? Yes, get rid of the gradients every time at the beginning of each epoch. Okay, yeah the rest is yeah, beautiful. Yeah, so guys this is it and now what happens is the rest is just bookkeeping. I keep at each epoch, we are keeping track of the losses. How much, what was the loss? Like how much loss is a measure of error, right? How wrong we are, that of the losses. How much, what was the loss? Like how much loss is a measure of error, right? How wrong we are. That's an intuitive way to think. Loss tells you how badly wrong you are. So you would imagine that in the first, in the first epoch, when you're just entering it, you would be terribly wrong. Isn't it? And then in the second epoch, you'll be less wrong and the less wrong and so forth and we should see that in the results we'll see so i'm keeping the epoch and every hundred epochs because i have two thousand epochs i don't want to print the improvement in every epoch so i've taken it to be every hundred epoch it prints out what is the loss now what is the loss now are we together this is it and this is the training loop this is the it's saying this is the heart of the training loop in python you will always have some variation of this but ultimately it comes down to making a prediction computing the loss back propagating the gradients then taking a step forward and improving the parameters. That's it. These four steps, you see the four, it's a four-step, it's a four-part journey. First part is you come, you make a prediction. Second part is you compute the level of mistakes in your predictions. That's called loss. The third part is you compute the gradients of the loss. That's the backward. And the final step is you use the gradients to improve your parameter value. That is the gradient descent. And that's it. That is, and in those four steps, you summarize machine learning at least for the supervised machine learning part. And in, once you understand that you, I hope you would agree that it looks pretty elegant in code, right? Four lines do it in PyTorch. Now is this code beginning to look easy guys? Okay. Let me ask the question the other way around. If you feel that you are still lost, please speak up. So to review, you need to read, I'll tell you which chapters you need to read in your PyTorch book. Give me a moment. This week's reading, and I'll post it on the slack of course everything that I'm saying you should take out time and review it by doing chapters so one is too basic like it has nothing in it a core pie torch what one actually can much. One actually you can play around with. One, two, three and four. Guys do, definitely do one and three. Two is fun. Two is for your own entertainment. chapter one, chapter three, and then get into chapter four. But it unfortunately starts with congulation neural net and it gets a bit confusing. So just glance through it, but definitely finish chapter one and three this week guys before we meet next time. The deep learning technique. To do it from this book, right? This book and if you have the other book The other book or I leave book for your lab there. It will be up to chapter two in this book. It is up to chapter two. Sajeevan G. Are you showing the book. Sajeevan G. Oh, look at me. Look at my picture. Sajeevan G. Are you guys seeing this book. Sajeevan G. And Are you guys seeing this book? Can you show the first book? Yes. The O'Reilly book you do up to chapter 1 and 2. And in the Manning book you do chapter 1 and 3 definitely. And you try to glance through chapter 4. You will find these things now straightforward. Sir, how the shape of X tensor looks? Because I understand you created a tuple earlier. Yes. But how it looks? X tensor looks like, suppose there are 300 rows. I forgot how many rows I'm creating. 300 rows. Okay, sir. 300 rows in one column because X is single variable, right? Okay. So when we are training here in network, we don't need to give the target variable. Yes, we don't. That is the whole beautiful thing, right? See function input produces an output. Now you compare it with the actual target variable to see how wrong you were for each of the instances. You see how simple it is. Input to a network is always the feature space, the feature vector. You don't need the target vector there. Then to compute how wrong it was, to the last function you give the results. You don't give the input, you give the results and the actual value and the predicted value. This last line looks like prediction you were to do earlier, but I was thinking that somehow training. Yeah. Do you see the get predictions? Yeah. This is predicting. So, you know, in scikit learn, you would say, right? Classic is some model dot predict. Yeah, yeah, I understand understand the equivalent of that is just this line okay okay so so where we train the model then in the network this is it no we are training it right here you know the network is the model network is made up of all these weights those weights are random we haven't trained them this loop is training it so let's go over it again, guys. You have a network, all its weights are jumped, right? They are random values. Yeah, I understand. It will produce some prediction. Results are the prediction. Those predictions will be bogus, right? They'll be very wrong. So now you can compute the amount of mistakes, the sum squared errors. Yeah, that we can calculate. That's why I put all these comments here. See, read these comments. Estimate loss. So now you can estimate loss. Given the estimate of the loss, now what can estimate loss given the estimate of the loss now what can you do this come now comes the deep neural network part you have to compute the gradients back propagate the gradients yeah so we will find the minima here it is one little step towards the minimum once you have the gradient then you can use it to do gradient descent, one step. So now what has happened, you started with the network which was rubbish, but you finished one epoch, you notice that now you're done, this is something out and so forth. One epoch and you'll go back. But what has happened to your parameters? They have improved over the random, isn't it? They're slightly better than random, right? So this time around around next time around when you get into the loop and you again make predictions on the data will you make better predictions isn't it okay so optimizer is like a regularization thing no optimizer is the one that helps you do gradient descent gradient descent okay the engine that does gradient descent is the optimizer okay then it's okay so ultimately we will find the minimum that is it right so this is it you get the prediction you estimate the mistake level of mistakes the estimate loss that you say back propagate the gradients and take one step forward right in the right direction that is it and then you go back now your parameters have improved and you keep on doing it and the parameters keep improving and hopefully they are by 2000 epochs they reach the minimum and we'll find that out so see what happens when i run this yes see what happens when I run this. Yes. On the backward, I think when we say back propagate the gradients, what we are exactly doing, we are adjusting those w's. See, no in back propagation you're just computing the derivatives. Just think of it as at this moment I haven't taught you back prop. Just think that for mysterious reasons you need to know the derivative of the last function with respect to every weight, every parameter. It's an internal thing. For some reason you need those derivatives. Remember when I showed you the, the okay guys let me go back we are going over this over and over again i think we need to go back to our notes see guys are you seeing this picture do you remember this this yes are you seeing the screen now yes so what do we do in order to go let's say from the b point here this pink thing to this point and this and this and this these are the steps over many epochs that we do now at each step for me to make progress, do you see this line? What does this say? I need to know the slope. You can't make a gradient descent step and a step of improvement till you know the slope. Do you see that guys? So you need to first compute the slope, isn't it? So you need to first compute the slope, isn't it? Yes. So you need to compute the gradient first, even before you can do a gradient descent step. Everything else is known. You know the previous value of the parameter, you know all other things, your learning rate, you know. But what you don't know is what is the gradient. So first, when you have the loss, using the loss, you what is the gradient so first when you have the loss using the loss you will find the gradient and once you have the gradient then you can do a gradient descent step yes now is it making sense yes yes thank you so we come here and so this is what we are doing we are making predictions we are finding how wrong we are then we are finding the gradients and then we are taking a step forward and so if we keep taking the step forward over and over again across the epochs we will be close to the answer and i just ran code. Let's run this again and see what happens. Let me decrease the font. You just ran it, sir. I just ran it, but I want you guys to see it again. I'll run it again and see. Do you see this epox on my machine? Unfortunately, it runs too fast. So it just quickly produced all these results. So what do you notice in this loss do you see the loss rapidly decreasing yes sir right and here i've plotted the loss with respect to the steps the number of vibrations you know the steps we have taken and what do you see here guys in the beginning very high loss yeah improvement is very steep, but gradually over different steps, it slows down. So one of the questions somebody asked is how many epochs or more meaningful question is how many steps total? Do you realize that even at 1250, we are improving at 2000. Now we are beginning to stabilize and improve a little less. But we can go on. We can go on to 10,000 steps. Actually, since this runs so fast, let's go on and do that for whatever reason. Let's try that. And if you run it for 10,000 steps, what happens? what happens? Right. Now we are running it for a very, very large number of steps. And let's see what happens. Well, it's going to go through a lot of these. And now let's plot it out so guys look at this curve how does this data fit the i mean the curve your prediction fit the data and now let's see what happens at 10 000 steps by the way at 10 000 uh steps and a fox your loss do you think it's decreasing anymore? No, it's increasing, sir. Yeah, it is just stabilized. It's not making much of a dent anymore. So 10,000 steps were a bit useless. And your curve is still like this. So what it means is, somewhere along the line, you know that you should have stopped at 2,000, maybe 4,000, but no need to go beyond that right so which is why initially i stopped at 2000. so that is again you know experience you do it you learn from it you know you know how many epochs you need so that answers your question pradeep you ask that question isn't it yes sir and it does look like that uh it has to run a couple of thousand times a box. Okay. So now guys, I'll explain the next part. What you do is at this moment, I did not do the split between training and test data to keep things simple. So I'm just taking the predictions on the training data itself. How do I predict? Remember, this is it. I've just copied this line here. How do I predict? Remember, this is it. I've just copied this line here. Final output, I take the whole X and then create output results. So this will be the prediction that it makes after the network has been trained, isn't it? So this is why I put the comment. Now we use the train model to make predictions. It's necessary to enter the evaluation mode. Actually, it should be a mode, not model, an exit train. So so for that you need to give this word network.eval so when you give the word network that eval you signal to the network that don't learn from the data don't try to learn from the data so it will not do any of that autograd and uh you know observation of the data and so forth it will just go quiet are we together the network now is frozen. It's not learning anymore. And then you do this and you can save this model and all that. And this plot is very useful. You should always plot your loss with respect to the number of steps because it tells you when to stop. And then the last thing is I'm plotting the curve, the prediction curve over the data. Do you guys think it's a good curve? Yes, sir. Very. Though this is a bit unfortunate. If you think about how I generated the data, you'll realize that I used a sine wave to generate it. So that was gradient descent, the full. How can I do mini batches? Asif? Yes. Just a small question before we move further. So instead of setting a threshold value, can we determine a minimum or a maximum delta, like the weights value, so we don't have to run that many iterations? Yes, very good, very good actually. There are techniques to do that. We will learn those things okay so you know we are taking baby steps today is the day of baby steps yeah i said one more question uh network.eval you said it it would stop the gradient from running again is that what you said it stops the weights from changing but uh wouldn't the network be frozen anyways after all the epochs are completed? No, no, no. What happens is that the network will continue to do, unless you call back prop, the weights won't be updated. But the computations are much slower because all the time internally it is waiting for you to do the, you know, it is keeping track of all the changes because it expects you to call backward on it, but you have no intention of calling backward anymore. Isn't it? You're making prediction. So it's unnecessary overhead. You don't want that. But don't you call backward explicitly when you run the each epoch large start backward yes you did but remember your training loop is only till here it's over now what are you doing you're going to make prediction maybe i'll remove this from i'll split the cell split this so this is the training part. The training part is here. This is prediction. I understand that part. I'm just confused why we need to explicitly say that, okay, now the network needs to be frozen. Every time you do forward, right, you know, that you make predictions. It is looking at how the value, you know, the output value is there, what value it came out with and so forth. It is getting ready to compute the derivative with respect to the loss. It's keeping track of it. It's keeping track of your loss. Do you really want it to do all of that? So, it's unnecessary. Hang on, let me finish with you. You realize it's unnecessary, right? And now you're in the inference mode. See, you do take your network to production. It is on entirely different machine. You have absolutely no intention of doing any training on that machine. You train a network on some very powerful hardware. It takes days and days to train. But when you take it to make prediction, you're running it inside some microservice. And that micro service is running on some, it has no graphic card most of the time. And lucky if it has some graphic computing ability, sometimes it's running on smartphones and whatnot, and it's running on cheap devices. You want to minimize any kind of overhead, isn't it? Yeah, that's it. Go ahead that's it. Go ahead, Sukhbaz. No, I was saying if you don't put network.eval, it will be just like one more epoch under the loop, network extends, it will train one more time. Exactly. It is effectively like a epoch. I mean, it is thinking of it like an epoch and wondering why you didn't call backward instead. So that's what happens. Alright guys, so that is that. So you get predictions. Nisarg, does that clarify your point now? And now you plot it out. So you look at the plot, it's reasonably good and you can do, sometimes you notice that there is a funny kink here right and if you actually use less number of epochs back to 2000 you'll realize that it doesn't have that level of complexity it hasn't overfit the data do you notice that the curve is simpler is variance yes it sort of goes to the heart of all of those topics so now i'm going to show the The curve is simpler. I experience. Yes, it sort of goes to the heart of all of those topics. So now I'm going to show the code for mini batch. And this one, the gist of it is we, here, I'm not creating the data as a data matrix directly. I wanted to show you how to create a data set by hand. In the previous one, I did it. I created to show you how to create a data set by hand. In the previous one I did it. I created the X tensor and the Y tensor because I wanted you guys to get warmed up to it. Here I show you how you can do it. There is a class for simple NumPy which will deal within the next few minutes. You give it, the way I've designed it is that you create NumPy objects and you give it and it will make tensors out of it internally you don't have to see it remember numpy it is Pytorch deals with tensors but if you are hand generating data you're likely to use numpy API to create a second data so this is what I'm doing by the way this was the function function 10 times 1 plus NP sine it looks always more beautiful if you you know in the lint space 0 and 1 you put NP dot negative pi and positive pi yes that's creates a very beautiful data yes so you guys play around with all that and then have you already played with that play with that no i did create some fake data sets so guys remember what was the point of a data set data set is just something that will give you the feature tensor and the levels target tensor now what does the loader do remember data loader's job is to slice it up into mini batches. Unfortunately in the deep learning community, almost every library, it doesn't use the word mini batch size. They consider it two bar goals. They feel that everybody understands it. So they use the word batch size, but batch size is actually in a more uh theory part it is the mini batch size so i've taken a pretty big batch size 32 right because i wanted it to learn quickly in my hardware you can start with one and you will see or two or something like that do you notice this parameter shuffle is equal to true what it does is every time you run through an epoch equal to true what it does is every time you run through an epoch it will reshuffle the data right now there's a bit of so now what i do so this is the use of a data set in this let me quickly go through the code of that the rest of the learning is simple with one change now one step is not one whole epoch so for for a given epoch, you see another for loop inside. So these are the mini batches of data. Data is a mini batch. Maybe I should have used the variable batch. Batch and label. Data and label. You feed the data into the prediction engine network. It will produce the results, exactly the same same code and we start accumulating the epochs now the number of steps will be much more right so you see it run and you notice that the epochs are running slow are you seeing these guys why are they running slow because each epoch has thousands of steps or hundreds of steps so it will run its way through and when it does so the more steps you run something through the better the learning but at the same time if your mini batch is too small it screws up with the learning because it's too you know it keeps fluctuating all the time are we together right so you see that yeah look at this when you look at this graph do you see how much the loss fluctuates in mini batch it does in stochastic it will fluctuate even more. Now you look at this curve, it sort of has fit the data perhaps a bit better than this one, the previous one. Would you say that guys? Yeah. A little smoother. Yeah, it's smoother, but it has also begun to show some slight dangerous signs of overfitting too a little bit right so that's you have to then worry about the regularization aspect of it so this is with many batches and guys is this code now simple and easy to understand the only thing is that you go with for every epoch for every mini batch you do exactly the same steps what do you do you make prediction you compute the loss you do back propagation of steps. What do you do? You make prediction, you compute the loss, you do back propagation of gradients, and then you learn from that. You take the step gradient descent step. That's it. And that is the heart of learning as far as PyTorch is concerned. All of machine learning, it looks at essentially the predictive modeling part as just these four steps. So now you get the mini mini now this is the answer now last of all we want to do the stochastic gradient descent what is it it will one step is just one data instance right we can do it very easily all we have to do is take the previous bit of code but here i've set the batch size to one nothing new batch size is equal to one, guys. So now when I run through this code, what happens? It takes a very long time. You notice that this will take six minutes and 32 seconds, which you don't have. But what does it look like? Do you see the learning? The loss keeps fluctuating. It's a strange world. And then you get this curve, which is pretty good, but a bit of overfitting perhaps. So this is it guys. And so this is it. In mini batch size, how to decide the mini batch size? It's a hyper parameter. You have to play around and do it. Okay. And so this brings up the whole world of hyper parameter optimization that I referred to before. We'll learn about that someday. Asif, can you explain the inner loop in the mini batch? Yeah, yeah. Let me do that. So see see this is for the epoch right for each epoch now what this does is this for loop is interesting when you iterate over the data loader it will give you it will each each time you go to the data loader it is like you know this is python's we are saying data loader give me next you know in java you say next right on a high trader is it like bootstrapping sir no nothing to do with bootstrapping see imagine that you are sitting in a shop right or remember that guy the shovel guy the sand in the shovel there is a whole pile of data that the data loader is sitting upon. Who is the data loader? He's the guy with the shovel, isn't it? So when you iterate over the shovel guy, what will he do? For each effort that he puts in, he'll return you one shovel worth of sand. One shovel worth of sand is this little bit of data and it's X, little bit of X and little, and, and they're associated by the labels. Are we understanding this now? So you can imagine that to go through an epoch of one pile of sand, you will go through many, many shovelfuls of sand and that is it so each shovelful you're using it as a learning exercise and so you do the same in a learning loop this loop uh rafiq you're familiar with by now right this is familiar so this is the way to think see guys make it very real for yourself bring your own metaphors or your own uh you know real world pictures if in the beginning when you're learning it it really proves useful to think in terms of something like that i think of it as sand and shovel guy you think in terms of whatever makes sense to you but remember that you need to make those mini batches somehow very real to yourself that's all this fall like this right so each the inner for loop the data for each iteration corresponds to one batch one batch yes one batch one mini batch yes so it's an n in our theoretical discussion exactly in the notes it was a little n yes yes understood excellent you absolutely small n. Yes, understood. Dr. Vaisakhan Thampi You absolutely got it. Do we really need to set the flag over there, start, start, true and false? Dr. Vaisakhan Thampi Oh, this is just for me. Otherwise what happens is that it will print value in every single step. Do you notice that there is this little and start? I want to print this only once per epoch. I don't want to print the value every second. Yeah, I got it, sir. That's that. So this is the stochastic gradient descent. Just set batch size to one. Now, let me show you how the code is written, guys, because now we need to go. So guys, play with this, but let's start peeling the onion. I was hoping we'll do it today it's already 1013 let me give you a quick overview maybe on Saturday or next next I suppose next Wednesday we'll go deeper into the code where is the code the code is no this is the notebook so where do you find the code it is not in the notebooks it in it is in SV learn it is here actually in the approximator go there regression network so here this code is something actually there isn't enough time all right let's take next Wednesday by starting with walking through this code see if you can read through this code. I have commented it forward. And we need to understand this, but let me at least do the init part. So init part is in constructor, people in object oriented programming use the word constructor. Constructor means you sit and build an object. So when you build a network, here is where we build a network. So here it is. We're building one layer at a time. See, we are not connecting those layers at this moment. We're just building them one at a time. So what are we doing? The first layer has input dimension and number of hidden and here I've hardwired things. Do you see the hidden layers? 128, 64, 16. So I have three layers. Firstly I have 128 nodes, second with 64 nodes, third with 16 nodes. And so I build this, there's nothing, this code guys, does it look easy to understand? How many edges for a, how many things come in and how many things go out that's an input out for each linear layer anybody wants me to explain this particular line 36 and if you're lost with the python code guys, remember Saturday we'll do the remediation. Creating layers. And so you notice that initially I used to assume ReLU, but I happened to just while playing with this code, I made it 10-inch. If you make it ReLU back again on your laptops, I would suggest change it to ReLU. It will run much faster. And the forward pass. This is the one that makes the prediction. This code looks at this moment arcane to you, but I'll explain it to you guys later. Just glance through this code that we need to carefully walk through it next time. Now guess what this pretty table does? Formatting. Yeah, it just does a formatting, nothing magical about it. And this is the simple NumPy data set. So if you have one, one dimensional input and one dimensional output, here is a little bit of code that takes, makes it easy for you. I will this much and understand that it will be good over there try to read through this code see how much you understand I don't expect you to understand because I expect you to understand the notebooks guys but this part of the code I'll carefully explain line by line next time this by the way is creating that helper method that created the regression data as you you can see, I use this function 10 sine X and I added Gaussian noise to it. This is the builder of the plots. Remember the two plots with two side by side, this builds the plot. So this is just matplotlib. Again, I hit it because some of you are new to matplotlib. So I hit that part. And by the way, one of the things I will do eventually this weekend is I'll get you to load this up in PyTorch so that you don't have to open a notebook. You can run this code and debug it in your, not Python, sorry, PyCharm or PyDev, Eclipse or one of these debuggers or in Spyder and you can just debug it right there. Like find mistakes and so forth. So do you notice that I tend not to write long paragraphs of code in jupiter notebooks yes sir that is the practice you should follow usually when you see a lot of junk code in jupiter notebooks it's a clear sign that the person is not doing very disciplined including. All right. So all right guys, it's almost 1017. I would like to reserve the next 10 minutes to I said 1030 to taking the question and answer. I'll just summarize this week. What did we learn? We learned about neural networks property as an approximator. We learned about what activation function is at least in a way conceptual level, it's some form of distorter. And then we learned about one more thing today, gradient descent and three kinds of gradient descent, the batch gradient descent, which takes one big bite of the data the whole data set is one bite then there's mini batch gradient descent which takes small bites of the data shovelfuls of the data and the third is the stochastic gradient descent which learns from one grain of sand at a time right so if you think of if you think of a pile of sand a batch means you take the entire heap of sand and you sort of somehow spill it all over your project. The mini-batch gradient descent is you take it shovel full by shovel full and you use it. And stochastic gradient descent is you literally build your project and your paving stones and whatever it is that you're doing one grain of sand at a time. That's sort of a way, I don't know, metaphor or analogy that you can carry. That's that. And so the PyTorch and every machine learning framework in this space, with PyTorch in particular, it explicitly asks you to do four explicit steps given a network it's asked you to make a prediction well you say how can it make a prediction when it's all wrong we haven't trained it it says no problem just start randomly and make a prediction then look at the mistakes from the prediction the laws then compute the gradients because you leave the gradients with the gradient descent step and then do the gradient descent step and then repeat keep repeating till you are satisfied with the answer till the network makes good predictions in other words till your loss has decreased to a level that is acceptable to you later or at least it has stopped improving. And that is using PyTorch for machine learning. So all of this API is new to you. Trust me that once you get familiar with it, it can look very natural. But the first time you encounter it, all of this, it looks a bit intimidating, which is why we are going slow and which is why i am giving all of this you know helper making it simpler for you but in you so at this moment i would suggest for this week the code this moment will look hard definitely definitely make sure you understand the linear regression code here right notebook. Review every single line of it, understand it. If you don't understand, ask questions from us. I'm there, the TAs are there, other participants are there who will help you. Definitely don't hesitate in asking questions. Make sure that this part is crystal clear. Next Wednesday, we'll deconstruct and unpack that simple neural network that we created. And if you get the pattern then everything else, everything you will learn in this workshop all the way till January follows exactly this pattern. Exactly this pattern. So you are laying down a pattern for the first time. It's long and strange. Once you get familiar with it this is the pattern we always want that is a correct so i'm stopping the recording so you guys can answer questions