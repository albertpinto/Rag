 So this paper is a very recent paper and it deals with the topic of object detection. So when you have an image as a bit of a background, it's a complicated problem problem so suppose you have an image you have an image and that image contains let's say it contains a house and it contains a tree and maybe it contains a duck somewhere and your duck is in some lake let's say it's a little pond and it's swimming in the pond but I'm just concocting a situation so now what you have to do in object detection there. What are the objects? And the second question is, where are those objects? So, for example, you may say that this guy, let me just say, this is a tree. This is a house. This is a duck. But then you need to be able to tell where in the picture these things that you find. So you'll have to do, the word that people use is the concept of a bounding box. So you have to say, let me draw it out like this. You have to say that this is, do you see this green box that I'm drawing? Maybe I make it more saturated. This green box. You need to make a rectangular box around the objects that you are seeing. This object and this object. The word people often use for this is bounding box. So if you're giving the bounding box, what are the coordinates that matter? You probably need these two corners to uniquely specify the bounding box. Let's say you can call it X1, Y1, and X2, y2. So to represent a rectangular bounding box, you need a variable, a basic tuple that has four variables, x1, y1, x2, y2. Are we together guys? It's a four dimensional. Now traditionally what you do is that you represent the entire geometry as made up of from 0, 0 to 1 and to 1. The size, you assume the size of your images, unit size. So therefore, each of these values, XI, YI, they will belong to the interval 0, 1. And this bounding box, let me just call it the bounding box bi belongs to it will be basically four numbers each of them belonging to belonging to the interval 0 and 1. is that simple guys we're just framing the problem like what is this problem of image detection? So for example, one box could be something like that. A box two could be something like this. 0.2, 0.3, and 0.7, 0.9. So these are your x1, y1 and x2, y2. The two corners of the rectangular box. So far so good guys. It makes simple sense. So Asif, the notation you gave for bi is 0 and raised to 4. Is that the right notation? Yeah, it is basically. What you're basically saying that it's a, so each component of BI. So it's a vector, essentially, it's a four dimensional vector. Each component belongs to the interval 01 isn't it. So this is this is what is used mathematically. Yes, it could be it is a valid notation. So it is a specialization actually of the notation, slightly different notation. If you remember, if how do you represent a four dimensional vector for notation so it is a specialization actually of the notation slightly different notation if you remember if how do you represent a four-dimensional vector for the vector you would say it belongs to r4 isn't it except that here except here it is limited to it is not all of R4, limited to 0, 1, unit interval. Right. And if you want to define U is equal to unit interval like this, then you can say that your BI belongs to U, 4. Unit times unit times unit times unit, isn't it? Four possible, four values with each belonging to the unit interval. But then the bounding box contains something. It contains a tree, for example. So you need another thing to uniquely specify that you have recognized an object. You would agree that you need to be, you need to tell the word tree that you need to so let me say one object detection would be fully described by this the class of i, the label, for example, it could be the word tree and then the coordinates, then the BI coordinates. So it would be this combination. So I said one question, how would you define that the two coordinates X and Y on the next two are two are opposite each other and not aligned? It is possible. So, for example, you can have a rectangle which is so narrow, right, that x1, y1, x2, y2. Here, x1 is almost the same as x2. It's possible. But basically, it has to be a rectangle, right? And you pick two opposite sides. It doesn't matter which two opposite sides you pick. But how do you define that in the model oh so when data comes to you it's very simple suppose data is comes to you so how do we get data training data good question training data training data will come to you like this it will come to you with a collection of collection of images right so this will be let's say you have an image that is 128 I don't know some bigger image let's say 512 by 12 across the three RGB channels just as we learned this week but at the same time for any given given image I you would have a label. So this is your input vector. Right. This is your So you'll be given another file which says that for this image, this is the class and these are the bounding boxes. And they'll follow a convention. They just give you the values of the opposite. So the bounding boxes would have opposite values. Exactly. That's typically how you specify. So now comes an interesting question. So if you observe this and let's break it up into two parts. If you notice this part from an image you are going to a class. Is this something we are familiar with in this week? We know that we have been able, so this looks like a classification problem. On the other hand, what about this guy? Let me pick another one. Yeah. So what about this guy? The bounding box. What are you doing? You're predicting into numbers, isn't it? From an image, you're going to a number, a set of numbers. So that is a problem in regression. That is right. So this becomes a regression problem. And that is what makes, and not only regression, you're not regressing into just a number, but you're regressing into a whole vector, a four dimensional vector. Four vector spaces target. And as if we are consciously, we are being conscious here that these could be overlapping spaces. Absolutely. That is another important point. So for example, if you have, let's say, the house is here. Right. And then a tree, it just so happens, is growing. Somehow you can see the tree growing in front of the house. Then what happens? You have two bounding boxes, one for the house, which would be like this, and one another bounding box, perhaps for the tree. Sorry, let me take a fatter one, a tree, which would be like this. Isn't it? And as you can see, these two rectangles are overlapping. So far, are we clear guys? I hope there's no confusion because I'm just framing the problem. How do people in this domain, when they do object detection, look at these problems? So this mathematically frames the problem. Now, why do we bother about object detection? What are some of its practical uses? Very simple. For example, if you're talking about self-driven cars, you cannot have self-driven cars unless you have an AI that can look at the scene ahead and the camera image ahead and quickly do object detection with high accuracy. It knows this is a pedestrian, this is a car, this is an empty road. If it is an empty road, I can drive forward. If it is a car, I need to then wait, slow down. If it's a pedestrian, I need to slow down. Things like that, object detection in a scene is a very crucial operation. Are we together? Likewise, picking up the instances of humor in an x-ray, things like that. So we do object detection in a lot of places. Now, the trouble with object detection or the thing that makes it very hard is that first you notice that it is it is sort of a portmanteau. It's a problem. It's a compound problem of doing a classification and a regression at the same time. Right? And so there are many ways in which you could get your answers wrong. First of all, in how many ways you need to get it be right? which you could get your answers wrong. First of all, in how many ways you need to get it be right, right? Or you could make a mistake. A, miss some objects. And this could be pretty catastrophic if you miss that pedestrian, right, errors. The B error could be you could make a bounding box that is off, right? So for example, bounding boxes is off. Let's say you have a house like this and then your bounding box goes let's say that your bounding box goes like this so do you notice that it sort of is near the house but not capturing the house are we together guys okay and you could have other problems like for example you could have other problems. Like for example, you could have, what if you did two things? Suppose there are two houses. I'll take an example, another example. Suppose you have a house here and this is your neighbor's house. And then there is a neighbor's house and then there is a neighbor's house and what if it makes a bounding box that includes both of these that too is not correct because it identified two houses as one house right so that is another kind of mistake you can make right make two bigger bounding box. Wraps around. Two objects. As one. So you can see that creating bounding boxes is a little complicated. Not to mention that you can have classification errors. You might misrecognize a house for a car. There is also the problem that did you recognize the house correctly? Did you get the number correct? Did you get the location correct? And so forth. So for these reasons, people have traditionally considered object recognition to be a hard problem. And people have created all sorts of algorithms to do object detection. And most of these algorithms, they have a lot of, simply put, hacks built in based on experience. So people have found it's not purely a mathematical operation. You inject some prior knowledge, right? So for example, if you're looking at lungs and so forth, you sort of inject the knowledge that look here more carefully, look there more carefully, and so on and so forth, right? And then do object detection. So with all of these hacks or all of these tricks or prior information embedded in it, these algorithms tend to be very complicated. And so there is a data set called COCOA data set in which you try to do object detection and people benchmark it, how well you do. It's like ImageNet for classification. Now, there are different algorithms that achieve different state of the art. At this moment, the best one of them is the R, something called R-CNN. We know now what CNNs are. It's sort of an adaptation, but it has a lot of cooked up stuff built in. Over the years, it has gone through a lot of evolution and people were using the R-CNN to do object detection. So that is the context of this paper. What this paper did is it asked this question. We have two powerful mechanisms in the world. We have CNN, convolutional, which contain the convolutional layers. What do convolutional layers do? If you remember that it does, suppose you have a cat, right? Suppose you have a cat, what it does is it is able to do, so you have a few convolutionation layers as data goes through all of this at the end of it you get feature extraction and those feature extraction then you feed it into a few dense layers right dense a feed forward network and then you you get your classifier suppose you're doing a classifier you put your softmax and you get a classification so this is what we are familiar with isn't it case so far so good this is a review of what we did this week con one con when I say convalescent layer it includes your max cooling and your activation and everything con two, con three, et cetera, et cetera. After a certain number of convalescence, what do you do? You have done a lot of feature extraction and then you flatten that and you flatten and feed it into a dense feed forward network and then you get your classifier like your softmax and so forth. I have a question on the flatten. The three channels, how are they flattened? Are they all in one vector? Yes, yes. Imagine that you have, let's take this example of flattening. At the time, suppose you have 20 channels. So each image, let us say it is a 10 by 10. So 10 by 10 looks like this, right? Something like this. What you do is you start at this end too. So you start reading it row three comes here do you see how we are present right you put it and then row four comes here but then we have just done with one channel so now you go to the next channel right you go to the next channel, right? You go to the next channel, channel one. Now you go to the channel two and you repeat the process and then you keep concatenating all of those here, the end of it. So at the end of it, what will you get? You get a flattened vector, one dimensional vector. Yeah, like in some cases I've seen people show it as the width and the height are flattened and the channel is shown as the height of another vector of the vector, the flattened vector and I didn't understand why they showed you that. That is odd. That is really odd so maybe maybe maybe there's something I missed show me that thing and from that maybe I need to learn something so that's that all right so now what people learned is that see, convalescent is very good at feature extraction. Now when people used to do object detection, they would use something called and we will learn about it this week. There is a class of models, very good for auto regressive data. Regressive Data. regressive data models so let me just say auto regressive models they deal with data that is auto regressive now the word auto regressive needs some explanation what is auto regressive look at the stock price of one company or of Nasdaq, would you agree that it is like, you know, we can never predict whether it will go up or down, right? But we do know that it will follow some sort of a trend like this, right? I'm just making it some shape. So you agree that the value here depends to, especially if you think of it as moving averages, on a lot of value, a trend that has been happening in the past, isn't it? What you won't see is something like this. And then after this, the value is something like this. Very rarely do you find such catastrophic discontinuities, isn't it? Another example is weather. If you're looking at weather or temperature, just weather during the day, right? Not even climate, during the day, during the day variation. Would you agree that the weather, suppose it is 70 degrees now, 72 degrees, 74 degrees, 76 degrees. If I ask you, what is the weather here? This is very well predicted by the past values. What is the temperature now? You would say that, you know, let me look at the temperature trend that has been happening since morning and i can use that to predict the temperature at this point are we are we making sense guys and especially if you have a lot of weather data from the past days of temperature and you can see the cycle you can use all these past values to all the past values to predict the temperature here so what you're doing is you're doing regression all the past values to predict the temperature here so what you're doing is you're doing regression on the past values of the target variable itself but that is called an autoregressive data and the model that can do that is an autoregressive model an example is suppose another example is suppose you are writing a sentence it was the best now what suppose I ask you to predict what is it here you could say that it is off is a likely combination the best of times and so on and so forth right especially if it is our best listen let's say that I put it best of times is here and the next sentence goes on to say it was the worst of what? And you would guess that something like this would be, makes logical sense. You can predict, you can build a language model that predicts this. So when you would do that, these models used to be called autoregressive models. This data, completing a sentence certainly is an example of autoregressive sort of data. So that is what the word is. So what people used to do is an image segmentation. They used to bring a particular kind of auto-regressive model called or architecture called RNN we are going to study it this week recursive neural networks are amazing things the examples and then there are sophisticated versions of them GRU and so forth so for the longest time they were the workhorse whenever you got autoregressive data but there were articles written one famous article by Kapati is the amazing something to the tune of the amazing effectiveness or the of neural networks. People were amazed that it could do so many wonderful things. But then they also had limitations. They, for example, could not hold onto long-term context and things like that. So the next big breakthrough that happened in natural language processing, for example, was people discovered an architecture called the transformer architecture. Yes. Unreasonable effectiveness of RMM The unreasonable effectiveness of RMM, yes. so far in years. And then that got, then what happened is even though they were unreasonably effective, I suppose very effective, they were not effective enough in many real world situations and we needed to sort of break through to the next level. And then came architecture called transformers, something that we are going to cover this week just a little bit because this is the fundamentals workshop, but we are going to do it in great detail in exhaustive detail when we do when we come to that later in this workshop like as in in a month or so later so it's called the transformer architecture and it is based on something called attention attention there's a landmark paper that is called Attention is All You Need that completely changed the world. It came out in December 2017 and pretty much a lot of these fields that deal with this sort of auto-aggressive modeling, they would not be the same anymore. Transformers have largely taken over from RNNs in many, many areas of analysis. So transformers really are the big thing associated with them. You may have heard words like BERT and GPT. For example, recently the GPT-3 came out which is extraordinarily smart and big and seems to be able to do all sorts of amazing things. So that is the world of transformers. So what the gist of what this paper says is, suppose instead of doing the traditional way in which we were taking in a CNN, so the one invariant that people have is you need CNN because if you give it an image CNN will do feature extraction after that convulation layers are gone so the key not say CNN let's not say CNN, let's just say ConvLairs. ConvLairs will do feature extraction. But then instead of feeding it into all sorts of RNN and other things, what would happen if you instead did something, you brought two different ideas together. First is use a transformer. Use a transformer and obviously at this moment you don't know the transformer so what I'll do is I'll go over this paper at a certain level but later on when you know the transformer we will revisit the paper and do it in greater detail. So today I'll keep it a little bit lightweight and we'll keep to the big ideas that are there in this particular paper. So we use the transformer and then it uses something which is sort of a Hungarian algorithm algorithm and I'll explain what that is. So there's nothing very scary for matching bipartite graphs. Now this is partied. Now this by now begins to look rather scary. Transformers is something that looks scary enough. Now we have Hungarian algorithm for matching bipartite graphs. So once I explain to you, you'll realize that this is actually nothing very sophisticated. It's straightforward. So let me give you the big idea that this paper describes. But before we do that, let me sort of walk through the beginning of the paper and highlight a few core sentences in this so again before i do that this thing this new architecture which at this moment is very popular it R. So people also call it D E T R. It's a common word that has been associated with it. All the source code is available on GitHub and the code is actually surprisingly simple. What I will do is before I go into this paper, let us start by, let us start with the code itself. And I believe that you folks will immediately recognize this code. Where am I? This is it. So at this particular moment on my screen I have this code which I will sort of magnify a little bit so that you guys can see it is it readable on your screens guys should I make it bigger yeah yeah I think it is reaching the limits of what it will let me do. Okay. Yeah, this is quite good. This is quite good, right? So see, this code is barely 36 lines of code. Now let's see what we do and see what part we can recognize. Our goal is to see what we recognize. Have we gotten used to doing import torch? Does this look easy, guys? Yes. yes yeah this is straightforward right so we do that import nn dodge import and then that is you know that neural net a lot of the functionality of the neural net module etc is in that now this statement have we encountered this statement what are we doing torch vision dot model says dot a torch vision dot models what does it contain like pre-trained pretend models isn't it remember that it was the very first lab we did when we were playing with that golden retriever, some images that you guys did. And so it is made up of a lot of images. If you remember, we actually used ResNet-50. Right? So I see one related question here. So when we say import from Torch, import NN, it is importing a set of functions and various things and keeping it import resnet 50 how large is that object that's getting imported is it a it's like a bunch of weights and we're talking these are in the order of hundreds of thousands of weights that is true so what will happen is the moment you give that statement if that model is not there on your local machine under the covers that code will start going to the internet and fetching that giant giant model and these models are approximately half a gig uh resnet 50 is not a big that big in modern day you would call resnet 50 relatively baby but there are bigger versions of it. But even there, ResNet-50 is filled with a lot of parameters, thousands and thousands, I mean, millions of parameters, not just thousands, millions of parameters. So it will download all the parameters. So remember we said that you can save a neural network by simply saving the parameters, the value of the parameters, the weights and biases. So in order of gigabytes, right? Usually, ResNet-50 is I think less than a gig. It is three, 400 MB or even less because it's one of the smaller models by today's standards. So that is it. So that will come onto your machine. It's a pre-trained model. Remember, we started this thing with transfer learning. Remember guys, transfer learning. Transfer learning. What is transfer learning? You benefit from a pre-existing, a model that has already been trained. So you don't have to train it from scratch. So that was it. We started with that and so that is it. Now let's look at line five. Does it look familiar to you guys? What does it seem to be saying? When you say class data and you give it in brackets nn.module, what is it doing? Making a derived class, deriving from module module exactly and so this this is a classic signature wherever you see anywhere you know that somebody is creating a neural network isn't it it's literally declaring a neural network now this is the constructor let's look at the constructor the init is the constructor let's look at this constructor and see what it is doing. It will take self, of course, number of classes. Number of classes means how many objects there are that you want to detect, right? If you remember, we also used to take the size of the labels. You know, what is the output layer? You need to specify it in the and you can imagine that you'll need to specify it somewhere in your output layer see here it is in the very first last layer where you do classification what is the size of your output layer the softmax layer this is you remember guys you're trying to detect exactly the number of classes so if it is cat and dog it would be two so you need to know how many classes of objects right the output layer yeah and it will be usually helped by something like a soft max or something that that would be the activation you'd use but we'll leave that so and we will come to it or either softmax or sigmoid or something like that i will come to that so this makes sense hidden dimension uh well where is hidden dimension used this is something we use for the transformer and we'll come to it. Number of heads in the transformer. These are things that you use. These are all words that you use in the transformer architecture. Just think of it as a transformer. Whenever you build a transformer, it has these knobs. The four knobs are number of heads, number of encoding layers, number of decoding layers. And what they are, of course, this week you'll learn about it, but just think that a transformer has four knobs that you need to dial in before it can work. And those four knobs are these four knobs here. What is the number of classes plus one? Why plus one? Yeah, that is a very good question so so hold that thought in your mind okay maybe answer it right now see what happens is that suppose you you identify a box right in the image so suppose this is an image you ident you have a box bi that box may have one of the objects that you are looking for or it may not have any one of the objects that you are looking for it may have found a box around something like another class yeah and so you give it the virtual the null set yeah it is called a null in the language of these things. It is called a null value. So when you do sets, like in the mathematical theory of sets, when a set is empty, you say that it is none of these. You say it contains the null element. Null is typically written as zero slash. And that is what explains why you need an extra thing because it could be nothing there. You have made a bounding box, which looks interesting, but it turns out that it isn't one of the things you're looking for. And that explains it. So this is that. So now let's look at this line. I will highlight that line to see. The very first line that we do is we do this. What in the world is this doing? then a chop off you see the minus 2 here the the minus 2 here which is so I'll just write it because it was like this what it is saying is take all the layers except the last two layer take all except last two there. Why throw the last two layers away? Because in ResNet, the last two layers are fully connected layers. Remember when we did the CNN, a typical CNN we talked about is just now we talked about the convulation, convulation, convulation, right? C, conv, one, conv, two, conv, three, etc. And then finally you flatten it and then you feed it into dense layers, fully connected layers, right? FC1 this out this part out and you just want this result right so when when you do that this is very convenient you take a pre-trained model and you chop off the head a little bit right and once we do that then what you do uh then this is the flattening part you just feed it into a corn layer actually might as well remove the yellow thing here so that this shows what it is saying is that you're taking a 2000 a convent into then you're saying how many hidden dimensions that you want whatever the hidden dimensions are you of the hidden dimensions go into what it is in the transformer so it needs to be fed into the transformer you feed it here and then what you do is you create a transformer on the one hand but at the same time you do something else you create two parallel tracks one track so let's say that you have reached this far after that one track goes through the transformer and another track goes straight into a classifier can you guess what the classifier will detect what is it meant to detect our labels the labels exactly the class labels it will detect whether it's a bird it's a cat it will detect whether it's a bird it's a cat it's a dog it's a house whatever it is so so so I say instead of transformer did you mean like regression like one is classified another is the regression instead of transform that's what the transformer will do okay so if you look very carefully at this you have taken this output and you are building a linear layer for classification this is for your classification. And you're building another linear layer of dimension 4k. Why do we have this dimension 4 here? Hidden dimension 4? Four points of the rectangle bounding box. The four dimensional, the bounding box is given by the four dimensional vector. And so what do you want? You want a linear layer. What is a linear layer? It's just a dense layer whose input dimension is whatever hidden dimension came out from the con and all that. And then you want an output of four, right? Then comes something, this query vectors is something for now we can sort of ignore. I'll tell you what they are in a little bit so guys is this code looking familiar if i erase out the transformer will the rest of the code and i erase out this parameter code 17 18 19 if i erase out and i erase out 13 14 the rest of the code would it look familiar to you guys uh yes like c for 10.5 and so the rest of it also now let's see how how are we stitching it together remember in the forward we specify how the message passes so they're calling this backbone the rest network so i said the backbone the 11 so is it for fine tuning later on or no no we're dropping the layers that does feature extraction oh okay right so it is the feature extractor so you take that feature extractor and you so this layer when when you apply the backbone which is essentially applying resnet without the dense layer heads what do you think this is doing 22 is doing it is doing feature extraction then on the feature extraction you get this hidden layer which is which is putting it through a corn layer to flatten it now that you have flattened it what you do is you can then shape it you know the shape of the height and the width of the bounding things like that you can get the rest of it position embedding this part is a little complicated i'll just ignore this part for a moment but then observe what you do you feed these things the h here you are feeding it into a transformer now transformers are objects that take two things they take your input data and they also take a form of something called position encoding, which actually it's sort of, we won't get into it. It gets a little bit technical until we understand transformers, which will be a month later from here. But so at this moment, don't try to understand every aspect of it. But the most important line that we should understand at the end of this whole story is this. This guy. Now let us see what is it returning. Do you notice that it is returning you two objects? It's returning you one is this object and the other is this object. When you get a sigmoid, the result of a sigmoid, what does it tell you? When do we use a sigmoid activation? Classification. So classification, this is your classification layer classifier this is your basic classifier or class label detector or object label detector object that is a bounding box oh you're right you're right it is a B box H and yes you're right this is here this one is here you're right so this one is here and then you're taking this linear box and you're feeding it into the sigmoid okay I need to think about why they put the sigmoid. Okay. I need to think about why they put the sigmoid here. It's between zero and one, right? That's right. Oh, that is right. You're right. Because you want the values to be between zero one. That's a very good observation. Thank you. Thank you. This is it. I said, one thing I don't understand is why is it linear? I said one thing I don't understand is why is it linear? See linear is just a word in the world. See in a Python, in TensorFlow and Keras, they use the word dense and Python just uses the word linear. It doesn't really mean linear. It just means a dense layer. Are we together so if you create a very dense layer of a dense layer in which everything is connected to every other thing right like this let's say at this example right this is an example or something like this is an example of a dense or linear. Keras, they will use the word dense. PyTorch will use the word linear. So because the last layer is linear layer, at the end of all of these architectures, you put these dense layers. So the feed forward, let's use this word fully connected layers whenever you encounter fully connected layers it is just a convention to name the variables or the layers with the word either linear or dense based on which library you're using that's all it means awesome question you also mentioned that transferent formers taking two things i kinders take in two things. I kind of missed the two things you quickly mentioned. Yes. See, what they take in is, we'll come to the, so I'll just give you a little bit of a preview of a transformer. See, a transformer is made up of, well, okay, at this moment I'll make a black box but they have an encoder layer right and they have a decoder layer and actually they have one more encoder here but let's think in terms of encoder and decoder so whenever you give it something that has let's say you give it a thing like the brown the the fox jumped over something i don't know over the fence i don't know what the exact i forget that exact sentence the quick brown fox jumps over the lazy dog that's right jumps over the jumps over the quick brown fox jumps over the lazy dog so then what happens is each of these words we do something called word embedding word embedding and that's a topic in itself we'll cover this week word embedding converts a word, let's say the word quick, to a vector. That vector will be typically a 300 dimensional vector. Now why in the world would you do that? We'll learn about it this week. But just assume that it does that. So now what will happen is each of these words will become a vector, but there is a problem. In a sentence, each of these words will become a vector but there is a problem in a sentence it isn't just the word that that has information it is the location of the word and that is that also matters isn't it for example if I interchange quick and lazy the meaning would change it would become the lazy Fox jumps over the quick dog isn't it so the location of that or the position of the word also matters position of word also matters and so when whenever you send in a word you need to give it its word vector and in some sense a position vector people in literature they often use the word pause for it pos to specify that you need to somehow feed in the position of the word also and then you have given the word and its context in the sentence right so that's where the word position comes from and that's do they use norms for position do they use it like mincoskey distance no no no no they use something uh something quite different we'll come to that okay uh we'll come to that at some point but let let's keep that for later, guys. We have a whole month to give to these concepts. At this moment, I'm just giving you a very quick teaser of making it work. So guys, if you look at this code, even before reading the paper, you can make some head and tail out of it, isn't it guys? Are you able to make something out of it isn't it guys are you able to make something out of this this code and see how simple the code is and yet it and there's a little bit more to it for example the transformer that you have it is not the cook the built-in transformer that comes with PyTorch what they have done is they have simplified they have they've changed it they have removed a couple of things from it. So there is a bit of a code change and so on and so forth. But fundamentally, it's a very straightforward thing. So you have a neural network, you need a loss function. That loss function they use is the Hungarian loss function. We'll come to that in a moment. And then you're done. And yet it's a profound breakthrough in many ways. So now let's go back to the paper and see what is it trying to say i said what was the hidden dimension it is just the dimension that you will feed into the transformer remember like when you feed a word a 300 dimensional word it is 300 dimensions in the same way when I image comes out and you feed it into the transformer. What is the dimension of the image. That's it. So you choose a dimension. I don't know 2000 right it's up to you. Right. Yeah, go ahead. When previously you said that con, the con, CN, con-relational network layer only will classify, right? Again, why do we need a classifier with a regressor? No, I didn't say that Harini. I said that the conditional layers will do feature extraction. They will extract good features. Remember, what do con-relations do? They detect the edges, they detect horizontal, vertical horizontal vertical edges diagonals and things like that so they feature detection after that we do object detection is it yeah so once you have the feature it is a part of the journey to object detection so when we want to do object detection now look at what I said a little while ago in object detection and let me decrease the font the size a little bit yeah remember what did I say the full object detection needs many things to work you need to be able to tell that this is a house this is a tree this is a duck and you need to put boxes around it and tell what it is. Where is it in the picture? Where is that house? Where is that tree? So in other words, the full object detection is much more complicated. It needs you to produce this. I don't know. Can I, let me take that thing. It needs you to to produce this do you see where I'm highlighting with my pen both the label what is it it's a tree and the bounding box around it and a convalescent layer will not do that it will just do feature extraction for you which you can feed into dense layers if you want to do the classification to get this part out of it if you want. Are we together now? Yeah. So now let's read this paper. It's very interesting actually. The core idea is quite simple, which is why I chose this paper. So they go and they talk that this is a huge improvement. It is also a simplification of architecture till now object detection seems to be rather hacky and now it isn't so the the important thing is to look at this image let's look at this image let me be a little bit better see here is the thing so they take the example of two birds right there are two birds here uh they look like seagulls to me are they seagulls any bird specialist here i'm really bad with birds yes those look like seagulls they do look like okay so these two now the question is how do you detect this now do you notice that there are many interesting features that we may not be interested in for the question is, how do you detect this? Now, do you notice that there are many interesting features that we may not be interested in? For example, there is this feature. Do we care for that? No, we probably don't, right? So, but what we care for, what we would ideally like to see is this. So suppose there is training data, it would probably contain two boxes like this, with labels. So let us say that you get this training data. The training data will contain this. Let me call it one and two. It will contain bird bounding box one, this location one. The other again would be bird, two birds to specify that there are two birds bounding box two. This is B2 and this is B1. So would you agree that a good labeling of this image, somebody, some human being who has sat down and annotated this picture, this is what he needs to give as labeled data for this image. Now what happens to this, if you feed this image into a convolation network, remember this is not a full network, it's just a few layers to do your feature extraction. So what comes out here? A set of image features come out here, right? Those image features, you pass it into this transformer encoder decoder. I just use the word transformer and what the transformer emits Asim Kadavig- And at this moment, you may as a first approximation. Think of it as magically or mathematically somehow it it outputs. a set of predictions, of boxes, these boxes. So input, I have only two boxes, but you take the transformer and you decide how many objects you want to detect. And that is hardwired. So suppose you say n is equal to 50. So what it will do is in every image, this combination, this architecture that is here, it will always detect 50 objects, no more, right but there is a problem of course in this picture there are only two birds you don't see 50 birds isn't it so how do you reconcile the two that is the main thing that we will deal with so what happens is that suppose it has let me take this example of three i'll take an example of n is equal to three suppose n was three what it would do is that it would take do you notice this picture they have shown one duck one bird two bird and then a green box and what is the green box pointing to no object are we we together? Right? And so therefore, the way you work with this is quite interesting. It is like this, and this is where the word bipartite graph and other things comes in. See what is labeled data for this image? The label data is labeled data or ground truth data the ground truth that images let me take okay we need to agree on some value of n how many boxes we put let me just put four boxes four boxes so a ground truth is that there are one box for bird one, bounding box one. There is another, and it contains the word bird. And then another bounding box says bird, and it gives you the location of the second bird. And now your machine has come up with with four boxes box alpha box beta box gamma and a box delta right four boxes the problem is you don't have four boxes here so what you do is you take the input data and you pad it with two extra boxes. Here you put no object, right? And the location doesn't matter. And another box, you give it an empty, again, object, nothing here, nothing here, right? Doesn't matter what it is. Now what you do is, this is some label. So let me just call it C alpha and bounding box alpha. C beta, bounding box beta. C gamma, bounding box gamma. C delta, bounding box delta. You have four boxes and now you ask this machine that C do a loss function by doing this bipartite graph matching that sounds quite like a mouthful, but it's simply bipartite graphs are graphs or edges between two types of objects. So here are the two types of objects are the ground truth versus predictions. These are predictions. So you're going to link each of the ground truth boxes to one of the prediction you are going to do. And when you do this bipartite graph, you have a one is to one matching. A prediction cannot be associated with more than one box in the ground truth. And two prediction boxes cannot point to the same bird right so what it means in practical terms is see the mistake that you want to avoid is suppose you have four boxes four boxes are like four people and suppose you have these birds let me just make this birds like this. This is my best attempt at a bird. And let's take another bird. Forgive my stick drawing Right. Suppose these are the two birds. What you don't want to do is you have four boxes and each of them says, look, a bird. This one says, oh, look, a bird. And this one also says, hey, look, a bird. And this also says, look, a bird. And this bird says, what about me? Nobody is paying attention to the second bird. You realize that would be bad. And so the way you avoid it is by saying to this system that see each it can go to let me just call it one two three four you need to collate it you this can for example point to this this can for example point to this right this is the association between these two these two are associated right in other words the first boundary the first prediction box better say that there is nothing there the second one let us say goes to this right second one also says there is nothing there the third one points to this let us say right and i'm taking a this is all random it doesn't matter what order in which it is but what really matters here is that each box there is a one-to-one pairing with the label date with the label bounding box are we together guys right so when you do that the important thing is that the the loss comes from multiple things. The loss comes from, now let's think about where the loss comes from. First of all, you can misclassify a class, right? You can identify the same bounding box. Alpha may point to, let's say that gamma is pointing to two but it may say that you know what but it's a dog or something like that the labels may not agree so there is a miss the loss comes from misclassification from mislabeling and it also comes from wrong bounding box. In some sense, wrong bounding box. The sizes do not match. But because these are numbers, this is a little bit more like regression kind of a loss. This is more like a classification counter for loss. And there is an algorithm that somebody has created, a loss function somebody has created. It's called the Hungarian loss function. And that loss function existed before this paper was written. This paper sort of adopts it and uses it for this situation. And when you do that, all you have to do is make sure that there is the best possible matching between these are we together and that is what this paper is all about now it has many advantages first of all you won't have this situation what about me because if one box goes and detects bird the first bird the other box cannot so in the training process it will say oh you know what bird one is taken i better go look for something else right some other feature in the image and some other object in the image so the other guys the other boxes here will be forced to go search for some other images in the in this picture right and that is essentially what this paper is saying at a high level right now when it gets into the technical details there is more to it we won't do because it needs a bit of an understanding of uh transformers but roughly speaking one way to look at it is okay we won't get into that. What basically it says that you can think of it as different elements, each of them saying I'm going to focus at the lower one corner of the image. And so you go and focus on the other corner of the image. This is focusing in the center of the image, center of the image. And the other guy is saying, I'm going to focus in this region of the image and so forth. So they're paying attention to different aspects of the image and the other guy is saying I'm going to focus in this region of the image and so forth so they are paying attention to different aspects of the image and where to pay attention to is also a learned thing the neural network is learning it right so that is what this paper gets into this is the famous transformer architecture right this is those of you who attended my transformer talk will be remembering that. This is the encoder piece, you know, the input goes in. In this case, an image feature goes in coming from the CNN and it goes out and gets fed into the decoder part, but the decoder also gets input from another encoding, the so-called object query. In the traditional attention, the original paper, this would be like English language text and this would be like the previous French words that have been produced, French so far. And this and this combined would get decoded into the next French word. So it was a very sequential operation, but nowadays when you do transformers, you can do it in parallel. That's one of the advantages of it, massively parallelizable and so high performance. I won't go into the transformer architecture yet, it's for a future date, but we'll do this architecture in great detail in this particular course. But see this particular thing is not perfect at this moment. People have looked at it and they said how well does it perform so that means how many objects does it miss and so it is quite interesting if your number of objects if you look at the standard data set do you notice that so long as the number of objects is small look at this region if you look at this region and let me blow up my screen a little bit is it visible guys do you notice this picture that's it's much better up to 40 pictures then this detection transformers, they do very well. The error rate is close to zero, isn't it? This is zero here. Let me erase this part here. You can see that the error rate is very small, but after that, if the number of instances increases, then then it says keep on cruising there they rapidly increase the pointer that this is a bit of a limitation at this moment and I'm sure in the subsequent time people will now start researching how to make sure that it does well even when there are a lot of objects so this is a result this is pretty encouraging to see most of the time you're not looking for 40 objects if you're looking at the road ahead of you it's just very unlikely that 40 people are standing right there in the center of the road or things like that right so it is still very good but there is scope for improvement and they'll improve upon this so that is that guys that is all that this paper is about there are some technical details uh which you will understand once we do our transformer just as now you know uh this thing you know as now you know this thing, you know the congulation neural nets, so at least the connet part of this architecture looks straightforward because you know it, right? But the transformer part doesn't look so straightforward because you haven't learned about transformers yet. So guys, this is in summary what the paper is about. Its main breakthrough is that you don't know how it was done in the past, but let's just assume that it was a very messy way of doing it and sort of a hacky way of doing it. And what they are seeing, the main advance that they're seeing is that you can now do it without hacks. You can do end to end in an elegant mathematical manner, take a congulational network and feed its output, congulate, feed its output into a transformer. And when you do that, you will get a very, very good object detector. In fact, it matches the state of the art performance, which is the F CNN, which is really, you know, with a lot of hacks, people have improved upon so uh the in the very first version of this paper in this implementation they already match the performance of the state of the art and they beat a lot of other algorithms in different areas right and the idea is that in the in the coming months they will certainly go and be rcnn so it's just a new technology that has come onto the horizon. This paper was actually published in May 2020. So as you know, just a few months ago, and it's quite a bit talked about. So that's all I would say at this moment. Any questions, guys? So I had a question, Asit. It seems like for that matching, the model can only recognize or create bounding boxes and classify images for our training data, which includes those number of images. For example, there needs to be a training image with two ducks in it uh if then if it's just one duck uh the training data only has a image with one or three ducks in it it'll be difficult for no no that is not true in a sense it needs to in your label in your training images there must be ducks so that you have a label for ducks but you don't need to have in the training images two ducks so suppose you feed it another image which has seven ducks it will go and find bounding boxes around the seven ducks the point is it needs to know the ducks that's all right but then how will it match one to one with the reason for that is the number of bounding boxes that it starts out with is typically like 50 right it is looking for 50 objects now suppose your picture has seven ducks so your label what it will do is it will detect seven of the ducks and the next 43 boxes will just say null right go pick random spaces empty spaces in the image and mark it as null so based on the training it can classify the ducks so they'll look for seven ducks and then yeah exactly so the thing is you may have trained it on only one duck. Most of your images may have just one duck, but it has learned to find the duck in a picture. So if there are many ducks, it will find all of those ducks in the picture. Right? That's a beautiful. Yeah. Yeah, like your example for the bipartite graph, it actually had two ducks in it. So I thought maybe 22. No, no. Suppose your n, the value of n is 50. So what happens is it needs to fill, it needs to just put 50 bounding boxes on the image. So it will find 50 interesting features in the image or 50 regions of interest in the image. And then it will say, which of these is a duck? And two of them, hopefully it will say is a duck because the actual image had two ducks the ground truth had two ducks and the rest of it hopefully the next 48 will just say null so that was my question if the ground truth was not ground truth as in like the training data only has one duck then obviously two lines can't go to that one suppose you have a training data right it has only one duck what will happen is think about it this way your algorithm is not only recognizing this it has learned to pick pick a bounding box around it isn't it right now you realize that if it can find a duck and put a bounding box around it, isn't it? Now you realize that if it can find a duck and put a bounding box around one duck, it can easily put around arbitrary many ducks, right? If you have another image, a bigger image with more ducks or even overlapping ducks, it will detect all of them, right? And put bounding box around it. So the number doesn't matter. The important thing is, can it precisely determine a class and its location? Bounding box is essentially the location, right? Right. So that is what matters. So long as it can do that, it will succeed any number of it. So what will be the loss function for this? It will be... The loss function is made up of two parts. One is, did you misclassify? So that looks very much like your cross entropy loss. Remember your cross entropy logs, log minus log PI, the probability that it is that. And then the other loss function is associated with the bounding box, bounding box location. is associated with the bounding box bounding box location right so suppose the real bounding box was here and this bounding box is here real versus predicted there is a gap between the two isn't it so you can have another sort of a loss function associated with the bounding boxes. And that's what it is. There's a bit of detail. If you read through this paper somewhere here, it goes into the loss functions. And I'm not doing that because, you know, you guys are still doing the fundamentals of deep neural network. So it may become a bit heavy, but trust me, as we go through the next few months all of this will become see look at this what is the first loss function doing in this image guys classification isn't it and is it your familiar cross entropy kind of a loss looks very similar to your cross entropy loss isn't it yeah yes and what about this guy this is the loss function of the misalignment between the box and its prediction quite literally. Isn't it? You're comparing the, uh, the box location and the predicted location of the box. Right. And the loss function is like, how much do they diverge from each other? That's that. So a question asks if asked, if the images, so a lot of these boxes I've seen, they're all, you know, the orientation is very, it's orthogonal. Like there is no diagonal. Yes, yes, yes, yes. And that is an aspect actually with image segmentation you some people i don't know maybe they have ways to do orthogonal i mean non-perpendicular non-aligned with x y axis boxes but traditionally you just make a bounding box which is x y axis aligned so sir when the object is moving then how the location is precisely decided? It's very simple. A video is made up of many frames of static images. So many frames are there. If you run a 50 frames per, like 30 frames per second to the human eye, it looks like motion. So we just made up of a lot of frames each frame is an image that's that asaf because because you mentioned that there there are two things that we're looking out here the classification and the bounding box if i have uh let's say if i have some images that from the from the ground truth that we used for training but i rotated them and i put them in the test will that overfit no it does very well actually because see if you rotate a duck it will still find a boundary it will recognize the duck and put or put a bounding box around it and it comes from the property of cnn remember cnn and this was one of the questions i gave you in the quiz it is translation invariant it's location when it does detect a feature or roughly speaking when it detects something it doesn't matter whether that thing is the top left-hand corner bottom corner is upside down right right rotated and so forth it will detect it right so that's how i asked sorry what i was saying was if i if let's say let's say i have one image of that of the duck uh in the test yeah and then i wrote in it but i put i assigned it to the training um will that overfit will my model overfit because um we're looking at two things now the bounding box and the classification I don't think I understand your problem let me try to understand the question you're asking I think it's more like data augmentation right yeah yeah yes you're saying that suppose I have let me get this idea right suppose you have a duck right yes limited this is one and you create another data which is like this duck pointing the other way around like this right this is the second image of the duct let us say so can i feed both of these into training is that your question no no uh if one if one got assigned to training and the other one got assigned to... This is for training and this is for testing. Will the model overfit? Because we're looking at... The overfitting is a specific thing to do with the training time, right? Models overfit during training. During training, it's not even seeing your test image. Models overfit during training. During training, it's not even seeing your test image. And in fact, if you feed both the images during training, even better because that is your data augmentation. If both of these were in training. Otherwise, there is no... See, overfitting is not affected by what you're hiding in your test data. Because during training, the algorithm does not see the test data. You hide it under the pillow. All right. So a more practical experience in project management is, you know, how do you work with the team? So it's a cat and mouse game. Suppose as a, there's a very interesting thing you may be observing in your companies if you ask an engineer how much time something will take he'll always pad up his estimate suppose it takes two days work will give you a week's estimate or slightly more depending on your personality some people are aggressive and say I can do it in a day and they won't be able to do it they'll do it will take them two days some people will look at they know that it it looks like two days work they'll give you a week's estimate so what do people do on the business side they'll say oh you know what we absolutely absolutely must release it by december right now everybody will run around and say, then engineers will come back and say, well, we can give you only three features out of 10. And then you negotiate that three won't do, we need at least six features or something like that. So the whole thing you negotiate, and then finally people come to some compromise and then they go about implementing it. Except that of course, there is a padding on the business side also in the negative direction but they didn't really need it in december they actually needed it in april but they'll never tell engineering about it they'll say we absolutely need it in december are you guys familiar with this Yeah, so it's like that. So what is not told to you, right, cannot control your behavior. So if you really didn't know that it was due in April, and so you have to take it as a fact that it's due in December. So the engineering will work hard and try to fit it into December. That's like that in training and machine learning also. If the machine learning, if the algorithm only knows the training data, to it, the reality is the training data. It will fit to the training data. It doesn't know that there is another data set hiding in there because it can't see that. Remember, in more practical terms, the loss function, you do epochs over the training data and you minimize the loss. You don't see the test data. So as of here, when we evaluate for object detection, it's better to have all the images put into the training. We don't need to have a separate pile or stack or bin that will be used for predictions in the end? No, you do that. You always do the separation. You have a lot of label data. You will give, let's say some proportion of it for training. And then how do you test that the network has not simply memorized all those images, or overfit to those. One easy ways, now you bring out the images that you hid under your pillow, and then you feed it to the algorithm, to the model, and see how good prediction it makes on the test data. Right, right, Asif. So if one of them was just a flipped version of a test, of a training, it won't overfit. I won't It has to do with overfitting. In fact, it will be pretty happy. Because it will be very similar to what it has learned from Any other questions guys? That's a good question. Transformers are generally used in NLP and text based, right? So here they use it for object detection. So that's a different. Yes. The interesting thing is that you're right. Whenever people think transformers, especially when you look at the implementation of the transformers they are thinking bird you know they're still bird roberto long and all of these long format and so forth gpds one two three but no transformers are applicable beyond beyond words transformers are also applicable beyond natural language processing. For autocorrelated data it's a no brainer. For example, you can use it for prediction, time series prediction. You can use it for other things. Now what they are saying, what these guys are saying is that transformers can be used for image recognition, object recognition. And it's a very interesting idea that you can do that because we know that transformers are really, you know, before transformers for all sorts of data, it used to be just recurrent neural networks. But with transformers coming, RNNs are lesser used in word, natural language processing. There are still good use cases for it, but they're not used everywhere. The transformers are used quite often in many of the situations. And what this paper, for example, highlights the fact is that you can use this transformer for image and object detection also. And it's a lovely thought. Sir, I have a question. for more for image and object detection also and it's a lovely thought sir i have questions so in image processing models say training data training has been done on a lot of images but there is totally new object uh so once the testing is done it will identify the shape of object object, but how will it name the object if it has not? It is null because null for all the things that are not in the classes that are valid. I see. That it doesn't know about, but still it will know that it is object, but it just doesn't know the name. That is right. And you know, this is not just neural networks. In philosophy, there is this question that is right. And you know, this is not just neural networks. In philosophy, there is this question that is asked. Do we not see aliens, for example, as a hypothetical question, as a thought experiment, because they are not there? Or because the human brain has never been trained to recognize aliens? because if your pain is like a convalescent network let's say or a classifier it simply won't recognize what it hasn't been trained on so sir extending the same argument say then what is the point even if detects some new objects? Because it cannot name it. It will just mark it as an interesting feature that happens to not be one of the classes. So the bounding boxes, in some sense, they look for something interesting in the image, right? They throw a bounding box around. So it might do that and it might just say it's null. And you might want to go and check it out. So it does process only, it does identify accurately what it has seen in some form, basically. What the labels are in the training data. And remember, categoricals, you have to specify all the classes, right? So for example, if you're cows and and ducks if you have taught the child only how to recognize cows and ducks now you bring in a zebra and the child will look at it and the best it can say is maybe a cow or otherwise it may say no no no I it's not a cow I don't know what it is so the learning is not basically at the full-ledged learning. It is just the improvisation of what it already knows. Absolutely. Learning is from the known, what it knows to recognize. All you're doing is you have trained the machine to recognize things that you have shown it in the past. I see. And that's why these image models are huge because they try to, their testing data is basically, oh sorry, training data is basically everything we know about. See neural networks, no that's not the reason. Neural networks have a lot of parameters you know but they're very, the huge parameter space. You realize that the number of right this is in millions and now billions right you g3 well okay not gpd3 with that is for natural language processing but some of these neural architectures ever bizarrely large number of parameters so the more the parameters the more you need to feed in data so that you don't over fit correct thank you sir it. Correct. Thank you, sir. Anything else, guys? Whether it wasn't a useful this paper? Absolutely. Oh, very. It helps get dive in together with your Yes. population. I thought this would be a good way to showcase how it is being used. And also transformers are something that I'll at least talk about this week. So it's a good segue into it. This week is natural language processing. By the way, what do you guys feel? And I'll stop the recording for a moment now so that we can be frank. Thank you.