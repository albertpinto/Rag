 It's had slows us down. Okay. All right. I'll do it. Or what I can do is not pause it. Just let it run all the time. Those of you who are remote. Okay. All right. I'll do it. All right. What I will do. Now is restart where we left off. But before I do that, a quick a couple of quick checks. So you guys are able to hear me in the classroom clearly because of the amplification. Those of you who are remote, are you able to hear me clearly? Yes. Is the screen clearly visible? Is it legible what I'm writing? Yes. Yes. That too is is here thank you for the feedback and are you able to see me clearly or am i completely out of the screen out of the camera no we can see you but if you could move it so that the doorway behind you is not in sight that's a little bit better isn't it hang, let me take care of that. Yeah. Why not? Sir, another thing, can you lower the camera a little so we can see, we only see your head moving this side, that side. Oh, my head bobbing up and down, eh? Yeah, so if you could make it a little lower. Yes, let me do that. So we can see a bit of your, yeah, a little lower. Sometimes when you show the bowl examples, we are unable to see because when it's touching your hand, it doesn't cover the bowl. Oh, yeah. Oh, thank you. Thank you. Yes. Okay. How about now? Are you able to see me more clearly and yes i can i think the thing that you're saying is turn you away from the door now you're not seeing the door i hope no you're not it's good now okay and now let me see where I kept my writing pen. Here we go. All right, guys, so we will start. So this class of functions that have this shape of a stretched out S are called sigmoid functions, right? Now, there are many sigmoid functions, but amongst them, for classification, by far the most commonly used is the logistic function now this word logistic is rather strange logistics when we talk about we think in terms of trucks and shipping and whatnot isn't it the logistic companies they are that they are the trains and the trucks and the shipping and moving things here and there, and the logistics of a project and so on thing. So what is a logistic function? It has its history actually, surprisingly. It is in, I believe it had something to do with World War I, I forget what, but it has a military background, or maybe even before that, it has a military background. It was for the moving of food and other things to armies in that context it became famous but it's a simple function and we'll i'll motivate it with a different with a way in a way that i hope you will get it easily so let me go back to the simple decision boundary so i will remove everything else just draw the decision boundary. So I will remove everything else, just draw the decision boundary. And I will say that for a point x, this is d of x, right? Simplified way. You realize that d, now let's look at the characteristic of this function. We know that when x, very large, p of x, probability that it's a cherry is equal to one approximately when dx is very large negative dx is equal to zero in other words d probability associated with this with d let me just make a column here and tell me if this agrees with your intuition. Suppose I make a column here, d of x, and this is p of x. When this is infinity, d is infinity, what is px? one when d is minus infinity what is p zero zero when d is zero what is p 0.5 so now let's build through this intuition of x. This direction is p of x. Right. This is 1. Right. This is 0. So with this intuition in mind, let's think of what you go. What you see is something very interesting. When you look from a mapping from D to, it's very simple actually, when you look at a function, think of a function, F, that maps from D to P, right, to probability. Literally, that is the P of D. d so actually let me see that this function p itself is mapping to this p of d so it is mapping minus infinity to infinity this interval it is somehow able to map to zero to one isn't it guys it is able to squish an infinite thing it is somehow able to map to 0 to 1. Isn't it, guys? It is able to squish an infinite thing. It is able to squish it down to 0 to 1 in such a way that you have a smooth rise up, isn't it, in the y direction. So let's think what function can do that. First of all, let's solve half the problem. And I'll motivate it to you. Suppose there is a function that goes from zero to infinity. Look at this interval, something that goes from zero to infinity. Now, anything that's defined in the domain 0 to infinity, what happens if you take the log of x? Log of x will go from where to where? It has to be an open boundary on the side. You have to have an open boundary on the side. You have to have an open boundary on the left. Yes. It will go from minus infinity to infinity. In the asymptotic limit that it goes to zero, it's minus infinity, right? Log of x would go like this. Now this begins to look very promising, isn't it? Oh boy. Maybe we need to somehow bring in a log of D somehow, isn't it? Log of D, but there is a catch to it. D goes from zero to infinity, log will go from minus, so minus infinity, if you go like this direction, it is easy. If D is the log, it's like this, right? So you realize that whatever it is, this is the relationship. Now, how do you bring something that goes from 0 to infinity to 0 to 1? Suppose you could do one more step and you could go from 0 to 1, right? So this is the progression, guys. Imagine that what is it that we need to do could go from zero to one right so this is the progression guys imagine that what what is it that we need to do to go from here to here all you have to do is suppose this was x right or this was d you know that you could do e to the d it would go from zero to infinity isn't it what is e to the minus infinity zero to infinity isn't it what is e to the minus infinity zero zero so this step will take you there what is that operation e to the d let me write it here what is that operation that takes zero to infinity to zero to one yeah so there is something very interesting, actually. You can go sort of backwards. You can say that if I want to go this direction, there is a common word that people use. They call it the odds. Suppose you have the probability of success is probability that your team is going to win a particular football or soccer or cricket or whatever it is. The probability is 0.7 what do you call the odds have you heard the word odds the betting people don't use the word probability they use the word odds what is that that is the probability of success over probability of failure right that is 0.7 over 1 minus 0.7. That is 7 over 3, isn't it? So they will say that there is more than twice the odds that your team will win. Are we together? You say that there is more than twice the odds that your team is going to win, right the word for odds is odds is probability effect of an event one minus probability of that event odds goes from what odds can be zero to infinity odds is zero to infinity. Do you see this, guys? Right? And so suppose your probability of it being strawberry is this. Right? Then if you write the probability of it being strawberry you convert it to odds odds will become what zero to infinity you go this direction odds will take you here and what takes you back to this zero to infinity how do you convert to minus infinity to infinity by taking the to infinity by taking the log of it. So log odds. Log odds will make it what? Log px, 1 minus px. This will go from what range to what range now? This will go from minus infinity to infinity, isn't it? Do you see this, guys? I'm saying very simple thing. If probability goes from 0 to 1, odds goes 0 to infinity. We are halfway there. We just need to bring in minus infinity here because the distance goes from minus infinity to plus infinity, isn't it? I take the log of odds, and now I'm looking at one transformation of probability that has the same range as what? The distance function. Do you see this mathematical trick? It's a simple mathematical fact. So we say, what if, what if, and so we are at this moment trying to invent a theory, right? We say, what if, what if we pose it what if we posit that log odds of px of the probability of x which goes to which actually belongs to minus infinity infinity is basically proportional to or y proportional let's just say it is the same as is equal to the distance function up you can put a proportionality constant but it doesn't matter right what you you realize that this also goes from minus infinity to infinity so what if you pose it that this is true we could could do that, right? It would make sensible. Think about it. When you're really far in the positive direction, probability is close to one, d is infinity, and log odds would also be infinity, isn't it? When you're literally d is minus infinity, right? Probability is zero. When probability is zero, log odds would be minus infinity. So it all seems to agree. And if you make this statement, you have essentially discovered, rediscovered, I should say, rediscovered logistic regression right in fact this is logistic regression this is logistic actually logistic equation and if you take distance to be the d to be the distance from the decision boundary in fact this is this this becomes regret This becomes regret equation. So let's write it down. Log of natural log, by the way, log is always natural. P of x, 1 minus P of x is equal to the distance from the decision boundary, dx. By the way, distance is often also written as Z. Right? Some literature often, often, this is not often, often, often, ML textbooks. So, but I'll call it, this is actually, so now what is, now the same equation, hold your thought one second. If I write it, if I do the inverse, guess what it will be? P one minus PX is equal to E to the D. Would you agree this is true? Right? And now let me make it into a more traditional form i could say p of x is equal to now if you just work out so what you do is you add px to the denominator so it will become e to the dx 1 then of course this is one plus e to the dx now divide numerator and denominator by e to the dx it will become one over one plus e to the minus dx right or also often written as so probability of x is equal to one plus e to the minus dx often also written as 1 plus e to the minus z are we together it is often written as that this is the logistic regression but it's it solves one part of the problem the second part we still have to work out is what in the world is d in terms of our you know x1 x2 axis right The part that remains to be solved. But before we go there, guys, is this reasoning simple? You saw, we just made a theory. Hey, you know what? I need to convert probability into something, somehow do transformations to probability, such that its values go from minus infinity to plus infinity. Why would I do that? Because D goes from minus infinity to plus infinity. Why would I do that? Because D goes from minus infinity to plus infinity. Do you see that? If I could do that, then I could posit a theory. I would say, this is the same as this, right? The transformation of the probability is log odds. So log odds is the same as the distance from the decision boundary, right? That is the basic idea. That is all there is to it, guys. So I'm giving you this intuition because most I don't know how many textbooks actually explain this like this. Usually most machine learning textbooks give posit this equation as a God given fact. Logistic regression. This is the fact. Right? So I thought I would explain where all of this magic comes from it's a very beautiful reasoning it leaves one last topic there distance function d of x we still need to evaluate d of x what is it isn't it we found the X. So we are going to do that the next time. But before I do that, guys, the odd logs business and probability, is that self-evident? Is that easy? Right? Now we'll just solve this last problem. So let's look at this. We have this, where goes our decision boundary there we go here is our decision boundary decision boundary now let us take an arbitrary point let's take an arbitrary point. Let's take a point. Which point should I take? I'll take this point X, an arbitrary point X, right? And I will draw... Actually, might as well draw this vector. I drew this vector. Right? Actually, this point looks so orthonormal to the thing, I don't want to take this point. I take this part. Let me take another point where the geometry is more visible. this part let me take another point where the geometry is more visible let me take a point let's take this as my point let me see if i am capable of drawing. Yay, I got my point right. Now, what is the distance of of x, isn't it? Now, given the x vector, how do I get to the d of x? So one thing you realize, if I had, now we'll bring in a vector, which I will call the unit vector. For whatever mysterious reason, I will use the symbol beta you don't have to use beta you could use theta unit whatever this is the unit vector and these axes are x1 and x2 axis i'm not using x y because y in machine learning is busy for the target variable these are the two feature space so you would agree that this beta vector beta vector would be something like it would go something like this at any given point it would be so just draw a perpendicular bisector from here. You know, something that will hit at 90 degrees. This is 90 degrees. Let me call this distance for whatever reason, let me just call this distance. Give me a symbol guys. What do you want to call this distance? Let me call this distance t, right? For no rhyme or reason. You could pick whatever you want, right? I'll do this. Let us take this unit. You realize that this line that perpendicularly hits is the line that is perpendicular, a vector that is in this direction, this vector, is the vector orthogonal or perpendicular to the line. Do you see that? It is perpendicular to the line. Any doubts about it, guys? It's a very self-evident fact. Now, this unit vector, if I make it unit vector, means unit vector so such that beta 1 beta 2 the beta the the components beta 1 beta 2 are such such that beta dot beta is equal to 1 what do i mean by this the unit vector dot product with itself is 1 the magnitude of the unit vector is one but what is the direction the direction is what we are seeing here beta hat right so now look at this vector and observe that if i just extend this line and i finish this curve tell me one thing what is the dot product let's ponder over this fact for a bit what is the dot product of x dot beta unit vector right what is the unit vector in this direction what is the dot product of this x in the beta hat direction? It is, would you agree that this is, isn't it? Dot product is nothing but the projection on the ground in the direction of beta, isn't it? So this red direction, this distance is that, right? And so how much is dx would you agree now observe the relationship of dx to this entire thing and you will observe something at the most remarkable and simple fact it's it's a beautiful beautifully simple idea guys you will see that d, t plus dx is equal to x dot beta hat, right, x, of course, being, x being made up of x1 and x2 components. Would you agree, guys? Is this self-evident? d plus t is the total length x dot beta. Anybody, anywhere has doubts, I can repeat it. But to me, I hope I've made it very simple, right? So if this is true, now see the magic, guys, and you'll absolutely love it. So you would say that dx is equal to x dot beta minus t right so now let's work it out what is this x is x1 x2 right dot product of like how do you do the dot product? Transpose, you know, in matrix notation. Anyway, I'll just leave it to you. You guys know that the dot product of this would be beta 1 x1 plus beta 2 x2 minus t. This is the distance, isn't it a t being the distance of the line from the origin the shortest path shortest distance of the line from any point basically the orthogonal distance of the line from the origin this now you notice that these are all betas suppose we call this by definition we call this to be by definition beta naught then what does this equation become this equation becomes beta naught plus beta 1 x1 plus beta 2 x2 feels like magic isn't it simple very simple equation that's all it is so what we are saying is that the distance function is basically saying that it is given distance from the decision boundary of any point is given by this we just need to know the orthonormal vector and we need to know the distance of the line from this origin. Now you say, wait a minute, wait a minute. Why in the world is orthonormal business? We are all taught that equation of a line is mx plus b, isn't it? Why this strange way of writing it? So the thing is, we can we can of course reformulate it in terms of mx plus b and so on and so forth we don't do that here's a reason why see a line is given by mx plus b but when it comes to a plane where mx m is the slope but when it is a plane or a hyperplane of higher dimension now what but you would agree that even if it is a plane or a hyperplane of higher dimension, now what? But you would agree that even if it is a plane or a hyperplane, still a vector, orthonormal vector, captures a hyperplane of any dimensions. It is a more general idea, isn't it? A hyperplane, an orthonormal vector to a hyperplane, along with the distance of the hyperplane from the origin, these two things will uniquely quantify a hyperplane in with the distance of the hyperplane from the origin these two things will uniquely quantify a hyperplane in any dimensions right it's very easy and geometric to state it like that that is why actually in much of mathematics except of course high school you always call a plane you identify a plane by its orthonormal vector. Here we call beta hat, but people call it u hat or whatever it is. The n hat normal vector, orthonormal vector, and the distance from the origin. If you don't give the distance from the origin, there will be infinitely many planes parallel to each other, isn't it? But the moment you fix the distance from the origin, even it will just collapse it down to only one possible plane now, right? And if you only give the distance from the origin, they can again be infinitely many planes tilted at different angles, which would still be at the same distance from the origin, right? You can imagine a unit circle or circle at that distance, and all tangents to that circle would be a plane that is at distance from the origin. Right. So you but when you give both the distance function, the origin distance and the orthonormal vector, you can quantify a hyperplane. And now you think of regression. We now bring it to machine learning. We are doing machine learning with many features, isn't it? So we are actually searching for a decision boundary in a feature space, in a data space, which is of arbitrary many dimensions, p-dimensional space. And we are searching for the hyperplane that separates, the decision boundary as the hyperplane. So it is best to quantify that hyperplane using the orthonormal vector and this. And so let us summarize all this. If D is this, then you agree. Let's plug it into our probability equation. Remember we said probability of it being a cherry is equal to, or can I write it in a more formal language rather than the subscripts and so forth let me say probability of cherry given x at a point x in the feature space x is a point in the feature space is equal to remember we did that one plus e to the minus dx this is where we were last isn't it and that is equal to one plus e to the minus beta naught plus beta one right minus beta naught plus beta one x one plus beta p x p right d whatever let me just say beta n n dimension suppose you have n features do you see this equation now what happens is in your textbooks you what is it oh sorry yeah yeah yeah and close in the bracket you're right is in your textbooks, you... Close the bracket. What is it? Oh, sorry. Yeah, yeah, yeah. And close in the bracket. You're right. So in your book, you typically see it in a more abbreviated form. People often write it just as px, even though it's a conditioned probability. They just sort of write it as 1 plus e to the minus beta naught minus beta 1 x1, right? Minus beta 2 x2 and so forth right and what this probability looks like of course we know that this is a sigmoid it has a sigmoid shape so what happens is that it has a shape like this asymptote 1 asymptote 0 right d is equal to this is a this is where x probability is given as are we together this is how you see it so now there is a bit of abuse of notation they will often call people often call this the sigmoid function but this is actually wrong even though books call it sigmoid is a whole class of s shaped functions there are many this is specifically the logistic function what you just discovered is the logistic function and it is everywhere guys this logistic function and sigmoid functions in some form or the other any one of the sigmoid functions they dominate nature right if you look at the in some sense we think the fact that I'm talking we are exchanging ideas we are thinking our nerves are firing the firing of the nerve across the synapses if you look at the electrical potential the the sodium potassium balance. What is the shape of the activation? It is the sigmoid. If you look at a transistor firing, it is the sigmoid. It is shaped like the sigmoid. If you look at the population growth, suppose you take every way you look, you look at the population growth in an ecosystem. Suppose there's a lot of resources and you have just two people, one couple there. They create and so on and so forth. And they have children. Some children died. And so they have so forth. They have more children and so forth. At the end of it, what happens? The population is limited by the total amount of resources available, isn't it? Because after sufficiently high population, people will have just enough to survive and population will start saturating. Population growth has a sigmoid structure. Wherever you look, you'll find sigmoids in nature. Logistic is by far one of the most commonly used sigmoids to model such behavior, population dynamics, and many, many things. And in fact, it is one of the, this is the logistic regression. What I just taught you is logistic regression. Are we together? So you take data and then you do that. Now the question comes, well, that is fine, but what is the loss function? How do you optimize? How do you find the decision boundary? How do you find the best beta naught beta one? So one may be tempted to say, well, we all know the sum squared error, right? The trouble is we don't have the notion of residual. It's not the gap. Remember, it's the confusion matrix, that matrix in which how well the child was able to tell the cows and the ducks right let's go back to our cows and ducks you don't have the notion of it is wrong by this much what you are wrong by suppose you predict a probability of it being a cow and it was a cow so what is the ground truth 100 sure it's a cow you predicted that it is 70 percent it was a cow. So what is the ground truth? 100% sure it's a cow. You predicted that it is 70%, it's a cow. Now your probability, because you are predicting a probability now. So you can't look at the gap between 0.7 and 1 and try to square that. There is a reason for it. But the reason for that has to do with something called Emily. There's a whole thing, concept, called MLE. But if I were to choose to be mischievous, let's call her Emily. So we will all become friends with Emily after lunch. I noticed it is one o'clock, so I will let you folks go. So, Emily, we may do today or we may do perhaps next time. Why? Why we don't take what is the nature of the last function? We have a choice, guys. We can take a short break and continue for one more hour, but that will really push out your lunch. Or we can do lunch and then do this. But we also have a lot of lab. Or we can do lunch and come back and do the labs only, because we have a lot of labs, and push this to the Emily part, the loss function part, to next week. What would you like? Do you think this is enough theory for today that is it then then that will bring me to Emily last function is Emily based any any feedback from those of you who are remote are you in for more theory I think we should do lab and then we can do the Emily next week. Next week. Okay. Any other voice? That's a good feedback. Any other feedback? Anybody who wants to have Emily today? Nobody. So why are you trying to do the Emily now? Because it would close the topic of classifiers end to end thing. That's but it's a long journey I think we should stop here it will take more than an hour it will easily eat us no no I don't want to do only of theory there's a lot of labs we don't want to fall behind on labs today is a very very long lab very long lab right you should switch to lab let's let's switch to lab in the afternoon all right guys so we will do the last one for classifiers next time the the last function of classifiers is something surprising it is not some squared error Now, why classifiers don't have some squared errors or laws is, again, a beautiful reason. The reasoning is absolutely beautiful. And for that, we'll make friends with Emily and then do it. All right, guys. Question on the family of sigmoid curves. Yes, yes. This is only one. This is the largest. back yes yes this is only one this is the largest again so the family when you look at it they always have to be 0.5 over there and just the angle that you're changing no no no anything that is a stretched out test correct but what i meant is that it has to go through that center point yeah the center point may be at zero for example if it goes from minus one to one okay right so what i meant is that it it has some yeah yeah it has to be just the angle and the curve at the edges is that what yeah see yeah that is true that is true uh see see, let me put it this way. See, when you look at, if you just stay within sigmoid, and let's say one dimension, there's no x2, the two parameters are beta naught and beta one, they control where the s is like, could you repeat the question? Yes, yes. Let me repeat the question. What Sachin is asking is, in this particular logistic function, what are the degrees of freedom? Like what changes the shape of the sigmoid, of the logistic function? So the answer to that is quite simple. The beta naught controls whether, if you see see their beta naught and beta one they control whether the sigmoid is like this or whether the sigmoid is like this in general they control the shift typically and beta one and beta the other thing they control is whether the sigmoid is like this or whether the sigmoid is smooth, right? So these two factors, they control that, right? Then the one parameter that we don't need to worry about is the amplitude is fixed at one. But if the amplitude also has to change, right? Then it becomes a three parameter model model which we don't show here because I thought it was the curve also it becomes like this right yeah yeah that is it so you control with this beta naught beta one right so with these parameters you can control how strongly it goes up and down but that's with the logistic but the family of sigmoids will control even the angle at that curve no no no no no no no see the families are all very similar so I'll give you an example so let me just say the two things right one is that see am I going like this or am I going like this to this? Actually, there is a word for this. What is the, for example, I deal with education. So one of the questions we ask, so let me tell you something about where, one more location where these functions are used. It is called item response theory. So see, when you ask in a test people questions, the most obvious way to grade is to each question give one mark. If they answer the question correctly, one mark, right? So it's somewhat, and you know that that is not true. Some questions are harder than others. Giving only one mark per question is not good. In our quiz, I tend to do that, but it's not good. So that reminds me of a king in India. There's a saying, and there was an ignorant king in his kingdom. Everything was priced the same. kingdom everything was priced the same simple food and delicacies were all one one rupee each so it is like that what you want is some questions are harder you need to give more weightage to that so then the next step of evolution is you notice people do professors do in the exam they will give what teachers do they will give more marks to harder questions and less marks to easier questions but then the question is how did they determine that it is their gut check you know that it's not reliable sometimes a five mark question will be easy and a two mark question will be hard they get it right so the thing is how do you make sure that questions are truly a differentiator or ability to tell a person's talent. And so the answer to that is, what you do is if you say that the ability is this is the ability score, the actual ability on that particular thing, let's say calculus or something like that, ability at calculus, I'm just taking it, right? And you plot it against the probability that the person will answer it. So you know that at high ability, the person should be able to answer it. At low ability, the person should not be able to answer it. Zero, right? So you want a curve that goes like this. But what happens is that a badly designed question will take like this. But a well-designed question will have this. In other words, between not having an ability and having an ability, the probability rises very fast. So in other words, if you did answer this question, the people who did answer this question, they all certainly have at least this minimum amount of ability, but very quickly it can tell. So these things are literally controlled by the beta not beta ones itself but a separate question is why did i use the logistic function here i use logistic so i'll jump the gun i'll tell you a solution there are many other functions that you could use one of them is the so there's the logic there is the posit right but you know what i shouldn't say that because then you won't go and discover the other sigmoids. But when you discover the sigmoids, yes, you could do that. One of the reasons people don't like to use some of those other things is not because you can't. Logistic is famous because of some very simple reason. As you will see, if you take this function, logistic function, it has some beautiful mathematical properties. Amongst its good properties is the derivative of logit, right? dp dx is actually p1 minus p, right? And other sigmoids also, some other sigmoids also, well, I'm now tempted to tell you all the answers, which I'm not going to because this is your homework. Let me erase this also because this derivation 2 was your homework at some point so there are error functions and other things which also have exactly the same shape but in a error function is let me okay let me get the cat out of the back for one of them only one of them error function is related to sigmoid sorry the bell curve the bell curve has how many parameters sorry the bell curve the bell curve has how many parameters mu and sigma do you see how these things develop right now there are many more sigmoids i won't tell you about go go and find it out let's take lunch because i'm going to announce a homework which is literally for you to go discover i mean i have announced Go discover all the sigmoids and observe their properties. Right? So that is that. Do that. All right, guys. So with that, the time now is 1.10. So guys, I propose the following. Please take a short lunch. Try to finish your lunch within an hour. And do me a favor, really. See, as you know, I put a lot of effort trying to teach you because my goal is to make sure you guys are strong. That cannot happen unless you reciprocate with equal effort, right? Please go do your quizzes. Finish all four quizzes right now, four or five quizzes that I have. Right now, in the lunch break, please go finish it. If you have finished it already, go have a long lunch break, right? But when I come back, guys, please let me start the lab sessions with all of you having taken the quizzes. Please do take it. And if you have not finished your project, do please put in an effort. But that is for later. I've given a lot of homework i gave univariate 4 and univariate 5 to analyze i gave the wine data set to analyze so one of the things i will request is either you finish it during the lunch time or please finish it right after the class stay back here i'll i'll stay here till six seven eight in the evening use the facilities seven, eight in the evening, use the facilities, form groups, finish your homework. Remember the homework was to analyze univariate four and univariate five and the wine dataset. So two homeworks are due by now and I haven't seen many submissions come in. Please do finish your homeworks. It'll be fun. We have closed the chapter on regression. Actually we will today, we'll do some labs on regression and we'll finish the chapter on that. Tuesday we'll finish it. We'll do the last topic, multicollinearity. So it is high time we finished all the homeworks and quizzes associated with this topic. Please do that. See quizzes, there is no judgment here. If you don't do well in the quizzes, so what? It's all right, you took the quiz, you got a reality check. You go and review my notes and lectures and come back to the quiz. Those quizzes are completely aligned to the lectures, right? And the homeworks are also very much aligned to the lectures. It can just give you a grounding in everything that we have learned. So let's do that. So well, having talked all that it is 115. We will give 90 minutes to it 215 245 245 will regroup at 245 guys, I hope that you have taken care of at least the quizzes. Right. And we'll finish the lab today. Okay. So we take a break. I think you can stop your YouTube recording. Oh, YouTube also, right. Where's the YouTube? There is no pause live streaming.