ご視聴ありがとうございました As if we can't hear you. All right, folks, welcome back. We are now going to do the LoRa paper reading. You are live on youtube so because you're live on youtube uh be aware of that i said no audio audio are you guys able to hear me yes Audio, are you guys able to hear me? Yes, we are able to hear you. This paper is one of the big, I would say, very interesting results that solved a critical problem. The problem is of scarcity of hardware resources. As the models become bigger and bigger, we have one problem the problem we have is that you need a gpu just to load the model not even do the computation just to load the model you need a gpu which has that much memory now generally we are all told about moore's law and how gp how cpus and gpus are doubling in compute performance which by far has been happening the gpu makers claim that their gpus are getting, are beating Moore's law, they're getting more and more powerful much faster. And for all we know that is probably true, it is true I think, but it is also a fact that the field of AI, after the transformers, has been exploding at a much faster rate, much faster rate than the growth of hardware. So the hardware constraints are becoming more and more acute. So today, most of us, even though we are techies and we are tech enthusiasts, the reasonable, sensible thing is we hold on to 140 90 right and that is expensive that is still two thousand dollars plus minus five hundred dollars and you put it into your computer your computer better have a lot of memory at least 64 gigs of ram and so forth but you can see that you're hitting a wall a typical 4090 has close to 18 000 compute cores 18 000 cores which is a huge number of cores but it has only 24 gigs of ram whereas and that 24 gigs of ram in that machine the gpu costs you two thousand dollars give or take whereas if you just build a very simple machine with 128 gigs of ordinary RAM with a processor on a motherboard, it will cost you a thousand dollars, give or take. Isn't it? So there is a huge asymmetry of resources at this moment. GPU memory is extremely expensive. Fitting models into GPU is very, very expensive. So as you make these models, you have an interesting dilemma or you have a crisis. On the one hand, we are finding that the cheapest, the easiest way to make a model perform better, the one trick that always seems to work is just make it bigger. If you're out of tricks, just try a bigger model. Right? Get more data, try a bigger model. There are also phenomenon observed, it's called emergent phenomenon or emergent abilities. What it means is the same model, which at a certain scale couldn't do arithmetic for example when you scale it up with a lot more parameters it seems to more or less at a certain scale abruptly starts doing arithmetic right so those are called emergent abilities now people are researching it in this spectrum their views are quite varied whether it's really there how powerful it is it's an artifact or something else and so on and so forth but practically you see that now you make the models bigger it's great more accurate we all wonder trouble is now there's only like you can count the number of companies who can run it on your fingers then like you can count the number of companies who can run it on your fingers then right it doesn't work like that so what do we do what do you and i do we're sitting we are we are the beggars sitting with our 4090s sitting by the wayside and hoping we could do some ai so that is where all of these methods like model distillation you remember we did distillation right so for birthdays distill bird and so on and so forth model distillation is one approach right now let's take this thing when you have a model today we have a two-phase approach you start with the foundation and what people often call foundational model for whatever with the foundation and what people often call foundational model for whatever value you put on such terms the idea is i would use just the word pre-trained models the the reason it sort of makes sense to call them foundational is because like for example bird it has been trained on a very trivial task masked language model but it is capable of doing a whole variety of tasks. It can do classification, it can do this, it can do that. So those pre-training and then you use it good enough does it solve your problem for your problem? If the answer is yes, you're done. If the answer is no, then what do you do? You still have to fine-tune it on your domain specific data or your own specific data. Fine-tune your net on say, domain specific or task specific data. Does it make sense? You can do that, right? And obviously there are other variants. The other thing you could do it is you could then feed you could instruct it use instruction you know uh in context learning and so on and so forth there are other techniques that you can use but broadly this is where you are you can do prompting prompt engineering you can do fine tuning so the other alternatives are prompt engineering prompt instruction engineering right and see if that works but when those things don't work then finally you say okay i need to fine tune this big model but when you fine tune the model, there is now comes a consideration, which is also a very practical consideration. You take, let's take these things. The favorite for quite some time of the open source has been the Lama Lama 2 models. 70 billion parameters. I think there are versions of it that are 13 billion, 7 billion, 30 and so on and so forth. 7 billion cutting and so on and so forth right even those to load it onto your machine quite often you have to do 16-bit quantization 8-bit sorry 8-bit quantization 16 even 4-bit the joke is that praveen keeps making that finally we'll have to make it one bit quantization. Zero. Zero quantization. Anyway, so what is it? Those are desperate things. We get pretty desperate trying to get something out of it. Right. So obviously fine tuning has many approaches that you can apply to it. Two, make it more memory efficient to be able to load a large model and do that. But here's an interesting observation that you can leverage. Suppose you have a model. Model M. If you use it only for inference, inference means the weights are frozen the weights and biases the parameters are frozen if the parameters are frozen and i'll make a snowflake here to imply that they are frozen then you do only forward pass computations generally forward pass computations are all alternative sequences of matrix multiplication followed by some activation function some distortion function followed by more transformations more right and so forth you see a whole sequence of those whether it is attention heads or it is feed forward network and so on and so forth you see this thing happen isn't it are we together guys right so it is massively parallelizable number one isn't it once the attentions have been completed it's very very very uh sort of parallelizable and you can do it the if it is in training mode, so it needs a resource, let's say the resource needs are delta or let me just call it d, the needs are n. And let us say that you take the same model and you're training it and you're back propagating the the errors back prop and gradient descent and learning. You unfreeze the weights and you back prop the weights, the loss, and you update the weights. Now what happens? You need much more storage because you have to compute the gradients. You have to have an autograd framework that not only computes the value of a function, but not only computes f of x, but also computes grad of f of x at every stage of the way. Isn't it guys? Every layer output, you need to do that. And then of course you have to do the backprop, the gradient descent in the backprop phase. So the resource utilization is much more. Are we making sense? Or plus you're sending large, reasonable batches of data through the system. So let us say that the resource need here is, this is the number of resource needs during inference, and this is the resource need during training. Let me just call it N needs during training so generally you notice that ni it's much less than entry right for example to train these models quite often people use sometimes 25 like 24 sometimes sometimes 100 servers, right? Or even more, sometimes somebody told me that close to, I don't know how true it is, thousands, tens of thousands of servers were needed to train some of these very large models, right? Anybody who has more detailed input, please share it. So, but this is it. So this is much, much bigger. Training needs are much bigger, but inference needs are modest. Typically inferences can be done on a single box with eight GPUs, like four, four, plus four, eight GPUs. Typically there's a processor sitting in between. If you look at the picture of the servers. So, and quite often one machine is enough to do, you don't even have to do a distributed inference. One machine can give you the inference. You can parallelize it if you get way more traffic, you can distribute it. But this is where you are. And in fact, inferences are such that the people who make CPUs, not GPUs, are competing the likes of Intel. They're trying to get into the inference market quite a bit because you don't need heavy guns to do inference. Now comes the interesting question. What do you do? Suppose you and I want to fine tune a model. So we want to do domain adaptation. Let's say that you want to adapt it to, I'll give you an example. You say something is short. We are short of food. Doesn't look like a crisis. You just go to the grocery and get some. We are short of cabbages. But if in the financial markets, in the financial world, if you say we are shorting on the stock. That has very specific meaning. It probably means that there is this person believes that the company is going to tank right so words can have different meanings so let's say that you want to do emotion something very basic as sentiment analysis the same words the same language can have very different meanings when you do in specific domains. So you want to do what? Suppose you have a model, big model, model, and you get a data set, you will get fine tuning data. So the traditional approach is this is made up of let's say one seven two hundred billion parameters let's take a number two hundred billion parameters you take a 200 200 billion parameter model and you say we need to fine tune it what does fine tuning mean fine tuning traditionally means you unfreeze the weights some or all the weights right and then, you start not with a completely random model but this train model pre train model. And from there, you run a few cycles, a few epochs, with this data specific data, label data, fine tuning is usually done with label data. So now if you have enough hardware resources, business as usual, you can say that's fine. But if you don't have enough to unfreeze the model and run your gradient descents and so on and so forth, it gets sensible to ask, surely there has to be a smarter way around it and people have been coming up with a lot of smart ways memory efficient fine-tuning ways lora is is one of the best really nice way of doing it and how it does it is quite interesting so before that let's look at some of the prior approaches you could for example unfreeze only some of the last layers. Why would you do that? It is done on the belief of representation learning that the early layers learn foundational things like the basic syntax of the language. And what goes with what and so forth. It is the later layers that sort of focus on higher level semantics and so if you unfreeze the later layers you may get away with it one thing you could do or if that doesn't work you start unfreezing more and more of the layers you can try that you could layer one more neural network on top of it layer one more neural network on top of it and do complexify it even more. You could do all sorts of things with that. Right. But one of the neat observations, I think that's what this paper at heart boils down to in my view, and so I'll argue it in the way I look at it, and so I'll argue it in the way I look at it, which I think is the same as the way they look at it, but I'll interpret it in a slightly different way. So the core equation is this, that I'll use as a starting point. And I'll just read this part out. A neural network contains many dense layers which perform matrix multiplication. We talked about it. The weight matrices in these layers typically have full rank. Full rank means? That's a very cryptic word. What does that mean? What is a full rank matrix? You cannot do dimensionality reduction. Right. In other words, that matrix represents a transform in a vector space that that it's supposed to represent, and the data doesn't exist in a lower space. It's not something that, like, for example, rotation just about one axis, right? You need not have a full rank, right? It's a simple operation, simple, simple. So when a matrix can be, it has sort of redundant information in it. You say that you can reduce the rank of the matrix. On the other hand, a full rank matrix has no redundant information. So when you build a neural network and you train it fully, all the weights form a matrix all right. Layer one weights, so look at this, oh sorry, layer one, layer two, layer three, just think of dense matrix for now there each of them has weights well all of these weights have values so ultimately it's actually a tensor but let's use the colloquially use the word matrix all of these weights have been trained and generally if you have traded with enough information all of those weights will form a very high dimensional representation of the data they represent the trancy ultimately given input x this entire network can be thought of as y hat is this right or in this case the user notation h are we together some hidden output the logics that come out forget the softness the logics that come out is some function of the inputs are we together right now generally a fully trained neural net on a vast amount of data doesn't have redundancy. I mean, in some sense has, but for the sake of argument, let's say that's fully informed. All pieces are needed. But when you do fine tuning, what do you expect in fine tuning? So suppose you have a network. When you fine tune it, do you expect it to become with just suppose i give it only one piece of data imagine i'm fine tuning it with one data point only do you really expect that the whole matrix all the weights will completely change if i were to unfreeze it would that happen no because this thing has been trained on two like you know hundreds of billions of instances so if it has reached any level of stability if the loss function has splattered off one more data point is not going to create a cataclysmic shift in all the weights it's not going to change the whole shape of the function or it shouldn't that would be an anomaly or unexpected behavior what it would do is very minor perturbation somewhere very tiniest of the changes somewhere does that make sense guys now bring in two data instances and you realize that compared to hundreds of billions of data points if i fine-tune my model with let's say even a thousand instances or ten thousand instances it's a drop in the bucket. That training in a fully trained model is not even running one more epoch for the entire data system. Tiny little extra step of learning. So it will cause small perturbations in the weights. So would it be reasonable to say that suppose the initial weight matrix was w w naught was the initial weight matrix right there and with all its activation actually let me put it this way let's say that the h naught was the prediction initially that your pre-trained model would make. This is equal to F naught of X, right? And then you fine-tune the model. And after fine-tuning, it's making a prediction. Let me put hats here. It's slightly departing from the notation in this paper suppose it's producing this for the same input now it produces this prediction. R. Vijay Mohanaraman, Ph.D.: Would it be reasonable to say that. R. Vijay Mohanaraman, Ph.D.: H hat is F naught plus some small perturbation. R. Vijay Mohanaraman, Ph.D.: Right small. small. It's a small perturbative chains, little bit of data can only make small adjustments to the baseline rates, the baseline function, it will make a slightly different prediction. You don't expect that it will start calling all the zebras into houses right these perturbations you're looking at the output so you would you're claiming i'm saying i'm arguing that the output would differ the logic ways that come up will differ somewhat but not widely different right this function has not dramatically changed so your your, your, your first few years the foundation of learning that that was not shut off. No, no, no, no, Patrick I'm not thinking first few years of it. I'm thinking, look at the whole thing as a box as a function. So, initially the function was F naught. And I open up all the layers complete all the layers are open for training and I find you. Or I open up only one of the few layers. Whatever you do at the end of fine tuning. Let us say that it's predicting this on the day. You would agree that this like this has to be H naught plus some small perturbation. Right. So, it's up to you. It doesn't matter. You may open only one layer. You may open all the layers. But still, this should be true. The end result cannot be vastly different from what you started out with. what you started out with and if you think in terms of weights you would agree that the weight matrix can be written as w naught plus small changes right a small change in weight so now let's look at the small perturbation small so small changes in response can only be produced by small perturbations in weights. You can just wait for our our bias including race and biases parameters. I should use the word parameters, but people you people get people are strong, right? We just say waste for every so small perturbation of weights. So now think about it. If you have very small perturbation of weights, here is one way to argue. Would it be reasonable to say that in many, many areas, a lot of the weights would simply not have changed at all? Right. Or would have changed so little it doesn't make sense or very small changes so now i'll give you one uh alternative way of looking at it see this delta w is the new learning isn't it from the new data the new data is small now basic complexity understanding is the complexity of the new model of delta w you know this this model should match the size of the data right so, for example, if you have only hundred data set it doesn't make sense to make a linear regression model of a billion parameters. Srinivasan Parthasarathy, Ph.D.: There's no way you can train it it wouldn't be sensible training. Srinivasan Parthasarathy, Ph.D.: isn't it, and so you really need a small model to train. So therefore you you can argue intuitively that this delta w, even though it has all of the size, let's say that the initial matrix was big, a billion parameter matrix, 200 billion parameter matrix, delta w intrinsically, even though it looks like a 200 billion parameter matrix intrinsically it cannot be that complex hidden in there is a lower dimensional presentation. Make sense. There's hidden in their simpler model. So how can we get to that simpler model. One idea that you could use is your idea. And here you hard back to your encoder decoder do you remember what do encoder decoders do what does the encoder do in an encoder autoencoder it creates a lower dimensional representation in the presentation right you create this make sense guys this is what you create and then in such a way that suppose this is the hidden x becomes h of x and then you reconstruct it it becomes x prime the x prime minus x which is the prediction this is your loss function and you minimize the loss function means you find that hidden representation that pretty much reconstructs the whole data and when you do that you achieve a significant dimensionality reduction how much if and in terms of matrices a lower rank matrix representation a lower lower dimensional vector representation than the original data isn't it you have taken the data from let's say let's look at the digit the MNES digits 768 dimensions is that right 28 cross 28 is 720 28 is 720. 764. 764 dimensions, right? You take that vector, if I could reduce it down to, let's say 10 dimensions, right? That's good, right? Because the intrinsic dimensionality is only 10, or three dimensions, right? And it turns out that when you do that in two dimensions, two dimensions, it's sort of two smaller dimension, but you get the clue that the real dimensionality is actually close to that. It is not in the range of 700, but by the time you reach even 20, 30 dimensions, you're doing pretty well. So what it means is that the intrinsic dimensionality is lower in the data am i making sense we can get away with it and so that's the argument used here that can we use that thinking and say delta w is actually let's say I don't know which one they call A and which one they call B but one is the mirror one brings it down let's say that this is A, B right so you apply A to it and then you apply B to it or maybe B, A I don't know which one let me take it down to that multiplication it and then you apply b to it or maybe b a uh i i don't know which one no underfit the fear is underfitting not overfitting because you might end up with two simpler models right yeah first a and then b yeah sorry where am i i seem to have lost my drawdown yes there we go so does that does that argument make sense so what do you do but you say wait a minute wait a minute that's only for the perturbation the full signal is still the composite of a function of w, not the initial weights, plus a function of delta w. Isn't it? That's your full signal. It means your architecture is this. You keep your big full model. You keep the weights fixed. But you want to train it. So what do you do? You freeze this, keep it frozen, and then you can take a smaller network which looks like a decoder kind of a shape. This. This has far less parameters, isn't it? So suppose this guy was for the sake of argument. Let us say that this was thousand, and this is three. Well, three is pretty small, but let's take even three. Right? Three thousand times three is just three thousand. That's another three thousand on the other side 6000 right but if i was doing thousand times thousand right now that is pretty big right so uh three thousand versus million so go to your neighbor and said your house is worth that worth a million Would you please sell it to me for 3000? And wait for his reaction. So it's like, sorry for my poor joke. So I'm sure that there's something worse than poor joke, dad joke. That's what my daughter said to me. So there we go. Are we getting the point? So you are actually taking a much less complex model. Another way of looking at it is it if it has only 6000 parameters, let's say you're representing data in a 6000 dimensional space, instead of a million dimensional space. Right. But that makes sense, because perturbations are much simpler beasts. They should admit a low dimensional representation. And that is, in a way, the word rank is associated with matrices. Intrinsic rank of a matrix is when you remove all the redundant rows and columns what remains right or what is an alternative lower dimensional representation that will do the job right for a matrix and so low rank matrix and low dimensional representation are sort of very close cousins so they are often the same thing right and that's what we are doing we are saying okay keep your big model biggie model take this little guy this these guys are called loras right you take this lower what you do is you take the input pass it through this pass it through this this will produce f w naught a function of the the initial weights right and for all you care you might have one big gpu on which you are running that in prints if you want you can even do a distribution and this can run on your little guy your 4090 right produce this this is f of lora and then your result y hat is just f w naught plus f lower. Add the two results up. Right? So this is the pre-trained result, pre-trained model result. And this is your fine tuning. So what you do is you have only opened up the back propagation you allow only for the lower. Are we together? Yeah, you don't allow the other model is frozen. So let me bring back my what what is a good picture of a snowflake. This is frozen right you keep that as frozen it only does forward pass any back propagation all of them have to keep retraining this model now you say well you're not fully utilizing the back prop info you say fine more and more cycles epochs and ultimately this will stabilize laura will stabilize now what's the advantage of it? First advantage is LoRa's become easier to train. They're smaller models, you can save it, but they have another good property, which is the additivity property of it. They are additive, they're associative, they're very good. You can do lovely arithmetic to it. So for example, let us say one loras the job was that given i'll take a sec take something it contains information that is only sensitive to the delta between the general language and financial markets right so it will just go and make big changes to the results that the pre-training model is producing for the word short. When it finds short in a sentence and it says no scary, right? Something like that. You come up with another learner which understands politics of a country. Now every country's politics is different. Again, I keep quoting Tolstoy, all happy families are like each unhappy families are happy in its own unique way. So you look at all the countries in the world, 187 or whatever number of countries there are, and you see 187 unhappy families politically right ask anyone can any citizen of any country are you absolutely happy with the politics in your land that's like but most of them perhaps i don't know maybe there's some countries that are i'm told bhutan is the happiest nation so maybe they'll think otherwise right so anyway so now you have a lot of policies. Now what can you do? If you end up with an issue, a topic, which is the political impact on financial data, or something like that, where politics and economics mix up and markets mix up, what can you do what can you do you can bring both the loras together while making predictions even though you have not individual you don't have enough data in the world to just specifically train for both political and financial conversations but you can take political conversations, you can take this and you can get not perfection, but you can get some reasonably better results with that. Is it plain addition? Just add. Yeah. So what happens is, lower has become building blocks. And those people who do stable diffusion, et cetera, et cetera, they are lower as for putting nice lipsticks. They are lures for darkening your eyebrow, right? They are lures perhaps for making your nose longer, right? Or whatever it is, like, you know, the joke is, you know, Pinocchio's joke. People, if they want to show that somebody is a liar, they'll take that guy's face and make the nose longer. I'm sure there's a Laura for that. There's a Laura that will add hair to a bald head. So you can pick and choose the adaptations. How do you want to change this person? So very practical impact. And you can go on successively applying Laura's to that image. So Laura has had a profound effect, actually, in this entire generating and AI world that we are in. Asif, how do we come up with the low-rank weight matrix for the LORA? Oh, you need data to fine-tune it. You need fine-tuning data for your problem. No, no. How do we build this low-rank weight matrix that you have for the lower you just take a new so weight matrix is what a neural network so you leave the initial neural network alone you take a smaller neural network so just do exactly what you did no that's not what i'm asking i'm asking is if i have a if i have say a 70 billion parameter large language model and I want to create a load out of it how do I go about doing that you said something about that we we every model has lower dimensionality so we have to pick the lower dimensionality weights right yeah so there are so first of all one of the interesting results is how do you pick the dimensionality and when you look at the results of lora experimental results they are quite surprising actually i will show you there's a table i want to speak so we have a question whenever you're ready. No, no, let me answer this first. I want to show something interesting that you can do LORAS of very low dimensionality and still get very, very good results. So it is a lot of hit and miss. So, Mahaswami, there is no, at this moment, right, you don't have a hard and fast way. People have hacked and played and they have figured out that this is good enough. And there is some discussion, I read this paper months ago, there is some discussion that they were surprised at how low dimensional they could make the adaptation right um we reduced okay so yeah look at this on g sorry um look at this statement with r is equal to only four Look at this statement with r is equal to only four means they're projecting data down to just four dimensions. Right. And only the query and value projection matrices being adapted, the check sizes reduced to this amount, and yet actually their performance doesn't degrade too much. Right. They can do their fine tuning just fine with that. So you can practical benefits and a huge asset to drop it down to a lot of them to do a much, much lower dimension. Does this mean that your data set it's going to have only just a few big commonalities yeah see what it is doing is it is basically asking in some sense i have learned the language right so one people in the lower world often give this sort of example that this uh so amrit has just gone through college right he wants to learn model fine tuning he has to take one deep learning course, which is a small addition. But that course you cannot take unless the base model is there. And all of the 17 years of learning are behind. Because it's additive knowledge, a delta addition. Just one course is enough. But if you had to take a baby and teach that person fine-tuning, neural network fine-tuning, you have to start with ABC, right, and do the whole journey. But here, one course is all. So how much content is there in one course? Very little. Right? Though Amrit won't agree. He'll say I have to work very, very hard for that course. But it's still a finite amount of work. The same is with dora if you have to just learn a little bit to capture that learning you just need a few ways that's the gist of that so let's say for the images you just want to let it learn more yeah you can bring it down to really low dimensions You can bring it down to really low dimensions just so that it's all looking to be, all the sample being the same. That is right. So suppose you bring it down to let's say 10 dimensions and you started out with 1000 or let's say that you started out with 1000. You realize that you are still looking at 20,000. Wait, 20,000 is a whole lot less than billions, right, or hundreds of millions. That's the main idea. And it works because with it and see. Yeah, go ahead. Somebody there? Yeah, I said then do we expect to see sort of domain specific LORAs propping up? Oh yes, yes, of course. There's a LORA for necktie, there's a LORA for lipstick right i'm talking about you know literal for example if you take the these gen ai models right then right now you can't train them on a industry or domain specific data so if somebody wants to use them then you can potentially have a domain specific LoRa and still use? Yes, absolutely. LoRas are almost literally the answer to domain specific adaptation. Heavily used, heavily used, heavily used. Is that what these, you know, for example, the C3.ai's of the world, are they relying on these kind of concepts when they are coming up with industry-specific solutions, AI solutions? See, LoRa is one technique, right? It's a very powerful technique. And then there is QLoRa, which brings about quantization of the data. Actually, why don't I, even though this paper doesn't talk about it, let me give you the idea of quantize, the quantized LoRa. The next evolution of LoRa is q LoRa. q LoRa. It's spelled with little q, little o. So what is q LoRa? See, it is all observing the shifts in weight. The perturbations of weight, they all happen around the base weight. If you look at the perturbations of, or let me put it this way, if you look at all the new weights, LoRa weights, they're all very small. And if you build a histogram, they all have a normal distribution which makes sense because most of those weights that you learn, they're either zero or because nothing happened or very small shifts from the norm. It is very unlikely that some particular weight would have learned a lot, would have made a big departure, big step away from the baseline. Isn't it? The baseline gain, because it's zero, let's say a normal distribution. So based on this fact, what people said is that, I know something unique about the distribution of the weights in a load. So what can I do? I can create a special quantization machine. I can create a new data type. So you know in your programming you have data types like integer, load, rule, short, long and so forth. Now when you use a short, when you notice that some number is going to not be big, just a few values. So things like that. Here you know something very special, very machine learning specific. The weights are normal distribution. so what can you do you can create a new data type normal flows which does optimal bucketization of the bell curve knowing that most values are centered, it will put far more values here. Actually, I should have made the far off ones more spaced out. Right. So let me remove a few of these. And far more bins near the center. And if I use this, I can replace every value. Let's say I can replace this value with just this approximation. Are we together? So, for example, I can replace 0.1139, which is 0.1. Does that make sense? And so I need far less space to save this weight am I making sense guys so that is quantization into a normalized float normal float when you bring that you have suddenly taken a number you know you take float 64-bit 32-bit whatever it, and you have reduced it down to 4 bit or 8 bit. Isn't it or 16 whatever you like but in a very efficient way, because you're exploiting something that is not true of all numbers, but that is only true of numbers in this specific context that's only true of weights in a model right you exploit that so when you do that it becomes q lower to do this we need to know the range of variation right variations are always perturbations around zero so given the fact that you have w w not taken away the shifts from w is what you are looking at so the shifts will be small perturbations around zero so you would have more values near to zero range of values and as soon as you use the rate makes sense in other words most weights won't change at all some weights will charge very rarely some weights will change drastically so is it like instead of a linear radiation you have something like a logarithmic variation uh bell curve so is basically a further approximation for one more additive step and so it sounds like the motivation would be when you have decent performance and now you want to start saving on space. That is right. And you know what people have found? LoRa and QLoRa often beats fine tuning the entire model. Why would that be true? Because when you're fine tuning the whole model, your gradients, the changes are propagated throughout. And you don't know whether the whole thing is in an unstable state. You're unlearning also something. Right. So remember that every time you do some fine tuning or domain adaptation, you don't know what the model is unlearning. Right. Whereas actually with this sort of approach, there is no unlearning that learning is preserved you're just learning extra additive apart from inferences how fast is it or how easy or is it difficult to switch between oh very easy very easy because what you do is you literally add it code wise right you take the inference from the first inference from the second just add it plus so all you do is you literally add it code wise right you take the inference from the first inference from the second just add it plus so all you all you do is in the other thing now you set a config parameter set this one okay so so so we can essentially create a language module that can either talk to a position or just talk about exactly exactly you can do that and no no cost to inference, it's just a matter of switching the LoRa. That's right, that's right. Can you repeat the question? Couldn't hear, that was Patrick, right? Patrick asked this question. How is it, is it programmatically to switch LoRas? And I say very easy because it's just a config, which LoRa to load and use inferences for. And you can do additives and so on, you're doing it in stable diffusion and so forth, in fact, you were the person who used a loader to put. Some actors some face on a horseback. So you're practically using. Their influence. I was just wondering if the lowest test isn't gonna have to affect your inference. No, no, it's just a very simple thing. it right i was just wondering if the lowest test is it going to have to affect your inference no no it's the same thing see if you think about its stable diffusion etc in a very rough and not quite it is encoder decoder and llm in between there's diffusion models yeah so i asked you for a question on this switching that i i think i understand what patrick is saying but assume that you did this q load on different problem sets right it will say within the same domain then you can have all of them existing with the original model itself right so if i know how to put a mustache and then you want to put an eyebrow let the eyebrow be hanging around that will not show up a hit correct yeah yeah that's right you just change this waste to zero that's it correct that's what you do basically right it is ultimately the same thing the only thing is that every time you load a lower remember there's a memory footprint to it so you don't want to load a lower and then multiply it so it's also the lower it doesn't go cross model oh it can go cross model it can do whatever everything that you're doing he can do so when we say quantization of a model is this the same thing that we are doing are we using this normal plot data type history model quantization is a broad topic and people use many techniques but this is certainly one of the popular if you want to quantize down to a lower dimension for floats lower and lower number of bits of your normal float is actually a good approach because it is memory efficient especially in the context of context of your because you know that there's a lot of weights are going to be perturbations around zero. All right guys, so did we understand that paper that ends up paper reading part, I will end if anybody has a question, please ask, that helpful, by the way, when it was a session useful it's okay good and i'm going to any any major questions before I end this session. With. No, no, the fine tuning is captured in the Laura right. By. captured in the lower right like uh by including like uh opening up the last few layers oh yeah yeah you don't touch the previous model it's not you don't touch it you capture all the learning yeah see the world is at this moment pretty chaotic everybody is doing whatever they want right and hoping it works see one of the having been in two different worlds the mathematical world and the engineering world is interesting what each world thinks of the other engineering will think mathematicians are a bit abstract in that fact mathematicians will look at all this deep learning deep learning engineers you know network and you say you know what they do they hack hack hack till something works and then a post hoc they put some interpretation on top of it and write it it's a it's both sides have some truth to it but the fact of the matter is in the deep learning world at this moment many things are learned just by hacking and seeing what works right experimentation and many things are learned by reasoning carefully and learning so all approaches works of our people still opening up last few years, they are all people using Laura there and then many more there is just one of the. So, as we make progress in the boot camp and we talked about many other papers. the binary representation remains the same right so is there a calculation or equation that can help us to convert a plot value to normal no i'll give you an example if you take a number a float and you just chop off like you just say that i'm not going to keep too many like if i know that the number size will never exceed let's say three right will never exceed so then our mind and never go below minus three so how many levels do you need three here three there zero in between seven to catch a seven how many how many bits do you need three bits maybe four bits right you can do it like that but if you don't know and your number can be any size and you try to capture it in four bits now how would you do it minus infinity to plus infinity how would you capture in four bits there is no way right so that is the difference. So when you normalize the data, a normal normalized data normal curve within that and normalized data z value, it's called z score. Z score, almost all the data is within minus three and three is the nature of statistical nature. But here aren't we supporting more ranges range of values near 0 and less? You could do that or you could just say I'm going to have only 4 bits if I have just 4 bits. Right. All I need to do is the boundaries. I'll keep it so I won't have fixed boundaries like minus 3, minus 2, minus... What I can do is between 1 and minus 1, I can have maybe instead of just two levels i can have four levels and the fifth sixth seventh eighth level will capture everything from two to infinity and minus two to minus infinity you realize that right so you give a little bit more concentration of buckets if you have eight buckets give four to the just one to minus one range let us say buckets if you have eight buckets give four to the just one to minus one range let us say and then two buckets to so maybe one bucket to go from two to like one to 2.5 and another to go from 2.5 to infinity and likewise the other way around these buckets like how do we mathematically understand that literally any value that is uh closest value the closest value to the bucket right so is it close to zero zero is it close to one one is it close to 2.5 2.5 is it close to whichever that is it rounding is basically rounding to those bucket boundaries. As simple as that. That's it. That is it. Then it's essentially the bid reduction. Like, you know, from eight to four. That is huge. I mean, from 32 to four. So that is QLORA. All right, guys. So with that, I'll end today's session. Read the paper, guys. These things have far reaching impact in practical terms. This is the only time you'll get. You'll have busy lives. You won't get time to read this paper carefully. While you're at it, please do read it today. And with that, we end paper readings for today. We'll go to optional demo. Those of you who are interested in project demos,