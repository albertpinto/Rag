 Today we'll talk about an important topic, density estimators. It has overlap both with the data science course that we did and with the visualization course that we are doing. Now to set the frame of reference for this discussion, look at this image. If you see in this image, there are quite a few ducks and there are a few geese. you see quite a few ducks. Now let's try to represent this data visually. You could say let's count the number of ducks and geese. There are about 11 ducks that I could count. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11. So 11 ducks and 4 geese. So you create a little data frame which contains exactly that. It's a simple data frame saying how many ducks and geese. So you create a little data frame which contains exactly that. It's a simple data frame saying how many ducks and geese. How would you represent this data in the image? So by now we are familiar with how to make pretty plots and so forth. You can represent it as a bar plot you can represent it as a bar plot as we did. So you say well here is a bar plot, there are about 11 ducks and there are four of the goose. Now four geese. Now if you were to ask a different question, suppose a more abstract question, we say, all right, what is the probability that a particular bird in this pond is a duck? How would you say? So suppose you put a finger on one of the birds here and say, what is it? Is it likely to be duck? Is it likely to be a goose? And what are their respective probabilities? So if you take the approach that this evidence itself, you have to derive some or infer something out of it, you would say, well, that is easy. What you would do is you would take each of the counts and divide it by the total, the total being about 15, 11 plus four, 15 birds. And that should give you the proportion and to the extent that this is the only evidence that you can say reasonably that that's a probability of a bird being a duck or a bird being a goose. So you say that duck's probability is 73 and goose probability is about 26, 27 percent. So obviously in statistics we call it the probability mass distribution. So if you think about it that if you have and one intuition that I always carry is imagine that you have a bucket of sand and it is one unit, one kilo or whatever it is, and it is one unit, one kilo or whatever it is, one kilo or one pound of sand, and you have to share it between two columns. What is the mass of this pillar and what is the mass of this, sorry, pillar of bar? What is the mass of it proportionately? So you would put a lot more sand here than here. So there are probability mass distributions you do. Now, ducks are discrete animals, or birds are discrete things. As a random variable, it's a discrete random variable. On the other hand, if you take a continuous data, let us say that you have univariate data, and I'm generating some data, it doesn't matter. Just say that it could be the height of people or whatever it is. We want to represent it. How would you show what values are more likely? Ask this question. You have a value of x, which goes from, let us say, whatever it is, minus something to something. It has a range of data. In this particular case, perhaps minus three to seven is where you'll find the data obviously it's a synthetic data i know the answer so but suppose you get data right you're looking at for example this could be about the ducks and the geese you have to you every time you encounter a duck or a geese what you do is you weigh it you take the weight of the duck or a geese, what you do is you weigh it. You take the weight of the duck or you take the weight of the goose and whatever weight it is, you note it down, let's say. So what you will end up with is what you say, a random variable whose value will be whatever it is, a range of values. Now, how would you represent it? The simplest way is you just say a frequency count. Let's create a histogram. We are all familiar with histogram. But then if you ask a different question, what is it likely to be? Where are the values more likely to be concentrated? So you can fairly well imagine that whatever is the, sort of the quintessential weight of a duck, or what is the most frequent weight of a duck, there the values will be more concentrated because it will have some bell curve sort of a distribution. Likewise for goose, whatever is the value of the goose that you expect, the values will be concentrated around it. And there'll be some outliers. Some ducks will be smaller than the normal duck. some ducks will be smaller than the normal duck, some ducks will be bigger than the normal duck, and likewise for the goose. So from that comes this concept, this very basic concept, where are values more likely? Can you show, or in a way, if you think of it as the same one bucket of sand, one unit of sand and you want to put more sand at places where values are more likely and put less sanded places where values are less likely and so distributed so that visually you get a very good sense of what the values are what the values are which values are much more likely. So how would you do that? To do that is to create something called, to know the density distribution. Now, what happens in reality is that if you have all the population data, like if you could go and weigh all the birds, all the ducks in the world and all the goose in the world, but then you would know exactly what the weight distribution is. But in reality, we don't do that. One of the fundamental facts of data science is that quite often, because data is hard to gather, what you have is a sample from the population. If you don't have the entire population, but what you have a sample for the population, then you have to ask this question, what is the estimation of where the values are more likely? How would you estimate that? You don't know the actual distribution, density distribution. You have to go estimate that. And that is tricky. So suppose I give you only one point of data what does that data point say right you could argue that what that data point is saying is that at least at that point there was at least once a value present right there was at least one let's say duck or goose whose weight was that one bird whose weight was that but you know that there's always a little bit of a measurement if that specific value is present it is possible that you have values around that value right it is quite likely that you if that's the only data point you have you would you would guess that maybe there is some duck whose value is adjacent to it but or some bird whose value is adjacent to it and as you go further and further from the value your confidence that you will find a bird of that weight decreases you don't know all you know is you see a value like this one so then how do we use that intuition and now we bring in more data points or so suppose you bring in another data point and some other value once again at that value you have confidence that there is some data there is some bird whose weight is that adjacent to it you have lesser and lesser confidence if you go sufficiently far from that value you don't have much confidence. There's not much evidence that you would believe. Think of it like that, that when you take the weight, your scales are very rough. You measure the same bird thrice and it will come up with three different values. So you know that your scales are approximate. If that's the situation, there's an instrumentation error built in and so you can imagine that whatever value you measured or you got adjacent values are quite likely big deviations of big errors are less likely right so one could for example say that you can build a little bell curve kind of a thing. Rather than just claim this is the value that we see, we can say that the values are distributed around each point and you could put a bell curve. So imagine a bell curve shaped like a hat and you can, you just go pour a little sand there, right, at that point. Wherever you see the evidence, you pour a little sand, you find another data point and you pour a little bit of a sand there. And you keep doing this. So gradually, what will happen is you will end up with the sum total of all the data data points speaking, isn't it? Now, the big intuition is that this little sandhill that you created, or this symmetric function that you created, in which the value, your confidence, or you believe that the probability of finding values, as evidence from a single point, decreases with distance from that point, right? To capture that intuition, you create a function, any function that sort of decays with distance from that point, right? To capture that intuition, you create a function, any function that sort of decays with distance. Now, people create all sorts of functions. They sometimes create a function that says, okay, I will make a little box around it. So there is a notion of a top hat. Okay, let's do this thing. What you do is you take the interval from, let's say whatever to whatever range, let me just say zero to 10, and you break it up into, let me just say 10 pieces, 10 intervals of zero to 10. So suppose a bird comes in and you weigh the bird at 5.3 pounds, right? One thing you could do is you could take a little box, right, or a desk or whatever it is, you can take a little box and you could put it in that slot, in that bin, because you have taken the interval and you have put bins there. Let's say that you have put vertical column bins or cylinders next to each other so in that 5.4 means it is between five and six that particular range you can go and either put a box or in in our example where you're pouring sand go and pour one small amount of sand a standard amount of sand into that cylinder because you found a data point that belongs to that cylinder are we getting the cylinder because you found a data point that belongs to that cylinder. Are we getting the intuition? Then you find another data point, another bird is 5.7. Well, again, you put it here, another is 2.3. So you put it in that particular cylinder, you put a standard amount of sand. So now what will happen? After a little while, because you started with only, you know that you have, there are 15 birds, let's say, and you took one unit of sand and divided it into 15 parts. And wherever you would find a bird, you would find the contributory sand in that. You will end up with a histogram. You will end up with, you know, all these different cylinders filled with sand. The sum total of sand is still one. But you will get a plot that would be something like this. Different intervals you would get. Imagine that each of these little intervals of bins is a cylinder. So you would end up with this, which looks like, which essentially is a histogram. It's a count. Every time you find a bird you just go at it it's a form of counting except that. You have normalized it, you have divided it by the total time 15 words so suppose you were putting one unit of sand a bird, then this whole hills weight would be 15 if you want to scale it down to a weight of one unit, you'll have to divide by 15, isn't it? So a normalized histogram is a good estimator. So suppose I ask you this question, just looking at this, where is the bird's value? Well, by the way, forgive me, when I was making the plot, I wasn't thinking of the weight of birds so you can see some of the values have gone negative here um but okay but just give me a little bit of a um license with this data so if you if i were to ask q is the value of six more likely or the value of four what would you say you probably would say what would you say let's see the i see quite a good sizable hill around, let's see, minus two, four. Thank you. Yeah, four is very good. Four is very, very far more likely than, for example, at six, a bird will come less likely. And so where did this intuition come from? It comes from the fact that we just added sand, or we build this histogram, and we sort of normalized it. When we normalize it, it becomes a probability density. So anything when you see, whenever you give the probabilities in such a way, you distribute what I just said is you take one unit of sand and you spread it out based on how likely the evidence is using whichever methodology, like our methodology was to divide the sand into 15 little blocks because we have 15 birds. As an example, wherever you see a bird in that cylinder, in that bin, you go and put a block of sand, right? So what you end up with is a normalized histogram, but the sum total adds up to one. These functions are probability density functions. They take values from zero to one. So the probability of the bird having a certain weight can be as low as zero, or it could be as high as one, but the sum total will add up to one, right, at the end of it. So for example, if the only weight possible when you do engineering, you make a bolt or something like that, and to the first approximation, all the bolts that you make, a machine makes, are the same weight. So then it's a probability mass distribution because you can say with certainty that all the bolts will be this particular mass or this particular weight and all other weights, the probabilities are to the first approximation zero. So this is spreading out the probability of some random variable. That is what a thing is. So now when you get a data like this, you call it an estimator because you have worked with a sample from the population you don't know the whole population if you have the entire population data there's nothing left to estimate right you have exact answers but when you're sampling from a population then this is it so that is it so a little bit about regular histograms versus normalized histograms see regularly when we draw histograms we just count we put y-axis is count x-axis is the value but and so that is fine but when you normalize it by the total it becomes the normalized histogram it was the first density estimator to to my knowledge, that was ever created. It was created by the student of Galton, Pearson, Pearson of the correlation thing. He created the, he thought of this idea of using histogram as a density estimator, normalized. And obviously he was a student of Galton of regression towards mediocrity thing. Remember the thing that gave linear regression algorithms its name. So it has a long history. Now when you deal with it in matplotlib, whenever you get data to make it into a probability density instead of just count, all you have to do is say density is equal to true. Now, when you look at this function, there are a few things that don't look nice. The things that don't look nice are the facts like, and you see that it is very jagged, right? And it doesn't agree with intuition. You know that fundamentally the weights of the, let's say birds will peak at this value, but suddenly you see this little valleys, unexpected valleys in the data, and it's very jagged. So you say that in, you do a thought experiment and say in the asymptotic limit, if there was, if the sample size was infinity, what is this data telling you? What would be that curve? What would be the probability density curve? That is our goal. So then people came up with other ideas. One is a top hat idea top hat idea says don't put it in this don't discretize the x-axis the the access for random variable values or do this wherever you see a value literally put a little box on centered on that value now it's hard to believe why this box won't topple so it's a thought experiment and so what what you will end up with is then you're no more susceptible to the binning process. It turns out, as you will see in a little bit, the binning process, like for example, the binning. So we have this count by binning. The shape of the histogram varies by bin size. Do you see how different the three histograms are for the same data? One is maybe 10 bins, one is 50 bins, and one is 100 bins. From the first one, you can't see the shape of 500 bins. You can't see the shape that you see in the rightmost emerge from or it is not so obvious in the leftmost bin that the shape is more likely to be what is emerging in the rightmost and when you don't have enough data here we of course have 17 000 data points so we have the luxury of going high in the number of bins but when you can't go high, then one of the problems with normalized or just basic histograms has always been that the intuition you derive is very much dependent on the amount of bins that you create. So it can be a little bit misleading. So the next thing you do is you can say, well, you know what, we won't bin, we will do this, we will take a block of sand and we'll put it centered exactly where we find the bird. That is called the top hat argument, that is based on top, it's top hat kernel. You say that a kernel because the amount of sand that you put is essentially, if you think of it as a mathematical function, that function is called the kernel. You say that I'm putting it right here in a square, like it is shaped like this. Wherever the value is, I'm centring it and putting a block of sand right here. So then you get something which is better, which is not hijacked by binning process, but it still has a problem. The problem that it has is your distribution will still have jaggedness to it. Now, the jaggedness is fine. Its main problem is that it doesn't capture the physical reality. For example, if you look at weights of birds, they don't change, they don't have jaggedness. In reality, you know that the probability weight, a probability density function that's about the weight of birds has to be a smooth function, right? It has to have smoothness and continuity to it. And a top hat would again violate that. So people have tried different ways of dealing with it. At the end of it, the way that I mentioned to you, what you do is, so this is your kernel density. So now we are quantifying what we are saying. We are saying is that how you pour the sand in or around the point where you find the data. So suppose you find the data X1. What do you do? What is the shape of the sand that you poured there? Is it square block? Is it, right? What is it? And where is it centered around that point? That is described by a function which has been called the kernel function now what does this kernel function do so once again imagine that you have you have a unit so this time around what you do is we take a unit amount of sand and we say here is a point and you put it whichever way you like so there are many ways you could do it you could just make a square block there right at that point that is a that is your top hat or you could say well this point belongs to this bin so i'm going to pour the sand in the bin itself if you have binned it that's your regular histogram but a more intuitive thing that we said is that you can pour the sand to make a nice smooth hill we said is that you can pour the sand to make a nice smooth hill right any nice smooth hill which is desirable and what would be the quality of that hill you say that that's a probability density function where it is high it's more dense where it is low it's less dense and you're basically saying that each point is an evidence and so you can build a little probability density function who which integrates to one right because you say that the probability of finding that point if i assume some level of fuzziness in the way data was gathered is somewhere in the neighborhood or adjacent to the actual value and the probability density should add up to one but that function is a kernel function. The most obvious kernel function that comes to mind is the Gaussian function, because quite often we say that the measurement you have will have instrumentation errors. But one of the contributions of Gauss was he showed that error functions, they tend to take a bell curve shape, Gaussian shape. In fact, that is why it is called the Gaussian function, because Gauss discovered it as an explanation of how errors are typically distributed. That's how the normal or the exponential function, I mean, the Gaussian function was discovered in the context of error analysis. So if you take that kind of a thinking, you say the further you go from the value, the less probabilities. So now we are talking of a nice hill, like a hill that is shaped like this. But if you look at this hill, there is still a parameter, which is the width of the hill. I can take the same amount of sand and I can make a more sort of a peaked hill, taller hill, concentrated near the point. Or I can spread it out a little bit, and I can make a more rounded hill that's not so high. Isn't it? And still use unit sand. So like this. Imagine you could make a hill like this. You could make a hill like this. You could make a hill like this or this. You have spread out the values a little bit isn't it to illustrate this i made it like that so four different people can lay out the same amount of sand still using a bell curve like function a bell curve but in four different shapes isn't it guys and this is of course your variance or the spread of the bell curve. So the, and you control it using sigma squared. Those of you who remember the bell curve, you remember sigma variance is standard deviation squared. Okay. So now this is often also called the bandwidth. also called the bandwidth. The bandwidth word is often used for this. So when the bandwidth is high, you notice that you have spread the sand pretty far. The gray box, if you look at this, the gray area, it's pretty spread out. Whereas if the bandwidth is small, you're not letting the sand run away. You have peaked it into a nice tall little hill here. pour the sand of belief and it's a it's you think of it it's a sand of belief that the value is around this point right or how likely is this value to be present and that's a very sort of a biation way of thinking but that is a good intuition to have but now you have a problem and you can have by the way many many kernels are there if you remember when we did the the comprehensive intro to data science or ml 200 and so forth, you remember that I introduced you to many, many kernels. We talked about the exponential kernel, the sine kernel, which is the Epineh-Chinkov kernel. Then there is the tricubic kernel. I haven't listed the tricubic kernel here and many, many kernels. And if you remember, at one point I mentioned that there was an effort to find which is the best kernel to use in any given situations. For example, when you do kernel k nearest neighbor, which of the kernels do you use? So at the end of a lot of research, now we believe that it doesn't matter too much. You can take any sensible kernel and work with that there is some evidence that this the the eponish chico of kernel or the sign kernel works superior it sort of has in many situations it turns out to be winner but by a tiny margin it turns out to be a winner but a very easy kernel to use is the Gaussian kernel, right, the bell curve kernel. So go pouring sand there. Now comes the question of density estimator. Each point is making its own vote. Okay, values are likely to be here, values are likely to be there. There's another point here. So values are likely to be here. Now what happens when you have put little kernels in each of these places where you find the value of the random variable, you have taken not one bucket of sand, but suppose you have 15 birds, you have put 15 buckets of sand. Now you have to normalize across them because the probability cannot add up to 15. The probabilities when you integrate over the area under the curve, or in simple terms, when you take all the sand together, they need to add up to one unit. So what do we do? What we need to do is, when we look at the probability density, a probability of finding a particular value, we take what each of the kernels is saying you know this little a hat there's little curves that you put around can you pause it for a moment got it so what we need to do is obviously normalize by how many data points we use because we use one bucket of sand for each of the data points. So you would agree that this formula in view of our discussion is essentially the most obvious thing we could do as a probability density. Does it make sense guys now with this discussion? Yes. discussion yes okay so this is it now obviously the parameter the variance is how the bandwidth parameter controls how spread out or not is your kernel kernel is there let's say you took a gaussian kernel you have how your basic intuition is it's less likely to be present each what the what each point is saying is like value is likely to be adjacent here in this neighborhood so the further you go from that point the less likely you want to claim or the kernel function wants to claim that is probable to see values it's not sure confidence goes. So another data point there might increase the confidence, but this data point cannot say that. So the rate at which it decays, you know, it attenuates, your belief attenuates, that is your bandwidth parameter. And you can see that happen here. And I've showed it here. Now, how do you compute the kernel? Well, now comes to the brass tacks. With all the theory in place, how in the world are we going to do it in practice and visualize it? So the way to visualize and to do it is scikit-learn has this function called kernel density. I would very much like you to get acquainted with that. When you acquaint yourself with that, you can use it at many places. Like for example, Sachin, you're talking about those numbers, right, without getting into the concrete stuff, those numbers. One of the easiest things you can say is what is the likelihood that the values are this or that. Creating a kernel density estimator gives you an intuitive sense which goes beyond just a frequency table or things like that. You create that and you get a density estimator like this. Now, you notice that I have arbitrarily taken a bandwidth of 0.25. How did I do that? Well, the way I did that is I know that the data is sensitive to the bandwidth. So I created, I played around with all possible bandwidth and I created a table. I created a matrix. Let me do this. Do you notice that I played with values of bandwidth from H almost close to zero? It is not zero, but it is close close to very very small to very very big like here h is equal to one so when you look at it what can you tell like what is the intuition here this is two jacket it is responding like if you make your h too tight too small then each data point is saying the value is right here so what happens you know that that cannot be true because if you take a different sample the shape of your estimator will change right and you will have you'll have a very high it is a high variance situation right on the other hand if you take your x to be too big let's say say in this neighborhood, you realize that this too is wrong, right? It is wrong because what we have done is we have used a thing that is more like this. We have spread the sand around the point too broadly. What we are seeing is, you know, the value, if you find a value, let's say 5.74 there, then the, you know, the probabilities could if you find a value, let's say 5.74 there, then the, you know, the probabilities could be pretty broadly spread, right? You're assuming that there's a huge latitude of where the value is. And what this point is trying to say is that the value could be anywhere in this pretty big neighborhood. When you take big neighborhoods, you realize that the function doesn't change or respond to to the evidence much you you get an oversimplified model so if you go along this you see your bias variance trade-offs i hope you can see that that the first situation is the presence of situation of high bias this one and as like of course you can keep slowly moving downwards but for the in the interest of speed i will just go diagonally this is high variance and this is high bias somewhere in there is a good place to stop and one could say that a good place to stop was maybe here, right here. Isn't it? 0.25. Because you see, you get your smoothness and the shapes are delineated. The two shapes are delineated. And of course, in this particular case, I happen to know the ground truth. How do I happen to know the ground truth? Because I generated this data from a function. And I know that the function values are like that the spread of the first hill left hill is twice the spread of the right hill right which seems to be well captured here but whereas if you look at this one the spread of both the hills seems to be the same right and here it is hard to tell what actually is happening. And so this is the bias we're in straight off when you deal with a kernel estimators. So this is me as doing it directly as a machine learning exercise by using the kernel density estimator. On the other hand, now it happens that the visualization of this is when the kernel density estimators came upon the scene, people immediately realized that it is perhaps one of the most intuitive ways to understand the distribution of the data. In fact, far more reliable and more intuitive than the histograms, because histograms can be hijacked by pin sizes. And so most of the visualization libraries, they began to give support for kernel density plots. So Seaborn, for example, literally has a KDE plot. Likewise, Plotly has the dest plot, distribution plot. All of them started, and I didn't't put plotly here, but we can. And you can write in multiple ways. You can either show the line or you can show the area or you can show both. So to illustrate the point, because I did this with synthetic data. So to illustrate this point here, I've taken the auto dataset. Auto dataset is something we have been getting through the visualization class and through the machine learning class so we will do everything repeat everything here that we learned in the simplified case so we can create a normalized histogram let us look at the normalized histogram we took our arbitrary bin size of 25 about justyanar? You can see now that compared to the Colonel I mean Colonel estimators it looks histograms look decidedly. Raja Ayyanar? The exact truth, but as probability estimators they are somewhat weak. we try to do the data let's take one particular each of the numerical values miles per gallon displacement horsepower weight let's try to draw out the density estimator so this is how the density estimators look so if i were to ask you what was the most likely mileage in those days in the 70s 70s you could look at this plot and say, well, it was about 16, 17 miles again. The weight of the cars used to be around this, 2200 pounds. So obviously we see that the cars were not heavy and the mileage was terrible. Today we have a situation that even heavy cars, for example, 6000 pound cars, they give you about the mileage that you used to get in those days with a very light car. The world has changed, of course. So kernel density estimators, therefore, they give you a good gut sense. And therefore, as visualization tools, they are of great importance. They're a very central tool for a data scientist. Whenever you have data, do not ever skip making density plots. These plots are very informative. And it's surprising, many people skip it actually. They don't realize it, especially in the business or analytical community. The density plots are not very common. And by the way, these density plots, you can break it out by some categorical variable. So you can say, OK, give me the mileage by the three origin, you know, the continents of origin, Europe, America, Japan, and you can see that they are different. For example, the mileage tends to be pretty robust in this region three which happens to be japan right and when you look at the weight of the car a region one tends to prefer big heavier cars and so on and so forth so separating it out or gripping it out by some categorical variable is very easy to do And these libraries give you the ability to do that in a very, very intuitive way. How did I do that? All I had to do is add this part of the parameter to this argument to the KDE plot function. So they make it very, very easy to draw these density plots. And they have meaning. Once you realize what they mean, density plots and they have meaning. Once you realize what they mean, they have, they are very valuable. The next thing that comes is, can we make it, how do we generalize it to higher dimension? For example, in the miles per gallon, there are five or six numerical features. We are making density plot in one, but what if we take a feature plot, a feature space that is two dimensional three dimensional or five dimensional it turns out okay there is a problem here i don't know why let me fix it oh i forgot to put a dollar here and dollar sign here much better so you can generalizing the kernel is very easy so suppose you take two dimensions well it's much more intuitive imagine the x and y axis and it's like a page and wherever you see a point of data just go pour sand over that page over that plane isn't it and then you will get a kernel density all these kernel kernels will add up and give you a probability density, which is normalized over the total size of the dataset. But basically, it is this, right? And this is essentially your probability density function. And you can see that in two dimensions. Let's say that this, you look at, which is it horsepower versus mileage. This value is far more likely. So what do we learn? There seems to be, data seems to be concentrated around, this is the biggest concentration, around 75 horsepower engines, giving you an approximate mileage of about, let's say 25 miles a gallon, right? That is one stereotypical reality about cars or automobiles in those times. The other reality seems to be, there seems to be another island here. And in the language, those of you who remember the DENCLU algorithm, in the language of DENCLU, you would call these points, these peaks or the maximas, what would you call them? Attractors. Attractors. Very good. You would call them attractors. that while most cars fall under this or they tend to be around this neighborhood there are some there's another cluster of cars which is another cluster which is around high horsepower and of course you compromise line you compromise mileage so today if you look at 150 horsepower gives you uh well that's a 16 17 miles a a gallon, right? Obviously, technology has advanced. Now you have a completely different reality, I suppose. Nowadays, you can get a 600 horsepower car and it will still give you a 15, 16 miles a gallon, right? Something like that. In fact, I tend to like a little bit of a sporty cars and i would say that i fall into this this sort of category except that this value is no more 150. so in 50 years technology has advanced and moved this over to 600 or very high numbers now you can see the same data here you could see it as contours, so that it's much more obvious what are contours. Those are curves that are equal density. And you see them. And you say, well, this is very informative, but this was just miles per gallon and horsepower. But I can find many pairs. If I take five variables i can create what is it five times four divided by two is ten plots ten pair plots out of these right wouldn't it be nice to see all of them next to each other instead of one by one creating all those ten plots and like one of the strengths of seaborn i think i mentioned that last time on saturday is that you can do this lovely thing you can do this pair plots so here we go we we do the pair plot and when we do the pair plots you get a you get a picture like that i've deliberately done one thing in the top diagonal area i've just given the scatter plot of the data itself, just shown the data. Along the diagonal, shown the histogram, and along the lower diagonal, I've put the kernel density plot. So you see three different aspects of the reality. You see the reality, it's statistical discrete grouping, and then it's continuous kernel density grouping. I find this plot, by the way, very useful. And you may have noticed that in the comprehensive intro in the data science workshops that I gave, I would often make these plots to show the as an essential part of exploratory data analysis. So this is that and the code for that is extremely simple. What you do is you create, you declare, and you have to be a little bit more sophisticated here. It's not a one-liner, one function. You have to first create, you say that you're creating a pair plot using this numerical data, you know, this data. Then these are of course, just the aesthetic arguments, edge color, this, that, then then size four then what do you do you say that map diagonal along the diagonal put histograms along the upper put scatter plots i don't know if this code is visible along the upper figure put the scatter plots of the data, scattered plots. And along the lower area, put lower region, lower diagonal region, put the KDE plots, kernel density estimator plots. And which is what we seem to get. If you look at this, you have the scatter plot above the diagonal, you have the kernel density estimators below the diagonal, and the diagonal itself is the histogram. I hope when you look at this bit of code and you compare just this region, it should become fairly straightforward what we are doing here. Now you may say that all right this is good but wouldn't the for example the categorical variable the origin of the automobile was so significant. Can we color this data by the automobiles, the origin of the automobiles, and group it by that? That is straight, again, straightforward. All you do is the same thing. You say, excuse me, the hue is the origin, and you draw it out, and now you get it separated by that, the origin of that automobile. So anyway, what is the takeaway? The main takeaway I want you to take is kernel density estimators and histograms, or any form of density estimators, they're high value. We are used to histograms. They are informative, but we can do better. There are more things to bring in. Bring it in, multi-dimensional. Now we could take it to three dimension, for example, like, and maybe when we do the 3D lab, I might use this as an example and show you 3D renderings of the density estimators. And you can generalize it, you can visualize, but you can generalize it to higher dimension. If you remember DENCLU, the algorithm is based on the fact that you're looking at regions of high density, you're looking for maximas. Using a hill climbing approach, you're finding the maximas of the attractors, local attractors in the feature space given data. And if you remember, that was a very scalable, one of the most scalable methods or algorithms for density-based clustering. And density-based clustering generally are very powerful. So that is the overall message. And it's visualization. Once you realize the value of this, the visualization part comes in. We must visualize this data, right? Density estimators, not just compute it because if you compute it it's a mathematical function a density estimator function but when you visualize it you get a lot of insight into data yeah one question say the one when you split it up by origin, right? So you got, you got to see three colors and the shape of it is slightly different because you have the density by the origin, right? When you remove the separate, when you remove the part where origin was not done, the, the plot looked kind of where all of these things were meshed together and brought up. The shape changed quite a lot, right? Yes, for example, at this one, the first one, and you compare it to what you see here, do you notice that it's quite different. For example, the Japanese car, they have their own complete Japanese automobiles, their kernel density is completely different in the European ones from the American ones. So because in this data set, American automobiles are more present, they sort of dominate the scene. And you realize, I mean, the stark difference between the automobiles and how people and citizens of different continents perceive automobiles to be, a vastly different approach becomes obvious. For example, you look at this, right? This is displacement and what is the y, what is the axis is miles per gallon so these people they are or a horsepower or so they are not obsessed with horsepower isn't it they're not at all obsessed with but they would rather go from higher mileage in us we like to drive our cars you know we like to accelerate and be very fond of how many seconds, how few seconds it takes to reach 60 miles an hour. Remember the acceleration. So we tend to use more high horsepower machines. So you see the distinction and that is what these things bring about. You see that. And so from this, you know, that if you filter the data down just to Japanese, right? The entire graph, everything would look completely different, but you don't need to filter just by coloring it. You can already see the effect. All right guys, so I would like to conclude with that. I hope it was useful. I don't usually get time to explain kernel density estimators in any one of the workshops because we move fast. We did that for DENCLU, but sort of in the context of DENCLU. We did it again in the context of kernel KNN, right? But now you see all those intuitions come together. Why should we apply a kernel to a point? Because it shows that the value is adjacent to this, rather than claiming that it is this, which is likely to be, because there's always instrumentation error and things like that. So kernel KNn makes more sense right and kernel uh i mean of course for a clustering kernel uh kernel all the kernels add up to a scalar field the probability field into which you can do hill climbing any any further questions anyone else? No, it's good to see. Thanks guys. So we can stop the recording.