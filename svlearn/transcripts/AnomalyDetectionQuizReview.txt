 This is Saturday, November, of December 19th. We are going to review the quiz on autoencoders. And this also seems to have 12 questions. Let's go through those questions. This is of course the famous picture that is used in all image processing. All your textbooks will have it. It's sort of iconic. For some reason, the field decided that this would be the picture to use for seeing the effect of different algorithms and so forth. So it's called the Lena. And I believe it was first taken from Playboy. Consider the original image on the left, the famous and widely used Lenner image in the world of image processing. As an input to a denoising autoencoder, some noise has been added to the image. Observe the image on the right. And observing the image on the right, we conclude that specifically, salt and pepper noise was added. So how would you know, I'll zoom in for you. That's why I put a relatively high resolution picture here. Okay. The way to tell is, you look at this noise, for example, look at the cheeks here. And you look at the noise on the cheek. Are those noise black and white in color? I had to see more pictures to understand that. I'm sorry, you need to see more? Well, I had to go look at pictures of the type of noise because I didn't Oh, right. not quite understand salt and pepper until a little deeper into the quiz and looking up some images of noise examples. Yes. And so, in fact, I don't believe I covered salt and pepper noise. It was something for you to see this question, go read, and you did that, which is very nice. So we noticed that the noise is not black and white. It's not that some pictures have been completely turned on and some have been completely turned off, right? So it is not, the answer simply is, oops, sorry, yeah, so it should be false. The correct answer is false. And I think you didn't get many wrong, but this one you got wrong. So training of the autoencoders is an illustration of reinforcement learning, unsupervised learning, learning by memorization and supervised learning. Now notice that I didn't give you a choice self supervised learning. If I had given you the choice self supervised learning, then that would have been your best choice. But given the fact that I did not give you the choice, and as I said, some people consider it to be unsupervised learning right so it is unsupervised learning that you could choose and there was a subtlety here because you know as you notice that in the subsequent quiz i throw in the word self-supervised and it is just to make you familiar with the different words a different sort of a perspective that different people take on this. And the next one. The loss function of a sparse autoencoder has an additional penalty term. Now, what is that sparse autoencoder? Remember it is in the, it forces many of the notes in the latent representation to be effectively switched off. I spoke about it very briefly in the class. I was hoping that once you see this question you would go and read up a little bit more. How many of you went and read up a little bit more on sparse autoencoders? Oh I go look up a lot of stuff during the quizzes. Nice. So sparse autoencoders are over-complete encoders. They don't have a bottleneck. They actually have more nodes in the latent layer than in the input layer or the output layer. But the point is you would say, then what's the whole point? It will just copy the input to the output. You prevent that from happening by forcing many of the nodes in the latent layer to be effectively switched off. That's the point. And it's a form of regularization in some sense. An underlying assumption about training a denoising autoencoder is that, what's an underlying assumption of denoising autoencoder? The correction of the input must be the noise of an isotropic Gaussian distribution. No, we don't say that. We don't require that. It could be salt and pepper. The correction of the input must be the noise of a uniform distribution. No, it could be a Gaussian or salt and pepper. The latent representation is robust and stable with respect to minor perturbations in the input. And that is the whole point with autoencoders. See, autoencoders are great when they work. Training them has always been a bit problematic because they can lead to unstable instability in the in the representation and which is why you need to put in things to make them more sort of a robust so when you train a denoising autoencoder and for example one reason that variational autoencoders are were considered a breakthrough is generally a Bayesian reasoning is a very strong sort of a built in regularization in the Bayesian argument itself in the Bayesian inference itself. And so it prevents that instability overfitting. using autoencoder for example with variation you get a stability but no matter what which autoencoder you use the main point is the latent representation must be stable so in other words a picture and its noise should have essentially the same latent representation then only you will be able to produce a cleaned out picture do you see that guys so in the latent space they should map to the same value small perturbation shouldn't affect that or noise shouldn't affect that any questions there in training an auto encoder one hopes that it will learn an effective latent space representations of the data. Of course, that's the whole point of an autoencoder. Practically its definition. Next question is when we use an under complete autoencoder. So what's an under complete autoencoder? It's an encoder which has a bottleneck, right? Less nodes in the hidden layer, latent layer, than in the input or output layer. So which of these is true? The latent representation has fewer nodes in the input layer. That's it. The other one says the same or more is not correct. Overcomplete autoencoders, i.e. autoencoders which have more nodes in the latent representation layer than the input layer, are rather useless since the autoencoder will always learn the identity function and transfer the input straight to the output, no matter how you train. And so the trick wasn't the last phrase, no matter how you train. The point is, over over complete auto encoders are very effective, for example in generating sparse representations and so forth, so you can use some regularization in training. Vipul Khosla, Ph.D.: And that will make it very useful, it will prevent the system from just you know transferring the input to the output and therefore. it and therefore you can use over and you do use over complete autoencoders you just have to remember to have a very strong regularizing document somewhere what's the example of application of this sparse representation but paradoxically to create a sparse representation of a sparse representation is what let's say that suppose it's a 10 dimensional representation a vector so you can say that only any one of the three dimensions will light up will have non zero values seven dimensions will have zero values right hmm that's a sparse representation of a point yeah and what's the application though where would we use that where would we use a sparse representation uh that is a good question actually off the top of my head i can't immediately think of a place i used to know a few examples i don't know at this moment is escaping me maybe in a few examples. I don't know. At this moment it's escaping me. Maybe in a few minutes as we make progress, the moment I recollect I'll tell you. Oh, you look at this. This is, now if you look carefully at this, what is the nature of those noise? Can you tell? It is clearly black and white isn't it definitely salt and pepper definitely salt and pepper classic signature of a salt and paper and consider the noise that was added to the image while training a denoising autoencoder and the resulting image that became the input to the autoencoder shown below. By observing the image, we can conclude that the added noise is salt and pepper. What was the other noise? What was it? Was it even? Three noises are pretty commonly used. One is just uniform noise. And if you look at the code sample that I created for denoising autoencoder, I don't know, did I use the uniform or the Gaussian? One of the two I used. The other is the Gaussian, bell curve noise, right? Small, centered around zero. Lots of small noises, very few big noises, big shifts. And salt and pepper is extreme. There is no small noise. It's all big noise, either black or white. So it's the opposite of the Gaussian is the salt and pepper. You got that Sanjay? Yeah. Yeah. So Gaussians will shake it a little bit most of the time and a little more a fewer of the times. Salt and pepper will be just extreme. of the times. Salt and pepper will be just extreme. So of course sparse autoencoders use an over complete representation. It was a simple straightforward one. Next question. Actually I think there is a typo here. It's a contractive, not contrastive. A contractive autoencoder aims to learn a latent representation of the input that is robust to slight perturbations of the input so how is this different for example from a denoising autoencoders, and I give you the math of it. If you remember, we use the gradient of the Jacobian, the squared gradient of the Jacobian, or the gradient of the Jacobian, a dot product with itself. You add that as a regularization term in the loss function. So something that you probably forgot, but that's that. If you go back and look at the notes, you'll see it there. Next question, consider the vanilla version of the autoencoder. See, often I use the word the vanilla or the direct version. What it means is the auto encoders is a whole subfield now. People have created so many variations and subtle changes to the auto encoders that you almost feel compelled to use the word vanilla version when you mean the simple, you know, the way you learn it first before you throw in everybody's own special variation. Consider the vanilla version of the autoencoder which reproduces the input in the output and learns a latent representation of a lower dimension than the input. Let xi be an input and xi prime be the corresponding autoencoder output. Then the loss function can be written as, well this is just remembering the last function there's nothing magical here uh this is reconstruction law so you compare a thing with this oh there's a type i don't know why i became cap but xi minus xi prime squared um what are the other choices i put here Other choices I put here. Here, the square is missing. Here, KL divergence has been put. None of the above. And this is cross-entropy loss. Certainly not that. So obviously, if you know the very basics of autoencoder, this is your last one. Most of you got that right. Then last question variational auto encoders are generative models right of course like what do you learn internally you're you learn a gaussian representation low dimensional gaussian or latent gaussian representation of the input. And when you have done that, now you can go on generating as much data as you want by sampling from the Gaussian representation. So that's the beauty of the variational autoencoder. Well guys, that's that.