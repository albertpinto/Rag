 All right folks, can you hear me clearly? Those of you who are remote? So, yes. Wonderful. So welcome back to the to our 10th week of the LLM boot camp. I hope you're having fun. You may be a bit tired working on the projects, but I hope you're also having fun. Once again, it's a beautiful California morning here, Saturday the 11th. It is also based on where you are, Diwali. So I wish you all a very happy Diwali. Today, the topic is Document AI. Document AI is a really big topic. It merits a little workshop of its own and we have to compact it within a week, and today we'll compact the paper readings and the core topics to just a few hours. So we have a lot of territory to cover. Now the way to look at it is, if you look at it in the context of your project in the project, you have our text that you are indexing for your search. You're doing drag, you're doing question answers, you're doing essay generation from your text. You're also doing it for images, image comprehension, visual language understanding, video understanding, depth image understanding. Then the next stage that we went to was the generative aspect in which we said you have to generate a complete essay out of the search responses there are, but it must be understandable by an eighth grader, by a young person, and you also had to generate images using a diffusion model we took a digression we understood about adversarial attacks on these systems that can be made and how do you defend against those adversarial attacks, the situation is as far easier to attack than to defend. There are no very clear proven ways to defend at this moment. Then we went into audio. The last week, we gave a lot to the audio generation aspects of it. So on the audio generation, of it so on the audio generation if you have reached this far i thought i'll give you guys a bit of news one of the things i gave this project ultimately should have one of the projects is you should create a virtual teaching assistant right how many of you made progress or have succeeded in doing that or come anywhere and made any progress on that? No. So I will as I was then I shouldn't release the solution. But I'll give you one hint. I took all the videos at support vector so you may not be able to so easily download it because you'll have to scrape the web there are ways to do it but you'll have to scrape the web there are ways to do it but you'll have to scrape the web so I'm going to give you the video IDs of all the videos YouTube videos there are about 350 of them videos they cover the entire range of topics from data wrangling data visualization machine learning right or date the the tab machine learning, the deep learning and everything, transformers. So I took a few of the videos and I saw when I passed it through Whisper, the output is pretty good. I thought it would make a lot of mistakes with my accent but it did an amazing job uh v3 the latest v3 it's just amazing and uh one more thing that i found i didn't know that i mean i sort of knew that i talk a lot but it becomes embarrassingly obvious when i look at the transcript because it turns out to have no less than 35,000 tokens in one video. 35,000 words per video. So that's a whole lot of words. But what it also means is that you need to have, if you do anything like summarization or anything, you need to be able to support that context length. An alternative thing that you could do, use a large language model which spans such a large context length. I did that. I was able to successfully have really good quizzes out of it. And I asked Bart to evaluate the quality of the lecture. Can you guess what it said, anyone? I suppose I'll show you guys what it said shortly. It was quite instructive. It made some very detailed feedback on the lecture, but basically it said very clear and so forth. But it's amazing that it could give such detail. And I asked it to specifically point out if I missed a topic or is there any area to go deeper into and so now you can get all of those answers we live in a world that is heavily ai centered so once again remember guys we have taken a long journey how many of you have felt that we have really covered a lot more territory than we imagined we would cover more territory than we imagined we would cover. And yet, as most of you feel, you have covered a lot of territory that will become obvious. You will, I must say that it happens to be less than half of what I thought I would cover. So I, one of the things we need to think through is what do we do with the rest of the topics and how should we frame it out a lot of example fine tuning is yet to be done graph neural networks is it to be done a vital topic as anomaly and fraud detection. outliers anomalies and fraud detection. We haven't touched upon graphs are very important of course things like alpha pole etc come into that some crucial topics in reinforcement learning along with this is very much in context we haven't covered that and yet we have been moving i hope you feel very fast isn't it so we need to I would like to have some proposals of doing that. So one thing is that this is enough, we are tired. The other, and so no more courses, no more bootcamps, et cetera. This is the terminal one. The other alternative is we do go and cover all of those topics. Now, just to give you a measure of how much I'm having to condense, Document AI is a vast topic. It covers a breadth of history. You'll be surprised when did Document AI start? Like our ability to extract data from just a picture. When do you think it happened? Make a guess. One year ago. Now it goes back to, well you'll find how far it goes back to. Soon you'll find out. So once again, support workers, we are shapers on your journey. I hope those of you who reach out to us and ask for help are getting the help. Is there anyone who is not getting enough help? If so, please do reach out to me directly. There are a lot of us willing to help you. This is obviously the mission of Support Vectors. We translate leading-edge research and innovations into high-impact products, which by now you must feel that we do have expertise in. This boot camp is all about this going from theory to practice, theory to enterprise class products. It's it's very easy to write five lines of code, copy it off Hugging Face or some blog and say oh look look ma i did this right it's an entirely different thing to be able to build something practical and go to market with it and you will realize as you guys are doing this i hope you're realizing that you're facing issues of scale you're facing issues of automation, right? Of management. By the way, one other important topic that I missed is feature store management. And I'll cover it at some point. But anyway, those are all like, it's a big world that we have to cover. And I keep saying this, this slide by now, you guys must be tired. I always start with this. The point is to drill it into your heads. I hope by now you know that AI is more than just ML models. To build something of value is a giant ecosystem. It's true for everyone. For example, chat GPT, the only thing that we know is the amount of infrastructure that has gone into it is far more than anyone imagines. That's obviously about Microsoft and OpenAI CEO, they keep pointing that out, which means we don't know what has gone into it, but it's a huge amount. But what is true of that is also true of any of your projects but by the time you're done with an ai project you'll realize that a lot more goes into it than just more just a model or two right it has you have to build the data infrastructure the ml platform the ml ops you have to train the people you need to gather a bunch of really bright people around you to get it done you have to worry about the ethics and interpretability ai is dangerous or you have to worry about the many things monitoring security compliance etc etc the data visualization and it's a very iterative process this is your mlop stat are busy, data scientists and AI researchers are busy building models, better and better models. Data engineers are busy retrieving the latest data, creating accumulations of data and feature engineering. The DevOps people are busy creating the entire pipeline to take it all to production. So it's a big journey that happens. And this journey has by now many, many steps. These steps, the main pipeline you're now aware of, it takes ingestion, cleaning, validation, transformation of the data. All of those are data stages stages sometimes labeling of the data these are very expensive steps quite often it eats up 70 to 80 percent of the time and you will realize that when i ask you to find you right so there is this part when you are doing the teaching assistant, AI driven teaching assistant on all the videos, you'll realize that you can't do it without fine-tuning. A good teaching assistant will need enough fine-tuning. I want you guys to collaboratively discuss and figure out where exactly you need fine-tuning. That fine-tuning will require you to prepare a lot of label data, right, which will, and I'll give you an idea, people who came and asked me, I said sit down and create at least 2,000 instances of label data, a data set of at least 2,000 rows for each of the tasks. So obviously I'm leaving it as a hint because I know that most of you have not yet even started thinking about it. So I won't give you out the solution, but you have to do all of that. Then comes the training stage. You will realize that you have to do a lot of experimentation to get the virtual assistant going. And please do it well. point. Right. And please do it well. Really, I'll request you to do it well. It's a very focused problem, creating the virtual teaching assistant. And it well, think of it, if you do it well, it's a gift to the next cohorts, the next generation of people who come here, because whosoever does it best, will make it official and use it will host it on our portal and use it so that's that so there's deployment considerations we uh as you guys know it takes a while to deploy these things complicated infrastructure so then you have to do monitoring so what's our lesson plan for today? Today, as again, our format remains the same. We are doing some, oh by the way, some administrative notes. Once again, teaching assistants are here, logistics, food, logistics, you guys are fairly good with keeping your rooms clean so um but occasionally i find fast food remnants uh do clean it up or if you use utensils bring them back to the sink and rinse it otherwise Otherwise, I'll be after a long day. I'm busy rinsing those dishes. So please help me out with that. Teaching assistants are available to help you. And now that we've walked through what we have done. Now guys, take this audio part, the teaching assistant part again, very seriously, you will learn the issues involved in multimodal learning. Like when you deal with audio, what are the qualities? It is not that easy to train a transformer models and get it to speak to make an AI speak with my accent or my style of speaking. So you want the teaching assistant to converse with you in voice? In voice, yes. To be able to talk in voice. So imagine you have a voice interface. You ask it a question, it goes and finds the right information, generates a text. Now that text doesn't exist in my voice, but it has to synthesize and speak it out in a way that I would speak it out. It should capture my accent, my choice of words, my linguistic mannerisms, and come out. Is it possible? Anyone? Is it possible? I think it's possible. It's dangerous. Very dangerous. Everybody is scared. Already you're getting calls, right? Somebody, I mean, virtuous, is calling you nowadays. Right. I don't know that my body's answering for that. Yeah. You know how many divorcees are going to happen? People playing crime. That's true. So, guys, let me put it this way. I have the solution with me. Right? But I'm not going to release any more solutions. I've just released small snippets. Because I know that you guys are behind. And the basic rule is don't release a solution till everybody has tried. So at this rate, if you continue, you'll see that solution in January. So please, please make a little bit more rapid progress. I know it's holiday season, but you know holidays can have their own blues, this dark winter cold winter nights. What better thing to do to cheer you up than to start writing code. All right, I'm joking. So we will do a lot of paper reading. Actually, today we'll do six papers. I'll cover some of it, depending upon how much time permits. I will do guided paper reading of some, or maybe some you guys can do. Because it's a vast topic, and I have condensed it into a single day. We'll do our team presentations. Is there any team that's willing to do a presentation of their work? Please go ahead. Yeah, we might be able to do that today. Okay, wonderful. So we'll take that. On the generative AI guys, today surely the Cambrian Explosion series, surely we have something to share, right? The OpenAI has just had a new release. Many of you must be excited about a lot of things a lot of breakthroughs are happening literally as we speak any how many contributors to the Cambrian explosion uh anything new share something new there wasn't much interesting stuff this week oh there was but you you are busy okay so uh in that case we may use because we have a lot of research papers to read i may stretch this much further we have a lot of papers to read so here one request asks if last session we wasted a lot of time can we stick to time today and under little early in interest of diwali so let's not wait today and stick to the schedule sure we can do that we could do that uh i suppose it is uh it also means that i'll have to do so so guys how would you prefer like i'm ready to do all the paper readings would you prefer that or would you rather that you guys contributed and stepped forward for paper readings can you do the reading today and early can i do the paper readings and and early two separate requests uh uh and early we can do irrespective of who does the paper readings. Anybody feels that you guys would prefer doing the paper readings? Any volunteers? No. Okay, I'll do the paper readings then. So let's go over the territory we have covered so far. I called it the journey of a thousand miles and it's beginning to feel like that isn't it right. So we did we did text processing, we did vector databases. Would you agree we have done these things right. We did hybrid AI search using that we did multimodal search. We did oh memory is misspelled. Memory efficient fine tuning. Laura and so forth. Come again? Forgotten? Forgotten, eh? No, no, not yet there I suppose. Okay, yeah. We did retrieval augmented generation, RAG, an active RAG. We did diffusion models. So you're saying, did we read the paper or did we do it? We read the paper, we didn't do it. Diffusion models? No, the memory efficient fine tuning. Memory efficient fine tuning? We read the papers. Yes, that is true. We read a lot more papers. You haven't done it. We have read a lot more papers than what you have done. Than we have done, yeah. So you would need the LoRa thing, you would need for the virtual TA. The virtual teaching assistant will need you to do LoRa. If you want to do it well, you can do without it. You need to do domain adaptation. That's a hint. That's a hint. So think about it this way. It's a domain adaptation. We are talking of a very, very narrow domain, machine learning. Therefore, to get better answers, you need to fine tune. I agree. That part I agree. But not for virtual assistant. The point is, what should be doing a tech search after that what you need the the clora or lora is to find you a broader like a mystery or something on the limited content for that particular field that's what i want to do field that's what i want to do that is for the virtual d because i thought virtual d is what we did last week adaptation to your accent no no there are two aspects to it one is the the actual content that is produced by the teaching assistant that will need laura right because if you look at it you already have actually approximately 35 000 tokens per video and 350 videos so you have millions of tokens absolutely the right amount for fine tune and then only will you be able to get answers much more precisely based on what is said. Why can't we use RAG for this? It is RAG. I'm saying you need to fine tune that. But why do we have to fine tune? We could do an index search. We already have the hybrid search. We can do an index search and then feed it. Okay. Who would like to answer that question? Infant. Okay. So not just content, if you want to make it to sound like Asif and to get Asif's choice of words when he decides to format a reply, passages that asset says so that so that it becomes like a template of this is what how asset would probably answer the question and this kind of sentence structure this kind of choice of grammar and vocabulary pitch your pitch yeah that's not even just the content so what patrick said in case you didn't hear and he got it absolutely right. See, it is not just content, the semantic content. When you want to speak like a person, you have to capture the linguistic mannerisms and the diction, the language. Everybody has a unique speaking and writing style. And to sound more realistic, And to sound more realistic, you need to capture that, the style. And that style capture, that part, is where you would use Lula. That is one part. The second, of course, is the voice part. The second part of the project that Satyam was referring to is the voice part of it. Of course, you need to train on the voice and speak that that also you need to do so that answers the question yes go ahead yeah i said the question was that say suppose you have these videos where the content is spread across uh different videos right and we ask a question that takes takes snippets from two different videos, it has to put together and make a cohesive, a concise, like summary of those two snippets come together. Isn't that also required as part of the fine tuning initially, like the chain of thought processing or something like that? I may be saying it wrong. Yes. So Sachin, there's a lot that you can do uh depends upon how much you are willing to do but what i would what i'm saying is that use this as a good reason to use uh things like lora q dora etc right because that will get you into the game of fine tuning there is a big infant server it's just waiting for you guys to get busy with it. All of you have now, each team has an account on it. I know of only two teams that are using it currently. So I was going to quote it that every team will get only one day because I anticipated that all of you would be jumping all over it. But instead at this moment, I guess it's tired's tired i mean i overestimated how fast i can go so you guys are a bit behind but when you come around to it you'll realize that fine tuning there are many occasions or places to fine tune and every time you fine tune the performance or the the how well your model does goes up significantly why it's a very limited domain what i have taught in the millions of tokens that i have produced right actually it's more than 10 million 10 million tokens that have reduced is it big data it's almost big data right so it's a very it's all about one topic it is a perfect use case for fine-tuning. My language, I am told, is unique. In fact, not just told, if you do a linguistic analysis and statistical analysis, you can literally see the fingerprint of my language. And I know that it is quite distinctive. fingerprint of my language and i know that it is quite distinctive people spot my language from a mile away in the companies that i work people would always know that i wrote this document or this paragraph so it is a perfect opportunity to fine-tune yeah so I want to understand the difference between like a rag versus a fine-tuning. Okay. Like, not to be over-deserved, right? See, the thing is, whatever you have taught, let's say we transcribe it, we have the text for it or whatever, we use it in our rag system. It will give us the content to answer the questions so the fine tuning is actually needed to produce it in the form that asif says so it is for the text of the dictionary i mean that is one way to do it that's one thing you need but think about it this way even the vector embeddings that you do right because you're talking of such a limited domain. So what happens is, so okay, think about this. You have a model that sentence embedder that does sentence embedding, right? What it does is, it will efficiently use the vector space. So what it means is all the world's knowledge, it has diffused across the entire vector space now what happens you bring a very domain specific data machine learning lectures you you use that sentence embedder what will it do it will go and populate a tiny corner of that vector space right no so it is occupying a tiny so hello is it does or as if it's as if I think we lost him. Thank you. Hey, Kate, are you trying to alert Asif?ご視聴ありがとうございました Thank you. I think there's a power outage or something. Thank you. So are we planning to go till the end of what is he doing but we did propose to end early okay anybody uh in there in person who's also on zoom let's ask all absent people raise their hands Okay, he's rejoining. Mausumi? Yes? Mausumi, you said you're presenting today, right? Is it going to be a day or a night? Mausumi? Yes. Mausumi, you said you're presenting today, right? Is it going to be on Zoom or are you going to do it in class? I think on Zoom. On Zoom, okay. Yeah, we're not planning to be there. I'm a little pissed that we're having this class on Diwali. So I don't plan to be there. Okay, okay. Yeah. But I thought like we had to do a good beginning on Diwali, right? So there are other ways to do good beginning. Okay. My wife is also not too happy. I'm with you. But not too happy sir i am with you but he's like i i talked to asif last night about that and he said he'll make it very short doesn't look like it's going to be shot i have a lot of homework madhu I have to clean dishes, vessels, and all of that stuff. Yeah, a lot of festival chores and the class is like too much today. Yeah, and this is how we are doing the class. We might have as well given the day off, right? This is what I mean by wasting time. Yeah, I actually recommended he did he uh did the class and tomorrow but you know yeah that was a good idea yeah well you know as his daughter is home so he's still doing the class you can imagine that yeah we'll uh i think he'll keep it short I think he'll keep it short. I'll talk. All right, folks. I am back. I apologize. We had an internet issue in this place for reasons that I will understand soon. Hey, Asif, in the absence of the emperor, there was a big rebellion. Oh, what happened? Revolution that happened. You didn't know about it. Tell me. Madhu will give you what's up everyone was hoping that we could keep this class short today and leave off the festival oh i i on the other hand was hoping that we'll all do firecrackers in diwali at support vector success was hoping that we'll all do firecrackers in Diwali at support vector success. No, guys, I will make it short. Thank you. How about this? Document AI is a big topic. How about this? I do only half the topic today and we cover the rest next time. Is that fine? Yeah, let's do it like that. So guys, that is it. We have to do LORAS and there'll be opportunities to do LORAS. We did diffusion models. We integrated it into your project. The one thing that we haven't integrated into the project is adversarial attacks, but we did cover adversarial attacks. It is important to know that despite companies coming up and saying they'll protect you against adversarial attacks, but we did cover adversarial attacks. It is important to know that despite companies coming up and saying they'll protect you against adversarial attacks, most of those attempts or so-called safeguards are snake oil salesmanship. Right. Because to my knowledge, nothing works. Um, if you look at the official solution for that, please exactly train it on adversarial data. Oh my goodness. And that is the official security solution. Yes. So Satyam is pointing out that the official security solution is to train it on adversarial instances. You cannot. There are infinitely many of them and your model will become poor and it's not going to help you anyway. Then we did voice cloning. We did a lot of audio stuff last time. And now today we are going to do document AI. So I must modify that by saying today and next week. So we are going to give two weeks to document AI. It's a big topic. Right. Uh, but then core what is, what is that? Anyone remembers what this means? Who goes there? Now try it out better. Very close. Quo vadis? Where are you going? Actually, it's a biblical reference. I think you probably remember this one from the it's from the new testament okay so um where are we going so the question is where are we going i would like to give a minute to discuss that see guys we are moving very fast but we are not covering all the territory so one of the things that I would request you guys to discuss is, come up with a format and obviously it doesn't mean that you're committing to registering for this course. You're just helping me in curriculum development. I will list out the topics that we haven't covered. You guys suggest what should be the format of covering those topics. You guys have made good strides. As you can imagine, we have covered a lot of topics here. A vast territory we have covered and yet vast territories remain to be covered. Come up with the idea of how we should go about that in the future. So today's topic is document ai again a vast topic so it is using ai to process documents documents usually mean this doctor you see this document guys yeah you see that this color here there are footnotes here there are things there are figures right i just randomly took which one was it excuse me this was the active rag document sitting on my table and by now you know that whenever i read a paper i tend to do a whole lot of highlighting so you see yeah so this is what it is about so what is document.ai document.ai refers to the automated processing and understanding of documents using machine learning and natural language processing techniques big big sentence but that's what it is document.ai allows computers to analyze extract data from and make sense of documents in an intelligent way anybody can tell that I don't typically write like this so I was going to write and I felt lazy and I asked I think Bard now define document here so it came up with this or was it chat GPT I forgot which one I asked it and this is what it came up with this or was it chat gpt i forgot which one i asked it and this is what it came up with i tend not to write such language is a terminology in the ml community yeah yeah yeah it's a formal term it's a it's a subfield it's a big subfield this is probably very new document is probably very new correct document yeah well let's see so here it is the ancient history of document ai 1966 in the world of vacuum tubes right so if you think it is new think again why because as computers came one of the first questions people asked is, oh gosh, we have libraries full of books and offices filled with documents, government offices, bureaucracies. Do you guys remember the word red tape? It comes from actually folders that had red tape around them, which would contain your case. You apply for a loan, your loan application would be in a red binder, a red thing, and it used to literally have flaps. And you would cover it up and then tie it with the red tape. You remember the office thing that used to write the next person's name and use the same folder, put it in the out thing and it goes to the next person. That's right. Yeah, it would go to the next. So all of that that world mountains of paper documents are all sitting there all we can say is people have used scanners to scan a large number of them as you know google has scanned huge vast libraries a madhu put online i think a link to some 10 million documents or 10 million or million textbooks that you can use so long as you're willing to do good document AI, good text extraction from it, accurate text extraction from it. So that is the challenge. You guys can use that, right? This is it. So the first commercial OCRs were already coming and doing pretty well in 1992. Then came deep learning. And by the way, those were not bad. I mean, a lot of them use support vector machines and other things, and they were doing pretty good. Then came the deep learning, changed the game with CNNs and so forth. The RCNNs and CNNs, recurrent CNNs, et cetera. These were game changers. In their time, there were big revolutions. In fact, the RCNN model is still in fairly widespread use. Then came, obviously, transformers. And then people began to ask how can we use transformers and then those transformers became this big large like some of them became large pre-trained models because soon people realize that you can't train a transformer. You need a lot of compute power to train a transformer. So transfer learning became the norm and so you started with pre-trained transformers and the rest is history now if you feel that this is one backwaters of ai think again this is one of the most active areas of research for a simple reason solves a very practical commercial problem. So remember, you guys are creating in your project, you have taken in text, you have taken in images, you have taken in audio, you have taken in video, what have you not taken in? Unstructured. Yes, unscanned documents. Right? Or things written. For example, can you look at this image when I talk of coffee and I give you a picture of this cup? Can you bring up this in your search result? Right? So things like that. Or if I were to ask you, do you have AI that can read what's written on this? You realize that it is not very geometric. It's not rectangular. One of the fallacies that has been there in this world is just find bounding boxes. And from there in each of the bounding box, then try to figure out what's there, rectangularing box it wouldn't work because the text is first of all it's set at an angle and it is on a curved surface do you see how hard the problem is right so it is a big area of work much much has been done but much still needs to be done. Let us just say that it's a work in progress. No, chat GPT vision will do it. Chat GPT vision will do it. Try it out and see how well it does it. Yeah, go ahead. Are these actual milestones in document AI? Yes. Because I noticed that the gaps are decreasing, like 30 years to 20 years. Oh, yes, yes. I did not space it yes i did not space it i did not space it uh i just uniformly spaced it it is non-specific but does this mean that it's we're we're creating we're reaching a point where it's it's exponentially already yes uh most developments and most big things have happened after 2020 right in fact the papers that we are going to cover are all 2019, 2020 onwards. So. Hey, I have a question. Yes. Going back to the timeline thing, would you think that the 2020 and with large language models, it was some sort of a reset and it's a new generation of OCR capabilities built? I would say, see, in AI, right, you don't use the word new generation. Yeah, new generation, you can say. I wouldn't use the word reset because, I'll come to that, because of the no-free-lens theorem. There will always be a class of problems somewhere, practical problems, that an old model will do much better at. I'll give you an example. Even today, there are many, many OCR situations where the RCNN does pretty good. An older model, it does pretty good, older model it does pretty good actually oh sorry just bear with me there is a problem with this so guys i'm seem to be having a whale of a problem with things so i'll go to the pdf version which i uploaded to the course portal or did i just lose uploaded to the course portal. Or did I just lose internet all over again? So guys, I hope you're aware that all these lesson plans, videos, everything is uploaded to the course portal. Even I go there as I'm going there to get a handle on it. All the live session recordings are there. So today's one was lesson plan 2, Document AI. Let me open that. Oh. Oh, can we save downloads? Give me a moment, please. How do you go into the presentation mode? This is all very, ah, no, I don't want that. Let me just open it with simple Acrobat open open with acrobat reader and yet isn't there a view mode, page navigation, full screen mode? Okay. Let's go here. Oh. I have no idea what it is doing. Yes, it's Windows. It's the last time. Okay, I turned the towel. I'm moving to Mac. Since I can't use Linux to write on, so for the teaching thing, I'll do that. All right guys, good day. Sachin, what are you doing? Today is a good day to make the move, sir. Okay, we'll do that. Diwali, yes, of course. Time to change direction. Invest here. Go ahead. Yes, I had a question regarding the previous timeline. Could you give a sense of the difference in capabilities that were brought in by, like when, say, there were improvements made in 2010 versus 2020? Because, I mean, if we think of ocrs for example i've used some let's say expensive items right that they ask you to upload uh receive uh to expense right and it will be right away so yeah so could you give a sense of what was not possible then like maybe a high level really very good things like okay when you use our CNN, right, or CNN-based methods, which started coming from 2010 onwards, we now know that attention, for reasons that we don't quite fully understand, is a fundamental unit of neural networks, more fundamental than you thought. is a fundamental unit of neural networks, more fundamental than you thought. And so there was a missing crucial thing. Now, OCR even today is not a perfect field. It makes a lot of mistakes. But the amount of mistakes that it made before and the amount of mistakes that it makes now, there is a sea change. Things that we simply didn't think possible without transformers before the world of transformers are now possible we give like an example uh as to for this simple example of say reading a receipt right yeah what was what could not happen because on my end whenever i would like upload a receipt it would come back with hey this was like yeah yeah that is right so if you look at the quality of what it read with transformer based models it's very accurate so okay let me put it this way there are two aspects to this document one is the document detection detecting what is there in the image that the bounding boxes this is the text area and so forth. The second part is understanding what's in there in that box. Right? And today both of those aspects. So the first is sort of like segmentation. Segmentation we used to do using again CNNs with the coming in of VIT, the visual transformers. with the coming in of VIT, the visual transformers, that itself has become much better. Right? The second part is the understanding part of it. Once again, the moment it comes to understanding, because of attention, like you're able to tell this in the context of what has the previous couple of characters right um it has become much much better see if you think of the cnn right it calculates in a very local area whereas what does attention do it will look across those patches all of that text to determine what word it possibly is you see the difference right and it makes a huge difference. And you'll see that. That is exactly what our paper readings of today and next week are. As you go through the paper, they will talk about it, that this couldn't be done and now we can do this. The claim of each of these research papers will be that so far this couldn't be done, now we can do this. that so far this couldn't be done, now we can do this. So like maybe I'm, let me try to come up with an example. So let's say I have two seats, right? A grocery, a receipt and let's say, some bill from a restaurant I went to in 19, that's something from. When you say understanding that what is written in those scanned images and when you talk about context is it this context like in what uh context this invoice was created and uh is that the context you're talking about everything everything so for example given a page a typical scientific document, right, has, let's say, look at this page. It has tables, it has text, right? Look at another, let me just go to another page. It has sort of a table box, it has a figure, it has paragraphs, to be able to take it apart and know each of the things that is there extract the table as table the figure as figure and so on and so forth we couldn't do that before and line it up as it appears as it is that is right exactly we couldn't do that and that's where we are literally coming to see just hold on to this show the slideshow uh processing because so what are the things you need to do there are things you should do even before you do it clean up the image right denoise that do things and you will see what are the steps you need to do as pre-processing it says image processing but think of it as pre-processing don't just feed your image into it which is what most people do they shouldn't there should be a pre-processing step that sharpens and clarifies the image people used to think that you need to do ocr one of the basic ocr optical character recognition and then you do layout analysis entity extraction that this is the address this is the person's name, this is that, so forth. And then some downstream things. Let's say you're classifying it. Is it an invoice, right, or is it a tax form, right, things like that. So this is an example of a pipeline. You could do that, but some things are important to do. Like for example, binarization is a word, it's an odd word that is used to say that you want to convert it to black and white, because for the text extraction purpose, it's much better. Now, if you want to recognize the cat in the picture, in the document, then perhaps not, but binarization will help you because you know that text is usually written in black. And even if it is not written in black, converting it to gray scale, it's to better use. But de-skewing, you scan the image like this, just straighten it out. So you see that these days, all these applications on the mobile, like Lens and other things they do that right? Documenters like this they'll straighten it out. So that is de-skewing and when you straighten it out what happens is a lot of these things they assume horizontal rectangles, they build horizontal rectangles. Right? As a fin binary, so if you have a black and white image of a cat do you create a third channel or you just keep it still you just keep it still so you don't standardize no no no what you do is you take the image and the output is another image which is binarizing that's it and de-skewing is important because why the bounding boxes that you make a lot of them they just assumed vertically and horizontally aligned rectangles so if your text is like this it better turn otherwise it won't do in fact that is one of the limitations and i would like to cover one paper next time called text snake which is actually a lovely paper but not much recognized and not much talked about but quite a bit used actually in practice um that deals with uh suppose a text is written literally in a peculiar shape right or for example here look at this the text here you notice that it is both at a slant as well as it is curved right how would you extract this for situations like this it does pretty good we'll touch noise removal remove their speckles and artifacts so for example you could use a denoising autoencoder right or remove coffee stains or infilling so okay okay All right. So optical character recognition is the granddaddy of all document AI. A lot of people when they think document AI, they think, oh, OCR. It isn't just OCR. In fact, one of the things that I want to, and that may surprise a lot of people who have been in this field also, that you can do document AI entirely without OCR. That's one of the more, like for example, we are talking about recent breakthroughs. That's one of the big breakthroughs that you could actually forget about the character by character recognition and still understand the whole document and extract text from it. Right. You don't need to go to the level of the character. So, So OCR is to create another data set that is just the text? Just the text from it. And there are tons of OCR commercial software. If you ever buy yourself a physical scanner, I'm sure it will have some basic OCR built in. So it's not the training of a system that can recognize text, it's the actual generation of the extraction of the text in the document yeah so imagine that you give this page and it will just extract all the text so you're paying us at this current age of technology we don't even have to go to that step of creating that separate data set of your text so So, see what has happened is people just assume that OCR is the first step. It's called document detection. Where are the bounding boxes? What is the text inside the bounding boxes? That used to be the traditional way. boxes? What is the text inside the bounding boxes? That used to be the traditional way. Very recently, to our great surprise, a whole set of transformer-based models have come that says, ah, forget about that. We can get away without it. And it's a surprise. So we are in a transition phase. The OCR-based methods are still very much there and dominant, and not many people have woken up to the fact that you can actually do without it. That's a new thing. That's why I mean, the whole point of this bootcamp is to bring you up to the latest. So this is it. So. So look at this, I wanted you to see the seriousness of this. Look at this page. Right. This page is a real page that is there. By the way, this is an example taken from Hugging Faces. So all credit to them. I should have said Hugging Faces somewhere. Look at this. What does it not have? It has tables. What does it not have? It has tables, it has figure, isn't it? It has text. This is how scientific documents look. And if I remember right, Shivani used to be very concerned, how do I read tables from text and so forth so and i told her wait and the answer will come i don't know if she's in the audience shivani no um anil anyone of you no okay so you can do. And this should give you an idea of how tough it is to do document AI. You see that, right? It isn't just extracting plain text OCR. There is a lot more happening here. So for the first plot, is it like interpreting that plot and describing the trends going down versus adding. You can do that. Yes, you could do that. Yes, that's the whole point. It will recognize it as a figure. And then you can ask this question, what is this figure saying? There's a lot happening here. There's a text, there's a table. To be able to extract the tabular data as it is. We're entering layout analysis, I say. Yeah. We are. The goal here now is just to create the segments in the document. Yes, this is a layout. This is a given attack. This section means a particular part of the document. That is a particular part of the document. Selectable of contents generated. Exactly. And in fact, almost all the state-of-the- art models that do this layouts are language models. They are transformers. In fact, one of the leading was literally called Layout LM, which is in v3, version 3 at this moment. This is only for an image document, you are saying? No, no, this is a layout detection example of what it could do what format should it be from pdf oh yes yes pdf of course you're doing it from pdf because a pdf is a pdf is an image format it's one of the recognized image formats that is right and i literally have a paper reading on how to convert archive documents into proper good text now all of you used what what did you use ticker to convert or something or pi mu pdf or something or the other to extract text and did you notice that it was rather gibberish? No, that is fine, but we left out all the tables and images. Exactly, that's what I was pointing out. It misses out all of that. So the question is, is there a technology in which you don't miss out that? Tesseract. Tesseract. No, Tesseract won't do. Tesseract is actually one of the worst things you can think actually is the most popular thing but for a situation like this scientific document like this tesseract is a disaster and the reason ticker failed is because it used tesseract it's only for text it does right because has integration with this right so anyway also tesseract doesn't recognize handwriting right like in most of the medical claim processing and invoice processing this is one of the issues that almost every company faces the limitations of ocr yes and they all resort to cloud solutions and even Amazon textract uh it probably performs the best but it still has a lot of uh you know short oh well I wouldn't call it the best but I'll let you guys benchmark it that's one of the exercises try these things out you'll realize that the state of the art is more forward. I mean, are you clear? We're using extra for handwriting. Oh, no, no, it is the most popular use, but let me give you a hint. There's something, there's a new kid on the block. Right? So hold that thought in your mind. We have to do. So what is it? What are the tasks that you do in documentation? So simply, what are the paragraphs? Detect the paragraphs. So in this, what are the paragraphs? Here is a paragraph. Text, text, text, text. Can you break it up into paragraphs? Then you have to do table detection. Table detection is yet a specialized task. Detection table detection is yet a specialized task. Do you notice that you don't just say this is a table, but you have to tell what are the columns in the table? What are the spanning cells, right? Text cells and column header set, etc., etc. which is more complicated than that. And to be able to do all of that is quite remarkable that we could do that right figure this is straightforward detecting which part of which is figure uh going back to that sorry going back to that earlier thing um this is where we initially started with 10k documents which has a lot of tables and we were trying to see how we could improve on second sites dot ai which actually doesn't do a great job with tables thus far and 10k is all about tables yeah and a hint hint there's an internal project within support, we looked at 10k filings and we started it out. Praveen actually did a lot of the early work on it. One of the things that I did is I took one of them and because this question was asked by Shivani, can we extract tables out of it? You can and you'll see, not only can I, but now you will learn how you can you extract tables out of it you can and you'll see you will not only can I but now you will learn how you can so I have also put in a link llama index games they have an unstructured.io which seems to be doing a reasonable job I haven't tried it out I put a link on chat yeah so all right let's continue guys i'm almost done so look at this we have text versus non-text now do you notice that look at the logo inside the logo is a text to be able to tell that there is a text there and everywhere there is a text now do you notice that i'm doing it all with printed text the moment you do handwriting things begin to fall apart so the state of the art today is not perfect with handwriting it's a very very imperfect thing at this point the other is reading order in what order should i read left to right top to bottom what is the word and things like that headers and footers like for example to know that this is not actually part of the text it is just the header or a footer right so these are things then after that you can do information extraction you can take out the unstructured text you can extract out the key entities key relationships semi-structured things like tables, forms, surveys, downstream applications. So many things you can do. After that, you can use it for document classification. All right. You can categorize using a classifier. You can classify using a predefined rule, things like that. So you could do all that. So anyway, I'll just give you a few uh open source projects um uh remember no cloud project this is open source only uh that does a very good job for the ocr part now having said that you can actually bypass osia still i'm mentioning the ocr projects right uh tesseract i believe that everybody not hiding under a rock knows about it. So we have all used this way. If you remember in the Nlp with Transformers course, we used it very early on in the second or third lab. We used this right? Right? So Open Cv is a computer vision thing. We are all familiar with. Why would you need to opencv for no other reason than to just read your image and pass it on right now hugging face i'll come to in a moment mmr mm ocr is very good oh i forgot easy sorry i forgot one big one the easy ocr i apologize write it down. I'll update the slides. Easy OCR is another good one. Actually, easy OCR does a very good job of document detection. The bounding box and paddle OCR, for example, does a very good job of understanding what's in the bounding box. So you have a question now. Okay, then MMOCR is another library that's a pytorch based that's trying to do a pretty good job and does a pretty good job i tend to use like using all of this there are no perfect tools but what i'm finding is that the hugging phase document ai is perhaps the best in my experience is doing very good right we did all of these in documentum years ago like we had a product line called Captiva we did a whole bunch of OCRs because a lot of companies and forms so you define a form and accordingly it could structure the code would extract and put it in the content management system so yeah nice that is ocr exactly so what albert points out is that in documentum the company did it years ago right um true and now they must be using some of the more recent you sold the company and i think who got it who bought it multiple times emc bought it when emc sold it to open text open text okay so document them was basically a document database so you can build a vector what to say a content management system oh that would be the king i feel seeing all this yeah because the challenge there was to search searching for documents right how do you provide security say in your case if you do a semantic search against your documents it will go against everything and show everything. So you want to control who sees what and that would be another challenge. Okay, nice. Yeah, that's right. The whole access control and everything in the enterprise aspects. Great. So guys, it is good that we are breaking it up into two sessions or how about this today i keep it to only one of the papers paper reading just one paper is that enough guys one or two and the rest we do the next time but i have a request guys if you are here go to hugging face and do that and there are a couple of blocks and articles there and do that. There are a couple of blogs and articles there. You're reading. Read the paper but after that, do those exercises. Can I please request you to do that? It's a very easy one. You'll be all out of here by 2 o'clock or 2.30. Is 3 o'clock good enough? Is that early enough, guys, if we just end today at 3 or would you want to end even earlier? 3 is good right okay so we can all be out of here by three or we just have to be more time efficient let's do the first two papers and do the hugging face exercises that's the scope for today exercises hugging face is a document ai portal Hugging face is a document AI portal. And then there's a blog called accelerating document AI hugging faces written a blog read that blog. Very good blog. Or should I bring that up guys. Let me bring that up here. Yeah. for the reason that hmm no just looking at it because the abstract says two or two that's yes that is that yeah so here we go guys the document ai now this is it i want you guys to read this article, Document AI, Accelerating Document AI. It's a good introduction to this field. This image, as you can see, I took from here. I shared the link to that blog, Asif, on Zoom chat. Oh, thank you. Thank you. And see, guys, the fact that you can do all of these things today. So you were asking, Prashant, what couldn't be done before? see that right so if you go here actually i've given you a paper reading and all of these these are all big models right it was updated a year ago right but still they're very good the one thing that you don't see here is the Nogat. I've added Nogat to your list. Nogat is important. And the other thing that I've added is, so this is where you start reading. I'll explain each of these. Where am I? The TR-OCR is transformer-based OCR. Are we together? Donut is no need for OCR at all. Big thing, the Nogat is the one, those of you, some of you started by reading computer science papers on archive. Which team was it? Initially started with some computer science archive papers. Patrick, you and so on. So what you could have used, a better one, would have been Nogat. Right? Meta. Meta. Yeah. Layout LM literally does that layout detection we talk about. This is a, Lilt is a very good paper. Read it. And table transformers. I brought this in specifically because a lot of you asked how do we extract table data from PDFs? Right? This is it. And these are, guys, it's a vast field this is just a sampling of what we can do right now there are a few classics that i want you guys to read well-read papers like for example if you go to this paper uh let's go here. Real time scene detection with differentiable binarization. You notice that it has a lot of citations, 404 research papers cited. That's a fairly healthy number. This paper is often quoted and talked about. Do that. Then the other one that is important is, so this is on the detection part, detecting where the text is. This is on the SVTR transformer. This is on the SVTR. So scene text recognition with a single visual model. This one is on the understanding side. Right? Paper. Then there is this one paper that's not much looked at. It's almost forgotten, but it's very useful actually. I find this thing very useful. So might as well. Let's go back and see. This is a, where did I go? Copy. Okay, let me just see. Yeah, sorry, snake. I'm not able to get there so I'll just do a text name. Maybe the link I put was not archived. And a paper, if you look at this paper, I want to show you the image of what it can do. Okay. Okay. It will read text from an arbitrary shape. So how arbitrary shape? Let's look at it. And this is what I'm talking about. Look at this, guys, coffee. You notice that if you try to do a bounding box rectangle this is how it will be if you do it as a slant or any of the traditional approaches it doesn't work but there is a way to read it perfectly and this like you can have texts written literally in the shape of a snake but That's why it's called tech snake, this algorithm. Now, when you look at these images, guys, you realize that these are really hard ones to get. And yet it gets it. The core idea actually is based on two of the papers that we have already covered, one of them being UNET and the other one being the pyramid of uh inferences where it talks about it images it takes a pyramid of images and does inferences on them somewhere it does mention that this inference is okay so these are these are the papers that i want you guys to do. Worth reading. Let's do it in two parts. And that's all for today, guys. So let's get started with the paper readings. Let's do at least two papers. The first two paper. Does that look reasonable? And we will start our paper reading a bit earlier. Let's say 1 or 130 and be done in one hour right one one and a half hour 130 will start and be finished by three awesome why did you not talk much about that layout lmv3 it is it is there we'll do it next time here it is layout okay okay right here it is one quick question yes this document I relate to name entity recognition and stuff like that which we talk briefly in the NLP course uh see NER is a part of this when you when you get the text out of it the next question you ask in the document AI pipeline is, what are the named entities in this document that I just said? But a NER is a more low level task, it's more granular task that is applicable broadly in many situations. Yeah, Patrick, go ahead. I said that for one could do a really robust document AI analysis. How will you standardize the output of that thing, one LLM to the other LLM? Oh, there are these benchmarks. Let's say we have one that's layout analysis and then one that is table detection and then content. How do you... Well, you create a pipeline, use all of it. So it runs one after the other. That is right. It's not output of all of these equals... No. The sum total of all of these is one dot Q. No, you're quite playing it so in reality when you do all of this and that's the that's the thing i was hoping you guys would do um so for example this is the layout one and who asked that layout are we going to do it i asked yeah yeah so in fact the figure that hugging face quoted is literally the figure from this paper that Hugging Face Coated is literally the figure from this paper. So the reason why I asked was, there's this competition, right, ICADAR, I think, which does the document analysis and recognition, which has more of these competitions for form recognition, received recognition, and things like that. And I asked Bart to say what are the technologies that over the years have started to show prominence in those competitions. It started showing this layout, LLM v2 and then v3 starting to show up and Google and Microsoft are actually hands down beating Adobe in this area, which used to be the leader. So you see these models are outperforming Adobe's what do you call technology. So that's why I started reading about it as you are talking. Okay. These are that's what I said. These are the very, I mean, I wouldn't by any means claim that I'm up to the latest. So if you guys find even more recent latest and greatest work, let's share. I will learn. Go ahead. only focuses on say text and image in context with these right uh are these uh these transformers more generic and therefore maybe poorly performing on determining the content you can specify which parts you care about so once you have the layout of a document right you can say ignore the figures ignore this just give me the text remember that place for both to be used both to be used some of this in fact usually it used to be a two-phase approach first would be the detection phase what is there in the document second would be the understanding phase. So, all right, guys. So, I'm going to stop the recording. Anybody has a question before I stop and we end today's session, this session here? We start again at 1.30, guys. One and a half hours to read the two papers. And have lunch. Oh, so start at one. Okay, we'll start at one. Which papers are we reading, Asif? Only the first two papers. The papers are Transformer, OCR, and Donuts. Asif, I have a question go ahead so the bounding box method right uh i i've not read this paper yet so more so some of the challenging ocr situations that companies faces sometimes there is no key, okay. And the value is based on the position of the form. This is a very prevailing problem in the medical claim processing. So, in that case, even if we extract value in order to determine, I mean, there are like keyless checkboxes just lying in the middle of the paper, in the middle of the document. And the way we deal with this, like, you know, we take the pixel coordinates, which is based on the bounding box of the key value pair form as a reference and use that to extract the information. So how can transformers help here? Okay, help here? Okay. I have considerable expertise in dealing with those medical forms and doing it right. I know what you're talking about. So there are multiple approaches that work very well. Actually, you're right. Not many people know how to handle the situation. You can do a chemtetized approach. You could do and there are multiple approaches. You can do a chemtetized approach, you could do and there are multiple approaches, but the bottom line is that you can actually do a pretty good job and achieve accuracies in the high 90s. We do that, but I do know that at this moment I don't know if there are many open, there are many commercial open source that specifically focuses on medical forms. I had to do it, so I did it. That's good to know. So I'm eager to try this transformer. Sure. But let me put it this way, that everything that I use, you guys are learning. Why not? We're doing the projects well.