 All right guys, this is week one, session two of our introduction to data science workshop. I will start by recapitulating what we did the last time. Last time I introduced the subject of machine learning or artificial intelligence. And I said that fundamentally artificial intelligence is different from programming. We have had about 60 years or more, 60 years of programming the machines. And so programming has become a dominant discipline in computer science then i mentioned that artificial intelligence though is completely different the goal here the core of which is machine learning. Machine learning, it is to make machines learn from data on their own and therefore go on to generalize and do a task better. So the definition of machine learning that I gave was this. You take a task T, some task, you let either a human being or machine perform the task. You quantify how many mistakes are being made, what is the error. In this space, we call the sum of all errors or some mathematical function of all the errors put together. We call it the loss function. But at this moment, let's consider it just the errors. We take the errors and then we ask this question, how can we reduce the error? So with human beings, it's quite easy. You talk to them and the error, you talk to your child, you explain, you do things and thinking process helps you, helps them reduce the error. They understand. As understanding forms, the errors go down. And that is the evidence of learning. With machines, when we try to teach machines, it's a little harder. Machines don't think. There is no evidence that they understand anything at all. Yet, they are capable of strictly learning in the sense that they will start out with a certain amount of errors, but they will learn from the data, and they will form some sort of a concept internally, a mathematical representation, or some model, as it is called. It will build some model. And when it builds a model, it will use the model to make predictions. It will generalize. We took two specific examples, cows and ducks. If you have to identify whether an animal is cow or duck, and suppose you have not encountered cows and ducks, then it is in the beginning quite likely that without any training, you will make a lot of errors, and so will the machine. But after it has seen enough examples, or the child has seen enough examples of cows and ducks, gradually a sort of a concept of a cow forms and the concept of a duck forms. You get an idea of what it is for an animal to be a cow and what it is for an animal to be a duck. Once it has that notion, then the next time there is a new animal that instance that you present before, either the child or the machine, the errors are likely to be less because there's now a better understanding, but there's some concept some learning has taken place so learning is not just a prerogative of humans or or living things animals and living things it is a more broad concept and you can make machines learn therefore the field of machine learning which is the core of AI now much a machine learning is inherently different from programming. Programming is imperative, do this, do that, and so forth. In learning, in machine learning, you give the machine enough data and then you ask it to learn. The question is, what is the machine? Is it that piece of hardware? Partly, but it is actually a set of algorithms that people have come up with. Some beautiful algorithms that help you to help the machine to learn. Now, these algorithms, there's a wide variety of them and every day newer and newer algorithms are coming. There's a wide variety of them and every day newer and newer algorithms are coming. The field is extremely hot just in the space that since we met last week to now, yet another significant breakthrough or maybe more but at least I'm aware of one has happened in the field. So the you know the frontiers keep moving forward. There are more and more algorithms, better and better algorithms, better and better algorithms, better and better ways of training and helping the machine learn. So that is the subject we'll deal with today. Now, if you remember the last time we talked about, we talked about examples. The other example we talked about was an entrepreneur trying to open a shop for ice cream and sell a certain amount of ice cream. So when she sells ice cream, she has to go and buy ice cream in bulk from some wholesaler. Now, if you get too much ice cream, it will go to waste. If you get too little, you lose business. So you have to predict, you want a machine to learn and see the pattern in the data, some historic data that on this day, we sold this much ice cream, the temperature was this, the wind speed was this, and so on and so forth. And based on factors that you think are relevant, the machine has to come up with some understanding of the relationship and be able to predict and the prediction is of real value when you predict a number like an amount a measurable quantity the the algorithms are called does anybody remember what are they called recreations they're called regression and when you identify something a class such as a cow or a duck then what do we call the algorithm anyone classification excellent and then the other form of machine learning is pattern recognition. Clustering I gave as an example and as we move in the workshop, we'll have more and more examples. So today, what I would like to do is give you guys enough background in regression. We will do a lab in regression. Imagine that you are predicting something. If it helps you, think of an entrepreneur who is trying to predict the amount of ice cream needed in a given day based on the temperature of that day. Temperature at noon or the average temperature that day during the daytime. So we will take only one predictor and of course one response, one output, namely the amount of ice cream that would be sold. And with that, I will just set up enough framework or that we can jump right into code. Now the code that we do today, I'm going to take you through a lab. If you have given time over the week to pick up the basics of Python, this will actually be, it will come to you as a pleasant surprise that you don't have to do a lot of programming. Most of the code is three lines two lines five lines or so in fact machine learning code is very very small the ordinary visualization code is longer and those of you who have done the Photoshop or any one of these things you know that making good drawings a good graphics always takes time that is true in machine learning too it is is easy, it's very easy, but you have to get used to it. And it takes time. It takes time to write that code, but even then it will be like three, four lines of code at most. So with those preliminaries, I would like to would like to just do a very quick review and then we can launch forth into this let me know guys when you are seeing my screen are you guys seeing my writing screen, writing board? Yes. Yes, you are. So just as a review, if you remember, we said that regression is when an input X goes in and an output, let me call it Y, comes out and that is a number, right? In simple terms, it's a number, but in some more advanced cases it could be a vector and so forth but we won't go there we'll keep it simple in classification an input goes in but what comes out is an identification a cow or a duck a label right and clustering is you detect clusters clusters in the data so today what we are going to do is we are going to take data, three data sets. In each data set, I have a CSV file and the file has X and Y and that is all it has, x and y. And based on this x, y, you have to predict y as, based on x, you have to predict y. So y would be some function of x. All right. So we said that, I apologize for the interruption. So suppose there's a value here, three or two. for the interruption. So suppose there's a value here, three or two. Now your job is to predict a number. What will it be here? And if you can predict it accurately, then you say that, suppose this is supposed to be five, right? And there is some relationship between x and y the relationship is let's say the relationship is like this right if you can figure out this curve this is your x and this is your y if you can figure this relationship out then it's very easy to predict what the value of y would be at any given value of x. Let's say at this value x alpha, you would know what the value would be. That would be just f x alpha. This writing part. Okay. So this is a relationship. So we will explore that. Now this is the notebook we are going to explore. Let us forthwith get into our notebook with that background now a couple of bookkeeping things i am not sure if you have joined the course page i created a course page for this workshop two things you need to do. You need to join the Slack. I still see very few people have joined Slack, about 14, but I see 37 people here in the audience. And I think the last time the audience was 50. So if only 14 people have decided to stay for the workshop, only if that is true, look at yourself and ask, are you planning to continue with this workshop? And if you are planning to continue with this workshop, then ask yourself, have you registered with the Slack? Join the Slack group. If not, reach out to Kunal. If you're having difficulty, please reach out to Kunal. He can help you with this. he can help you help you with this the other thing is there is a course web page now course portal you need to log in or register yourself with the Google account and then I will add you to this this thing. I'll show you what I mean by that. Suppose you go to, here we go. This is your page guys. At some point, should I make the fonts bigger? bigger like or is it big enough to read are we all able to read it you are able to read so see this workshop is named I called it a fast-paced introduction to data science, because that's what it is. The goal here is to get you writing, becoming practical and useful in this field or fluent in this field as quickly as possible. So in the interest of getting you productive sooner and giving you confidence that you also can do it, it's a slightly risky approach in the sense that I will be introducing you to code early on, the way to do things, training the machines. So you will get an early start. You'll jump straight into it. The downside is, of course, if you are not used to Python, or this, then the first time you see this code, it will, it may look frightening. So if, when I show you the code, if it begins to look frightening, even after I explain it, it possibly means that you need to do a little bit of a, this week, you need to get really serious and go learn a bit of Python. Python is a straightforward language. Truly, if you give it one or two real days to it, you'll master it. If not, give it the whole week. If you have absolutely no background and you feel you're struggling, if you truly give a whole week to it, any person without any programming background whatsoever coming from any field can master it quickly. So with those things let me walk you through certain aspects of it. One of them is this is the course portal. You should be able to come here. If you are not able to come here maybe today I'll keep the lab session if we finish it early I will be here to help you get in to this course page it's important that you we are not able to access actually this course okay then we need to solve that problem let's do that in the break or after the lab right away I won't leave you guys without solving your problem. The second aspect is Slack. Make sure you have Slack. So the first thing that you have here, let me just walk you through, is this. You have a GitHub. It's a public GitHub. Here you have some datasets. As we do this workshop, we will actually do an analysis of all of these datasets. Many of these datasets are the ones that I have generated. A couple of them are famous datasets in this field that many generations of data scientists have used to learn this field. So we will have a mix of both. Today we'll focus on three data sets, univariate one, two, three. Those are simple data sets and that's what we'll focus on. Give me a moment. I want you guys to download a notebook. Let me see if I did not upload it. So what we will do and I think I need to give you some background, there is such a thing in this field called notebooks. These notebooks are and some of you may have background in it so I apologize for it but I would like to just give an idea to people who are completely new to this field. See, in the scientific disciplines, traditionally there is a concept of labs. if you're in a biology lab or a physics lab or a chemistry lab, you will go do some experiments. There would be some, maybe a learning and the professor has given you a set of instructions to follow and learn, do some experiments and then have some findings. And those findings and what you did, you typically put in a science notebook, a lab notebook. In the lab notebook would be what is it that you want to do, what's the objective, what are the steps you took to do it, and then what was the result of what you did. Those lab notebooks, later on if you end up discovering something interesting, you write it up into a paper which follows the same format. Introduction of what is it about that you're doing, methods, what are the steps you took, procedure, a method you did in your experiment, results, what were the results of those experiments and then conclusion you have a conclusion or some observations or some remarks on the experiment overall what you learned or something like that so that's a basic format of almost all scientific papers if you look at scientific papers they have the feel of a lab notebook now in the world of data science, our lab is the computer. We are trying to teach the computer the algorithms. We are training the algorithms to learn from certain data on their own once we expose them to data. And then we will follow a set of steps to make it learn. And having made it learn, we will then look at the results. How well has it learned? We'll test it out. And then we'll hopefully have some conclusions to draw from that. So that in data science is done. Now there is a software. It is called Jupyter, JupyterLab notebooks. The word lab is indicative that it's meant to be the equivalent of what you have in other scientific labs. So JupyterLab notebook. So today I will give you a taste of the JupyterLab notebook. This notebook is a mix of English. We write our things and the procedure, the steps we follow are in Python. And it's very, hopefully it is simple enough Python. If you find the Python very hard, even after I explain it, then it means you need to give a little bit more time to going and going back and brushing up. Now, for those of you who are new to the field, let me assure you that I've been, I suppose, teaching at the university, college level and PhD and you know, graduate school level for the last 30 years. And over the years, you come across a large number of people of different mental makeup, different personalities and so forth. So there are some people who are very confident that they will make progress and go ahead. Quite often, it turns out that the people with high potential are actually the ones who are very underconfident in the beginning and they're concerned whether they'll be able to get started or not. So don't have any such fears. If you put in the effort, you'll get through. The libraries have become extraordinarily simple. The sort of things that we can do with one or two lines. When I was younger, in my grad school in later days, 25 years ago, we were typically doing it in C and Fortran and it was not easy. It was, it would be reams and reams of code. So much of the things that we do now easily and anybody can do it, used to be things that it would take a PhD to do or a master's to do in this field. So that is the good news. So with that good news I will continue on both the everything that i'm discussing is here on the course page already so for example if you go here and click on the course page a pop-up will happen and you'll get this jupiter notebook table of contents and this notebook and i'll explain what the notebook is in a moment. I'll walk through it very carefully. That's what we'll do. Now there is one, after I explain this whole notebook, then we will do, then we will do, if you have brought your laptop, I will take you through the installation of Python properly and help you reproduce at least a few elements of the notebook and the rest of it then you can keep trying on your own. So that will be the format of our session today. Before I go into it if any one of you have questions this is a good time to ask. Anyone would like to ask a question? Anyone would like to ask a question? All right. So in that case, see guys, don't be quiet. You know, if you have questions, do feel welcome to ask. I'll be happy to help you guys. And that's the whole point. I want to make sure that I don't lose you right in the beginning of the workshop we make progress together so with that in place i will bring up a jupiter notebook so that you can have a sense of what it looks like uh here we go when you open jupiter this symbol that you see it belongs to an open source project that gives you these notebooks. The good part of these notebooks is it is almost as easy as any web application that you're used to. You can just go and enter text. It's like using Gmail, except that it's a little more you need more aware of the subject than you if you guys can please would you please mute yourself and I appreciate that all right so with that, I have put a read me here, which is your homework. This also I'll post in this. So let's go to the read me. Your homework is this. Read chapter one to three of the textbook so that you don't have to fully understand it. just go read it and see how much of it you grasp. It is forward reading, in other words it covers things that I will explain in the next session, next theory session. So we'll keep alternating between theory and lab and theory and lab. So if you read those chapters you will be ready to and you'll understand more when I go into the technical details the next time. The second is a play around with Python if you are not familiar with it which need not be said it is very important that you have basic fluency with Python. Now for data science you only need basic Python you don't need esoteric aspects of it for the most part. And what happens is the more you become familiar with the language, nothing is ever esoteric at all. Even things that in the beginning look very esoteric, they begin to reveal themselves and look very simple after some time. So familiarity makes a huge difference. So get started with Python early, time is in your favor then. And in particular, we, what we care for in data science are three particular libraries, NumPy, Pandas and Matplotlib. NumPy gives you matrices, something called matrices, which is basically an array of numbers, a bunch of numbers, structured, put in a certain structure that people call matrix. Then there's a Pandas library, just as you have a spreadsheet, you know, Google spreadsheet or Microsoft spreadsheet and so forth. In the data science community, we have something similar. It is called a data frame. And R and this and every data science language that I know of, Spark and so forth, they all support the concept of a data frame. To the first approximation, a data frame is nothing but a spreadsheet. If you think of a spreadsheet with columns and rows of data and columns of properties of that data, you're doing pretty good there. So imagine that there's a person name and there is an age and there is years of education and there are other things about the person. That would be a good understanding of a data frame. That's what a data frame is. Now, that is what you get with pandas. Pandas is of course a reference to the animal, the cute animal panda. And last is Matplotlib. It is a plotting library as the name says. It's a mathematical plotting library. It helps you make simple plots and actually pretty good plots. It is the default in Python. There are other plotting libraries which are more aesthetically pleasing and so forth. But as we make progress, we may get introduced to some of them. So this is your thing guys. And lastly, there is one more thing. The real lab work to do is there in the notebook itself. So we will go to the notebook and now it's time to get started. This is what a notebook looks like and we'll go through the installation process in a moment. What is it? So let me... What is a notebook? It has these things that are called cells. Do you notice that this blue border? These are called cells, a cell. A cell in a notebook is a collection of little cells. Think of cells as small paragraph boxes. So for example, this is one cell. It is one cell of text. Are we seeing that guys in the center of the page? Import the relevant libraries, and there is some explanation here. This is a cell, but the next one is a cell of Python. And don't worry about this. This is boilerplate. You don't have to understand all of it to get started. Just assume that you need to put it in the beginning of your notebook and you'll be fine for the time being. And I will explain it in a little moment. Everything is a cell. So this is a cell, this is a cell and so forth. So when you feel like adding something of your own, you just go to the bottom of it. of it. And then, like for example, here is your homework that you have to do. What it is is, and this homework would be straightforward once you get used to it. But see, I added a new cell. Now I can start doing something here. In fact, I can start doing the solutions to the homework. And that is what you will do once you get more familiar with this notebook so let's go to this notebook in the beginning so in this notebook it is made up of two kinds of cells one is plain english cells you know that is meant to talk to or explain what you're doing to your colleagues. It is the explanation. And what you see in the gray areas, they are usually the code, the instructions you give the machine to engage with the data. You pull up an algorithm. Typically, you point that algorithm to the data, and then the algorithm begins to learn. So that's a basic idea. So the first cell that I have put here, this is just to those of you who are familiar with HTML. These are just HTML things to make the page look aesthetically a little bit better. So you can ignore the following. So as it says, its purpose is some basic styling of the notebook. So completely ignore it. So when you do data analysis, what we are going to do, as I pointed out in the GitHub, there are a few files, a few datasets. We are going to analyze it. To analyze it, we will need a few libraries. The core libraries that we need are NumPy. NumPy gives you high performance matrices and operations on them, arrays and so forth. Now, you may ask those of you who are familiar with Python, that Python has a very elegant and simple concept of list. Why not use it? The answer, the reason we don't use the standard Python's feature of a list is because Python is what people call an interpreted language. Leaving the technical jargon aside, it's a much slower language than other languages which are from the ground up very high performance languages. So these languages are Fortran and C. In my generation, for example, when I was young, all the scientific computing we did, we did in Fortran and CEE. So those libraries are still there, the contributions of hundreds of thousands of people and scientists from around the world has led to the formation of very, very reliable battle tested libraries, right? It is as, if somebody once told me that if you look at the amount of effort that has gone into creating those libraries, and think of the, if a company, if a private company tried to hire a lot of people and have them recreate this library, how much would it cost? And I think that he told me that just to create these from the ground up, assuming they didn't exist, it would cost somewhere like $10 billion minimum. So think about that. And I think that's an underestimate. So you can just imagine how much effort has gone into for the last 30, 40 years in perfecting these, or 50 years, actually more than that. Then perfecting these very high performance libraries for written in Fortran and see, these are the things that you use when you do particle colliders, you know, subatomic particle physics, those atom smashing experiments. This is what you use. This is what NASA uses when they launch their spaceships and do all those travels to the moon and the Mars and so forth. This is what we use in our telescopes and so forth. So these libraries are really, really, and you use it for weather prediction. If you think a weather prediction has become accurate, all of them is done. So what we do here is that Python do here is that python does not reinvent the wheel and neither does our or other languages they all sit upon these libraries they all benefit from these libraries right so any new framework that you see comes if you look under the covers most likely you'll find that they are all sitting upon the the very rich high performance libraries that have been created over the years. So NumPy is one. The specific library, if you want to know, it is the BLAS, Basic Linear Algebra System of Fortran. And then along with it, there's a little bit more, but okay, we'll stick to that majority. Now, Pandas, as I said, gives you the concept of a data frame. Jay Shah, Dr. for our things, a data frame is much, much easier to use than doing all sorts of gymnastics in Google Spreadsheet or Microsoft Excel or things like that. It really makes, in fact, a lot of people have told me that after getting used to data frames, they can never go back to spreadsheets ever in their life. It just feels almost like a torture to go back to spreadsheets. So that's that, bundle's data frame. Very good for once you get used to it. Then matplotlib is for plotting. Now, it is a basic or default plotting library. It's quite powerful, actually. And there are other libraries that sort of sit upon it sometimes, or they do alternate implementation. Seabourn is a library. Those other libraries, they try to be aesthetically more pleasing or give you better syntax or so on and so forth, more modern or whatnot. But the base library is Matplotlib and it does the work quite well. We will use that for this workshop. Seabourn occasionallyON occasionally will use and then there are things like Altair. You can go and look it up. Those people who like love visualizations and graphics, you can go to Altair and check it out. The next library we'll use is Scikit-learn. Scikit, this is the main library for data science. It assumes that you know you have NumPy available and Pandas available and after that scikit-learn it very well integrates with Pandas and NumPy and Matplotlib. But it is the library of algorithms and so you will get introduced to it today. Then there are something called exploratory data analysis. It is like when you get this data, you want to just see what it is, do descriptive statistics, inspect what columns are there in the spreadsheet. We all do that. So, Pandas profiling is an efficient tool to automate the exploration of data. And so with all that, let me get started. Now, these libraries, what you have to do if you're going to use it in your notebook, you have to do something in the jargon of Python. We say you import the libraries into your notebook, right? Or if you're writing Python code, then you have to import it into your script. So what does importing mean? See, there are lots and lots of libraries that your machine may have once you do this. You can't load all of it into memory. When you import, roughly speaking, think that you are loading it into the memory of this Jupyter notebook. So your notebook is now aware of that. It is also a good thing to give an alias. Sometimes some of these libraries have modules, they have rather long names. So for example, NumPy, people find it tedious to keep typing NumPy, numpy numpy so they give it a nickname np so in the rest of this jupiter notebook we'll refer it refer to it with the word np likewise pandas will refer to the word pd and pandas profiling it turns out we'll just use it as it is so with that there now let's talk a little bit about the pre-processing data. See when you get data, actually some of these we won't need today. So actually most of these we don't need today, but I've kept it for future labs so that as we make progress when you need it, you can use it. Now metrics. So we will build a few something called regression algorithms. There are two algorithms we will take and we will see how well they perform or don't perform. One of them is linear regression. It tries to fit a straight line to the data. So given a data, it will ask what is the best fit straight line. The other algorithm is called random forest, random forest regressor. Random forest regressor is a much more complicated algorithm. We won't learn about it in this, but this is just to give you a taste of the fact that there are quite, there's a wide variety of algorithms and people keep coming up with many, many. And for any one dataset, one may do well, the other may do well and so on and so forth. So the mileage varies based on the dataset. There is in fact a theorem called, in this field, called a result, an observation if you will, but it's a mathematically rigorous observation, it's called a theorem. It's called the no free lunch theorem. Actually they are not one theorems, but quite a few. But the gist of those theorems is that no one algorithm is necessarily more powerful than another algorithm. Data sometimes, one data set may favor one situation, one real life need may favor one algorithm as the best and another situation may favor another algorithm as the best. So be aware of that. And in that spirit, we will use two measures to find out how good, how well was the learning. Mean squared error. Remember I told you that we use the number of mistakes you make and some derivative of that as a measure of learning or not learning. So that is our mean squared error. The smaller the error, the better the learning. And then this is called mean squared error. The smaller the error, the better the learning. And then this is called R squared, even though it's written as R2, it is called the R squared score. There are fancier names for that, which you don't need to remember. The more formal name for this is the coefficient of determination. Now what that is, we'll see shortly. Next we import a few of this Matplotlib and Seaborn plotting libraries. This one actually, oh goodness, I left this Folium there. You can install Folium. I'll help you through with the installation of all of this. Folium is for maps. Do you notice that these days on the internet if you go to any place they will show data on maps, map of India, map of the world, map of US or so forth. So whenever you need maps and data to be projected onto maps locations then the folium library makes it extremely convenient to do that. So that is that. The next thing is only for those of you who are familiar with Linux. If you are, there is such a thing as LaTeX. LaTeX is, think of it as almost a benchmark of how perfectly a book can be written in, how beautifully the fonts can be laid out and so forth, especially in the mathematical domain, in the data science and other domains. So most of the books in this field, good quality, like for example, your ISLR is written in LaTeX. And from LaTeX, it was not written as a Microsoft Word document, it was written in LaTeX. And the scientific and the research community, mathematical research community almost, always writes their works in LaTeX. If you have it on your machine, it's fine. But that is what this little text is for. You can set it to true occasionally. Otherwise, just leave it as false. Don't play with it. So this is a long introduction to the imports. Now this imports, it looks like at first glance, it looks like oh goodness. But the reason I gave you all these imports is so that you can carry it forward from multiple labs. Many of these imports you don't need for this lab. Today's our starting lab. But you can use this as your template so first thing we need to do we need to go load the data now if you remember where is the data data is yeah I put it in github it's a public github github is a place where you can keep files, many, many versions of a file. So every time you make changes to the file, you can give it to this thing called Git, version control systems, these are called. And what they do is that when you create a new version of it, they will store the previous and the new. So they keep every version of what you are doing so later on if you realize that you made a mistake you want to go back it's very easy to rewind it's almost like a time machine you can rewind back in time for that particular file and see and essentially undo them undo anything you don't like or you, maybe there is some one piece that you have in your notebook long ago, but you deleted. You want to keep the new work, but you want to cherry pick some things from the past version. And it gives you the ability to merge what you have done now with what used to be there and have additive effects. So you can do very powerful things. So for those of you who are in the creative field, creating anything, writing a novel, your poetry, your essay, and particularly in data science, because you experiment, you have many versions of a notebook. You keep checking it in. Here, I've checked in the CSV files the data is public you can just go and get it so now we look at the code and how to go and get it our first goal is to go to this URL remember this is the website where it is and download these CSV files. And then there is this thing. So what does PD stand for? Let me remind you, PD stands for Pandas. And what was Pandas? Pandas is the one that helps you create a data frame and do all sorts of powerful operations on the data frame. It is the spreadsheet of the data science world. So you're telling Pandas PD to go read the CSV which is at this internet location. So it will go and read it. And so when I do this analysis, we could have done the analysis with only one of them. And I was toying with that, but then I thought it would be helpful if you see a comparative analysis within three different datasets. How does one algorithm perform on each of the datasets side by side comparison can be very useful. So I have taken three datasets. So what you will see is that I will repeat everything thrice because of which the code may look longer, but actually it is very small. So when you look at this code, this is just telling the URL where it is, right? And here, one line of code reads one dataset. Do you see how easy it is? And if you're not yet familiar with Python, you're getting up to speed, just take it as a template. This is how you read a CSV file. You point it to the file, whether it's on your local machine, on your laptop or workstation, or it is remote. You can just point to it and it will come down to you and become a data frame. Now, because we are going to analyze three data together, so I load all of these three together. Now, when you get data, the very first thing you should do is you should go and look at the data. Just do basic row count and how many rows are there? What is the mean of each column? What is the max in each column values and so forth? You get basic sense of the data. It's like eyeballing the data, but you're doing it in a little bit more condensed way. Now, those of you who remember, I hope you all remember that in school you did, that if you get a bunch of numbers, a column of numbers or a set of numbers, you can find them, how many there are. It is worth knowing. Mean is the average minimum min is the smallest of those numbers max is the biggest of them standard deviation is a measure of how much the numbers are spread out from the mean right and you may or may not remember standard deviation if you don't uh do go and review this right and then these are percentiles like what is the value at 25 percent at 50th percent 75 percent now most of us have taken exams and we we are told we are in this person time i'm hoping that you are familiar with percentile if you're not this would be a good occasion to go review that. So just looking at it, it seems that X value goes, the thousand of them minimum is zero and maximum is 10. Y goes from 41 to 92, it seems with the average being, oh, 41 is the mean side minimum minus 11 to 92 that's a lot of range but data 2 is different data 2 goes from a minimum of minus 17 to plus 17 interesting and the mean is 1.8 right somewhere in between that the third data set again if you look at it we see that it also has certain characteristics it goes from 35 to 102 in the Y values so now it is a very useful tool in Python see do you notice guys that these are just one line of code with one line you could load the data are just one line of code with one line, you could load the data, right? With one line, you could look at, do descriptive stats of the data much easier than taking a paper and pencil and figuring it all out, right? Especially when you have large amounts of data. It is basically like a powerful calculator that's doing the work for you. And because I'm doing it for three data sets, I'm repeating twice. So now I call something called data one. So I remember I loaded in this named spreadsheets or data frames, data one, data two and data three are the three data frames. And if the word data frame looks strange, think of them as spreadsheets now this is very interesting you are asking this tool a profiling tool you're asking this data to just tell you a lot about itself that is called exploratory data analysis you go explore what is there in the data fortunately things are quite powerful here with one line of code, you can actually get a pretty detailed report. So let's look at this. What does it say? It says something that we just saw a little while ago. It says that the number of variables are two, X and Y. Number of observations are thousand. None of the things have missing values. That's a good news. This is the rest is memory size. It also has some warning. It says that there's a high correlation. Why is that a warning? Later on you'll realize that sometimes high correlations are not good. And I'll explain what correlation is in this coming Sunday session, the next Sunday session, theory session. So what are the variables? X is a variable. And you realize that there are 1000 of them. And the mean, you know, all of those values that you saw before, y. Now, if you want to see the plot of x and y, this shows you the relationship between x and y. this shows you the relationship between X and Y. So what does it look like to you guys? Does it look as though it fits roughly a straight line? Yes. So it seems that Y is a linear relationship, linear function of X. It's almost a line with some noise in it. The value sort of jitters. It sort of jumped around a little bit from what it would have been if it was a perfect line. So that is called, so you say that there's a linear relationship. Now what is the correlation between X and Y? This is important. I will focus only on the Pearson correlation. Correlation is something I'll explain next time, but the best way to understand it is to say, and one of the things is in the notebook you can toggle and it'll tell you what correlation is, but this may be a little hard to understand if you're new to it. It basically means if X is increasing, is Y increasing or not, or decreasing? If it is increasing along with X, it's positive correlation. It's a number between minus one and one. So it will be some number above zero. If as you increase x, y decreases, right? So then you, it's sort of like an anti-correlation or negative correlation. So the correlation value is negative. And if the correlation between two variables is zero, then it probably means that the relationship is certainly not linear. There may be a hidden deep relationship, but it is not obvious. So when we look at this, it says, if you look at this color scale here. Let me take this color scale even bigger. So you can see this Jay Shah, Dr. V. P. M. D. Yeah, if you look at this color scheme, you know, all of this looks so excellent. Of course, every variable will be correlated to itself perfectly does x increase well it's a logical obvious thing it will and likewise for y but what is important is the off diagonal pieces but this also looks like deep blue to me so it means that as you increase x y increases and if you look at the legend it seems to imply a very strong value of correlation isn't it so this this is the thing and there are no missing values. Now, let me zoom out a little bit. So, oh no, this is to become too small perhaps. So let me know guys when it becomes too small for your screen. Missing values and so there are no missing. So the data looks good. Now let's look at data two and see what it looks like. Once again, the thousand rows we just saw, the y values are distributed very interestingly actually. Do you notice that how they're distributed? They seem to be not balanced. So these are little things you'll gradually get to see. So what do you say about the relationship? So these are little things you'll gradually get to see. So, What do you say about the relationship. Do you think that straight line would be a good explanation of this data. A good fit to this data. Anyone Know, so No, sir. It wouldn't, right? Because maybe for sections, just from zero to two, you might be able to fit a straight line. Then between two and let's say four and a half, five, you can fit another straight line that goes down. Then between four and a half to let's say eight, you can have another straight line maybe going up, then another straight line going down. And so it is hard to think of a function that, you can make it out of piecewise straight lines perhaps, but it is not certainly representable by a single straight line. In other words, it's not linear, the relationship is not linear. And so you can see the result also in this dataset two, X and Y, what is the correlation? What do you see from this color? It's practically white, isn't it? A practically white means that it is a very low correlation between the two. Then no missing values. Now we go to dataset three. What happens to data set three here? Let's go and see. Once again, I'll quickly skip through to the diagram. Oh boy. Suppose y is this function of x. Is it easy to figure out what that function is or what that relationship is? No. It's hard to tell right so so one easy one sort of intuition you can have is suppose this X was temperature then when the temperature is very very low right then by and large your y, the amount of ice cream you're selling is low. Some small ups and downs. Maybe there is a, in winter there is a festival and some people do come to the beach or something like that or whatnot, right? Where it gets a little bit better. But broadly it's low. And then as the temperature becomes beach friendly, you know, inviting people to the, it gets warmer and warmer, then children start coming to the beach and you sell more and more ice cream. So this could be a description of it. So in any real data, relationships are often not linear, but they may be approximately linear or piecewise linear or something like that. But you don't know what this function is actually Vinesh Pramlalli, Jr.: Because I generated the function. I know that I deliberately used a very complicated function. Vinesh Pramlalli, Jr.: To generate this data so that you couldn't guess what function it was mathematically and the point is that you shouldn't be able to your job is to fit an algorithm, make an, teach an algorithm to figure it out. So once you have explored the data, the next step that you do, which is also a part of exploratory data analysis, EDA, is, and that word you'll hear a lot,DA exploratory data analysis so here is a dirty little secret in machine in machine learning in data science 90% of the time that you when you work in your job data science job you are actually doing exploratory data analysis and data preparation cleaning the data analysis and data preparation, cleaning the data, fixing errors in the data, things like that, transforming the data, data manipulation, and just exploring the data, visualizing the data. It is 90% of it. Maybe 10% of your time actually at the end of it, or maybe 20%, 15% goes into the actual deeper intellectual stuff like algorithms and so on and so forth. And in many, many domains that you don't need to go very deep into the algorithm, you can become productive. In fact, I would like to claim that if you do this lab and review this lab, you would already be in a state that you are productive as a data scientist you can solve problems just following this template with whatever data that you get so the next step here so therefore i'm paying attention to the exploratory data this part this is the plotting part the plotting actually is simple but because I'm doing it you notice that there are three paragraphs here one paragraph for each of the plots you notice that there's a unit area at one data set you need very to unity so for each of the data set I'm making a plot right and the way you do that plot as you say you go to the plotting engine which is the mat plot, as you say, you go to the plotting engine, which is the matplotlib, and you say, I want to make a plot which is made up of actually three subplots, one row and three columns. One comma three means one row and three columns. So then it will give you two objects. One is the F object, which is the figure itself, which you typically ignore. which is the figure itself, which you typically ignore. But then it will give you a handle on an empty, so think of it as three empty subplots. These are named AX1, AX2, AX3. The reason they're called AX is some technical reason. In matplotlib, they're called axes, right, or dimensions or something. But it doesn't matter. Just think of it as three subplots, three empty subplots. Now in the empty subplot, you need to paint some data. So we'll start painting the data. Now our data, remember, had the X and Y column. So the way you refer to the X column of data one is this interesting notation. So let me zoom into this notation a little bit. Do you notice that I'm doing that I'm taking the name of that spreadsheet or the data frame and doing a dot x. Dot x means the x that belongs to data one. Likewise the y that belongs to data one. You would like to plot the data, isn't it? Because data one is made up of those two things. And so when you do that, you will get this. So right away, now what is a scatter plot? A scatter plot is a xy plot in which you decide the x-axis and the y-axis and each data point will look like one point on that graph in that plot you literally place each point where it belongs on the plot that's it and scatter plot is something you may or may not have done in school but if you have not then it's as simple as that right so you give it a title you you see the title here univariate one that comes from this line actually just line number five is more than enough if you do on so these things are just niceties like setting the x label setting the y axis label uh you know i said the x axis label as x if you notice and the y axis label as y and the title as univariate one. Even if you don't do it, you've got to do it. The only necessary line was this line. Line number five was the only necessary line. So I'll explain what it is. You first give the x axis, the y axis, the color Indian red C is the color this color is called the Indian red color is very close to red but it's a little different and it's a Indian red color and alpha is equal to five yeah so you don't have to give color or alpha these two are both niceties they make the graph look nicer so C is for color so you see here it's red colored and alpha is half. Means what happens is that matplotlib will just draw a big, deep, deeply colored circle. Deeply colored circle does not look aesthetically as pleasing, as beautiful as if you fade it out a little bit, you make it more pastel colors you know these days people seem to like western colors so you can get that by just fading it out a little bit alpha is equal to half once fade sort of make it transparent or half transparent half visible okay that is that that is what you're doing with the length the rest of it is no brainers you set the title you set the label x label y label and then you once you do this so this part of the code guys i hope it's similar you just have to remember that you create you're telling it that i want three blank subplots and this line helps you just go and plot it that's all there is to it. And so you get these three beautiful plots. Just looking at this, you notice that the three data sets are entirely different. The first data set seems to match a straight line. What about the second data set? It seems to move, swing back and forth, goes up, goes down, goes up, goes down. And the third data set is even more complex. right? It is hard to tell what the relationship is. It's a complicated relationship. So now we will see whether our algorithms can understand the relationships, right? We don't know the function, but can it infer or create some understanding to predict it? So one of the first things that you have to do, this is very specific to Python machine learning, excuse me, that you have to separate out the predictors, the input variables from the output variables. They need to be in two separate buckets. So for the first data, we separate them into x1 and y1. So underscore one just refers to one of these three data. But do you notice that I'm using a capital X? It is very common for people to use this notation. No reason why you should, but it's very common to use this notation, capital X and little y for this data. So you have the data now split. You know, the feature is separated. The predictor is separated from the target or the output, which is y. Now you do further another split, which is interesting. What you do is, and for reasons that we won't go into now, just take it as a fact, that you don't ever show the algorithm all your data you don't do that you never let it learn from all the data because you run the actually this is one very intuitive way to explain it is that you run the risk that the algorithm will just go memorize the data it won't actually learn anything. And if so, how would you ever know whether it has just memorized or it has learned? So one easy way to do that is you hide 30% or choose an amount, some proportion, maybe half, maybe 30% maybe 60% whatever, but you, you take a sizable amount of the data and you go and hide it under the pillow. You call it the test data, right? You split the data into training data and test data. So imagine that you have hidden the test data under your pillow and you go to the algorithm and say, here is the data and you show it only the training data, a part of the whole data. So the algorithm now learns from that whole data. And you now what happens is if it has just memorized the day the answers, then it won't be able to make predictions on the test data. Isn't it because that test data is not part of what it has memorized. So it is like a student who has only memorized the answers to some important questions. So in the exam, if those important questions, answers are not asked, but entirely different questions are asked, that student is going to fail. On the other hand, a student who has actually understood the subject, in other words, learned it, to him or her, it simply won't matter whether it's one of the past questions or one of the questions she has never seen, because she will answer it from her understanding. So that is the purpose of dividing data between test and train and hiding under your pillow. So test data, remember, is what you hide under the pillow. Except that there are two data now. Remember, you took the data and you split it into the X and the Y. So now for each data set, you end up with X train, X test. You split it into two parts, and the Y train and Y test, likewise for dataset two and dataset three. So now we have all sorts of pieces of data. Each data is broken up into testing and training data, as well as predictor and output data, input and output parts. So four parts, each data has now four parts, which are these four parts here for each data set? The training, the input of the training, the input of the test, the output of the training and the output of the test. So now comes... So how are we deciding which data is going to the training one and which one for the test? that which data is going to the training one and which one for the test oh what you do is uh see when you do this split train test split this function what it does under the cover is it will take the data and it will just shuffle it mix it up right and then it will take a random amount so suppose you say you do 50 50 split then it will randomly pick random amount. So suppose you say you do 50-50 split, then it will randomly pick half, put it in one bucket and the other half in the other bucket, but it will try its very best to randomize. You see the shuffle is equal to true. It deliberately tries to mix it up. If you don't want it to mix up and then give it to you, then you can say shuffle is equal to false false but generally you prefer shuffle is equal to true because then it will jumble it up and get spread it out keep hide you know it will give a representative data to you and in a similar part but that it will hide under the pillow let you hide under the pillow basically right and all each split split data in 50 50 or we can decide that how much data goes for the training homeless yeah you completely it's up to you so one of the exercises is i forgot what the default split is i believe is 75 25 or 70 30 by. So you can say train ratio is this 70% or whatever. It's just an argument. Sir, this is not case sensitive, right sir, because I see some double parenthesis over the year and the single one in the case of y output. Python is case sensitive. Python like C or Java is case sensitive. It's case sensitive. Okay, so. Python like C or Java is case sensitive. Very few programming languages are case insensitive. So we split this. So now comes the meat of the matter. And this is where the fun begins. Now we will take one algorithm. It is called a linear regressor. What the linear regressor does is it takes the data and it will always take a straight line through the data. In one dimension, two dimension, it will always try to draw a straight line. In higher dimensions, it will try to draw a straight plane through the data, but basically it will claim that there is a straight line and the data fits a straight line. That thing may or may not be true. You don't know. Till you build a model, you can't tell. In this case, because the data is two dimensional, you could tell. But if the data was 10 dimensional, it becomes a little hard to tell or see it. So there you try out a linear regression. It is very easy. One of the first things you learn about, and I will explain it in the next Sunday, I think. And so you notice that I'm repeating myself twice. So you don't have to read all the lines. Whenever you see blocks of three, just read one line. So in this one line, let me highlight it. In this one line, line number two, you take a linear regression algorithm and you say fit to the training data for the first data set. What you're saying is, here is the data, go learn from it. The fit is the same as learn, go learn from this data. So the model will go about happily and look at the data, make mistakes, then improve upon its mistakes, again look at the data, and it will keep looking at the data over and over again, and learning a little bit each time. Each time it makes predictions internally, then gradually it would have hopefully found the best line that it can fit to the data. So that's what it is. So now if it comes up with a model it has some line in its mind. You ask the model to predict on the test data. From the test data you just give it the input part. So it will make some predictions about the output. In the data science community, often you make the output wear a hat. So you notice the word hat, Y hat. It is a very common convention. The predictions wear a hat. Right. And so when you go through the notebook, you'll see in your class textbook, you will actually see that over the Y, there is a actual little hat if you look carefully, whenever there is a prediction. So this is the prediction. So now you've got the prediction, but you know the truth. For test data, you always know the truth because you have y test, which you never gave to the model. So now you can go under the pillow and see what the y value should have been for each of the x test value and compare it to what the y hat value is, what the machine predicted, and see how close the machine is to the reality. It will still make mistakes. You'll almost never be mistake free for reasons that we'll learn next time, but how close it is. You try to minimize the errors, not eliminate it altogether. So now that we have predictions, this is it. Now we need to go under the pillow and compare these two, the actual value that is there against the predicted value. So there are a couple of ways of thinking of doing it. One of them is to look at this mean squared error. It's a concept that I'll explain next time. The square, or did I explain the previous time? I forgot the square of the errors, y minus the prediction squared. Anyway, most likely I didn't. So we'll explain it next time. The other is called R squared. The word is, the formal name for it is coefficient of determination. It is also in books written as r to the power two. And in code it's because you can't put powers here in code, you write it as r2. This underscore one comes because I'm dealing with data set one so ignore that so r2 again compares the y test y hat the reality versus the prediction and then here i'm just printing those numbers now guys when i print these numbers uh i am printing it with a lot of fancy formatting ignore this fancy formatting you know this fun this extra 0.4. It just limits the number of decimal places. You'll have to pick up a little bit more Python because you become very familiar with this formatting, string formatting and so forth. Even if you don't do it, the output will be something. It may not look as clean, but that's all right. The answer would still be correct of course so this is the so it is so the mean the error a good algorithm will have low mean squared error a bad algorithm will have high mean squared error but the r square is the opposite coefficient of determination the bigger it is it will be on a scale of zero to, I mean, it could be a scale of minus whatever to one. It will never exceed one. But the closer it is to one, it can even be negative, but the closer it is to one, the more, you know, all other factors also being right, it may be indicative that you have a good model. It doesn't guarantee that you have a good model, but it probably indicates that you are on the right track somehow. Are we together, guys? So when we look at this, this is R squared, and this is how to interpret it. 0.944 is 94.4%. It is almost one easy way to think of R squared, a coefficient of determination, is to think of r squared or coefficient of determination is to think that this model if this model was a student taking an exam and then how much mark what is the marks it got out of 100. So it seems to have gotten 94.4 percent you would probably consider the student to be a good student has learned something that is a basic intuition that you have right and then forget about this model coefficients part and this number on the other hand you want to keep it low right so the next thing you do and i'm putting it here for now you do something called a residual plot residual plots are very important it's one of the things most people forget in fact most books forget to emphasize you do something called a residual plot. Residual plots are very important. It's one of the things most people forget. In fact, most books forget to emphasize, but very important. It is, you look at the errors, they should be positive, negative for each data point, the predictions, but in the residual point, a plot, one thing should be true. You shouldn't see an obvious pattern. If you see a very clear and obvious pattern in the residual plot, then probably your model is wrong. It can, they can be a better model. Are we together? So when you look at this plot, and I want you guys to all stare at this plot. Oh gosh, this has become too big. Where is the plot? Residual plot. Yes, residual plot. Let me make it a little bit bigger. Yeah, too big. Okay. So guys, look at this plot and tell me, do you see amongst the blue dots or the gridons, just the blue dots, do you see any pattern or they're just randomly thrown around? As your eye scans from right to left, do you see them all increasing together or decreasing together, doing something funny together? You don't see anything, right? So this is almost like, yeah yeah there is no clear pattern there's no um you don't see something happening here when you don't see a pattern in the residual plots residuals plot it is a good thing right it's quite interesting absence of information here is a good thing and if you see a clear pattern here a clear something then generally it's bad news it means that there is a better model that you can build right and you missed you have missed the mark yet so it is like a student who is not who has gotten a b or c grade right it may still be useful, but there's a hope that there's a better model somewhere. The prediction that are more concentrated to the line, middle line. VASANT DATARLALAKRISWOLAWALAVARNINI KUMAR- Well, that will always be true. So what you do is, as I said, when you scan it from, so look at this box, the 0 to 20 box, and look at the box 20 to 40 do they look materially different no they don't they both seem random noise isn't it but data will always be more concentrated near the the zero axis and it is important that they be this is called the histogram the frequency plot and the frequency plot should look like a bell curve, you know, the smooth bell curve. The word for that is Gaussian distribution. So we'll talk about these things next time theory session we'll talk about. And so it reflects on the side, you see this, this bell where my mouse is, the frequency is much higher of points close to the zero zero axis right x-axis and it is very few points are there stand in the outlier area and that is a good thing it should be like that now this is another plot it just checks that what is your model a straight line through the model versus this straight line here how different would they be it seems that they seem your model a straight line through the model versus this straight line here how different would they be it seems that they seem to have a pretty good alignment right best fit and identity what is the identity line you just draw a direct line through them right so it's a pretty good alignment cook's distance actually you can ignore for now because when I explain this in the theory session will actually it's irrelevant it just tells you if there are rogue data points hiding in your data that can hijack or completely confuse the machine the algorithm from learning so what happens is it's something like you know one easy way to think of it is that suppose everybody gets one vote in an election so everybody should have equal influence but there may be one person that everybody is listening to or for whatever reason whatever he does other people may start doing or God knows some reason that person has more influence so what do you want to do if to have a fair to offer whatever reason whatever he does other people may start doing god knows some reason that person has more influence so what do you want to do if to have a fair election you want to take that person out of the equation somehow right maybe he's bullying everyone to work one way or something like that so that is that is what uh what this a cook thing does. It finds points of high leverage or high influence. Because you don't want any one point to have a bigger vote than a lesser point, and any other point. You want all the points to have equal vote in training the or helping the machine to learn. So all right, that is that. So we did the completed the analysis of model one. It looks pretty, what it will do, we still have to do a little bit more, but let's go and look at model two and compare. So model one, you remember mean squared error was approximately 34, but look at this. We do the same thing. What is the mean squared error of this? Is it more or less? It's almost done, isn't it? And the R-squared is a disaster. When your R-squared, a coefficient of determination is negative, you pretty much have absolutely a barking up the wrong tree. This algorithm is not going to help you learn about the data. So that is it. It's bad news basically. And that bad news shows up often in the residual plots. Let's look at the residual plots compared to residual plot for data one, where you didn't see pattern. How about this? Do you guys see a pattern? Does your eye see a pattern in the residuals? Just look at the blue points. Does it all flow? Yeah, it seems to be flowing one way, then flowing the other way, and so on and so forth. The green points also are doing the same thing. So the net result is you become very suspicious. So I put some notes here. is you become very suspicious. So I put some notes here. There is a very strong pattern visible in the residuals, which should caution you. Right? The second is, if you look at it, does it look like a nice smooth bell curve? It doesn't. The bigger hill on this side and smaller hills, flatter hills on this side. Right? So it doesn't quite look like a bell curve for some reason it's cute your prediction plot is a disaster look at this this is where the predictions are and this is where it should be this is the axis where it should be actually the relationship between y and y hat so it's a disaster the cook's distance again ignore this part we talked about points of high influence so we we seem to see that for a second data set linear at this moment is doing poorly what about data set three this is quite interesting actually what is happening in data set 3 actually no there may be an error here I may not have run this yeah if you look at data this is better more believable data set 3 is 36.4 that is a pretty good thing isn isn't it? The linear model was spread in the very first was giving you values like this. And not only that, look at the R squared. The student seems to have gotten very high marks, 96.3%. We are still using the same linear model, but linear model seems to be working quite well on dataset three, isn't it? Let's see from the residual plot, if hopefully we don't see a pattern. So this is the residual plot. What do you say? Do you see a pattern or you don't see a pattern? We see a pattern. We see a pattern. This pattern is not as pronounced as dataset two, but it is still, there is a pattern. So this is, this is, that is the importance of residuals, of making residual plots. And it's unfortunate that most machine learning textbooks that you buy, like, you know, those programming books, they don't emphasize this. It's important to do that because if you look here, this model looked very good. You know, the student has gotten 96.35% marks effectively. Mean squared error is reasonably low, but you may begin to feel that, yeah, you have done a pretty good model. Linear regression is doing a great straight line fit. So you might actually start believing the data is linear. It follows a straight line, but two things will give you an idea that it isn't. If the data is low dimensional, like two dimensional, then the sheer visualization of the data, like, you know, look at data set three. A visualization will clearly tell you that it is not a straight line, though it is almost a straight line for some interval, isn't it? For some part, it seems to be a little bit like a straight line. And the other thing that gives you a caution is the fact that when you look at the residuals of, you still see a pattern. It means it's a given clue that data is probably not linear. So this is a prediction plot. It seems to be doing pretty, you know, areas it does good and it does bad and so forth. It's not really hugging the 45 degree axis. So that's that. I forget the Cook's distance for that, for the dataset tree. So now let us do one thing. Let us compare the prediction of the model to the data. So for the first dataset, the linear model is predicting, linear will of course draw a straight line. See what is it predicting compared to where the data is. Do you think this is a good agreement between model and data? do you say guys yeah it seems to be pretty good right date date the real data has noise any model you'll build will of course by definition be a pure theory it will be perfect yes it won't have noise so this seems to be a pretty good agreement for data set one. What about data set two? Do you think these predictions are sensible? No. It is an absolute disaster, isn't it? Yes. Right? So linear model doesn't seem to be working for this data set. And that is a lesson to learn that, you know, you can't assume that an algorithm is good or bad in the abstract it may be good for our data it may not be it's like asking is this medicine good each medicine is good for a particular disease not you can't just say that is it good everywhere good like whatever your illness or maybe you, maybe you need an injection of insulin, for example. That would be a disaster. You don't think like that. And so that is a broad thing. Remember that as we learn data science, you should avoid thinking or get too attached to one algorithm because somewhere it will work and somewhere it won't. Dataset 3, now you see something interesting. Do you see that it sort of works? If you were to stretch out Dataset 3 and make it a straight, straighten it out, probably this would be the straight line it would become, isn't it? Right? So, and now you also see why it sort of worked, why it got 96% R-squared, because it sort of makes sense. It goes through the data, but it does not capture the nonlinearity at all. So then you say, well, you know what, linear model, linear regression is not a good algorithm. Can I try some other algorithm? Fortunately, there are many, many algorithms in data science. And just for fun, I picked another algorithm called random forest. Just take this as a name. And at this moment, when you're learning, just go look at the scikit-learn library, and you will see a long list of regressors. the scikit-learn library and you will see a long list of regressors. Just pick any, you don't have to know the full theory behind it to actually be able to use it. Just a little bit of reading the documentation often serves the purpose in the beginning. See, understanding the theory makes you deep and work more efficiently and not make mistakes. But when you are learning, it is all right to go and toy with different algorithms and use it for your real world problem. In fact, 99% of the people who are practicing data science actually don't know the theory, the foundations of machine learning. They just go pick it, look in the library, see a name, they'll go try it out, read a library, see a name, they'll go try it out, read a blog, right? Somebody has written a blog who also probably tried something. And it is a little bit like the blind following the blind, except that they don't fall into a ditch most often. They end up solving whatever problem they are solving because many, many problems in the real world, like when you go into the industry, there are lots and lots of fairly simple data sets, which you can almost solve by looking around, finding an algorithm, try it out, and see if it works. what's there in the car the internal combustion engine the you know the piston the crankshaft and all of that the differential gears that takes a probably a degree in mechanical engineering but to drive a car doesn't take much you just get into the car you know they're steering you know the brakes and you know the accelerator and that's all you need you're on the move so think of these algorithms like that because you can they're plug-in replacements instead of saying linear regression remember i just changed it to this a random forest another algorithm by the way all these arguments you can remove just change the name to random first regress. It will work. Now you notice that everything else remains the same. Fit. The fit and the fit is there. Then you get the predictions y hat one, two, three. Then let us evaluate this on the three datasets. Now I'll go a little bit quickly. The R square seems pretty good good 90% but it is less than linear regression. Do you remember linear regression. What was the prediction. I'll just jump there to just show you Shubham Tulsiani, Linear regression. The evaluation was 94.4% you remember Shubham Tulsiani, Right for data set one, whereas this random forest. Shubham Tuls random forest is giving you only 90%. So it's all right. You take it. Then look at this residual plot. Do you see a very clear relationship of function sort of there in the residuals or is it more or less random it's close to the y the the horizontal axis of course you know that's where more data points will be but do you see a pattern here guys you don't see much of a pattern would you agree guys something yes yes so this is good news. Yes. It's more of a scattered. Yeah. Yeah. Yeah. It's more. Yeah. So this is good news. It means that we may be up to this random forest may be good for data set one. Not the best, but good. And even this prediction error plot, it seems to be close to the dotted line, the identity line. So it looks good. Then you try dataset two. Let's see what happens with dataset two. And this is where the good surprise is. Remember linear model on dataset two was a disaster. You remember? If you go back just to recap, on dataset two, the linear model had a r squared below zero, right? It was a disaster. Whereas a random forest on dataset two seems to be really good. The mean squared error is very, very low. And the coefficient of determination is at least 77% already. Pretty good coefficient of determination isn't it? Like the student has gotten 77% marks in effect. One rough analogy. Now let's go and look at the residuals of this random forest for data set 2. What do you say about the residuals you see maybe a little bit of a pattern you may start seeing but not much is still more or less spread out isn't it and the bell curve is also nice in this the bell curve also is nice at least more or less there so so so it means that the random forest seems to be doing well for the second data set also but but do you notice that in the first data set it did not do as well as the large as the linear regression model and that is something to know that you know for any data, there is not a rule that one algorithm will always do better. So on the second data set also, this prediction error plot seems good. Now let's go to the third data set. And there you see it shine quite a bit. You notice that on the third data set, it seems to say that the error is only 96. I mean, it has gotten 96.35% score, like doing very, very good. But the test is in the residual. Let's go look at the residual. And you notice something interesting. Data is clubbed here, but still there is no pattern here, right, between the predicted value and the real value, because the data really had a lot of values clustered around this point. And then it sort of spreads out. So having said that, if you look at the vertical, so as you scan your eyes from left to right, just see, do you see a very obvious pattern? And you won't see a pattern. And here the bell curve also seems to have formed well. So random forest seems to be doing pretty good on dataset three also. So now there is a final test to do. Remember we plot the models on the data itself. So after this, plot the model on the data. So on random forest is plotting like this. Do you notice that it's not drawing a straight line? Isn't it? For data set one, it is not drawing a straight line. Whereas the regression model, if you go back and look at regression model, it was, what was the, sorry, the largest linear model, not regression the linear model the linear model was drawing a perfect line through the data isn't it yes but the random forest one it seems to be doing a slightly different job it seems to be uh scattering it a little bit which explains why it is not performing so well right because this data, when I produced it, I literally took a straight line and then I added a lot of confusing noise to it. So obviously, what you say in the language of machine learning is random forest may be overfitting a little bit. It may be listening to the noise also in many ways. What about dataset two? Oh, look at this. Remember that for dataset, linear model for dataset two was a disaster. I'll just remind you of that. So look at this. Do you remember this plot? This was a complete disaster, isn't it? Relatively, Random Forest seems to be doing, let's see how Random Forest does. Or much better, would you guys agree? So here, the blue one, or the colour that I used is yeah the blue the blue is the actual data and the red one is the prediction the models prediction so obviously the models prediction seems to follow the data isn't it so it is very aligned. So you say therefore that it is doing very well. What about data set three? Data set three, once again, the linear model, let's see what it did. I forgot actually what it did. Oh yeah, it tried to fit the best possible straight line through the data. Isn't it? That you know that the data is not straight, it sort of has just this nonlinearities or distortions or bends, but all it can do is fit a line and it did a pretty good line fitting the best possible line through the data. Still the limitation is it did not capture the nonlinearities, the bends. but what about random forest what do you say to this model guys hang on do you think the predictions the maroons go along with the blues everywhere yes, it seems to be doing that. It seems to be doing, yeah, and this is dataset three, I apologize. You see that how closely it follows the nonlinearity of dataset three. Isn't it? It is almost as though if the dataset three is a snake with lots of wiggles, then the maroon dots seem to follow that. So this is your lab folks. Now what we will do is let's take a little bit of a break and now here's your homework. You don't have to understand what these algorithms mean. It's too early, but the game is to get started and to start doing something practical, to become in other words, a bona fide data scientist, at least partially. So if you go to the scikit-learn library, you'll find many algorithms. There are at least three in particular I've mentioned, and repeat the same exercise with three. What it means is that your entire code will remain the same. The only place, the only one line that you would change is. So for example, I go to the linear regression, right? Instead of saying linear regression, you will say SVM, right? Or a decision tree, or X XG boost or something like that. And you will import those libraries. That's all. So the code that you would have to write practically would be, I mean, every code that you write, of course, multiplies by three because you're dealing with three data set. But actually the sum total of code you'll probably write will be two three lines right but you'll have to spend some time going and reviewing this code after that maybe two lines say I mean yeah one line for XGBoS one for decision tree one for support vector machines so that is it. And then when you come to support vector machines, you will realize why this AI lab that you are taking workshop in, why it has such a strange name called support vectors. I called it support vectors because in this field, support vectors were absolutely magical algorithms that are often used and that brought about a revolution into machine learning. Many years ago in the 90s, it used to be called the kernel methods revolution. So obviously I've named my company or this lab, EIA lab in honor of that. So all right guys, so let's take a 10, 15 minute break and then I'll meet you again. I'm going to pause the recording somewhere there. In the first part of this session today, we did a lab in which we covered three datasets. The task we took on hand was that of regression. In other words, how do you predict a number based on some input, some amount? One sort of a picture I gave you in mind is think of the input as some temperature or some factor and output is the amount of ice cream that the entrepreneur hopes to sell or has sold on the beach and the entrepreneur is trying to make a model to more accurately decide how much ice cream to buy from the wholesaler every day so if you use that mental frame or picture in mind it will be easy to understand the lab. Now, as I showed the lab, a few things are there. I have done the lab in a language called Python. We use libraries, NumPy, Pandas, and Matplotlib. You saw the three. There is also a library called Yellowbrick and a couple of other libraries. So today in this part, we will go through first the installation of all of those onto your machines. In fact, I deliberately took a machine which itself needs to be upgraded to that. So it is a good thing. And after that, we will install the libraries. And then once we have done all of that, then I will come to individual cases. Many of you don't have access to Slack. Reach out to Kunal immediately on the WhatsApp or whatever other social media channel you have used to communicate with him. If you are otherwise, send me an email directly instructor at supportvectors.com. And the other thing you need to do is go register on the course portal and that is also something I'm going to show you in a little bit so I'll start with the Course portal for a moment. The course portal is this if you go to supportvectors.io and Then just go and register there. So let me see what an anonymous user sees so I have a better idea. Let's go and see. Control shift. Okay here we go login. So when you go here login using or create an account, you can create an account and connect it to your Google account that would be perhaps the easiest way to do that you can sign into Google and then you can create it and do all sorts of things so go through that process become here and then once you have joined this what would happen is you will ultimately in your my courses you will start seeing that course here in this location in the menu you'll start seeing it also in the course page itself you will you may see it but i have to enroll you first uh just because you enroll to the portal doesn't mean that you have registered to the uh this now those of you who have already registered this now those of you who have already registered or maybe you can so by the way once you come to this this course page will keep growing every time we have a session like for example the videos all the videos I'll put here so you don't have to go to the support vectors YouTube channel and keep searching for the videos they will all be properly listed here so one thing you have to do just keep coming back to the course portal are we together right now in this course portal let me see if i can administer users enrolled users okay enrolled users okay and we'll use it so uh kunal were you able to add any oh yes you have already added a lot of users thank you yes sir yes so uh these guys have to register first for us to enroll them so i could not find a lot of people in this so guys uh here's the thing. Can I request you that if you have not registered, just ask yourself this question. Do you have a way to reach Kunal on WhatsApp or something? If you don't have, now is the time to speak up. Anybody who does not have a way to reach should speak up now. On the other hand, if you have a way, then just reach out to him and could I just make sure that they all are in there. They're about 39. I'm getting I'm getting messages saying that this account is pending email confirmation. It is pending. Yes. So can you confirm these guys? Yes, sir. kumardivad.gmail.com Yes. All right. So that is one thing, guys. And you also saw the YouTube channel. And you just keep coming to this portal. And this workshop is essentially a fast-paced introduction to data science. I suppose the fast pace was evident today. I lean towards quickly getting you guys started, but may or may not have been the best strategy. But in the theory sessions, we'll go slow and learn. So if today looked overwhelming, don't worry. When we get back to the theory session next week, it will be much slower. So that is that. That is one. Do the homework. This Jupyter Notebook that we went through is right here, both in HTML form and in the Jupyter Notebook form. So you can just download it and just start, you just adopt it into your machine. The next thing that you should do is, this is the GitHub for data. We already got data from there. You saw that here. It takes you here. This is where you'll see a lot of these data sets. And the other thing I would like to show, where am I? Home page, Univariate course. Now this is a different course all right now I would like to show how do you do the installation anaconda.org so for that I'm going to from this local machine make sure that I don't have it or if I have it I'll delete my own setup so that you can feel you can just follow along with your laptop whatever it is that I'm doing. Alright guys so let me go and delete this user anaconda.org. I don't have anaconda on this machine. This is good. And program files or something. Also I don't have. Okay. So what you do is you go to this website, anaconda.org. Those of you who are on your laptops and would like to follow along, anaconda.org. Those of you who are on your laptops and would like to follow along, please follow along. Are we all here at this page? We then go to this download Anaconda. Do you see where my mouse is guys, at the top right-hand corner? We click on this. When you click on this, it will straight away take you to the download button here, your data science toolkit. And it is widely used. As you can see, over 20 million users worldwide are using this. So all we need to do is we need to click on this. When we click on this, it jumps down and it says which operating system are we using. Depending upon which one you're using. Now I'm giving this session from the Windows one. So I'll take this Windows 64 bit. Whatever your operating system is, pick that one. system is pick that one so it will start downloading and this download is a huge one so obviously it will be a little bit hard for you to follow along because you may not have the bandwidth to immediately download it so what I'll do is I'll start the installation and as we talk hopefully a download completes and you can catch up. But the steps are very simple, there's no magic or confusion in that, it's one click install. All right, so on my machine, the download is finishing. This is Windows as I said, will proceed to install now. Do you guys see the installer? The installation, hang on just to make it obvious and clear the screen of all of that letter on my screen you So, there we go. Just follow along. Don't change anything at all. All right. And the one thing I would suggest that may help is click on this button. Add anaconda to my path environment variables. Even though it says not recommended, it's a good thing to do. On your personal workstation, if you're doing it in the office, don't do it. Asasur, just one thing. I don't have access to how to add the pending invitations. Oh, is it? Well, that probably explains why you're not seeing them. Let me see if I have, if I can get to them. Administration, course administration, site administration, users, accounts. All right. Browse list of users. This is the list of users. Show filters, full name name user full name contain ad filters show more yes somewhere here should be confirmed is no how many people are not confirmed and what do we do now where's the apply button add filter all right so so many people oh yes they are all waiting to be confirmed. So let's keep confirming. Confirm. Is there anybody's name that somebody would like to say? Kunal, you're taking care of it. I'll take care of it. All right guys guys so now let's go and see what's happening to the installation process Oh installation is complete and then what happens next individual edition tutorial so there's some tutorials and things like that it will bring up you can spend some time on the website doing the tutorials I won't so now what you do is you go to the start menu if you want and type the word anaconda bring up the anaconda navigator app i'll just leave it here so most of you can see what i'm doing do you see that i'm selecting the application anaconda navigator and starting it you can do that in your mac and those of you who are technically savvy using the linux of course you just give the anaconda command on the command line and i couldn't navigate it when you give this command can i give it guys are we all here right and then your install will go through in a little bit but as it goes through eventually you will see something and let's wait and see that later all right guys so uh when you open the application for the first time you might have to click a couple of buttons here and there confirm and so forth but eventually you will see a dashboard like that it is your anaconda navigator this is your dashboard it has a lot of things built in and some things you can install for example you can install the R studio you can install a visual studio code which is another free one but the only thing that we will need for this particular thing is actually not even the lab notebook. Do you see Jupiter notebook? Just simple Jupiter number is the third item From the left this one so just become familiar with this symbol Jupiter notebook and all we need to do is click on this in the beginning when you're just getting started click on this in the beginning when you're just getting started and lo and behold it will bring up your home directory there will be many things so typically what you do is you go create a new directory for yourself neo folder let us say okay there is an untitled folder here I'll just go in here and then I'll just delete this first of all let me rename this folder this name okay sorry uh you can go it's a little bit clumsy uh in doing the things but you can actually go here and click on the rename button i'll just call it fast paste intro intro some folder which is fast space intro and now i'll go into that folder at this moment let me delete it so that you guys see it from scratch okay there we go so now what I'll do once you are here all we do is we start a new Python you see Python 3 so just click on Python 3 you go here and then it's ready. You're ready to write your first cell. So for example, if you were to do basic stuff, 2 plus 10. Let's hit enter. Let's run it. Do you see this run button here? You can either click this or you can just hit control enter. It seems that 2 plus 10 is 12, right? According to this. And so you can continue creating new cell, you know, just click on the plus button, new cell. Now, if you get started with basic Python programming, let's say that your X is, you create a variable. a variable x is equal to, I don't know, 2, y is equal to 10, then you can say x plus y. And then when you run it, it turns out that yes, in the variable x, you call these things variable. They're like boxes that can contain something, a number. So if the X box contains two and the Y box contains 10, X plus Y is 12, which seems to be correct. And so you can move on now. And for example, if you want to do numpy, import, let's be more into the data science, numpy np. You can do, there we we go this import has happened you could do import um find us the three libraries that we talk about spd right import mat plot lib dot pi plot so this these libraries at this moment you essentially essentially memorize. You copy paste from my notebook till you become familiar with this. So now all these three got imported. Now we'll go and create, let's go create something useful. Let us say that I create an array of numbers, x is equal to, and here what I'm doing is there is this function called linspace in numpy. It helps you create a number. Let's take a number between zero and 10. Right? let's say 10 and how many numbers do we want let's say we want 20 numbers let's see what comes out this is it so now we have X let's see what X looks like just say X oh we got these numbers so you can see that the biggest number will be 10 and we got 20 numbers spaced equally between 0 and then. Well, if you want 100 numbers, you can do this and you get 100 numbers. If you want it between, instead of 0, you want it between minus 10, let's say, you hit this and you get it between minus 10 and this so this is how you think about it right it's very easy this language is uh language of data science as you can see right everything is a one line command most things in the beginning when you're doing so suppose y is equal to, let's take sine of x, np.sine of x, right? And let's see what y looks like. There we go. So for each value of x, it has computed the sine of x, right? So then you may want to say, okay, this is very very good but how about plotting it and seeing how it looks so you would say a plot plt dot scatter right and you scatter it you make a scatter plot and you say x y right that is it and when you do that ah amazing see does it look like a plot of a sine wave guys yes that is it so this is it you know and then suppose you want to make your function a little bit more complicated let me make it more complicated Y is equal to plus X X to the power 2 this double star is a symbol, or maybe I'll just say X star X, X multiplied by its simplest square. Now what happens? Let's do this. And then what happens? You go to the next one and you plot it. And then this becomes, oh oh look at this what does it look like it looks differently I can go and play with this and say a 10 times sine X between 0 and 100 30 times signing just just for no rhyme or reason I'm doing this cooking up some function just to show how easy this whole thing is. And then there we go. And now we have a complicated function like this. So this is how you get started guys with the, with Jupyter. It is as easy as that. And just pick up the basics of Python and you'll, you'll, you'll hit the ground running and look up the basic tutorials on NumPy, Pandas, right? And plotting, plotting almost comes by example. The world is, the web is filled with examples for this. All right, guys? So that's all I have for today. If anybody has a question, can ask, otherwise we'll end today's session thank you I got a question regarding the residual thoughts that we talked about residuals yes residual is a concept I'll explain to you next week, but you can ask the question anyway. Okay, so you said if there is no noticeable pattern and it means the model is doing well. Yeah, that's it. What does it exactly signify? What does it mean if there is a pattern and if there isn't? And this is something we'll cover later, but let me give you a preview of it. It means that you have left some signal. Your model has not picked up all the signal in the data. So if you think that the information in the data, the real clues in the data, if you think of it as, let's say, rupees, a hundred rupees. So when you start seeing this pattern in the resume, pattern is information. It is almost like somebody laid hundred rupees on the table and you, one rupee notes, if those still exist, and then you went and picked up maybe 60 of those and or whatever number, whatever proportion and you left a large number of the rupees just lying on the table. That's one. Basically there is still or if you think of it as a lemon, you haven't squeezed the lemon fully. When you see a pattern in the residual, that pattern is information. That information is that which the model didn't learn for whatever reason I couldn't learn does that give you the intuition yes yes maybe when you explain it later I'll get it better all right guys anything else and I have one doubt so so in the plotting thing what you have explained actually I'm the fam new to this Python so I just had a doubt so in the plotting thing you have have explained actually i'm new to this python so i just had that doubt so in the plotting thing you have put the scatter thing so the scatter thing is applicable apart from the linear regression to everything sir any any data x and y axis data any data you can relate on a page as a scatter plot and there it is that's it okay whereas in case of linear regression so what you explain and i'm not sure with that so only in those case we need to put the linear thing to get this right line curve no linear or tries to build a model okay it asks this question what is the best straight line that that will that could represent this data will that could represent this data the word linear comes from line it's basically saying what is that straight line the best straight line that I can pass through the data so if you look at this I'll show you the examples look at this in a straight line here it is easily able to fit a straight line. Do you see that? Right? Yes, sir. Maroon. Look at the maroon color. In this one case, it is really confused because the data goes up and comes down and goes up and- It is like sine wave, sine wave. So here also, sir, in the same thing like you have mentioned as a plot scatter. So I mean to add the scatter is a variable name which is used for everything while plotting a graph it's a function name yes it's a okay okay thank you you're right about that right so that's that okay and likewise if you build a different model then it it may approximate like for example this random forest regressor. This is able to do a much better job fitting to the bends in the data, you know, the nonlinearities in the data. But scatter will always plot points, whatever points you ask it to plot. In other words, the plotting and scatter and all that, they don't directly have to do with machine learning. You give it data, any data, it will plot it for you. So guys, your homework is this you try if you can just replace the words above with SVM or XG boost or decision tree and try it out and you'll see that it's very easy as I said harder part this week you'll struggle to understand this code because you guys will have to brush up your python or learn python but once you do that and then this homework is five minute homework you can finish it after that installation will take time and other things but homework is five minutes but the important thing is what it means therefore is and i really mean it take this technology take this notebook and go try some other data sets in your workplace or any any interesting data sets you find anywhere and you will find that you have already become productive you have already doing something useful that was the point of getting into action soon all right guys and today's session any other questions if I want to edit this in my mission how do I go because when I launched this that is in the local host and I am able to add and do the sample which you showed. So if I want to open the file, I have to save this file first. Yeah, let me do it. See what do I do? I have this JupyterLab notebook. I'm going to click on it. Well, don't click on it. Actually I should download it. Hang on. I'll just right click and download this. Let me make it more downloadable. Save. Yes. So it is willing to download this somewhere. Let us say that I created a folder. Did I create a folder? I think I just created a folder, right? New folder or something. Let me create a folder on the desktop. Suppose I create a folder called workshop. You just go into the folder and save it. Let's say I go and save it here. I saved this file. Now what do I need to do? I can go back to my, a folder named workshop why is this not showing up here okay maybe this is a nice new desktop directory yes desktop directory is I'm not used to Windows actually I mostly work on Unix so this is a good lesson for me yes and so as you can see I go there and I just click on it see I just downloaded it onto this Linux machine and here it is and now I can start working on it. Are we together? Yeah, yeah, I got it. That is it. It is as simple as this guy. So take this, you can't destroy it, you know, go and edit it, play around with it. What is the worst that will happen? You won't remember how to fix it. You can download the file again and soon you'll become pretty comfortable with Python and this will all look very easy Thank you.