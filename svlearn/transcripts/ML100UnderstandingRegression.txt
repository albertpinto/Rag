 So this is Saturday evening in California and morning in India. The topic that we are going to cover today is regression. today is regression. So far we have gone over machine learning, the broad ideas, the big ideas there. How does machine learning differ from programming? What does it mean to learn? And when we said what does it mean to learn, we realized that learning has two steps. We need a way to quantify errors or measure performance. And then as the model, as the machine learns from the data, there should be observable increase in performance at a task. In particular, if you're quantifying it with errors, there should be a measurable improvement or decrease in errors. It makes less and less mistakes so that was what we talked about the last time we also talked about various forms of machine learning we talked about predictive modeling you know in which we have the predictive modeling part was in broad strokes. Let me make my thing a bit bigger. Predictive modeling. Predictive modeling. This is also called more commonly in the literature for reasons that is often misleading to people from outside, supervised learning. So what was supervised learning? The basic idea is that this is a model or a machine or an algorithm, I'll go, or if you want to think of it as just a machine, it's a machine. Input goes in, let's say that there are various features, X1, X2, and let's say it goes down to XD. They're D features. For example, if you're trying to sell ice cream on a beach, and let's say it goes down to xd. There are d features. For example, if you're trying to sell ice cream on a beach, the features could be the temperature, wind speed, and so on and so forth. And what you're trying to predict, some prediction comes out, y hat. And the way you train a machine learning model is you always take data, let's say that x1 column, x2, all the way to xd column. And then you have a y value. This is the ground truth. You need to take some data, some examples, for the machine to learn from, for the algorithm to learn from. And you need sufficient amount of data., for the algorithm to learn from. And you need sufficient amount of data. And what the algorithm will do is it will build some hypothesis about the data and then it will look at that, the predictions of that hypothesis, then it will find what the error is. And then from the error, it will now say, what can I do to make less mistakes smaller errors right so that is what we are going to learn today now when y hat is a number if y hat belongs to r it's a number and actually strictly speaking it can be a vector also so it could be a p-dimensional vector, but we won't talk about it. Just think of it as a number. When it is a number, then it is called regression. If y hat is identifying cats or dogs, classes in the language of machine learning, you say classes of a category, for example, cat, dog, cow, duck. Suppose the only four animals on your meadow are these four animals. Then you would say that you have a categorical target variable. This y hat is called the target variable. This is called the target variable. It's also called the output. It's also called the output. It's also called the response. And there are many such names to this. This is called the input. Predictors. These are independent variables in the statistical literature. So, you know, this kind of models or machines were being built in different fields. So you know this kind of models or machines were being built in different fields. So it sort of predates machine learning proper. A lot of these ideas go back to statistics and even before statistics. The history of this is more than two, three hundred years old. So they're called independent variables, independent variables. And these are therefore called dependent variables, the output. Now, we won't use this in order to be consistent. And there are more things. Sometimes you'll say these things are called regressors. And this is, if it is a number, it is called a regress and it's an infrequently used term i will stick to something very simple features and i will use either this most of the time i'll use this sometimes i'll use this and sometimes very less often you just use predictors. If I want to emphasize that, you will rarely see me use words like this or to use words like regressors or something like that. I wouldn't be using that. They're all synonyms, but I like to use simple words. We'll just say input and output or features or input and response. I like to use this. Sometimes I use the word target variable. You'll rarely see me use the word dependent variable and so forth. So this is classification and regression. So when it is category, it is classification. Classification. So an example of classification is the child is trying to recognize or the machine is trying to recognize whether the data, let's say that you give it an instance of data, which is the weight and the height of an animal and so on and so forth, and it tries to determine is it a cat or a dog? That's an example of classification. So that is that. So this is a review of what we have done so far. What we are going to do, like you said, the scope for today, today what we are going to do is we are going to learn about these two things. Quantify errors and systematic way to learn from errors. Learn from errors and reduce them. Are we to it? So how are we going to do that? That's the main topic for today. And let's get into that. So before I go further, in point of the review from last time, are you guys are fairly now comfortable with the broad ideas of machine learning? Yes. This is it. So this was predictive modeling. Now there was another kind of machine learning I talked about, two other things. One was pattern recognition or unsupervised learning. So the word supervised and unsupervised are very common in the industry. I try to, at least in the beginning stages when people are learning, I recommend that don't use the word supervised, unsupervised. It can get a little bit confusing in the beginning. Just ask yourself, are you making predictions? Then it is predictive modeling,, are you making predictions? Then it is predictive modeling. Or are you recognizing patterns? For example, you search for clusters in data. For example, you have data and you look for clusters of data. There are two clusters here in the data that I just drew. Or you do something called dimensionality reduction. We'll talk about that reduction. And there are many other things you can do in the unsupervised space. And we do that. You can build all sorts of generative models. We won't talk about that in this workshop, generative models, et cetera. There are many things you can do on this side. And there is yet another form of machine learning, which is based on the carrot and stick principle that we talked about. So we talked about another form of machine learning called reinforcement learning. And we saw an example of that, you remember the video that I showed in which there were two groups of people, two groups of actors, they were adversaries. One was the finder, one was trying to hide, one group was trying to hide and the other, you literally saw how the AI, both sides got smarter and smarter, smarter and smarter over time. So the way you do that kind of AI is that to those agents, to those things that are playing the game, the actors, you give a system of reward and punishment, the actives, you give a system of reward and punishment, reward and penalty. It does something that actually helps it win. You associate a reward with a positive action and you associate a penalty with a negative action. So, for example, if a course of action leads to failing in the game, then you have a penalty. Then the machine goes and says, we shouldn't have done that. Let's try a different strategy. If it wins, it remembers that, okay, maybe let's go and improve upon this strategy to win even more. So that form of machine learning is reinforcement learning. So these are roughly the broad areas of predictive modeling with its classifiers and regresses are very, very prolific everywhere. Clustering pattern recognition is also everywhere. Then they are in more places than you realize, actually. These days, as you type code in many of these powerful so-called IDEs, like Eclipse and other things, Visual Studio Code and so forth, you may not realize that. But behind that, the reason these IDs, these tools to write code, they feel so nice is that today they're heavily, heavily driven by artificial intelligence in the background. So there is artificial intelligence that's trying to figure out what is it that you're trying to code. And then it goes and gives you the most intelligent responses or it does auto-completion of your statements. So we have literally reached a stage that in some of these more powerful IDs, it would be hard to argue that it is the developer alone who is writing the code. It would be fair to say that developer is merely providing the hints and most of the code suggestions and most of the sort of laying out the structure of the code. Eventually, the AI is taking over without the developer knowing about it so and that's a remarkable thing there's a few as i see in the audience there are people who are familiar for example with visual studio code or with eclipse and some and those of you who are not don't worry about it but increasingly now they are ai driven that's why you find it so convenient to write code with that the same is true for compilers actually when you compile c code now to an executable and you use the gcc compiler to create the executable it turns out that the compiler itself makes optimization decisions based on machine learning. And so machine learning is everywhere in the United States. If a letter is sent from one end of the United States, it reaches the other end of the United States almost always without getting lost and it reaches in a week or less. We are talking of a distance of close to 3,000 miles, 2,700 miles, or more than 4,500 kilometers. And it covers that distance twice the length of India, I would believe, or twice that. It covers that distance in a week, and it never gets lost. The reason it doesn't get lost is the moment you drop your letter in the mailbox and it goes through to an artificial intelligence character recognition system, which immediately recognizes the address that you have handwritten on the envelope. It translates that into machine sort of a barcode of sorts, little digital code. It attaches that to the letter and for the rest of the journey no human being engages with it. Automatic sorters from place to place route that in the optimal way to the destination and then it even reaches the the box or the cart of the postman of your neighborhood without his knowing. So he gets a bundle of letters to deliver to your neighborhood. And it is only when he comes to your street that a human being, namely him, actually touches the envelope. So the entire journey has gone more or less untouched and it has been navigated by computer systems using artificial intelligence. Now this in the US is a very old technology. I'm guessing that this technology is now prevalent in India also because people tell me that let us reach sooner than they used to 10, 20 years ago. So it is everywhere. When you drive a car, it is hard to say whether you are driving it or the automated systems in the car are partly driving the car and preventing you from making mistakes. In the US, most cars are such that you get used to the automated systems. If you take a car of the 1970ss before the world of computers and AI and you give it to any person today and ask the person to drive, people have done such experiments. The modern driver gets into the accident in no time. We have actually forgotten how to drive a car which is not computer assisted. It is a measure of how far technology has come and is coming. Today, as I mentioned to you, you find situations in which the car is driving a person to work on the highway and the person is quite literally asleep and people have got YouTube videos of that, Which is very dangerous because the artificial intelligence while it is there It hasn't reached the level that you can take such risks It is not perfect yet and there can be accidents in rare cases There are accidents, but there are situations where it's a lifesaver there are many situations for example, where a person driving in the highway has a heart attack or for example, where a person driving on the highway has a heart attack or has a stroke or gets incapacitated and the car, these intelligent cars, they drive, they recognize the situation and they gradually either stop at the corner of the road, side of the road and call emergency or they just take the way off the ramp and call emergency and so forth. So these things are not possible in the old world. It is not something that you can program for. These are things of artificial intelligence, the power of artificial intelligence. And the gold standard that people are looking for today is true automated driving in which you just sit in the car and the car knows where it has to take you because it knows your schedule and just drives, navigates through the traffic and takes you to your destination. We haven't reached that, but there is certainly a lot of effort to take us there. And we are very close to it. The last mile is a very hard problem. I think it will take a few years before that is solved. But that is happening all around us. So that is the broad scope of machine learning. Today, we did a lab last week, actually, in which we looked at one particular problem, regression. So to recap, what we did is we did a lab just to give you a flavor of what regression is. So recap, regression is a machine algorithm in which at this moment we took one dimensional data, just x, and we got y hat is some function of x. Right. Our goal was to try to fit a good function somehow without actually telling us or needing to tell us. We tried two algorithms. We tried linear regression, which tried to fit a straight line to data, fit a straight line to data because our data was one dimensional. In higher dimensions dimensions it would be a plane or a hyperplane and the second one we tried was random forest if you remember the lab there were three data sets the in the three data sets the first one was truly linear it was very easy to get a sense of the data if you remember data first data was like this so you could draw a straight line the other data was more along the sine wave so it was much harder to fit a straight line but the random forest did reasonably well this was not quite a good fit and the third data was even more complicated basically the curve was something like this right so it is much harder, of course, to fit. But nonetheless, even in that situation, we realized that a straight line was not doing that bad. It didn't quite fit the data, but it was still able to make reasonably good prediction. So one of the questions that comes in machine learning is, what is a correct model? And do you guys remember what answer I gave to that? How do you know that if a model is correct? By making more number of data. More number of error. Sorry. Go ahead. Go ahead. Sorry. Bit louder. a little bit louder depends on the error the error but can the error become zero the minimizing the errors yeah you minimize the error but see you'll never get the error to zero so when do you know that you should stop trying that you have hit upon the correct answer whenever inaccuracy is equals to accuracy inaccuracy equal to accuracy would be a half right and half wrong situation do you really want that or you can do better than that? So, for example, would you like to go to a doctor who can recognize or find, correctly identify cancer in patients only half the time? Any other attempts, guys? Try and see. You can only think aloud and try because you're learning. So it is okay to make guesses. Is it the predictions with minimum errors yes you want to make predictions that make very few errors so i'll tell you a statement which i think i mentioned but may have slipped you in science broadly not just machine learning so making some clustering groups like making a groups and then coming to the conclusion do you know these are good attempts good ideas but uh now let me ask see not just the machine learning in science yes we can try different companies compare the multiple multiple results uh whether the previous result is giving some kind of update i mean the result you can try different algorithms see the question is this so you can say that method one is better than method two or algorithm one is better than algorithm two because it has less errors but how do you know that algorithm one is the correct algorithm there might be yet another algorithm algorithm three which may be even better than one so the science itself if we don't have a right model it's an ideal model ideal solution exists in the science so everything like uh fair and like what you can say the result always will not be a correct madrid is nothing in as such in science that's what you mentioned in the last yes thank you for him for hitting on that so let me state it for everyone see in science there are no correct answers There is no truth with a capital T. Science actually is the opposite of that. It says that the foundation idea of science is this. It is, first of all, it does not search for truth with a capital T. There is no scientific truth as such. When we are children, we learn in school that, for example, Newton's law is true. But what is Newton's law? It is human model for observations in the physical world. It is a model that works. It's a theory that works. We consider it a law of nature, but it doesn't grow on trees that we can pluck and eat. It is something purely constructed by human beings. And you can easily imagine that some other form of intelligent life has an entirely different explanation, an equally valid explanation for why things get attracted to each other. The force of gravitational attraction. They would have entirely different concepts possible. But even for Newton's law, is it correct? We know for a fact it's not correct. It's superseded by Einstein's theory of general relativity, gravity theory, which itself we know cannot be true because it contradicts quantum mechanics, which quantum mechanics as an explanation of the atomic world itself is not true because it doesn't agree with either special theory of relativity or general theory of relativity. Now, those of you who know this little bit, I'll give you a peek at that. When you can fit quantum mechanics, the explanation of the small world, microscopic world, with the special theory of relativity, the first theory that Einstein created. And the two can get together and become relativistic quantum theory, which is the dominant theory these days. But then it doesn't agree with the general theory of relativity. Einstein came and he essentially superseded his own theory with a much bigger theory, much more general theory. But now there's a problem that doesn't agree with quantum mechanics or quantum field theory at all and people have been trying to resolve it for 60 years, 60 years more than that, 80 years almost, 80, 90 years they have been trying to merge the two theories, actually a hundred years almost, and nobody has failed. So do do scientists believe there is a truth with a capital t no there is no such thing as a truth with a capital t in science in science all we have are two things you have effective models or explanations or or theories as you may say, theory or hypothesis. Theory is by definition, hypothesis that evidence seems to data seems to support. But it doesn't mean that this is true one example that people often give is the so-called thing that in europe or in india for example most of the time if you look at swan the swans are white so you may make a theory that all swans are white right you may take it as a fact in biology or zoology that all swans are white. And that would hold true in your world till one fine day a black swan comes and sits in the lake in front of you or the pond in front of you. Now what has happened to your theory? Your theory has gotten disproved. You have a black swan now. It's a rare thing. And actually black swans do exist. So the nature of science is the theory should be such that evidence should be able to disprove it. And that is the important other pillar of science in some sense, it is the falsifiability criteria. It's a fundamental premise, the falsifiability criteria. What it says is that no theory in science can be considered scientific unless it gives a way to disprove itself. So for example, if Newton says that F is equal to, just going back to high school physics, over R squared, there is a proof. You can do experiments and see whether the data agrees with this theory or not, this model or not. For example, if it turns out that it is M1, M2 over R cube, immediately you'll be able to use data to disprove that theory, right? So you need, science is that body of knowledge that is falsifiable. Anyway, that's a huge digression. Now I want to come bring that a little bit more down to it and say, and the famous statement of Box, the great mathematician of this space, but some are useful. Are we together? All models are wrong, but some are useful. So we stick with that. What we ask is the model that we built, does it serve the purpose? So for example, look at this data, the data set three, where the actual curve was, the truth was probably something like this. And suppose you build a theory, a linear theory, the question is, is it useful? That depends on the practical use case. How much accuracy of prediction do you want? For example, if you're trying to guesstimate the sale of ice cream on a beach, this model would be a pretty good model. But suppose you're trying to determine the trajectory of a spaceship in space, errors like this can be catastrophic. This theory is hopelessly perhaps, hopelessly inaccurate. So you just look at the usefulness of theory and that also gives you a way or a guideline of when to stop because you build a model and you build another model is better than the first one and you build another model and you can just go on infinitely because you don't know the truth the force that generated the data you can keep coming closer and closer to it by building better and better models and after a little while you may not be able to build anything better but should you keep trying all that the two things that stop you are practicality time budget how much time do you have to solve the problem and two how accurate how accurate do you really need to be so these should be your guiding principles if somebody gives you a project and says I must have a solution in one week, well your time box, the best you can do is go on making better models and at the end of the week you say well this is the best I have and that is your useful model so long as it has the accuracy that meets the business requirements or the practical requirements. Are we together guys? or the practical requirements? Are we together, guys? So that is something to remember. That's foundational to machine learning. So every hypothesis is a theory. For example, when you write an equation, y is equal to equation of a straight line, which I used to, a plus, what is it? In school that you learn to be, do you remember an equation like this, mx plus c, the equation of a straight line? Yes. Isn't it? So when you hypothesize that a straight line describes your data, then the problem results to just finding the m and the c. Isn't it? You find the m and the c, and you're? You find the M and the C and you're done. That is a good enough model now. And now the only question is, are you fine with that line and the predictions of the line or do you need something better than that? And you go and pursue that. So each hypothesis, if it is some function, that function has some knobs that you can turn. This is the slope, if you remember. This is your intercept, if you remember from school, of a line, right? So if you have a line like this, this part is C and the slope is, M is the slope, right, of a straight line. So you need to just find out any one hypothesis uh is it effective so we'll pursue that as a broad code a lot of the fields in humanities they can get pretty sure about the answers they have but science is the science is about knowing as little as possible not knowing as much as possible just know and say something that is absolutely sure of that it no evidence no data contradicts it right so that is the that is the pillar or that's a basis for scientific truth so by definition science tries to know as little as possible and we carry to the extent that we are doing data science we'll do the same thing we'll build models whose purpose will be to be effective and useful, not to hit upon the correct answer because the word correct is uncannily like the word truth with a capital T, right? So we don't pursue that at all in data science. Anyway, so moving forward now, what we will do is we'll do this business of regression. We'll bring these ideas to the ground. Our goal is that you have data. Let us say you have this data. I'll just put this data here. Your data set one. And let us say that you decide to fit a straight line to it. This is your x. This is your y. Now you have to fit a straight line in a plane. How many straight lines can you have? A bit louder please. How many straight lines can we draw in the plane? Infinite straight line we can draw. So I think it's three. Why infinite numbers? Infinite number of lines. Yes, actually, it turns out that infinite number is the correct answer because you can make a line here like this, like this, like this, like this, you know, here. Any one of these lines, of course, I've made little marks, they're infinitely long lines, so there are infinitely many, there are, isn't it and any one of these is a relationship between x and y isn't it so you could write any one of them as an equation and you can say maybe this line produced this data with a bit of noise in it noise added to it produce this data or fix this data, right? But the thing is you would be wrong, most likely you would be wrong. So for example, if you make a hypothesis like this, that this line, this is a hypothesis, would you say that this hypothesis, let me call it hypothesis A, is this a good hypothesis for the data? No. It isn't. And why is it not? What is your basic intuition that says this is not a good hypothesis for the data? Because less number of data belongs to this line. Less number of data. So suppose I make another model. Let me very carefully make a model. Carefully, it does not touch any data. You notice this line, it actually doesn't seem to touch any data i can cheat a little bit here i'll put it here and i will put here so let me call this hypothesis b uh b the line b even line b doesn't touch any data so if you had to choose between a and b which one would you choose we will choose b and why would you choose b uh because the data is more concentrated along with that line is also for infinite number of straight line i don't think there is a correct hypothesis because there are infinite number of lines possible here and i don't think there will be a one proper line in that case i mean hypothesis line okay so now uh what we need is a measurable way to tell if a line is better than the other. See, remember that we are not searching for the correct answer, minus a. We are searching for a model that is better, right? So think of it as the one-eyed amongst the blind. A model, so if you can say that B is somehow better than A, right, and perhaps let's take another one, which is this one. C is, what do you guys say about C? A C is better than other two because it is covering the more data yeah so the word that you want to use is not covering you want to say that which is the line such that most of the data is closest to it are we together yes the the word that you want to use is which is that line, if I may just write it here, which is that line, that line, let me just call it line hypothesis line model you can use hypothesis or whatever line model hypothesis that the data is closest to if you frame the question like this you would agree that the the answer between A, B, and C, how would you order it? How would you order the performance? You would say that C, would you say the best one? Which would you call the best one here? CBA. C. C. C is better. C. And then B. Then B. Then B. Then A. Yes, this is the best, right? This relatively is the worst. So this intuition is a very crucial intuition. Now we'll quantify that. What does it mean to say that the data is closest to? One way that you quantify that is by saying, let's look at any one of these. At any given value of x, let's say I'm looking at this value of x. Right? Or this value, some value of x, right? Maybe this value of x. So at this value of x, what is the data? Let's say the data is here. You can say that, just take a line and you have a point, X i, your data point X i, and this is X i, Y i, because this is your Y axis. So this is Y i, this is your X axis. So this point is this. What is the gap between this and this? Because your model is predicting this as y hat, right? Your line is saying at this value of xy, this is your prediction. This is how much ice cream you'll sell on the beach. This is called, there's a word for it. It is called residual error. In other words, it is a good measure of mistake, isn't it? Ideally, if this point was sitting upon this line, we would have loved it, isn't it? We would have preferred that line so that all the points sit upon it. But if you look at the data, there is no such line that exists. Because of noise, the data is scattered. It is not all on one line. So all you're looking for is not the non-existent line which fits all the data, but you're looking for that line that data is closest to. So the definition of closest is not being close, the distance between the line and the data point or between y the prediction so given xi reality is y i and prediction is y i hat the gap between these two r i is y minus y hat for any given data point by i means for any point any point of the data this is the residual error. So is this concept clear, guys? It's basic algebra, school algebra. We are saying that the gap between prediction and reality is the error. This one, focus on this. This is your residual. And this residual, now you have many have many many points so you have another point here another point here another point here so what will happen is you will have lot of residuals any for any given line you'll have r1 r2 suppose you have n points right so for every point so suppose you have x1 y1 x2 y2 you can find what the predictions are so what can you do you can use your line to equation of the line to predict your y hat 1 y hat 2. so do you notice that i'm putting a hat in machine learning there's a convention uh or sort of a practice or a tradition that whenever you make a prediction you represent it with a hat symbol so that everybody knows that this is not data this is the prediction from a model right y1 y2 so likewise x3 y3 y3 hat now the gap between these two is r1, R2, R3, and so forth. And suppose they are the N data points, YN, and you have a prediction here. So you'll have RN. Now you know that somehow you want that line which makes the least total mistakes. These are your errors. Is this idea clear, guys? These are your errors. Is this idea clear guys? This, this, these are your errors. Take a moment to think if it is not clear, ask. All right. So we know that these are the errors. Now the question is, how do we add up the errors? One obvious way could be, let us do this r1 plus r2 plus rn. Let us do this. Total r error could be this. Now is this a good idea or is there a problem with this? You could take average. Yeah you could average it if you want. You can divide it by n. It doesn't matter. But would you consider this a good way? So think that it can be positive or negative. Exactly. So what happens about a line, this and this, this much is positive plus three, right? And this much is positive plus three right and this much is minus three so what will happen to the error the total error will become zero that's not minus yeah plus and minus will cancel each other out right so so we realize that we cannot simply add it right because then what we really care for is the size of it. We don't care whether it's above or below the gap. We want to look at the size of this, not the sign, not the positivity or negativity of it. Then you can have a better idea and you can say, what if I did this, R1, you take the size of it plus R2 the size of it plus R3 the size of it and so on Rn the size of it now you'll be in good shape because this will become what will be the size of this size will be three and the size here will again be three right so? So now you're making, it's beginning to make more sense to that. Are we together? Okay. And so let's again divide it by n. So this one, it is actually valid. It is called the mean, the word for a synonym for average is mean, mean absolute error. Mean absolute error. Mean absolute error. And people sometimes use it, but it turns out that there is another way that you could think about it. And so by the way, do you realize what the symbol is when I put something between between two parallel bars it means the absolute value so for example what is the absolute value of two it is two what is the absolute value of minus four anybody four four exactly that is it you just look at the size of it you don't look at it so you can do it like that but but you can also think of a clever idea what if i square this r1 square plus r2 square all the way to rn square if you just square all of these and now you divide by n what will happen to negative square minus 3 square is plus 9. all of these squares will be positive because square of either a positive or a negative number is always positive. So you could have averaged the square of the numbers, right? And this is called, and this is M-A-E, this is called MSE. Mean squared error. Are we together mean squared error and in a more scientific sort of a common notation people use this symbol for summation summed over all values I to n RI square 1 over n like the total number of data points so this is your msc sorry no not this way let me let that n be there at the top uh the expression is here so does this make sense this is the this is the only bit of mathematics that we'll use guys uh for this workshop this is it so this much you realize we are squaring and adding it so now the question comes well now we have two ways of doing it actually if you think about it you can come up we have two ways of doing it. Actually, if you think about it, you can come up with even more ways of doing, sort of trying to say how much mistake a theory or a hypothesis or a model makes, right? You can take the fourth power or whatever it is. You can, once you have the absolute value, you can take other powers. So which is a better way to deal with it? It turns out that there was a guy, a mathematician named Carl Frederick, I hope I didn't mispronounce his name misspell his name gosh we all know him as cause in the world of mathematics he's called the Prince of mathematics so often he's given the designation Prince of mathematics a tremendous prodigy somewhat like Ramanujan, but in a different area. Ramanujan was in number theory and he was in continuous math and calculus and things like that. So he made a monumental contribution to many fields of mathematics. He was a child prodigy and it turns out that even when he was very young, he already had figured this out that this is the best way of doing it so there is a there's a bit of theory here uh it is called the blue theorem best linear unbiased estimator and so on and so forth and there's a gauss markov so there's a bit of mathematics here for those of you who are interested gauss markov Mathematics here for those of you who are interested, goes Markov theorem. So it basically says that of all the choices, this is the best one to take. Now, why is it the best one to take? The intuition is that, see, suppose a point makes a big mistake. Theory makes a big mistake, right so you don't want to you want to severely penalize it so when you take numbers one two three four five and you square it nine sixteen do you notice that the big ones this contributes a lot more than the small ones isn't it because exponential it's an exponential curve. It's like it's a quadratically growing curve. So when you take the penalties, the penalty from points that are far from the line is very, very severe. And it turns out that that is a good thing to do. You don't want to penalize for small mistakes because you know that small mistakes are coming from simply the noise in the data. But large mistakes are coming because maybe the model is incorrect. It's not listening to the data properly. So that's the basic idea that you want to go for a big mistake. So this is it, this concept. So the takeaway from this whole discussion is we have figured out now for regression, I'll just summarize where we came to, that this is a good way to quantify the mistakes, ri squared. Do we understand this formula, which in simple terms is just the average of r1 square plus r2 square plus rn square over n this is just a this is a more mathematical notation of saying exactly this so far so good but now we have one problem we want to pick that theory or that line or that hypothesis which gives us the least msc now right that would be our definition of uh the line that the data is closest are we together guys now we have a way to say which of the lines to pick among so if we go back up here and i look at a hypothesis one two three would you agree that clearly the residuals to the third one is very small look at this yellow residuals they are much smaller to for example hypothesis a if you look at hypothesis a the residuals are how big residuals are like this big do you get the sense that a huge well uh i sort of view the lines crooked, so I should draw them straight. This is here. Well, again, I'm drawing it crooked. But OK, there's this. There's this. Do you see how big the residuals are? Right. And even in this direction, this big, this big, this big. This is getting better. This one is huge and huge and huge so you can then say that now we have captured our intuition into a simple mathematical expression would that be a fair statement guys did you see that the line the the sticky things coming out of the yellow line, those errors are much smaller than the sticky lines coming out of the blue line. The errors from the blue line, isn't it? So you say therefore that C is a better hypothesis, a better model for this data than A is. So and we have quantified that using this expression, right? Mean squared error. Just again to remind you, squared error. It is the mean squared error. Sometimes now there are some variations of it. Some textbooks will not divide by n. See, if you are reducing the error, it doesn't matter whether you divide by n or not. It's just a constant. So some people talk about not MSE, but they talk about the sum squared error, which is just adding up all the points and ri squared. So they don't bother to divide by n. So you'll see this. Now, the textbook that I suggested to you, it uses another term. uses the term rss which is exactly the same it stands for residuals sum squared so let's try to understand it what are you doing you have you're first squaring it and then summing it all up right do you see this square and sum or maybe squared sum i don't know uh squared sum whichever we think about it this is what your book uses as the as the word but they all come to the same definition we need to pick one. So let us pick this, because this, in most of the modern textbooks now in machine learning, they tend to go with MSE. But remember, this is not the only definition. You could go with mean absolute error. You can, and as you will learn, if you get deeper into machine learning, that this is actually just the beginning of ways of quantifying errors. Now you will also see sometimes a word used loss function. Loss function is a powerful concept. At this moment while you're still learning it think of it as something like it is the mean squared error plus something. Plus something, but we won't worry about that something. So when we use the word loss function, just think of the mean squared error. So now we are in a nice place. All we need to do is draw lots and lots of lines in the plane. And for each one of them, calculate the mean squared error. Now that, then suppose we draw a thousand lines and then we pick the best one that we can then declare to be our best model is that right guys so amongst the thousand lines whichever line has the least mean squared error would you agree that that is the best amongst the rest, best amongst the thousand? Anyone? Yes. Yes, it is the best amongst thousand. But there is one problem. The mathematician may object and say, well, it may be the best amongst thousand, but it's somewhat like a one-eyed amongst the blind. How do you know that this is the best model and there isn't anything better that you can search? Also, the other objection to this is blindly, randomly drawing lines and then picking the best. What if all thousand lines that you drew on the page were absolutely lousy? Not one of them was close to the right, well not the right answer, but effective model. close to the right to well not the right answer but effective model right it is possible so now you need a way to somehow find the best line somehow find the yellow line right and the way it is done is very elegant and is the beginning of machine learning this idea is came to of all people a person so the way I heard the story, which I don't know how true it is, but it's certainly told in the community by many people. It goes like this. Goss, apparently, he was a child of 10 or 11 years old. And then one of his more senior students, somebody many years older than him, I think some cousin, came to him and said, can you help me solve this problem with chemical reactions? And as Goss was trying to figure out how to help his cousin, a friend, I don't know which, he in the process, just as a byproduct, he ended up discovering an idea that I'm going to talk about now. That idea is, well we'll talk about that in a moment, it's called gradient descent he came upon that and it was a remarkable idea but he didn't at that moment think a lot about it later on there was a challenge in europe a grand challenge in which you were supposed to find the location where a certain planetoid or comet or something, some astronomical object would show up in the sky. Apparently, it had gone behind the sun and it was going to resurface somewhere in the sky on some day. And so there was a grand challenge for mathematicians and physicists and astronomers to predict where in the sky and on what date it would show up. And people submitted the results. It turns out that there was only one person who came to the result with uncanny accuracy and that person was Goss. His results were so far superior to other results that people immediately realized that there's something really very interesting going on. How could you tell, you know, if you look at the sky, it's vast. Lie down and just look at the sky. It's vast. To pinpoint a location fairly accurately and to even be able to tell what day or time is quite a feat. And yet, Gauss pulled it off. And he wrote a paper which was in 1793, in which he talks about the planetary aspect, and he alludes to this method of, so actually he used, not gradient descent, the method of least squares. He calls it the method of least squares. of least squares. The fact that you should use the principle of least squares to solve the problem. He came upon that and then he had a way to solve it, which we'll talk about. So this is it. He said that you, you need to minimize this. And if you can minimize this, you can do that. But he didn't use the word method of least square. Another mathematician in 1805, Legendre, some of you may have heard about in Legendre of the Legendre polynomial fame. He discovered and rediscovered this thing that the way to solve problems is to use the least square, method of least square or you know minimize msc the the some the mean squared errors somehow minimize that and you'll come upon the solution so when he wrote that paper the the title of his paper was quite literally method of least squares and then gosh jumped in and says now wait a minute i discovered this before you and so forth and so ultimately this debate actually raised on for 200 years. This idea is so profound in science that there was a lot of squabbling over it for 200 years. And now we know that both of them independently discovered it. And in fact, there was one more person who independently discovered it. At least one more person, there may have been other people. So it's a little bit like, you know, the idea of who discovered the radio. Was it Marconi? Was it Jesse Bose? Was it this? Was it that? And then if you do a lot of research, you find that actually the time was ripe for a lot of people to sort of co-discover it. A little space in time, a little bit here and there, and then let the historians figure out amongst that little interval who was first that sort of a situation so that happened and so what is the idea that you can do of MSC, mean squared error. Isn't it? That is why it is called least squares, because these are squares, squared errors, least squared errors. So there are many ways of doing it. So I will motivate that today with a little example which is that we will do a small game. Suppose I give you a function. A function. Imagine that this is a function here. In mathematics, in basic mathematics that you may have learned, there is a concept of a slope. What is the slope? Would somebody like to enlighten me? What is the slope of a function? So for example, if you have a table against the wall or any plank against the wall, what do you mean by the slope or the hills slope of a hill? Slope would you say is a measure of? Mass into acceleration, sir. No, no, no. Ste steep steepness how steep so suppose you're looking at a hill let's take two hills here is a hill and here is another hill a b now which is more steep climbing which is more steep b right. Right, a B right. So you would say that suppose you are here, it is far more steep to climb B. So you would say that B has at this point, slope of B is greater than the slope let's say here at this point. This is much more gentle or even if you take it even more gentle, see, you would agree that this is like if you're climbing this hill, then even perhaps old people like me can climb that hill. But this is meant for young people, the B. So slope is just a measure. If you quantify in some number how steep a hill is, that number is called the slope. And it is very easy to quantify. The quantification is how many for unit movement in positive x direction. So positive x direction is this direction. So suppose you make one unit step, right? How high did you go? Right? So let us agree that a unit step is this much. So if you look at B, you have risen this this much right you have gone this distance in B, whereas for C, you start here and you go the same distance, and you notice that this is much smaller for a you go the same distance, and it is this much, but you agree that if you do the same thing for let me exaggerate this a little bit for this right at this distance that b is the steepest because you have climbed the most in v so slope is defined as climb per unit forward movement how much did you climb that's a definition and slope is basically the idea in then you build a little bit of mathematics simple mathematics around it and you also call it the derivative the slope of a function a function it is often also called derivative you may have heard the word derivative which looks rather scary but that is all it means, derivative of a function. Slope of function is also called derivative. And people have a bit of notation that you may remember, df dx, right, of a function, right? Sometimes they write it as f prime and so forth. There's many notations associated with derivatives and so on and so forth. But at the end of the day, actually, geometrically, it's a very, very simple idea. It just basically says, if you move one unit step in the forward direction, how much did you climb? So you would agree that at this point, you climb a positive distance, isn't it? if this is the plus y axis positive distance what about this point at this point if you make once one positive step do you how much did you climb at this point let me call it t you make one step positive from there here how much did you climb in height say it is distance traveled by time taken distance travel quite time distance don't don't bring up formulas from high school think through it think through it i'm saying that you have moved from this location to this location. Then you were on this hill here. If you move one step forward, the only way you can do it is you can go this way, right? And down this hill. So how much have you climbed? Actually, you have not climbed at all. You have come down, isn't it it you have lost height would you agree you haven't climbed you have lost height does that make sense guys so suppose you go from p to r to the base back from p to r would you agree that you have not climbed any further up the hill you have just come down the hill so in other words you say that your slope here is negative slope is negative because if you try to move in the forward direction you'll end up coming down the hill whereas at a point let me just call it q at q if you try to move one step forward basically here the only way you can do it is climb up the hill you would have actually climbed a distance up the hill right and so that is it that is all that there is to this calculus business the differential calculus business and there's more the rest of it is the mechanics of it but the basic idea of calculus is exactly that so the slope is positive when you are genuinely climbing is negative is negative when you are losing height. Are we together? So now let's look at this picture and tell, suppose I take two points. Let me call this P and let me call this point Q. And the point up here is the function of P and the function of Q. This point. Right? At point Q, and therefore when you are here, if you move one step forward, will you gain height or lose height? Look at this. You would gain height, right? You are gaining height and so you would say that at q the slope is positive in other words the df dx is greater than zero what about q at q at p sorry what about p if you make one step further you would go from this point to this point right you're coming down here have you gained height or lost height lost the height so what would you say about the slope at p negative at p the slope is so the question is suppose you are at some point and you want to go home you want to go to the point the bottom of the valley right bottom of bottom of this pole you want to go to this point let me call this home you want to go home if you are at P what you want to do is you want to take when you take a small step what is the new value of P? So this is a let me just call it FP. P plus this little step delta x. What is it? You have gone in the right direction. So what you want to do is you want to take, so this is sort of, I'm not getting into the whole calculus. At p, you would say that whatever the value of the function is or the position is, x, suppose x is p, then take a, take a step in the forward direction. Right? So you can say that the next value of x should be your previous value, which is p minus d at that point, function of p, dx. That is minus the slope. Actually, why am I even writing it? Let me just write it as slope minus. Now think about it, what will happen if I subtract minus slope of p, slope at p. Now you know that slope at p is less than zero, right, negative. So when from x you subtract a negative quantity, what will you get? So it will be let us say that it is let us say for sake of argument this is equal to 0.5 minus 0.5 so x minus minus 0.5 is equal to x plus 0.5 so So it means your next value is actually a little bit more in the positive direction, right? You have gone from here to here, right? This is your next point. Let me call it P prime. And so are you moving towards home? When you go from P to P prime, are you getting closer to home yes yes right so you want to keep making steps like this but what about q do you want to make a positive step or the negative step negative yeah so what you want to do but the slope is positive so you want to make us if you want to make a step like this, let's think about it at Q. At Q, you would say the next value should be the value Q, basically. I'll write it as X because X there, but you know that it's Q. Minus, you know that the slope is positive. Slope Q is greater than zero. So now suppose you say minus slope Q, then what happens? Suppose the slope is 0.5, let's say. Now this becomes X minus minus 0.5, isn't it? Would you agree? It becomes negative. So a negative means you're going in which direction forward or backwards? You're making steps backward. So when you're making step backwards, are you going home? Yes, we are going home. And so we come to the conclusion that whether you're on this side or this side, wherever you are, one easy rule you can do to move forward is you can say x next is equal to the x, whatever value of x you are at that point, whether it's p or q. You realize that it's the same equation, whether p or q, minus the slope at x slope at x, right? Now what happens is that you can take this, but you can multiply it by, for example, you may choose how big a step you take. Typically, how big a step you take, you multiply the slope by some number. So for example, you could subtract 0.5. You could subtract 0.2. Divided by 2, it would still be the same uh same step so suppose alpha is equal to half you you need not take a big step you may take only half a step half of half or even smaller alpha could be 0.1 and so forth so this is called in machine learning there's a important this is a very important term it is called the learning rate there's a important this is a very important term it is called the learning rate right it controls the speed at which you are going home right how big a step you are taking towards home right and this is a famous equation and the heart of machine learning actually what we just did in a few steps have derived in many ways the foundational idea that is that is needed in machine learning in fact this this has a name to it it is called gradient descent gradient descent why gradient means you're going you're going against the slope rising slope right descent you realize that when you move home you're going like this you're descending down the hill right in the f's the function is becoming smaller and smaller and smaller irrespective of whether you were at P or Q so you're coming down Right. That is why it's called the gradient descent step gradient descent method and This is it in machine learning you have fulfilled two tasks. Remember I said in machine learning quantify the errors Because errors is a measure of performance. Less errors is better performance. The errors. So we did that. How did we do that? MSE is equal to 1 over N sum of the residual square. And then the other thing is decrease the error. So now we have gradient descent to decrease somehow gradient descent we are going to use to decrease the error and I'll explain to you how or maybe it doesn't matter. So this will this is your rule that to do this you have the gradient descent rule for anything says x next is x minus learning rate times the slope at x, slope of some function of x, right? And so that is it. These two things are actually the foundation of machine learning. And to a large extent, it captures what machine learning is all about. So if you have understood this little bit of mathematics that I went through, you have pretty much understood what machine learning is all about. So, you know, I'll post this video and then please review this and try to explain it to you in as simple a way as I could. This is all you need to know in machine learning at this moment, at least until you become deep experts at it. And even then, the same thing will remain true more or less, at least for regression. You just make it a little bit more complicated as you go ahead, but this will serve just understanding that so now the question is gradient descent how do we do gradient descent for data and find the best line there is only one last piece of the puzzle that we have to bring which is to say that suppose you have a data space x y and suppose this is your data so So in this space, you can have many hypotheses. You could have this hypothesis. You could have this hypothesis. You could have this hypothesis. You could have this hypothesis, this hypothesis, whatever it is. There are many, many hypotheses that you have. Now, one bit of convention. See, when you learn about the equation of a straight line in school, you write it as y is equal to mx plus c or b, whatever it is, intercept. In machine learning, what happens is, there are a lot of symbols flying around. So people have agreed on a convention. The convention is Greek versus Roman. is Greek versus Roman. So the Roman letters are, the English alphabet is Roman. A, B, C, this is Roman. This is the Roman alphabet from Europe. Greek would be alpha, beta, gamma, delta, epsilon, theta, and so forth. These are Greek letters. So in machine learning, for no rhyme or reason, you need a convention to be able to tell apart what is data and what is your model. We use a rule. parts of model always have Greek notation. Well,. Often, x, y, x, y is used to do that. And the third, so this is, these are the conventions. And the last thing is predictions wear a hat. Y hat. So the predicted value always wears a hat. You see this big hat here? This is it. So follow these three conventions. So when you follow these conventions, you realize that this will not do because this is all made up out of Roman letters. So what you typically do is you replace this with beta 1 and this and staying with the convention of your book you replace it with beta 0. so the new way of writing a line would be y is equal to beta naught plus beta 1 x and that's the only other thing you need to remember right now when you look back at the data space, there are many, many lines possible. Each line will have its own slope and intercept. So each line will have its own slope and intercept, beta naught, beta 1. So this is common right? Every line has a different slope, different intercept. Now what happens is each line is a model. So you think now this is the data space x, y. I call it the data space. And now let's go to the another space, magical space, which we hidden space, which you call the another space magical space which we hidden space which you call the hypothesis space hypothesis space right in the hypothesis space see any one line let's say a b c d right all of these lines are what they suppose is is coordinates are beta one beta not beta one. So you realize that each line, each point here is a unique value of beta not beta one. And so it represents some line, some particular line in the data space. And this is a crucial observation actually. There is a relationship between data space and hypothesis space. In the data space, you see data. In the hypothesis space, each point represents a hypothesis, a possible solution to your problem. And to each point, you realize that so long as each line is given by beta naught beta one and associated with this is a mse because there is a there is a there is a mean squared error that is associated with this line right and so it turns out that you can do this business, and I won't go into all the details, but basically in this space of beta naught, beta 1, and this error, MSE error, this three-coordinate, three-dimensional system, the error surface, the MSE is a surface. system. The error surface, the MSE is a surface. And we won't go into all of this because at this moment, you're just getting started. So take it on faith that what happens is that you do this gradient descent business. You take any random point, any random line. And then what you do is you can find a way home. This is just a two-dimensional version of what we did in one dimension you can find a way home just by using the gradient descent rule right and the generalization of a slope to higher dimensions two dimensions and higher what is slope in 1d one dimension you know becomes for a line becomes the word gradient gradient in higher dimension and i won't go into the gradient the symbol for the gradient is an inverted triangle right there's nothing magical about it it is just a generalization of slope to so when you are on a hill you may have noticed that if you move in one direction, how steep it is may be different from how steep it is in the other direction, a perpendicular direction. And that's all it is. You need more than one slope. You need slope in different directions to quantify how steep it is at that point. Right? And that's all it is. So we won't get into that. But the basic idea is that you do gradient descent in the in the hypothesis space. And people don't use the word hypothesis they use the word parameter space because beta one beta two are parameters, the word to the slope interceptor the parameters of the line, they point, this point, this special point, beta naught, beta one, let me put, mark it with a star, this is your best solution, beta naught, beta one, star, star. It represents the point at which you make the least error, lowest MSE, lowest errors, and so that is what you're searching for. It's the home, the final solution. So what you say that if you believe that straight lines may work, you're in search of the best straight line. And the shortest way to get to the best straight line is to start from any straight line and take the path from there to a better and a better straight line till you get to the best straight line. And this is your shortest path that you can take. Are we together guys? And that is gradient descent. So now this is better because this is provably the best solution. And also you have the fastest way to reach it, right? That is why in machine learning, we consider gradient descent to be the heart of machine learning. It is the learning in the machine learning. Are we together? Once you have quantified the error, the way to reduce that error is through gradient descent. Which is it? So guys, I've taken you a little bit deeper today into the mathematics and this might look frightening, but actually the good news is frightening but actually the good news is that is all there is to machine learning right if you get this idea you have pretty much gotten everything right at least as far as regression is concerned in classification other things are small variations of this big idea right that you quantify the error and then you do gradient descent to find the best solution. Are we together guys? Right. So when we used in the lab all of these methods, linear regression, random forest, all of them, what they were doing is they are in effect building models and trying and using some form of gradient descent to find the lowest the model with the lowest error quickly are we together guys are you following me everybody is very quiet are you guys following me yes yes yes yes and you know this is a if you can get this journey, really, the core ideas of machine learning are now with you. And you can, so what I would suggest is go and read up to chapter three properly. Now when you read that chapter you will see that the same thing that I'm saying is said, but a little bit more formal language, you more sort of a heavy notation but it is the same thing that i'm trying to say here in simple terms it is there now when you do this there is one bit of extra thing we need to learn here it is called the bias variance trade off variance. I'll talk a little bit about this. What is bias? So let us say that you're throwing in suppose you have a dartboard. You know your target practice, you're doing some target practice. And you're throwing darts at it. You want to ideally hit here, but let's say that your darts are falling like this, right? All the darts that you're throwing, they're falling like this. What do you observe about these darts the person is like more towards to the I mean something like yeah so basically you say that the average of these values right there is the it is shifted right it is shifted, right? It is shifted. You almost seem to be aiming for a bull's eye here rather than in the center of the board, isn't it? Your hand somehow is thinking that it needs to hit here. Are we together? So when all your predictions are shifted from the right answer in one particular direction, this thing you call bias. You say that you have a bias. It could be, for example, if you're taking temperature, you have a thermometer. A good thermometer will give everybody's temperature as let's say 37 degrees centigrade. will give everybody's temperature as let's say 37 degrees centigrade right but your thermometer and so if somebody has fever will be 38 and so forth but your thermometer gives you 27 degrees 28 degrees and so forth but it is accurate in the sense that the guy who has fever will will be offset from 27 rather than 37 the deviation would be a measure of the correct temperature all right but it would be offset there's there's a big offset the base is wrong do you see what i mean the base is wrong so that is an example of bias your your instrument has developed a bias and you have to unbias it. You have to recalibrate it. Are we together? Another example is, suppose you stand on the weighing scale, right? And this is, I suppose, common experience. I always wish that whenever I stand on the scale, the number seems to be bigger than what I want to see. So I keep hoping that the scale is wrong. It is adding a few, let's say 10 kilos to my weight rather than I have actually, I'm actually 10 kgs heavier, right? So what is that? But suppose the scale was actually doing that, like whatever your your weight it was adding 10 kg to it then all your readings every child whether it's a child it's your family everybody's weight would be 10 kgs off do you see that right so that's an example of bias another example of bias that you see is in your watch suppose your watch always shows five minutes ahead, is either five minutes ahead or five minutes behind. It is biased, right? You need to recalibrate it. Those are biased errors. So sometimes what happens is when you're trying to shoot this point, but you end up hitting this, these sort of mistakes are biased errors. Now there is another kind of mistake which is called variance errors. So suppose you think of this bullseye again, right? Do you know what I'm talking about? A dart board or a target board or bullseye? This is the bullseye, right? it this is where you want to hit and suppose you hit like this now what do you notice what can you say about your crosses But would you consider a player whose shots are like this to be really an expert or a champion level player at dartboards? No. No. No. What you want is something like this. If you want to say this, what you really want is a guy who hits like this. Something like this, isn't it? So this is the best. Or the word that you use is optimal. This is high biases. But here, do you notice that the way he hits varies all over the place? Sometimes he's hitting to the right, sometimes to the left, sometimes above, sometimes below. But he's not really hitting the bullseye, right? So you see that there's a lot of variance in the variance in a very direct way you see the thing varying all over the place right and there's a and to capture the spread there is a mathematical term variance right it is a measure of how spread out things are and so here the the dart the those darts have hit over a pretty large area. So you see that this sort of a thing, this player is suffering from variance errors. Are we together? Now let's bring this idea. So now what happens is you have bias errors, variance errors and this is the optimal solution, right? And now it turns out that when you build a machine learning model, you may have the same problem. So suppose your data goes like this, but your model goes consistently like this, it's far off, it's spread out, it has a bias or something like that. You say that, so maybe I'll do this, something like that that it has a bias error on the other hand if it makes predictions so if you if your model is like this so your predictions are wildly off you know all over the place for different values you're making you know such a very various predictions then you say that your model has variance errors now there is a problem though when whenever you build a model remember i said that models are never correct there's no such thing as a correct model they're always wrong but what you try to do is be be wrong as less as possible minimize the error. But no matter what you do, any model that you build, any model, errors in the model, will be made up of two, three parts. It will contain bias errors. So to some extent, your answers will always be slightly shifted. A bias, and for reasons that is mathematical, we'll talk about this. So there is a variance errors and then there is the irreducible error. Ganesha, And this you can take at this moment because you're just learning now as a fact of life. Bias what happens is for a little bit of a technical reason that the formula for bias is such that people should have it is actually bias square. Right, but ignore this square is basically a bias errors, variances and irreducible errors. So we talked about bias errors, we talked about variance errors. Now what in the world is this irreducible error? Irreducible error comes from the noise in the data. You notice that your data is spread. It is not along the line data is all over this place. And even when you do a best fit line right you still are left with these little errors so this thing comes from noise in data and it also comes from that which you don't know That which you don't know, your model does not know. So it's very simple, guys. For example, take a situation. Whenever you make a prediction or you're doing this, suppose you're trying to determine, is it going to rain tomorrow and you're determining it based on the fact that it is the month of let us say April or May right it's pretty hot and sunny it hasn't rained for a few days it's like this is not the season for rain all of those factors may convince you that it's not going to rain tomorrow or the total amount of rainfall tomorrow is close to zero that is it but what my model didn't tell you is that there is a storm coming just now right and is going to bring a lot of rainfall with it so now what happens is there is something that your model does not know about, namely the storm and its effects. Or let's go back to the example of the ice cream. Suppose the entrepreneur is making a model, a one-dimensional model, in which he's just looking at or she's just looking at the amount of ice cream sold based on the temperature of the day, the average temperature of the day. A pretty good model, but there is a problem. For the same temperature, you would sell more ice creams on a holiday than on a working day, isn't it? Because on a holiday, children will be there and parents and children are both free. They'll come to the beach. They'll all have fun. Whereas on a working day, parents will be working and children will be in the school so you will have less traffic during working days so what happens is if you build a model of sale of ice cream simply based on temperature for a given temperature you will see a variation a wide band of results isn't it based different days. So this irreducible error captures that fact. It captures the information that is there, which your model is not aware of. Are we together? Now without, now I'll just mention something. See, when you build a model, you can build a simple model. Let's say there is a thing called polynomials, polynomial function. So look at a function. Y is equal to just a beta naught. So it will be supposed this is beta naught intercept. It is a straight line. be suppose this is beta naught intercept it is a straight line flat line horizontal line right then this is you call it a polynomial of degree 0. now take another equation which is beta naught plus beta 1 x this is the equation of a line beta 1 x this is the equation of a line this is a polynomial of degree 1 because x the highest power of x is 1 this will be some some line right y is equal to beta naught plus beta 1 x now you can go to another you can go on making it more complicated. Beta naught plus beta one x plus beta two x squared. And this is a degree two. And this will look go to degree five. One interesting thing that happens is the quadratic has one bend. The line has zero bend. What happens is if you take a fifth degree polynomial, you'll end up with four bends bends so if you make a model that has four bends it will look something like let's say it will look something like this one two three four right so it is this one right our fifth degree polynomial equation may look something like this so So now comes an interesting question. Suppose your data is like this, like your dataset two, the one that I gave you guys, regressor two, regression two. Second dataset, it was like this. If you build a model like this this let's say the first model is like this you realize that the line is too simple then suppose you make a quadratic so this is line order one then you make a quadratic it will go something like this better than the line perhaps but still this is the order two if you make it of order three with three bands you'll probably get it right right three and then let's go to five or something like that a four you suppose you go to two three four four so of all of these this is the fourth one of all of these models polynomial as models for the data which do you think is the best just looking at this picture This is the best because it is closest to the data. But if you think about the line and the quadratic, what you see is it doesn't have enough number of bends to fit this data. And the fourth one, the fifth degree polynomial has too many bends right so the way you say it in machine learning is and this is a very important concept model one two are too simple model four is too complex compared to the ground truth the ground truth is ground truth ground truth data namely the data that you see favors model three right three, right? Three. So if you happen to take model three, you would get a really good model. Isn't it? So you say that in the language of machine learning, you say that these two, they suffer from bias errors. High bias errors. And why? There's a bit of mathematics and so on. So if it doesn't matter, take this as a fact that they will have high bias errors and why there's a bit of mathematics and so on so if it doesn't matter take this as a fact that they will have high bias errors you remember i explained what bias errors is when you when you're offset when you hit the target off the mark in one location right so models simpler than the reality will tend to have high bias errors and models more complex than reality will have high variance errors. And so what you need is a model which the data favors in which the bias and variance errors, they are mostly as low as possible. So the question is, how do you do that? What you do is that you take your data, suppose you have this data, you split it in the training set and the test set. Ultimately you train the model here, but you test it on this data and calculate the data that you're going to use. And then you split it in the training set. And then you split it in the test set. And then you split it in the test set. Ultimately you train the model here, but you test it on this data and compute the test errors, the mistakes, test error site, MSE on the test data. And then you pick that model, which has the lowest MSE. And when you do that, there is an interesting observation that you have. See, what will happen is on test data, on test data, if you look at the bias and variance errors as you make more and more complex models, think of it as degrees of the polynomial, higher degrees polynomial, then the bias errors will go like this. The bias errors, the more complex model you make, decrease with increased complexity. This accesses errors, right? This is bias errors. Then the variance errors, on the other hand, tend to increase. on the other hand tend to increase right this is the variant sentence errors increase with complexity now remember i said that the total error, just to recap, total error is made up of three parts, bias errors, variance errors, and plus irreducible errors. So irreducible error, you can't do anything about. So irreducible error comes from noise in the data, from not having the right variables, or even not having the right hypothesis. Suppose your hypothesis is completely off, you'll have that. So anyway, let's leave that aside. Now, to the extent that your total error is made up of a sum of these two, your total error, therefore, is the sum of these two at least and some irreducible error. You would agree that the sum total of the yellow and the red is the green line, right? A bias plus variance. A bias plus variance. And so what happens is that there is an optimal level of complexity at which the error is minimum. On the left of it, in this direction, you have high bias errors. Do you notice that in this direction, sorry, this yellow, this green line I wanted to pass through, and so this direction, right? And let's do that. I'm being a little bit sloppy. Okay, so in this direction, if you take a model simpler than that, do you notice that the yellow line is high. And what about this direction when you're in this direction. What do you notice which error is high. Variance variance. Hi. Variance there is and this phenomenon is called, there's a name, there's a nice term for this observation of this phenomenon. It's called the bias variance trade-off. It's a very important term. And by the way, it's a very favorite question in data science interviews, variance trade-off. Now there's more subtleties to it. At this moment, we're just starting to learn. It's good to think in terms of bias variance trade-off. But we now know actually that you can build models so bad that it has both high bias errors and high variance errors. But let's not go there. Let's keep life simple. And so this is your bias variance tradeoff. Now, when you have high variance errors, when your model is very flexible, you have something called overfitting to data. If you have to data if you have complex models. What happens is suppose I give you this data point and you think it's a line, but if your data has gone, if your model has gone and it has drawn something like this. Sorry, something like this. Let me move this point to here. Do you notice that this is a very complex and flexible model and it has gone and fit to the data? But something tells you that this model is unnecessarily complicated. You could have gotten away with this model. Are we together? You prefer the yellow model because it's a simpler model to the very complicated model. So just as a small allusion, this refers to a principle in philosophy and foundational to science. So science you ask this question, what is correct? You know, I told you that we don't have a truth or a correct as a God-given concept, right? In mathematics you can say 2 plus 2 is 4, that is correct. 2 plus 2 is not 7, that is incorrect. But in science what do you do? And so there is a definition of correctness in some sense that comes we say that if you have many effective models many theories that work or solutions that may work or hypothesis that may work the definition of correct is the simplest of them so whatever is the simplest explanation that works is the correct explanation. And that is called the Occam's Razor principle. So it's a principle. Occam's razor. It's a term in scientific philosophy that says that the definition of correct or true in science is that explanation which works and is the simplest of all other explanations. So an example would be, you can say that, why is the earth going around the sun? So there are many explanations. One simple explanation is, for example, in medieval Europe, they used to believe that a whole bunch of angels are pushing the earth around the sun. Well, they're not pushing the earth, but they're pushing the sun around the earth. They make the sun rise and make the sun set. But then it gets complicated. What about stars? What about things? And you know, they all seem to have very peculiar cycles. These are called epicycles in their orbit. Very complicated orbits and then comes a person like copernicus and kepler and what they do is they say no no wait a minute all of these explanations and calculations become tremendously simple if instead you put like as kepler said you make an ellipse and you put the Sun at one of the two focal points of the ellipse and so every planet's orbit is around, is an ellipse such that the Sun is one of the focal points. So the Sun is stationary, the planets are moving relative to the Sun. So that is the Copernicus and the Kepler model of the planetary system. Now when we say it is right, what do we mean by it is right or is correct? Actually, as opposed to putting the Earth in the center and making the Sun and the stars move around it. The old idea was that this was the earth, this is the sun, and then you have the sun going around and then all sorts of stars doing like this, planets doing like this. They had all peculiar orbits. Very, very complicated way to compute those orbits. Those were the epicycles. And then suddenly you have a theory that's very simple and it works. So you say that actually we prefer the Kepler model or the Copernicus model as the correct model and the old model of Ptolemy, this was supposed by Ptolemy and other astronomers, which was dominantly true for thousands of years is actually not the correct explanation but you know today if you really look at it and ask everything is moving there is no nothing stationary right the space time itself is a dynamic concept there is no such concept of any one thing being the center of the universe and being fixed and everything moves relative to that there is nothing that's the whole theory of relativity right but everything is moving relative to everything else there is no fixed center that is just stopped there if that is true well the sun itself is moving in the galaxy the galaxy itself is moving in the galaxy. The galaxy itself is moving in the universe. So what do we mean by saying earth is moving around the sun? And that's a better answer than the sun moving around the earth. It turns out that, see, in relative motion, you can take either of these two theories. But if you take the old theory of making the earth stationary and all the other things move around the earth your explanations are very very complicated and your calculations are very complicated your models are very complicated on the other hand if you put the Sun in the center and make the planets go around it or the solar systems planets go around it and then the whole plane of the Sun and its planets together moving in the galaxy much simpler model and you can explain most of the data and its planets together moving in the galaxy, much simpler model and you can explain most of the data very simply with the effective accurate model. And that is the meaning of correct. It's not that the other model was incorrect. That too can work. Actually somebody showed very elegantly that you can make the Ptolemy model work. You just have to deal with complicated Fourier series. But whereas in the Copernicus model or the Kepler model, life is much simpler. You don't need complicated models. Calculations are simple. So you call it the correct. So anyway, this is a little bit of a digression today into more the foundations, or the philosophical foundations of science itself, it is worth keeping in mind because data science is science and so it must be imbued with the spirit of science. And so remember that models are important. They are theories about the data, about the forces that generated the data, right? And as such you have to be careful. You should be very careful in designing your models and searching for the best model. And never get carried away by saying, I have the correct model. There is no such thing as correct. So that is that, guys, for today. We have been talking a lot. I have covered everything that there is in chapter three. A couple of minor things I didn't cover, like, for example, this topic of collinearity and so on and so forth. We will do that maybe in the coming months. I usually do it when I give the more extensive workshop here at Support Vectors. And people stay with me for three months just to do what I'm giving you in a very condensed way. They do it slowly and they do a lot of labs and a lot of practice. So obviously we are not doing that. We are doing a much more compacted and fast-paced version of it. So I'll summarize what did we learn today. We learned about these ideas regression. We reviewed what we have, what is predictive modeling or supervised learning, what is unsupervised learning, pattern recognition, reinforcement learning. We learned about regression, it produces a number, whatever the input. We also went on to a digression saying that we are not, science is not about the truth, the capital T, it's about practicality, it's about effective models. Nobody in his right mind says, I am in search of the truth with a capital T. You're in search of explanatory models, things that explain the physical universe around you and the data that you see in front of you through your sense organs, through instruments and through whatever. And then you try to create theories, models to explain the data. So the famous statement of Box is, models are wrong, but some are useful. So we are in the pursuit of useful models. I also said, Occam's Razor Principle says that of all the useful models, pick the simplest one. That is what we define in science as the operational definition of correct. So we talked about regression. We got introduced to the concept of residuals and residual errors. We realized that you could do mean absolute error, or you could do mean squared error. And it turns out that Gauss and a bunch of other people managed to prove the Gauss-Markov theorem that, of all of these situations, the MSE, the mean squared error, is the best. It's the one you want to take. Now this error is represented by different names in different books. MSE, some squared error, just error E. So what is the preferred notation that I use? I actually don't like all these three letter symbols. I just use the word E. You'll often see me use just the letter E for error. E for error. Then, but your textbook uses RSS, residual sum squared. So then we learn the concept of slope or derivative of a function. And slope is nothing but how much height or altitude you gain if your function were a hill at a given point. If you move forward, how much altitude or height would you gain? That's all it is. And that is the whole idea of calculus is summed up differential calculus is gist of it is captured in that one sentence. And so there's this concept of gradient descent, which basically means that a go against the slope at a given point if you want to go home. In other words, if you want to go to the minima of a function, slowly go against the direction of the gradient or the slope. Generalization of the slope to higher dimension is called gradient. Generalization of the slope to higher dimension is called gradient. Think of it as a, you know, just common sense. If you're on a hill, you're on a slope. In one direction, the slope may not be so high. If you move north-south, the slope may not be so high. If you move east-west, the slope may be very high. Sometimes look at the picture of Mount Everest and you'll see that. Sometimes look at the picture of Mount Everest and you'll see that. So suppose this is the Mount Everest. Those of you who have remembered it, there is one thing and one side is very steep. But on this edge, typically people move up and go to Mount Everest because here it is not. Whereas if you try to do this here, this is literally like vertical walls in this direction. You don't want to fall down that wall. So that just brings hope the concept that slopes are different in different directions and the sum total information of slope in different directions put together is the gradient. Right? So you do gradient descent. We talked about the fact that in science, in machine learning, in particular, in data science, we tend to use Roman letters for data, Greek letters for models and pieces of the model. We call them parameters, parameters of the model, or parameters of the hypothesis. Then, and we make predictions that our models make where they're at. That's that. So in terms of Greek notation, now the simple equation of a line mx plus c is rewritten as beta naught plus beta one x, betas being the Greek letters. And then we have the concept of gradient descent, which basically says that in higher dimensional space, the same function that we talked about becomes a bowl. And so you can find the minima of that at some point. I also mentioned that, see, in data space and hypothesis of parameter space. Machine learning in many ways is not done in the data space, but in the parameter space. Data is given. many ways is not done in the data space but in the parameter space data is given we want to find the best parameter that fits the data and so what you do is you do gradient descent in the parameter space right that is the crucial idea it's a hidden space in which you go and do your gymnastics like most of the machine learning exercises right then we went to the concept of bias and variance this is a bias know, you're totally off to one side. Right? Variance is when you're all over the place if you're doing target practice. And the optimal model is you don't want to be all over the place. You want to be concentrated and you want to be concentrated at the bull's eye, not offset, not biased. So when you look at a model, it will all models will have some bias errors, some offset errors, some variance errors, and some irreducible errors. A bias error comes from your model being a little bit simpler than the reality, variance error comes from it being more complex than the reality. And then the irreducible error comes from that which you don't know know or the fact that you essentially uh have not picked yeah i mean so basically that's what noise and other factors they bring that to you right the ear so you don't try to minimize irreducible error you can do that not in the learning process you can do that just by getting more data more types of data so if you sell ice cream on the beach one fine day as an an entrepreneur, you'll wake up and say, oh, God, I need to keep track of the day of the week also. Is it a holiday or not? Now you have reduced your irreducible error. You have a richer model and your irreducible error decreases. Another day you may wake up and say, oh, goodness, windy days days not many people come so winds matter now once again you know more your model knows more and so your irreducible error will come down then there is still the question that you don't know the ground truth so you make models of different complexity if you build a model less complex than too simplistic compared to the ground truth you will have a lot of high bias errors. If you make a model that is way more complicated than the ground truth, you will have high variance errors, and your data will go and overfit the model. You notice that in this situation, sorry, your model, not data, model goes and overfits the data. As you notice, this curve, white curve seems to go through every single data point. Whereas the yellow line is not going through all the data points, but still the yellow line is superior. You say that this very complicated model has overfit the data. It's way too flexible. That's what we learned. Finally, we talked about the Occam's Razor principle, which basically says, people often say that the simplest explanation is the correct one. No, the simplest explanation if it doesn't work is not the correct one. Right? So the among the explanations that seem to work, that do work, the simplest amongst them is the definition of the correct, correct explanation. Are we together? That is the Occam's razor principle. So in machine learning, we tend to emphasize the Occam's razor principle in data science, because you can build a lot of models, very complicated models to explain data. Now, which model would you like to take? The simplest one. So let me make it very practical. Let us say that you discover, or you guys are all very young, let's say that your parent, you discover that your father has diabetes or mother has diabetes, right? If they are reaching that age at which diabetes would come in, right? So you go to the doctor, they go to the doctor and the doctor says, hey, we did the blood test and so forth. And so your parents ask, so what should we do and the doctor says make sure that you drink only 200 milliliters of carrot juice then in the in the early morning drink one whole glass of karela juice right and then don't take sugar make sure your food is only this much and you know you get a long list of requirements and they say that our model says that this is all that you need to do your parents will come back feeling pretty lost because it's such a complicated bunch of things to do but if on the other hand you go to another doctor and the doctor says you know what you have diabetes now you need you you have to stop eating sweets you have to stop exercising more and you have to stop exercising more, and you have to control your portion size. So a very simple model is just saying cut down on sugar, cut down on portion size, and significantly increase your exercise. Now which of these two doctors will be more effective in behavior change for their patient? Which of these will be more effective in helping the patients with diabetes what would you say second one second one is practical you know the first one is also right maybe you follow that very complicated regime pointed by the first doctor, it would have the same effect, right, helping your parents manage the diabetes. But certainly your parents life would be much simpler if they followed the simple set of instructions rigorously. So you say that that is the power of simple explanations and that is why you should always search for simpler explanations in all situations. Don't build very complicated models. Maybe sometimes people build that just to impress their colleagues, but it's not necessary. After some time, you realize that after you're done playing with the shiny toys, you need to mature and find the simplest models that explain the data, that agree with the data. And that is all for today, guys. Thank you.