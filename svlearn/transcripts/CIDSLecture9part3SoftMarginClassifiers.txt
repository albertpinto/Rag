 So in reality, what happens is that, would normally draw your decision boundary like this your margin so not decision boundary your margins like this and't it but let us say that on a river there is a boat house have we seen boat houses a house on a boat so first i said you know you don't have houses in the river obviously there are so let's widen the reality and say what if you have, you notice a house shining right here, right? And let's say that there has been some flooding and a tree has, and you notice that during flood season, trees get washed out. And so a tree has gotten washed out here, right? Now, there is a bit of a problem. You could say that, and not just washed out here, let's say that it has gotten washed out, trees can get washed out pretty far. What now? You could say, you know, my margins are now, let me use a different color. My margins are now, well, I can't flow it like this. My margins are now this and this, right? And this yellow house sitting on top of this margin. Isn't it? Would you agree? My river now is very narrow that I can flow through the data. This is the best river I can do. So far so good? Yes. And of course, then you would flow another river, another median through that as a decision boundary. Can't we curve the river? No, no curving. All rivers are straight in this ideal world. For now. Can you break into sections? Not allowed. So at this moment, it stays with the fact that rivers are straight. And completely clean. And completely clean. Railway track. What's that? Railway track. Sort of like a railway track, yes. So we have this. Or think of it as a canal right some engineers when they bring canals they're completely straight actually you can always tell a canal from a river because rivers will bend and canals will go straight the great uh i believe it was uh salvador dali who used to the great painter who used to make a very interesting statement. So I learned a little digression. At one time, I aspired to be a painter. I learned from a great painter, my teacher, who had graduated from Oxford. And he was really a master, amazing painter. So one day, he told me a remarkable fact and i think it's attributed to somebody he said that in nature you never find straight lines and edges rarely but in engineering human-made things you rarely find smooth smooth flowing curves everything is geometrical shapes right so you can tell a canal from a river because the river will sort of meander its way through the land and a canal will go straight just something to keep in mind but anyway at this moment we are pretending that it's a straight River for now straight section of the river It's a straight river for now, straight section of the river. So now if you were to pick which of the river looks better, the blue or the pink, intuitively to you? Blue still looks better. Blue still looks better, but blue, according to this theory, is wrong, isn't it? The only permissible river is the pink one so this brings the idea that see data has errors right so trees get uprooted and sail into the river houses develop lifeboats i mean houseboats right so you have to keep a little budget so suppose we did this we say okay let to keep a little budget. So suppose we did this, we say, okay, let's keep a budget. Budget C for errors. Right? And what is the error? See here, suppose, suppose I take epsilon one is the degree of the error so you say that the decision boundary for this or the how far it is uh from so by the way this distance is m the distance that the median is from the deceit from the banks is let's let's call it m margin so you say that for this yellow for this guy distance from the distance for the lighthouse is not m it is actually approximately some some uh it is a part of m actually the way you say it is a part of m. Actually, the way you say it is 1 minus epsilon, where epsilon takes values from 0 to 1, m. So if it is a small error, if it is a small error, distance would be exactly equal to, so for example, the lighthouse is here. It's practically m. The distance from the decision boundary is m. So the bigger the error, the further away it is, right? The further inland it is, right? Are we understanding this, guys? So you say that we allow an error for this point, this error epsilon. Now that is OK. So we said this point will granddaddy in, will say, no, you don't have to be, we don't consider you to be the support vectors. You are, we know that you're a light houseboat and this tree has gotten uprooted. So we'll forgive it too. We'll say that we are going to ignore it, right? Not consider it to be one of the crucial points. So these points that you choose to ignore, they will each have an error associated with them. Isn't it? Because this is your, how much inline you have come is given by your epsilon i. Now, you don't want to allow too many errors otherwise what will happen is you will if i start making a decision boundary that looks like let me choose a color i haven't chosen pink will that look different okay i'll try to pick a color i haven't chosen okay forgive me suppose i make a decision bound like if I make my riverbanks like this, this wide riverbank, why is this not okay? You're going to crash. Yeah, there are too many houses and too many trees that you have to sort of ignore, isn't it? You don't want to do that because being too relaxed means you have overshot the mark isn't it you're not you're not going to the bank and as kate said you're going as a as a boatsman you're going to with your boat you'll crash into the banks so you don't want that either you want to avoid crashing into the bank but you allow for some mistakes because you believe that there are mistakes in the data. So you keep a budget. And so this brings us to a concept that you keep a budget such that epsilon one plus epsilon two, whatever number of epsilon Ks you have, it is less than equal to some budget C. Are we together? some budget, C. Are we together? You think of each error as amount you have spent, and your total budget is, and think of error spending, error penalty that you are facing at so e epsilon i is the error penalty effectively you can think of it like that right and you say that the sum total therefore of epsilon i has to be less than equal to c so as if who will decide what is the value of c has to be less than equal to C. So, Asif, who will decide what is the value of C? Very good question. Remember, I told you that every model has hyperparameters. The cross-validation will tell us what is a good budget. Like by just looking at how many testing errors we get, we'll have to adapt the cross-valid validation. We'll have to adapt the C. Now it's a hyper parameter. So let's put it this way guys. We realize that the distance, now what is the distance of these error points from the decision boundary? Distance of this point XJ of X xi is equal to what is it it is a beta naught plus beta hat dot xi right dot xi or let me write it in my notation which i prefer it is this angle bracket this is it and we are saying that for some mistake it has to be it has to be basically greater than equal to some amount m 1 minus epsilon means the you the margin must be the distance from the margin you should still have reasonable distance from the margin. You should still have reasonable distance from the margin, but you allow for some errors. Now, you also realize that beta hat magnitude is equal to one. That's a constraint, isn't it, in this equation? So for example, if you write this equation, but you multiply it by 10, now your beta hats are no more unit vectors isn't it they still go to zero, but they don't, so this is a constraint and the last constraint is. That you have some of all the epsilon is is less than equal to some budget, it is, do you see how intuitive it is what i'm saying is just common sense right. you're just saying that you're allowing that is all the distances should be such that up to a certain error amount. Most points should have 0 errors. So if you are beyond the margins, if you are beyond the blue margins, what is your error? So think about it. This guy's if I have a house here with respect to the blue line, how much mistake am i making none none because this is more than the margin greater than the margin away from the uh from the decision boundary and so this point is fine no errors errors are only there only when you are inside the margins so then come so this is it. And therefore we write this equation, modified equation. This equation is, these are called, so this adaptation of the maximal margin classifier in which you allow for a few errors up to a certain budget, because that's how data looks, right? This is called the soft margin classifier some people also call this the support vectors classifier i don't i reserve that for the machines. But soft margin classifier, use this word, soft margin classifier. Why soft? Because you allow something, some data points to be inside the margin. Whereas, Pradeep, this answers your question, right? In the original thing, you were pretty harsh. You said nothing in the river. Clear separation of data but now you're allowing up to some error amount up to some budget c that in between the margins the two the two types will be mixed up right sir but um i had slightly different question that there could still be data which cannot be separated um in in categories in other words they cannot be linearly separated right one easy example is suppose you're inside a circle is all the cases are positive and outside all are negative you realize that there is no straight river going through this so we will come to that but let's not get there remember world is simple so at this moment now support this soft margin classifier allows for a situation in which your data is like this mostly houses houses and then green trees most houses and then green trees green trees right and some green trees even growing so far in but mostly green trees here green trees here and maybe some houses one house spouting here right which is a huge error because now you're making big mistakes right and you still say that hey you know what my i will still because my budget is good i will still flow my river i believe my river flows like flows like this sir what is the like i i get some of it, the question, the answer for the question, which I'm going to ask, but I wanted to get your opinion. What is the point of having the maximum margin between two classes of the data? Because technically, unless the data from one class doesn't cross the decision boundary, we should still be good. Yeah, I'll tell you why. See, let me answer that question. Why the maximum margin? We did that. See, what happens is that, suppose I have a lighthouse. So let me make this lighthouses again. I mean, this houses again. Suppose you get only data that is like this, and you get green data. Even in the simplest form, you get green data that is like this. And you get green data. Even in the simplest form, you get green data that is like this. Sorry. These are your trees. Are we together? So you take seven samples of data, eight samples of the data like this, or nine or whatever it is. This is your sample. Remember that any data set is always only a sample right and now let's take another sample of data which looks like this oh god it's yeah you've answered it at least yeah i did right so suppose i have this right and this is even here, some, and this green point. So let me finish the argument as a review. Right. And let's say that you have Now you notice that the data is different. So now let's flow one decision, one kind of a blue margins wide like this. But instead, if you had flown a pink margin like this, two points. So now obviously the blue has wider margin than the pink one. Now look at this data. Does your pink one survive with another sample of the data? No. No. But blue doesn't very much. It's more or less the same. Isn't it? And so what you say, you always want a model that is robust in the presence of sampling. It shouldn't change every time you take another sample of the data. And that is why you prefer the maximal margin. Yes, that's why it is very robust against outliers. That, against outliers. All right, guys guys so what i just taught here two major algorithms one of them is we learned we learned maximal margin classifier these are also called the hard classifiers hard decision because they make hard margins you're not allowed to have anything in between and we learn also about soft margin classifier some people call the call these as support vectors classifiers but doesn't matter we also learned that lighthouses now in the soft margin classifiers what are the lighthouses so go back to the intuition if you want to row your boat carefully do you do you only need those lighthouses on the banks of the river or you need it in the middle of the river too right so suppose look at this. Let's go back to this problem. When you are rowing your boat, let me take a yellow color. So this is again, intuition guys. So suppose you're rowing your boat. It is okay. These lighthouses have lit up. This also has lit up. And this tree has lit up christmas tree right but you realize that that is not enough because you can go and hit this tree that's in the dark floating in the river or you could go and hit this house are we together what do we need to do we need need to light up the things inside the margin also, otherwise, you go hit them. Make sense? Right? So that is the intuition, you need to go light them also. So in other words, they also need to become lighthouses. Or another formal way vectors in the case of in the case of soft margin classifier the support vectors lighthouses are all points all points data all points, data points, of course, data points on the margin, on the margins, sorry, I take that back, this is atrocious spelling, all the, on the margins and between the margins, between the margins. Another way to put it is inside the river. Are we together and we have a question how you won't hit that as a boatman if you forgive these arrows but you don't mark them but if you mark them also support vectors at least you can avoid yes exactly so you need to light up those otherwise you'll hit them right so this is the this is sort of obviously i've made it quite um with analogies but this i hope gives you a sense of this this whole technology this whole thing that we are leading up to the whole maximal margin thing so think of support vectors as lighthouses and the decision boundary as the path that the boatsman will take. Always. And the boatsman would prefer that the banks were not too close. You don't capsize. They were far apart. It's a wide river. Then you get most of the intuition for these things correct. And those are support vectors. So next I will go to it is 12 13. it's time we took a break one quick question uh massive just to correlate with what we had done before so does this uh does this uh two boundary two margin uh kind of a model converge into a single linear model single linear boundary model in uh kind of a model convergent to single linear model single linear boundary model in any kind of conditions no decision boundary is always single it's the median between the two margins right okay okay okay okay so okay you're combining okay yeah see between these two margins there will be the decision boundary remember this is only to get okay got it there's only one boundary still yeah there are two margins that is you need this two symmetric margins the median it is the median between the margin is the median okay is the median margins. So now things are beginning to look rather interesting. From here, now we will take the next leap in the next session in which we'll say that, no, wait a minute. Rivers never run straight, right? They wander around, or more precisely, in data, you almost never get linearity. You see a lot of non-linearity. So how do we deal with non-linearity in the data? That is a problem we are going to solve in the after the break now uh it is that is going to be a bit of a session it's 12 15. let me first review what we have done so today we learned about We use the analogy of a river and its banks and the boatsmen to learn about the concept of maximal margin classifiers. We want to flow the widest river we can through this dataset, not the narrow, because narrow rivers will change, the shape will change for different samples of the data right those lighthouses are called support vectors and the path that the boat must take in other in order to safest part to avoid capsizing against the bank is literally the median of the river and that is the decision boundary so So riverbanks are the margins, the formal terminology. So let's say this entire thing in more formal river, flowing the widest river through the data is the same as having a maximal margin hyperplanes, right? Creating two maximal margin hyperplanes. All points on the maximal margins are called support vectors. The median of the median plane between the margins, maximal margins is the decision boundary, right? And this is it. Then you soften it up a little bit for reality. And you say, well, we'll allow for a budget of mistakes. When we allow for a budget of mistakes when we allow for a budget of mistakes we then can still flow a fairly wide river but we get a classifier which must agree to this constraint that see in the hard margin where are we why did everything disappear okay in the hard margin are we why did everything disappear okay in the hard margin ah strange my screen went to blank what is happening let's go slowly yeah in this hard margin classifier what is the constraint that each of the points should do the green ones let's say and let me just write it the green points green points let me let me call this as plus one y is plus one right because you need a label and the label for reasons will take as minus one now observe something suppose we consider this distance as positive distance and this distance as negative distance from the decision boundary. So what do you want for all trees? For all trees, you want distance of a tree is greater than equal to the margin. Would you agree? This is what you want yes right and for all uh houses you want what do you want here for distance of all the houses should be, this is what, minus m, should be less than equal to minus m. So it could be even more negative, right? Would you agree that this would be the reasonable constraint for the houses? Right? Now, it turns out that you can play a mathematical game. Right? Now, it turns out that you can play a mathematical game. What you do, just, it's just a cute game. The meaning is exactly this. Remember, we took this point, says y is equal to minus one. point i what will it be if i multiply this equation by minus one it will become positive m which begins to look like the tree equation where y we set as positive one isn't it so on this side also we could say the same thing uh this y j times distance of this tree j is uh is greater than equal to m isn't it do you see that guys simple which is exactly the same equation as this so the way you write the constraint is, the more elegant way that you write the constraint is that for maximal margin hyperplane, the constraint is yi times the distance of a point xi is greater than equal to m, right? In other words, this is your main constraint in the case of this is the hard margin in the case of a soft margin you allow some latitude you say you you say you say d xi is if not greater than m it is greater than m minus a small at least a small part of m right greater than equal to something like this right you you relax it a little bit it can be in the river or you can have a few mistakes right this is it guys so that's actually going back to this i said that this is equal to but actually i should say y i because for ne for houses the constraint is different so it is this greater than y i d i which is y items this this is the distance is greater than that thing and remember beta is a unit vector which you say emphasize by saying it must be of size one and you have a budget you cannot go on making accepting mistakes and saying ignore this fine this is fine this is your main budget so so what happens is what happens when your budget is zero if your budget is zero in this situation let's look at this which of the which would be the margins the pink the purple or the blue The purple or the blue? If your margin for error is zero. Purple. Blue. Margin for error is zero. If C is zero. Oh, it would be the ugly pink one. Yeah, that's right. So you would say C is zero would be this situation. There are no mistakes here. No errors allowed. On the other hand, what about the purple one? Purple one, you would say that C is big. Lots of errors allowed, isn't it? Because you see between the purple lines, lots of houses and trees, they seem to be floating in the water. The blue line, hopefully the blue is C is for this, C is hopefully optimal. How would you know that it is optimal? How would we know that it is optimal? The errors are within budget? The errors are within budget? No, when you take the data and you make predictions on test data, on validation data, you will find that C gives you less validation errors, isn't it? Less errors on validation. So you have to. So now it says that, how would I know what is the optimal C? You search, typically you take lots of budgets, like C from zero to, C will go from zero to whatever big value, let us say a hundred. And then you take little intervals, zero, 0.1, right? Even 0.001, very small, then one, 10, et cetera, et etc etc you take all sorts of values of C and you have to keep on making your classifier for each value of C and see it gives you the least validation errors. That is the correct or that's the optimal choice for C. So C is the budget in this. Remember, so now let's think in terms of bias variance trade-off. If the budget is very small and you take another sample of data, will the model change? It will change, right? Yes. So when there is no budget, you have high variance in your models. That's what it maps to. But on other hand if c is too generous look at the purple line it is suffering from high you take another sample you know more or less it will draw the same thing but it is overtly simple it is not it's going to misclassify a lot of points and throw them into the river. Sort of like regularization. Yes, and that's what it is a bias. Yes, it's like that. So the buyer situation is too big a C variant situation is too small a C. Somewhere in between is the optimality between the bias. Variance is the bias variance trade-off the right value of c so far so good guys and that is it if you have understood it so far then you have pretty much gone through the large part of the intellectual journey, from here onwards, there is an interesting twist that takes us to the nonlinear domain and into the true, what I call, support vector machine algorithm. So let us do that after a 10-minute break.