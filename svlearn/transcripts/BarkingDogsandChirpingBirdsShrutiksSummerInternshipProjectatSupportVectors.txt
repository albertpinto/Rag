 you All right, guys, hope you're doing well. I just want to introduce myself. So my name is Shrithik. I'm a rising sophomore at UC Santa Barbara, and I recently switched my major to data science. This is my very first, you know, kind of CS data science project ever, and I'm really excited to share it with you guys. So yeah, hopefully you guys are doing well. I'm going to go ahead and share my screen now. And yeah, let's just get right to it. So first things first, I want to go over what exactly my project is. So this project is pretty much an audio classifier. It's essentially, you have two data sets of audio, So essentially, you have two data sets of audio, one consisting from a dog and a bird, and the program classifies it, right? So as a brief overview of the agenda of what we're going to go over today, I'm just going to go over a brief overview of the project, and then I'm going to go over both the approaches that I use to go ahead and tackle this problem. I'm going to compare the two approaches for you guys, and then I'm going to go over a couple of challenges and potential improvements. So firstly, the overview, as I mentioned earlier, it's an audio classifier that distinguishes between the sounds of a dog and a bird. And there's two approaches, the first being a convolutional neural network, which is the more, you know, kind of common approach. And then there's the vision transformer approach, which is the more powerful approach. So pretty much the main idea between both these programs is it creates a spectrogram of the audio. The model trains these spectrograms on what these sounds essentially quote unquote look like. And then the model predicts what this audio is from a dog or a bird based on the spectrogram visualizations so if you're unfamiliar with us with what a spectrogram is um i kind of just scraped some code out of an older version so i can kind of show you guys what it looks like um so this directory variables will be something you'll see commonplace between the demonstrations just for convenience. But pretty much using the Labrosa library, we're able to get mouse spectrogram data, which is essentially just a visualization of the decibels of an audio file over time. And then we can just use a pie plot to you know visualize it right so this is a sample dog audio and as you can see it looks distinct from a sample bird audio and then this looks distinct from sample noise which I compiled into a third directory for you guys so as you can see they look pretty distinct and uh you know from a human perspective you can see, they look pretty distinct. And, you know, from a human perspective, you can be like, after seeing enough, you can be like, yeah, that's from a dog, that's from a bird, that's from something else, right? And our goal essentially to create a program that can do this classification for us, right? So the first approach I took was the convolutional neural network. So if you're unfamiliar with how a CNN works, what it essentially does is it creates a feature map. So a feature map, in kind of simple terms, is just like a way for the program to kind of classify what the most distinct features of an image are, and then use an activation function to introduce non-linearity so this allows the uh the program to establish more uh complex relationships uh pooling is then introduced so all the feature Maps condensed from all the training data um you know kind of just gets you know merged to create like yeah this is the threshold of what a dog audio looks like or a bird audio looks like essentially um this then creates our fully connected layer which helps with classification and then obviously uh you know we have to train the model and uh you know the final output is the classifier itself so So now I want to dive into the architecture. And I'm going to point out a couple things because this project is still like technically a work in progress for the initial scope of the project. So I'm going to point out a couple things that are meant for later development for further, you know, to make this program essentially better. So first things first, we have the required imports. We have the pass definitions that I mentioned earlier. And then this essentially allows us to use a GPU to help it run faster. So first things first, we have to generate the audio dataset, right? So this function creates our spectrograms to, you know, create the image classification for the program to work with. So, for example, you'll notice over here that there's augmentation, data augmentation as well. And I'll go over this a little more in detail later. But the reason we, you know, augmented data is we need to create, you know, a bigger sample size. So the program would have sufficient things to train from. So yeah, this is the data augmentation function, pretty simple. You can stretch how long it takes, you know, the speed control, this is pitch control, it takes uh you know the speed control this is pitch control uh this adds a random noise to make it so um you know the uh the audio can essentially be different um and then after that we have the actual audio classifier so um this essentially modifies the uh it's a fly on my computer um There's a fly on my computer. This essentially modifies the shape so the classifier can work with it. And then the relu activation layers essentially analyze the patterns of each individual shape. So I had these print statements and I commented them out and this is just for debugging purposes, but if I were to not comment them out, you can see how the shape of each layer gets changed throughout each of these function calls. So next thing is the training function. And this is just a pretty classic PyTorch training function. It assigns all the training data, has the data loader process the prepared data iterates over all of the audio data to create, you know, have the spectrograms get processed. And then output predictions are generated. And that's pretty much that. Next thing is the audio classification. So we have the temporary labels assigned, which helps us classify whether it's from a dog, bird, and then you notice there's another label, which is other, and this is. This is for something I'll go over later but it's pretty much I was, I was trying to implement anomaly detection into this but I'll go over that process. When we go over the challenges. So yeah, then we just kind of put it all together. The data gets processed, the training and testing data sets are made. And then everything gets classified. The model gets trained. And then we have a scheduler over here, which slows learning rate. And this helps increase accuracy because a slower learning rate gives the model more time to process the data. So yeah, then we train and evaluate the model. And you'll notice that I only have one epic, but ideally you want to do something like 20 so the model can train properly. The reason I did one was just for demonstrative purposes, but ideally you want to do 20. And then, yeah, so then we can feed it an odd an audio file so in this case this dog single is a file path to senior away file same bird single and the audio classifier utilizes the labels uh the predicted labels that we went over earlier in order to kind of classify the um the audio so after that you can kind of just see the output even with one epic this program is decently accurate at like 97 percent um and then you can see that when we feed the audio the dog when we feed the trained model the dog audio it's able to accurately predict that yes is from a dog and then same thing from a bird yeah sure um let me get those audios up for you guys so this is the dog audio me make sure my audio is on. And then the bird audio. And then, you know, this folder is filled with just sounds like that so that's pretty much the cnn model um pretty simple pretty straightforward um and now we're going to the vision transformer so a vision transformer um this model tends to be more accurate over time because of how it works, which I'll go over right now. So what a vision transformer does is it divides an image into patches, and you can specify patch size, but generally are pretty much all the case. You want the patch size to be like a multiple of the total image dimensions so for example um if you have an 8x8 and if you have an 8x8 image and then you have patches of 4x4 then you have four patches because um total dimension 16 then 4x4 or the total dimension 64. 4x4 patches 16. yeah simple math and pretty much what it does is it takes these patches and it tries to build a relational understanding of what exactly this image is and how it can classify it into patterns. So an embedding is essentially a numerical representation of these patches. And once the program divides the image up into patches, it can build a relational understanding via these embeddings. So these vectors are then one-hot encoded, which is pretty much just using, like, how do I describe it? You can use, like, ones and zeros to represent different categories. So we'll kind of, it's easier to explain when you kind of see it in the code. And then a self-attention mechanism assigns important scores to the patches. So for example, certain traits are assigned to be more distinctive than others. Residual connections are then made for processing of processing efficiency so essentially just remembering what the most important traits are and then the data is normalized for stability and then obviously we train the model so what's nice about a vision transformer is that it can be pre-trained which means that um it's trained on not only the data sets you provide it but um data sets that is worked with over time so that way it can be more accurate and say yeah this is from a dog this is from a bird whatever if anyone else has done like a project like that so going down to it um you know going back to similar with the cnn this function creates our spectrograms this function um process the files in the folder so essentially this just creates like a list of all the files in each directory and then you know shoves it in the make spectrogram file to essentially create each audio file into a spectrogram um this assigns labels to each of the spectrograms. So pretty much it sticks the spectrograms with the dog folder with the label 0 and then 1. And then this is the encoding pretty much. And this is our augment audio, which we discussed earlier. So this is the vision transformer model and you'll notice that we standardize the image dimensions to fixed dimensions so we can use a patch size of 16. And we ensure that the model is pre-trained so we can you know use that that data from you know prior models um and then so something you guys might notice is i use categorical cross entropy instead of binary cross entropy and this is because uh what i'm demonstrating to you right now is kind of just a retrofitted of what i was working on which i'll i'll show you guys later but pretty much um for a two a two problem classification you ideally want to use binary classification because that way you know you have to work with two things um but the reason i did categorical cross entropy was eventually i was trying to create a third class which was other and you guys saw like an inkling of that in the cnn as well so yeah um this we have our learning rate scheduler which is the same model as before just um it it takes it slows uh after the fourth epic it slows learning rate by about by 10 so learning rate scheduler and then has our validation data via our test data and test labels. And we do a train test split later on. We have our evaluation over here. So this just pretty much what this does is it makes a prediction via seeing like because the softmax activation gives us probabilities for what what each of the classifications are this essentially says okay let me take the max like the maximum or let me take whichever one is is most likely and then assign a label to it pretty much this is our schedule that I was talking about earlier um and this was um this this part of the code was something that it's like a threshold to determine if say that a prediction isn't entirely solid. Then it can return other. But the implementation for anomaly detection hasn't necessarily been finished, so you won't usually see or you shouldn't really ever see the other prediction pop up. yeah absolutely because you know there was a couple bad files that I got rid of in the data set. And those worst files had pretty low confidence scores. And it was just around like 0.79, 0.8-ish. So this is like, you know, these numbers came, the 0.8 just came from experimentation. But since anomaly detection isn't implemented in what's on screen right now, you shouldn't see other generally um so yeah then we just have um uh this this gives us our training and validation validation loss graph and over more epics um you can see that and this essentially just tells us whether the um the model is overfitted or not overfitted underfitted uh with more epics um the graph should show up but uh if we scroll down we'll see that the graph is empty because i only did one epic um i would have i would have shown you guys 20 but that takes like 10 hours to load on my laptop and i didn't you should run it on the server yeah i can i can probably run on the server what is the gap between validation and training losses? How much is the balance? We can pull that up. Right here. Yeah, so the validation loss is 0.1688. And then the precision is 9349.9349 and what's your training loss my training loss um i don't think i have that on hand right now um that should give you an idea. Oh, just like the regular loss? That's 0.2572. That's odd. Oh, no, this is not normalized over number of data points. This is a net loss. Which loss are you using? Cross-entropy loss. you using cross entropy cross entropy yeah is it normalized how can your main loss be more than your validation loss that's a good topic yeah um anyway so most likely it's a net loss. Yeah, but yeah, where was I? Okay, so there's just a model evaluator. So this gives us a confusion matrix to just see how well the the program is doing. So, you know, there's just the plotting and gives us the true classes compared to the predicted classes over here. And then so I was initially going to use clustering as a form of outlier detection, which is why I use the K-NN based predictor over here, K-Neighbors classifier. And there's other ways to predict this, but this specifically just uses a neighbor's classifier to determine whether it's from a dog, bird, and then this, and then any other case, other, whatever. And the way it does this is, if you guys remember, the embeddings are the numerical representations of the images. So it essentially just plots the embeddings and then uses the neighbors to make a conclusion pretty much. So this was an attempted form of outlier detection, but it wasn't very effective. outlier detection, but it wasn't very effective. So yeah, and then as usual, same as the previous model, process the data. We went over the prepare data function, and then creating train test split, and then using our labels that we defined earlier, with the dogs being zero, birds being one. And then compile our model, train our model, evaluate our model, print the accuracy. So as you can see, the accuracy is lower compared to the CNN, but this is only because we did it over one epic. Over 20 epics, the VIT does end up outperforming the CNN by approximately 1%. CNN by approximately 1%. With the CNN, I believe over 20 epics, we got around 97-ish percent oscillated kind of between the higher and lower ends of 97 in terms of like 97.01 to like 97.8, stuff like that. This one's able to hit the higher ends of 97 to around to like the lower ends of 98. hit the higher ends of 97 to around to like the lower ends of 98. so over time this does outperform uh the cnn but you just need to give it more training time um so yeah then we just uh use the knn predictor that we you know went over earlier and as you can see if we feed it the bird single that we're talking about earlier currently predicts see if we feed it the bird single that we're talking about earlier correctly predicts it's a bird feed the dog single that we were talking about that we showed you earlier then it correctly predicts it's a dog so yeah this is a confusion matrix um and something i want to point out um i guess actually we can go over why the um um it was initially just meant to be a form of outlier detection i thought that maybe the embeddings for um maybe the embeddings if i just fed it like a random file would be kind of messed up and then the clustering would essentially just be like oh this isn't close enough to either cluster so it would be classified as other so that's why i used knn to begin with because it was a the first attempt at outlier detection um but it didn't end up working and the reason it didn't end up working was because um i'll go over this in more detail later but the main idea was that when you when i tried to train the model on so there's there's two methods I used for outlier detection. The first one, which was I created a database full of audio samples of just random noises. But the problem with that is that those random noises weren't uniform. Like, for example, that folder with other contains sounds from different animals such as sheep, cows, whatever, but also has data from like planes taking off from glass breaking stuff like that. And if you have variable so much variety in data and then you say, shove this all into one class, it not. Yeah, it destroys the accuracy and everything gets messed up. yeah it destroys the accuracy and everything gets messed up um so yeah this is the confusion matrix um and you'll see like there's a lot of misclassifications on the dog um and this is partly because uh i ran into a lot of trouble finding a good data set for dog audio um while with birds there's i mean if you can there's just a gajillion data sets out there. So finding bird audio was not an issue whatsoever. So that's the general idea behind the CNN. Is the data from the Audubon Society? Is the data set available? From the Audubon Society? I don't think I used that. There is a couple good bird data sets on Kaggle. Yeah. And then, yeah. And then for dog audio, I had to combine a couple, just had to combine a couple data sets. So yeah, that's just the general overview of each model. So now I want to compare the two. So first things first, let's go over the pros and cons for the CNN. So first things first, the CNN has good feature learning. So obviously, you can extract relevant features from data, which helps image classification. And that's essentially the foundation of both models. It converts the audio into an image, and it's essentially an image classifier. It has good hierarchical representation, so you can have more complex structure recognition. Translation and variance, it can recognize patterns regardless of location in the data, so it can, you know know compare data pretty far apart and then obviously it's it's less intensive um some cons that it's data hungry but in general it's good practice to already have a big data set so this isn't that big of a deal um it can be intensive if you run it on your CPU. Getting information out of a CNN can be difficult. And it's not really flexible. So you can't use it for it's more difficult to work with if you're not trying to do just image classification, while with vision transformers, it has a little more potential. while with vision transformers it has a little more potential so with a vision transformer long range dependencies so they can recognize patterns that are far apart it's very scalable so you can use it for any data set size and any model size um it has unified architecture so for example cnn's are primarily used for image classifiers but vision transformers can be even used for things like text. And then the vision transformers are extremely easily trainable. And part of this is because you can also get a vision transformer that's pre-trained. This may be a hardware limitation, and I'll go over this in some of the challenges I face. But the VIT is obviously very computationally intensive. a hardware limitation um and i'll go over this in some of the challenges i face but um the vit is obviously very computationally intensive um it took a lot longer for me to run the vit on my laptop compared to the cnn um it's very data intensive uh for example i wasn't really able to hit um i wasn't really able to hit above 70 80 80% accuracy with a sample size of maybe 200, 300 sounds while the CNN was able to do so. But once we started hitting the thousands, we got back to our 90s in terms of accuracy. So this meant that I had to supplement it using augmentation, which is fine. So not too big of a deal but it's just mainly it's more computational, more computationally demanding. So which is better? I think it depends on your use case. If you have larger data sets and you have the computational efficiency, I think you should go with a vision transformer because it's generally more accurate and it's more powerful. But if you're limited in your data sample size, for example, if you're collecting the data yourself, a CNN might be a little better. So a couple of challenges that we ran into during this whole process was versus computational limitations. So my laptop is a MacBook Pro M2. And because of this, I ran into a whole ton of just random things not working out because this is a fairly new laptop. So not everything has been updated to, you know, work with the chip I'm using. to, you know, work with the chip I'm using. So an example of this, as I was discussing earlier, was the Vision Transformer, CNN, you know, intensive, one being more intensive than the other. I couldn't get CUDA working with my Vision Transformer because installing it on m2 was extremely unintuitive um another example is we use the atom optimizer and the atom optimizer is pretty good it's flexible um i wanted to use uh adam w but adam w wasn't working with my chip so i couldn't um and then if I want to use something like a tensor flow CNN, for example, that also just did not work with my chip. So I had do a whole ton of awkward workarounds because of my hardware limitations. So that was the first challenge we ran into. Second chip. Yeah. into second check yeah um the data set size was another um challenge so as i mentioned earlier getting the bird sounds was not really an issue um but you know i will literally give you like 10 million dollars if you can find any dog data set size any dog data set on the internet that's that's open source that's like above 100 files like if you can find me that like i will literally give you born child. I remember that. Yeah. I am unfortunately petless, but yeah, I don't know. Maybe I could steal one of my friend's dogs or something. In any case, finding the data set was honestly probably the biggest struggle because right when I thought I hit gold with my dog data set, I ended up listening to it. I was like, wait, why is my accuracy so low? And then I listened to all the sounds and I'm like, these are not dogs. So I had to start from scratch pretty much on the data set size. So although it's elementary, I will say that having good data for your data set, essentially makes or breaks your model. And even finding something as simple as a data set can literally take you days. So just, you know, something to have a heads up for. Finally, outlier detection. So this was the big thing that I was hopefully going to be able to show you guys, but unfortunately I wasn't able to get done on time for this presentation. So pretty much outlier detection. I mentioned the KNN from earlier, K-Neighbors Classification. That was the first attempt at outlier detection. So the main problem that we're running into, so the first attempt of the outlier detection I use, I think it's probably in this one. I think it's version six. Yeah. So the first attempt I used for outlier detection, which didn't really work, was I created a huge folder full of random sounds. created a huge folder full of random sounds. And so these random sounds are just like ambient noise, cat noises, like whatever, just random stuff. And I wanted to essentially create a third class, which is why we use categorical cross entropy, because I wanted to create three classifications instead of two. But the reason this doesn't work is because the variability within the data. So if you actually end up running this program, I did a keyboard interrupt, but you can already see through the first couple seconds of the epic training uh these stats are pretty bad like you have 80 loss um pretty horrible precision and um although it's very short into the uh epic training life cycle um these stats don't get better trust me when i say they don't get better um i was hitting 75 80 accuracy which is not something you want for a vision transformer um so i had to toss this idea of the window of just using a out the window of just using a um a data set full of random noises and then saying and then feeding it another random noise and saying hey maybe it'll fall under this um umbrella of random noises that wouldn't work now if I want to really be you know ultra dedicated then I can create like a class for sheep class for ambient noise class for rain class for grass grass breaking but that's not practical because that's too many things I have to account for. So the next attempt that I was trying to use that I didn't I wasn't fully able to get working in time for this was a recommendation from my good friend Dennis, which was using an ISO map. But I just couldn't get it working. And there's just an in time for this presentation. So I think it has potential because, um, during the first couple of run-throughs, uh, it was able to classify, um, it was able to classify birds accurately. Um, but, uh, if I fed it, I think, um, I don't accidentally delete the code here, because I was trying to change it around. But you can kind of just see the remnants from when I did try to run it. It was able to classify birds properly. But if I fed it dog audio, it was unable to differentiate between dogs and other. So I probably just need a larger data set for that, and I might need to tweak around with some hyper parameters um but this approach could have potential but i wasn't able to uh get it working in time so did you try any of the different anomaly detection techniques um not yet yeah um but yeah that pretty much concludes um what i've been working on uh it's a pretty fun project uh this is the first time i've done anything remotely in this field so i just tried my best to learn everything and hopefully um i'll be able to answer any questions um to the best of my ability yeah that's my presentation hope you guys enjoyed sorry for taking so much of your time but appreciate it excellent thank you thank you thank you yeah on them too yeah no it's a mac laptop so you need to use my laptop we don't have food on the cpu it takes no yeah okay okay in my basic recommendation stance if you want to do ai Okay, in my basic recommendations times if you want to do it, you start by donating your Max to someone. Yeah, you can use Google. Okay, yeah, i'll i'll probably get on that so I don't have to wait 7 h between each test run. It's a new thing. It's $50 a month. Now they give you access to A100s and that will make a pretty quick job. I don't know what computer unit is. I don't know. It's more than 500. Yeah, 500 computer units is one and... I don't know that exactly. It's their own definition. No one really knows. is Thank you. Thank you. So, guys, first of all, thanks a lot for being here for the presentation. I should think this is amazing job, amazing internship. Going forward, finish that DAD. Apply the DADs to it and see where it takes you. It's an important step to do. You identified the good technical point. See when you take random swine, there's a difference between random as in truly random, like as in Gaussian noise versus random uniform noise or white noise, versus randomly picking up sounds or sound files is not random because they belong to something. It belongs to a truck. It has a pattern. It belongs to a cat. And so this will confuse the model for sure. And that's probably one reason why you couldn't get out of it. Anomaly detection from that perspective isn't good. One approach that you could do potentially is use an an auto encoder well you haven't taken that when you take the deep learning course you realize that one possibility is that you're projected into a latent space in such a way that you force all the dogs to go into one region of the latent space and you force all the birds to go into another region of the future space so whenever you take a point you look at the net reconstruction loss. If you get a sound that is neither a cat, I mean, neither a dog or a bird, the reconstruction loss will be high. And you know that this is a wrong time. Yeah. So those things will come to you when you take the neural life. I mean, for you who has practically no background in this field at this moment. This is already very impressive work. But then follow it up with more systematic learning, and you will see a difference in that. But once again, this is an amazing job. I hope you continue to be associated with us. It's a pleasure to have you here, and a very, very successful internship. It's very successful. Thank you. sure guys very successful thank you all right guys so with that we move on to the third item any comments guys any questions for shirthik before we move on to the next item any comments I don't know. Go ahead. Seriously, you did an extremely great job. Exactly. Your task was distinguishing for audio files and audio files, right? Yeah. So what went into choosing vision models? Why a vision transformer? Yeah. Or even a content that goes in for .. Yeah, a vision transformer is just more more powerful more accurate more scalable so. I don't know it's just recommendation but. It works. Yeah, most of these guys who are doing like if you look at my torch audio etc the standard models, somewhere along the lines, people have tried many many approaches, they have looked at the fo transform, they have looked at audio snippets as a data file itself, and many approaches have been tried. Recently, like for this kind of a problem, which is in a way straightforward, and the difference is quite high between a dog and a bird, when you look at the spectrogram of the two, you can literally visually tell them apart, because visually they are distinguishable right therefore a vision transformer makes sense. Audio is a sound wave, is it a Fourier transformation? Yeah it's a sound wave, so you can do a Fourier transform and you can look at the spectrum of it and that's not very far from the spectrogram it's somewhat along the same lines here. An internship for what sorry it was a summer internship yeah so we had we had six uh fairly successful summer internship this was actually one of the very good ones it's a better project than mine in masters that's very good all right So guys, please stay back for lunch. I'll go and get lunch now. So you guys can continue talking and I'll take a break. It takes about 10 minutes to go, 10 minutes to come back. So in 20 minutes, Len should be here. I'll go get it. Anybody who wants to accompany me is welcome to join me is the presentation all that done time to finish