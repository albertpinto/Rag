 Last week we covered some NLP fundamental topics. We covered what is a text processing pipeline, what are some common words, what is the word vocabulary, corpus, corpora, document, tokens, mean, all of these words mean. We learned about various forms, various activities of speech tagging. We learned about stemming and lemmatization. I did not cover stemming a lot, but I did cover lemmatization. I tend to use lemmatization a lot more than stemming, though I do use stemming occasionally. Then I mentioned that you should go back to the textbook, the spacey textbook and read it. I hope you did that. If you haven't had time to do it, please do so. Otherwise, you won't get time later. Now, we also learned about dependency diagram. Given a sentence, you look at the sort of the linguistic structure of the sentence using a dependency diagram. These dependency diagrams, if you recall, they are rooted at the sort of the transitive verb, the main verb. Now the verb connects usually a subject to an object connects usually a subject to an object. And quite often by looking at the verb and the object, you get to understand the intent of the sentence. For example, if you say, I would like to order pizza. In this sentence, we obviously, you see the connection. Order is the transitive word, object is pizza. So order pizza is the intent of the sentence. So, for example, if you were writing a chat bot, you would zero in in the chat bot to these words, isn't it? In the course of this workshop, one of the goals we have, or one of the labs we have is, we will write a reasonably good chatbot to engage with us. So this is the beginning of the idea. You try to figure out what the user is trying to say. One of the simplest ways you can do that is you can zoom into the to say, one of the simplest ways you can do that is you can zoom into the transitive word and the object. And then sometimes you need more. For example, I would like to order a really hot pizza. So the word, you And the object doesn't immediately follow. There are some adjectives in between. So that's the value of semantic analysis, linguistic analysis. analysis. This book, Spaces, the textbook, I don't have it right in front of me, but I believe I showed it to you folks last time. I cannot emphasize enough that two things. It is very easy to read, and therefore there is no reason not to have read the whole of it. Start with the first few chapters. By the time you're done with three, four chapters, first of all, the chapters are very easy and short chapters. Once you do these chapters, you'll realize that it's fun. The return on investment is quite high when you pick up these libraries properly. So do please pick up these, finish that book, or if you don't prefer the book go to the spaCy website and work your way through the documentation in detail make spaCy a part of your tool set your tool chain it is very useful now why did i pick spaCy as opposed to more commonly use things like NLTK and so forth, the answer is captured in one word, speed and the production grade. In many ways, Spacey is opinionated. It doesn't have all the features that you find in NLTK, but it gives you the sort of things you use in production environment. And it works very well. It works at a fairly high speed. So it is a limited opinionated production ready library. In contrast, for example, if you look at NLTK and so forth, they are written in Python itself. The implementation, the underlying implementation is not in a C derivative. It is in Python itself, the implementation, the underlying implementation is not in a C derivative. It is in Python itself. For that reason, it's a bit slow. However, it is a favorite of academics. And most of the textbooks in NLP, they tend to use NLTK as the base for teaching people about NLP. So I made a deliberate choice when I chose not NLTK but spaCy in the interest of taking you to straight to production kind of environment. Last session was NLP fundamentals. We did not use deep learning. And you might wonder that in this deep learning workshop, why are we staying away from deep learning? Deep learning is there. Neural networks are there. Spacey rests upon neural networks. But it was not visible. So I differentiated NLP subject into two parts. I said that you have, for example, natural language processing, and you have, sorry, natural language processing and you have, sorry, natural language processing broken up into the world before transformers and the world with transformers. Two topics. It is my use of terminology, I don't think many people would classify it like that. Maybe it's some word. I use the word classical NLP to refer to everything, including neural networks, but not using transformers. And then is the NLP with transformers. Transformers are a game changer the NLP with transformers. Transformers are a game changer and their impact is, we are feeling the impact in a dominant way more and more. As I mentioned just a few, just in the last couple of months, there has been yet another significant breakthrough that makes transformers, their performance, their main attention units, which used to be quadratic performance, and therefore very slow. I mean, for very long sequences, it would slow down. Now we have a breakthrough that makes it faster. It makes it of linear speed. So breakthroughs are coming in the transformer world pretty quickly. And then there are models like the GPT-3 these days, they are so shockingly effective that some people are beginning to wonder if this is the very, very early stages of true AI. It seems to be able to do math, it seems to be able to do all sorts of things. We know that of course it is not intelligent, it's dumb as stones, but it is so insanely big a model and so clever in many ways that it sort of gives us a very strong sense, it projects intelligence in many ways. So transformers are very important and I will devote practically the rest of the workshop to transformers in a big way. Today, I was going to start with transformers, but then I realized there's one topic that I should cover and I haven't covered. It is the topic. It is, well, the topic itself. It's a little bit secular way of putting it. It is topic modeling or semantic analysis. So I will do that without using transformers, using the classical approach, because it is worth understanding the classical approaches. So today goes to that. Now let me remind you guys that we did word vectors in the past. You remember in fundamentals, we devoted some time to word vectors. Word to vec. So in the coming few weeks, I'm going to do the word vectors at a slightly deeper level. I'll review the word to vec, the glove and so forth. I am also going to that Skip Graham model. And then I'm going to also do attention. If you remember when I did attention the last time when we were doing fundamentals, I moved fast. And many of you gave the feedback that you were sort of a little bit lost. It went too fast. And now I won't. Now we have literally three, four weeks to do the attention and transformers. So we'll do it properly and deeply. And we'll do a lot of practical examples with that. Now, before I go into the topic of today, a little bit of bookkeeping. Guys, I hope you have formed your teams. If you haven't, this is high time you formed your teams for the project. And this Saturday, some of you, it would be good if you could schedule some time with me to show your progress with the project. Is there anybody who has achieved anything with the project so far? Any group? So I'm not hearing much, but I hope the starter code was of some use in the project. So use that start. If you look at this code that I'm releasing, it is partial. It's contributing a lot towards your project. So keep using that. We had to reproduce some parts of it from the lecture. They weren't in the downloaded files. Sorry, please say that again. We had to uncover some files from the lectures because they weren't in the files we downloaded from the homework. Oh, is that so? So I did not zip up the latest version. I will do that. I mean, anyway, I'm going to give another dump this Wednesday and make sure that I have the I have the latest for you guys. All right guys, so with all of that in place, let's get to the topic at hand. We are going to do something called topic modeling. Are you guys able to see my screen? Yes. Okay, so this topic modeling asks the following question. Suppose you have documents, D1 to D. Actually, let me use capital D for documents, D1 to Dn. And I give you a bundle of these documents. You can ask, what are these documents about? Now, by document, a document could be, remember in NLP, can be a sentence, a paragraph, contents of a file, or whatever you want it to be. So the word document is more general. It could be any one of these things. Usually you don't mean word as a document. Usually you go sentence, paragraph or more or some other larger logical unit of words as document. So suppose you get a lot of documents. just imagine that you get a lot of news articles or blogs and so forth. The question that is being asked is, how would we categorize it? Now, human beings are very good at it. We read articles and we immediately start making buckets, right? So something like we may start making buckets, like even when you go to the news site or something like that, start making buckets like even when you go to the news site or something like that but instinctively even if you if you're not told you will start making buckets like politics which is obviously a very relevant today evening and we as you know we are all on tenderhooks wondering what will happen tomorrow and will they even be law and order and so forth. So anyway, politics, it could be sports, it could be a pick a category, what else should we take? Politics, sports, let's say science and technology. Science and technology step. Step and more of these, but let's say that I take a finite set of category of topics. What we do is each of these buckets, we tentatively start associating some of these here from this, one, two, three, we start associating to these topics. But even when we associate a document to the topic, some documents, they don't quite fit into a topic. What they contain is even though they may be talking about sports, in the middle of the sports there may be they may contain some bit of so given a document di you can say that you can do a sort of proportionate attribution you see you can say something like uh let me be put it this way 70 sports 20 politics sports, 20% politics, for example, for whatever reason that it also talks about some political acts during the whatever game that was in the stadium. And it could be maybe a 10% medicine, like medical or science and technology STEM. Let's say it's talking about some sports injuries and how people have recovered or the physics of the game and so on and so forth. So any given article when you associate it with topics, you acknowledge the fact that it's not a watertight compartment. There is always a bit of overlap. So what you can say is that if this document were a pie, if this document were like this, you could sort of divide it like this, that this part is sports, this part is, I don't know, politics, when this one is too like this. This is politics. And this little part is STEM. Are we together? So this is a document di to these three topics. Now what do we mean by the word topic? So we know that the words that we are creating is document, topic. And then you have words. A document manifestly is made up of words. That is the evidence. That's the observable, isn't it? So, pay attention to the way I'm saying it, guys. A document is essentially a stream, a sequence of words, that is by construction, that's the observable. However, when we talk about topics, topics is an abstraction. It is a human construct, you can say it's a linguistic construct and so forth, right, based on experience and so forth. Now quite often, unless the document has already been tagged with the topic, you don't know the topic. Topic is something you infer about a document altogether. Now suppose you're given a lot of documents, but you're not given the topic. So then what happens is a topic becomes, in some sense, a latent variable. A latent fact is the underlying reality that these different documents can be bucketized to different degrees into different topics. Am I making sense, guys? Is that argument very simple? If I just give you documents as a sequence of words, then inferring the topic, there is topic, you know, the documents are not random collection of words. They semantically, the semantic meaning of the document is centered around some topic. Right? And so the topic needs to be, in many ways, we need to infer the topic that this document is about. And to infer the topic is to get to the semantic essence of the document. It's part of the exercise of understanding the document, the semantics of it, is to figure out the topic of it. So let us have this framework of thinking in our mind. Once we have this, we can make progress. So, so far so good. Are we together guys? Any questions? So document can have multiple topics, right? Yes, a document can be associated with different topics to different degrees. That's the statement I'm making. Asif, I know we're going to get more into the details of topic modeling, but at the very high level right now, if one has to look at what would be some of the contexts where topic modeling is applicable, what would you give us a couple of examples? See, the examples are like this. See, the obvious example is that you have a whole pile of documents. So, for example, that particular participant is not here in this workshop, but he was there two years ago. He had this problem. He worked for a bank. The bank had a lot of documents and they were supposed to be, because it gets a little bit confidential, they were centered around certain departments. That this document belongs to this department to be processed by this department, this department, to be processed by this department, this document has to be processed by this department and so forth. And obviously, we are talking about banking, so you can imagine that those things are very sensitive documents. Now, as always happens sooner or later in IT, at some point, the classification which was done by hand, some secretary or somebody assigning when the forms would come or the documents would come, there would be forms and so forth, they would be assigned to different departments or categories. And they were mis-assigned to such an extent that it was a mess. And then there were a lot of documents that were just sitting in the backlog forms that were just sitting in the backlog, unassigned to any of the departments. So what would you do? I remember he came to me and asked, is there a way to do that? So the typical way you would say it is that can we write a classifier and so on and so forth. There are many ways to approach this problem. Now what I suggested to him is in the interest of speed, try something simple. And in fact, one of the techniques that actually worked for him is something I will teach you today. And it worked like a charm so what was a frustrating project and there were vendors who were offering to do it and charge quite literally millions to the bank to do it he sat down and literally finished the entire project in a week and he was able to recategorize them to the right place. Another example that I'll give is, and this is an example from Oracle. This I did a long time ago, a long, long ago in a previous lifetime when I worked at Oracle. And you know, Premji, there was a view. There was a, in Oracle has a bug system. It is called Bugb based on that and it is not jira it is not any of the modern systems that we know of but it works for oracle it used to work in those days and it had gazillions of tickets so-called uh you know issues filed by customers and so on and so forth tickets now one of the things that would happen is in such a vast ecosystem of product and such a vast ecosystem of clients quite often the same issues would be filed by different clients or across different time periods on different things and different customers support people or sales people or tech support people would not actually tech support people or somebody would handle that and it would percolate its way occasionally down to the engineers or to people like that. So the issue was that we had a pretty, in those days, and I'm talking about 10, 12 years ago, there was a bit of inefficiency in the process. I remember taking a dump of the database, of that whole bugs stuff like that. I treated each issue as a document in the sense of this document. And then I asked myself this question that when these tickets come, instead of, you know, those tickets would have something associated, this area of the database and so on and so forth. Those things are all right. Broad areas are okay. But could you from that quickly infer which functionality it is related to or what group of functionalities it is related to? So there are quite a few things I tried. I remember one of the things that I tried was literally topic modeling. And when I did topic modeling, and remember those were the days even before the deep neural networks, the sort of powerful techniques. I'll teach you from next week onwards. In fact, the sort of techniques I used is what I'm going to teach you today. And it was surprisingly effective. So there's a little backstory to that it worked i showed it there was some vp uh in the in one of those divisions such as sales or support or whatever it is it used to be and not say it's support or tech support or something uh he was very very excited the client facing groups is very excited he reached out to to the VP who was the VP of our group, Premjit, you remember at that moment, it was Anurag Gupta and Dennis here and so forth. And their thing was that, no, at this moment, we have much more important problems to solve in our database. We should focus on development. So anyway, not much happened to that project, but the point is that, see, sometimes very simple ideas applied to relevant business problems can lead to very good results. Otherwise, you try to solve those problems with a lot of manpower. In a traditional approach, you get a lot of manpower. And the same problem gets solved over and over and over again, and each time differently. A lot of effort goes to rediscovering the wheel, and so forth. So these things are very useful. By the way, they have reached out. Would Google News classify as something that fits for topic modeling is it an example of topic modeling because google news needs to classify things into a directory structure that on the basis of where it thinks the document should fit right yeah google news is a very interesting thing that you raised the original implementation of google news was a form of topic modeling it's k means clustering clustering you can think of as a form of topic modeling and we will do that today yes the answer to that is yes and then later on they used many many techniques but it was surprising that you could take so the idea with google news was this see what you do is you just look at all the sources of news that are being emitted and so each of these is a document being emitted in the document sense that we are writing and what you do is literally you wait and watch and you wait for clusters or top either clusters or topics to emerge. And then quite often the clusters are around topics. You wait for those to emerge automatically from the document. And then what happens is from this corpus. So the corpus is like literally a dynamic corpus, right? It is the emerging news from around the world. And what would happen is suppose there is a news that it is most unexpected. One news, for example, was there was an earthquake in Nepal, unexpected and a fairly devastating earthquake in Nepal. So, you know, no editor has to sit and say, this is important. It will emerge because there will be too many documents talking about that, right? And so what you need to do is, amongst the many ways of topic modeling you need to find the core the core message of those documents and you have just suddenly created a headline news which says that there's an earthquake in nepal without any human intervention so if you go back and look at so yes the simple answer to your question is it is in a very broad sense yes it is just topic modeling and uh the i don't know if google still brags about it on the page google news i really like because that's what i visit they at some point they used to brag that they managed to come up with a new edition of a newspaper of the google news every eight minutes right so if you think about it the new york times comes to the morning and evening edition at most. But Google News has an edition every eight minutes. Of course, the catch to that is that Google is not a content creator. They are just simply aggregator. And then they do all sorts of modeling, machine learning on top of the content that is emerging from other sources and so that is the power that is the power of the topic of today that we are going to talk about okay this topic is uh it is the reason i want to talk about this topic is it is enormously effective enormously powerful and sometimes these days especially in a mad rush to get to the latest cutting-edge bleeding-edge thing that is happening we tend to forget what powerful gems are there and we should do it so again let us to recapitulate the observed fact is you have documents which are sequences of words topic if it is not specified topic is what you discover in a corpus of documents so what is a corpus think of it like that so here is a simple way that i will give which is very programmer friendly if you take a document di to be a list of tokens, words, right? The cow jumped over the moon. Let's say this, right? And so this is d1 and d2 would be like that so then what is a corpus a corpus would be corpus c would be a array of arrays of now the cow jumped over the moon and the second sentence could be uh i don't know the the moon and the second sentence could be, I don't know, the Cheshire Cat. So you know, I'll just put, and remember that this is very sort of notional. In reality, what would happen is you would probably have lemmatized these words, isn't it? You may also have removed the stop words, the common words like these, you may have removed from that. So you may have, it would be there for a matrix. So think of it like this. DI is analog, is essentially a vector, vector of tokens, isn't it? Or words if you want to think of it. It's a sequence of words. And therefore the corpus is an array or matrix. It's a matrix of words where each row is a document, isn't it? Are we together, guys? That's one way of looking at it. Now it gets a little bit more complicated because you don't want to represent just words as words. So now we come to more interesting and a more mathematical way of looking at it. So let me put it this way. We are seeing that documents are related to. So this world of documents are related to a small number of topics. So can I, for the sake of argument, take these three topics? What are the topics? Politics, sports, and STEM. Three topics, one, two, three. Politics, sports, and STEM. Do we all know what I mean by STEM? Science, technology, engineering, and math. Yes, exactly. Science, technology, engineering, and math, right? And these topics, they again fan out to gazillions of words. The vocabulary, the vocabulary, let's say M, words, making up your vocabulary, V for vocabulary vocabulary and if you you'll notice that i tend to use this word a very uh v vocabulary and what do i mean by this if you do does anybody recall when i imprison the v between two bars what do i mean by that in two bars. What do I mean by that? If you're okay, maybe so I should have emphasized last time. It is the size of the vocabulary. Cardinality. Yeah, the cardinality. Okay. Cardinality or another word is cardinal. So you know it's a choice. When we look at words, some words are more frequent than others. So when you're looking for words in a document, you can choose to keep all the words. If you keep all the words, then what happens is you may end up keeping words that are very, very common. For example, words, the things like this, these words are sort of like the glue that hold more meaningful words together, isn't it? They don't have much sort of a semantic, much meaning associated with them. They are part of the grammatical structure of the language. So very, very common words are not terribly useful. On the other hand, if you look at the word and their frequencies in a corpus, you take a sufficiently large corpus. So let's take an example corpus. Let's take Wikipedia. If you take Wikipedia, where each article is a document, where each Wikipedia, where each article is a document, where each article is a document, then so you would have, and you look at the unique set of words, you will realize that you will end up with millions of words. Some of these words are very rare and they probably show up in just an article or two because they are very specialized words. Isn't it? So for example, in what proportion of documents in your corpus would you see the word corpora? Only the ones that deal with natural language processing and things like that. But if you see the word corpora, it's a very interesting word. The moment you see the word corpora in a document, it immediately rings a bell that we may be talking about linguistics or natural language processing. Do you see that, guys? So somewhat infrequent words, they have outsized influence in helping us decide what a particular document is about. Right? So suppose you find the word, the words attention and transformer in a particular document, these words like there, what do you conclude about the document? It is about AI, isn't it? Right? Or you see very specific phrases like support vector machines and things like that, random forest, they do that. So these words are not extremely rare, but they are uncommon enough that when they are present, they nudge us quite a bit in cluing us or sort of giving us clues to what this document is about, what the topic of the document could be. On the other hand, if you find the word the, what do you conclude is the topic of the document? Can you tell whether it is about politics, sports or STEM? Nope. You cannot say anything about it. So very common words don't contribute anything. At the same time, you might encounter a word which is extremely rare. Sometimes people cook up a word, or the word is very, very rare. It occurs just once or twice in the entire vocabulary. But when it does occur, of course, you get some clues to it, and you like it. So now I'll tell you about a fact. It is called the Zipfian Zip Law. Or it is also called the Zipfian Distribution. So I'm going to give the Zipf's law in our field, in linguistics, in natural language processing in a simplified way. It will be, what does that mean? I'll eat up all the constants. I will write to just bring out the meaning of that. So it says that if you take a sufficiently large corpus, corpus, corpus, then tabulate word frequencies. tabulate word frequencies so every word will have a certain frequency isn't it if you remember this is practically one of the most basic interview questions people give when they do big data or something count the frequency of words word count isn't it so suppose you do the word frequencies, what you'll end up with is a table, word one, frequency one, right? So on and so forth, word n, frequency n. And suppose you put these in a decreasing order of frequency so that the rank of a word, rank of a word is where it stands with respect to frequency so the most common word has rank one maybe so you know the word the let's say would have a rank something like this first rank and the frequency would be very high right how often it has showed up in the document? Maybe one million times. Then if you look at the word horse, right? Far away is horse, right? And you get the idea, right? So what will happen is if you rank the words by their frequency. And this is a crucial thing, guys. Please get this right. If you rank the word with their frequencies, you will notice something very interesting. You will notice that the frequency of a word in the... So for every word, you have now two things. WI has a rank I and a frequency I. Would you agree? The frequency is of course how often it happened in the corpus and the rank is, how does it rank with respect to its frequency? How does it compare to other words? Rank one is the most common word. So far, so good. I hope, guys. This is simple, isn't it? Yes, sir. So then there comes an interesting rule. It says that the frequency of a word is inversely proportional to its rank of the word. Rank. Yeah. Up to a power s. Now, in the initial version, for simplicity, you can take s is equal to 1. You can take. So do you remember how 1 over x distribution looks like? All those distributions, they look like this, right? All 1 over something, 1 over x to the n distributions, they all look like this. Do you remember this, guys, from your basic math? You may have forgotten, but this is how they look like, 1 over x distributions or 1 over x squared x cubed. They all look some form of this, right? They all look like this. Do we remember that, guys? Yes, sir. Yeah, they look like this. And so when you look at this and you ask yourself that, so this is a very interesting observation. So the question is, what are most interesting words? Do most interesting words matter? Actually, they don't. They are useless words. Do most interesting words matter? Actually they don't, you know, they are useless words. So you call the most, the first few rank words, the stop words. Quite often you say, if you encounter them, just throw them away. Are we together? They are not contributing much in helping decide the meaning of it. So when a document is a sequence of words, go rip out the stop words, the most common words, and throw them away. Now comes the other question. Look at the, and by the way, this is the Zipfian law, Zipf's right? Now comes a very interesting, when it comes to linguistics like natural language, now comes a very interesting sort of a question. How do you represent a word? It is not okay to call a horse, it's a string, right? And machine learning does not deal with strings at all. What does machine learning deal with? Machine learning is a box. What is the input to does machine learning deal with machine learning is a box what is the input to a machine learning box it's a number it's a vector isn't it yeah it needs a vector so you need to convert your words somehow into vectors and how would you do that so there are many ways you can do that and now we are going to do that. I covered some of it in our fundamentals, but let's go over it a little bit more slowly. What you could do is, first you could say what is my vocabulary for every word starting with ard. What is that the first word in the dictionary I hope to some ZZ whatever word the last word is in the dictionary, you could write all of the words and to each word, let us say, and somewhere here is the word horse. So when you want to represent the word horse, what you can do is you can ask yourself, is horse, is this string, str, string horse? Is it aardvark? You say no. So zero. Is it this? No. Is it this? No. Is it this? No. And finally, is it horse? You say yes. So you put a one here and you go and put zeros everywhere else. Now you get a zebra, and once again, you'll put zeros all the way till somewhere here is a zebra. And then in the zebra, you'll put a one and then zeros everywhere around it. Are we together? So when you represent words like this, it is basically called one-hot encoding. Why is it one-hot encoding? Only one of, in this entire vector, everything will be zeros and there will be only one spot, one index in the array, one location in the array where the flag will be up, right, which will have the value one. So you see it's one, only one spot is white. All others are white. So this representation of the word, when you do, you realize that it has a couple of problems. The first problem is this array, this vector, So suppose I do the x vector for horse, right? And you ask what is the dimensionality of it? How many, what is the dimensionality of the x vector? Or what is the dimensionality of the zebra vector? If you ask, writing it this way, you would agree that the dimensionality is the same as the number of words in the vocabulary. Isn't it? Because you have to enumerate all the words in the vocabulary here. And then do one hot encoding. So, guys, this is a very simple, very important point. Do we agree with this yes right and in the vocabulary what you can do is uh suppose you you fear encountering a word that is not in the vocabulary you also keep a word other like uh some sort of a underscore let me just call it of a underscore let me just call it none or something like that you can keep a special token which is uh which is to reserve for a word that you haven't encountered right but you you can keep a vocabulary like this and now you're creating these vectors the trouble with this is these are large vectors so they're very huge vectors. And so you might say, well, you know, this is going to slow down the computation, amongst other things, it's going to slow down the computation. So you say, well, you know what, in our domain, let's say that you have a particular domain, which is natural language processing itself, like this subject. In this subject, there are many, many words that simply don't occur. For example, you would not be talking about, for example, things to do with the human body, you would not be talking about in linguistics, I mean, to you, those are words. you those are words but if all the words are taken from papers written on NLP then you would agree or NLP or anything on machine learning then you would agree that terminology that deals with the fishery or deals with cardiology etc. will not show up in your machine learning language, machine learning papers. Does that make sense or very unlikely to show up in your machine learning papers? So what you could do therefore is you could create a smaller vocabulary. vocabulary you can take a smaller vocabulary size of words that matter so you go back to your ranking of words here you see this ranking and what you do is because you have ranked the words by their frequency you chop off and then you chop off from the bottom also. Chop. Ignore. Very rare words. You can say, suppose I don't care for words that have happened just once or twice in the entire corpus. But then what happens is, because your distribution is Zipfian, see what happens is that again, it can come back to the argument. Your word frequency distribution is like this. And it keeps going on and on and on and on. It never stops on the right hand side, right? It never stops. It's a long and heavy tail. It's a long and heavy tail in some sense. It keeps going on like this. What you could decide is, at this point I'll put a cutoff. Right? And any word, all the words beyond that, you will not keep in your vocabulary. Likewise, you may have a cut off here, a throw away the most stop words. And so you may keep your vocabulary to this. Does that make sense, guys? Yes, sir. It is just capital advice. You may or may not do it. Some people don't, for example. Google, for example, works with a vocabulary of millions of words that I go. Most people work with a vocabulary, they're happy dealing with a vocabulary size of 10k to capture a domain. So a mileage varies and so it depends upon how much computational power you have and exactly where you put the cutoff. But ultimately you have to put the cutoff somewhere because the English language is the language that is forever growing. There is no end to it and words keep appearing. So you cannot have uh you know you cannot all the time be saying let me have every single possible word especially very very rare word may just be typos do you realize that right sometimes the the probability that some extremely rare word is not really a real word but a typ typo is pretty high. So, yeah, watch out for that also. So anyway, you take a vocabulary of a limited size. So now comes this problem. Look at this rule that I gave. The frequency of a word is inversely proportional to the rank of the word, isn't it? To some power s. Asif? Yes? When you mention vocabulary here as a terminology, are you referring to unique values in the corpus? Yeah, vocabulary would contain, let's say your vocabulary may contain only animal names. So it would be cat, dog, horse, crocodile. Irrespective of how many times it comes out in the corpus? Yeah, yeah. Like the speedy, like speedy the cow. Would that be too speedy? So basically what we are saying is the distinct number of words. Right? Not about their frequency. But then when you make a frequency chart in a corpus, you see this behavior. Right? By the way, this is a symbol of proportionality. If you want to make it exact, you can say that, okay, let me use the word I. I stands for the ith word. Some word I rank some I word. So what happens is that you can make it an exact thing. I is equal to. All you have to do is just, you know, divide it by sum total of all the words in the vocabulary. Rank off every word and you do this. This is just a normalizing constant. And that makes it an exact Zipfian law. But let's stick with this. Now observe this thing. This rule is very interesting. This is an inverse relationship. What happens if I take the log of both the sides? Log fi is proportional to up to some constant. See, this is some constant, right? Proportional to what is it? Would somebody like to enlighten me? What will this become? log of 1 minus log of s times log of the rank of the word. Would you agree? Something like this, right? Or actually, why am I doing something so complicated? Let me just make it simpler. It is easy to do something simpler. You would agree that the rank of this word to the power s is proportional to one over the frequency of the word, right? So, you can sort of get a sense of where this word is ranked amongst other words by looking at the inverse of the frequency of the word in the corpus. Would you agree guys? Yeah. Great. And see, suppose you want to get rid of this, this S power and all of that. And because this entire thing has a Zipfian distribution, see, you know, how do you convert a one over X to the N distribution? How do you linearize it? How do you linearize it? How do you linearize it? Suppose you get a relationship y is equal to x to the n. And it looks like this. How can you make it linear? Very easy. Take the log of y, and that will be equal to minus n log of x. Isn't it? And now if you plot, so this is x, y, but if you plot, let me use another color. If you plot log of x, log of y y what will it look do you see that it should look something like this guys it's it's a line yeah right because if you make this as y prime and this as x prime y prime is equal to this x prime is equal to this so you're saying that y prime is equal to this, x prime is equal to this. So you're saying that y prime is equal to minus n, some number, positive number, times x prime. This is the equation of a straight line. So what have you done? You have linearized the relationship. This is important. So what we are saying is, in some sense, to get a sense of where the rank of this is, take the log of the inverse frequency of the word. The inverse frequency of the word is log of ri. And let us say that we take s, s factor is there, but it doesn't matter. take s factor is there but it doesn't matter x is just a normalizing factor let's say proportionality is there so the if you want to see how common the word is remember what is the rank a measure of what is the indicative of what is the relative commonness of that word relative to other words isn't it so if something has a rank one, it is the most common word. If something has rank 100, it is a little less common, isn't it? Probably not a stop word or something like that. If something has a rank 1,000, it is fairly uncommon. So rank is a measure of, or an indication of relative commonness of a word. So far so good guys? Yeah. Right and so we so that brings us to a very interesting notion which I explained to you last time and I will do that again. So remember let me summarize where we are. We are saying that we can write the word horse using a zero zero zero everywhere and a one for wherever the horse location is, right? Zero everywhere. And we can write a zebra as 0, 0, 0, 0, 0, and one where it is the zebra, zebra's index, and zero otherwise, right? Zero otherwise everywhere. And this is huge, huge vectors, right, x horse, x zebra are huge. So what do we do now? We want smaller vectors. So one way is that we already reduced it a little bit and now what we can do, we ask this question that can we have a vector space, a better Can we have a vector space, a better vector space representation of a word in a document. So this is it. Can we represent a word in a document in a little bit better way? So hold on to this, whatever it is. So should we just assume that this is 10K? Size is 10K? Just assume it's 10K. The question we will ask is, can we represent the word in a little bit more meaningful way in the document, in a particular document? So you say, well, you know what, let's look at a document. Suppose the document is about, let's say that you're reading the document that we were reviewing last time. Attention is all you need. What you notice, the word attention is very common. Attention is frequent, like removing the stop transformer. It's frequent. But the word the and is etc, are also much more common than the word attention. So think about a paper or think about any document, or any sort of document as a sequence of words. If you really want to find out what this document is about, you want to find one naive way would be to see what is the relative frequency of the words and what words they are. So for example, if the most frequent word is the and is, what does it tell you about the document? Nothing. Nothing, right? So we realize that somehow frequency, so first let's do the, let's get the terminology right. A term frequency. Term means a word. Frequency. right, a term frequency. Term means a word. Frequency. People use the word term, but just think of it as a word. Word frequency is equal to number of times the word is present in a document. So we are trying to figure out what the doc seems to be about. This divided by total number of words. So this makes sure that you get a fraction. This is of course just a constant when you're comparing frequencies, but this is how you define the frequency. Number of times that the word is present in a dot divided by the number of words. Common sense, right? So suppose there are 100 words in that paper and the attention word has happened six times. That looks significant, doesn't it? Are we together? But there is a problem. You can't tell what a document is about by just looking at the very frequent words, isn't it? by just looking at the very frequent words, isn't it? You need some other measure of the importance of a word in a document. Just frequency won't do it because the word the and the word is will come and completely blow you out. Would you agree? Yes, you need to disregard them. You need to disregard them. You need to disregard them. So what you need is something that sort of amplifies words that are uncommon and suppresses words that are common. And so the way that you do that is you go back to your ranking business. You know this log rank business? Right? And you realize that the bigger the rank, a big rank means what? It is uncommon. A word with a big rank, let's say the rank of a word with respect to frequency, you know relative frequencies is 3000. What does it say about that word compared to the word the whose rank is let's say one? You would agree that the word whose rank is higher is more interesting a word yeah isn't it it is a more interesting word so let's bring in this factor log inverse frequency into our game so what we do is term frequency was a good beginning. The word people use is TF as a short form, term frequency. But what if we took the term frequency and we multiplied it with that log rank business, right? Which was essentially log of inverse frequency, right? But the frequency of what? Of the word, the way you do that is you do it not just across a word. So when you look at document, but you do it slightly differently. You say, how many documents are there? And how many documents contain the word, right? So you ask it in a more sort of a granular way, which is sort of the thing. How frequent is the word across the documents? It is the same thing. See, number of documents that contain a, maybe I'll use D or whatever, number of documents that contain the word divided by total number of documents. Do you realize that this is again frequency, right? It's a number of documents. Do you realize that this is again frequency, right? It's a form of frequency. In a way, it is a measure of frequency of the word, right? It's the relative frequency of the word, right? How frequent is it across documents? And so 1 over FW would be N over NW, right? And the log of it, so we were saying we want the log of the inverse frequency. And so log inverse frequency is here. And we are doing it over documents. We do this traditionally over documents, right? So it is basically saying that what is the rank of the word in the vocabulary? Are we together guys? In some sense, in a very rough and ready sense, we are saying what is essentially the uncommonness right, of the term. In some sense, that is what it is. And this is the frequency of the term in the document. Frequency of term in a document di, right? So what we are looking at is a tf, and there is a fancy name for it i'll tell you about it it is of a particular word wi in some document uh w some word w in some document some document right so in the document you have a term frequency of that word and then that word across the corpus how frequent it is one easy way of getting it is looking at the inverse document frequency this is the word called inverse like if you look at this the way I've defined it what is this document frequency right this is document frequency, right? This is document frequency. And so inverse of document frequency would be this. And then the log, why are we taking the log? Do you remember? Because of the Zipfian distribution, it basically makes sense to not use the rank, which is just to linearize it, use the log ranker because it sort of linearized things. Otherwise things, the scales are very different, right? So this is it. And so the word for this is tfidf, tfidf. Of the word, now I'll attach the subscripts, of the word i in the document j is equal to the term frequency of the word i, right, in the document j and the inverse document frequency, right, of just the word itself across the corpus. Right? So the first is a function, the term frequency is a function. It is the relative frequency of the word in this particular document, in this particular sentence or paragraph or whatever. Whereas this is a more sort of a global thing. How prevalent is this word across the documents? Like how many documents is this word across the documents like how many documents contain this word do you see that guys so you can also say how rare is the word uncommonness or rarity of the word or the interestingness of the word so now what happens think back at your situation what will happen the word attention let us say the word. So now what happens? Think back at your situation. What will happen? The word attention, let us say the word attention happens six times and there are 100 words in the document. D, in the document, there are six. So what is your term frequency? Six. Yes. A term frequency of the word attention in the paper, that particular paper, in that paper is 6 over 10. So remember, the word may have an entirely different frequency in other documents, but in this document it is is that, but now when you this isn't now the term frequency of the word the. Raja Ayyanar? In the same paper. Raja Ayyanar? Let me just put a number here for the sake of argument, let us say that it is 25 over 100 so if you only look at term frequency certainly certainly this document seems to be about the, isn't it? But when you multiply it now with the rank of it, the rank of attention in the, which, you know, the log rank of attention or which becomes the inverse document frequency, the uncommonness factor of attention is much higher. Do you see that, guys? Right? So let us say that you multiply this with uncommonness factor of this, let us say, is 100. Right? And how do you do that? It's. You go, okay, let's just be very precise. Let's make a guess. We are looking at inverse document frequency, right? So in how many documents do you find the word attention? Number of documents that will have the word attention over total number of documents. So suppose you pick 1,000 random documents in machine learning. How many of them will talk about attention? You would agree that very few will talk about attention, isn't it? Because machine learning is a vast subject. It has computer vision, it has structured data, it has medical data, it has whatnot. Do you agree? Agreed, right? That frequency and so suppose let me just arbitrarily say the number of documents is thousand. So guys, take a number, the number of times that the word attention happens is, should we say one in hundred, there are ten documents about attention is that a reasonable thing you can therefore say that frequency of the word attention the document frequency is equal to number of times you have attention times divided by thousand and it is equal to a 10 over 1000 is equal to one over 100. Right? So this is here. And now when I invert this, so therefore one over F attention, this is equal to what? This is equal to 100. And when I take the log to the base 10 of this, one over attention, let's take base 10 because it's easier for us to kill. What will this be? 2. On the other hand, actually, I would say that 10,000. Let's be more generous. Actually, attention word is not there in 1% of the machine learning literature. It's more like 0.1% of the literature. So I would like to put it like this. Now, look at this. In how many frequency of the word the, how many documents would contain the word the? All. All, right? So a thousand will contain the word this that becomes so one over the one over f the would be equal to zero no one right okay and then oh goodness And now you take the log of this, log of 1 over, which gives you a sense of the rank, log rank, is, what is the log of 1? Zero. And now when you go up and you say, oh goodness, the tables have turned. This is multiplying it by 3 and this multiplying it by 0. Isn't it? So now when you multiply these two, this will become 18 over 100 and this will be zero. So now term frequency multiplied by inverse document frequency, what does it give you about a word? This seems to achieve our goal, right? It shows how important this word is in the document. Do you see that? It gives you the importance of the word, statistical importance of the word, in the document, right? And thus the concept of TF-IDF, right? So remember I explained TF-IDF to you in the past and I used a slightly different argument to explain it. And today I'm using a different argument. The reason I use different arguments each time, hopefully is that the idea gets in our mind. So TF-IDF therefore, a measure of interestingness, word in a document. And remember TF-IDF now, if I may use this idea, it's a rather big thing. It is a word in a document J. Right. But now you ask yourself this question. There are 10,000 words possible. Right. So suppose I write this document J. Can I talk about the TF IDF, something with respect to the document J such that for every of the 10k words that are there, horse, attention, transformer, zebra. Now you realize that for each of them, I have a TF IDF value of this word. You realize that, right? There is a TF IDF value associated with to every word in the vocabulary, except that there will be, most of the words will be absent so there will still be a lot of zero values so for example this would be some value what did we compute it to be 18 over thousand 18 right there's zero zero zero zero and then transformer would be uh present and then something like that you know there would be something then transformer would be present. And then something like that, you know, there would be something maybe transformer would be 12 over 12 with 100 and so forth. Does that make sense guys? Like when you do the TF idea for all the, for the entire vocabulary, then every document can be represented. And this is an interesting thing. We can create an array, right? Do we agree? We can create an array, a TF-IDF array with respect to our document such that each place in that array represents the TF-IDF of the word, that particular word in the document. Right? And this representation is called a TF-IDF vector. It's a TF-IDF vector, right? Offer document J. So far, are we together? So far, are we together? Okay, what I have done is not something magical, straightforward, but I'm gradually taking you on an intellectual journey here. See what I'm doing is these things in the book are written in just two, three lines. You're given it to it as a fact, just get used to it. I'm just trying to motivate the ideas behind it. Where do these things come from? So we started this session by saying that we will be doing topic modeling, but even before we do topic modeling, we need to do first a vector space representation of words, because the three core entities in topic modeling are documents, topics topics and words right document is simple document is a sequence of words or tokens and tokens are basically words and then punctuations roughly speaking words numbers and punctuations whatever it is right more generalized way of looking at it and some more things perhaps i don't know what i'm missing right a beginning of this beginning and the end and end of sentence and all those things put together but roughly speaking in a very colloquial way a document is a string or a sequence of words but then what is a word word cannot be considered a string we need a vector space representation of a word so then we go on to the journey of a vector space representation of a word. So then we go on to the journey of a vector space representation of a word. But before we go that, at a very high level, we realize that, see, when you have a document, it is partly belonging to this topic, partly to that topic, partly to this topic. Documents are not pure. It's something to remember. Sometimes they are, but quite often they will also contain a little bit of impurity. It will have little bit of some other topics also mixed in, always. Research papers of course tend to be very pure in the sense that the whole document will be about a stem field document, it will be about stem field and so on and so yeah. So then we introduce the notion of a corpus. Corpus, simply put, you can think of it as a matrix of documents, array of documents. Are we together? Where a document itself is an array, is a sequence of words, and therefore a corpus is a matrix of words. But then the question comes, well, okay, so even before we go there, so we realize that, see, documents are plentiful. Vocabulary, which is the set of acceptable words in a language in your machine learning exercise, should be a finite set. Question is, how big should that finite set be? And that is an interesting question because that looks us to the discussion of what words are English words for example if you take English and then you realize that the words are actually gazillions right where gazillions is I believe as of this moment three to five million right and. Then you realize that if you do a one-hot encoding representation of those words, like here, hard work, you start with, you have a long array, and every index of the array belongs to one word. Then you can do a one-hot encoding of the words. When you do that, you have a dauntingly large vector, a vector that has 10,000 dimensions. Right? And so amongst the other issues that we will discover with such vectors, one obvious flaw is that it is huge. Right? But we will take it. We won't disturb the fact. Someday we'll address it. One more thing. What you do is you then try to shorten the size of your vector. So one easy way to do that is you throw away the the words which are stop words. Right. And this and that you throw it away. But now I'll give you another way to throw it away, but let's say that you keep it. Then you throw away very rare words also, things like that, right? So you can decide, you can rank the words by their frequency. When you rank the word by the frequency, you get this famous Zipfian distribution. These distributions are the Zipfian, the famous Z ziffian distribution these distributions are the ziffian the famous just ziffian distributions that have power law behavior this is your ziffian distribution right now as if in distribution is one so in the space of uh natural language processing what it says is that the frequency of a word is inversely proportional to the rank of the word, potentially to the power s. And that s doesn't matter, so just to reason through it, in your mindset s is equal to one and you'll be able to reason just fine. People have all sorts of terminology. For example, one over R directly, some physicists if you're talking to, they will start calling it the gravitational distribution or something like that and so forth because the gravitational potential goes as one over r. So we won't get there, but occasionally you hit upon literature, even in NLP, and suddenly you find, okay, the distribution looks gravitational. The first time I encountered it, I was like, where did suddenly physics come in? Then it turns out that it's just a word, it's just a notion that people use sometimes. I don't. So it is some power of that. So the frequency is inversely proportional to rank. Rank is the relative commonness of the word with respect to other, right, when you sort it by frequency. So now comes the interesting part. You can take all the words in the right rank, rank ordered, and you can throw away the initial, the stop words, and you can throw away some very rare words. And you have a much more limited vocabulary. You still have this issue that this is a curve distribution. What it means is some values will be significantly larger than the other. It is, you know, the scales are very different. So one way you can have a much more meaningful scale is to take a log transform. When you take a log transform, you have a linear scale. And linear scales are good when you compare things. It's always nice to have a linear scale. It makes comparison more meaningful. So you can take, when you do that, the Zipfian, you'll get a log rank as log inverse frequency. And that brings in the concept of inverse frequency. But inverse frequency where? We'll come to it in a moment. Now let's go back to our document and ask, given a word, so we are still representing the word with one hot encoding. That's all right. We may have shrunk our vocabulary. Our vector space is now only 10,000 dimensions. Let's say you took a vocabulary of 10,000. Now comes this question, given a word in a document, how interesting is that word? So one easy way, like how relevant is that word to the document? Or how much does it tell? What this document is about? One easy way is you can say, well, let's count which is the most frequent word in the document. If it talks about horse, horse, horse, horse, it's about horses. There is a catch to that. When you try to do that, you realize that much more common than the word horse in an article that is genuinely about horses, much more common than the word horse will be the word the itself, is, and so forth. So they will have, they'll be prolific, they'll be everywhere. So we need to find some other measure. Frequency is a good start, the relative frequency of the word compared to other words in the document, but we need to go a little bit further so then what we do is why don't we multiply it by the this thing the log rank the intre the interestingness of the word or the the how the bigger the rank log log rank the more interesting that word is i'm more likely that if that word pops up in your document, your document is about that word. Does that make sense? You mean smaller than rank, right? Not bigger. Smaller rank will have a lower frequency. No, no, no. Rank is an integer. Rank one is the most highest ranked, in the sense that the most sort of frequent word. A rank thousand is the absolute value of the rank. When we say rank, right, in common language we mean the opposite of the mathematical value. That's that. so the log rank gives you a sense of the interestingness or the uncommonality of the word so for example i'll give you an example if you encounter the word heteroscedasticity in a document you guys have taken my course what is that document likely to about? Machine learning or classification or regression. Yes. Time series analysis. Yes. So it would be basically clearly about machine learning and now you're looking at some form of regression time series. You're basically looking at regression in some form. Right. So You're basically looking at regression in some form. So important. That word, that one word pretty much already convinces you that you're looking at a document which is probably about heteroscedasticity because you don't find the word heteroscedasticity littered around, let's say, a history book or let's say, you know, a cardiac surgeon, maybe they do some analysis these days, or something like that, a history book or a novel or something like that, a music book, you wouldn't find that there. You would find it only in machine learning literature or some applied machine learning literature. So that is the value of the uncommonness, uncommon word, the measure. So now let's go back and say, all right, so I had the word attention in a paper. It happened relatively six out of 100 words for attention. And when I multiplied by its inverse document frequency that essentially law grant in some sense, it has a value three. And on the other hand, what happens to the stop words, the? They have an IDF value of zero. Isn't it, guys? Their interestingness of the word, of stop words, of common words is zero or close to zero. Do you see that, guys? Why is that? Let's say that the word the, the happens in 100% of the document. So it's relative document frequency is one, log of one is zero. And therefore the IDF, which is a measure, and so therefore we come to this result, the TF-IDF is a good measure of how much the word is telling you about the document. So it is made up of two things. The relative frequency, the TF part, the relative frequency of the word in the document, which tells you how much the document needs to take this word seriously. But you need to weigh it with the interestingness of that word itself, isn't it? Which is the idea factor, right? Which roughly maps to your log rank in the word distribution, right? And that suddenly makes sense because it automatically throws out the stop words, the is, has, and all of that. You see that? And so now we have a good measure. You can just look at a document and look at the words and you can rank the words by their tf idf value and you will immediately come to know what this document seems to be about at least in a very rough and ready manner do you agree guys yes and by the way do you notice that we have not done any deep neural networks or any of those things here? This is just purely a statistical argument so far. Now, taking this argument forward, if every word in the document has a tf-idf value, then you can create a tf-idf vector for the whole document itself by again going to the word vector all the word you know the representations and for every word you can put you can put the tfid of value there rather than the frequency value value of one hot encoding value right so now we learn three things let me just mention the three things. One is word encoding. One hot word encoding. So this is like horse becomes wherever the word horse is supposed to be. One, let's say zebra, zero, all the way 0. Then if you have the word zebra, you have 0 all the way 1. So this is the word vector. Not a terribly useful one, but good enough. But then suppose you want to do in the context of a document. A word WI. Now you can have another vector. So suppose you have the word horse and you can have the so-called count vector let me just make a halfway station count vector is the same thing you take the word vector and you ask how often did the word frequency a horse came let's say that the horse was mentioned 11 times right and the cat was mentioned dog was mentioned three times and zebra was mentioned one time so remember now we are talking about document dj this is the count vector for the document in this document the different words horse has happened 11 times zebra has happened 11 times, zebra has happened once, dog has happened three times. So this is your count vectorizer. Very simple idea. Are we together? Are we together guys? This simple. And then lastly, the next concept that I start here is a tf idf of the document j is basically that in this document j you have the tf idf of every word word one with respect to document j tf idf okay i wish there's a symbol that actually can i represent this with a phi because i hate writing it again and again. Phi 1, phi word 2 with respect to this document, phi word 3 with respect to this document j. And so what did you end up with? You end up with another vector, and this phi dj is the tf idf vector for the document in this corpus are we together so you have managed to represent a document with a vector so you have managed to represent a document with a vector tf idf vector you have managed to represent word with a tf idf vector that's why not with a vector tf idf value a word has a tf idf value isn't it and so you can have a word which has a one hot encoding one hot encoding and a tf idf value with respect to this document dj right of the word actually forget this i the word i with respect to document j you can do a one hot encoding of the word and of course you can do the W-I-D-J-F-I-D-F. Let me just use the word T-F-I-D-F, let me not confuse it. Of the word in the document. On the other hand, for the document, for the document D- the other hand, for the document, for the document dj, we have the tfidf vector, which is different. Remember tfidf for a word is a value, isn't it? Like for example, 18% for the word attention. If you remember here, we found this number, right? 18% for the word attention and zero for the word the and so forth. But for a document, it is a vector. So I will stop here because now I want to start the crux of the thing, the topic modeling aspect of it. So we know how to represent words. We know how to represent documents as a vector. We can even do count vectors. So I taught you three ways, just word encoding, then the document can be represented as a count vector, or it can be represented as a TF-IDF vector. All of that is good, but we still have to get to topic modeling. And I noticed it's time for a break. You guys have been with me. Let's take a break. Today's an interesting topic. All right, guys. In the first part, we talked about the vector space representation for words for documents. One hot encoding for words, then we talked about the concept of tf-idf, we talked about the Zipfian distribution or Zipf's law as applies to the word frequency and rank, then we use that to motivate the definition of tf-idf. Then we talked about vector representation of a document using the t-IDF, the TF-IDF vector, the count vector, simpler version, and so forth. So now we're going to do something else. Suppose we ask this question, what is the relationship of documents to topics? Topics are themes. They have semantic meaning, you know, those words, those documents have semantic meaning. Can we uncover the topics? So when you try to uncover something that is not as a stated fact before you, you say that you're trying to uncover something that is there, something that is hidden or latent. So, which is why quite often when you do this, you often see the word semantic latent analysis. Semantic means the meaning, the latent. The latent means hidden analysis. So people often use the word SLA and you will find the word SLA used in the NLP literature. Whenever it is used here, it obviously means semantic latent analysis. So now let's again review what we were saying. You can have many documents, D1 to DM, and then the vocabulary can contain many words. And this is the size of the vocabulary. And we agreed that let's take a vocabulary, let's say approximately 10k realistically. You can throw away the stop words, you can throw extremely rare words and so on and so forth, right? And then we of course have a vector space representation for words, a one one hot encoding we have at least two vector space representation for documents either the count vector or the tf idf vector right but now the question is we have this as reality a given a word a document will be in many words a document will will have threads like this, many, many words. And this would have, again, a lot of words will be present. So there will be a lot of, you know, this D, J. There'll be a lot of interaction, isn't it? Given a document, it will have some of the words from the vocabulary and given another document it would have a other set of words now you could look at it backwards and say given a word it is present in multiple vocabulary multiple documents so that is the stated fact but if you ask this question in a slightly different way and you ask why, why is that word present in the document? Why does it make sense that this word should be there in the document? That's a little harder problem to answer. Why is it there? And the way we of course do that, because we know the way we reason it is, we say, actually, this is not the relationship. The relationship is actually not this complicated, huge entanglement of lines there. It is there, but it is actually through one level of indirection. So what you have instead is dj, dm, and then you have the words w1 and let's say wi, wn. you have the words w1 and let's say wi wn right and what you have is a small number of topics so uh three of these uh basically three and so what happens is that a document each document has a certain degree of each topic these are the topics. Think of these as, and when we talk of topics, we can talk, like we said, what were the topics, STEM, sports, politics. So each document may may have different levels of coloration. It may be colored to different degrees by the three topics. Likewise, the words, if you take a word, let's say, let's take a representative word. Suppose I take the word election, which is quite topical today. You would say that election is certainly has to do with politics. Then you think a little bit about it. And then you realize that, hey, the word election is often used in computer science, for example, there are all sorts of election algorithms. Right, for example, leader election algorithm and distributed systems and so on and so forth. So the word election is not only found in politics. It's also found in STEM fields, but obviously with a meaning that is entirely different from the meaning that belongs to computer, that belongs to politics. Likewise, there may be some utilization of words in sports. I don't know the word election in sports. Much more rare, perhaps, but dominantly in politics, somewhat less, much less in STEM fields and perhaps very, very rarely, if at all, in the word, in, for example, let's take this word election because we seem to be all about elections tomorrow. So you realize that even a word like that is not just about, let me color this something differently upside I'll color this as differently sports and let's take a different color for STEM field. STEM. So you can say that with reasonable sense that let me give a sort of weight to that. You would say that this line going from the word election to politics would be much darker. The line going from there to sports is probably very thin, very tenuous. Maybe I'll give it very rarely it's present. It will be something like this or this. So let me give a little bit of a thing here, a very thin line as you can see at view. And maybe for the red one, I would draw this word, this line that is, let's say something like a little bit more, let's say this. Little bit thicker line. So I hope the thickness of the line conveys that the word election is far more, contributes far more to the topic of politics or is associated with the topic of politics than with the topic of STEM fields, isn't it? And even less to sports perhaps, right? Just throwing it in the word and we can throw other words also but see each word has different degree of association to the three topics and from there we can do this at the same time each of these things we we do uh associations likewise given a document dj uh this document if it is if you're reading today's newspaper, for example, online, you would realize that most documents today seem to be about politics. Right. And so when you see, and then as an example, and then to certain degrees, it might be, maybe it is also a lot about, for example, Maybe it is also a lot about, for example, suppose you're looking for how politicized sports has become for whatever reason, right? Then what happens is you might suddenly find that there is a pretty significant line going from here to sports also, right? And a very thin line going from there, the document is totally not about STEM fields. So it might be some relationship which is very tenuous. So I hope I'm conveying the picture here, guys, isn't it? So that's how it is. And some other document may be a lot about science and so forth. So what we have done is we have taken this mess. Look at the picture above, which is a huge mess of quadratic connections, huge number of connections, right? And we have simplified it by bringing in latent variables, a latent space, a latent thing called topics. And we are saying that the relationship between words and documents are indirect. Documents are about topics. And specific topics have a certain frequency distribution of words. Am I making sense, guys? Have a certain degree of certain frequency of words showing up. For example, this topic will contain these many words but not those many words. For example, very rarely do you see, let's say, a topic of politics and suddenly somebody mentions heteroscedasticity there, right? Or starts talking about, I don't know, kernel methods. about uh i don't know uh kernel methods but god forbid i hope politicians don't discover that so uh the sort of the thing that is it so we will look with this level of interaction but then comes the interesting question now now wait a minute how do we do a mathematical representation of this idea? You realize that these are vectors. We have a way to represent document as vectors. We have a way to represent the words as vectors. Now, once we bring in topics, what does that mean? How does it change the game? I can't represent document directly in terms of words, which is what we were doing using the tf-idf vector and so forth. Now what happens is I can write a document j, let me take a document j, as some something of the three topics. For example, it may be this document J may contain 0 let's say, it may contain what, 0.29, is it more or less, 0.29 of STEM, right? So what have I done? If you observe initially the document in terms of words, so in terms of topics, let us say, in terms of topics, let me just use white because we use red for politics. If I represent the document vector in the topic space, it is just a three-dimensional vector. Do you see that? But the document vector vector the same document vector actually how should i write the word topic here okay the same document vector in the word space you remember for example as a tf idf vector was and in fact if you remember we use this word phi dj was a vector which was 10k 3d dimensional and this is 10k dimensional vector where we had the tf idf value of each of the words where we had the TF-IDF value of each of the words, word I in the document J. You remember this Geyser, that this is what we were doing. So far, are we clear guys? Because I intend to run a little bit faster now. So far, is it obvious that this document now used to be, in terms of words, if I write, it is this long, big vector, a 10,000-dimensional vector. But if I were to write it in terms of topic, we are talking about a tremendous simplification. You're saying, any document, I can write it in terms of just three vector. And except that, that obviously I need to now know the other side of the story also the topic the topic vector topic k it for each of the words how how frequent or how important is that word in this topic right for all the doc if you only take documents of this topic let's say you only take all the sports documents then you can now talk about tf idf of a word in sports are we together right and here the notion of the document is all the documents but like basically you're looking at it in the space of uh sports, right? So every topic can be represented in such a way. In a very rough way, we are taking all the documents in the corpus that belongs to sports as one big document. And therefore we can now start talking about, you can do something, you know, something you can put, which is some function of the word wi in this, right, and you can represent it as a 10k word, this thing. So from the word you can have, and every word itself can be represented, if you look at a word in the topic space, again, word vector, the reverse way way you can write it as a topic word i can again be written as you know a three-dimensional vector why why can a word be written as a three-dimensional vector in the topic space could somebody enlighten me because there are only three topics yes yeah so so you know the word is likely to show up very much in politics or something in this and something in this, right? So once again, a three dimensional. So you realize that if we could somehow discover topics, we could represent both the documents as well as the words in much lower dimensional space. So now the question is this pursuit of topics. There's a whole theme in this semantic analysis. You go ahead. Asif, one quick question there. So in the depiction of the problem statement, you're making an implicit assumption of mutual exclusivity for the topic set. Yeah, by definition, you expect that the topics are orthogonal. By the way, I sort of may have got latent semantic analysis, LSA. It is the other way around. I was just thinking I'm saying something wrong today out of tiredness and this is what it is. It is LSA. I think this somehow sounds more familiar to me. Okay, LSA, latent semantic analysis. You find the word latent semantic analysis. I just reversed the order of the word found in literature. Yeah, but yes, frame read. The idea is that the topic should be in some sense orthogonal to each other. Are we together? Okay, by definition here. By definition. And that brings us to a very interesting idea. Now I'm going to relate it to a set of methods of increasing mathematical complexity. The first method is how do you do this so-called dimensionality, what you're doing is you're doing a form of dimensionality reduction. You're representing words and their sentence and the documents with much lower dimensional vectors. These hidden vectors, these latent variables are the latent vectors are the topics. Latent variables are the latent vectors are the topics. How do we do that? You see that, right? So there are a couple of ways that you can do that and deal with the topic. Sometimes you don't have to really discover the topic but you can be told the topic and your problem is to find whether a document belongs to this topic or that topic. So that is the simplest situation. And it represents supervised learning. I'll give you an example. Consider spam detection. So this, of course, would be a supervised learning, right? Why supervised learning? Because you will get like the input would be some text, right? And they're selling you, I don't know, millions, Nigeria relative uncle. So you all must have gotten offers. You all, I'm sure, have an uncle in Nigeria who has left you millions, right? So you have spam. So you can have yes. And then here is a thing that, let's say that there's an email, hopefully from support vectors, vector, notifying you of something. And hopefully it's a no, it's not a spam, right? You want to write a classifier that doesn't call it a spam. Are we together? So one way to ask this question is, we can treat spam versus non-spam. Somebody says it's spam. How do we do that? It's easy, we have training data. This is the dataset training, right? And actually data, because I'm using D for document. Let me not use this. Let me just use the word training data. Suppose we have that. What we can do is we can shatter it into two parts. Only spam data. All the rows where the label is yes, yes, yes, yes. So you take only text, all the spam text, all the spam documents. And then you take all the other non-spam or ham documents. No, no, no, no, no. So all the rows here are non-spam documents. And what can you do now? What you can do is you can hypothesize that they, that, let's look at the word vector for each of these. You take the word vector, W1, W2, Wk of only the spam. What you do is in the word vector space, they'll fall out. You just find the average of all of these vectors. What will it give you? It will give you just find the centroid of spam, spam word vectors. Remember we did the one hot encoding and all of that. So you do that or you could do that Remember we did the one-hot encoding and all of that, right? So you do that. Or you could do that, or better still, actually, you could do the TF-IDF vector for each of the documents. So you take, actually, that would be better. Let's take that. Centroid of the TF-IDF vectors for each of the spam docs document. So you take the spam documents in the TF-IDF and in this space, they will all hopefully fall close to each other and you create, you find a centroid. And then you do in the same space space you go and find the other documents, which are good documents, which are non spam or the ham and you create a location for them. of non-spam vectors in the TF-ID space. Are we together, guys? Right? So in this space, which is a very high-dimensional space, now you have two uh vectors one vector is for spam the white vector is for spam and the uh the the green vector is for non-spam the centroid the vector going to the centroid from the origin to the centroid we end up with two vectors are we together guys and now you can do a classification very easily because all you have to do is let's say that you want to determine a random vector let me take a yellow line here suppose you have a document that is here right and so this is going from here. And so this is going from. So now let me call this theta one and let me call this theta two. So if you want to decide whether it is spam or non spam, just visually looking at it, what would you say just look at the angle just look at the angle if it is closer to the spam centroid it is spam if it is closer to the ham centroid it is ham easy does that make sense guys? Yes. Very easy. Just do a dot product between the vectors. So suppose you have a document vector, some query vector document that you're looking at, document query, and all you need to do is do a dot product with the word vector for spam, sorry, document vector for spam, spam centroid, right, and obviously cosine, let me just make it, just to keep the magnitudes there. And you can do a cosine of the query vector with the non-spam centroid. And whichever is there, you can therefore classify it. Now, those of you who have been taking machine learning with me since ml 100 do you remember what this reminds you of clustering and this is a classifier remember in classification what is this method of doing classification you say the k means yeah no so imagine what you're doing is you're doing a line here joining these two and you're finding a decision boundary which is a perpendicular of this line what is the word that you use any vector on this side is spam any vector on below this red line is that so this is this effectively is your decision boundary the linear discriminant analysis excellent so this is nothing but your linear discrete and analysis so you must wait a minute I have heard that the word LDA in NLP stands for something else. And so it's a little bit, it is true. Actually this word is overloaded. What people mean with LDA. So LDA in this, this is LDA and also something we are going to learn about, which is much more sophisticated. It is called latent Dirichlet Dirichlet Okay, I'm getting my spelling all wrong. Dirichlet Dirichlet Allocation spelling all wrong, direct let allocation. Fix my spelling, Dirichlet. Dirichlet. So you just say Dirichlet allocation. That too is called LDA. So obviously it's confusing which one you mean, and most of the time people mean the latent Dirichlet allocation. Now here's a little fact guys. Do not throw away the simple algorithm that I have here. Surprisingly when you are doing this kind of a topic modeling and you already know what the topics could be or things like that, a linear discriminant analysis is extremely cheap a because all you're doing is cosine distance with the you find the centroid and you're just doing a cosine distance cosine computations are linear computations right order one computations not even linear they just order one computations isn't it guys you do the cosine with the spam centroid you do the cosine with the this centroid excuse me my pen fell with the non-spam centroid and then you're done you have already been able to tell whether it is spam or not and so it is it is actually uh you'll be very surprised how often this gives you results comparable to or better than sometimes some of the state-of-the-art modern newfang methods right again goes back to something i taught you guys once the no free lunch theorem just because something is more complex or more recently discovered doesn't mean that it will always give better results. So don't ignore it. This is rarely talked about in books, but it is worthy. In fact, one reason that I recommended you the green book is because it's, to my knowledge, the only book that I read on NLP, at least that I read, which actually used this basic algorithm. It is very cheap. All you do is, for each of the classes, find the centroid and then do dot products with it to find it. So that is the latent discriminant analysis. So that is easy guys. We understood that. Then comes another method which goes back to a different idea. Remember that I taught you guys at some point recommender systems and things like that. We had talked about some magic word called singular value decomposition. decomposition. Singular value decomposition. It's a mouthful, SVD. Now we won't go into SVD, those of you who took math with me, the math of machine learning, of course we have covered it in a great detail, but today I'll give a simplified version of it. Now singular value decomposition is actually a crown jewel of linear algebra and it is surprising how ubiquitous it is in machine learning. A lot of the machine learning algorithms can be mapped to the singular value decomposition and we are going to do that. When we apply singular value decomposition to our problem, this algorithm is literally this thing, the latent semantic analysis, LSA, or semantic latent analysis. Today, I don't know. I can't believe that I forgot what is the order of the word. But anyway, one of the two. That is what it becomes. So what does that mean? So once again, take all these document vectors, DI. And you ask this question that let us, we don't know. Now the way we ask this question is in the previous case, we knew the topics, spam, non spam, but I'm asking you this question we don't know the topics but I am saying go find three topics in the document how would you find three topics in the corpus of documents so you know you have documents D 1 D let me just say J because I've been using DJ DM in this long corpus of documents. What just happened. Sorry. In this long corpus of documents. What presentations actually sorry I think I pressed a button. Yes, and What happened. I don't know. Give me a second, guys. Yeah. Okay. I apologize. So, given this long corpus of documents, all we are told is, can you discover three topics? So the question is, how could you discover the topics? There is a trick to it. Now look at it and so I'll motivate it first by giving an example that I have given in the past to some of you here is that see if you think of a document as as a user. So you remember that user one, user two, user n, and then you have items that he's purchasing. Item one, item m. Let me make this m and this n. You remember that recommender systems are based on this, that a user i has purchased a user j has purchased item i right and this is the rating at this moment this used to be the rating matrix if you remember who covered this in ml 200 if you have forgotten this all right and how did we see that we asked this question why did this user purchase this item? And the way we answered that is we said, you know what? There are some hidden traits. Let's take three hidden traits. Every item in the shop has a certain amount of those traits. And then every user can also be represented with. So these are for the items, these are the traits. And for users, these are the taste for the traits. Taste for how much taste it has for each of the traits. Right? So for example, let's say that the item is food. So this is spicy is one of the traits of that item. Expensive or pricey. Spicy, pricey and what else can we do? Health value, how healthy? So you realize that if you're looking at food, spicy, pricey, and healthy are probably two, three attributes of any food item. And a user may have a certain degree of taste for spicy, a certain tolerance for spice, for price, like what is the price range that the user will accept and how healthy the food item is and that may determine the purchasing habits that may determine whether this user purchases this item or not do we remember that guys so what did we say is these were the latent latent factors. And the way we discovered the latent factor was to say, was to take this matrix, this big matrix M, and say that M can be decomposed into something very interesting, U diagonal V transpose. This is the so-called singular value. And I'll just move fast over it. Singular value decomposition. So what it, and I'll give you the intuition behind it. What it basically said is that we can represent each user in terms of just these three attributes and we can represent each item again in terms of just these three three attributes right so you can the word we use is truncated and i'll come to it why we use the word truncated in the singular value decomposition so now this is reminiscent of it if you think of the users to be documents and items to be words, you can say that the words have certain underlying meaning, right? And their meaning here being the topics. They are associated with some core topics and documents also are about certain topics. Do you see the relationship between sort of in a way the recommender system and this document to a word relationship guys? Do you see the analogy here? Yes. Very direct analogy, right? And in a very similar way, you can think of this latent factors are your topics. You can think of this latent factors are your topics. If you make a, in other words, in the recommender system, if you think of it as a recommender system problem, collaborative filtering problem, you can think of the documents as the users, right? And the things that the users have or purchase are the words, right? So a sequence of words in a document are effectively the things, the items that that document has in quotes preferred. And the reason it preferred is because it was about certain latent factors. So when you reason like that, the mathematics that comes involved here is called the singular value decomposition. Now singular value decomposition is a way to break up, decompose a matrix into smaller matrices and it has a ring of understanding the matrix. So when you get a number like 21 and you are told, oh, it is three times seven, right? You break it up into prime factors. You feel that you have understood that number 21 in some sense. You know what it is built out of. In the same way, when you get matrices in machine learning, generally you try to do a singular value decomposition to get an underlying sense of what is happening, especially if the rows and columns of the matrix are different entities, the singular value often has a story to tell. And it sort of leads you to those latent factors that may be at play causing that matrix to happen and not some other values to happen in the first place right so the same argument we'll use here but i won't use the decomposition of a rectangular matrix life is simpler when we think in terms of a square matrix right so let me take a square matrix and show you what it means. You see, suppose you take data like this, and I will draw this, a very quick one, and this is, I don't know, oh my goodness, it's 10.30. So obviously I will keep some of the arguments for next time. Suppose you get data like this what do you observe about this data guys looks nice and linear it does look nice and linear isn't it and And you can sort of shrink wrap it in an ellipsoid. Are we together? And now, this is, in fact, that linear relationship is the regression, is the correlation. Relation is the degree the more ellipsoidal it is the more there is a presence of correlation between x and y right x and y are correlated on the other hand if you look at this picture oh sorry let me put another color Actually, let me put use red for data. Suppose I take this sort of a situation. Is there a correlation between X and Y in this picture, guys? No, not really. And so you ask this thing, what in some sense this is noise, no correlation. This has a relationship, y has a linear relationship. So geometrically, if you look at it, there's something very interesting. A noise is isotropic. It's sort of, well, mine didn't look like a circle, but basically it's isotropic, right? It's uniform in all directions. Maybe I should make the circle a little bit better to convey the point. Maybe I hope this forgive my inability to draw circles anymore. So here we go. You take an isotropic noise and somehow you can shape it into something where there is a relationship between X and Y. Absence of relationship has been captured into a relationship. Now comes a beautiful part. Suppose you have, take each point. So you have a point here, x1, y1, xi, yi, yi. Then it turns out that the same point, like suppose they are equal same number of points, what you can do is you can take a matrix, let me use some other color, you can take a, yeah, you can take a matrix, Yeah. You can take a matrix, matrix M. You take a matrix, square matrix, 2 by 2 matrix, and we're together. And with reasonably well-behaved thing, make sure that it is symmetric. Right? Take a symmetric matrix. So in other words, this value, A, B, B, C, something like this. And it needs a little bit more well-behaved property, but we'll sort of gloss over it a little bit for here. So you take a matrix and you can fill it with whatever numbers you want, right, practically speaking, and then do this. Apply this matrix onto this data. Apply this matrix onto this data set. Let me call this data set A and this data set B, right. If you apply apply m to the data set A to x. Actually, why do I not call it the vector x vector? x vector. And this is x prime vector. x vector. So what will happen is each point will x. So x vector is equal to x y. And x prime vector is equal to x prime y prime at any point in this space. So what happens is when you apply this matrix to this, it will transform every point to some other point. So for example, let's take this value. If you take a, b, b, c and apply it to, let's say a point that is 2, 3, right? Or 2 along the x-axis in this. So you realize that this will become something like 2a plus 3b. And this will become 2b plus 3c, right? So this is your x-prime vector. So every point moves to some other point. And this will become your x-prime at this point. And now comes a magical part, which I'll give it to you without explanation. So you see that this m will transform this to that. You take a different m, it will transform it to something different. Right now, this m, if you break up this m, It turns out to be that you can write it as U, D, U transpose. You can break it up into one matrix U, one matrix D, and one matrix U transpose. Now, the U matrix will actually be made up of vectors. This vector, so let me put some color here okay let me put the vector u1 and perpendicular vector u2 can you guys see u1 and u2 here in this diagram yes so u is and again u1 and u2 will be written in terms of x and y so it it will be u1 of x and y. So it will be u1, u2 vectors. Two vectors will make the u matrix. And here is the most interesting part. What happens is that if you look at this point along that direction, this point, this point in the circle has gotten stretched to this point. So in other words, this point here has gotten stretched to this point. This point would have been here, original. And it has gotten stretched to this final point. And this point is very interesting. It has gotten through a stretch. The other point at the antipodal side also has been stretched. And along this axis also, these points, they have been rather squooshed. This point has been pulled in. This point has been pulled in. And this point has been pulled out. And this point has been pulled out. So that you get this elliptical shape. Right. And the degree to which these are pulled in or out let's say that this has been pulled out by in this direction by lambda one and this has been pulled in by a by a factor lambda two now the amazing thing that comes out and this is the most magical thing that this diagonal matrix here is actually made up of this. It's a diagonal matrix. So you say that you have done something called eigenvalue decomposition. Eigenvalue decomposition. These are eigenvectors and you see them. Those are the vectors. Those are the axis of the ellipsoid eigenvectors axes of the ellipsoid ellipsoid and these are called eigenvalues right so what it means is that any point can be represented now in new coordinates in terms of u1 and u2 and then comes the the fact that if you take so this was in two dimensions uh if you take a sufficiently large dimensional space so now generalize it to uh let us say a huge dimensional space, 100 dimensional, 100 dim matrix. When you look at the diagonal matrix, you'll realize that you will have lambda 1, lambda 2, lambda 100, isn't it? Lambda 2. There's a very interesting relationship. Lambda 1 is greater than equal to lambda 2 is greater than equal to lambda 3, greater than equal to lambda 3 greater than equal to lambda 100 and not only that this lambdas the lambda value lambda value will have lambda 1 lambda 2 lambda 100 you will notice that the value will have a very strong distribution like this so So you can say that, you know what, I'm going to keep two or three of these lambdas and I will approximately zero it, zero the rest of the lambdas, right? When I do this matrix decomposition, I will just blow them away, erase them away. And when you do erase them away, that explains the word truncated singular, you know, we use the word truncated singular value decomposition we use the word truncated singular value decomposition so the only difference between what i just explained and what the svd is is that svd deals with rectangular matrices not just square matrices but it doesn't matter the the intuition is the same so when you truncate it what it means is that when you have an ellipsoid, you may choose to keep only U1, and you may choose to ignore, let us say, maybe keep U2, but let's say there is a U3 perpendicular to all of that, you may say, I'm not going to care about the U3 direction. And so you may, if you just keep U1, U2, and you don't care about U4 and other things, so what you have done is you have now reduced the dimensionality of the data just to unit u1, u2 eigenvectors. And when you do that, and maybe let's just say u3 also in this particular case, but you have ignored all of the u100. So u4 to u100, right right you have set it all to zero and you have set lambda four all the way to lambda hundred to zero approximately zero because they are much smaller than lambda ones and so what you have done is you have reduced the dimensionality of the space to this uh these root vectors this uh those eigenvectors so in the case of rectangular matrices those singular vectors and those directions are the real directions along which the data is spread and what it means in the case of document is those are the real topics that describe the document though you know the document has certain amount of this topic and that topic and so forth. So the topics are nothing but these singular value vectors. So just think of them as eigenvectors in a simpler sense. That's what they are. And it's a very beautiful argument. So when you use this, right, to do that, you have, this is a remarkably efficient and powerful way to discover the topics in a document. Are we together guys? What you have done is you have said, document is made up of words, but why is it made up of these words and not some other words? The reason is these documents belong to some underlying hidden topics. Those topics are the underlying vector representation, the real vector space, which represents this. And the way to find that is very easy. All I need to do is take this matrix and I just do a singular value decomposition. Now, the intuition of singular value decomposition is it's much easier to give the same intuition using a rectangular to a square matrix and eigenvalue decomposition is the same intuition but that's what it is right it is the direction of the ellipsoids in effect that get created when you take random noise and any matrix you if you just ask this question what is that what are the directions of maximum scatter of the data of the document vectors which is the axis along which there is the maximum scatter that will be your first principal axis the next most scatter that direction orthogonal direction would be the second principal axis and so on and so forth. And those principal axes are your topics. Right. And now there's some bit of technicality when you do this. One of the things is when you do this kind of analysis, it helps to normalize the data. In other words, when you have the document and you have the word and you have the tf idfs of the words etc before you do the analysis maybe you can subtract you can zero center it zero center the data actually forget it it's just a technicality because when you do that remember that when i made this argument because when you do that, remember that when I made this argument, it is around the origin, right? So any data can be pulled back to the origin by sitting in the centroid there. All you have to do is subtract from any value its mean and you'll be sitting in the center of that set of data set. So that's a minor thing you do. So what you have done is latent semantic analysis. You just did. And that is the main argument. That's how you discover topics in this way. So guys, are you following what I'm saying? Is this making sense? From the engineering math, yeah. Yes, yes, yes. It is that. Now those of you who haven't done ML 200 or engineering math with me, it may be a little bit of a stretch, but just take it as a fact that singular value decomposition uncovers those latent factors that cause that interaction of document and words to happen. Those, you know, corpus of document vectors to be what they are. And those latent factors are the topics. And it helps you discover it. There's a bit of mathematical machinery. And this mathematical machinery, by the way, is not just some unheard of or arcane machinery. This is literally one of the crown jewel, I would say, of linear algebra. It's at the heart of a lot of machine learning that we do. And so if you get time, do learn about eigenvected, eigenvalue decomposition and singular value decomposition if you get some time. But when you apply it, you can get the topics in the documents. Now comes an interesting one limitation though. Singular values or eigenvalues, they do linear transformations. So if the relationship between documents and words are not linear, then the SVD will not work. By the way, people use the word PCA, principal component analysis, and SVD, and so on and so forth in this space. They all are referring to essentially the same method, zero-centered and so forth. So that is your latent semantic analysis. And that is another way of discovering topics in the data. I noticed that it is 10.40 now. We have exceeded the time significantly. I would like to stop. There's one more way that we will discuss. Well, there are many ways, but in the classical natural language processing methods, there is one more way which is called latent Dirichlet allocation. Right. Its acronym, too, is LDA. Nowadays when people say LDA, more often than not, they mean the latent ratio allocation. The original word, a linear discriminant analysis is actually rarely mentioned in books, surprisingly. In fact, one of the reasons, as I said, I recommended you this particular NLP book is it does talk about it. It is surprisingly effective. It's amazing how often the simpler solutions work. So before people pull out the big guns, they should try out simple solutions. Just find the centroids and then given a document, check which centroid it is close to. The downside of that method is a supervised learning. You need to know how many topics there are and you need to give it label data. Whereas the SVD method here is unsupervised learning. You just take some data, you discover a certain number of topics in it. But we'll do later in the location the next time. And with with that i would like to stop for today before most of you fall asleep any questions i have a question so in this case um how does the algorithm know what are the possible types of topics are available it's abstract you don't know actually so what you do is because you are truncating it to certain number of eigenvectors it you can look at the eigenvalue sizes and uh you know usually the well in the singular value sizes rather they will all follow this kind of a distribution see when the number of you know this the singular values they become very small you know that you have reached the point of cut off. Vaidhyanathan Ramamurthy, But because you don't want to do all that sort of mathematics. Quite often, it becomes a hyper parameter of your model. How many topics. Do you want to discover because this truncated singular value says, how many Vaidhyanathan Ramamurthy, Decomposition just says, tell me how many dimensions should I reduce it to? Three, four, five, and that's a number of topics. No, no, that I understood, but I didn't understand the, for example, politics as a topic or sports as a topic. How does the algorithm know that there is a topic? It doesn't. Very good point you raised, Rafiq. See, to motivate this example or the concept, I brought in very understandable topics like politics, like sports, like, you know, STEM field. Sometimes, but what happens is when the math grinds, the math doesn't know these human topics at all. So it just grinds through and comes up with some abstract topics, abstract vectors, topic vectors. And in this abstract topic vectors, then you, somebody like the data scientist, looks at it and says, hey, you know what? This topic seems to be all about politics. This topic, from what I can make out, seems to be all about STEM. So they will then go and name the topics give it some intuitive names but the math does not math will just come up with abstract topic vectors okay um ask if i had a similar question to what a rustic was asking so for the topic would it be okay to assume it's like a cloud of words for one topic there is another cloud visually so i would say it like this see what happened is that if you look at uh let me take this word three words like uh let's take this topics, right? Sports. I'll just make a sort of a simplicial surface. This is STEM politics. So what will happen is when you're looking at documents, documents will start falling somewhere. Let's say that the politics documents will start falling here. Much more closer, but they will still have some stem field and then they may be like this. Then your sports field will start spreading out like this. So you notice that any one document, it is not completely devoid of other topics. It has some bit of the other topic too. And then lastly, let's say the STEM field will be something like this, in this surface. So in other words, a lot of the documents, these are, for example, the sports, the STEM field documents, STEM field docs, right? So that is how it will work. You impose the meaning on it. All it will come out with is T1, T2, T3, right? The math will not come out with these names these names are what you ascribe to it after just observing the documents it's like an n-dimensional vector but we have to give meaning to it you have to give meaning to it and that is an exercise see when when you do the lab you will see that that becomes that is not as hard. So it finds those topics and usually a post hoc, I mean a posteriori, you sort of inspect and say, you know what, I'm going to call this topic this. Are we together? So I'll give you an example. I'm facing this situation. We are gathering some documents from the Internet on certain subjects, but our problem is the inverse. There are a lot of documents which we need to associate with certain subjects because we are in the learning field in my day job, learning management, learning and development. So we find that employees can benefit from a lot of emerging trainings from the internet. But, you know, wasting time searching for it is hard. And we can sort of bring it forward and annotate it, attach a lot of meaningful annotations and stitch it to the fabric of other knowledge. Suppose you get a piece of text and people haven't told you what the subject is. It is there in your ecosystem some partners some providers sold it to you content providers sold it to you and they did a very sloppy job of telling you what subject it is right so the subjects are the topics sometimes we don't know whether a piece of text is about leadership right or it is about sales Or it is about sales, or it is about, for example, conflict resolution. It is about personal development. It is about health. These are all topics that are somewhat relevant to employees in workplaces. These are because we are in the space of training employees, right? Or is it a compliance? Like for example, basic training about sexual harassment or the racial, like not having biases and so on and so forth, or diversity training and so forth. So the thing is, there are all of these topics there. You get an arbitrary piece of document, but now you need to create a machine learning framework that is good at being able to classify. And the you can do that problem in many ways. In fact, I made it deliberately a part of your project. Do you notice that I've given you a subject detection as part of your your project. You can do it. You can do it in a very much, much simplified version. You don't have to, obviously what we do is a lot more complex, but you'll get a sense of it. But do you see how when you are trying to detect subjects, the word, the topic modeling is directly relevant. One way of doing subject detection is subjects are topics. So this is all very useful. There's a more recent algorithm that was discovered in 2003, actually a little bit before BDU, I think is the author, I forget the initial creator's name. Latent Dirichlet allocation. Latent word by now you must have gotten the hang of. What is latent? The topics. Dirichlet is an interesting thing. You know that you have a Gaussian distribution, a Gaussian curve, Poisson distribution and so on and so forth. Dirichlet is very interesting. It is a distribution of distributions. Now what in the world is that exotic beast? We'll learn about that next time. And it is a very powerful way in the classic NLP for doing topic modeling. Asif? Sorry, yeah, Dennis, please. How would you say what's more efficient um pca ica or single singular value decomposition not ica uh pca see here's the thing pca is nothing but svd right okay it's a zero centered svd is nothing but SVD. It's a zero centered SVD. And when you do that for a rectangular matrix, it's eigenvalue decomposition. It's very related. They're all very, very related. That's why I'm saying that ideas like eigenvalue decomposition and singular value decomposition, they are the crown jewels of linear algebra. And they are everywhere in machine learning. PCA is nothing but that. Do you think ICA is ever useful? I see it everywhere, but I'm not sure what it's about. Okay, let me talk about that differently. That's a slightly different topic. I'll talk about it at some point. Okay. Any other questions, guys, before I stop recording? Asif, continuing with the previous discussion, I mean, I'm more used to the SuperVise where, you know, you know already the topic and then you go and label the documents. But now here it's doing the reverse. We don't know the label for the documents. It's the topic and then you go and label the documents but now here it's doing the reverse we don't know the label for the documents this topic for the documents and it's yes yes it's magic see here's a technology uh today right see when we learn we just say supervise and supervise it all looks too theoretical here's a practical reality see labeling data is very very expensive in our own workplace right we have these documents these uh you know things uh resources or text or content with and then we have to hire people who will study that and say what the labels are. So the big push in machine learning these days in AI is more and more to be able to do what we used to do with supervised learning but to do it with unsupervised learning. Even basic things like for example you look at the ImageNet dataset and so forth, somebody has sat and labeled the data but the whole question is that can we auto label the data discover the classes and so on and so forth so in some sense topic modeling is that see when you do spam non-spam you get label data and do it what does it mean somebody sat down and labeled that data biology isn't it? That is very expensive. And as the data volumes increase, it's no more feasible to pay that kind of money and get labeled data. So we- But you mean, someone starts off labeling, and then we have a sense of what it is, and then later go and extract more topics or labels, and see the nearness, and we know which bucket it falls into. Yeah, you model the topics, if the topics look reasonably pure, I mean you look at the documents where they fall in the topic space, then once you have done your analysis, right, you go and post hoc try to apply some meaning to that topic. A post UI, you give some meaning to that topic. And once you have given a meaning, a semantic meaning to the topic, now you can treat those topics as classes and you can apply, now you have labeled data and you can try supervised learning methods. Do you see that? Right, right. That's it. That's why- We have the LDA like techniques which will extract vectors close to the meaningful topic and we know this is very close to that meaningful topic yeah and remember guys when I use the word LDA that acronym is overloaded there are two meanings to it it could be linear discriminant analysis which is the simplest go-to thing most people don't do that and the other is the other end of it the more sophisticated latent Dirichlet allocation. We'll learn about it next time. So like I usually like to disambiguate, I tell very clearly which LDA am I referring to. But it's also because I don't spend my time mostly in NLP, I spend my time all over machine learning. So outside NLP, of course, LDA is usually a linear discriminant analysis. All right guys, I'm stopping the recording. Thank you.