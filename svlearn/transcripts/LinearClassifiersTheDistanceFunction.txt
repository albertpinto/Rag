 The distance function is valuable for classifiers such as the logistic regression or the support vector machines. In order to motivate our discussion, consider a data that looks like this. Here we have a two-dimensional plane. In the plane are points. The points can be either blue or they can be, I suppose, pink. Now, your problem is to determine if a given point would be blue or pink. It's a very simple problem. You would say that the decision boundary would go something like this. Let's draw the decision boundary here. The decision boundary would go perhaps somewhat like this. It's a linear decision boundary. and we notice that things below the decision boundary are all blue, things that are all above the decision boundary are all salmon or pink, a different color. And our classification problem is the target variable is the color. We have to determine the color of any point in this feature space which is essentially R2. Now if we look at this a little bit carefully, we would say that let's take a point here, this point. Just visually looking at this we say we are quite certain this point is going to be a blue point. We are even more certain about this point being a blue point. But if I come close to the decision boundary, for example, what is it here? It is less clear. and suppose we cross the decision boundary, this place, perhaps this place looks like it could be the salmon color or it could be blue, but much more likely to be the salmon color. If you move away from the decision boundary, let's say we come here, then we are far more certain that this is a blue color. So as we go along this axis, let us say we go along this direction. As we keep moving along this direction and the likelihood that it is or the probability that it is blue color keeps on decreasing. We will use this as a motivating example because this brings about the notion that we need to know the distance from the decision boundary. What is the distance given any point how far it is. Now, we will take a line that connects the origin to the decision boundary. This line, by the way, we are using the word, this is a vector. The decision boundary itself in two-dimensional plane is a line. But if you generalize it to higher dimensional plane, it is going to be a hyperplane now they will always exist one line the shortest line between the origin here the origin here and that hyperplane, in this case a line. That shortest distance, now this point that is the shortest, I will call this point the M point and a vector, it can be represented as the M vector. I'll call it the minimum distance vector. Now we observe that this vector will be far closer than any other vector like this from the hyperplane to the origin. Any other point vector, point belonging to the hyperplane will be much further off than this. Not only that, the angle that it makes, this angle or this angle would be either acute or obtuse, whereas the point that is at minimal distance from the hyperplane will make a right angle. Now, this is worth noting down. Given this hyperplane, we realize that we can uniquely define it. If you think about it for a moment, you'll realize very uniquely we can define it with a unit vector. uniquely we can define it with a unit vector. Let me call the unit vector a beta hat or actually let me just call it w. I'll put a hat here which is common when we talk about unit vectors. So let's write it down. W hat is in terms of components, it is some component along x axis, some component along x2 axis, x1 and x2. And then we can generalize it as we go along, let's keep generalizing to Wn. This is a vector. A point in space of course is any arbitrary point in space would be x is equal to, in vector notation, we would say it is x1, x2, x3 and so on and so forth to xn. Obviously x3 is not visible in this plot, it is two dimensional. But we assume that it is generalizable, of course the same things apply there. Now, let's consider a point that is on the hyperplane, in this particular case a line. Let's take this point, let me call a point on the line as p. What can we say about p? And the unit vector. There is a crucial observation here, which is that if you dot the unit vector with p, you realize that you are looking at, suppose this is this, and there is some angle here. Let me call the angle theta. You realize that this is equal to W hat, the size of W hat, the size of P, cosine theta. Well, the size of W hat is 1. theta. Well, the size of W hat is 1. The size, now P has certain size, the distance from the origin, which is obviously more than the distance of the point M from the origin. But what is P cosine theta? What is this variable? If we look at this triangle, which is made by the origin of P and m, we realize that origin to p is the hypotenuse, origin to m is the base, and therefore, and the cosine is of course the ratio of m, the m vector. And therefore, we can say that a point on the line will be at essentially, when you take its dot product with the unit vector, what you essentially get is a measure which is exactly the same as the distance of that line or that hyperplane from the origin, which I will mark it as the size of m. Now, how is that useful? For one thing, it gives us a new way of defining a line. We can say a line is a set of all points, which when dotted with W hat produces the constant value m. If you think about it, that definition will hold true for all points along this line irrespective of where those points are. Now, what about points not on the line? Let's take one such point which is certainly not on the line. Let's take one such point which is certainly not on the line. I'll take a point, let's say I take a point here. I'll go from here to a point and I'll call this point an arbitrary point X how far is this point from the decision boundary so if we the question is how far is this this distance to find, let's connect these two points and create a vector. Let me call this vector S vector, where S is equal to S is a vector connecting M to X. connecting m to x. Just an observation will convince you that x is s plus m, the vector. So if you go from origin to m and then from there, traverse the distance of s, what you will end up is x. That's what it is trying to say. It's the vector sum. x is the vector sum of m and s. Let's see what it can give us. We can write the sum as, this s vector as x minus m, this is basic vector arithmetic. And now, let's look at this. This s forms a triangle. If you look at this, this is our yellow vector, this is our unit vector. Unit vector. Unit vector, here I just repeated it here. What do we get if we take the dot product of S with W, with the weight W. Let's observe it carefully. Once again S makes an angle, let me call it phi. This would be equal to the size of s times the size of that and cosine theta. This, of course, is 1. Now, once again, this is equal to the size of this vector, cosine theta, and an observation will show you that this is equal to this. Now, this is exactly what we are looking for, distance x, is distance from the decision boundary and we just observe that that is given by this quantity here and therefore we write distance of a point DX or let's write it in a different kind because it's a result we want to remember the distance of any point in the feature space in Rn from a decision boundary is essentially given by the unit vector of perpendicular or orthonormal to the decision boundary dotted with S vector which connects the the point M the special point M nearest to the origin to that point X. Now let's expand it a little bit. This is this dotted with X minus M which is equal to now now here we bring about a little fact if you take two vectors and you take a dot product you can do it component wise. What it means is we are dotting the vector W, W hat, the unit which has these components, wnxn minus, well, W hat dot m. But what is this? This is equal to, we already know that W hat and m point in exactly the same direction. And W is a unit vector. And so this becomes, this quantity is w hat size m size and cosine of what's the angle between w hat and m is 0 and that is equal to 1 this is equal to 1 and so we can write that the distance function becomes let's write it here, the distance function becomes w1 x1, sorry, x1 plus w2 x2 plus all the way to wn xn minus m. Now, that is a result, except that it is much more conventional to call this w0. If we just call it w0 because ultimately it's just a constant, it's a naming thing, we get to the result we were seeking that the distance of a point x from a decision boundary whose orthonormal vector is W is given by W1x1, let me just put W0 in the beginning, W0 plus W1x1 plus W2x2 all the way to Wnxn and more succinctly we can write it as w naught plus the w vector times the x vector and this is the result we were trying to derive. This is an important result. So to recap, given a decision boundary, now the question is how did we find the decision boundary? Well that is the learning process. We have to do, we have to go through the whole process of setting up a loss function, then minimizing the loss function such that of all the possible hyperplanes that we can draw in the plane, we find the optimal hyperplane, which is the hyperplane that we have drawn here in this particular case, which is the line that we have drawn here, looking at this particular decision boundary, the linear decision boundary. And so that is a separate topic, we'll come to that. But given a decision boundary, we have an orthonormal vector w, and once we have this orthonormal vector, this w vector, then we have just one more piece of the puzzle which is how far is that hyperplane from the origin? There is a minimum distance which is our m and which is in our new notation it is the minimum distance is equal to m, which is the same as minus w naught. And so we get this equation for distance from the decision boundary, which essentially says it is a certain amount, w naught, plus the dot product of the orthonormal vector with the point itself, point being the vector point. And that concludes our derivation.