 All right guys, so today it is Sunday. We are going to read the paper Generative Adversarial Networks. This was quite a landmark paper when it came out. It was heralded as the beginning of almost a new phase of artificial intelligence or machine learning. This was in 2014 and some of the biggest guys like Lee Koon and so forth, they thought very very highly of this paper. The ideas in this paper are actually quite simple, surprisingly simple. But the way they have put it together is quite nice. We'll cover this paper. We did this concept, generative adversarial networks in the previous week, in this week. So I will just simply review this a little bit by saying that you have a generator and you have a discriminator. Generator is like the thief, discriminator is like the police. The generator tries to produce counterfeit bills of sorts or pictures and the job of the discriminator or the cop is to catch those fakes. When you train them together on data, first you take some real data and you train them together, what you find is that gradually the generator and the discriminator, they would become pretty good at it. And the generator in particular starts producing remarkably accurate pictures that are hard to differentiate from the real ones. Are we together? We also talked about the fact that in machine learning, if you remember, we have two kinds of algorithms. We have the generative models and we have the discriminative models. This is something I don't remember. Your generative models and discriminative models. The discriminative models simply try to draw a decision boundary in the feature space between the classes. They sort of partition the feature space in such a way that one partition contains predominantly data of one class. And that is their pursuit. That's how they directly go and classify images. The generative models, on the other hand, their main goal is not so much to find a decision boundary, which is sort of a consequence of its primary goal. Its primary goal is to find the underlying probability distribution of each of the classes. And so I wrote this thing, if you remember, that in a discriminative model, you're just trying to tell, given this data point, what is the class? This is the gist of a discriminative model, what I have put in yellow. On the other hand, the gist of the goal of a generative model is the opposite. It says, given a class, what is this probability distribution in the feature space? And if I can find that, some advantages of generative models are that you can go on creating more and more data if you can figure out the underlying distribution Aditya Karia Aditya Karia Aditya Karia Aditya Karia Aditya Karia Aditya Karia Aditya Karia Aditya Karia Aditya Karia Aditya Karia that's a distinction between generative models and discriminative models in machine learning. Now this paper puts a very interesting idea forward. It says that suppose there is a discriminative model whose job is to tell between fake and real data. between fake and real data. Are we together? And it can do that. It is a discriminative model. It will try to build a decision boundary within data of the fake class and the real class, two label class and the fake class. So let it go do that. Let it find its decision boundary. On the other hand, let's have a generator there whose purpose is to somehow figure out the distribution of the data. Now, typically how you do it is when you train a generator in normal architectures, you give it the data and you train it with the data. But in generative adversarial networks, you don't train it with the data. You instead say, I'm not going to give you the data at all to the generator. I will instead give the data to the discriminator. Now you back propagate the mistakes that the discriminator makes, and from there, try to figure out what the data is. So it's a very interesting game. You blindfold the generator and say we won't give you the data. We'll instead give you noise. Now you need to deform the noise into the shape of the data that you can't see. So quite a clever idea. But how will it do that? It will generate something. It will give it to the discriminator the discriminator will tell it's fake or fake or real and then the generator will say the thief will say huh that didn't work the discriminator could figure it out let's now change what we have to something that will work and gradually as it gradually begins to learn what will work, what will fool the discriminator, it begins to get a very strong idea of the original data distribution. And when it gets an idea of the original data distribution and now, of course, it can generate lots of data for you, fake data for you, which looks uncannily like the real data. So that's the main idea of generative adversarial networks. It is a 2014 paper. I would say that it's by now considered a classic paper in the world of GANs. There has been so much progress, and there are so many versions of the generative adversarial networks available. If you remember, this week we covered two architectures, the classic GAN, and we covered the DC GAN. DC GAN brought about the sort of arithmetic with images. You could do a man with glasses minus man, plus woman becomes a woman with glasses, that sort of thing. The kind of arithmetic we could do with word embeddings isn't it in the NLP space. So you can do the same thing here. You can come up with images will have latent representations and that those latent representations tend to have, tend to follow that kind of arithmetic same as word embeddings. So that is a very remarkable thing actually. So with that preface, I would like to get started with a more mathematical understanding of the paper. But before I do that, I will tell you about a different algorithm which is not generative adversarial network. It is an algorithm that is called, hang on, I'll remember the name, and I'd like to start with that actually. I'm looking at it here. It is called maximum mean, yeah, it's a generative moment matching model. And these generative moment matching model. The reason I'm going to talk about that is it gives you a good backdrop of what we are going to do with GANs. It gives you sort of an alternate perspective on the GANs. Now, the way I'm going to explain, this is not my original explanation. I learned this explanation from the excellent lecture from Ali. There's a very nice video which I put on the class website. Mr. Ali Ghazi, I believe. Ali Godsey. It's a very nice video, and I would encourage you to go and review that video. And maybe hearing the same thing from two different people often helps you understand that. So I will bring that and slightly change the language to the language in which we have been discussing things in the class. But broadly, this session that I'm giving you guys is along the lines and inspired by Ali Godsey's wonderful lecture. So, Asif, can you also start from the point of what's a moment of a distribution? That's something that was running through my mind when I listened to the Ali Godsey lecture. Oh, you did listen to that. Okay, yes. I will talk about that. That's what that was running through my mind when I listened to the Ali Godsey lecture. Oh, you did listen to that. Okay, yes, I will talk about that. That's what I'm saying. When I explain it my way, I will explain from a slightly lower level, you know, going back to the fundamentals. So that is exactly what I'm going to do. Anybody else who listened to ali's lecture started to yeah he's very good at explaining actually i like him he andrew and all of these guys are great at explaining things so all right so uh i'll explain to you the idea now ideas now from the very basics. And to do that, what I will do is, between, let's draw a line here of what we learned in the past. Now, what I will do is, I will, between that and now, let me create a lot of space. See, suppose you have a distribution, a probability distribution. What is a probability distribution? So we'll start with the basics. And we'll review it. So think of it like that, like this. Suppose you take children, like take children at a large school. Now these days, California, Bay Area schools, they are like mega schools. They have close to a thousand or ten, I don't know, two thousand students in them. But take even bigger. Imagine that you have a lot of students. And now look at some quantity. Consider their height. So what you will notice is that if you make, the height of every child is different. So one child may be, let me give it in scientific units. scientific units, let's say that a child is 1.52, so 152, 150 centimeters. Is that a reasonable number for heights? I think, I hope so. that is about three and four and a half feet that's right in u.s terms 150 centimeter but actually what will happen is it this is a rounding up it may be 150.23 six centimeter another maybe 149.2 or i don't know eight three another maybe one or I don't know, 8, 3, another maybe 136.2, another maybe 163.4. Do you see that these are decimals, right? In other words, if you take this to be the word you use, it's the random variable for weight or just a variable for weight is the name for weight right then you would agree that X belongs to X is a real number right X belongs to real more, it may belong to an interval like a children in school are at least three feet, 100 centimeters to, I don't know, maybe 250 centimeters. That would be excessively tall, perhaps. Something like that. Even limited to this interval. Maybe to this. And maybe you might occasionally come across a child who's even smaller than this, bigger than this, but broadly most of them will be like this. So you do something. So it's very hard to do what is called frequency plot. So the typical histogram would do this, would say, would break up this line, the number line zero to, I'll just take a value here, 300 centimeter. It will break it up into small tick marks. Let's us say that each tick mark is one centimeter. So you will take all people between 149.5 to 150.5 as approximately 150. Are we together? So you have to do some coarse grading. And so then at 150, you will find So you have to do some coarse graining and so then at 150 you will find that you have Let us say you have 200 children right and so a basic idea is you can make this of frequency plots and When you do this frequency plots and this by the way, maybe medically completely off So take it with a grain of salt but the point is it's a mathematical point i'm trying to make right now you have all done such statistical charts in school something like that right now the smaller i make this interval this delta so this is x the smaller and here delta x is this one centimeter gap now i can make delta x even smaller i can make it half a centimeter one millimeter and so forth and when you do that in the limit so so you look at this distribution and uh just connect the tippy tops of this distribution or this frequency graph. If you were to connect the tippy tops, it would look something like this, right? And at this moment, it would be like sort of a not a smooth curve, but in the limit, in the abstract limit where delta x tends to zero, like very, very small bins, right? And obviously assume very large n. n is the number of students, a very large n, let's say students, where is it? Large students, so n approximately 2 2 000 or something like that but now go to 20 000 go even further or 100 000 or something like that and very small interval very large n what what happens is that this frequency distribution it begins to in the limit it begins to look like the bell curve right this particular thing so what you say is that when you divide this frequency distribution by the total number of students so suppose i have that and i divide it by 20 000 what does it mean this is the proportion of students who belong to this 20 out of 20k let's say right what does it mean one in uh one in 100 students is 150 centimeters, isn't it? So that you call the probability of X. This is the probability that the value at any given value, let's say X. students have their weights between x and this point being x plus delta x and the gap being delta x right how many students have weight what proportion of students out of them isn't it right come again it's height not weight right oh i'm sorry i was taking height yes how many how many of them have height between this interval. So you can ask this question, what proportion proportion of students weight between I say not in height. I keep using the weight height Right between That between X and X plus Delta X Right and the answer to that would you agree that it is basically P X Delta X And the answer to that, would you agree that it is basically px delta x? px approximately px delta x computed at x. Would you agree with this? So suppose this is xi. This is the number of people at xi, pxi and small delta x. This is the amount. This is a rectangle, right? The width is delta x and the height is Px, isn't it? And so this term Px is called a probability distribution. It is the probability that a child will have this height or close to this height. Probability distribution, right? And think about this. If you go to just the frequency plot will have this height or close to this height, probability distribution. And think about this, if you go to just the frequency plot and you divide it by the total number of students, what do you get? Any one box represents the proportion of people, the proportion of children who have height in that range, maybe one in 100 or one in 50 or whatever it is. And in the limit of that, proportion becomes probability. In the Frequentialist definition, probability and proportion are exactly the same thing. One is the limit of the other, sort of. In our particular case. So now we have established, now we know So now we have established, now we know what a probability distribution is, isn't it? Right, so it is, it has a property. If you look at any one distribution and I add up all of this, you know, all of this, I look at all of this, that would be a px dx integral over minus infinity to plus infinity. Now what should it be? It just means what proportion of people have heights between minus infinity and plus infinity? What would you answer that? What proportion of students have height between minus infinity and plus infinity? That'll be one, 100%. And this is a property of probability distributions, not just true of height, but always. So what proportion of whatever has value between minus and plus infinity well of course the answer would be always one and this is the defining property of a probability distribution mathematically that the total area under the curve is always one are we together now there are many probability distributions for, if you look at people and their wages, it is approximately like this. And you have a long tail distribution. Long tail distribution here. This is how the wages are likely to be. If you look at the number of years of experience in a company that people have, that will be even more skewed. It will most companies in tech, they have a distribution like this. So these are all different shapes of probability distributions. And it doesn't matter what the probability distribution is. That is it. So now what happens is in machine learning and now you generalize it to higher dimensions. Now you can generalize. For example, I think about it this way. Suppose you have the, let's go back to our classic cows and ducks that we keep talking about. So suppose this is your weight, this is your size, right? This is X1, this is X2. And you have some ducks here. And then you have some cows here. Let's make the cars like this. In this space. What you can tell is that if you look at this data, the ducks have a probability distribution along both of these axes. So if you were to now project it in a three dimensional place and you look at the probability distribution of this is x1, x2, what you'll find is those blue little ducks are here on the plane sitting here. And so rising above them is their probability distribution, which will be, would you agree that something like this, a three-dimensional surface like this? Likewise, your white big cows are sitting here because their weights are bigger and their size is also bigger and so their probability distribution will rise. It is very more frequent here because they are much more, you know, a cow's having weight right in the, around the average. So their probability distribution will look something like this. Sorry, I should use red. We'll look, what's happening here? Okay, it will look something like this. Would you agree? I'm just generalizing the concept of probability distribution to a higher dimension, to a two dimension, two variables, using the example of cows and ducks. Do you see how simple it is, guys? Do we get this concept guys? Yes. It's as simple as that. So now we realize that probability distributions can be a high dimensional thing. Now think about images. Because we are going to talk about images. When you take an image, let's take a consider MNIST image, digit image, which is 0 to 9, what happens is any one digit is 28 cross 28 cross 3. Height, width, actually 1 because this is grayscale so we can just leave it as 1. So this is 764 I believe. 764. So if you just flatten this image it will become a vector of, if you treat it as a vector, it will become a vector of 64 dimensions, 28 cross 28. Isn't it, guys? And so you have just unrolled it. In other words, you're reading this image, one row followed by, you know, you can do row or column, whichever way you want. One of the ways you can do it is you take this column and then you make this column below this, you make this column below this, you'll end up with a long column vector. Or you can take it as rows or whatever. Some way you can make it one dimensional. So then your data is a zero will look like something, you know, maybe here, and then something here, something here. Then the other numbers will have their own distributions and so forth. So what will happen is you will end up with, across all of the numbers, you'll end up with some peculiar distribution. Let's say that you'll end up with a distribution function that's very, very complicated. Are we together? Of all the images like if you take all of the images put together you don't you don't care whether it is zero one or whatever it is and you just look at the places where they have number you know their values are here and you create a probability distribution uh here you'll get some sort of a distribution probability of the data. Does this make sense guys? So maybe this is a place where there's a lot of a lot of these bits show up because it's maybe it's common to many digits that they will put a value there. Right, So this is your probability distribution for a particular image. It's a little bit abstract. How do you think of 764 dimensions? But it is just generalizing the ideas of your cows and ducks together. And just saying that in that very high dimensional space, all of these data points will fall somewhere. And so you'll have a more or less probability of seeing data at any given location X here. So far so good guys. Yeah, actually, so no, maybe. I shouldn't have said it like this. Actually, I should not have made it into a single line. I don't know why this is wrong. Actually, I already screwed up. So let's go back. It has 764 dimensions. So this is the probability of x. But these vectors are your x1, x2, x3, x4. Yeah, that's why it wasn't making sense to me. x5. So what happens is think of it as your cows and ducks. All you know is that somewhere there will be a lot of points gathering? Something gathered like that. Forget that initial picture that I made. You would agree that in this feature space there will be places where points are concentrated because each digit, each image is a vector in this space. Each image is a vector in this space somewhere. So then some points will be close together, some won't be, and so on and so forth. So you can create a very high dimensional probability distribution. Just as you can think of a two dimensional probability distribution, you can think of a high dimensional probability distribution. So for the data. So imagine that your data set, just for the sake of argument, is MNIST. It doesn't matter which data set it is. It's MNIST. You have 60,000 images. So you can create in the input space a probability distribution. This is known. Would you agree that this is known? Because you know the data. You know this probability distribution are we together yes except that this distribution is horrendously these this is a this is in 768 dimensions it's horrible to compute you can't really actually go and compute it you say that it is known but only in the sense that it is there it is sitting there in the data you can't really actually go and compute it. You say that it is known, but only in the sense that it is there. It is sitting there in the data. You can't actually go ahead and compute it, or it would be horrendous to compute it, right? So, or write it in an analytical form. So, what do we do? We hold that thought in mind, and now let's take a digression. Now let us take a digression. We will ask a different question. Suppose, and this is, we will say, similarity between distributions. or the opposite of similarity is dissimilarity. Dissimilarity is usually called distance between, distance between distributions. So suppose there is a distribution PX and a distribution QX. Are we together? And let us say that you have N point here and you have M point here. And let us say that you have n points here and you have m points here. For this thing, you have n data available and you have m data available for this. One of the questions that is there is that let us represent the word distance between p and q. That is a measure of how dissimilar or similar they are. To give you an illustration, consider this distribution, P, and consider this distribution, which is Q. This is your situation A. Now, consider another situation. P is like this, and Q is like this and Q is like this. Now, this is your Q and this is your P. Now, this is your situation B. Now, if you were to just look eyeball it, in which case do the distribution seem to be more similar? Definitely A. Definitely A, right? Is N and M the number of data points for each of the different distributions, like N points in P of X and M points in Q of X? Yeah, so that is just to emphasize that you may choose to have, I mean, sometimes reality is like that. For example, you may have one disease, you may have only so many patients of one disease and many more patients of the other disease and so forth. So you don't know how many instances you have from each of the distributions. But if it simplifies in your mind, assume that the number is the same. Thinking of the same number of instances drawn from P and drawn from Q makes life easier. Now the question is, visually we can see that A, P and Q are closer and in B, P and Q are far apart. But there has to be some way to quantify this mathematically. Right? And the way you do that, mathematically is using this. There are too many ways of doing it. The classic ways are KL divergence. Divergence is distance. How divergent are two distributions so k l of p given q p and q right how is p how different is p from q and it turns out that the other way around how different how how distant is q from p right ideally you don't you want it to be symmetric but not only now these quantities are always greater than equal to one zero they're positive kl divergence is a measure that is positive the bigger the number the more different they are so you would say that a kl divergence in b is greater than kl divergence in the situation a. Are we together between p and q? Let me write it out in full form. You would say that KL divergence between p and q is much greater than KL divergence in the situation a between p and q. So now we are beginning to create a little bit of a notation. This is just a way of saying guys, it's just a way of saying. Now, there is another sort of a divergence that you can use, which is which is called the Jensen-Shannon divergence, which this paper refers to. So I'm going to bring it about. The Jensen. the Jensen. What it does is Jensen Shannon says that J S D is basically between P and Q, PQ is basically a symmetric version of it. It basically says take half of both KL, P given Q and plus KL, distance from Q to P. Right? Now it's very, a little bit counterintuitive and I'm not going into the mathematics of the definition of KL divergence. I'll just leave it for now, because that gets a little... Quick question for the Janssen-Shannon divergence thing. Is it P bar Q and then in both sides? I see P bar Q on the left and then P comma Q on the right. Just a little semantic thing. Oh, right, right, right. You're right. you're right i thank you for fixing that i'm being sloppy better now right so there is a there is a sort of a symmetric version this is a more symmetric version why is it a symmetric version because if you interchange p and q, this value will remain the same. That is that. So now comes, but ultimately what they measure is the distance between or how different or divergent is one probability distribution from another probability distribution. Now with that in mind, let us now start building up a story here. KL divergence and Jensen-Shannon divergence have been there for quite some time, but recently there was a new sort of a method proposed, which is quite interesting actually, and it comes from kernel methods. Those of you who took ML 200 and learn about kernel methods, do you guys remember your kernel methods and the quiz on kernel? Mostly. Mostly? Yeah. So there is a way of finding divergence based on those ideas. So what it says is that something very surprising. It says that if you take a point, let's say that the point x belongs to p , the probability of x, and another point belongs to, let's say y belongs to the q distribution of Y do one thing create a mapping function create mapping function and I'll use the notation that you are familiar with but you remember this notation that we used Phi says that given X or y, it doesn't matter which, x says that it is taken by phi to phi of x in a higher dimension. Are we together? Take this point x and project it into another vector phi of x in a higher dimension, in a kernel space, right? Vector, vector, and kernel space. So far, so good, guys. This is a trick I trust all of you are very, very familiar with. So we do this. Now, well, likewise, of course, you can take y and project y to, implicit to this is of course, the same kernel function you can apply to the point y, and it will also go to a kernel space, higher dimension kernel space. And then, so first let me tell you the abstract statement that it is making. Once you go to the kernel space, something beautiful happens. That you can actually say that the distance between a probability distribution, P and Q, is actually becomes, for all the points that belong to P and Q, what happens is this becomes nothing but, okay, well, okay, before I do that, let me just create a bit more notation. So you have n points, we have n points of x belongs to the probability distribution P. Find its mean in the kernel space. Not in this space. Don't go and add up all of these points and divide them by n but in the go and do it in the kernel space very interesting right so what happens is that you are trying to find mu of let's say the probability distribution px is equal to 1 over the number of points phi xi because you have n points drawn from the p distribution isn't it n examples taken from p so one easy way you can think of it is a p is children human human children and q are penguins right and the children are penguin kids so you take maybe 70, 80, 100 measurements of human children, and then you go and take maybe, I don't know, 100, 200 measurements of penguins. Are we together? Then for each measurement, you just project it to a higher dimensional space, and you find the average for P, dimensional space and you find the average for p and likewise you find the average for q right is equal to 1 over m whatever number of points you have for the penguins i to m phi x maybe i'll just change it to xj. xj, right? These are points for the penguins. So we get an average, and these will be vectors because these are vectors. So these are vectors. You get the mean vectors in the kernel space of that. And then comes the very interesting statement. The statement is that the distance is actually, distance between p and q can is actually very well represented by mu p minus mu q square or rather distance square if you want to say d squared but this is a pretty good measure so people don't have to hear this is a pretty good measure people write it slightly differently actually they say that in in many way this is just a notational way of putting it p minus q squared is this right p minus this being the distance between p q remember p and q are distributions. You can't just subtract them directly. So this is just a notational thing. So here we go. We have mu p minus mu q. And it looks very surprising that it works. So now the question is, why does it work and why is this trick so useful so the way it works is this let us say let us create phi out of the moments of a sample so this brings up the concept of moments. Moments are central to data. So there's a moment you can do by looking at, by subtracting the mean from it or not subtracting the mean from it. So it basically says that whatever the probability distribution is, let's say x times the probability of x. All right, p x. Before we get to that, just in fact, I appreciate that p minus q squared equal to the, in terms of the transformed, the mean of the transform distribution, right? Now, there's an implicit assumption being made here. And when we say distance, we're taking it as the Euclidean distance. No, we are not. That is precisely the point. We are not. So what distance is it then? It is the logical. Remember we said that suppose you have a distribution. Let's go back. See, the concept of distribution is divergence. How different is one, the shape of one probability distribution from the shape of the other? So you need some notion of distance that works very well so that if the distance is zero when the two probability distributions are identical. Are we together? So KL divisions was one. One new measure that showed up recently is the, it is called the... The Janssen-Shannon. No, no, no. Janssen-Shannon is just a symmetric key. I'm saying you take this and this particular is so-called maximum. What you do is you define the maximum distance along any one of these means. The means, right, you take the distance between the mean vectors and that is your distance. So now let me make it very real using something called the moments. Okay, first let me give you the hand waving explanation of moments. Vipul Khosla, Ph.D.: You realize that if you have a lot of data points right this excite belonging to taken from the end of these excise let's say whatever Vipul Khosla, Ph.D.: And samples. Vipul Khosla, Ph.D.: and instances rather. Xi belongs to the problem. So it is drawn from the probability distribution P. Are we together? Now for any bunch of numbers, forget probability distribution itself, you can find something called the mean mean is what x1 times xn divided by number of elements are we together right then uh you can do mean standard deviation, which would be what? X minus mean square. Actually, let me just take it this. What is this? This, the averaging of the square of the distances, X1. You remember X minus mu square. And so you can generalize this to higher and higher values. You can say that the first moment, the second moment, so third moment would be So third moment would be x minus mu cubed summed over i, right? It would be this. The fourth moment would be 1 over n. And you may say, this is getting rather abstract. Do these things even have a meaning? They do, actually Now I hope I don't get this one right. This is skew, I think. Yeah, the other one is kurtosis. And this is the kurtosis. Kurtosis, yeah. So what it means is, what is skew? For those of you who are not familiar with the skew, we all know the data is skewed and this and that we keep hearing. What skew measures is that, see, if you look at the data, sit at the mean and do the two sides look alike. If they look alike, it is not skewed. Are we together? But if the data, so not skewed. are we together but if the data so not skewed but look at the data like this now where will the mean be the mean will be somewhere like average of the values it will be somewhere here right now one way to find this for this one what's that the mean will be more to the left the mean would be more to the left right so it would be somewhere this is the mode actually this is the mode the The mean would actually be somewhere here because there is a lot of data. It just keeps on going. So what happens with this is that, so here is, anyway, here's an easy way for you to remember. When you look at data like this, right? This, imagine that it is a duck, a duck a duck the face of a duck or slide right so this is the face of a duck is the head of a duck which way is the duck looking is it looking to the right or to the left to the right to the right so you call it right skewed right so so the the basic idea is that the the way the naming convention is one easy way to uh think about it is imagine the distribution is the headw. So for example, this way, the duck seems to be not looking anyway if anything is looking up. So no skew. This right skew. this right skew skew and the next one is left skew right are we together okay so this is a measure of skewness. This is what skew is. It's the third moment. Then there is a fourth moment. The fourth moment is actually something called kurtosis. Kurtosis is a measure of where the bulk of the mass is. So for example, is the tail light, right? There's not much mass there in the tail. And most of the distribution is closer to the mean. So you would call it leptocurtotic data or leptocurtosis. data or leptocartosis. On the other hand, if the data has a very heavy tail, it doesn't matter, it need not be symmetric, but it is a very heavy tail. It goes on and on and on. You call it platycultotic. Platy means flat, platycurtotic. Platy means flat platycurtosis. I suspect the name came from the animal platypus, which has this big heavy tail and twaddles around. So on the other hand, if the tails are heavy, then you have platyatic cartotic data. Right. So in other words, how heavy cartosis measures, measures how heavy is the tail of the probability distribution? Are we together? So now you can generalize from mean, standard, variance, Q, kurtosis. You can keep going. And you can say that the moment, the nth moment of a data is 1 over, so I can kth moment because they're k data points. K is x minus mu to the power k is equal to this. So this is the way generalizing for a situation in which each points are equally likely. And now comes a little bit of a sophistication. What happens is that when you deal with continuous variables and things like that you often look at probability weighted distribution so the only thing I need to do now to make it to to put the last piece of the puzzle what you do is you say it is not X. So the mean is mu is not just adding up all the excise and dividing by n. It is X probability of X. Are we together in a in a in a situation where the data is continuous. So suppose you have a lot of, and you have a probability density, right? So it is the integral of this. This is the continuous version. This is, there is nothing fancy about it. I'm just multiplying it by P. And so a sigma square is X square, X minus mu square, probability of X DX for continuous variable variable and so the kth moment is the only extra thing we did is multiply it by the density function d of x dx so this is your kth moment this is how moments are defined this is moment the word is very odd moment but the moment word comes from physics actually. There's a very physical, I think so, I don't know. In physics, it has a very clear intuition sort of. So now that we understand moment, we now, one, let's go back to our notion of kernel. Remember we said that distance between the p minus q, the distance square can be thought of as mu of p minus mu of q. So you remember, we are not even talking of the average in this kernel space, mu of phi p rather, mu of phi q, the kernelized version of it squared so you say well how is that possible how can something so simple be true that you project it into higher dimensional space and all you are doing is you're subtracting the means between the two and it will tell you the gap between the two so here's something that you that you do imagine that the function is and that's where the kernel comes in so so not the kernel and the moments comes in so suppose you take a function X and it becomes Phi of X goes to X let's take powers of X x square x cube something like this you just take x things like this now what happens do you realize that if i average average of this will be the mu of x it will be very close to the mu of x average of this quantity up to an additive factor will look like sigma square of x variance of x isn't it guys right and the third one x cube if you do it will give you this q factor of x and x4 will begin to give you the kurtosis and so forth so just stick to the first two things let me just take a two-dimensional vector and get an intuition into this. When you try to get an intuition into this, look at this image. So imagine the two curves are perfectly agreeing. This is p and this is q. I deliberately took a situation in which a p and q the mean is the same but you still know that these two functions are different these two distributions are different isn't it but you realize what happens the moment I represent it as x and x square right for this point x. Now I represent each point as x and x squared. You realize that the mean of p will roughly be mean of phi of p rather. Once I go to the phi is x is equal to this, then mu of the phi of x would be mu of x and mu somehow of x square do you see that guys you're taking the average of the squares what is the average of the squares this is basically the variance and this is the roughly the mean so what what will happen the when i look at phi of p and phi of distance between that, q, it will be comparing. It will have two components. The first component will be 0 because the means are the same. But the second component will not be 0. They will be different, isn't it, guys? Am I making sense? they will be different, isn't it, guys? Am I making sense? Because sigma square P is not equal to sigma square Q. And in fact, what you get is sigma square P minus sigma square Q, the variance of these two. Which, of course, and the more the spread, the bigger the variance difference, the more the two curves look differently, right? Distributions look to differently so that is an intuition of why this works right and when you use it then you call this max so the concept is find the maximum mean distance between these two between the point so along all of these axes find whichever is the biggest this is certainly the biggest and this is your distance between the two points and when you try to use this to find it out you you say that well you know what let us do it computationally and see what it what it really comes to right so yeah to. Right? So yeah. Can we say like any distribution can be precisely identified by a vector of its moments? Absolutely. That is the main idea. Okay. That you take sufficiently many moments, right? You characterize the distribution. Okay. Thank you. That is the fundamental statistical point. Actually, because you come from a statistical background, you must have done, did your prof ever mention this fact to you in college? No, I kind of just came up with it. Okay, good, very good. Yes, it is. It's just the perfect intuition. All these moments together give you enough description of the distribution that you can uniquely define the distribution. So now, I mean, as if I just want to let you know, I can make the connection now. Yeah, I think Professor Gott in Ali Godji's video, he kind of touches upon this, but he didn't drill in further into the theory behind the multiple moments that are possible there. That's right. Yes. He generalized it just in a word. Yeah. All I'm doing is bringing it down to our level, his talk. So guys, this is, remember, this is all a precursor. We haven't gone to the GANs yet. So maybe I should stop this video at some point and say we didn't do cats we instead did a digression elsewhere but all right this digression is worth it so now let's look at the distance between p and q right i'll i'll write pq distance i won't use that pq square notation this is what it is saying is that it is basically the average between these two now what is the average it is 1 over n of the phi of xi i goes to n right this is the average so you you realize that if i break it down to the pieces mu of phi like mu of phi, like mu of p given phi in this kernel space, this is literally the definition. You take each point and kernelize it, and then you just take the average. Are we together? And we will, for the moment, not worry about what kernel function we use whether we use the moments or not but we know that if we use moments that's a pretty good way of doing things right so i can write it like this i can write phi q as 1 over m and let me just write it as j for lack of a better word phi uh should i put let's yj, to distinguish the points from p and from q distributions, let us say, and they are m points. So now let us find out what is the square of this. And this is where the fun begins, actually. Let's look at this. Now I'll write in smaller. P minus mu Q squared is equal to, what is this? It is equal to this, 1 over N. And now there's a very basic amount of algebra that we will do of phi xi minus 1 over M. Let's call it J of phi yj m right square square means now this i can write it because we know that these are column vectors we can write it in matrix notation which i'll just bring it it doesn't matter ignore that if you if you're not familiar with matrix notation you can write it as with matrix notation, you can write it as i xi minus 1 minus m j m phi y j transpose, I mean, basically dot product with itself. So, you know, x squared is the dot product of x with itself, would you agree, right? And which in matrix notation is x transpose x? It's just a notation. i phi x. Well, because I used i here, now? Instead of j, k, l, let's say l, l goes to m, phi y, l. I'm just trying to distinguish all these different summations, right? And so when you do that, and this, by the way, will make sense only to the people who have done kernel methods with me. Now, let's look at this term. So it will become 1 over n squared. Phi i transpose xi phi k. And let me just mark it phi a k going from n to phi x k are we together guys right this is it right a plus one over the other term one over m square summation and i'm and i'm not writing two sigmas here the sigma sigma i'm just writing it together j and l over m both of them taking values over m for different values of j and n uh for this no actually yeah this would be phi y j not j y j yeah phi j transpose phi y l right then you have the cross term so the cross term would be minus two over N M. And this is just going into the details for what it is worth. J K. Let's say phi Y J. Let me just write it X K. write it xk and this goes to n and m right n and m then xk k and phi y j right so this is the transpose so now comes the interesting insight so why in the world are we doing this weird and complicated algebra this This, if you remember, and this is a question from your past quizzes, what is this quantity? Do you remember what is a dot product of in the kernel space? It is nothing but k xi xj. The kernel function, isn't it, guys? Remember the kernel function. This is literally the definition of the kernel. This is the kernel of y, well, j, y, doesn't matter, yl, right? And this is a kernel of xk, yj. and this is a kernel of x k y j so this is a different kernel right now why are we getting so excited about kernels do you remember the real reason we used to get excited about kernels anyone Anyone? It, but the inner- Because kernel ultimately end up becoming functions of just the dot products. So kernel x, y, ultimately, right? It is actually, by definition, it is x transpose phi y but what tends to happen is it becomes some function of x dot y so uh do you remember that it used to that you had to just look at the dot product between x and y and you could get away with it very often if you choose your kernel properly remember the mercer kernel and all that theory we did basically what it means is you don't have to compute phi this mysterious phi it. Basically what it means is you don't have to compute phi, this mysterious phi which is there. It is just a logical journey, an abstract journey from this to a higher space. You don't ever have to find the mapping function and you use the kernel trick to compute this distance, right? It becomes very easy. You can do this. Don't really have to. And those of you who haven't learned kernel methods with me, obviously this will all look very mysterious and you would be completely lost at this point. Forgive me for that. Didn't really need to compute pi by fx. So that's the awesome part of this algorithm. So now you can bring in back propagation and so on and so forth. You can train. So what you can do now is very interesting. Suppose, and this is actually the main idea. Suppose you have a generator which is trying to produce a distribution, p, of data. It will produce a data whose distribution looks like p. And you have another guy, the actual data, the truth, let's say, mnest. What you can do is you can take the data. This will have some distribution q. So the generator has a distribution p, and there's some other things. What you can do is because you have a way to compute the distance PQ, and you can do it even somehow. What you could do is that now, suppose this was a neural net, you could take this value, the distance between these two, and treat it as a loss function back propagate and gradually make P fake q isn't it guys if you could see the distribution of q is known all you do is you compute the distance between the what p is producing and what q is producing and you can keep back propagating because you have a notion of a distance it's sort of like a loss function you can keep back propagating because you have a notion of a distance is sort of like a loss function. You can keep back propagating till a P begins to fake Q or P's shape begins to look like. Yeah, so we can back prop learn so that D of q almost zero so p tends to q are we together guys we can do that it's just a learning thing and we can do that so we will use that idea here in this background except that in this thing uh the the the takeaway from the previous algorithm this whole mean maximum mean distance and all of that, and the moments. Vaidhyanathan Ramamurthy, thing is that we won't use all those tricks, we will do something quite different now we realize that you can actually learn. make P learn Q, right? The generator learner distribution. I took you through a long journey that says, yes, we can. But then comes this idea of generative adversarial network which does it in a very elegant way actually. Generative adversarial network. Let me just use the word GAN, adversarial network GAN. What it does is something quite remarkable actually. So you realize that a P, you know the generator need not see the data. All it needs to be told is how different is it from the data, isn't it? To train the generator says that the P that it produced, the distribution that it produces, it doesn't actually have to see the data. It just needs to be told how different is it, or how distant is it from the distribution of q. Then the generator can back prop learn a little bit, fudge the weights, parameters, and say, okay, how about now? We'll use that idea. That's the main idea that we use in the GAN. So this, and by the way, those of you who haven't done kernels, et cetera, you can ignore the prior discussion. Just assume that a peak and like you can train a generator to fake another distribution because we have some mathematics over with, some loss function over which you can back prop and learn that is the core idea that we use in the generated network so let us say that this generator the input is and p starts off as random noise yeah p starts off as random noise or anything doesn't matter so suppose you feed some z it is a prior people in the world call it a prior prior means just assume it is, it doesn't matter. So suppose you feed some Z, it is a prior, people in the world call it a prior. Prior means just assume it is something, it doesn't matter. This is your generator G and it produces data, it produces some data which is fake data. So PZ, it will have a distribution based on the kind of input it is getting. It will have some output. That output will have a certain distribution, right? So now what you do is you feed it into a discriminator, as we realize, a discriminator which tells them apart. Its main purpose is to produce a loss function right of how different what it produces is from the Q which is the MNIST data which is the data and now let's generalize beyond MNIST to any data any set of images so this will have a Q a distribution of the data right in the in this paper they call it P data but think of it as q. It is the p data. It's the probability distribution of the data itself, isn't it? Whereas this is the probability distribution of the fake. Our goal is to make this p look like this q. And by now, we know that we can do that. What do do you take some samples from here feed it in here some samples from here feed it in here you see the loss how how much the discriminator is telling you that uh the this fakes are different from the real ones and then we back prop it right so we went through the whole steps of doing it we can repeat that step step but let's do one thing. Let's take a, before I go into the mathematics of it, it's a little bit early game, I need a 10 minute break. I need to quickly grab lunch and come back. I'll pause the recording. So we are going to look at two different ways. One is that you could use the curdle trick, get a loss function, back propagate, and therefore train a generator to mimic the probability distribution of something else. Let's say images or whatever it is some data that other approach there's a name for it which is a moment matching and you can guess why it is so from the discussion we just had moment matching okay so let's use the word moment. Why moment? Because the kernel function is made up of moments matching. You make two distributions match their moments. Generative model, of course it's a generative model. But an alternate, which is far more popular these days, is the generative adversarial networks. This uses a different thing, actually. It says that suppose you play this game between the generator and the discriminator that we talked about. Generator tries to mimic the distribution of the data, and the discriminator looks for the subtle differences it finds the distance between the two the loss it uses the loss function and it may it does its business in such a way that it is able to tell p from q data from p distribution from the data from q data from q is the real data and so we will in this paper because it refers to it as p data we'll just talk about it not with Q but as p data and the fake data we use this notation pz. Let z is transformed some noise prior noise is transformed to this output. So the idea is that if these are images, you're trying to produce fake images that are as realistic as possible, right? And you can do it if you match their probability distribution. So that's the power of probability distributions. So the question is, what is the, how do we do that? It uses a game How do we do that? It uses a game theoretic argument called minimax. The minimax has two parties with opposite aims. One tries to maximize something. The other tries to minimize it. So what will the generator try to do? It will try to minimize the difference between fake and real. And what will the discriminator try to do? It will try to minimize the difference between fake and real. And what will the discriminator try to do? It will try to maximize its ability to tell one apart from the other. Do you see that, guys? Yeah, minimize the last. It is exactly, that is all that there is to it. So now let's go back to this paper and I'll walk you through this paper and occasionally explain the math as needed. This paper will now look hopefully far more straightforward. So here in this figure, first of all let's look at the loss function here. what he so first of all let's look at the loss function here let's pay attention to this loss function here D is the discriminator G is the generator so let's look at this term this is the data so as far as the discriminator is concerned let's say that the data has the label 1 and the fake has the label 0. Now, what do you want dx to be? is that the discriminator says uh see what dx is dx is this discriminator is saying what is the probability of it being real data isn't it that is what dx is discriminator is giving a probability dx of any instance input being real data. So when it is really the real data, how much do you want the probability to be? A good probability would be closer to? What would it be closer to? One, right? You would want to have this as one. And what about this guy? You want to make sure that a fake data, what would you like to classify a fake data as? So what is Z? Z is noise. What is G of Z? It is the output from the generator so this is the fake image and so when the discriminator acts on the fake which is D of G of Z of noise you want to have a value that is close to what you want to say that the probability that this is a real data should be high or low? Low. Low, right? So you want this to be low. In other words, you want it to be closer to zero. So you want, in other words, one minus that to be high. If the value goes from zero to one, you want one minus that probability to be more and more. That will push the value of the fake closer to 0. So that is what the discriminator would want. The discriminator would love it when these values grow. The bigger it is, the better. Right? Are we together? The bigger the dx is are we together the bigger the d-axis and on the real data and the and the smaller the d g-axis on the fake data the better it will be so the the discriminator is trying to maximize this particular thing and then what is the minimize what is the generator trying to do then what is the minimizer what is the generator trying to do generator cannot do anything about real data it is pointless so it can't play with the real data it doesn't get the real data at all but it can play with this data here and it can sort of hope that somehow it fools the discriminator into producing a high number for dGx and therefore one minus dGx will be small. So it will try to minimize this number. And here also it would rather that when it gave fake data, the probability that this guy comes up with here is such that the discriminator says no, this is real. So that is the explanation of this equation. This is the minimax two player minimax game. This is the crucial word. And now, now let's look at this. What he's trying to say is that, look at the black line here. The black line, the black thing, if I were to make it into a distribution, I could do like this. Hang on, let me make the black darker. If you were to do this, you would say, this is your Q, This is the real data. The green one is the P. This is the fake data. And so what the learning process is, and so what is the generator doing? It is deforming noise, a uniform distribution or something like that, the shape of P and this P is different from Q there is divergent from Q so what will it do it will learn and make this closer and closer and finally the two will match now the divergence is There's no divergence, isn't it? Divergence minimized. Do they throw away the discriminator then? Come again? Once it matches, do they throw away the discriminator? Oh yes, of course. You use the generator to now cook up all sorts of fake images and do all sorts of mischief after that. Are we doing that? What is the blue line? The blue line here is the current loss. I think it mentions, but I may be wrong. Let's read this. The generative adversarial nets are trained by simultaneously updating the discriminator, the D, blue and dash. Updating and the dashed line, discriminator. So discriminator is producing a probability distribution of its own. The probability that it is one or the other. So in the beginning, it will be way off. Gradually, it would have figured out so it is just that uh you know the output of the discriminator dx is that so that it discriminates between the samples and then um the logo should the blue line be like ideally ideally here the loss should be should saturate because the green line is sitting upon the the the q the p has now completely imitated the q so in that case actually the blue line won't be the correct line right yeah yeah no no this is the output so what happens is in some sense, the generator loses. I mean, sorry, the discriminator loses, the cop loses, the cop, the thief becomes too smart, right? So you notice that if you look at the cop, the loss, the output of the probability that one is different from the other, right? At this particular moment, there is some, this number varies based on the data point, but after a little while, what happens? It smoothly transitions to a straight line. It's not able to tell the fake from the different, no matter whether it's a fake or whether it is a genuine data, it comes with pretty much the same probability of its being genuine or fake. Isn't it? But so maybe I forgot, but weren't we supposed to retrain the discriminator after the generator turns? Yes, we will do this. But the point is that, see, the discriminator has a losing proposition because the generator is continuously learning so discriminator will in the beginning the discriminator will do better it will be able to tell the fake apart from the real data but then what is the generator doing is learning from the mistakes and so gradually it will produce better and better fakes so that it will manage to fool the discriminator. But when we cut it off in case where the back propagation doesn't happen through to the generator and we label those generated images as fake, wouldn't the discriminator become better over time? I know we are using a single loss function. See, if the fake is so close to the real, the discriminator is not able to tell them apart. That's the point. See, you can give it the real answers. You say this is fake, this is real. But if the two distributions look completely alike, how will it tell apart? It is like, you know, you are looking at yourself in the mirror. I take your picture and I take your picture that is there in the mirror. And I say that the picture in the mirror is the fake, whereas the picture of you is the real one. But do you think that it will be easy for anything to tell it apart? Do you see the problem there? Asif, the way I'm internalizing this is we are designing, we're making a discriminator good enough, such that it can generate a good generator at the end of it, right? Once it has done its job, what we want from this whole GAN is a good generator. Yeah, discriminator is sort of a judge At the end of it, it's a give up situation. Generator becomes very smart. And that's the goal of the exercise. We want a good generator. Exactly. In generative models, the goal is that P begins to look like Q. Q being the real data, P being the generated data, in terms of the language that we used a little while ago. So that is the main thing. But in this paper, they don't use P and Q, they use a P data and a PZ, you know, the distribution, probability distribution produced by this, the generator. So the idea is that these two should begin to match. Asif? Yes. If you, so when we're training the generator, would it be advisable to, let's say, we use already, we save our model and we use pre-trained weights, but add extra data, add extra true values, true data would that be advisable or would that skew the probability distribution no i didn't get your point so you take a pre-trained model and let's say let's say i was able to gather more uh more data for more more uh ground truth data and i added it to the model would that skew the probability distribution of my p original or may not see the the law of large numbers kicks in right so the more data there is the more underlying true probability distribution of the data surfaces so so long as you get more data of the same kind So long as you get more data of the same kind, the discriminator won't learn much from it. So if I started building a GAN, Asif, would it be advisable to just start from scratch, or can I train again off of what I've already built if I have more data? See, generally what happens is, just like with convulationalational nets you can take a pre-trained gang which is already pretty smart at generating you know which has been trained on let's say the cifar image or something like that image net and so forth so then you know it pretty much knows how to produce a reasonably good image now suppose you want to apply it let's take an example you want to only apply it only to specific pictures of trains locomotive locomotives and steam engines and so forth then what do you do you can do the last mile training you can take a huge amount of pictures which are only of trains and now you start training the generator with some pre-trained weights and you let the rest of the training continue from there the generator would very very quickly start imitating the train because it's not learning from scratch asif uh here there is no movement matching happening right no no generator models are completely different i mean gans are completely different. I mean GANs are completely different. So deliberately I told you two different algorithms just to give you an idea that see what ideas in GANs are new and what ideas are coming from the past. Even if there is movement matching they won't be able to distinguish. You won't be able to tell and what gans do internally is pretty abstract all right so now we understand that picture now i will this kind of mathematics here is a little bit abstruse so um i will explain this so you notice that this is saying that uh to update the generator you do you know the gradient ascend opposite of gradient descent because it's trying to maximize the loss and the generator is trying to minimize the loss it does gradient descent as opposed to ascent for the discriminator and so then he goes into this bit of mathematics which is very compact so i'll expand this mathematics out in a little bit more detail for you right so that you understand what is being talked about here all right but i'll give you a summary of what they're saying you're basically saying that the discriminator is trying to achieve the max, the generator is trying to achieve the min, right, of the cost function, of the sort of objective function. And when you do that, there are certain theorems. So certain results hold true. And this is very compact. Let's break it up into pieces and see how it all comes about piece by piece but before i do that let's do the the easy part of it the rest of the model is very simple it shows that this does very well in ms data right and different data sets are the adversarial net even in the first version ended up doing very very well right and it is able to fake digits, fake faces, fake scenes, and so on and so forth. And so I won't go into the rest of it. But let's go into the mathematics, which unpacked the mathematics in the paper a little bit. And this is purely optional, because this is going to get a little bit heavy. If you don't get it, it's all right right you don't need to know all this i doubt many people know about all this so remember the objective function the minmax minimax objective was the very simple statement saying that is equal to let's let's take a sample from the actual data right data x so then what you do is you take the expectation value of log of the discriminator the what the discriminator is predicting it to be the probability that it is real, that the discriminator says. You take this and you add another large quantity, another quantity, which is the, you take a data point from the sample, the fake data. So this is the real fake. right and so what you do is here you want to play the opposite game you want to make sure that log actually D D D of what D of whatever was generated by the generator. This is what the generator generated. Generator output. So GZ is essentially some fake image. And the discriminator should say that the fake image should be closer to zero. So now if you look at this function, this is where the min-max comes about. You basically say that discriminator will try to maximize this, and the generator will try to fool the discriminator and minimize this. But now the mathematical definition of the expectation value comes in. And so I will write this, g d or d g whatever it is is a p data p data of x actually I'll just use the word p d maybe I'll use the word p data of x log dx this is the meaning remember I told you that the moments are computed like that. Expectation is the first moment of it or the definition of fabric. This is over the x space. So you take the real data. Real data is x and you look at it like this. This is the first term and the second term is well you're doing it over fake data, fake noise input which is getting deformed into GZ which is then being discriminated by the discriminator and so this is a PZ the the distribution of times log one minus d of g of z very easy dz because here we are doing the expectation value with respect to the fakes, the noises, the noise being deformed into a fake. Now, you say, well, now let's look back at the generator. The generator was, we thought of the generator as this. Z goes in, and what comes out is GZ. And this GZ is pretending to be the x you know this is the image fake image so if you think of x as image noise goes in prior noise prior goes in what comes out is a fake image right so if you look at this if x is equal to gz then you agree that z is equal to the inverse function of x right so you can sort of if you have a picture and if you knew this function you could invert this function to figure out what z was now why would you care to know about z you don't it's just a mathematical thing you do to just grind through your mathematics and let's go grind through our mathematics. And therefore, the derivative of z, dz, is equal to the derivative of x. This would be a dg dx times dx, right? Dx times Dx, right? This is also written as g inverse x prime. Prime is just an abbreviated way of writing Dx. So we have a little bit of a result. This gets a little technical, but not by much. The result that we are coming to is that Dz is g inverse x derivative dz. By the way, dx. By the way, this is nothing but the Jacobian. Those of you who are familiar with the Jacobian, how do you translate, for example, if a data point is given by Cartesian coordinates x, y, and then Cartesian, but you insist on also writing the point as same points as r theta, you know, polar coordinates. So you remember that how do you do a translation from the xy coordinates, Cartesian, Cartesian x, y to polar theta. The way you do that is there's this matrix that comes in Jacobian matrix of derivatives. It's basically J is like a DX, DR, DX, D, theta. Then for those of you, I'm giving it to you. If you don't remember Jacobian, don't worry. In engineering math, we covered it. In the math of data science, we did cover it. If you forget about it, it's okay. This is this matrix. cover it if you forget about it it's okay this is this matrix right so d y dr d y d theta right this is your jacobian do you remember i don't know if you guys remember this from the past but this is it this is it how do you convert a variable into a variable in another system? You need a Jacobian in between, which is made up of derivatives. So we won't go into that. We'll just assume that this result is there for us. Given this result, let us reformulate the V and D. We write V D G as the first part remains the same p data p data log p log dx right dx dx this remains the same on the x space but now when you look at the z space it is a p z log 1 minus d of the what the discriminator thinks of what the fake that the, the fake that they generated it. So what we will do now is we'll replace all the Zs here with this relationship, X is equal to G, and Z is equal to X minus, oh, sorry, in z is equal to x minus, oh, sorry, x, right? And dz is equal to g inverse x prime dx. We'll just substitute it here. And when we do that, the first term remains the same here. It does not change. It remains x p data log dx dx. The first term is with real data, it will remain the same, but the magic happens here. So this becomes a little bit more complicated now. This is in terms of x now. So you would have a pz of the, you know, the fake distribution of z is G inverse of X, right? Log 1 minus D of X. What is GZ? GZ is X, right? X is GZ. So D of X. Then g inverse prime b x, right? So this is what this, this is a bit of mathematics that comes through explaining what is really going on under the covers and from that you now, suppose you define, if you look at this, what is this term? This term of PZ, G inverse X times G inverse prime DX. What is it? This you will realize is nothing but in some sense, it is PG, right? It is the PG of X, right? So PG is this. It is literally the probability or the distribution of the fakes, right? Distribution of the fake, right? So it is how the fakes are coming out. Now, what is your goal? You want to make your PG look like your PD, the distribution of the data itself, right? So in terms of this, there's a bit of a little bit of mathematical manipulation you do, DG. And this equation has a sort of a symmetric look. P data X log DX DX plus P G. plus p g this is the fake one p of x log d of x one minus log d of x sorry this is the this is the difference this is the ones that are produced by the fakes you want to do this so now is the game what will the discriminator try to do so let's think of this as a cost function cost from so assume that the generator is not so for now as you generator is not learning for a moment then what will be the cost function look like this whole G what is your objective given a generator you want to maximize maximize b g right in other words you want to maximize the separation between the fake ability to tell the fake and the real apart that's what the discriminator will do not learning the discriminator will try to maximize diff between fake and real, right, in some sense. So they will try to maximize this whole thing, this thing that you see here. It will try to maximize this thing here. And now, how do you maximize a function? One way to maximize it is at maximum, i.e., one of the things that happens at maxima is you, the derivative goes to zero. dx of v, d, g should to zero. dx of vtg should be zero. You know that this is a calculus thing that when you're at the top of the hill, it is flat. Flat means slope is zero. Slope means the derivative is zero, right? At the top of the hill. And so when you go back and feed it into this equation here, vg, and then, so therefore, if you really do this, what is the derivative? And we're almost there at home now. The full derivation of this paper is almost there. The full derivation of this paper is almost there. GD, D, D of DX is equal to, just to recap DX of what? Over the data, the data X log of DX plus integral of again X, but a P of the generated data of X log one minus P of 1 minus d of x. x, right? dx, right? So this is it. This is the, sorry, dx should be here and dx should be here. What you're trying to do is you're trying to take the derivative and set it to zero. So let's do that. If you remember, now comes the mechanical part of the whole thing, which is very simple. We all know that what is the derivative of log function? What is log? Do you remember this result from calculus? we'll use that and so this becomes what we are really trying to do is p data p data over dx are we together d dx when you when you take the derivative with respect to d and then you have the p data and then you'll have the dx part we can ignore the dx part let's look at the inside part minus and if you really look at it the derivative of 1 minus would be a p g of x and 1 minus d of x right and this integral you are trying to set to zero which means that you are trying to set this thing to zero right when you try to set this to zero uh hang on let me put back your integral here just to make it just to be sure we don't lose it but you know that this will be zero only if this is equal to zero, the inside argument is equal to zero. Otherwise, there is no chance that it will be zero. So what you do is you, I mean, it could be, but the most obvious is this is true. So now, this it is p data x dx is equal to p g x 1 minus dx now implies dx 1 minus dx is equal to p what is it p data over p g right did i get that one right yes and therefore this is, this is basic algebra. You can convince yourself that if I add the numerator to the denominator, it becomes just dx over 1 and pd over pd plus pg. Are we together? You can write it like this. So what it means is that the optimal value of the discriminator is when the probability distribution of the discriminator is like this. This is the optimal step that the discriminator will achieve, will learn. Then comes the, this is the optimal, you can think of it as your d star or something like that. Now, let us substitute this back into the equation, this d star in some sense. Substitute this back into the objective function dg and see what it looks like. And when you do that, the c, the cost function with respect to the generator, and when and see what it looks like. And when you do that, or the C, the cost function with respect to the generator, and when you look at it, you will realize that this thing begins to look like C, G, which is the maximum value from a discriminator perspective, it will become p data, data log dx. Log dx now is p data, x, P data plus P G of X here plus P G and again this is a little this is nothing very esoteric guys and this is just basic manipulation of algebra algebraic symbols are P of X data plus p g of x. Now you look at this and you begin to, that's when the whole equation begins to sink, right? It begins to look pretty nice at this particular point. So then people realize that, hey, we are looking at Kier KL divergence because if I did this suppose I wrote it as a half let me just say multiply the numerator by half and the denominator by half I'm allowed to do that half and half numerator and denominator both of which i can multiply by half and so this equation will become p data log p data x p data plus p g of x i'm just using d for data divided by 2 plus P G of X but so obviously this is DX and this is PG log of P G of X now things are beginning to sing a little bit if you are familiar with this this thing is here but then there was a log of half and a log of half. What is a log of half? Well, log of minus log of two. So this will become minus log of four, right? This quantity. So what it says is that now there's something very interesting about it. This is nothing but the definition. The definition, this is nothing but the very definition of KL divergence, KL divergence of data. So it is KL it is a KL of let me write it in the full form KL of a P data X given the given this other P data plus PG right over to obviously both of them are functions of X this is the KL divergence for the data and this is the KL divergence for the generator the fake right so this is a PG X given the same thing a P data SPG divided by 2 and so what you're looking at now is you you are saying that the the this thing the cg the cost function here which is the max of max d of v gd is basically kl i'll just use the word data plus kl generated fake minus log 4. now minus log 4 is a constant we don't need to worry about it what about this thing now one thing mathematically that's true about kl divergence is it takes on value always greater than equal to zero are we together it is always positive and because it's positive and because now we are doing a symmetric situation, we are actually looking at this particular situation is when you really look at it, you will realize that this is very interesting because these two parts together the definition of Jensen Shannon divergences literally this a 2 times JSD of a PQ two distributions is equal to p well plus q right kl of p and kl of q plus kl q given p divided by well not divided by two this is the let me just put it here this is the strict definition of j that so when you look at this situation here. This is the strict definition of J that. So when you look at this situation here, your P is the data and the Q is the fake, you realize that what you're looking at is the cost function is equal to twice the, because there's no factor of half here missing. P data and PZ of G, the generated data, right? Minus 2 log 2 or log 2 or minus log 4, whichever way you prefer writing it, right? So this gives us that given G, the maximum that you will achieve, the best discrimination that the discriminator can do is this. Now, what is the generator going to do? It is going to do the opposite. It will try to now shift towards the distribution, p data, and they will keep doing it back and forth, back and forth. And ultimately, it will catch up. So this now, why did I just do this long derivation? Actually this long derivation is very sort of condensed form. This long derivation is put there, here. So now let's look at this paper and see if it makes sense. What it is saying this is here this is our this is our this is your VD right and what you're seeing is you're trying to do in our notation you're trying to do this and you want to maximize this because you know you're in the business of maximizing it you're trying to find the max of this vd and the way you would do that is you take the derivative and set it to zero on the other hand what is the generator trying to do it is trying to minimize the discriminates discriminators ability to tell the real from the fake. So that is what it is. Now, if you go through the, now they make a claim that the best situation that you can have, and let me mention this, the minimum value, see, when I go back here, at the optimal value, right, you have a situation where you are looking at terms like this, right, a PG over P data over P data plus a P generated data. That is where G achieves its optimal form, right? You know, that is where the optimal values happen. And that is all that he's saying in the paper. They are saying in the paper that they prove it, that where are we? Now you'll see the same derivation here. They're saying that at the fixed point the the best that you can do is when at that point this condition is true right the optimal discriminator value did you see that the discriminator value achieves its best value here where did we do that let us go back and look at our derivation in this derivation I hope you guys are still with me and I've not completely lost you. If you go back and look at this result, do you see this result? The optimal value of dx is what? Distribution of the data divided by the distribution of data plus distribution of the generator, the fake, isn't it? So d star, which is the best value of data, is that. And that is what he's trying to say in the paper. That's what he says in the paper. They're not trying to say it is. Now, does this look pretty familiar to you guys? Does this make sense? And the proof is very simple. We literally went through this derivation. You start with this, literally the objective function, write it out in parts, and then you argue. You keep going down the arguments and finally you come to this conclusion when you substitute it back, just as we did. Then the other part is that the global minima of the virtual training is achieved if and only if. So when do you achieve the global minima? You achieve the global minima when... Let's go back down here. So you realize that it is made up of these two terms. When, what will be the global minima when these terms become zero? Because they can't be, they can't be smaller than zero minimizer zero so when will they achieve zero they will achieve zero when like for example the log of this when and you can take it as a fact that when what is the so what is the kl divergence between a distribution and itself? So suppose I'm looking at the data and PG. When will this achieve a minimum? Is zero if and only if? P data is PG. In other words, there is really no difference between these two distributions because KL divergence by definition measures how different they are. So this is your optimal point, right? And that is what we derived. And that is all that they're saying in the paper, the second theorem. They're saying, okay, now look at this. This will happen theorem one, convergence at each step and so forth. Where is it? and so forth. Where is it? If p data is equal to this, and at that point, what are you left with? You're left with that minus log four, isn't it? Because the divergence terms will go to zero and that constant of minus four will remain. And so this is the same derivation that I take you through, but in a far more condensed form. Right. And so I wouldn't go into enough. You have to prove that it converges and so forth. And so there it is, guys. That is the paper. That is all that there is in this paper. How many of you followed the derivation more or less? More or less. Good. Okay. So, all right. I'll stop the recording, guys. And... Thank you.