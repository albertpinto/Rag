 All right folks, I will get started. Last time, which was two days ago, we talked about what is generative AR. If you recall, we talked that it has a long history. It has become the buzzword in 2023, people often consider it synonymous with large language models. But it turns out, and I hope I did give you a convincing history of this subject that made you feel that it has been there for a very, very long time, since 1980s. Machine learning has had two parts, the discriminative and the generative. I mean, amongst other things, it has reinforcement learning and so on and so forth, but generative has been there for quite as long as discriminative machine learning has been there. Though in broad usage, generative is always a little bit harder. And so it took a while to find its stride. Now for the last decade it has been, it has had quite a dominant influence on the field of AI. Today I would like to go into one of the earliest generative AI models and the reason I'm going into it is it's simple to understand. Because I want to give you folks a flavor of the technology behind it, the neural architecture behind it. So I will be writing some things on the board and explaining it and then I'll show it to you in action. I hope that this will be a fun exercise. So this is an architecture that came out in 2014, almost a decade ago. This started out from an idea which has been there in pretty much artificial intelligence. It's a kind of algorithm that is called Min-Max, where one thing tries to minimize an objective function or a goal, and the other thing, the other agent tries to maximize it and the best way to explain this min-max idea and how it relates to generative AI is to start with a sort of example imagine that you have a counterfeiter somebody who counterfeits the US dollar let's say and so we will take a person. This is the counterfeiter. The counterfeiter. What the counterfeiter does is takes just a random input, a random trigger it doesn't matter what and every time it produces a new currency let us say it produces a new currency and i'm writing it as this z is just a random trigger because it it wants to on a whim produce a new kind of currency it produces a currency let's say that it looks like a dollar bill there you go something like a dollar bill with 100 it produces something like this this is the this is the output that it produces now people often call this the generator Now, people often call this the generator, because it is if the counterfeit is too good, the markets would be saturated with fake currency and obviously you can take down the economy in effect. Then everybody would start doing that. The reason you don't see too many counterfeits in the market is because, obviously, because you have a cop. Imagine a cop here or expert here who can take a counterfeit and this counterfeit is let me just call it actually I wrote it as x let me just call it given this this and this will produce a z of some random input it will produce a counterfeit gz think of it as a function i give it noise some random value and it manages to produce a counterfeit and every time it produces it looks slightly different now why would it like to produce a slightly different counterfeit and here is where it gets tough it turns out that the counterfeiter, the crook, has no idea whatsoever what the US dollar bill looks like. Let's say that the person has never seen a US dollar bill. And so in the beginning what the counterfeiter might do is, or the crook might do is, it may draw like a, I don't know, a bird and say, ah, here is my $100 bill. And the cop will look at this and say, ha ha, this is wrong. Right. But the next time, the generator will produce a different sample because the generator has absolutely no idea what a counterfeit bill looks like, what a genuine US dollar bill looks like. And so the cop will say, no, this is not what it is. But then the question is, how good is the cop and how does the cop learn? Now, what you do is you imagine that you have a pile of real dollar bills. And so what happens is every once in a while, randomly, you either feed it real data, a sample, an example of the real data, or you feed it an example of a counterfeit data and the cop has to tell whether it was real or it was fake the expert has to tell that and in a way the expert is also learning but the expert has an advantage expert is told the answer expert is told that hey i gave you this. What did you predict it to be? Let's say that it predicts it to be real. So then you say that was right, because it was actually real. You make predictions where a hat in this field of AI. So you suppose it was real. And then you say, oh, yes, it was real. so now the now what would the expert do it will say oh that is what a real bill looks like now suppose it gets a counterfeit and it says real accidentally it says real and that would be a mistake because it was a fake right and so the expert realizes that it made a mistake. What the expert produces in a more technical jargon is probability of real dollar bill. So what should happen for this case, for the case one, probability of real should be close to 100 percent, should be close to should be close to 100% should be close to should be this is the should be part should be and according to if the if the expert has learned then the probability that it should assign to a fake bill should be approximately zero percent probability. Does that look like common sense guys that you should say a good expert should assign. See, there's always an element of lack of surety to it, but it should do that. But what happens if it makes a mistake. And this is the beauty of AI, The intelligence part that we talk about in artificial intelligence comes from the fact that machines can learn. So you call a human being intelligent, the human being is a quick learner. In the same way, machines, the intelligence in a machine is its ability to learn from data. So when you tell it that it made a mistake, in the more technical jargon, you say that there is something called a loss. Loss is a measure of how wrong it was, how loss functions. People often use the word quite often, and you'll see it often, and you'll see it used in the literature the loss function loss function is how the overall how wrong was this expert and when this this expert is often called the discriminator because it discriminates between the real and the fake so So the discriminator of the loss function of the discriminator, what you want to do is if it is very good, then the loss would be less. On the other hand, in the beginning, let's say that the expert or the discriminator is also not trained. Then what would happen is the loss would be high. It would make a lot of mistakes. high it would make a lot of mistakes and when it makes a lot of mistakes then comes the question how do you teach it by showing it the real answer the loss will tell you how wrong you were and then comes the magical part that makes all of machine learning effectively or much of machine learning possible, what happens is that you can do something called a gradient descent. The discriminator has lots of weights and by the neural networks is made up of lots of parameters inside it. Those are weights, those are knobs. One easy way to think about it is that the discriminator has lots of knobs easy way to think about it is that the discriminator has lots of knobs and as the signal goes through it makes a prediction but if you turn the knobs then the prediction will change and now what happens is any one parameter which is sometimes called the weight you can update the weight from the previous value by doing something called a gradient descent step and I won't get into that it's called a gradient step now that's quite a mouthful and since some of you are non-technical audience here uh i would suggest um don't don't at this moment pay too much attention to it except to uh remember one fact all machine learning algorithms a large class of them there are two ways once you can learn by reinforcement learning punishment and reward but the other class of machine learning algorithms they learn through a process of or predictive models they learn to think or in some sense a form a form a conception or a function of the what the data is trying to say by minimizing the loss and when you try to minimize the loss there is a very efficient way to go about decreasing your loss and therefore learning at the fastest way possible and that way is called gradient descent the gradient word this inverted triangle refers, it is basically very similar to derivative. One what in one variable you would call derivate derivative or derivation of a function. It's a derivation of a function. If the function is made up of many, many, many. Parameters are variables. So this is the celebrated gradient descent equation but i wouldn't go into that suffice is to say that if you can quantify how wrong the discriminator was how wrong the cop is you can do something about it you can turn the knobs inside the cop in such a way that now the cop in the next time around, you show the cop the same dollar bill and it will say this is real. And if you show it a fake dollar bill, it will say a little bit less likely to be wrong. It will still be wrong quite a bit, but it will be a little less wrong. And that is one little step of learning that the discriminator has gone through now think in terms of the generator if the discriminator knows that this was a fake the generator would say oops that didn't fly so what can i do maybe with this bird i can remove the tail let's see what happens if I make small changes to it or let me generate another dollar bill I have no idea what dollar bills look like but let me generate something slightly different right I have a certain conceptualization maybe my concept of a dollar bill is it has a bird on it let me make it a little bit less bird-like. Make small changes to it and then try to push it. And then see what happens. And let's say that if the discriminator is a little bit less sure, it says, well, no, this is not obviously a fake. But maybe it is a fake. It very probably is a fake. Now the generator has had a tiniest bit of a win. The generator says, ah, that's good. It means it's moving in the right direction. It will do more of the changes that will confuse the cop. And even though it has never seen a real dollar bill by making small adjustments and just observing what did the discriminator think about it what did the cop think about it you just want the cop to be a little less sure till you finally want your end goal is as a as a counterfeiter as a fake general dollar bill generator to completely confuse the experts completely confuse the cop so if the cop is totally not sure or better still is quite sure that what you produce is a genuine dollar bill then the generator has a win the generator has a win. Are we seeing that? And so we have two very opposite forces at work. One force is trying to pass counterfeit data, counterfeit dollar bills as real and the other forces the cop the discriminator is trying its level best to not mix up the real from the fake and to be able to do a good job at discriminating and this process is called adversarial learning it is very very actually, if you take an analogy from real life. When you play a game, let's say that you're playing lawn tennis, what is the best way for you to learn tennis? You would like to learn tennis by playing it against a friend. And as you play, you throw the ball at all sorts of places onto the court, hoping that your friend, your opponent would not be on the field, would not be able to anticipate your move where you are next going to throw the ball. you throw the ball in a corner what does your friend then realize that ah you tend to he loses but he realizes or she realizes that well you tend to throw balls into the corners of the court of the court and so the so your friend now becomes a bit smarter is on the watch out for the next time that you may throw a ball into the corner ends of the court right and so is ready for that so now you lost that opportunity and now you try some other trick and so gradually what happens is as you and your friend keep playing with the goal of defeating each other you both over time become better and better and better. Right. And all the easy moves by which you could win against each other are gone because both of you have smartened up. And at the end, you see that it becomes a game of experts. And we see that when you see a World Cup match or whatever those are called, Wimbledon and so on and so forth, of a tennis game, you notice that the two sides are fairly equally matched. They are playing a superb game. Like it's just a pleasure to watch, which is why you have spectatorship in perhaps hundreds of millions of years around the world of a let's say tennis match or a cricket match or a soccer or whatever it is because it is it is the perfection that has been achieved by playing an adversarial game over thousands and thousands of games and millions of moves and things have been learned and tried out and their defense and offense have been tried out so adversarial learning is all around us this this concept of min max sort of a if I I try to maximize the times that I can get the ball at a point on the other side in such a way that my opponent would not be able to field it. On the other hand, my opponent is making sure that they can always field it. But when they throw the ball onto our side of the court, we are not able to field it. And that is adversarial learning. And that is adversarial learning. And a very similar situation happens here in this kind of a situation where a generator and a discriminator, generator tries to produce fake bills and the discriminator tries to tell a genuine bill, dollar bill, apart from a fake dollar bill. And so gradually they both become very, very good. If they run it for a few million times they become very good each time they learn a little bit just the tiniest little bit for example if the generator produces a bird on a dollar bill and the discriminator says aha that's a that's a total fake and the generator then slightly changes the door a bit and the discriminator still says it's a it's totally fake but is a little less sure generator has found a little just enough crack in the door to pursue that avenue and exploit it to see where it doesn't lead and the beautiful thing is in this game, if you played sufficiently long, the generator can start producing very genuine dollar bills without ever having seen a dollar bill. Because now through this process of learning, it learns in effect what dollar bills look like. in effect what dollar bills look like. Likewise the discriminator, even if the discriminator had no idea what a dollar bill looks like because it is given a dollar bill and told this is real, make a prediction what is it. It makes a prediction and it may say whatever it may say it is real or not, probability of real is high or low and then it is told the answer it said actually that was real then the discriminator learned something something from that exercise so both discriminator and the generator they gradually become smarter and smarter and soon you reach a state at which they be though they both become insanely good at it. They become like world class tennis players in a sense. If it were a tennis game or the generator becomes world class counter-tutor. But then you ask this question, now what is the purpose of playing this sort of a silly game right why would you want to make computers play this game right or make AI play this game and the answer to that is actually quite interesting when the generator and the discriminator have become very good then you can throw away the discriminator and you are left with a generator and the generator can produce for you infinite variations of the dollar bill each of them looking very very very realistic or for a check or as we saw in the last session you could have a train the generator and the discriminator to tell whether something is a human face. In the beginning, the generator might be drawing a brick or a house or a kangaroo, and the discriminator is immediately able to tell, well, no, no, that wouldn't do. That's not a face. But gradually, as the generator and the discriminator play this adversarial learning game after huge number of rounds of learning on both sides finally you'll get a really smart generator and a really smart discriminator and if you remember we went to this website and if you remember we went to this website this person does not exist and what did we see we noticed that the process the generator was able to generate very very realistic pictures of human faces faces of people that simply did not exist. And then you realize that feels like magic. How does this generator produce, when you think about it, how does it produce such realistic human faces? The answer to that is it has been penalized for producing all sorts of things that don't look like a human face. And so it has learned from that it has learned from the last function. And that this combination of generator and discriminator, when you put them together, there is a word for it, this kind of an architecture, neural architecture, it has a name, it is called, and now the name would look very obvious to you it is called generative adverse say real networks Adversarial networks. They are often abbreviated as GAN. Generative adversarial networks. So this is one network. This is a discriminative network. And these two networks, they are playing an adversarial learning game with data. And so this particular architecture was actually thought of in 2014. And here is how the story goes. And it was one of the pioneers in this field, Ian Goodfellow, apparently. So this is the story behind it. He apparently was at a pub and thinking that, see, in those days, we are talking about 2014. So let's rewind a little bit to see what those times looked like. Around 2012 to 2014, there was a sudden sort of a quantum leap in the success, successes of deep learning or artificial intelligence. For example, there was the so-called handwriting recognition. Handwriting recognition is important. People used to write on the envelope and letters, for example, had to be sent across the country if you could recognize the handwriting very accurately and convert it into well digital information then the postal service could route your letters very effectively for at least one important use case and today if you ever look at an envelope from your friend in which your friend has sent you a greeting card and the holiday seasons are coming you'll often have friends who will write your name and address on the envelope and put a postage stamp and put it in the mailbox if when you receive it observe very carefully at the very bottom of that envelope you will notice little squiggly marks those little squiggly marks are actually the computerized encoding of your address and what happens is that when you put your letter into a mailbox the the collection the postman collects all of that puts it into a big sorting bin, and from there the computers take over. They recognize the handwriting, convert it into little text, and then from there onward automatic sorting and routing machines take over and your letters actually run. They're just routed to their transportation systems and everything. And we don't pause to think about it, but there was a time when letters cross-country would take weeks to go through, and often letters were lost. People would write the same letter a couple of times over for it to go through. When I was growing up in India, it used to be pretty common for letters to take an extraordinarily long time to reach, especially if they were going a thousand miles. But along the came these automatic routing and sorting algorithms and then these algorithms that could detect human handwriting and convert it into an address and then it would take over. And today we sort of assume that if you write a letter within 10 days it would cross the ends of the country. You can send a letter from here to Los Angeles, from here to New York and probably in a week or 10 days it will reach the other end. And let us rarely get lost. The so-called dead letter problem has practically evaporated. Very, very rare to have a letter go to the wrong place or to no address at all. It is not so common for that to happen anymore. So that is partly because of AI, partly because you have very good handwriting recognizers. But some of these developments, while Linet, the initial handwriting recognition system that was deployed, and one of the grand successes of deep neural networks and artificial intelligence was adopted early on at the turn of the century much desired there was there was a huge scope for improvement so there was a grand challenge put uh by the national institute of standards and technology they created a data set which was a modified and a modified version of the data set is called the digits mnist digits data set it contained all sorts of handwriting handwritten let's say a digit so human beings may have written three like this and six like this now when you look at a six like this it's a little hard to tell is Is it a six or a zero? Right. You may write one like this and you may write seven like this and it becomes a little hard to tell whether it's a one or a seven. So you have all sorts of examples for written like this and for written like this. And it's hard to tell whether it is a nine. Right. And so on and so forth human handwriting has a wide variation so they they created a data set of about 60 000 handwritten letters or digits and the idea was it was a benchmark which algorithm could classify or recognize the digits best now to know that a three is a three is a discriminative task it is like a cop saying i know what it is it's a real bill so when that was happening, the deep learning, there's a particular class of neural architectures called deep neural networks. They were having a renaissance. They were having a very, very good time, progress. And around 2012 to 2014, there was this advancements one after the other, AlexNet, ResNet, and so forth. And they, in a very quick succession, they started beating the state of the art. And today, I mean, they already started achieving accuracy that surpassed human accuracy. Human accuracy is, we get it wrong about 5 to 7% of the time. One of the great milestones was when neural networks could recognize digits even better than we could today human beings are no match we still are sitting at five to seven percent error rate but these ai algorithms they are sitting at 99.9 something 99 accuracy right and the one areas where they don't do well are actually areas which frankly are not legit cases they are so badly written that no human being can make head and tail of it right but it is there it's part of the data set right so in other words the ai algorithms have achieved near perfection. And that was already happening around 2012 to 2014. And so the question that people began to ask is the discriminative models like this classifier, this recognizer of the digits is doing so very well. But the generative models were lagging behind, relatively speaking, they were not having such a wonderful day. And so, in a way you can say that great computer scientists and AI researchers like Jan Goodfellow were asking this question. Can we somehow harness the power of the discriminator to create a better generator? Discriminators have become good, classifiers have become good. Can we harness that power to create a really powerful generator? And that is how it, and the story goes that Jan Goodfellow was sitting in a pub he was slightly inebriated you know once a few beers down we tend to be a little bit more optimistic some of us so he sat down and he began to let his imagination run wild and apparently he came up with this idea that we could do this he got very excited he came back and I'm told that he he coded it up from in in one of the in an interview that he gave on I believe the Lex Friedman show a podcast he said it this is how the story is that he literally coded it up in a couple of hours and he was seeing results all the people all his fellow researchers sitting at the pub told him this won't work this is way too crazy right you can't use a discriminator to train a generator it looks it doesn't make sense there are many pitfalls in this but he did it in a couple of hours and then the rest is history obviously by the time the paper came out you will see that there are many authors to the paper they all contributed in implementing it or testing it out making things happen and so on and so forth but this is how great ideas like this came and i would say this for generative ai this was a very big milestone. Because it, for better or for worse, it was great. Out of this generative adversarial networks came the ability to do like we saw two days ago, to do neural style transfer, for example, you could write or you could transfer Van Gogh style to your photograph, right, easily and create a fake Van Gogh of your scene. There are many good uses of that generative style, excuse me, neural transfer. You could do data generation. You could use it for medical diagnostics. A lot of it. There are many, many use cases that we talked about the last time. I'm not going to rehash it, but it is generative adversarial networks have been, have had a transformative effect on the world itself, not just AI. But not all for the good. It also unleashed the world of fake images for example now fake images may not be that harmful but uh it can be so for example today in one of the most startling demonstrations of this was at mit i forget the year i would like to say it was 2018 or 2020. The professor started the course on artificial intelligence by saying we have a special guest to introduce the course. And that special guest, it turns out, was Barack Obama, who on the video gave a very good introduction to the course and then and it was very realistic Barack Obama's voice Barack Obama's gestures his facial expressions his accent everything was captured and then the professor mentioned that this was actually not Barack Obama. It was him speaking with Barack Obama's style of speaking, his voice, his mannerisms, his gesticulation, as his Barack Obama's identity imposed upon the content that the professor was speaking or I think it was one of his graduate students who were speaking a teaching assistant who was speaking right now why that is a most shocking and powerful demonstrations of the power of generative AI or of GANs you realize that it also unleashes the potential for great mischief. Today, we can take a couple of minutes of your recording. I can take just a few seconds of your voice and then have a voice recording of you saying whatever content I give. And all your friends and family will attest to the fact that it was you who said it. They hear you, even though it was not you. You can generate fake videos and so forth. So the other side of generative AI always is the potential for abuse or the possible, in what some people have called, very evocatively, a possible collapse of reality a complete blurring of the boundary between what is fake and real and if you think about it that is the entire architecture of a generative adversarial network the generator tries its best to blur the boundary between what it produces and what is genuine and when it succeeds it just succeeds spectacularly so what I would like to do is I would like to in the last 15 minutes give you a more a realistic picture of how this happens I told you that it tries to figure out what does the reality look like. Now I will use a small bridge and this may get a little bit technical to understand what it is about. Suppose a data, real data is circular. Let's say that it is like this. is circular. Let's say that it is like this. This is the real data. In other words, all the real data could be represented as in a page, if you draw it out, a two dimensional data in a page, they are all limited to the circle or a band or a ring. All the data belongs to a ring. Anything outside is wrong. This is wrong. This is not true data. This is not data. These are not data. But the only thing that is real data are points here in this ring, yellow ring. This is the reality. So this is the real data. Now imagine that you have a generator which is trying to take an input. What it will do is it will basically imagine that it gets data. Let's for the sake of argument, imagine that it gets a page, oops, sorry. It gets a page and it can take points somewhere here. What does it need to do? It needs to somehow fold these points into such a way that it only produces points that belong to the yellow band, produce a point that belong to the yellow band produce a point from the yellow bank because if it does that if I produce so let's take a point an orange point no let me take a color that would stand out let's say this color color that would stand out. Let's say this color. What color would stand out? Okay, let's try a lucky. This color. Do you see this big violet dot here on the screen? If the generator could produce this point, you would realize that that point is as legitimate as it could be it is very very hard to tell it apart from a fake i mean tell it apart from a real point isn't it because it belongs to the band or the yellow ring band of real points are we together so in a way the goal of the generator is to discover the data distribution and that is the statement made mathematically that generative models they discover the actual distribution of the data the complete distribution how is the data distributed in space the complete distribution, how is the data distributed in space? Because if the generator can discover that, what is realistic data looking like? Then it can produce infinite samples of realistic looking data, or more poetically, it can produce infinitely many faces that all look real. Do you see that, guys? So this idea, I would like to show you with a real lab example. At this particular moment, give me a moment to do here. So let's look at this. If you folks are able to see my screen, I'm going to show you a fun thing. I just took the circle. Let us say that the reality is like this real data are the green data points. What we are going to do is we are going to take one step of the generator and the generator has produced this data here. This sort of a data. Now a sample. You see generator is producing data points here. I don't know if you guys are able to see my mouse. It's producing data. Actually, this is too. It's producing data here. But this data, if you look at the discriminator discriminator says well i am a discriminator also doesn't know where the real data is and where it is not so if you look at this box or this big box discriminator is saying maybe real data is everywhere so the cop is confused doesn't quite know and so we will let both of them make a step. And what happens is that the. And you can keep on making steps. What happens is that the discriminator is noticing that discriminator tends to give higher truth values or trust data on the right hand side of this box. Do you notice that this is darker green? And so what the generator does, the fake data maker does, it is trying to push the data in this direction towards the dark green areas. Do you notice that these points have a tail to them? And all of those tails point towards the direction of the dark green. Right? And so let's go a few steps. And you notice that it is chasing it. And for some reason, the discriminator is feeling that, oh, my God, fake is coming from behind. So it is putting its trust more and more under the boundary of this page. And what does the generator do? The crook do? It tries to produce data closer and closer towards where the generator has trust. And now notice that something very interesting has happened. The generator doesn't trust data coming from here because it has figured out that that is where the fake data used to hide. And the generator has a lot of trust data in the data, dark green, on data in this area of the page. And so the generator, the fake data maker is trying to capitalize or exploit the belief system of the generate of the discriminator of the cop and so we will do this and you notice something very interesting so i'll let this thing run itself out for a few more and if you notice the loss function of this the generator is having a pretty good loss and the discriminator loss is decreasing do you notice that the discriminator at this moment is doing pretty well the loss is decreasing means it is beginning to figure out what game it needs to play to win against the cop right and the discriminator the cop is having a rather bad day. Its losses are increasing. So let's continue this a little bit more. Little bit more. And by now, the discriminator. Sorry, the generator loss is beginning to climb up, the discriminator is picking up. I'll just let it run for a few epochs. And you see how they go about trying to fool each other. But at the end of it, observe, and I'll just put the generator thing also here. This colour, this purple is what the generator believes it is producing. And do you notice how the purple thing is now beginning to overlap the real shape of the data? And we have gone through 900 epochs. 900 epochs means, and let's say that there are 100 points. It's almost like 900,000 times or a million times, it has learned. Both of these guys have made little efforts to learn against each other in the adversarial game. And it sort of continues quite a bit and by now you see if you were to see it it is producing pretty realistic points if you notice this what the generator is producing are pretty realistic the points are coming through and we can let this game go through for a few thousand rounds and then you'll soon notice that discriminator and the generator, they begin to converge, they begin to get pretty good at this game. After a little while you'll realize that the generator, the discriminator would be somewhat at a loss. Its loss of the discriminator stabilizes, whereas the generator tends to do better and better and better after some time. Now, I will just stop here because I think it is pretty obvious what is happening. If you look at the shape here, let's look at the shape. Look at this shape here. When you look at this shape, you pretty much get the sense that the generator has figured out how to generate fakes. It is still making mistakes. It is producing some data points in the center which the discriminator can tell and say, hey, that's fake. But still, the generator is doing a pretty good job. And even if you stop here, you would agree that the generator would be able to produce predominantly realistic data points, predominantly realistic data points isn't it and that folks is generative ai right in a very visual way that is generated that's gans our one of the first power very successful there were many generative algorithms as i talked about since the 1980s but this one really caught people's imagination and you can you can say to some extent that the gold rush or the renaissance of this sort of generative AI started in 2014 with algorithms like this the other algorithm that is similar to this is that of auto encoders variational auto encoders, which I will keep perhaps for another session. Actually, when I do the actual course, which is a more in-depth course, a seven week course, we go far deeper. We do a lot of lab exercises and coding exercises, and we go into the mathematical theory of all of this generative AI. In other words, it's a full formal course. There, when you do that, you will encounter these things at a much greater detail. But this one, because it is this particular sequence, because it's meant for a much broader general audience, I'm a bit hesitant going in deeper into it. And so with that, our hour is almost up. I would like to take another example. Let's take this example. These three clusters of points are the real points. And I will just let the system run and see how well it does. You look at this, the fake data is the generator is having all sorts of theories of what the real data probably looks like. And sometimes it is grossly wrong. Like at this moment, it's grossly wrong. And it is trying to make head and tail of it and it's having a pretty bad job of it. So I must stop it here or let me run it a little bit. Yeah, I'll stop here. Do you notice that at this well in this particular case there are points in between which are totally fake but the the realistic points that it is producing belongs to just one cluster right actually let me rewind it and play it again because it's a problem that used to be there in the early gans but we have found a way to solve it yes i'll stop here do you notice that all of these points are near this one cluster? And it gets a little bit better, but'll let it run a little bit more. Yeah, I'll stop here. So you notice that all the points that it is generating is near this cluster, the generator. It's somewhat like, you know, the generator has figured out how to write the write the digit tree. And it had a pretty good success. So it reinforces itself, it doubles down on that and says, let me write all sorts of fake trees. But it is not producing other digits. That is called mode collapse, when it pretty early on figures out that, hey, I hit the jackpot. I produced something, a shape that fooled the discriminator. Let me produce lots of these sorts of shapes. And then it doesn't produce anything else except trees. It used to be a common problem in the early GANs, but today we know how to solve it. And when you actually do the course with me and you do the lab exercises you will learn about all of those techniques practical techniques to prevent a mode collapse see the last thing you want to do is when you are told to produce all sorts of human being human faces the only thing you produce is the face of five-year-old boy lots of them so soon people would know that the easiest way is if it is not a five-year-old boy, it's probably genuine. But if it is a five-year-old boy, then maybe there is a chance of it being fake. So that would be no good as a generator. It doesn't cover, in other words, the space of all realistic examples. And that has a lot of practical implications in terms of generative AI so I will stop here right uh and I will take I will take questions now from the audience thank you asif we have a lot of questions in the chat uh we'll start with that and if you if you any one of you have a question please please raise your hand through the reactions button on the bottom of your screen. Let's start with the first question from Ramki. What if the discriminator produces only Boolean output, match or no match? You don't do that, Ramki. See, the nature of machine learning is, and remember, so since you are, Ramki, you are mathematically trained, so I will take the liberty of bringing in the mathematical notation. See, you need a loss function, and you need to be able to take the derivative of a loss function. You can take the derivative only of continuous differentiable functions. So the discriminator must produce a probability distribution, must produce p, which is a continuous differentiable function of the input. Are we together? And if you can, by by its very nature it's not a boolean it's not a true false but we often categorize it as a boolean by saying if the probability is more than 50 it is real if the probability is less than 50 it is fake right so you by thresholding the value of the probability you make it into a boolean or a binary answer true false answer fake or real answer but the you make the machine produce a continuous differentiable function because you have to take its gradient and you need the gradient to do gradient descent and the back propagation which are the pillars of neural networks does that answer your question which are the pillars of neural networks. Does that answer your question, Ramakrishnan? Okay, we'll go to the next question. Thank you, Asif. So the next question from Jeevan, when do we know when to stop these iterations in the context of the training that you're initiating? Oh, yeah, excellent question. Let me show you in a realistic way see you notice that here look at the top corner here the loss functions right when the loss function begins to stabilize and you don't see any further improvement the loss or the so for example this is the metric I believe the accuracy metric or whatever. Once you stop seeing improvement in that, then you know that it's time to stop. In practical terms, that's the best way to do that. You reach a point of diminishing returns. The generator and the discriminator become extremely good. So think about it for athletes um or painters or musicians at the end of the day great world-class musicians by the way this was one of the lessons in that book outliers they practice 10 to 14 hours a day day after day after day but at some point you have to say you know you have reached a saturation point these people have become world-class musicians right so it is somewhat like that with the generator at some point the generator becomes a champion faker it generates well i wouldn't use the word fake in a derogatory way i would say it generates realistic points, things that mimics reality to a very high degree of fidelity. And at that point, your purpose is served because what do you want to do? You use it, you want to use it for some real value thing. For example, you may be generating data for downstream experiments, right? Downstream tasks and so forth. And by the way, in data generation, especially in domains where data is hard to generate, is one of the excellent and great use cases of the GANs in the generated AI. Thanks, Asif. We'll go to the next question in the chat and then we'll come back to the hands. Gopi has a question. Is the real samples equally random or is there an approach to introducing a real samples from a pile of currency to speed up the learning? Does that show any bias in learning? So it was primarily with the example of currency faking. Yeah, people have asked this question, but the answer these days is you want to randomize the input data. You don't want it to, you don't get anything by having a particular order or sequentiality to that. Like you don't want to by having a particular order or sequentiality to the like you don't want to serve only one dollar bills wait for the discriminator for the generator to catch on to one dollar bills and then introduce the hundred dollar bills it isn't like that you just completely randomize it and then you keep and then you randomly to the discriminator you either give it a fake data or you give it the real data and ask is is it real or is it fake or if you look at the mns data set what you want to do is mix up all those 10 digits zeros and instances of those the real ones and have the generator figure out what the what those 10 digits from zero to nine look like uh you don't you don't bring in any order specific order thank you asif um we'll go to the next uh question uh from can you please unmute yourself and ask the question hi asif uh thanks for this excellent course uh what if the discriminator was smart enough figures out that the generator is pushing fake data and uh just like the the generator is getting smart and and and and you know pushing data it says you know i'm going to either data it says you know i'm going to either give him fake feedback so that he doesn't get any more information from me and doesn't learn any more from me yeah so i'm sure first of all there are two parts to your question one is what if the discriminator becomes pretty smart and the second part is what if it starts not giving information to the generator no you prevent that but in the way you set up your two neural networks you don't let the discriminator um hide information from the generator in other words the back propagation of the loss is equally served to the generator and the discriminator they both get a chance to learn from it it's it's the way you set up the whole algorithm the first part is what if the discriminator becomes pretty smart early on in fact that was the problem early on what happened is if the discriminator is pretty good you you tend to see more collapse like you know like in this particular case so here itself there is somewhere a knob which we can turn let's see where it is by which model graph yeah see here what you can do is suppose you you notice that the learning rate of the generator is a little bit more than the learning rate of the discriminator if you make the learning rate if you make the discriminator way smarter the generator gets into trouble actually so for example if I make this Adam let's see if this happens in this particular case or not and it has okay so this has a pretty good learning let me reset it and let's see what happens uh no uh sorry i'll stop it and yes and let's try this you realize that the discriminate the generator is sort of struggling bouncing all over the place and it's it has a tougher time to get to an answer so So one of the things that people did, they learned early on, is that, see, the discriminator has one edge, one advantage. It actually is told the, you know, it knows, it gets to see the real data. The generator never gets to see the real data. It's almost like generator never sees the actual digits or what it looks like, or never sees the dollar bill, but the discriminator does. So a lot of, over the years, people have found ways to slightly cripple the discriminator so that it doesn't have too much of a lead over the generator. Okay, thank you. Thank you,ph thanks sancho uh we'll take the next uh hand uh alicia if you could unmute yourself and ask the question well supposedly i you know i i Well, supposedly, you know, a member of my family has recommended me just to start. Could you please say it a bit louder, Alicia? Unfortunately, I'm having a bit of difficulty hearing you. Can you hear now? Oh yes, now I can hear you loud and clear. I'll go straight to the point. You know, I just thought, you know, an idea. Is there a possibility to have the generator and the discriminator go hand in glove just to cheat humans? And, you know, I'm engaged in some writing. Well, when, let's say, well, I live in Argentina, sometimes it's quite challenging when i have the my mind set to you to to write and all this you know when for example uh they create the process and when we have the generator and the discriminator working together perhaps at a certain point they you point, they stop because they are like kind of overwhelmed by too much data. And they say, well, no, I stopped this and I leave this, I put it in this box and I set this aside for a later time. And I'll continue with another task that the operator provides me with. And then, in the meantime, he, she, well, it gets more knowledge, more experience and just grab this pending issue, like, you know, and it says, oh, it's a piece of cake now. How can we, let's say, try to follow, you know, the interaction and try to get get lessons learned how how you know to let's say oil this process yes i i don't if i explain this the right way it's like it just occurred to me and i um because i think that uh you know uh if if uh they they start to work hand in glove it would bring uh you know some they may cheat us perhaps might in the long run yes that's a very uh very good thought now today we do worry about those things not about this particular simple neural architecture, but with the various AI agents that we are creating, there is a lurking fear amongst us humans. That what if we ask them to do one thing, but they collude and do something entirely different? And those concerns are completely legitimate. Or they learn in a way different from the way we ask them to learn. But for this particular thing that I mentioned, the generative adversarial network, these are very simple networks. They don't have any free will, I mean, or ability to reason or make decisions like that. They're forced into learning together. Uh-huh, uh-huh. I see, I see. Okay, thank you. Thank you so much. Anyway, I'd like to connect with you, you know, later in the year, perhaps. Sure. Yeah, thank you so much for your generosity and your time. Of course. Thank you, Asif. We'll go to the next question from Gopi. Can you explain your statement on how the generator leverages the power of discriminator as the essence of generative AI? That's a very good question. See, the idea that Jan Goodfellow and these people had is they said, see, the discriminators have become very good, right? had is they said see the discriminators have become very good right so if the discriminator can tell whether this is a fake bill or a real bill why can't we use that to teach the generator to backdrop to an algorithm called bad propagation but basically use that to force the discriminator innocence to teach the generator and that is the gist of it women max game it is like you know if you want to learn tennis you surely want to learn by playing it against a person who is stronger than you because if you play tennis against a person who is far weaker than you there is no way you are going to improve the only way you are going to improve is as a gen is if you're playing against somebody who's a bit better than you right or maybe quite quite a bit better than you and you have to somehow figure out a way to thrive and survive and that was the main idea of this adversarial network that discriminator is the person the generator is playing tennis with to learn and get better that's the gist of it you're playing an adversarial game just like tennis is an adversarial game and to play an adversarial game you want a worthy adversary otherwise it's not fun thank you uh as a follow-up question uh kop was asking, what is that element in the generator that is able to show the direction to migrate in that simulation? Oh, so that is the gradient, the direction in which it realizes that, see what happens is, it gets gets a little technical but let me explain that you mean those arrows those tails that these points had yeah what it means is that see when you have the loss the generator is looking at the loss function and the the gradient of the loss tells it that in that direction if i move, then it will look less fake. It is mathematics, basically. The gradient of the loss for the generator points in that direction. Actually it's the opposite direction, specifically. The gradient of the loss points in the direction in which you should move away from, but if you take its negative, it points in the direction that you want to go to if you want to be a slightly less fake a little bit more realistic and that's one of the beautiful things about mathematics that just the gradient see gradient is uh if i may just give you a poetic example today when you go from place to place or you use the google maps or some maps or apple maps or whatever it is that you use you use a navigation system and a gps it turns out in a way in a metaphorical way of speaking in the world of calculus in the world of functions there was always a map navigator built in if you wanted to go in the direction in which a function increases you just apply a mathematical operation called the gradient the derivative the moment you take the derivative of a function a multi variable it would always point in the direction where the function increases so if you keep on following that you'll get to the hill top of that function and if you follow against that you'll come to the valley of the function the bottom where the function minimizes and so in a way you can say that the world of functions they come with a built-in navigator they give you an idea in which direction it increases and decreases. And that is what the generator is using. Thank you. We'll go to the next question from Inder. How does these GAN models useful in the context of something like ChatGPT, just trying to connect the dots? See, ChatGPT is a pure generative model. When it generates, there's a lot that, see Chad GPT for example, is not one model. And obviously the architecture has never been revealed publicly, but to the best of our knowledge, it is a multitude of experts. It's a, there are at least a dozen to two dozen but to the best of our knowledge, it is a multitude of experts. There are at least a dozen to two dozen independent GPT 3.5 models and many, many other things that have gone into it to create a generative event. Now, I give GAN as the easiest one because its architecture is easy to explain. In the coming week, I will be talking about so-called transformers. Those GPTs, they belong to a class of transformers which emphasize the so-called, the decoder half of the transformer much more, the generative half of that much more. And how they do it, we will talk about it, so you could say that the chat gps architect will not chat gps architecture slightly different. What chat gps architecture internally is or how many pieces, it is made up of no one knows only open Ai knows, and they have been pretty secretive about it, but all we know is that it is not one model it is a whole constellation of models that they use thank you uh we'll go to the next question uh from Mahesh uh how do you balance the bias between the generator and the discriminator, the goal of these systems is to create a very good generator. So you do everything in the system possible to favor the generator, to give it the advantage. Because at the end of it, you'll throw the discriminator away and use the generator for most situations. So you cripple the discriminator, you do techniques. And by the way, there's this beautiful thing called vector arithmetic that you can apply. After some time, if you figure out that if you give this input, it produces a person with a clean-shaped face. And if you give this input, it produces a face in the latent space it produces a face that is that of a bearded person so now what can you do you know that if i want to go from a person to a bearded person all i need to do is traverse the path between these two vectors go along the direction that goes from the one vector to the other because gradually and it's really amazing to see that you can see a beard a beardless a well-shaven person gradually transform into more and more and more beards till the person looks completely bearded or to go from a man to a woman or to go change the age of a person to go from a child to an old person middle-aged person so you can do all of these vector you know smooth transformations after a little while once you create very good generators so look into the work for example of style gans and so forth and conditional gans the whole field of gans is become a whole field of generative AI. And it can do amazing things. So you remember that there was a time when you would sit in a, like if you are a witness to a crime and the cops would make you sit with an artist and say, okay, tell me what did the fellow look like? And no matter how well you described, the artist would come up with a face that could fit just about anybody. Like it barely, you know, it was not quite close to the real deal. But imagine what generative AI can do. You can today, and this is one of the things actually we do in the workshop, the course, we convert textual descriptions to realistic faces that match that. And then if you say that, no, no, no, a little bit more mustache, a little bit wider face, as you say it, the faces transform. They change. So that you end up with, in a couple of minutes, you can end up with a face exactly like the person you saw. Or very close to the person that you saw and imagine what it does what's what's a new world that you're looking at thank you as if that actually answers the next question from Rajan so uh you pretty much clear two questions in the chat uh we'll go to the next one uh can you please talk a bit about vector databases and feature stores in relation to this example? I think we're going to cover that in our next session, so I'll probably pass that. Would you say that generative AI uses patterns from the discriminative data? It's a question from Dr. Brown. Yes and no. See, the purpose here is not to make the discriminator a really good discriminator, though that is one of the effects of training again. The discriminator also smartens up. But generally you find more efficient ways to create very good discriminators. You use it mostly, the end goal is mostly to come up with a really good generator. Right. So you don't, you don't really, there is a bit of work, there is a bit of body of research that says, could we not use GANs or this sort of approach to in itself, create much smarter discriminators, much smarter that. I don't think there is a vast amount of literature. There is some body of research that has successfully done that. Thank you. Last question and then we'll conclude the session. This one is from Jeevan. Is generative AI is equal to a predictive AI? AI is equal to a predictive AI? Roughly. See, here is the thing. I would say it is sampling of a probability distribution. What do I mean by that cryptic statement? Let's go back to this. See, when you figure out that your data is around this yellow band, you have created a probability distribution of the data in the language. And so you could sample off this distribution. You could, in other words, pick points in the yellow band, and they would all look realistic and they would be the data. So that is, therefore, generative presupposes understanding of a probability distribution or a predictive model. Then only can you generate. That is part one. Another way to look at it is, if you're thinking these days, people often look at LLMs, chat, GPT, et cetera, etc and they say when i ask the question how is it able to generate an answer is there a prediction going on the answer is yes what it does is it produces the problem it says what should be the first word of my answer so in a vocabulary of 30 000 words of english let's say vocabulary of 30,000 words of English let's say which of the words should I start with to answer this user's question right it will pick a word that is most probable now in that word in that probability distribution it's a new sample of it you don't necessarily pick the most probable word experience shows that if you do it you tend to be very repetitive and bland so you for variety you take the either the most probable or the next was probably excuse me or some sequence of words along that high probability words and it has to do with temperature it has to do with by the way these things I'll talk about in my very next session. But so before every, so these LLMs are next word predictors. They just predict the next word and then they ask okay what should be the next word that I should predict and the next token and the next token and the next token. But those tokens are all taken from a probability distribution. A probability distribution is a prediction and therefore you can say in a very realistic sense that all generators presuppose the construction of a probability distribution and therefore are predictive in nature. Thank you. We have two more questions, but I think we'll make it quick. Are generators and discriminators sort of parallel to the blue and red teams in a cybersecurity field? I think that's a good analogy. Yeah, that is a good analogy. It's an adversarial game. See, adversarial learning is one of the most effective forms of learning. Yeah, excellent analogy, actually. I loved it. the last question uh generative ai is focused on creating new content from images to music why predictive ai leverages historical data for future trend forecasting is that a true statement not really see predictive is just to create see there are two aspects to it I think what you're trying to tell is to predict based on past data those are discriminative models predictive models are a large class okay if you want the specific technical detail it is something like this if I'm predicting whether it's a cat, given the input attributes of the animal, that it has a mustache, that it has two eyes. Is it a cat or a horse? This is a discriminative model. And its job is to predict based on the learning from a lot of training data which is historical. So to that extent, yes, this is one class of predictive models. The second aspect of predictive models is you learn the entire distribution of animals and their attributes together, right, in a feature space like weight, etc., etc. What do these things look like? And then when you sample off it, you just randomly take a point in that space, a likely point in that space, that point may not exist. So yes, you have learned from historical data, even for generative models, because it is the past that informs you, the data that informs you the data that informs you but at the end of it whether you build a discriminative model or you build a generator model that is your choice and it depends on the purpose you're going to use it for thank you very much Dr Rasef and thank you everyone for joining uh we will conclude today's session We'll look forward to seeing you on next Tuesday. Please make sure you follow us on LinkedIn. And if you have any questions or any further future guidance that's required, or you're looking for it, please reach out to me at madhu at supportvectors.com or info at supportvectors.com. So thanks a lot. Have a wonderful rest of the day, rest of the evening and take care.