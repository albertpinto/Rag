 This is April 15th. It's a beautiful Saturday morning. Spring, neither hot nor cold. I hope you would agree after a long time. We missed a couple of papers in the last one. So today we'll give quite some time to research papers in the afternoon. The two things that I would certainly like to do, if possible, is Distilbird, which we have been using. It brings in an interesting architecture. And it's one of the most widely used transformers. So really worth learning about. And stable diffusion. Stable diffusion goes to multimodal learning using transformers. So in a very quick summary of what we'll do in the afternoon, Destilbert is trying to learn from a larger model to create a much smaller model. And the purpose of that smaller model is it tries to maintain or retain most of the accuracy performance metrics of the much larger model but still being memory efficient being much smaller and computationally less hungry in more practical terms you want a model that runs fast on your laptops isn't it it shouldn't be that the model the only place the model runs is if you want a model that runs fast on your laptops. Isn't it? It shouldn't be that the model, the only place the model runs is if you have a workstation with no less than 240 90s in it. So that's the point. And what is stable diffusion? And clip, actually be made to clip also three papers. But these are interesting adventures recently that the Transformers have taken with great success. Clip, one of the early ones, one of the first ones, took this question, can we have auto captioning of images? So suppose I show you an image, can it immediately say, put a good caption to it. Now it is different from object classification. So suppose it's a cat or a dog. You can show a picture and it will classify it as a cat or dog. That is, that is computer vision, a classic sort of a classification problem, the normal classification problem. But what if you're holding a dog in your hands? So can we go further? Now if you remember, in the very first lab we did something very surprising. We searched not for a cat or a dog, but we search for very human emotions. We ask this question, can you search a visual library for things that represent a love of dog? Now, a love of dog is not a dog. It's an emotion. And how can you find images that sort of represent or are evidence of a person's love of dog? And we found that we could do that. The architecture. Now, if you remember that, how did we do that? As all great things that you accomplish in natural language processing, you have to line up many of the ducks one of the ducks was the the sentence embedding the other was creating a vector database into which we could search fast searches the third one uh was again the clip right now we have done sentence word paper we also understand what this approximate nearest neighbor is we went through the theory but we didn't do clip. We'll do clip. Now the continuation of that is the next step. Clip goes from images to text. It can give you a summary or understand an image, but can you do the reverse way? The reverse way is a generative process. Can you give it a prompt, a text? And from this, ask a transformer to paint a picture, represent a picture, and you could give it pretty absurd things. For example, you could say a cow on the moon reading a book. and it should be able to understand that cows won't survive on the moon unless they're wearing a big astronaut suit take an astronaut suit dress it dress the cow in it put the cow on the moon real realistic moon rock and then have the cow hold a book in the only way cows can hold books or things and yet get away by doing that. You saw that we did it, right? Or we could give a pretty absurd things, monkey carrying a chair while jumping from one tree to another. And well, I don't know if in real life or in the history of this earth, monkeys have ever done that, but they certainly can do it in the make-believe world of your transformers. And isn't it amazing that you can do that? And we went through all that, so we'll study the research papers. But today, we will go through, we'll continue the journey of transfer learning using the Hugging Face transformer libraries. So to vet your recollection before we delve into the labs directly, what did we say? We said that training a transformer from scratch, are these large language models, the fancy word people use for transformers these days is language models. These are language models and transformers are already ginormous models. They have like billions of parameters, hundreds of millions of parameters. And now if you say a transformer with a hundred million parameters, a few hundred million parameters, people would scoff at you for using such a puny little model, right? You have to go not just billions, but hundreds of billions. And now has come GPT-4, which presumably, or we are told is in trillions of parameters. And GPT-5 is about to come out, which is supposed to weigh in at no less than a hefty 2 trillion parameters. And what happens with these models is, as these models become bigger and bigger, two things happen. They develop emergent capabilities, things that models of a lower scale actually did not have. For example, they developed the ability to do arithmetic, to reason, to do things that prior models did not have. The other thing that it does, and which is something we'll delve upon, is that they become a bigger and bigger environmental disasters to train. The carbon footprint to train one of these large language models is astonishingly large, and that is one of the topics that we'll talk about. The other thing that is happening, and it is a social phenomenon, there is a tremendous amount of euphoria and endless possibilities in the current, in the zeitgeist, in the current mood. People feel they have almost blind faith and optimism in this. There is talks of AGI having been there, that we are reaching points of singularity, where AI will take over, where many things would happen. The fact of the matter is, you have to be extremely cautious. It's our social responsibility to be very, very cautious around these large language models, because they are deeply, deeply flawed. And we will delve today in one of the flaws that these models have that perpetuate historical stereotypes and biases. We'll see it with an example. Finally, we ask this question, if these models are extraordinarily expensive to train, then what is the way forward? And there, fortunately, we discussed the concept of transfer learning. Transfer learning says that transfer learning has a flavor of reuse or recycling about it. See, as good environmental citizens, we are told the three R's. Wherever possible, refuse to consume something. So for example, when you drink water from a plastic bottle, it's an environmental disaster actually the water from the tap the municipal water from the tap at least here in california in the bay area has been proved approved scientifically to be far purer and more germ free than the bottle water it has a lot more germs. Think about it. It's stagnant water that's been sitting in a bottle, right, cultivating germs, whereas municipal water is running water that's just been purified. So first thing you can do is you can refuse to use bottled water. I'm just taking an analogy, right. When you do that, you don't cause the, you don't add to the environmental disaster that that is the world of plastics and microplastics it turns out that there will be no more any generation of living beings on earth that won't be infused with microplastics in their bloodstream in their tissues that's the damage people went to the deepest part of the sea and in the deepest crevices guess what they found not some exotic life form but more plastic right so it speaks to the fact that the first thing we can do is refuse the second thing we can do is reuse. Instead of drinking water from plastic bottles, perhaps we could get purified water from a store, but fill up our water bottles with it, giant water bottles, and then reuse, refill those bottles. That's what we do here at Support Rectus. Refill the water bottles. Next thing you can do, obviously, is recycle. If you are done reusing it, you don't need something like your soda can, you can recycle. Recycle is far more expensive, of course. It goes through a whole process. Tremendous energy goes into reforming that metal into something else, or paper into recycled paper. These are the three R's. In some sense, transfer learning has a flavorital paper. So these are the three R's. In some sense, transfer learning has the flavor of reuse. What do we do? We train, we pre-train to use the language of transfer learning. These models, once they get trained, we call them pre-trained models. The world makes it accessible, the whole world except OpenAI. It seems that they want to hold on to their models now. But most of the world open sources their large language models that they train. Once they become available, and Hugging Face is a great repository of those models that people make available, then what can you do? You look at your business problem. You look at the task you want to solve in real life, and you ask, does a model exist that addresses it, already exists? There will be something either speaking to that task, something adjacent to that task, right? That is doing a problem that's slightly similar to that. Right? So what you could do is you could download a checkpoint of that model pre-trained model now what is a checkpoint of a pre-trained model who would like to enlighten us what is a checkpoint the weights the weights. So what happens is when you take a neural architecture, it is a particular layout of the neurons and so on and so forth, the attention heads and so forth. But then initially it's all randomized. But when you train it with vast quantities of data, all of those weights, they stabilize or they become something meaningful. So that is what you call the learning. You go through the process of gradient descent and back propagation, and they acquire certain meaningful values or useful values. Now what you're downloading is of course the clue to which neural architecture it is. And more than that, you're also getting the weights they have been trained into okay that's your checkpoint so a pre-trained model more specifically is downloaded as a whole bunch of weights once you get this weight you can load it locally now comes the question you apply to your your task and see, does it give you a good performance? Now, if the model was trained for that task, it would probably give you a good performance. Now, here again, certain caveats. We realize that when you get the model, typically it comes headless, isn't it? It's sort of, if you remember, and we'll review this in the Jupyter Notebooks in a moment, but let me just say it comes without the pre-processing unit, and it comes sort of bare bones like that? Right? Why does it come like that? Can somebody enlighten us? Why do we get these transformers, these pre-trained models like that? Sure. You can give it an instruction. Wow, fine tuning. Please don't be afraid of that. Yes, you're saying your meaning is right, but you're not using the exact technical words. So let's give somebody else a try. Anyone else? The question is, when you download a transformer or a model, pre-trained model, say from Hugging Face, they typically are headless. So you can find unit? One reason is you could find unit, but you might as well have attached a head to it or a pre-processing unit to it. See the reason is one trained transformer can serve many purposes, many tasks. If you remember the original BERT paper mentioned at least six tasks in which that bird trained bird just beat the state of the art right right off the park it hit the ball straight out of the park on six of the benchmarks and six of the tasks what it means is and what these pre-trained models are often trained at is something that you actually won't use it for um So let me delve a little bit more on it. Let's take BERT. If you recall, what we fed into BERT were two sentences, right? And then there was a lot of attention and all of that multi-head and the whole, you know, the mechanics of it. At the end of it, the whole you know the mechanics of it at the end of it you get certain uh all of these like you can use it for classification tasks you can use it for fill in the blank task you can use it to infer whether the second sentence is a paraphrasing of the first sentence you can infer whether the second sentence is an entailment of the first sentence continue i mean it is appropriate a to follow up to the first statement. You can ask the question, is the second sentence an answer to the first question, the first sentence? And if the first sentence is a question. So do you see that you are using it? Or you could altogether avoid the second statement, just fill it with padding and ask this statement, this sentence, does it have positive emotion? What is the sentiment of this? You can have the sentiment on a scale of one to five. You can have, as we did the last week, you can have six emotions. If you remember, joy, anger, sadness anger sadness and so forth you could put it into those buckets or you could just have a vanilla thumbs up thumbs down positive or negative sentiment you could classify this the text the sentence but remember when we say sentence we mean it in a more general way it could be a whole paragraph of text but in this NLP we treat a sentence as basically a unit of textual input. It could tell us whether this text is about politics, it's about science, programming, right, it's about religion. For example, the 20 News Group dataset helps us here to classify any given text by the topic it belongs to. You could do many, many tasks when you use a transformer. So the first thing you realize with this model is that once you get trained, you could use it for many, many times. Then you realize, now, wait a minute, how can I use it for many, many tasks? A classifier, for example, a binary classifier expects a logistic regression unit. Remember, cat or dog, right? Positive emotion or negative emotion, right? Science or politics, things like that or a more multi multi class classifier would say like for example emotions uh it would say tell is it joy is it sadness is it anger right so you're classifying into so then you're using a soft max at the last end right if you're asking this question, fill in the blank and you hit some word, it's a mass language model. At that moment you're saying which word is likely to be here? So each of them, they have a different head that's making the inference finally. And so what you would rather have is a headless model onto which you screw in a head the way you um for the purpose that you do with that it's a drill bit yes and I use the example of a drill gun a drill bit right so with your mouth you come here i told you don't touch the mouse look at me over here okay could you run it so you will see this then click power this machine on okay yeah so um if you look at a drill power drill what do you get when you buy a drill you at a drill, power drill, what do you get? When you buy a drill, you buy a drill. You don't get any of the drill bits. But based on what drill bit you're using, you're threading whether it's a starhead it's a flat head it's a allen kind of a philip philip head and so on and so forth or are you using it to drill a hole or are you using it to bolt tighten a bolt right all of them are different heads different bits and a drill gun is in itself useless right i try putting it to your hand and just turning it on it just will spin it right it doesn't do anything so that's a that's a relationship between a train a model a pre-trained model, and a task. To do a task, you need to put a drill bit in a drill gun. In the same way, to do this, you need to attach the proper head. Go ahead. So you attach at the end, or you attach in the beginning? No, at the end, at the end. It produces the output, is the head that produces the output. So for example, let's take multi-class classification. These layers, transformer layers have produced, let's say, they will produce a whole set of, these are called logics or potentials or values. I always look at them as energies, right? With a more physicist bent of mind. They produce the different energies or potentials. Now, what can we do? We can pass it, if we so choose, we can choose to pass it through, you know, a whole dense neural net, which we have as a classifier, right? Not just necessarily a softmax. We can choose to do that to further, you know, add a few more training layers and then train that. We take some data and train that last layer, the head. Or I can just attach a softmax and say, well, I don't need to do further training. Let me attach it and get the job done. Isn't it? So we can do that. Now look at the back end of it. So this is the output end. You need a head at the output end. Now, what about the back end of it? Just as in a cordless drill gun, you need a battery. In the same way, you need something in the beginning of a model. What do you need? It expects data in a certain format. So, for example, if you're using BERT, it is expecting a token that all it is not expecting text, it is expecting that the text has been converted to tokens sequence of tokens and, in fact, the sequence lens should be 512. In the in the standard word case right and the other models like big bird which expects 4000 and so forth, and there are different things you can look into that so and they expect that they. into that so and they expect that they the the beginning has a padding called a special token called the cls the end has between the two sentences you you put yet another separator token right and at the end you have a you know you have padding token and if you are masking a word you have a special masking talking and all of that it is expecting the input to be like that so you need a tokenizer that converts it into that format and we went through the tokenizer the tokenizer itself is a complex beast it is not like your string a basic string tokenizer that you use in let's say, standard programming, C programming, Java programming, JavaScript programming, et cetera. It's a little bit more involved. In fact, it is itself an AI-trained model that is smarter at tokenizing. So text goes in into the tokenizer, for example. If you are doing text classification, from the tokenizer tokenizes it and gets it ready for the model, it goes into the model, energies come out, or logits come out, right, this potential, those are further fed into the classifier network, which may be just a softmax layer or a logistic classifier node or something like that a function or it could be a few layers of nerve or it could be whatever nothing prevents you from for example uh venturing out and saying i'm going to put a gradient boosting layer like or i'm going to put a support vector machine at the end of it and you could do what you want from the head because you you get the hidden states you know by the way these words. Because you get the hidden states. By the way, these words are all synonymous. The hidden states produced by the transformer, the logits, I call potentials, energies. You may also see me use colorful words like voltages, so on and so forth. They are all the same thing. Then they feed into the next actual classifier and they will classify. And so you have a text classifier. The last classifier cannot classify text. It can only classify certain hidden states or certain latent representation vector or potentials. vector or potentials right and what is a good potential that potential represents a very semantically meaningful representation of the text what knows how to create such a semantically meaningful representation of the text of the sequence text sequence the transformer because of its attention head and the tokenizer has fed the transformer. Tokenizer feeds the transformer. Transformer feeds the classifier. The classifier produces the output. Do we see the relationship, guys? So that is what we have been doing. Now, where were we last time? If you recall, we went through the tokenizer in some depth. We realized that the tokenizer itself is a complex beast. And we learned that there are various tokenizers out there. For example, there's a BERT-uncased-based tokenizer, which uses a small BERT model. I believe BERT-based uses... How many multiheads does it have? A puny little 12 multiheads, I believe. Which today people will say, ah, small. But actually it's huge. You don't need even, because when you load that model, it's ginormous. It's hundreds of megabytes. Right? And actually it's a phenomenal beast. Even base uh base but you could use large if you want but that and then uncased stands for all the before the tokenizer first converts all the text into lowercase right so it is not case sensitive go ahead go through them the first few notebooks are just the installation then we did the search the ai search we learned about hugging faces so i'll start maybe hugging face introduction to sentiments and zero shot learning and text generation texture image do we remember the named entity resolution we did that visual qa then we learned about transfer learning let's go into transfer learning And when we do that, we said that transfer learning is made up of two steps. One is the pre-training of a large model, right? And then saving the checkpoint and making it also available. If you don't make your checkpoints available to the world then the world can't use it right or the new way seems to be to charge a ton of money to use it through apis so well that's that and the second thing is you can download the model and fine tune it so today we didn't do the fine tuning. We did all the rest of the stuff. Just to recall what we did, we realized that Hugging Face comes with, first of all, the pipeline. You can use the pipeline API to do a lot of the work that you want to do. Do we remember that, the simple pipeline? Ready-made, ready-to-use pipelines that would do many things. It also comes with a lot of classes. For example, let's look at the tokenizer stuff. We talked about the tokenizer last time. tokenizer last time. Now, if you go back and look at the tokenizer, it'll be very quick. I deliberately took a sentence, the tokenizer does tokenization, it does us to have fun with tokens. Somewhat not terribly meaningful sentence, but it illustrates the point that when you tokenize, unlike programming or Java or something like that, tokenization here is very context aware and it tries to create the smallest possible vocabulary. So we know that the word token, the I's the word could be the suffix to many many things right to say is the suffix to many words so someone on site needs to mute their computer folks please mute yourself yes who is not mute you can also like uh admin view from zoom itself i'll check oh uh okay okay i muted her uh all right guys so can you hear me now? So instead of breaking it up into using as four different words, it is far better to just have one word token, another word piece is up, another word piece S for plural because S suffix makes the plural of a vast number of words. So you can remove the duplicate simply by having the the s suffix and that's what this does yes it does because yes they also will have their own word in the vocabulary when you do this then you tend to have a much smaller vocabulary. The English language has 5 million words, but most of the things can be done. For example, the word uses a vocabulary of only 30,000 tokens. Some tokens are reserved, they're special tokens, like the CLS token, the paddock, the mask, the separator token, etc. But by and large, they are actually derived from word pieces, word segments. Now, 30,000 is much smaller. You want a smaller vocabulary, because when you one-hot encode, you don't want to convert a particular word into a 30,000, into a 30,000, a 5 million dimensional vector, especially when you have 512 words, it would be just way, way too computationally wasteful. So you use word seconds and we saw that, right? So for example, here are all the words that end with eiser. Then we learned about attention masks. We learned about unknown. So then we have a special token for unknown words. So what's the anatomy of a tokenizer? The tokenizer itself is a pipeline that's built out of many parts. One is a basic normalization. For example, if you're using a tokenizer like the bird base unkissed, what it will do? It will convert everything to small letters. It will remove the accents. Then it does something called normalization, which is something I don't understand what it is. But apparently it's done for languages like Japanese and so forth. So that's that. Apparently the same thing can be written using different characters, and they mean exactly the same thing. So they are literally identical to identical characters for the same thing. Same letter. Then we have the pre-tokenization. Think of pre-tokenization is what you would do, for example, in a language like C or Java. Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam the tokens are being advised case as well over here the words are also being tokenized like tokenizer the token is yes and iser and s is yes so you break it up into word so what will happen is normalization let's take this first The sentence is converted to lowercase. And then, you know, if it is a foreign language, remove the accents, all the accent marks are removed. Then the next thing that is done. So let's say that these two things are done. If you have multiple white spaces that condense to a single white space or something like that those are basic pre-processing things then you do what you think of as programming tokenization you know breaking it up on the white space bar or the as the boundaries in the in the world of natural language processing you call it pre-tokenization you break it up into words but then what will happen is you'll end up with things like tokenizer nization right tokenizer tokens all of these words then the real tokenizer comes in the model the ai model that has been trained kicks in the birth model the tokenizer model what it does is it looks at all of these words and it has actually figured out what the right pieces should be, what pieces should be. So when it encounters the word, let's say cup, it leaves the word cup alone. Right? Because it doesn't see the need to break it up further. But when it comes to a word like tokenizer, it breaks it up into two pieces, token and iser suffix. Is it like a concept of like a base word, the root word and then the other word and then It breaks it up into two pieces, token and ISA suffix. Is it like a concept of like a base word, the root word and then the root word? Stemming. Yeah, that's a good question you asked actually. What is this relationship to something called stemming in linguistic processing or NLP? Stemming is that, the root word. Like for example, run, and so for runner etc i have the root word the stem is the word run and all others are suffixes and there is a close analogy but not always so uh the word pieces are learned so stemming is linguistically driven pieces are learned. So stemming is linguistically driven. Somebody has sat down and created the stem words, isn't it? A corpus of some linguist has sat down for that language and created the stem words. Whereas here, the word pieces are learned. The AI model studies the whole corpus of words and thinks. What's the basis of that learning the learning actually is pretty simple it's smart now that it's has token and then iso right so breaks it up into two what it does is it looks at the fact that uh the word segment iso has a pretty high frequency of occurring in the corpus so it says that because it has a pretty high frequency of occurring in the corpus. So it says that because it has a high frequency of occurring in the corpus, it will treat it as a word piece, right? It notices that many, many words end in S. Therefore, it breaks it out and says, statistically, it's so prevalent. I will consider suffix s as a token. It is there's no magic there's no magic it's just a statistical study of all the words and figuring out where if you split you'll have the smallest possible vocabulary. Go ahead. What about special characters? All of them are typically left alone and made tokens in their own right question mark exclamation mark they're left alone. So they would have their own token. They have their own token. As we are removing both, how does the output have the concept of, as we are removing spaces, how does the output have the concept of, as we are moving spaces, how does the output have the concept of spaces? No, it doesn't. All it is a sequence of tokens. So remember here's the word, the cow jumped over the moon. The becomes a token, cow becomes a token, jump gets broken up into two tokens jump token and suffix ed token over remains a word token the remains a token of course moon remains a token let's say let's say that it's like this so my question is regarding how does the the model differentiate between tokens that have space limit between them and tokens just like gem j ump is one token it is another oh because it puts those tokens have a special marker they are suffix tokens so let's very good question so let's look at this uh see uh where am i so can i for me Kate, I apologize. This is too big a font. I'm not able to scroll reasonably. Would this be acceptable, guys? Are you able to read this on your screens? That's okay. Okay. Yeah. Just to make it a little bit readable. So observe that that if you take a sentence, let's take a sentence. The tokenizer does tokenization. It does this to have funded tokens. I've deliberately created a sentence that has a lot of token variants. So what would you have expected? You would have expected something like this without knowing this. Think of this as your pre-tokenization step isn't it if you were using c or java this is what you would get c c++ java javascript this is what you would get isn't it yes you know then what what does it do it it first look at the thing the everything has been converted to small case then you notice the token but you notice that suffixes have a double hash to them right so when you decode it it will put it back it will go and sandwich it to the previous word previous token right that's it go ahead as you ahead in the state the whole uh the statement you have multiple spaces like the same quantity go up the token if you need that one more time it has the tokenizer and then after tokenizer does then after that tokenization so the four gaps yes so in the entire statement whatever we have had in the forum there are lots of other spaces is one uh tokenizer no what the tokenizer does see let's look at this i'm just referring to the space between just the space between the words yes yeah so look look back at this picture first so think of that statement in your mind you convert it to a small case right now think what the pre-tokenizer will do what the pre-tokenizer will do is it will break up the sentence the whole text using see boundaries so you use something like space is a natural boundary the tokenizer if you find a space doesn't matter one or more space split it right treat more than one space as one space and split it and of course you don't make spaces into tokens so the tokenizer remains right then so those things happen now you got it? And it will keep doing it. But then at the end, there's a full stop. It turns out that not only space is a breaker, but punctuations are also breakers, because any word ending with cow jumped over the moon dot. So moon dot is not a word. Moon is a word and full stop. The period is a token in its own right. So the pre-tokenizer will take care of that. Then comes the real tokenizer, which will do further mischief by doing word pieces. And then what is the post-tokenizer doing? The post-process processing does, notice this. Where was I was my sentence. Yes, it adds the prefix, the beginner, you know the special token CLS in the front, and the separator at the back. Right. So at the end of it, or what tokenizers produce? They produce token IDs. If you look at it, it produces these token IDs. So I take this sentence, it was Billig and the Slithy Toves, the first line of the Jabberwocky poem. By now you must be pretty sure I love Alice in Wonderland and Lovish Carol and Through the Looking Glass. So this is the, I love it because it makes sense and yet it's made up of nonsensical words. So the 101, that's just, you notice that most of the tokens are four digits or five digits, but 101 and 102, what could they be? 101 is CLS and 102 is the CEP. So what happens guys, as you become more familiar with all of these models, these things will get imprinted into your memory. You won't forget it. things will get imprinted into your memory. You won't forget it. So I've seen this three characters being used for office. Is there any specific reason why it's three? CLS? I mean, not CLS. All the pockets, they're just having three character ones. No, no, no, no, not at all. Not at all. Let's look at this. Look at this. I took the sentence to urge human to forgive divine. Do you notice that some tokens, forgive, for example, is a token in its own right. So the length of a token is arbitrary. So you don't split all words. No, no, you don't split all words. If a word is sufficiently frequent, as is it's usually prevailed. For example, human you could have done, you and man, man is a suffix, right? So, woman and so forth, but it chooses not to do that. It's interesting you split foreign because far and give are two different words. Foreign give are two different words. it did not because partly because the for when you split on foreign give semantic context is lost meaning changes so that's it this only breaks it down right so let's let's say you have in a textbook a word a word has to hyphenate to go to the next line the tokenizer cannot piece back together that way actually you can do in the pre-tokenization step so one of the things you do for example when you download one of the free books from gutenberg right you notice that it has gone through automated scanning and so forth so there are a lot of hyphens that are there. So there are dehyphenation tools that will dehyphenate it. So dehyphenation as in pre-tokenization. Yeah, you can pre-token it. I mean, you can do it. I don't know whether the BERT one has it, but there are tokenizers that will dehyphenate, that you can build. So remember that because a tokenizer has got given to you or sort of given to you by the people who contributed to the hugging phase a repository it doesn't mean you have to use it you can always cook up your own tokenizer or one simple way is you could prefix the pre-tokenizer with your pre-pre-tokenizer isn't it and make it a part of your pipeline so that's that guys so now are we all together so far we are still we're going a little bit slow because i lost some time so then we did the datasets api or dataset is a way to and this is just waiting your recollection why do we use the datasets core library in Hugging Phase? It makes loading data very, very easy. See, datasets, there are many people have realized that the fuel behind, the fuel that feeds AI revolution is data, right? And if you start making data precious and hide it, it is as good as making all your research ideas private. Progress happens when researchers share their ideas and share the fruits of their research. You know that, right? In the western hemisphere, for example, one of the, and people have, so if you look at the fact that why did the west make such tremendous progress, while the rest of the world, the Africa, the Asia, the Americas in those times with these native Indians and Australia, they didn't seem to be moving as fast during the whole scientific revolution. There are many causes. One way to look at it is, hey, they pretty much made rapid progress with science, right? And a lot of it. But then why did they make rapid progress with science? Human beings, they have a certain degree of altruism, but that altruism goes only so far as their family and friends, usually. Maybe their communities, sometimes a little bit to their nation. People get nationalistic. But generally, altruism doesn't drive progress. Progress happens when human beings find a win-win situation and an activity that is fruitful, economically fruitful. and an activity that is fruitful, economically fruitful. And as a legal expert once explained to me, and I think his view was, it happened, the reason this whole revolution happened in the West is because of a very interesting legal construction. A few legal ideas. First is that the whole world was feudal, owned by kings and queens. And then there was the concept of a corporation, a corp, right? Or traded a few partners coming together. I believe Denmark or Netherlands, it was pioneered there in those areas. And that's how we have this shared. And it protects you as an entity that protects the individuals participating in a business and so forth the second big idea that apparently came is that uh the realization of intellectual property so a physical property is obvious this is my land right or my car right on my table or whatnot yeah but intellectual property concept when it came about, it said that people can, when they have some new ideas, they can publish the idea and they have copyrights to their creative, they write a poem, they have copyrights to the poem, they write a story, they have copyright to the story. Copyrights is for any of those activities and patents are for any implementation of an idea. So suppose you figure out this is a new protocol. This is a way I can make a hardware that a chip, a silicon chip that will this particular silicon chip will somehow run a transformer head, a tension head much faster than others. Then you can take that to the patent office and say, hey, I want to patent it because it costs me tremendous R&D and the blueprints will ultimately leak out. Some employee will leak it out onto the web right now all secrets are ultimately revealed so i need protection because i've just sunk a billion dollars into it or here's a pharmaceutical that just designed a molecule right so the patent office will give you exclusive right for a certain number of years to the implementation after that everybody owns it so it's a give and take. What do you give? You give, you as an inventor, give to the world the right to own your invention and use it freely after X number of years. In return, the society, the government, gives you protection from copying for X number of years. So it's a win-win situation, isn't it? It's just a digression on intellectual property and copy. But ideas are always free. It was very particular that ideas should be shared freely so that they are not continuously reinvented everywhere in a fragmented space and held as secrets. Progress, people realized early on, happens when people don't keep secrets. They share ideas and breakthroughs. So publish it. People will give you heap praise on you or give you credit for the invention. But let progress happen. So ideas are free. In the same spirit, data sets have to be free. Because data sets are the fuel that runs the AI revolution. So some data sets are proprietary. That's all right, but there has to be a push towards having lots of meaningful data freely available. Then only can we train models and do sensible things. And one of the good thing that is happening is they are now entire. For example, Google has a complete search engine just for data sets. I believe data sets at google.com will take you to a search engine that will search the vast repository, the world for online datasets, freely available datasets. US government has mandated it at all. If a taxpayer dollar led to activity that created that dataset, that dataset belongs to the public. It must be opened out. So you can go to data.gov and get it. There are entire search engines like this devoted to open data. Now Hugging Face in particular has accumulated data sets that you can contribute. You have a data set. If your company allows you to and doesn't feel it's proprietary and must be kept behind the curtains, post it to Hugging Face. Let somebody use it. Kegel also has a collection of data sets, but Hugging Face also has it. So in Hugging Face data sets, you can use it to get the data set. So this is a recap of the last time we did the Hugging Face. And Hugging Face makes it extremely easy to load the data sets, one liner. The emotions data set, here it is. after the data set we went to that uh ask a quick question uh right now uh the llms are not there yet but do you anticipate that the lms will be able to generate their own data sets in the future see because right now people call data as your moat and hold on to your data so that you can train your LLMs based on that. See, at this moment, we are in a very unfortunate... Sorry to digress from the topic. No, no, you are right. It is true that what is happening is, first, people began to hold back the datasets that would train these large language models. And then they altogether started keeping back the models themselves and not contributing. I think this is a really, really bad development that has happened. Somehow the economic model has to be fixed so that these things come back. Ultimately, these GPTs have been trained on public data. Our writings, our creative work has gone into it. So we do have a part ownership of that, or should have that. So to hold it completely back is perhaps not fair, but the legal system allows it. Some change is needed. needed also what data it was trained on means that you cannot no no researcher can reproduce your work right which is also pretty terrible and down to your question can large language models generate data large language models are generative all transformers are generated right well most transforms can be used for generative purposes. To the extent that they're generated, they can generate data. In fact, they are being used. There's a whole active area of research which says, how do you create representative synthetic data using generative models, using transformers? It's a pretty active field of work. But will we ever come to a day, your final statement, where only the data will be produced by large language models? No. Remember, machine learning always interpolates. So if you have enough points, and it can, one simple way to think mathematically is, remember I told you that all machine learning models do, for example, in in regression is they discover the embedded sub manifold on which data adheres to the data is proximity training data is proximity it discovers this embedded sub manifold but it can't discover an embedded sub manifold unless there is training data to begin with. Let's cover an embedded sub manifold unless there is training data to begin with. But once embedded sub manifold has been constructed, it becomes iterative. You can generate infinitely many points by interpolating. Those are interpolations because the original training data are sort of the linchpins holding the fabric of the sub manifold there. You can generate it. Will we ever come to a date a day we would that doesn't work simply because to train the model in the first place you need real life data so you can never get rid of real life data are we together yes thank you yeah all right so together. Yes, thank you. Yeah. All right. So transformers, oh, by the way, I hope you guys all got this. We will take a particular library of transformer and use it for classification. The easiest classification task you can think of is sentiment analysis, right? Now, I give you one example in which we did sentiment analysis with six sentiments. Now, this time we'll take slightly which we did sentiment analysis with six sentiments. Now, this time we'll take slightly different. What we'll do is we'll look at, we'll pick a sentiment analysis. Oh, library. I think image, classifier, pipeline. Let me just do that. Not images. Right. So the complete pipeline is tokenizer, transformer body, classifier, and encode, right? So this is a picture one of you were asking for. This is the picture, just to recap. Now, we will deliberately go to the Hugging Face repository and pick a sentiment analysis library. Let's see what it does. We pick a particular model. It is a bird-based multilingual model. So let us read this word. NLP Town is the creator. Whenever you create a Hugging Face account, you will have a name. That name could be you as a person or it could be your organization like Facebook, like NLP town, like Google and so forth. It's your organization name quite often. So slash bird. By now, a bird should be as common to you as your own name now. issue a bird should be as common to you as your own name now right so next time you go to the airport and the uh the cops the the authority asks you what's your name don't blurt out but go ahead mine already has a name albert yes in fact there is a whole transformer in your order so your order so so typically when models are trained after neural architectures are trained they are smaller versions and bigger versions so they're small base large and so forth so this is the base what is multi what could the word multilingual possibly refer to that it has been trained on a multilingual corpus uncased means all the sentences have been lower cased and furthermore it has been tuned on sentiment analysis data some form of sentiment analysis data fine-tuned right so let's take that let's see how we can load it you take a checkpoint so this model name is a checkpoint right so somebody has posted that you've pointed. So someone took the bird and then bird-based and then trained it on multilingual and uncased sentiment? Exactly. Sentiment data. That's what it is. So first, there is bird-based uncased. It just implies that it has been trained on English, not in multilingual. Then there is a bird based multilingual uncased, right, which is again trained on generic data. But then it has sentiment just says that somebody has taken a special specific corpus of sentiment data and trained it on. Go ahead, Albert. So you have to name it that way? No, no, no, no it and they encourage you it's just a convention it's like you know in when you're writing python suppose you import it import pandas as pd import numpy as np and mat plot lib pipe plot as plt nothing prevents you from saying import pandas as plt, import numpy as spd, right, and import pandas as np. And then everybody will have a tremendous confusion reading your code. It's a convention, right? So you can, adhering to the convention is a collective, it's a communal decision, guys. It's a community decision. It's just like, you know, what prevents you from driving on the left side of the road? Right? It's, we have all agreed to a set of traffic rules. And we don't drive on the left side of the road, at least in the United states isn't it so it's very small i said people already screwed on the head so for this i think yeah right yeah yes uh we will see that hold that thought in your mind. So we look at the transformer, but what happens to the transformer? Transformer expects token sequences. So what do you need before you feed the text into the transformer? You need a tokenizer. And you better get a tokenizer that understands what this expects. So you can't take a tokenizer that produces sequences with special tokens that this model doesn't understand so a very good idea typically is to take a tokenizer whose checkpoint name is exactly the same right because quite likely it's been done so we'll do that you take a tokenizer and then this is a tokenizer. Download the tokenizer as a model and it's if you download it. And then you take the input. So you take some text. I took a text which is from Keats' poem and it's the poem starts with the sentence, a thing of beauty is a joy forever. It must be something familiar to you. I took that and by now you guys must have inferred that I love poetry, but that's a separate topic. So you feed that line of poem into the stokenizer, return tensors of type pi-torch. First of all, what do you want it to return? They will return tensors. The two major frameworks are PyTorch and TensorFlow. Pretty much momentum is with PyTorch. There's also a framework called Flax coming up based on the JAX library. But at this moment, it's still immature. So let's look at the output of the tokenizer. I called it inputs. Why did I call it inputs? Shouldn't I have called it outputs? It's fed into the it is the input into the main model. That is it. And so there is a model as well. What's it? It does. The tokenizer organizes the model. Yeah, because it has been trained through ai so that's okay so now let's look at it what does a tokenizer model inputs contain it contains the input ids you know those numbers it contains the token type ids what is the token type id remember which of the sentences it belongs to the first or the second if it is the first sentence for word it will have a token id zero if it is the second sentence it will have a token id one and then it has the last attention mask means if it has added padding to the sentence when you do mini batch we'll see that but then when you batch some sentences will be longer than other sentences so what you do is the shorter sentence you pad it to the length of the longest sentence by putting padding tokens but then you don't want the attention hits to start paying attention to the padding padding is meaningless right so you can prevent it by putting an attention mask saying don't look there just like in photoshop you put a mask in an image to focus on only a certain region of the image that's it then let's look at it now what happens remember the model did needed a pre-processing unit it also needed needs a post-processing unit isn't it because it's producing voltages or potentials or energies or logits or hidden state. Pick your name, isn't it? Whichever word. Here, I've used the word logits because this model uses the word logits. And by the way, guys, you'll see it from model to model. Somebody will call it logits. Somebody will call it hidden states and so on and so forth. So a lot of those things will go on. states and so on and so forth. So a lot of those things will go on. If it is just, if it, if it is, if it doesn't have the word classification at the suffix of it, right, then it will typically just produce hidden states, which can be huge number. But if it has classification attached to it, then implicitly they are feeding it into, they know how many states, like how many output values are there, how many classes are there in the target space. So in this particular case, the target space is rating of one star, two star, three star, four star, five star. It's called the Likert scale. of one star, two star, three star, four star, five star. It's called the Likert scale. It turns, a lot of psychologists say that it's the most intuitive scale for people to rate. So a 10 is too many, 100 is too much. They won't be able to distinguish if something is, they want to give it 87 star or 88 stars but four in five stars we know it's a like at scale so it it it seems to be producing it on like at scale so that's why it is producing logins if it was just producing hidden state arbitrary many then it would just say hidden states that it is sort of a convention right so here we go logics but we need to convert the logic into probability because what does the classifier expect probability of its being one star or two star or three or four star so how do you convert logics into probabilities how do you convert a whole sequence of numbers into probabilities softmax softmax is a simple way it amplifies the big guy makes all the negatives positives right and so forth so we we cover that when we do introduction to machine learning remember we'll do softmax and a logistic function uh in considerable detail go ahead there's two stars of inputs is that? Oh, that's a Python way of saying that whenever you give maps or tensors, give it like that. What does no grad mean here? Oh, very good question. What is no grad? This is something you learn in your deep learning workshop. No grad means the model is already trained. It has two purpose. It is to prevent the model from learning even more. Like you're in the inference mode, right? Your model is trained. You're not consciously not training the model on this data. You're just inferring. Whenever you do inference, it's a good idea to do no grad explicitly to say, do not learn from this data, just give inferences. The other advantage of doing that is, so it doesn't tamper with the model. And the second important thing is, when you're in the learning mode, remember, how does learning take place? By computing the gradients and back propagating the gradients so each node needs to remember not only the output but it has to have computed the forward output but it has to also computer the gradients hold on to both right but that is that is utilization of memory and computation so you can prevent in fact computing gradients is the harder is the more computationally intensive part the forward is easy you know matrix multiplications and activations no backprop again no backprop exactly no backprop no no gradients and no backprop so like if we go to something like a child gpt now it would have like it doesn't learn from it oh you're in yeah exactly you're an inference unless you explicitly try to fine-tune no no what happens though is that they are recording and they are on their own consciously uh training it but you are not doing it you are in the no grad mode right essentially so that is the no grad so guys is this code now uh fairly uh easy to understand so what are we doing we we got the logics to get the inferences this line is inference outputs right you take the model you give it a text and you say well tell me the voltages you know here's the win way i think of these guys logics they are bulbs next to the values one star two star three star four what do you want whichever is the right rating that this point so that bulb should burn bright isn't it that is the way to think of logics and you see that the logics that got produced are this so which bulb is burning bright one star two star three star four star five star which one is burning bright the five star is burning bright isn't it now it is a five star rating. Now, if you look at the sentence, would you say that it's a very positive sentence? If somebody buys, let's say, something, let's just say you go to a shop. and you see a wonderful piece of craft, and you're so dazzled by it, that in great passion you say, a thing of beauty is a joy forever. What is it likely to be? A five star, right? So there you go. And so it is a five star rating that it produces, right? But then, well, most of us don't think in terms of logits, do we? What do we want? We want it to become a probability, right? Next step, next stop in the journey is, for heaven's sake, make it into probability. Which star is it? And how do you do that? That is where you need the classifier head, you know, the softmax head you you pass it through softmax it becomes probabilities right and now let's display it in a more sensible way like a user-friendly way so now i reprinted the probabilities by zipping it with the ratings and i said rating one zero probability up to two decimals i kept up to two decimals and i'm saying that one percent goes to approximately three three stars it is pretty clear that it's a five star rating isn't it pretty confident right now you don't have to pass only one text at a time for inference remember these uh transformer models especially when you load them, when this whole pipeline, when you load them, you load them into GPUs, right? Into this, right? And when you load them into GPUs, it's inefficient to pass one input at a time. You want to take a, there's typically a batch size, which is optimal, which will fit into the memory of the gpu and you want to maximize it because you want the entire multi-dimensional matrix right or tensor to go through its multiplications at one go right so you don't have the overload of loading little bits into memory it is very much like for example in the cpu what do you do? The locality of references, if you're going to get this variable from the file system, don't just get that variable from the whole page. At the same time, between the CPU, CPU has a cache. It has a cache, L1, L2, L3 cache. And memory is out there. Bank of memory has your data sitting there. But when a CPU hits a variable, it doesn't get just that little bit into the cache. What does it do? Using locality of reference, it gets a whole segment, a page into the cache. Think of it like that, that you don't want to bring tiny little bits into it. You want to preemptively have entire batches go through it right and so that is a rather the technical word is mini batch but you use the word batch here people abuse the word and use it just batch so so here's one batch three three pieces of text oh there's fondo right if you were to look uh what rating would you give a thing of beauty is a joy forever probably five stars. The second one, I would really not recommend this horrible shoe. It hurts on long walks. Bad. Then what about this? It's quite satisfactory, but not exceptionally good. Middle of the road. Let's see what happens. Exactly the same code. Now, let's see. It produces logits. See, this is the trouble with logits. What's the trouble with logits? It's a little bit indecipherable, isn't it? Unless you're mathematically trained. You keep wondering which is the biggest number here. If you think this looks like the biggest number actually no it is 10 to the minus 3 right and so on and so forth so you have to be a little bit careful so it's better to convert it into probabilities and when you do that when you convert it into probabilities a thing of beauty is joy forever. The last one has high probability. I would really not recommend this. This has a high probability and it's quite satisfactory. This has the highest probability. Well, let's write it in a more sensible way. This is just aesthetics. By the way, guys, you all have these notebooks now in your all i did is how would i convert it into a number first of all it produces five ratings i need to know which is the highest rating the which is the location of the index of the argmat means which index gives me the highest value index of the argmag means which index gives me the highest value and I add one to it why did I give add one to that it starts with zero it starts with zero exactly arrays start with zero in indexing then what this is the max this simply says what was the maximum value first is where was the maximum value what was the maximum value right for the prediction so once i get these two all i need to do is collate it with the text and create a nice little data frame and represent it like this right which is a little there's nothing new here it's just a more intuitive way of displaying this very uh intimidating tensor isn't it it's just a pretty way of putting it this way so what did we learn from this what we learned is let's put together what we learned you don't sometimes you get fairly good accuracy because your task happens to coincide with what a preach a model that has been pre-trained has been trained on. Or somebody has even posted a fine-tuned model specifically on your kind of task. That's a lucky day. That's when you say in Jawa-Waqui language, oh, fabulous day, kalukale, if you remember that poem. So that's when you go happy. right so that's when you go happy and but then all is not done you still have to use a tokenizer before and a suffix after and now that is what you would do except that sometimes what happens is even that is done for you hugging face comes with the pipeline concept. Guess what the pipeline concept does? It does that. If you'd say sentiment analysis, you can actually say this entire code that we wrote, you could have used the pipeline method and said, okay, here is my checkpoint and here is the, use this for the tokenizer, use this for the model, right? which means that it is exactly identical. It just so happens that the pipeline works for you here. Sometimes it does, sometimes it doesn't. Here in this task it was. So you could have written it alternatively in this way, where the point of doing this was to show how you would go about doing it from scratch. Isn't it? Because you would. So remember, we are peeling the onion gradually. First, we just use the pipeline. That's the easiest way to do it. Then we did use the pre-trained model because we want to understand the architecture of how these transformers work. So we understood it. And now going forward, we'll do the next step in which we will say, let's use a data set, task specific data set. Let's say that we'll take a task and we say we have some special data. And because your model has been trained on a general data set like a bird on it's a on a whole general class of problems we need to further train it on this task using this data set are you getting the point so then you now the last leg of the transfer learning journey that we'll do right but before we do it I'll take a small digression. Now, here is what I would suggest. If you make sure that you pip install the code carbon from a programmatic, this is a little bit you can do, guys. If somebody keeps reminding you that this is the carbon footprint you're creating, you tend to change your behavior, isn't it so uh just pip install the code carbon and technically you can do by the way you can do this for anything you want so I could learn this that anything any code especially the code that you run on GPU just say tracker dot start tracker dot end emission tracker and it produces an output like this so because here I'm not doing anything it says it it will immediately start looking into Anil Verma!urheveramgopalabhagwa.org Anil Verma!urheveramgopalabhagwa.org Anil Verma!urheveramgopalabhagwa.org Anil Verma!urheveramgopalabhagwa.org Anil Verma!urheveramgopalabhagwa.org Anil Verma!urheveramgopalabhagwa.org Anil Verma! I have only 125, 120, well, it's 128 GB, but available RAM is 125 GB of RAM, right? 32 CPU cores and one GPU, right? Which is 4090. And it says that the carbon footprint was zero kilowatt hour, right? Because I didn't do much. So this, again, this is my little interpretation. The transfer learning is the closest we can get to the common concept of the three R's, refuse, reuse, recycle. So with that, now the nice thing is, Hugging Face, the auto classes, all the auto, this pre-trained etc. It has the code carbon built in. So long as you do pip install code carbon, it will automatically kick in. So all you have to do is on your machine, do a pip install code carbon. And then what happens is it will keep bringing it in and we'll see it in a moment. Now I'll move a little bit quicker the other is other socially responsible thing that matters is so you realize guys i'm showing you the negative side of these large models ai models not negative but things to be aware of the other is the existence of social biases and now fine-tuning a model. So we know that we can use models that exist. What about fine-tuning it? It turns out that it's a delightfully simple process with Hugging Frases. So we are now going to use the Hugging Frase library to do fine-tuning of models. So the three core libraries we'll'll use actually two of them are part of hugging phrase evaluate is a separate library in its own but also with hugging phrase data sets transformers right we used it so we will take in this we will take a data set called mrpc it is part of the GLUE benchmark. In natural language processing, one of the most established benchmarks is called the GLUE benchmark, which is made up of a few tests, sub benchmarks. One of the datasets used in that is a rephrasing of the first, means semantically equivalent, true, false. So it's a classification problem, right? True, false. Or if you use a logistic classifier or a softmax, then it is a probability that it is true right there it is true so we'll take that data set now again the data set cpi gives you a simple way to load data all you have to give it is the name here a pairwise name because the glue and then the data will get loaded. When it gets loaded, you notice that what is the data consisting of? By now from last session, you know that good data sets have already split the data into three parts, the training part, the validation part and the test part, right? That's what you would do in scikit-learn, by the way, ML 100 or something. You do it by hand, but these datasets fortunately are pre split. But what does the feature set look like? So index we can forget because it's one, two, three, and so forth. The three things are there. Sentence one, sentence two, that from the BART architecture makes total sense, isn't it? Two sentences go into BART. Label. Now here the label says what? Label will tell whether sentence 2 is or is not a rephrasing of. Yeah, that is it. So that is what it is. Data set is, now observe something. The raw data set is not large, right? So can you train a transformer from scratch with just 3,668 rows? No. You'll be hopelessly lost. Imagine training 200, one of these models these days, 200 billion parameter. You don't stand a chance. It's too small, it's too tiny a data set. But you get away with it because you are fine tuning. You take a model that already understands human language and you fine tune it for the rephrasing task. You're transfer learning. You take a pre-trained model and you'll fine tune it on this data center. Are we together? So now what is the input here? Sentence one, sentence two, output is? Is a rephrasing or not? Yeah, isn't it? Now, we load the, so this is the bird. If you remember, we did the bird research paper. Now, there is one more beast we need to introduce into our zoo. And this beast is something called data collator. Now, what are data collators? See, what happens, as I mentioned, when you send data into a transformer or into a language model, you don't want to send one at a time. You don't want to trickle feed drop by drop and say, what's your inference? What do you want to do? You want to send batches of data. But when you send batches of data, sentences, human sentences have arbitrary sequence lengths, isn't it? So what will happen is the whole thing won't look like a tensor. Tensors are boxes like this, right? Like a book. You don't have books of like, imagine reading a book in which each page was of a different size you can have a notebook like that human beings can write on that but a printer will not be able to print on such a book isn't it in the same way you need to make sure that all the sentences that go into the or rather the token sequences that go into the transformer are of the same length so what do you have to do for the batch you will have to pad it as needed so sometimes you need sometimes you don't need padding so padding is one of the things a collator may do the other thing but the main thing that a data collator does is you have this entire data set 3668 instances of data you can't feed all of it into the transformer that is one extreme you don't have that memory it into the transform. That is one extreme. You don't have that memory. You don't want to feed only one at a time. That's the other extreme. You want to send it as mini batches, right? Or people just colloquially call it batches. So you choose a batch size. Now what the collator does is exactly that. It will automatically break up your data and feed it into the trainer batch by batch. I call it, so here is a maybe a metaphor that you may, you can, that may help you. Imagine that you have a, you wanted to do something in your backyard. You wanted to do something in your backyard. So somebody has come and dropped a heap full, a truckload of sand in your front yard. And you need to take it to the back and use it for whatever garden construction work you're doing. So you don't want to take one grain at a time. That would be rather inefficient. But you want to take buckets at a time that's one batch of sand for a purpose because that's more efficient but you can't take the entire truckload on your uh either right so collator will do that for you so what is the journey this is your journey the data set loader loads the data. Of course, you remember we use the dataset loader to load the data. Then you use the tokenizer to do, it's pre-tokenization, normalization, actual tokenization, post-processing of the tokenizer, but the tokenization, roughly speaking, to do all of those things. So now it has become sequences of tokens. Two sequences of tokens, one for the first sentence tokens one for the first sentence one for the second sentence that's your input so you have a huge pile of them pile of this pairs but now imagine bucketfuls of them have to go into the model right so who is going to bucketize all this who is going to divide it into the batches it's the data collater and while it is doing it you can ask it to do some extra things also add paddings for this purpose now add padding so that's what it does that makes sense guys so this is it from now this is the rest of it this is very easy what happens is that you have this entire data set and you want to transform the data set sentences into tokens. So you need a function that can do that. What is that function? The tokenizer, of course. You need to treat it as a function. So what you do is you create a little function, given an example, which will be a pair of sentences. You feed it into a tokenizer and it will do that. Now, the only thing worth noticing here is that till now, we have been to the tokenizer, we have been feeding in one sentence, but here we are feeding in two. That is okay. Any BERT-aware tokenizer can take two, right? And so it is fine. It will return you two pairs, two sequences in return, right? And so it is fine it will return you two pairs two sequences in return right and so it will do that so you take the raw data set and you map you transform it using the map function map as in map reduce thinking in programming into give it a function it's a mapping function and it will return you for each element it will return you the tokenized version of that element and now what do you do you create a data collator with padding and you give it the tokenizer so that everything can happen properly and after that it's easy go go get a pre-trained model we take a pre-trained model and now comes the interesting part. The interesting part, and by the way, this warning is worth reading. It says some weights of the model were not used when initializing BERT for sequence classifier. So certain weights were not used because this is expected. This is expected because you took a model meant for train on something else and you are using it for this purpose so that certain things have to be ignored. So now let's look at the trainer API, which is the heart of it, which is actually those of you who have taken the deep learning course with me before and the PyTorch loop, where you have to hand out the entire training loop for every epoch for every mini batch in the epoch run the forward loop forward process then do the backward to compute the gradients right then back proper back propagate the gradient everything then you have to do all of that yourself you don't have to it makes it easy it internally does all that guys and that's all it's nothing fancy but it is still a convenience that to the trainer constructor you can give it the model you can give it some training arguments you know some think of it as configuration what are the training arguments we use here just simple name of a give the name of a trainer so what it will do is see when it trains it produces a model right it needs to store the model somewhere what it will do is the test trainer will become the directory sub directory in your wherever you fire it off it will create a sub directory and store the checkpoint as a model by this name. Right? But don't upload it to Hugging Face calling it test trainer because nobody will know what test trainer is. Right? So this is it. Now you have a training set, you have the evaluation set. Why don't you have a test set? Why do we not pass a test set? It wouldn't make sense the when you train a model the last thing you should be aware of is the test data because that would be the that would be the leak that's a holdout yeah that is the absolute one thing the training process should never ever heard of you hide it under the pillow and say it doesn't exist right so that's the thing collated so this line of code guys is it pretty much self-evident right the constructor and so can you make the test trainer like what was your point on that that see what happens is that there are three things you train a model but because it has hyper parameters you'll have to you'll have to figure out what is the best hyper parameter so you change the hyper parameters but the thing is that what do you test it on you create a evaluation set you keep testing on that how well it is doing so to the ai to this process of cross-validation the evaluation evaluation dataset acts as the test dataset. It never really sees the real test dataset. Because if it sees that, it will start learning from that. Right? It has to do, this is the boundary. It should never ever come to know about the test dataset. Because then only whatever hyperparameter tuned cross-validated model that you have, that you'll apply to the test. And that will give you the true model performance. You never learn it because the cheating is when you say, okay, let me tune the hyperparameter so that it does well on the test data. That is wrong. So this is it. Guys, run it on your machine. On my machine, it ran in 33 seconds. Right? See how long it takes. Now, when we run it, obviously, the training loss decrease. But then comes one more thing, the evaluate library. It also is very much like, see, these data sets tell you what is a sensible evaluation metric, right? So for this, a paraphrasing problem, obviously accuracy, right? It's a binary classification problem. So when you have a binary classification problem, what are the metrics? Accuracy, precision, recall, F1 score. Now here, paraphrasing or not, it's hard to tell what's a positive, what's a negative. So let's take F1, which is the harmonic mean of the two. So you take that precision and, I mean, accuracy and F1 score. So it will tell that. You can use that. And all that you do, when you compute the metrics, what do you need? You need the predictions, the Y hat and the Y. Remember, in the language of machine learning y hats in the white the predictions in the ground truth you'll compare those two here it will say the it is a paraphrasing oops the real answer was it is not a paraphrasing error right so that way so you use that and after that the trainer just needs to be trained it's very one-liner to train it. And when you train it, you get a result like this. Again, about 30, 33 seconds. Try it on your laptop. So this is the work you'll do, guys. And now we will take, it is time for lunch. You have, it is one o'clock. You have two hours. We'll have five minutes for QA then the usual time two hours what will you do in two hours one hour for lunch one hour for labs labs labs do the labs and then come back if it doesn't take help from us and from each other and so forth run it on your machine if you have headaches then stop us and the thing is guys absorb it make sure that you're not just blindly running it every line of code if you don't understand it now i'll give you a homework this is the lab there's a homework beyond this class we have done it on a paraphrasing data set do the fine tuning there is a emotions data set remember the six emotions now take a binary classifier take a simple pre-trained classifier not a binary just a pre-trained classifier right transformer model and make it work on the emotion fine-tune it on the emotion data set and see how much your model performance improves are we together guys what i said there is an emotion data set there your homework is to fine-tune a classifier on the emotion data set and then report the model metrics please do that please do that that is your work if you can do it in the lunch and by the way it's possible uh with luck you can finish it in an hour if you can't do it later on after the class is over see we we used a classifier to do this now use the classify for another thing sentiment analysis fine-tune a sentiment analysis using the emotion data set remember emotion data set means the six emotions right and then report the model matrix like what is the accuracy which which is a F1 score. That is your longer homework. And guys, just follow this pattern. The hint is if you understand this notebook, you realize that it's exactly the same thing with minor changes, but then you should be able to transport or transfer your learning to another problem. So do that guys, and we'll take the break accordingly, run it on your laptops. So the answers that you're expecting. Today morning, we did a series of labs. We finally ended the journey with a series of labs. We finally ended the journey with a complete transfer learning. We worked with some pre-trained models. We learned about tokenizers, pre-trained tokenizers, and then we learned how we can use our data set to fine-tune a model for our specific task. So that concludes pretty much what you do these days to solve most of the problems. Most, but not all. There are problems for which you will have to think of new neural architectures or new algorithms and so forth. In fact, the fun of this field is that you hit issues in which the state of the art is not good enough. And you have to sit and think, what do I do? You can go some distance by doing all sorts of hyperparameter tuning and neural architecture search. But even beyond that, quite often, you have to go back to the books. You have to go back to the math and think of something new. And that's where the fun begins. In fact, you create many. There is a every week you see a crop of new algos, new architectures, new things coming up. The field is moving very fast. The fact that people are creating new models, new algos, speaks to the fact that the state of the art in many areas is not good enough, and people have a need to create this. So actually this was very interesting. Just yesterday night I noticed that a very well-respected author, he said that actually the real work is in systems engineering, of machine learning pipelines, the data and all of the pipeline. The actual training of AI model is the easiest task. I sort of disagreed when I posted an opinion there on LinkedIn. See, every work is difficult. But if somebody has done one part of your work for you, or it so happens that your task is easy, then the other parts look difficult. They remain relatively harder. But it's not generalizable. You can't trivialize the hard work that goes into thinking of new AI models or new machine learning algorithms and things like that. So don't do that. So anyway, that was the lesson from the morning. The second lesson is, despite the fact that fine-tuning is a last mile journey, nonetheless, your laptops and the time it took to train these models on your laptop, it speaks to the fact that that itself is very time consuming. Most of you took between half an hour to one hour isn't it to train to fine-tune this model to run a few epochs in fact just three epochs over this model now one thing you notice that by the time the first epoch finishes already most of the accuracy or f1 score is achieved isn't it in in the first, then loss function stabilizes and so forth. You never know how many epochs are needed. After a certain number of epochs, and you learn in deep learning, the curve flattens, you reach a point of diminishing returns. In fact, not only diminishing, nothing happens, it's literally flat, zero returns, because all you're doing is you're just moving the weights, perturbing the weights a little bit, and more or less keeping it the same. So it is very hard to know a priori how many epochs will do the trick, but the rule is you continue that, but what people do is early stop. Once they notice that epoch after epoch after epoch, you just keep checkpointing the model, and once they notice that epoch after epoch after epoch you just keep checkpointing the model and after they notice they they started off with let's say 20 epochs but let's say by fifth epoch they notice a flattening out they'll stop early right they'll stop the training early because they're done right they don't want to overfill so the so that is that so if fine-tuning the last mile journey is this time consuming you can only imagine what the main pre-training journey is like the journey more more like a journey of a thousand miles there right so typically it takes something like anywhere between 256 to 512 to 1000 servers, weeks or months of continuous running of training before one of these models are born. It should give you an idea of how these things happen. So that brings us to the topic that is now. I hope you guys are seeing my screen. The idea goes to the, the idea has been around since 2006, but it sort of gained attention once Geoffrey Hinton, pretty much the father of this field, wrote an interesting paper in 2015, which is called distillation laws or distillation learning. The idea was, and this idea obviously predates him, the concept predates him, but he came up with a nice way of formulating it. I believe it goes back to 2006 at least. The idea is this. Suppose you have a big model. Now i will put it in ways that is geometric and i haven't seen anyone else put it like this but i like to think of it like this see what does machine learning really do what are those weights ultimately saying those weights are so suppose there were only two weights one bias one weight it is the equation of a straight line isn't it what does it do it when you make a prediction using the in the way in the in the hyper in the parameter space in the weights and biases space or what i call the hypothesis space it is it is basically different representations of loss, and they form a loss surface, isn't it? The correct weight and bias is the minima there, right? Now, in real world, that represents a straight line of regression. If you have two weights and a bias, it represents a plane. Now you can generalize it. weights in a bias it represents a plane now you can generalize it so you could say that when you have a whole lot of weights and weights and biases at the end of the day what is representing is in a very very very high dimensional space it is some input space it represents regression let's say that the problem is regression then it represents some hyper surface to which the data is close sitting upon right practically sitting upon it slightly bouncing up off it right the way our raindrops sort of bounce off the water sort of around that now another way if it is a classification problem then that hyper surface is the decision boundary that separates out let's say the cats from the dogs isn't it that's another that's another way of looking at it but at the end of it the way i look at it is what these models learn is a surface you ask this question okay so um now the question is how do you train it so a little bit back to the basics and this idea is very interesting guys because these concepts go beyond just this distal burden so i want you to uh obviously we'll do it in deep learning in a much greater way, but I want you to, I want to try and explain it in this time. Yeah, let me re-explain. I did not get much of what you said earlier. Okay, there are two ways of saying it. One is, go register for deep learning. You'll learn it in a slower, nicer way. Go register for deep learning. You'll learn it in a slower, nicer way. But the other way is, I'll put it this way. See, so both for you, Kazi, and for Masme, what it is is the problems of regression and classification. Let's take these two classic problems of supervised learning. One way to look at it is if you build a model. So for one moment just forget just forget the deep learning right just think linear regression right or and you take linear regression right and what are you doing suppose there Suppose there is one input, one output. Not even linear, just regression. So linear regression would say, what is the best fit line between input and output? It's a straight line. Now you make the line flexible. So this is very rigid. You make it flexible. Go to polynomial regression, various kinds of regressions. At the end of the day, it is the search for the best fit curve. Another way to thinking about what is the best fit curve? A curve is a function mathematically, isn't it? And therefore, regression is the search for that function that best gives the relationship between input and the response. relationship between input and the response. That is easy. And now generalize it to two input variables. That curve now becomes a surface. Right? x, y, z, it's a surface. That gives you the, and the real data points will, now this surface will not go through every point, but it will be sort of close to it. And now you go go to and go to three dimensions it becomes a hyper surface isn't it so you can say that for regression it is the search for that hyper surface that best fits the data because it's the search for the function what are you looking for you're looking for that approximator function now you will never know which how the data was generated because this is it's a shell of whom's problem that we solve in AI machine learning. People don't tell you what the function is because if you know it, there's no there's no reason to build models. But because you don't know what the generator function really was, you're trying to approximate it with any function that works that fits the data yeah and it turns out that it is good enough you don't need to know the real function because within the domain the two may be equivalent up to very small uh variations now so now if you think a classification is what it's a search for a decision boundary mostly are we getting it sort of yes please continue yeah Masumi, are we getting it? Sort of, yes. Please continue. Yeah. Okay. So we'll do all of these things in greater detail, Masumi. So like, for example, in a certain space, let's say XY space. Okay. Let's take it this way. You have a certain weight, a BMI, right? And you have a certain A1c sugar value. You would agree that up to a normalization, once you normalize the data, the healthy people will be close to, well, let's just say in a ring, the BMI will be in a healthy zone and sort of the weight would also be in a, I mean, A1c would be in a relatively healthy zone. Right. So this sort of form a ring kind of a structure and they sort of go together. Well, I don't know if it is, but okay some some region in which it is good and other regions in which it is bad right so let us oversimplify the problem let's say that being featherweight is no not a problem being malnourished is not a problem and a1c going down to zero is not a problem then the only boundary is the high a1c and the high this so what can you do you can take people's bmi and plot it on paper and what you will see is a gradual decision boundary will emerge between the healthy and the unhealthy people unhealthy people will tend to have high bmi high high um a1c right and closer to the origin in the quadrant would be that so you can draw a decision boundary what's a decision boundary it's a curve in higher dimension it's a surface like two dimensions it a decision boundary? It's a curve. In higher dimension, it's a surface. In two dimensions, it's a surface. In three dimensions, it's a surface. In four dimensions onwards, it's a hyper surface. And classification is nothing but the search for the decision surface or decision hyper surface. That's what I was saying. Now you got it, right? So this is all basic geometry. And now take this idea further and say that, and a little bit hand-waving, but let's say that you built a model. Here is a model that has this very complicated surface that fits the data. Let's say that it's a regression problem. I'll take this example. Now the question is, Let's say that it's a regression problem. I'll take this example. Now the question is, it's making very good predictions on the data, right? But can we have a simpler model? The two ways of asking this question. The way this model has been trained. So you can imagine that if you just look at this thing, right? If you have a very complicated surface, you can have a smoothed out surface, right? Can you imagine? With a little bit more smoothed out, how should I say? So when you look at the mountain from a distance, you don't see every little bump. It's a little smoothed out approximation of the surface, isn't it? So now, if you try to learn a smoothed out surface, you're deliberately not trying to necessarily capture all the nuances. So you are willing to trade off a little bit of accuracy. But the question is, can you get away with a much smaller model? And that is the idea. Geometrically, that's the idea. So now let's ask ourselves, how would we do that? And this is where the trick is. This is the main intuition guys of distillate learning. See the bigger model, which we'll call the teacher model, right? With all its grand belts and whistles and ability to flexes muscles like this, what does it do? It sees vast amounts of data, and it says no problem. And it goes and builds this very complicated, bumpy surface out there. Right? Now, despite its fitting, you know that the real data will not be sitting on the surface, there will be residual errors. There'll be some of them may be sitting but most of them will be off and sometimes the error will be lost. That is what the residual error is like mean squared error is. So the squared error is there'll be all over the place. You try to minimize the mean squared error that's all right but basically that's loss, but ultimately there will be loss. If nothing else, there's irreducible error there at that. Now you ask yourself, what should the student model do? Now, traditionally you would say, hey, let me try a smaller model and fit it also to the data. And maybe it will come up with a not so complicated surface, right, but lose some accuracy. Then what would you do? You would train it against the real data, a smaller model but the key insight here is don't do that. For the simpler model, what you do is you don't try to catch up to reality or compare it to reality while training. You instead compare it to the prediction of the teacher. So you don't say learn reality. You say learn the great teachings of this Ustad or this Guruji. Whatever he says is right. So like in Google, I think there's a article about one of the engineers left like the MS scientist or something and he was comparing with these guys asking him to like use the chat GPT model to train the Google model. Okay, that's a student teacher. Isn't it what Stanford did for Alpaca? Yes, yes, yes. so this approach is a domain once you get used to it you will realize that you know you get jaundiced you see everybody has been using it and also you see lots of opportunities to use and use right so so think about this way even the see look at it this way why why will a smaller model work there's a complex reality but however complex reality the the bigger model the teacher model has still created a hyper surface which which is not perfect which because the the reality comes with the irreducible errors and many things that that your data doesn't know. Right? So it is itself an approximator. Remember, neural networks are universal approximators. So it has come up with all its bumps. It is still a smoothing out on the reality. So a smaller model is not trying to learn the reality whose complexity remains invariant. It is trying to learn a lesser lesser than reality model right it is trying to learn from the predictions of the teacher right it's saying hey teacher for this input what would you predict and now i'll predict something and see what happens. And I'll try to, I will try to converge the gap between myself and the teacher. So if I can be, if I can become the teacher, right, for all practical purposes, giving barring two, three percent, hey, pretty good, right? Pretty good. No. So, so for example, if you're learning from Shakespeare, you don't need to be Shakespeare. Right? Even if you come within two, three persons, you'll still be world famous as a poet. I'm told that at one time, a lot of poets, it was de rigueur to learn the style of Shakespeare. But we think Daudas protects too much. What was that? It's a Macbeth, I believe, right? Macbeth, yeah. It's more modern, right? It will have either more residual errors or it could pass the error on the master yes that is true it will have but the idea is you have how much more and the beautiful thing that will emerge as you as i take you through the mathematics is you realize you know what not much right and this is unexpected in fact this was the pa this was the main idea when the discovery, distillation is a mathematical discovery, that you can do that. And I'll explain it to you, but I'll explain it to you ab initio, means from first principles, if that is all right. So bear with me, guys. And if your mind is not fresh, take a two minute break to jump around a little bit, freshen yourself up and sit down seriously, because I'm going to go in a mathematical journey. So is there a right way to get the correct prompts from the guru model so that we can change? That's exactly what we'll learn. Hold on to your thought. Hold on to your thought. So you said Jeffrey Hinton? Jeffrey Hinton. You wrote this? Yeah. 2015. That's a metaphor that's like, if you want to descend down a mountain without snow, if you're going through all the cracks down. That's right. But if you're still in the ocean with the snow, you flatten out some of the bridges and it's still the same way. Yeah, actually Patrick gave a very good analogy. He says that if you have a mountain, it has a lot of little bumps and cracks and things like that, but when it snows, it becomes a more smoothed out version of the mountain. That's an excellent analogy, right? That's an excellent analogy. Very, very good. All right guys so uh I'm serious when I say freshen yourself up because this intellectual journey I think you'll enjoy yeah what's that word you're saying like from first principles I'm an issue it's a Latin probably so Patrick will tell you. So I'll go back to the principle by asking you, do we understand what probability is? So it turns out that, and of course, if you go to the, probability has two possible interpretations, the frequentialist and the Bayesian. But we won't go too much into it. One frequentialist say the simpler version is if I go on tossing a coin and do infinitely many trials then the proportion of times it is head is the probability of a head. The crucial word is long trials or infinitely many trials. The trouble is in many real life situations you don't get to do many trials. So for example, what is the probability that if we continue our human activities as it is, we'll end up with an environmental catastrophe, right? We'll end up with the extinction event. The problem is you don't get to boil the earth a million times and see how many times it caused extinction, isn't it? One time is too many. You don't want to risk it. And yet we speak meaningfully about probability. And this is, by the way, if you look at the people who are climate change deniers, they keep pointing this out. They say, where is the data showing that we are causing global warming? I think at the end of it, when you distillate all of that, it boils down, if you really pass the argument carefully, it boils down to the fact that they're taking a frequentist way of thinking. They're saying, have you actually had a million runs of people screwing up the planet and seen how many times it led to extinction? You don't have. So stop making theories up. All the scientists are just cooking up. It's a conspiracy. They're cooking up theories. But there's another definition of probability, which is the Bayesian definition. But there's another definition of probability, which is the Bayesian definition. It's a digression, by the way, I'm mentioning it. The Bayesian says the degree of doubt or the degree of belief. So it takes an approach and it says that probability is a measure of to what degree you believe. You say, well, to what degree I believe, how can that be? Probability has a very rigid math. Probability of two independent events happening is the sum of them. If they can co-happen, blah, blah, blah, you know, A plus B. All of these rules are probability. It turns out that if you look at degrees of belief, it follows the same arithmetic. Now, here's the interesting thing. What it says is that data takes you from a prior belief to a posterior belief. So what does it mean? Let's say that I believe that there are two hypotheses. The earth is a sphere or the earth is a pancake. Now I have no data, so I can believe half and half. Let's say that I strongly believe that the earth is a pancake. I may even say that I'm 90% sure, 100% sure. But never be 100% sure, but let's say 90% sure that it's a pancake. Now you have data. You keep going in a straight line, and you don't meet the end of the earth, and you don't meet the end of the earth. And after a little while, it seems that a lifetime may go by and you may not meet the end of the earth. And by the way, that hill begins to look like something I encountered a few months ago, even though I'm going in a straight line. So now what is this data doing to you? Belief. It is diminishing your belief that we live on a pancake, isn't it? And it is increasing the belief that you live on a sphere. So this is a Bayesian way of reasoning and all of science is Bayesian thinking. Science makes the position that you can in the beginning believe whatever you want, doesn't matter, right? But so long as you are willing to refactor your belief, once you see the evidence, you let evidence change your belief, right. So start from anywhere, it doesn't matter. You'll come to the same answer with sufficient data. So that's the way of thinking. So anyway, that is the probability. But with these two definitions, I won't squabble over them. I'm mentioning it because it's worth knowing that there are two ways of looking at reality. And also because in schools, they always tell you the frequentialist definition, which in my view is actually not the preferred way it should be. It should be the bias. So now, suppose the reality is that it's sunny and you have a prediction model. Your weatherman, has it ever happened that your weatherman says it's going to be sunny and it is sunny sometimes it happens but sometimes your weatherman says it's going to be sunny and you're unpleasantly surprised in the rain it rains on top of you so now here's the thing suppose he said the probability of rain was one percent and you went out so he's basically saying it's going to be sunny you went out and it rained on you are you surprised you are surprised on the other hand if he said 99 percent chance is going to rain then are you surprised not so surprised right so we will create a construction process. I'll just take you through this. Surprise. Oh, sorry. We'll create a mathematical formulation of it. It seems that it is the reciprocal of probability. considering that something has happened, if your model predicts the probability of it's happening as p, then this. So for example, 1% p is equal to 0.01, then surprises 100, right? Approximately 100. If p is equal to, so 1 over p is equal to so 1 over 1 over p is equal to this if p is equal to 0.9 right 1 over p is 10 over 9. right how much is the surprise approximately one isn't it so now that begins to look like a good way of quantifying surprise except that this way of quantifying it has one minor one thing that gets the mathematicians into a knot they say well you know there should be no surprise if there's a hundred percent chance model says hundred percent this will happen and it happens where is the surprise there should be zero so how do you take something that goes from one to infinity and convert it into something that goes from zero to infinity take the log of it right if you take this one way of doing it, there are many i'm sure there are arbitrary many way of doing it, so let me put it this way this so surprise. Right so let's just say surprise attempt one or v1 release v1 beta or something like that release of your formula for surprise one over p after a little while you realize that you can have a better version new release right v2 release this is log one over p P. Right? And now, let's take this. Suppose you're training a model, a binary classifier, and the binary classifier's job is to tell picket, is it a cat or not a cat? So you would agree that when it is a cat, let's say that when it is really a cat, you add up all the points where it was a cat, and you sum this up, this will be a total surprise. For all those situations, when it was not a cat, right, it was a dog, let's say, it would be log 1 over. So suppose it was a dog and you said the probability of a cat was 0.0001. Probability of a dog is high. There's no surprise surprise isn't it so it would be a 1- pj right situations where dog cat would you agree this is your total surprise right am i making sense guys it's pretty easy right and now suppose i insert the i say the label y for cat is equal to 1 for dog is equal to 0 now this is the total surprise if i multiply it by the label also i can say cat and a dog right well dog dog is zero then this whole second term will cancel out you don't want that so what you do is you multiply it by one minus dog the label for dog right so which is the same as saying this is y hat y log well okay let me not get into the exact terminology and let me just for the time being stick to this so right this is the label this is the actual reality cat y is the label like one zero in a machine learning model label and what is your y hat what is the prediction prediction is y i given y the prediction is just the probability isn't it so putting it in more traditional language you would write it as 1 over y i hat would you write it like this right a plus cross and j one minus like y j the negative case right log this is just a you know mathematical way of putting it y j but ignore it i'm trying to uh if this math is looking puzzling i hope you agree that if i just sum up all the surprises let's keep it very simple to the really meat of the matter one over pi or the prediction which is the same as one over y i which is the same as now log of one over something is negative log of that which is the same as, now log of one over something is negative log of that. Prediction, right? Why I had the predicted value. Would you agree? This is the net surprise for only the positive case, for example. Would you agree, guys? This is a total surprise. Actually, so when you train your neural network or whatever classifier it is, all you need to do is, there should be the least possible surprises. Its prediction should match reality. Would you agree or not? Isn't it? Does the one minus dog, if that's supposed to be dog, isn't it just one minus cat? You define both in terms of cat. That's right. And so the other term here is uh one minus like uh one mine yeah so the like y i so what happened what people do is they uh minus summation over log one minus y j for the dog. So this is for the dogs, which is the positive case cat. This is for the cats and this is for the dogs. No surprises for that. Both the terms are there. Cat, dog, not cat. Dog is not cat. That's about it. You would agree, right? that if it is a probability of a j let me just use j the second notation so what do you want if it is a cat you want the probability to be as high as possible if it is a dog you want the your model to predict the probability that it's a cat to be very, very low. Isn't it? Means one minus that probability should be high. That's it. So this is called, believe it or not, this total surprise is, what is it? Have you encountered this formula anywhere? This is literally goes by a very fancy name cross entropy loss. It is the loss function for classifiers. Now I gave you an intuitive explanation for it and suppose you as a mathematician you're frowning on me and you you are very unhappy let me assure you that this is valid you can use a maximum likelihood estimator argument and rigorously prove this mle right so and we will do that in the ml 100 course those of you who are registering for it will go through the derivation but suffice this for now so see what happens is this we now know is the loss function. Loss function is. Right. That you take. The. So suppose you take the cat is equal to one. So the loss function in a fancier way is to say why I. Because why I for cat is equal to one. So one I can prefix. It's just a fancy way of writing. There's nothing very special about it. Like this, minus loss is equal to this. Minus again, one minus yj cat dog, one minus yj, for dogs, log yj hat. Oh, one minus yj for dogs log yj hat oh 1 minus yj hat sorry now this is basically 1 and this is basically 1 you know it right if you call cat as 1 and dog is 0 then 1 minus 0 will be 1 also this is your last function and when you train a classifier you try to minimize the loss very intuitive way of saying let's build a model that surprises us the least would would you agree that that's that is common sense writ large isn't it so it's one of those things guys when you look at the mathematics and you really think about it on your own you will realize that it's saying something absolutely common sense. Now comes the question of the student and the teacher. So we will come to that. But before we come to that, I will take a small mathematical digression into something called softmax. Now softmax is an interesting term and many people have argued it should have been called soft argmax. So what does it mean? I'll keep it brief because in the theory sections we'll do it. So remember that some model whether it's a you know whatever model it is has produced some know whatever model it is has produced some energies some logits right voltages or potentials now you need to convert it into a probability one easy way that you could do it is add up all of those numbers and divide each number with that so each number will become a fraction that. So each number will become a fraction. And you can say, well, that's the probability. There are two problems with it. You'll end up with negative numbers, right? Because what happens is minus 10 is divided by whatever you divide it by. It will be that. And the second thing is, if the addition of all those numbers is very small or close to zero, your probabilities, even the positive ones, will be beyond one. So they won't be constrained in the zero to one interval. So you say, well, that's not good. Let's do one thing. Let's first convert everything to positive numbers. Right. Right? So one way to convert a number line from minus infinity to plus infinity to be constrained to zero to infinity is the opposite trick of what we just did, log. Antilog. When you exponentiate a bunch of numbers, what is the exponentiation of e to the minus infinity? It is one over e to the infinity. And so it is 0. Nothing. 1 over an infinitely big number is 0. On the other hand, positive numbers will remain positive. Isn't it? So negative e to the minus negatives will give you a number between 0 and 1. Because the hard lower bound is 0. And positive numbers will give you a value one and they'll keep on so so that is one problem solved but now there's a beautiful thing all the numbers are positive they are additive so surely the sum will be more than any one of the numbers isn't it and so now you take the proportion they'll they'll become probabilities do you see how simple the idea is guys common sense right and to do so goes by this by the fancy word soft max right let's do this example suppose suppose you get the potential minus two one three five minus 2 1 3 5 right now you realize that minus like actually let me just make it this way minus 1 minus 3 3 and 1 2 let me take it like this right now guys what is the sum sigma is equal to zero terrible you can't do probabilities with it now you'll have minus infinity and plus infinity that's all you'll get so that won't do but when you exponentiate e to the minus 2 e to the minus 1 e to the 1 e to the e square now let's see what these come out to be want to try it out right let's see one over e square is approximately e square is approximately 2.7 2.7 square should be about eight can somebody do it and see e square is approximately eight e square is approximately 8 right and 1 1 8 is 0.16 and this would be 1 2.3 point approximately let's give or take so i'm using approximation mental mathematics 1.4. Yeah. And 8. So guys, observe these numbers carefully. What do you notice? First of all, they're all positive. Right? Also notice that 2 was only twice as big as 1. The biggest number was only twice as big. Isn't it, guys? As the second biggest number was only twice as big isn't it guys as the second biggest number but once you exponentiate what happens yeah it becomes almost three times right so what what does the exponentiation do it exaggerates the large relative to the next large do it exaggerates the large relative to the next large isn't it and here comes Jeffrey Hinton's key insight I want you to just keep staring at it and think what could be what 8 tells is good now when you make it into a probability so if you add it up well it all adds up to 8 11 Let's say they all add up to approximately 11. 8 over 11 is 0.777 or something like that. 77% probability, right? 25% probability. Am I right, by the way, mental mathematics? I'm just hand-waving math in my head. Anybody could verify this? Anybody could verify this? So when you divide it by adding up 8, 10, 11, let me just say 11, approximately 11, what you will get is practically zero. Again, a very 0.04, will get right approximately 0.25 and approximately 0.75 could you please confirm that these values are correct and we'll proceed after that in a minute. Thank you. thank you So, so this this Okay. How many of you verified those numbers? What were they? Anyone, at least one person in the class should be. Okay. By the way, in Python is one line of code np dot softmax right of an array minus two minus one one two just one sentence yeah what did you say find the soft max of this array yeah that's a that's a definition i'm taking you guys what i'm doing is i'm retracing the history of mathematics of this field by making it obvious to you guys these things are not written in stone when you see these expressions in a book don't just say oh my god what is it okay i'll believe it right so it is point i got the exact yeah it's not 11 11.26 yeah yeah that's right but what's 0.71 the last one 0.23 yeah so let me let me color it this way uh 0.71 this one is point the second one the second last one is 0.2397 239 so 0.24 let's put it 0.24 to the two decimal and the third one 0.035 0.035 little bit off. Point 014. Okay, so my mental math was not bad. And paste it into the chat too. Oh okay, thank you so. guys that is easy, like all of these, I don't immediately pull out a calculator or a, you know, this. You can actually do it in your minds. Just stare at it and do it. OK, so what does it say? You realize that softmax. You realize that 0.71 is more than three times, more than three times the second largest one. three times, more than three times the second largest one. But now I'll come up with an argument to convince you. While it may be good, because you're trying to pick the bigger probability and say, that's the answer. But in real life, what happens is, suppose you have to tell, I'll give you an example. You have a cow, right? You have a, let's say a zebra you have a horse right you have a truck and you have a house and you have to tell this picture belongs to which of these so what will you end up with you will end up with five values and when you soft max it let's say that it was the picture of a zebra. You want the zebra to be big. But there is actually good learning to be derived from the second best, third best prediction also. Right? Because if the second best prediction is a horse, the model has learned not just to identify the zebra, it has learned something semantically. The meaning, between the hoarseness of the horse and the zebraness of the zebra there's a lot of commonality of characteristics isn't it another way of putting it is a zebra is just a picturesque horse nervous and picturesque horse so it reminds me of a kid's joke I used to play with my daughters. What is black and white and red all over? So the answer is supposed to be a blushing zebra. I thought it was the newspaper. Yeah, well, newspaper is a little bit negative connotation when it is red all over but zebra could do it well read over mean means everyone is reading it as opposed to the color red it's a play on words oh i never got that i thought bad news violence all over i learned something today okay so you agree right there's a second okay so you agree right there's a second the second high probability the third they all are telling something so what if we didn't exponentiate it all the way how can we do that suppose we want these outputs to be the output of a teacher and a student is observing, not just the best prediction, the highest prediction, but the second highest prediction and the third highest prediction and trying to reproduce that order. Because in not just trying to imitate the answer, a highest probability, but trying to also reproduce the sequence in a way. Do you realize that the student would be forced to understand how the teacher is thinking would you agree right so how would you do that that brings us do you see the key inside guys right so let's so now we need a mathematical trick so that the second third, they don't get suppressed so much. Let's boost them up a little bit. So when you boost them up, like at this moment, the biggest one is a tall pillar, thrice as tall, more than thrice as tall, isn't it? The second biggest is less than one third and other ones are vanishingly small. So while vanishingly small so well vanishingly small numbers are hard to deal with in programming what if we could slightly smooth it up right let's just say that soft max is not soft enough we want to make the probability distribution a little bit more gradual a softer gradation of it yeah if you are being silly, let's call it the softest maths. So if we could do that. So there's a trick for all of these beautiful thing in math is unlike programming, everything is easy in math. And so what he did is he introduced a temperature variable. By the way, a lot of these arguments are very resonant with something that you do in a field of physics called statistical mechanics, the study of many, many particles. So anyway, without going into that and partition functions and so on and so forth, let me just say that there is a bit of history to it. And he sort of used that. He said that, suppose I didn't do this, but what I did is I divided each of these numbers by 10. Let's take a number, 10. 10, 10, 10, 10. Now could somebody please exponentiate it for me? Right? So what happens is e to the minus 0.2, e to the minus 0.2, e to the minus 0.1, e to the 0.1 positive, and e to the... is this number closer to 1.1 okay well this one I was a little off what is 0 e to the 0.1 come again 1.1 and e to the minus 0.1 is how much is it 0.9 0.9 e to the 0.1 is 1.10 and uh and e to the minus 0.1 0.91 and what is e to the minus 0.2 and what is e to the minus 0.2 do it guys don't don't do it do it on your machine e to the minus 0.1 is 0.9 and to the minus 2.8 1 8 1 so you realize that's your point yes 0.8 so do you notice guys that if you divide by 10 right these numbers they they are not they are not so far apart so now we say well we have damped it down too much maybe halfway let's try halfway something smaller so you see so this the ratio the denominator we call the temperature. Right? And in effect, what you have done is you have heated up all of these numbers. So they're all bouncing around. Right? More or less with equal energies. Right? So let's not heat it up so much. As you can imagine, this temperature word comes from physics. Let's try two. How about 2? So 10 is very high temperature. They all practically look the same. Now let's do by 2. So minus 2 becomes minus 2 over 2, minus 1 over 2, 1 over 2, and 1. This will be 2.7. This will be a half of it, 1.36. This will be minus zero. Well, no, no, no. What am I doing? Yeah, minus zero point. What am I doing? This is nonsense. Okay. This is 0.5. This will be minus 0.5. This will be minus one, right? And now, I hope you're enjoying this little mathematical journey, guys. e to the minus 1 is about 3.5. Tell me, e to the... These numbers would be approximately 3.5. e to the 1.7 i'm just guessing e to the 0.5 i might know e to the 1 over 7 is 1 over 7 no no no e to the minus 1 is 0.35. Is it? 0.36. 0.36, okay. So I was close. 1 over the square root of, that will be 1 over 1.7 is approximately 0.6 something. Okay. And 0.5 is 1.7 approximately right and this one was a one and this is of course a 2.72 right do you notice so now when you compare 2.72 is still bigger right but the the exaggeration is not as extreme but it is still there is certain amount of heating, right? So we learn something. We have an extension to, we have an extension to the softmax. And this extension, we will call the, which color should I use? So guys, are you enjoying this mathematical journey? I hope I'm making it very easy. So now we are saying that softmax for a particular Zi would be E to the Zi, the value, whatever. So for example, the softmax for this, suppose this is Zi, is what? You exponentiate it, divide it by the temperature, Zi, and then exponentiate it. First divide the number by the temperature, Zi, and then exponentiated. First divide the number by that and divide it, right? And then you divide the whole thing by the summation of them. So 2.7 plus 1.7 is 4.5 approximately, plus 5.1, right? So everything divided by approximately 5. And that will give you the probabilities, isn't it? So that will mean 0.3, 115, right? So it's very small. You can say 0.14 or 1, no, 1.3, zero point is that correct one eight zero point one eight why am i getting this totally wrong no seven two zero six eight yeah double of this right you're right and 0.12 yeah 0.135 let's say and this you divide by 5 would be 0.34 yeah and this you divide by 5 it will be 54.0. zero okay so 50 probability these are your p values, right? Exactly. Softmax values. So, this is e to the, just sum the j over t, right? So, you realize that for all values, this is just the denominator, right? With the temperature thrown in. So this is the softmax in the presence of a temperature. Right? Heated up, a heated up softmax. Now so you say, well, we learned something about softmax and heating up softmax. But what has that to do with our problem? And here comes the last piece of the puzzle, distillation. What it says is just what we said in the beginning the student need not learn from the reality the student only has to learn to imitate the teacher right so what is the teacher predicting at any given point let's say that the ZI produces our input Ti. By the way, ZI may be the hidden state produced by your transformer. Your transformer is there. It produces some logits or some potential. It's your ZI. Hidden state. Now what can you do? Raja Ayyanar?n softmax teacher. Teacher i would be softmax of z i over t. Right? You do that. And what you do is student will produce a student will produce a student model will also produce a prediction concurrently that will be softmax of um this is the teacher right of the teacher's version and it will be the student's version the student version it produces and now what you do is you say that the loss function was what loss function if you remember the cross entropy loss was was the actual label times log prediction right minus for any given data point this is what it was so forget the summation symbol look at the core of it when the label was one cat log of cat log log of the probability that it's a cat is a measure of the surprise now that is good for for when you're training against reality but you're not training against reality you're training against reality, but you're not training against reality, you're training against the teacher, right? The teacher is training against the reality, right? So what you do is you say, I don't need to, suppose my master says the probability that it's raining is 0.7, right? I want to produce that probability, not the probability that it's a you get it right the student thing so what you do is you replace it with it is that's why it's called the distillate probability log student i right negative loss i sorry. You see how different is your students' probability from the master's probability, right? You take this as your loss function. This is your distillate loss function. Are we together? Do you see how it is? It's very simple. Student is trying to catch up to the master, not to the external reality, right? If it can imitate the master well, it has achieved his complete knowledge or education. Am I making sense? Basically. That's it. That is all the mathematics there is. And this is the distillation, this process of this is the distillation loss. Now distillation loss, what people do, and what Geoffrey Hinton in 2015 paper, there's a little bit of subtlety. What he found is that, see, when you set the temperature to one, right, then what happens is you're doing a hard fit. Means you're not dampening down any of the softmax probabilities, softening the softmax probabilities any further right so he called soft distillation when t is greater than one like when t is two or something like that take t is equal to two that is a soft distillation right so you're also learning for the second and the third most likely thing when you make it t is equal to one you're doing a more hard distillation. You're saying, you know, because you know that only the peaks will remain, the others will get pretty suppressed. So he created a loss function by adding both of them, hard and soft distillation. And then he found in his paper, he does go on to say that in his experience, the soft distillation does a far better job. And the hard distillation term is far better job and the hard distillation term is there but you know it's a its contribution is smart right but you guys you got the intuition right i didn't uh maybe i have done some screw up with the notation but the main idea you should have gotten so that is a distillation so now you look at this model, BERT model, and you ask BERT is one big behemoth of a model, clumsy model. What is it begging for? A student, a distillation, a distillation of its knowledge into a student. Are we together? You need a student that makes it even smoother, like snowfall hill right hill landscape hilly landscape and how do you do that exactly this this is the training of a student teacher am i making sense guys that is it and if you understood it now when you read a paper like this it will make sense this. This paper is actually rather dense, rather sparse in detail. It starts out by this showing that, you know, models are getting bigger and bigger and bigger. Right. As the performance improves, right, it's getting, I believe, yeah, as time passes, what happened? Oh, my goodness. i went past some things i'm still on the sentence work just still work okay if it is making you giddy seeing me scrolling i've asked for your forgiveness so as time passes number of parameters in millions goes up now the in 2023 you wouldn't put your y-axis in millions you put it in billions right so it's exponentially rising the point called maximum likelihood estimator yeah right and um log likelihood and so forth so from that these things can be derived but one of the things that it comes from that is a mle based argument uh especially forget neural networks simple emily like just as a classifier uh mle based loss function will have an absolute minimum but when you do straight with the logits and use a classification problem, there are multiple problems to it. Amongst them would be the surface, the surface loses its convexivity, right? And you do want convexity, you want one global minimum and no other traps around, local minima. And that is not forgetting even neural networks, I'm just talking simple plane or something like that, which is why for classification problem, you always gravitate to entropy, cross-entropies. So now we are getting into the deeper mathematics of it. Asif, what... We took a reference statement, we took a statement similar to it, and we took a random statement. So reference statement, positive statement, took a random statement right so reference statement positive statement and a negative statement right so with the reference and the statement similar to it you have a positive sample right positive pair a reference statement and something completely irrelevant you have a negative pair so now this is what this big word contrastive learning boils down to this simple statement. When you do that, you should maximize the surprise when two similar statements, their probabilities don't match. Common sense, right? And you should minimize the surprise when two completely unrelated statements don't match. Isn't that obvious? To say that is literally, there is nothing else. I didn't even leave an iota out of it. That's the definition of contrastive loss. You write your cross entropy function in such a way, you say, I'm maximizing. Yeah. The loss maximizes when two similar things are wildly different probabilities, right? And I'm minimizing when they are different, right? So what can you do? The probability, it's just a classification problem in some sense probability that these two are similar is a probability it should be high for positive case it should be low for negative case and that is contrasting learning so now let's bring so now you you clip we are doing the clip paper is the cl obvious what could cl possibly stand for contrast of learning good all right so and somewhere they say the word i hope i got so now oh my goodness where am i this thing i hate it keeps jumping on me oh no it jumped up sorry commander okay you search into this right okay yeah Okay, yeah, I should smarten up. So now look at the main, now pay attention to me, guys, right? If you're ready and fresh, just pay attention. Look at me at this particular moment. Whatever you're doing, stop doing and look at me. See, if I could convert an image, and this is the beginning of multimodal learning, one of the seminal papers in multimodal learning. If I could an image and this is the beginning of multimodal learning one of the seminal papers in multimodal learning if i could convert an image and a sentence both of them into a vector of the same dimension somehow i do it i don't know how i do it right so let's say that i have a way a neural network with random weights a transformer with random weights to transmute an image into a let's take a vector 100 dimension vector for sake of argument and a sentence into a hundred dimension vector right so you say well fair and good this begins to look similar to the sentence word except that there we were taking two sentences and putting them into vector and then looking at the cosine similarity. But here what we do is we take an image and a sentence that we know are similar. We do the same contrastive learning. An image and a sentence that are similar. Image is the reference and an image in a completely non sequester, completely irrelevant sentence as the negative sample. Once again, I convert them into vector, where image-text sentence was the same argument. And to the first approximation, there's a little bit of technicality, but pretty much do the same contrastive loss. What will I end up doing? I would now have embedded images whose semantic, whose context, is the same as a bunch of words whose content meaning is the same into proximity in the embedding space because i'm looking at let's say cosine distance isn't it do you see that guys so you ask yourself well that is very well very lovely to say but how in the world will you find an image in a sentence that mean the same thing and vast amounts of it and this is a trick as always pre-training has a big element of self-supervised learning what do you do you scrape the web now html web developers what do they do when they put an image they are encouraged to put the alt tag or put a caption there. Nowadays, as human standards is caption. You can scrape both of those, isn't it? And so you can come up with captions now that the reason this is interesting is a prior to this paper. The biggest data sets were the image net and so forth right in which there were x number of actual labels i believe there were 30 000 32 000 or something whatever number it is is finite isn't it number of objects but the world has an infinite variety near infinite variety of objects isn't it so just when you are done with cats and dogs, a zebra comes by, right? So what do you do? How do you deal with that? So the beauty of this is you get your image and caption pairs. What they did is they took, they sort of, they said that, see, when we have sort of pollinating the embedding space. Right. Let's pollinate with only a finite number that will treat this as a target space, and we'll just take a finite number of captions. Right. A caption could be this is a dog. This animal is a dog is a caption or something like that. So you sort of standardize the text that are similar to this, you will mark it as that image marks to this caption. So you essentially create in the pre-training process, you end up creating sort of label data, but in a self-supervised way. It means nobody is sitting and creating those labels. You don't have an army of people. For to create ImageNet labels, literally thousands of people had to work. But here you don't have to do that. You can do that. So now that you have a lovely label data, do you realize that the rest of the story is one picture says it all? What do you do? You take the text, you create an embedding. You take an image and you create its embedding. And then what do you want? You want that this particular dog, the embedding of that should be the photo of a dog. Right? Those two things should have the lowest cosine distance. Common sense, right right in other words they should be forget all of this weird thing all it means is that in the embedding space they should be proximal right that is it and if you get this picture you got everything that there is to this paper after that it is if you look at it bragging rights come. Asked a lot of students. How do you make this vector the same or close to each other? So what do you do in the beginning? So let's say that you have an untrained network. You give it, whether you give it an image or whether you give it, so you have a text image encoder which emits out a vector. Let's say 100 dimensional vectors. You have a text encoder which emits out, just like your sentence, it emits out a text vector, also in 100 dimensions. Now you know that this, so here's the triplet you create. The positive pair is this text was actually the caption to this image. And how do you? So what do you want to do? You want to make sure that where they land in the embedding space, whatever their distance using gradient descent, you want them to start coming closer right? then the picture and a random statement, they should not be together. They should be far off. So again, they will fall wherever, but then through gradient descent, you want to push them out. A few epoch after epoch. Isn't it? That is it. And so you keep on doing this little bit, little bit, millions and millions and millions of steps later, the things will settle down. So you're pushing apart the, pushing the third of the triplet from the positive pair of caption and image. Exactly. The random sentence. Okay. Exactly. The cosine distances are maximizing the cosine distance, maximizing the cosine similarity in the positive sample and minimizing the cosine similarity for the negative sample. That is it. That's your loss function. And the beautiful thing is any loss function that you can write as a differentiable mathematical function. differentiable mathematical function, magic kicks in. Gradient descent, the whole power of calculus. I mean, many times I've said, and people don't like it, that all of AI is nothing but applied calculus. So whenever you are getting absolutely amazed at this, remember that you are paying reverence to the great God of mathematics. It's all math. Just very simple ideas of math, gradient descent, calculus, minimization maximization it all boils down to that right in some n dimension space hyperspace or something so yeah if I had to do this link between a picture and another language right so then I have to go from that language to English and then create this embedding? Or once I create the link, I can directly come, say from Chinese to this dog picture over here, right? How would that happen? Yeah, so Sachin, there are multiple ways people have done this actually. So it is a given fact that much of the web is English, right? So obviously image caption pairs are much more likely to be found in English. So how do I deal with, let us say, how do I deal with Karnar? A very sparse amount of documents exist. What I can do is I can take multilingual corpus that has English and Kannada in it, right? Because Kannada has been translated to English and English has been translated to Kannada, isn't it? So what you will do is you will do the semantic embedding of that, right, into the same space. So now what do you know? Into the same space so now what do you know into the same space are images are english sentences and are corner sentences and so what you automatically ended up creating is a proximity between an image and a corner statement corner caption even though you did not have enough image to kernel cap association data. You see that, right? In fact, hang on. The person who created the sentence bird thing, Rem something, why am I forgetting his name? He is now heading another country, a company, just started a company. And one of the things they do very well is this multilingual embeddings, semantic embeddings. In fact, just yesterday, I saw his post on LinkedIn. So there we go, guys. So that is one approach you can take. See, Sachin, in this world of deep learning, the general feeling is anything is possible. You just have to find the missing links. So you have to put the ducks in a row. And once you put the ducks in a row, it becomes possible. So guys, do you see quite a bit of similarity between... This is classic contrastive learning, right? Images and caption. And then the bragging rights are enormous. Zero shot image accuracy. Why is it zero shot? Because there was no training data, no label data to train it. Well, zero shot, as I told you, right, is always cheat. There's always a cheat way, right? Under the cover, it's self-supervised. Let's just say that, that right you call it zero shot and some people say well really you did you did have a way to create label data but zero shortage means no human created level data supervised creation of legal data a couple of a couple of um sessions ago you told us how the languages are different so Indian languages structured differently to French languages to even English so Latin languages to even English which is I guess I forget the term but it's Germanic I guess the languages so if these are all different how can they create this how can they say this is all normalized? A very, very good question. So Charlie says that there are various categorization of languages. Some are pictorial languages like Egyptian, like Mandarin, and so forth. There are languages like the romantic languages, and then the Germanic languages, and the the languages of indoor origin, and so forth. Indic languages and the dra of Indo-Aryan origin and so forth, Indic languages and the Dravid languages and so forth. They have a fundamentally different structure. And yet, how can we do the embeddings? How is this even possible, what I just taught? It has to go, there are many ways to answer it. The way I would like to answer it, and I wouldn't say that I'm the authority, somebody can answer it the way I would like to answer it and I wouldn't say that I'm the authority somebody can answer it even better see my way of looking at it is through the lens of norm Chomsky norm Chomsky the great linguist hypothesized that he he showed the universality of all human languages which is a pretty deep thought he He showed that despite the variety, superficial variety of languages, underneath it, it has the same structure. In fact, that's how you come up with words like Chomsky hierarchy and grammar that you use in compiler. Did you encounter that? Okay, you do that, you do that. At least for programming languages, you have that. So anyway, I'll just finish my thought. In fact, so if that hypothesis is true, it means that if you think of the Edgy Wells story, the Valley of the Blind, or imagine an experiment like that, you throw a bunch of young children into a valley and they live and they grow a language of their own, then theoretically, they will grow a language of their own, then theoretically they will develop a language not dissimilar in structure to any of the other human languages because they all share the same universal structure. And Chomsky went on and has hypothesized that it has to do with the brain, the mechanics, the physiology, the neural findings of the brain are, the structure of the human mind is the same, is universal. And therefore the languages we create are universally alike, right? And I think that is the reason why we can do cross or multilingual learning or embedding, because the semantic is the same, right?