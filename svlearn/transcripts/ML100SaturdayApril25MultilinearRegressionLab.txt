 But now, let us recapitulate what we did the last time. Last time we said that instead of just blindly using linear regression or polynomial regression, if you can look at the data and you can recognize a familiar friend, you can recognize a function, usually some transcendental function or even a normal function but then fitting a hypothesis to the data becomes as simple as fitting the parameters of that function or that distribution to your data and that can have remarkable predictive power when you can do that. The second thing I mentioned is it is a good idea to make yourself familiar with transcendental functions. And in general, no function so that you can spot your friends when you see data. It's a very powerful technique and rather underappreciated in machine learning at this moment, community at this moment. So we did that. Can you share your screen? Is this not coming through the shared screen? Oh, it disappeared, okay. Once again, very interesting, okay. Is it sharing now? Yes, chapter two. All right. So the idea is that can we have a family of function that we know and that occur quite often in data in nature? And so we can recognize sometimes. So we go back and revisit dataset two and we notice that it seems to have a sinusoidal structure. it seems to have a sinusoidal structure so instead of doing a naive linear regression which doesn't give as good a model but it works sort of you have a residual residual still exhibit patterns you could instead or you could do polynomial regression which just sort of works but you could you could just directly hypothesize that it is some A sine times kx. And when you hypothesize, it just takes a couple of lines of code. You just tell the program what is the data is A and y, x and y, of course. The moment you write a formula, if R cannot map, the tool cannot map a parameter to the data a variable to the data it assumes that it is a parameter of the model that it has to learn it is as simple as that and then all you do is model the non-linear least square will model you give some initial guesses and it will start from there and it will model when you do that the results are quite remarkable, actually. You see the residuals are pretty random. There is no pattern in the residuals. There's no autocorrelation. There are no outliers, and the Q-Q plot looks healthy. So you hit the nail on the head in the very first time, and when you plot the curve over the data, you can pretty much feel that you might have, your hypothesis is a very close approximation to the ground truth, whatever it is. I repeat the same thing in Python. When we do it in Python, everything remains the same. The only difference is it's a little bit more verbose, but not by much, just three, four lines. Maybe it becomes now five lines, but still the same. Python doesn't give you much about confidence, intervals, and so you have to do it on your own, little bit of coding on your own. And the rest of that is just residual plots you have to make on your own and plot it out. So this is the code. As you can see, obviously in this code, Python tends to be more verbose and and so the whole question is r or python generally if you can find the solution in r it will be more brief you can achieve a lot in fewer lines of code it has a very robust library on the other hand python is a general purpose language everybody knows it and so quite often you have libraries, equivalent libraries in Python. They are maturing quite fast. And occasionally they are better than R. In the deep learning space, there is a Python first mentality. Most of these things like PyTorch or TensorFlow, et cetera, they come out Python first, which doesn't mean that they don't do R. Actually, you can do Rtorch and you can do obviously TensorFlow in R and so forth. So those supports come, but they come a little bit later. And both of these communities are very robust. It is worthwhile knowing about both. Now, one good thing is that there's a very high degree of interoperability between r and python for example you can call python code from r using reticulate and then you can likewise call r code from python right r libraries from python so you can actually do in a lot of interoperability between the two. The second data set we took was the data set two, which to us looked like the bell curve, so modeling it with linear regression was hopeless. Polynomial regression led to the range phenomenon, the wagging tails, the oscillations in the periphery. Then we recall that this data looks suspiciously like a bell curve. It could be a bell curve, it could be other distributions like a student distribution and so on and so forth. But let's take bell curve, it's the most common one. Let's see if we can fit a bell curve to this data. When you try to fit, by the way, this is something I will talk about later. There's a whole family of curves which have tunable knobs that can be made to look through a lot of these sort of rising up and coming down or exponential decay and so forth. That's called the exponential family of functions in machine learning they are very very dominant so we will I'll give a one-day session just to the exponential family and how to use it for data analysis and you'll be surprised at how much power you can derive just from knowing for one family of functions but anyway this is actually bell curve belongs to the exponential family and you can it has a rather scary formula if you're not used to it but once you get used to it uh it is nothing for example at the top the numeric the exponential you realize that it is just z values the z value squared says e to the minus z square over two so it takes a little while to get familiar with it but once you become familiar with it it's e to the minus z squared over 2. So it takes a little while to get familiar with it, but once you become familiar with it, it's very straightforward. So how do you plot it? No difference in code. Three lines of code achieves the same thing. Once again, the correlation between prediction and reality is very good. And you remember that we were getting an R squared of practically zero, 0.007. But when you do this approach, you notice that something very nice happens. The residuals are clean, the outliers are missing, autocorrelation is absent, QQ plots are pretty healthy. And when you plot the curve, your prediction over the data, you really get a sense that you may have hit the nail on the head. So you can see that this is the cleanest possible curve. It is even better than the polynomial curve that you got. Like polynomial curve has these oscillations. It has excessive complexity in the periphery. Whereas this curve, the bell curve curve, is very neat. It is a much simpler model and so and with equal, in fact, better predictive power, slightly better predictive power and so Occam's Razor Principle says that the simplest effective model is the correct, is the definition of the correct one one if there is a definition for correct so we go with this model right and that is the power of using knowing transcendentals knowing functions and using them recognizing them right so the same thing in python again just a few lines of code nothing different the code is exactly the same as the code before except that the formula has changed you just define the bell curve oh sorry um let me zoom out a little bit which is yeah you can just define the bell curve you notice that the bell curve definition by the way when i pasted this code the indentation of python has gotten all wrong and so this code if you literally copy paste it will probably not work you'll have to fix the indentation. Python is indentation sensitive. I'm going to fix it in the notes in some time. So this is it. Likewise, the last one. You look at this curve and you wonder what it is. Generally, you wouldn't have a clue what it is. But if you know your transcendentals, you might suspect that it could be, for example, the gamma distribution. And the question then is, can we fit it? Can we fit and try that? By the way, in the beginning, when you're becoming familiar with transcendentals, and for quite some time, you will have two, three functions or two, three distributions with different parameters that may occur to your mind for example you could have thought of the beta function you could have thought of the gamma function it might sort of resemble the log normal distribution and so forth so what you have to do is just try each one of them and see which of them gives you the best model and it becomes very easy because at the end of it all you have to do is change the definition of your function that's all at the end of it all you have to do is change the definition of your function that's all because the curve fit takes the function definition you just have to write two three function definitions and feed it into the model and you can then do it and so if you do that like here let me do that then you realize that the gamma distribution let's see how good the parameter thing is and the correlations are. You realize that this one is just absolutely astonishing. The correlation between data and the y and y hat is 100 percent, it's one, and that is shocking. It's most amazing. It so happens that in this data, actually, I forgot to, as I mentioned, I forgot to introduce any noise. So this is not realistic data that you would find in nature. Actually, gamma functions are all over the place in data sets. And if you can recognize that, it's marvelous. In this data set, actually, I should remember to add some noise so that you don't get perfect correlations but here it is sometimes right so you get perfect correlation and when you plot the model over data you can see that that you have absolutely gotten it if you try beta function or you try log normal you will get actually some reasonably good fit but not as perfect of it right and there are other distributions for example webel and gompers webel for example has been the workers of reliability engineering so when you take a flight from here to india most of the time you are worried about whether you got a window seat or not where did you get a middle seat or did you get a aisle seat and things like that and how tiring the journey would be most generally the the fact that you might become food for sharks is not something you're thinking about you trust the machine to take you across the airplane to take you across yet an airplane is a very complex machine and it follows very complicated aerodynamics. Just to believe, if you just sit back and think, this machine can actually lift off this hulk of aluminum and metal and luggage and people luggage and people to 300 people this whole massive thing can actually lift off into the air go across the continents and reach the other end and it can keep doing it day after day after day for years a typical life of an aeroplane is what 30 40 years it can keep doing it and most of the time nothing happens in fact the reliability of these aeroplanes is extremely high when people say that aeroplanes are the safest means of transport it truly is so but if you whichever way you look at it is far safer than driving but if you ask yourself why is it so safe? After all, an airplane is a very complex piece of machinery. And the answer to that is, there is reliability engineering. You can take a complex piece of machinery and still get a very high degree of reliability from it. You know when things are likely to fail. You can see the failure rates. You have modeled everything and you do preventative maintenance, inspection and preventative maintenance. So it is the maintenance that keeps these machines very reliable. But what is the theory behind it? When do I know it's time to replace a thing? And for that, it turns out that reliability engineering utilizes the Weibull function, this Weibull distribution quite a lot. And most of you, obviously, it's a new term, but those, if anyone of you have background in reliability engineering, you remember that your textbooks are filled with the Weibull distribution. So sometimes, you know, these distributions are tremendously powerful things. The Weibull, the Gompertz, and all of these are very, very powerful distributions. And you can build the so-called accelerated failure time models and so forth. And therefore, you have a whole reliability engineering and so forth based on that. So that's the power of transcendentals. You should actually become familiar with those. As a poor joke, I used to say that instead of doing transcendental meditation, meditate on transcendental functions instead so now as a change of pace let's come to the new topic which is multilinear regression regression when there are many many features involved not just one one feature before I do that I would like to know, are there any questions? Any questions on the previous stuff? One thing guys, do the labs. Unless you do the labs, you won't become good at it. This nonlinear thing is in particular. Is that the code like having which change? I think for one of them, the syntax changed. So is that working or like still we need to make it work? Oh Yeah, yeah the library syntax Haven't gotten time Maybe today I'll do that and I'll update the notes These little sleep you can see these notes are in the old format format, and the book that we are writing is in the new format. So obviously, I have to redo it. And I'm adding more examples. Like for example, after doing the multilinear regression, doing the examples that are there in ISLR, I felt, which is what is there in these notes that I have, I felt a bit dissatisfied and I have added two, three more examples. One of them is the California data set. Another is air pollution and some things. A couple of more data sets I've added. Likewise, for classifier, I've added the breast cancer and I'm also looking at, obviously, a bit of a stretch or aspirational at this stage. if those of you who are interested i'm adding the example for covet go with detection and so forth that would be nice yeah so the notes are expanding and by the way if anyone one i think i may want to request to help future students obviously this is a this is a i'm asking it with an ulterior motive but you will you will benefit if some of you want to contribute to this book help add material to it add some and you can do that by adding good references or you can say these are the terms we encounter that i don't understand and i wish somebody had given me the definition or explanation of these terms you can send me a list of those terms and I'll remember to add it to the glossary or you guys find any good references you know videos websites books let me know so that I'll add it to the bibliography section at the end of the day you know whenever you learn a subject if I may say so the thing that is most important in some sense is this last section of the book which is about you know tips the tips that you have are the cheat sheets you know the quick reference things the doc the pointers to good documentation otherwise you just spend a lot of time searching through the web the glossary and the bibliography. The bibliography means a lot actually. So I'm creating the bibliography, at this moment it's not very long, one, two, three, four, five, six, seven, seven pages long. Typically by the time we are done and if you all contribute to the bibliography, you don't have to write it in this format. Just give me the link or give me a pointer. I will add those references and I will also add a chapter explaining why those references are good so this I said maybe we can also add like for each section or each chapter we can add like question and answers like maybe 10 or 20 questions relevant to that topic or that chapter absolutely guys if you can contribute quizzes or questions, I would really appreciate it. Please do so. I know that Prachin. That will be helpful. That will help us understand that topic much better. Some to-do projects. Someone, if they want to challenge themselves like they can do like additional projects definitely like in that chapter yeah actually i really appreciate it if you can come up with something and all of you if you can come up with something help you do that there's an old african saying that it takes a village to raise a child writing a book is like that a textbook textbook is like that. It's a gigantic effort. This book, by the time it's done, will be close to 1,200 pages. It's already crossed 500, 600 pages. And I'm still feeling that it is only half done. And in a big effort like this, it takes the help of a lot of people to raise this baby to bring it up so help me around in whichever way you can tell me give me suggestions read find typos as you find and just let me know that you found this or that and that makes a difference or if you feel that there is a section that could be best avoided it's not it's a distraction it's tangential let you know and then we'll work with that guys that will really help you all right so i really appreciate that and that means it would mean a lot i do know that prachi and dennis you guys were creating a quiz what happened are you guys still creating it oh we'll get back to you probably tomorrow all the questions yeah yeah because uh yeah tomorrow or so, or maybe the day after, let's actually release the quiz on regression and classification. It would be fun to do that. All right. So and then I'll also add quite a few questions, try to find time to add a few questions to that. All right. Coming back now to our subject matter let us start oh goodness no this is not I wish I'd written this book but I have not this one multivariate linear regression by the way the other book was Kevin's famous book on machine learning excellent book but a little bit advanced at this stage. Multivariate linear regression. What happens when your response depends on many factors, many features, or many predictors? There's a word people use. These are synonyms. Features, predictors, inputs. It's somewhat synonyms though. synonyms I mean people make fine distinctions between feature extraction from inputs and so forth but we'll use the word rather loosely these words say input predictors features these are all more or less synonymous so so we will take this data dataset and this one I've left an R. I invite you to just because you I have so far given you examples in both R and Python, I invite you to create a Python notebook around it and see if you can do that. From the for example, it is the auto dataset. Let me give you the background. Somebody long time ago created data for cars, their horsepower, their weight, how much acceleration they have. Acceleration is interesting in US and by convention, acceleration is the converse of what physicists would call acceleration. It is like how many seconds it takes you to reach 60 miles an hour, 0-60. of what physicists would call acceleration it is like how many seconds it takes you to reach 60 miles an hour 0 to 60. so the bigger the number the more underpowered it is not powerful it is right so for example if you look at acceleration of a car something that has an acceleration of 12 seconds is not as uh sort of a sporty as something which is relatively low let's say 8.5 seconds well of course in today's modern world these days what is it you go from 0 to 60 in a typical sports car in what four seconds is that what it is i don't i'm not very much into sports cars if one of you are actually i probably should say i am i have one but i don't drive it like a sports car but anybody knows what these days sports cars do these and the teslas do 0 to 60 in what four seconds three seconds four seconds right so yeah so obviously the world has changed. This data is old. Number of cylinders is typically two cylinder, four cylinder. Typically they tend to go in even numbers. Six cylinder and eight cylinder is of course pretty mighty. You don't find V8s easily. V6s are very common, V8s are relatively rare. I have it in my big truck, the Infinity. But it's a gas guzzler, right? It gets barely three miles a gallon. Tesla model is 2.3 seconds. It's 2020 model. Wow. And it is the, what did they call it? The ludicrous mode, isn't it? Ludicrous, yeah, Ludicrous mode. Well, that's 1 of the best. A lot of echo. Say that again. I was a waitress. I please say that again. I didn't get it. I was a wait is a please say that again I didn't get it. 2.3 seconds is as fast as as fast as a Formula One McLaren. Wow, that's amazing. Likewise for horsepower when this data was gathered you see 130 165 200 and so forth. Today when you okay we can look at this. Cylinders, minimum number of cylinders seems to be three. Maximum is eight. Or look at mileage, nine and 46. And the year is 1970 to 1982. 70 is the 1970, 82. So in those days, the max mileage you could get is about 47, which is really admirable. Minimum is seven. Horsepower is 46 to 230. 230 these days would probably be considered a fairly average car or not, not a muscle car anymore. These days, for example, the muscle cars have 600, 700 horsepower and acceleration is from eight eight is the best they could do in those days 24 was i suppose the fuel efficient cars a weight look at the weight from 16 13 which is a fairly lightweight car to 514 zero pounds so obviously this is a massive but not massive enough these days the biggest uv is almost six thousand right the weight seems to have crept up now you look at the origin when you do a summary of the data you look at the origin and you realize that it has only three values what do you conclude from that these numbers one two three are probably codes for countries or regions you know Asia Europe Asia Pacific Europe and US regions and if you look at the documentation associated with this data set you'll realize that that is actually true and so you should you should study so this is our data exploration space in these notes I've kept it brief what I invite you to do as in-depth data exploration as I have taught you to do in the tabular data chapter, in the early parts of this chapter, for example, data analysis chapter. You remember the wrangling with data and so forth. So do an analysis like this, a full in-depth analysis. I leave that as a homework for you. But since we are focusing on a multilinear regression at this moment, I won't dwell on that too much. Now you notice that the name is there. Do you think name is of any relevance to the prediction of the mileage of a car. So here the target variable is the mileage. We are trying to predict, given the attributes or the features, what is the mileage of the car, right? And assume that you don't know. You have just come as an alien. You don't really know what's the relationship between displacement, horsepower, acceleration weight to mileage at this particular moment, right? So you look at names and you ask yourself that can names matter like can the mileage of a car be improved just by changing its name probably not it's an irrelevant feature and there's one thing we should remove it from there so this is what you see me do here do you see that select in the data set i take i take a subset of the data i remove name completely because name is irrelevant so we end up with this data after that you make a pairwise plot once again i've kept the exploratory data analysis to a minimum because i have taught you much better ways of doing it. As you can see, a PEARS is a pretty, I mean, basic way of doing it here. I've taught you even better ways in the first few lectures. Use those, but in the interest of speed, I've just put it here. Likewise, here's your correlation matrix. When you look at the correlation, let's study this. When you look at the correlation, let's study this. When you look at the correlation table, you observe that there are some things which have very strong correlation. For example, miles per gallon has a negative correlation with horsepower, cylinders, weight and displacement. So let us think about it. Do high horsepower cars have lower mileage? Does that make sense? And why would it make sense? Could somebody argue for it? Guys, could you argue for it? Why would cars with more horsepower have lower mileage? Burns more fuel. Yeah, for unit time, you need to burn more fuel. It burns more fuel. Yeah. For unit time, you need to burn more fuel because you need to produce more power, more energy. The only way you get it, the thermodynamics of it, is that you need to burn more fuel to generate more heat, more energy. And therefore, you're burning fuel much faster, so your mileage will be lower. Now, why should mileage have to do with cylinders more cylinders burn more fuel exactly it's like each cylinder is like a mini engine of its own so you're essentially having more engines right with its own piston its own chamber with its own piston and uh you know camshaft and all of that i mean obviously it's attached to the same camshaft but okay yeah then weight why is mileage related to weight more energy to drag heavy cars so it becomes more sure yeah you you have to achieve quite a bit to achieve the same velocity the same speed let's say 60 miles an hour it is kinetic energy is mv squared so mass is there so the more the mass the more the energy it takes to take it to 60 miles an hour or whatever speed you want and therefore you have to spend more fuel and therefore your miles per gallon will go down and now why should it depend on displacement what is displacement measure of anyone cylinder to compress the fuel the amount of displacement that the energy needs to do right the engine needs to do right like the engine needs to do so the displacement is exactly yeah it is exactly what you say to come this certainly or another way to say it is the internal ignition champ chamber right or the cylinders of total ignition chamber size so usually big displacements are signs of big engines so not only the number of cylinders but how big the cylinder is and so how much fuel you'll inject you know you you atomize or you mystify or whatever through the nozzle you spray into the chamber no you have a big cylinder, you'll have a big, and therefore a big displacement. You will have to inject a lot more fuel, and you can create a lot more ignition power. So the total amount of fuel you burn is proportional to the cylinders and the displacement of each cylinder. And so you can imagine that given that horsepower should be very much related to both cylinder and displacement isn't it nice then you notice that the correlation between miles per gallon and other predictors are a bit low but there's still some correlation we also notice that there's quite a few strong a pairwise correlations for example cylinder and displacements are correlated. Why are they correlated, guys? Could you give an explanation? Why are cylinders and displacement in a car correlated? What's the physical intuition behind it? Larger engine. Generally, people who have a lot of cylinders in the engine, V8 and so forth, they tend to be power hungry, like they want horsepower. So you can get horsepower by two means, by increasing the number of cylinders and the displacement of cylinders. They sort of go hand in hand. It would be silly to put a tiny cylinders and lots of them, but not go for displacement. cylinders and lots of them but not go for displacement so they usually go hand in hand the other is why is displacement and weight related so strongly generally when you have heavy cars or heavy vehicles you you do you do need more powerful engines one measure of power is obviously displacement and so you can see that even cylinders would be correlated with the with weight sort of isn't it 0.89 that's fairly strong correlation displacement and horsepower why are they correlated that's a no-brainer um more displacement will cause more ignition of fuel ignition will therefore lead to more horsepower so let's look at the so the way by the way this problem is there in your isla so i'm taking it through so that we are in effect solving a problem from the islr book from your textbook one thing is you make a scatter plot of horsepower and mpg miles per gallon when you do that here is the scatter plot of miles per gallon what is the relationship does it look like a linear relationship guys is this a linear relationship is there kind ofing, kind of curved down to the right? So this kind of thing, and again, it goes back to what I say, know your functions. If you know it, you will realize that this sort of behavior is unique to a very, it's a classic signature, not unique to, but a classic signature of reciprocal functions Y is proportional to 1 over X to the n where n can be 1 2 3 you can try it out plot it out on your R of Python you'll see that it plots like this data plots like this it begins to look like that so how do you linearize it if you are going to do linear regression one of the is, can you convert this relationship to a linear function? So I would like to tell you a trick, which is very easy, but perhaps not many people catch on to it. To linearize a reciprocal law, all you need to do is take the log of it. Because if I take the log of this, what does it become? Log y is proportional to minus n log x. It's elementary mathematics nothing magical you must have picked it up in your high school and perhaps forgotten it it doesn't occur to you when you see it it becomes obvious so now what happens log y as a new variable is linear in log log x isn't it so that is important you have linearized the relationship. So let's move forward and say, let's build a regression model in just one variable. Can I predict miles per gallon using just horsepower? If you try to do that, this is your linear model, one line linear model. You can do it in Python. Again, the code will be very much the same as before in your previous labs. Python. Again, the code will be very much the same as before in your previous labs. You realize that you get a fairly good model and your adjusted R square is 60%, 60, 61%, give or take. It's a pretty decent model. It's not bad. And if you visually inspect a straight line relationship, wouldn't be that bad. All the metrics look reasonably good. The t-value looks healthy. The f-statistics is good and so on and so forth. But when you plot the model over the data, you notice that it leaves something to be desired, isn't it? It's not the best model that you could have built. And by the way, all of this code is something that you're already familiar with by now. Hopefully if you have been doing the previous labs. So you just plot it out. What can we do? Is this the best relationship? You look at the residual plots. The residuals tell a tale. When you look at this residual, are you guys seeing this residuals versus fitted? Or do you see a pattern in these residuals, guys? Yes. In fact, it seems to funnel out, isn't it? Here, the residuals are pretty close to each other. But as you move on the left-hand side, but as you move to the right, the residuals tend to spread out. What is the name of this particular pathology? Do you guys remember? I've explained this to you guys yeah it is heteroscedasticity exactly right or what I call it is a cigar shape or a sort of a funnel shape whenever you see that generally when you see funnel shape this particular disease or this particular pattern in the residuals here is a hint guys it generally means that don't deal with raw variables take a power transform of them remember the box cox thing that i mentioned take a power transform and in fact we talked about it the power transform that seems to linearize the problem is taking a log transform isn't it uh zoom out by taking a log transform you can linearize the relationship which was like this and so data seems to uh sort of suggest that isn't it we will uh where am i going yeah we'll do that so all of these have leave something to be desired do you notice that the qq plot also is sort of okay but it sort of doesn't hug the axes very strongly all of it indicates that a hypothesis the relationship is okay okay it's not very good right do we find points of high leverage not particularly or but you're getting there a couple of points seem to be exciting under your influence there is a cook's distance there is a fence here it's called the cook's fence which you don't see in this diagram which means that you don't have really points of high leverage. But if there were, you would see that. And let's see if we can do better. What R squared did we get? We got an R squared of 60%, 60.5. What happens if we instead model and do a log transform? You take the log of miles per gallon and the log of horsepower and then you build a model it is something actually the isla book doesn't talk about these things but these things come through a mostly practical experience in the in the field so I thought I'll share this with you just doing a simple log transform do you notice that your model has gone to 72.2 percent r squared now in practical terms that's a huge jump right that is a 20 jump in your uh in your model goodness right so that is a remarkable with just one line change in your code just by thinking through and again goes to the fact that if you understand your math if you can think through the data you can do very well and right away you see is is this residual looking much better guys would you say that you see almost no heteroskedasticity here isn isn't it? There's very little pattern in the data. There is a little pattern, but your normal QQ plot begins to look much better. In general, it's become a much better model. So you do that, guys. Do think through your, get into, wear a mathematician's hat always. Remember the data, behind data is dynamics. And all of nature and all that happens around us, all systems are described by mathematics. So get into the mathematical mode. Now look at this. I'm projecting the hypothesis over the data. I've taken the log transform of the data and I'm projecting the hypothesis. Is the hypothesis of the model agreeing with the data? This is a far better agreement with the data than, for example, this model, isn't it? Look at this model here. Data seems to be sloping down. This is a straight line. On the other hand, here, they are both in a much better agreement. And realize that we have just taken one feature, just one feature, horsepower, and we already seem to be at 72% accuracy. What happens if we take all the features? Let's build a model with all the features. So what I'm doing is origin is 1, 2, 3, so we have to make it into factors. Factors means categorical variables as categorical. So when you do that, one easy way is that miles per gallon depends upon, you see this, the way to interpret this formula is you're saying miles per gallon, make a model where miles per gallon depends on everything, dot stands for everything, but remove origin and then add origin back as a factor. It means you have to convert it to a factor. You could have first converted it to factor, and then you could have just said miles per gallon depends on origin, but I was being a bit lazy, I suppose. When you do this, you get this model. Now this model, if you look at it, first thing you notice is the adjusted R square is pretty good, it's 82 percent all right so it begins to feel that perhaps you're making progress but then some things are alarming do you notice that the p value of cylinders what is it there are no stars here this model seems to say that the number of cylinders don't matter for mileage but it it goes against practical experience, isn't it? Number of cylinders do matter. Displacement seems to matter. Weight seems to matter a lot. Goodness, acceleration doesn't seem to matter. Cylinders doesn't seem to matter. Displacement sort of matters. What is going on? Horsepower doesn't seem to matter. How many of you feel that this flies in the face of common sense would you yes that if you really think about this model absolutely doesn't make sense so what do you think happened so it is a little bit hard to think what happens, but let me explain what happened. When there is a... Go ahead, Darius. It's because, you know, a cylinder's displacement, horsepower and weight correlate with each other. So we cancel each other. Exactly. Exactly. So, you know, there is a phrase in English called stole my tender, you know, to give you an example. Let's say that you have just made some sort of a, I suppose in software world, it does happen. It's not pleasant. Suppose you have been sitting in your office or your cubicle and you did something, your table and you created something really nice and some colleague is passing by and you very excitedly explain it to him and then the colleague walks over to the powers that be the bosses and explains that this is the way to solve the problem right and implicitly implying that he did it when We all sometimes go through an experience like this. So that is, he has stolen your thunder. Means you were going to present it, get the impact of it, but somebody has taken that away from you. So that is the other thing. No, you did not. It's very good. The good thing is that I taught this, so you remembered it. I taught this in the last few lectures. Something called collinearity or multicollinearity. Remember I taught about multicollinearity. When multiple variables are highly correlated with each other, and therefore they can be written in terms of each other, as equations in terms of each other, you have multicollinearity in the data multi-collinearity if you go back to the notes you will see screws up the modeling screws up linear models and that's one of the weaknesses of linear models when they are strong multi-collinearity the model will invariably end up picking one of the highly correlated features. Here it seems to have gravitated to weight. It picked up weight. It gave a little bit of value to displacement. And it basically stole the thunder of horsepower and acceleration. Isn't it? So now horsepower and acceleration seem to not matter but and it's very misleading what has really happened is that they're highly correlated with weight and displacement and so those two variables stand these two are not there and that is what you have to watch out when you do linear models you have to watch out for strong correlations and know how to deal with it well let's say that you make this multilinear model you plot it out you ask yourself is it better than the single model yeah so like here like since the label like it says that like it's a cylinder displacement like so we know that like if like if it was just said as like a variable one variable two variable three variable four can we still make the same determination that all you need to do here you look variable one two three the moment you see wrong correlations you can immediately pick out and know that if I'm going to use linear regression, I need to pick one of them or don't use linear regression. Use other regression methods that are robust in the presence of correlations. OK. That's how you would do it. So, guys, when you do this model, once again, the same problem happens. You project this model on the data. It's a linear model. Looks pretty pathetic. You again have the heteroscedasticity. Where am I? Heteroscedasticity is still there. You can see. And it's a reasonably good QQ plot. We're getting somewhere. Now, what do you make out of it? Given the fact that it has heteroscedasticity, you say, okay, we seem to make progress, but look at this. Something is wrong with the model. Can we build a better model? Then the other thing you can do, and this is because the way the problem in the book was posed, the next thing you can do is you can add interaction terms when you add interaction terms your model gets even better you get to 85% of correlation when you get it and you can literally see that you know these are the interaction terms the product terms that have come in and this is by the way the our language means of adding interaction terms in In Python, it's a little bit harder. You have to do it by hand, but R is a bit more elegant in that space. The same thing in Python, at least as of this moment, is many more lines of code. Using Skicket Learn. In the stats model, because you can directly plug in the R formula, it will still be the same expression, which is why STATS model has been getting quite a lot of attention in communities that mostly rely on linear regression methods, linear logistic methods. So there we go. The star means consider displacement, consider cylinder, and consider also the product, all three possibilities. When you add interaction terms, your model improves, you notice. But again, its cylinders don't seem to matter, which is worrying. And it's hard to believe this model. You say, what is going on? The model works, but it's a bit bit misleading can we build a model that is not misleading and again this is a 3d visualization of the hypothesis surface that it builds i have put two two axes weight and horsepower and this is miles per gallon because here weight and horsepower seem to matter i put mileage as you can see it's a non-linear sort of a relationship did you see that guys and that non-linearity is brought in because of the interaction terms so now at the end of it what can you do the thing is now i invite you to experiment with this data and make the best possible model that you can. So this goes beyond what the book asked you to in the exercise to do, but go beyond that and ask yourself, what is the best model I can do? The reason I'm mentioning it is that data science is all about persistence. You build a model, you build a better model, and you build a better model, and you don't give up. You keep on trying to build a better and better and better model so one of the things that this model is missing is a log transform we already saw the value of the log transform what if we brought that idea of log transform into the data so now we are going beyond what your book could imply so I'm building a model now in the log transform of the data and I'm'm just taking three variables, weight, horsepower, and year. You notice that I'm not taking all the variables. I'm just taking weight, horsepower, and year. I could have taken something else. I could have taken a weight display, cylinder, year, or weight. It doesn't matter. They're all highly correlated. If I build a model in that, you realize that you do. What do you think about this model now? So let's see what we think about this bit of code. Let's focus on this. Do the p values look good? They all look reasonably robust. P values look good. They are three stars. robust. P values look good. They're three stars. And what about the R squared? This has jumped up to surprisingly close to 89%, 88 and a half percent. This is obviously beating all the previous models. And we are doing with only three variables and we got away with it. Now, here's a bit of homework exercise for you. You can very easily beat this model by slightly different variables. I leave that as an exercise for you. And that will be fun for you to come up with a model that beats even this. Do that and have fun with it as you do the homework. So you can beat this model. But now I'll just stop at this model. And you ask, is this, let's look at the residual plot of it. And this model, therefore, reflects this relationship. The mathematical equation that you ultimately come up with is this. Let me zoom out. Yeah. A little bit more zooming out. Yeah. Look at this. Log mpg. So this is the equation that you right to describe the relationship between the variables does it work it is this equation seven point this is the intercept this is the how much the weight matters how much the horsepower matters clearly the weight matters a lot more than the horsepower and both of them contribute negatively to the mileage, which again makes sense. But year contributes positively to the mileage, which also seems to make sense. So it looks like a sensible model. If you look at these statistics or these diagnostic numbers, they all look healthy. So now let's look at the residual plot. When you look at the residual plot, you realize that. Do you see that heteroscedasticity here? Do you see any significant pattern in the residual plot you realize that do you see that heteroscedasticity here do you see any significant pattern in the residuals here you don't see much just a little bumpy just a little bumpy but more or less you don't see any pattern there so it's getting much better your normal qq plots are getting better and therefore you at this moment i stopped here but with the caveat that I know you can do better. Very easy. I leave it as a homework. Do this lab and then try to improve upon this model. My hint to you is you can easily beat this model. The other data set that we talk about is the famous car seats data set. This is exercise 10 in your textbook this data set is about let's go and explore this data set again do the exploratory analysis of this data set but these are the factors the target variable is how much car seats will you sell based on a whole set of factors now this one I'll go through much faster, but you can summarize the data, you can see what matters. When you do the pair plots, again, don't use the basic pairs, which I used here, because this is in the interest of speed. I wanted to focus on the main machine learning part, but do it the way I taught you in the exploratory data. Now this is do it with elegance and detail. So do that and make your pairwise plots and so on and so forth. Make your correlation plots and everything. Do the missing value analysis, do all of that. I have sort of just hinted at it and moved forward. So you build a model, a sale of ice cream depends, a sale of not ice cream, sorry, car seats, depends on the price of a car seat seems to make sense, or whether you're selling in an urban or rural area and whether it is US or not. So, for example, car seats would not have much of a sale in India because it's not the law to have car seats. And so a lot of people don't use car seats for their children in India, though I would argue the laws of physics are exactly the same and people should be using. But in US, it's the law to use car seats for children and so you would expect more car seats sell in the US. Let's see if that holds out. When you build a model like that, you realize that price, of course, it is price sensitive in the negative direction. You make your product more expensive, you'll sell less. US matters, right? If you are in the US, you're likely to sell more car seats. Surprisingly, whether you're selling to urban or rural areas doesn't seem to matter. Or maybe not surprisingly, because parents are parents, people in urban areas and rural areas equally care for their children. And it being the law, of course, we all buy car seats. So urban does not matter seem to matter. What is the R square we're getting 23%? Would you call that a good model or bad model guys? Would you be satisfied with this model? No. Any comments, Karim? Yeah, you would say that. No. Yeah. Now, here's the, and this is the reason I brought this up, because goodness depends on which field you are. In the hard sciences, like particle physics, et cetera, for physicists, a good R-square would be like 99.999 or something like that, some ridiculously high value before they believe things. They would be very pinnicky and grumpy, anything below 90%. In other fields, data is not so clean. For example, medical data generally tends to be squishy. First of all, you don't get big data sets and it's very hard to gather data. You have to do meta studies on studies and so forth. The other thing is data always comes with noise and unknown factors. Human body is very hard to quantify exactly. Data tends to be soft and you won't get R squared in the 90s there usually. Then you go to psychological or social data, social sciences data, data gets even more squishy. Your R squares generally tend to be much lower. By the time you go to things like advertising and marketing and sales, the models are actually too many factors at play. And usually very simple models don't quite work. So actually, as you will see, the next page, to your surprise, even with low R squared, do you notice that the residual analysis is pretty healthy? As you go from left to right, there is no significant widening of the residuals. Normal Q-Q plot looks normal, quite healthy, which is surprising. And the lesson here is that R-square is not a measure of a good model. Sometimes that is the best you can do. You can simplify your model by throwing away urban and aspects so you have just price in u.s you get a pretty healthy model and you you get satisfied that 23 and a half percent r squared is as good as it gets because the rest of the model diagnostics is pretty healthy together and so it's a lesson to learn in that you have to live with it the next exercise in your book and I would suggest you do this is exercise 14 which talks about collinearity of data you artificially create data you create some random numbers which you call x1 then you create x2 by taking half of x1 and then you add some noise to it a normal noise in a half of X1 and then you add some noise to it, a normal noise, you know, Gaussian noise to it. And then you write an equation which is Y is equal to 2 plus, you know, made out of this. When you create this data, you will end up with a data set X1, X2, and Y. With the understanding that Y is the target variable, x1 and x2 are highly correlated, isn't it? So this is to illustrate collinearity, what it does to the data. So this is x1, x2 correlation. You can see a strong correlation. x1 and y are not that strongly correlated. So when you build a regression model, you get a model like this. You get an adjusted R square of 19 percent. That is okay. You get only X1 seems to barely matter. X2 does not seem to matter. Now, what is more peculiar is that if you just change the data a little bit, just add a point or two and your model will completely change. In other words, it has a lot of instability you you just change the model a little bit here so here's the model with just x1 just with x2 individual just with x1 x2 are pretty good but the moment you stay with the original model and you change it a little bit go back and add just one more point to the model, suddenly x1 doesn't matter, x2 begins to matter. So this exercise is just to illustrate the pathology of multicollinearity in the data. When data is highly correlated, here we literally created synthetic data which is highly correlated. Then your model will get hijacked with multi-collinearity. Linear models don't behave very well in the presence of multi-collinearity. That is the one lesson to learn. And so I'll stop there. Then there is the Boston data set. Once once again really we're doing it's about crime the crime in boston depends on what obviously there's a pair plot now by now it should all begin to look very familiar to you this is the correlation table you build a model you see as you can see just because you got a lot of factors just in your data doesn't mean they really matter they may or may not matter or at least from a linear regression. They may or may not matter, or at least from a linear regression perspective, they may or may not matter. So I won't go too much into it. This is all about taking some log transforms and trying and doing better. But at the end of it, a naive model has a 43%, a 44% R squared. And you can see that there are a lot of factors that don't seem to matter you can play around with it at the end of it you can see what works for you i played around with it and then i stopped at this model when i stopped at this model in which i have log transform polynomials the two tricks that i taught you both of them put here why did i take the log transform by looking at the you know correlation and seeing the sloping the sloping behavior curvy behavior i put a log there and at the end of it you suddenly go to 87 percent almost double the r squared and this model certainly seems much more healthy if you look at it and That is the beauty of it. You can build much better models just through experimentation and using the tricks up your sleeve. So don't give up. What we have learned is when you do a multilinear regression, you don't give up, you keep on trying to improve the method. So I'll summarize what we learned. We learned multilinear regression. We learned a little bit about signal and noise variables. Some variables don't matter on trying to improve the method so i'll summarize what we learned we learned multi-linear regression we learned a little bit about signal and noise variables some variables don't matter they just add noise to it and you have to remove it removing them helps next lesson we learned observe the degree of correlation between the regressors between the inputs or the features see what it is identify outliers that's important see in the you see that in the residual plots and so forth identify points of high leverage in the data check for interaction terms and the effects be aware of the phenomenon of collinearity and how such unstable models can be disproportionately affected by points of high leverage. And so these are the lessons. These are some of the things we learned. So this is what we learned. Linear regression is often called the ordinary least square regression. The name implies that maybe there are things beyond that. And in fact, ML 200 is all about very, very powerful regression methods that go beyond the foundations this is a foundational make sure you learn all of it because you won't understand the very powerful methods will do in ml 200 unless you have done this lab and understood these things are we together guys still they're very effective linear regression the beauty is simple it's interpretable and so forth so guys that is it we can take a break we have two choices we can end today right here or we can take a break for let's say 20 minutes or so and then come back and then do the classifier lab which way are you leaning guys yeah we can take a break and come back and then do the classifier lab which way are you leaning guys yeah we can take a break and come back yeah take a break of about 11 approximately 11 20. let's come back at 11 45 and we'll do the next time oh you mean like 1 45 120. yeah yeah one one yeah it is 120 now and let's meet at 145. okay so we'll see you guys then yeah as if have a question you can pause the recording let me pause the recording let's see we are on record guys now are you folks able to see my screen? Anyone could give me a feedback if you're seeing my screen? Yeah. You are. Okay. So we will talk about classifiers now. Now, remember, the classifiers are predictive models whose target variable is categorical. It's a class out of a category of classes. So it could be a cow or a duck. It could be things like that, you know, whether to give a loan or not to give a loan to a person, you know, whether the person is likely to default if you gave him a loan and not to give a loan to a person you know whether the person is likely to default if you gave him a loan or not and so many things whether the person is sick or not or or maybe whether if you look at diabetes a person is completely healthy pre-diabetic or with diabetes and things like that multi-class classification so that is the context for today's lab now now classifiers are obviously the dominant part of machine learning most like a majority 60 70 percent of the data set presents present themselves as classification problem so obviously there's a vast literature on classifiers. Today, obviously we will cover the three classifiers we learned in the theory class logistic regression, linear discriminant analysis and quadratic discriminant analysis. To do that, we will take, well, this is some background work. I've done it in R and in Python also, I believe. We will explore data set, it's called classifier one so as always we load the data we split that now here we are going to split the data into test and training set it is important now I've started doing that what it means is you should only split the data so that you know that you have over fit if your performance on the training data exceeds the performance on the testing data by a wide margin so that's how you come to know we discuss this in the theory part so here it is the descriptive statistics their inputs are X and Y both are numerical and T is the target variable is 0 or 1 it is sorry not here it is 1 are zero, one, or yeah, one, two, I don't know. What is it? Yeah, zero, one. So let's look at this data here. First thing we want to do is plot the data. So here is the plot of the data, basic plot. Clearly, you can see a decision boundary. I hope you can just discern a decision boundary going diagonally through it, off diagonally through it. The red dots and the black dots, there's a region of overlap between them. So right off the bat, we know that we are not going to get a perfect classifier. There will be some error rates because they are overlap regions. Fortunately, the X and Y are not, they're mutually independent, they are not cross correlated. So we don't have to worry about correlation between the predictors. This is a histogram of their relationships, just to see the x versus x frequency, y frequency, and so on and so forth, but they're two different colors, t is equal to zero, and t is equal to one, right, the binary classifier. We have sufficient number of data. For linear models, there is a rule of thumb that I keep telling you is that there should be at least 20 times the number of predictors. Here, there are two predictors. So, it means that you should have at least 40 data sets. Fortunately, we have actually close to 1,500 data sets, which is huge. No correlation between the predictors. So now let's build a logistic regression model. The method is very simple. It's very much like linear regression. The only thing is that there's a G in front of it. Generalized linear model. Instead of LM, we write GLM. Formula remains the same. Generalized linear model, GLM formula remains the same generalized linear models there are many many what guys please all right guys so getting thing, we have to give, and this is generalized linear model says you can use a lot of functions giving the relationship between input and output, the input vector, you know, the x dot beta and the output. And here it is the binomial or the logistic function logistic function is also called the binomial function so we use the binomial method it's historic just put it there this would be the way you would do it the formula t depends on this the only extra thing is you put the binomial word there and the g here other than that things remain the same it's a simpler thing the g here other than that things remain the same it's a simpler thing the null deviance is the deviance think of it as sort of the residual it's different but in a very very approximate sense you can think of it as a as like the residual equivalent of residuals the equivalent of residuals. Now, for the null hypothesis, the deviance is, oh sorry, the deviance is 2057, and the residual deviance of your model is this. The point is that if there is a significant reduction of null deviance, you feel that you have built a good model. Now, some people use the term pseudo R squared. What they do is they treat this as a TSS, total sum squared error, and they look at this as RSS, the residual sum squared error, and they compute the R squared, this minus this over total. you get something called a pseudo R square. I tend not to like the wording because it's a misnomer. It's not really the same thing. But quite a few books actually talk about the pseudo R square. So I'm just letting you know that people use those terms. I don't. So you build a model like that and let's see what does it come to. You're trying to build a decision boundary. Intercept is this and intercept of x, the intercept coefficient beta naught is this. This is beta 1, this is beta 2, so here we go, three values. Let's look at the model diagnostics. In classification, if you remember, we look at the confusion matrix. So when you classify logistic regression, confusion matrix so what when you classify logistic regression you predict what is the prediction that it is a black point so you need to choose a cutoff right here the cutoff for another reason in the absence of any information you take a cutoff of 0.5 you're saying if it is greater than 0.5, mark it as 1, otherwise 0. Likewise for prediction, test prediction, test training, test prediction. Apply the model to both, create the confusion matrix, and look at the accuracy of the training and the testing data, both of them. So first we do it for training data. This is a confusion matrix. Clearly, the principal diagonal is pretty good. We have made 50 plus 58. That is 108 mistakes out of 1,500. A pretty healthy one would say training data accuracy is 92.8%. Error rate is about 7%. Reasonably healthy. And then, so that's that. If you look at, so there is a library called Carrot, which sort of makes things a little bit fancier. If you give, if you call the confusion matrix method on using Carrot, what it will do is it will take your confusion matrix, that is this, and it will predict a lot of statistics like sensitivity, specificity, a positive prediction value, etc. It will give you a lot more metric if you're presenting your data to clients, etc. It may be useful to show all of these metrics. may be useful to show all of these metrics. Carrot is obviously influenced by the older statistical generation. You can see the use of their terminology, but very commonly used. The person who created Carrot has written this wonderful book, Predictive Modeling. And I invite you to read that. It has a lot of practical advice on how to analyze the data and quite a few people use carrot still i prefer using not using it i use other libraries but for what it is worth it's a good library but i do use the confusion matrix function it was well done now on the test data when you look at the accuracy it's 93.66 what was accuracy there? It was 92.8. So you notice that the test accuracy is pretty close to training accuracy. In fact, it's more than the training accuracy, which is a small accident of the data, but it's good. It means that we have a model that doesn't have overfitting. This code is exactly the same at the top. It's a pretty good model and we can see whether that the decision boundary when we plot it on the data agrees or not but before that we also need to draw the roc curve remember i talked about the roc curve so when you look at the roc curve it seems to remember i said that the the further away it is from the diagonal the better clearly this roc curve is quite far from the diagonal. It's practically hugging the top left-hand side. And the area under ROC curve is 98.4%, which is excellent. This gives you the sense that you have a pretty good model. One of the things you worry about is you took the cutoff at 0.5. Was it a good idea to do that? You have taken a different cutoff at 0.5 was it a good idea to do that would you have taken a different cutoff right and so you can do a plot of the cutoff versus accuracy how much accuracy would change as you slide the cutoff value from uh 50 percent and so forth and you realize that at the top there is a plateau there's a at the top of the hill there's a plateau which means that it's not very sensitive to the choice of cutoff pretty good values you get for anywhere in this top plateau of course don't go to the extremes and finally just as a linear regression we plot the model predictions of the model over the data and when we do that we see this decision boundary So would you guys feel that this decision boundary is accurate? Anybody? Do you like this decision boundary? Yeah, it looks pretty good. It looks pretty good, isn't it? Yeah, so that's that. So it's a good analysis. Now, in this, you have to be aware. All of this code, you'll have to become a bit familiar. There's nothing magical to it, just one-line code. Model, code you'll have to become a bit familiar there's nothing magical to it just one line code model one line makes the model then you have to do use the model to make predictions create the confusion matrix show the confusion matrix then find the area under rsc curve and then plot the model over data exactly what we did for regression so this is it how do we deal with python python is no different it's very similar here we go with python you take the data you again summarize the data visualize the data split the data into training and test and then what do we do we build a model a logistic regression model you see that logistic regression model solver is newton ng you don't have to give it uh not giving it would be just by default is good enough but then you make predictions you create the confusion matrix you get the same confusion matrix you get a you get precision and recall people these days tend to use those words more often precision and recall are 93 percent both very good the harmonic mean of precision and recall which is the f1 score is this it's pretty good what is the accuracy that came out to be somewhere we must have plotted the accuracy also i didn't mention the accuracy we can compute it from the confusion matrix and these are the model values. You try to plot this model on the data. By the way, this is plotting code. It's a bit of a boilerplate code. We tend to use the same code again and again. But I thought I'll give it because a lot of people have asked me, can you show how I make this decision boundaries and how I show the probabilities? So I've given the code in its entirety. And obviously I'll post the Jupyter Notebook so you can have it. It's a bit colorful. It can literally say that at the bottom, you're pretty sure it's blue, one type of data. As you go closer and closer to the decision boundary, you're not so sure the probability becomes half. And finally, the probability becomes red right it goes to the other region so you can see the decision boundary here for both of it I've given the code of drawing it area under our RC curve exactly the same thing you can see is the same value 98 hopefully yeah 98.4 percent area diuresis which is very healthy so this is using the using the skicket learn library skicket learn is of course a favorite library for statistical i mean for machine learning outside the deep neural networks region learning outside the deep neural networks region we move on to linear discriminant analysis LDA go if you remember LDA looks for hilltops and does a perpendicular bisector of them if you solve the same problem with LDA code is just one liner as always LDA this and you can ask LDA to plot type both it will do that you can create the models to plot type both. It will do that. You can create the models and so on and so forth. It takes a while to see. I mean, it just reproduces the same results. The confusion matrix is that the accuracy is 93%, pretty much the same, 93. Training error is about 7%, 6.8, more or less the same 93 training error is about seven percent 6.8 more or less the same there is no material difference between using logistic regression and linear discriminant analysis right you get essentially the same result all of this code of model validation are exactly the same the only difference is the one line you use the lda function to build a lda model that's it one line change other things remain the same area under the roc curve again is 98 percent very good here i've divided the decision the regions into the red and the black so you see that how a decision boundary splits the region into two halves. The same thing we can do using, well, this was with R and the Python code is exactly the same. All you do is you change it with linear discriminant analysis and fit. It is the same as the logistic regression code, you just make one line change. When you do that, again, you'll get pretty good results. If you do quadratic discriminant analysis, things don't quite get better. They're more or less the same, because here, decision boundary was linear. So trying out a more sophisticated model is not going to give you anything more. So this is it. The Python version is, again, one line change. Replace the word linear discriminant analysis with a quadratic discriminant analysis but if you look at this picture here you can see that when you use quadratic discriminant analysis think of the white thing here the decision boundary you notice that it is not straight it is curved if you look very carefully you can see a very gentle curve of the line of the decision boundary that is the nature of the quadratic discriminant analysis right and so that is that let's look at another data set a data set like this this data set has four classes you know there's blue green maroon and black four types of data are there four classes are there sorry and then we have to find a classifier for that. So obviously, it's multi-class classification. Logistic regression is not appropriate. You can use it, but it's better to try LDA. When you try LDA, it works quite well, actually. The model diagnostic, the accuracy is 95.6%. Look at this. The code remains the same, guys. No change in code from the previous guys, no change in code from the previous one, except that you have changed the data set. If you draw the decision boundaries, would you guys feel that these decision boundaries are pretty reflective of reality? Yes. Pretty accurate, right? And so you have an accurate model, you have a high accuracy, And so you have an accurate model. You have a high accuracy, specificities, sensitivity, all of that. And likewise, you can use QDA. Again, you'll get 96.7. I don't think their numbers changed. 95.9 became 96.6. It's a very slight increase in accuracy, not much more. But the decision boundaries when you use quadratic, do you see that they are all curved? The decision boundaries when you use quadratic do you see that they are all curved the decision boundaries look entirely different a qda has interpreted the data very differently from lda so it's something to know that models may have the same actions but the way under underneath it the hypothesis they build may be different you see how the red points are all within an oval kind structure? Isn't it? And this is different from the linear decision boundary of the LDA. If you go back and look at the LDA, you notice that these are very linear decision boundaries. All of them seem to have gotten equal regions or more or less equal regions. Whereas here, red is confined to a limited space it gives you the same accuracy now let's look at data set three when you look at data set three guys are you following me there's a lot of very quiet people i hope i have a question actually so um i read somewhere that the the difference between logistic regression and the linear discriminant analysis, like one of the aspects is if we deal with the multi-class classifiers, LDA is better than logistic regression could you justify because it is native to it whereas when you use a logistic you are doing one versus the rest right remember the one versus the rest that i taught you in logic so you're doing you're building first of all you're building a lot of models internally right so that is inefficient. Secondly, generally, see at the end of it, there are no clear answers. But generally, multi-class classification, you can do with logistic, but it gets a little bit harder. LDA, QDA, discriminant analysis is very natural to do that. You should certainly try discriminant analysis first now for example this data set I can make it work I believe it or not I can make it work with a logistic regression will be messier you'll have to build more models see for models right one versus rest kind of a thing for models so a little bit more computationally intensive and so forth computational intensity doesn't matter these days but still not as clean okay look at data set three guys this egg shaped in the center is yolk and outside are the dots what do you think we can do this is a binary classification isn't it maroon and maroon and gray can we use logistic regression guys what do you think it's not very linear no so even though it's binary classification you cannot naively use logistic regression the way to use logistic regression would be to use logistic regression with polynomial terms. And I leave that as an exercise for you to verify that if you use logistic regression with polynomial terms, then it will work. But a naive logistic regression, a direct application logistic regression will fail simply because you just visual inspection shows that the decision boundary is a elliptical shape it is not at all a straight line okay so you need something to capture an ellipse and if you remember your algebra from high school you'll remember that ellipse are quadratic equations so at the very minimum you would need a quadratic term your logistic regression and then it will work okay i'll leave that as an exercise for you you can try the one hour i mean so another one you can try the lda and qda and you'll get an impressive 99.5 accuracy if you try that right so that is something to know i would i won't give the code because code is redundant it's exactly the same, just substitute the name of this data set, load this data set with the same code. This kind of AUROC is certainly very impressive, 92 accuracy. Now, one of the questions is, could we have used logistic regression, I asked. Yes, you could have, actually. As I said, you can add polynomial terms with interaction because you have x squared terms y squared terms and x y terms if you remember the equation of an ellipse and when you do that once again your accuracy will be pretty impressive 90 you see the accuracy here 95.8 it seems to be even better than the lda and qDA. And this may surprise people. Quite often people say that you can't use logistic regression because there's no linearity. The statement needs to be modified by saying, you can't use logistic regression directly if there's non-linearity. But you can accommodate non-linearity by going to polynomial terms and interaction terms, as I did here. Actually Actually this is one of the questions I ask in interviews to data scientists. What you're looking at is I say can you solve this data problem using just logistic regression? And usually the answer I get is you can't do it. Clearly you can't do it because it's that. But actually you can and I'm still waiting for a candidate who will say that yes you can't do it because it's that, but actually you can. I'm still waiting for a candidate who will say that yes, you can do it using higher order terms. That is that for classification, guys. These are three datasets. I want you guys to warm up to it. The next time I will deal with a real-life dataset, a couple of real-life datasets. One is- Asif, can you repeat that again? Just one more time. Look at this solution. I have actually used this logistic regression, but I have gone to a different space, a more expanded space with quadratic terms in it. When I go to the space, I can actually make the logistic regression work best you see how it is by attaching polynomial terms to the logistic regression I suddenly have a very successful model with the the area the ROC curve of this is 99.46. It's close to perfection, isn't it? Yeah, impressive. Yes. So the lesson that I'm trying to emphasize is do not believe in cliches that people tell you. Think. This field is all about thinking, not so much about coding. And you can use very elementary code and come up with brilliant analysis of the data and just through thinking and thinking carefully through the data and how you can use your tools i mean i suppose i'll end this with a sort of story that i like to tell and humor me for telling that see when you get into photography so i'm into photography as some of you know i don't quite get as much time to pursue it rigorously but i do do photography so in photography in silicon valley at least here in the bay area you can always tell an engineer engineer who has just made a lot he'll be holding a very expensive digital SLR camera right and roaming around everywhere and you can and if you look carefully at his camera it is set on automatic as photographers know that in automatic mode most likely his pictures are no better than an iPhone in fact probably worse even with that fancy equipment right so then after a little while you know he tries many many things gradually maybe learns to venture out of the automatic mode but then he figures out that okay to take good pictures you need a good lens so he goes and splurge splurges on very big lens you can see those people in the beach if you ever go to the beaches here in the middle of the day they would be carrying a camera with a long lens and there will be a nuisance to their spouse or kids because they'll be asking their spouse and kids and family to pose and people would be just tired of them i don't know if if you guys have seen it. They'll be standing far and using a big telephoto lens. It's a fairly common sight if you are on the beaches. You know, photographers know that actually. You can immediately spot these people because no photographer would take pictures in the middle of the day. Can you tell why on a beach? Could somebody tell me why? Anybody with photography experience? Or exposure. Yeah, is it the light? Light is not conducive for taking like picture like during the midnight, like the not midnight but the in the afternoon. I think it's also the waves, right? Like, gives you a lot of different lights from different angles. That's right. You need ND filter, sir. That you can do with neutral density filter, but true. See, in daytime, what happens is sun is shining overhead. When sun is shining overhead, it casts shadows on your face so if you look at any pictures taken on the beach during the day you'll see shadows under your eyes and so forth those pictures are not nice to not I mean they're good as snapshots as memory but certainly and as snapshots they're wonderful as memory they serve a very useful purpose but they are not works of photography as such as art during the evening light comes at an angle at your face it's a softer light and you get much better pictures and paradoxically when it is foggy or cloudy again the light is diffused you don't have strong shadows once again you take very good pictures in foggy or cloudy weather then those are the time so good photographers will come out and take pictures let's say on the beach if it is cloudy foggy or if it is very early morning or late evening nature photographers only take pictures during very early morning and late evenings they really take pictures during the day this they probably sleep and rest if they are on a shooting shooting thing so the point is that that's how people behave and so the joke is that these novices you know they realize that camera didn't do it lenses don't didn't do it what must it be and finally it dawns on them that probably what they need is a big tripod and then they ask the biggest tripod if is willing to carry big heavy tripod and so sort of a joke but if you look at great photographers the ones who do the National Geographic and things like that they don't use point-and-shoot they use a DSLR but their DSLR may not be the latest and greatest they just have a good professional DSLR maybe two three the latest and greatest. They just have a good professional DSLR, maybe two, three years old, and they will have a simple lens, one or two lens at most. And they can go into any culture or any forest or any place. absolutely beautiful pictures that make their way into the National Geographic. So those National Geographic and those great magazines that have those wonderful pictures, they're actually taken by photographers who don't carry too much equipment quite often. And that is the point I'm trying to make, the long story short. In machine learning, there are a lot of fancy tools and they act as the shiny ball people get very much they they caught up in the glitter of the tools it is not about the tools you can use very simple tools it is about the craftsmanship you know like the photographer who can walk into any place with good enough equipment professional equipment but not fancy, and not too much, but can come out of that with absolute zingers. Be like that, guys. Look at data, think about data. Most great analysis can be done without the shiny ball syndrome. Use the tools when you have to. There are situations to use very fancy tools, but there are quite a few situations where you can you could have gotten much better results with simpler tools and a lot of thinking and that works better because you have very interpretable models models that you can explain to business to the layman and so forth anyway with those words i'm done with classifiers any questions guys classifiers. Any questions, guys? Not right now. All right, guys. So we have done regression and classification. Our last topic for this workshop is clustering. We will do clustering in the last two lectures. I hope, guys, see one thing I'm not able to enforce during the video lectures of course and this remote session says doing the lab in the class if you were there on the premise of course i would not have let you go home until you had finished the labs and i can't do that and so it is purely voluntary whether you're doing the labs or not catch up on the labs guys i know that most people to various degrees have been you know falling a little bit behind it's a natural but do catch up on the labs guys i know that most people to various degrees have been you know falling a little bit behind it's a natural but do catch up let's not go into ml 200 till you have caught up with the labs and done all the labs and reach out for help as you need to do asif can you explain like the what's the student t distribution uh let's keep it for the math i'll explain it to you maybe on one-on-one uh it's a distribution that sort of looks like a bell curve but uh but is different uh it's flatter in some sense uh let's talk about it later because it needs a lot of explaining background okay yeah because in one of the exercises it was mentioned like so when i was doing it the exercises it was mentioned like so when I was doing it like talked about like the student fee distribution ah that is for the confidence interval and so forth yes yes yeah this woman just use it because in engineering over the distribution in great detail so let's keep it for that yeah I'll stop the recording guys