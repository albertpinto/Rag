 There are multiple ways of data augmentation, one is to literally go out in the field and get more data. But if you can't get data, somehow generate more data. Let's say that you have an image you have an image of a person let's say this is a a person with his hands raised so what you can do is you can create one more data point by having him stand this guy stand on his stand on his head or you could do the data augmentation just by so this is a rotation right you can do various rotations you can flip left to right right so you could do for example right to left left flip and and so on and so forth by various rotations and flips. The thing is, we are training. So suppose this is a cat. You can get a lot more images of cat simply by doing these things. The other thing you could do is you could just look at little patches of the cat. This patch, this patch, this region, this region. And so by looking at sub regions of an image, you can get more images. Now, once you know that these deep learning models, a lot of their lack of accuracy actually, there's a lot of overfitting always built into deep learning right so you're dampening the more data you give the more you dampen it down now why does it do that the explanation for this is quite simple actually suppose you have three points or five points so what will happen is you could do pretty complicated model like this. We just five points. But now suppose I bring lots and what has it done this curve has gone and fitted to every point of the data that you see here. But suppose I increase the number of data points. Let me mark more data points with a different color. Now suddenly you give more data points. Now what happens guys, you realize that your this thing cannot over fit this white line or a flexible model, it will be forced to follow this path right it will be a force to follow the actually we use some other color it will be forced to follow the path of the data right so it will be forced to follow this path would you agree that green becomes the best fit line now? Yes, definitely. If that is really what the truth was, then you have the reduced overfitting simply by getting more data. You have removed or you have regularized with more data. So that is the value of data augmentation strategies. Or as simple as that, just going back to the source and saying, I really need more data. With deep neural networks, no amount of data is usually enough data because some of these models are so ginormous that anything that you think of as a reasonable data can always be improved upon with more data. Data augmentation. reasonable data can always be improved upon with more data data augmentation any questions guys before we continue so as if in this when we talk about data augmentation there's an implicit assumption that the sampling strategy was robust oh yes that is very very important so it is well shuffled and the whole thing is truly randomized so the way we say that is if the data is auto correlated or somehow there is some pattern in the way these things are not independent of each other these data instances that you are learning from but then you are in trouble unless you explicitly want to do that so for example if you are doing forecasting in time series but by by definition the data will be auto correlated in other words the the the temperature tomorrow is highly dependent on the temperature today and yesterday and day before they're good predictors of temperature tomorrow right so average temperature of this month or the moving average would be a good predictor of tomorrow's temperature. Other than those situations where you explicitly want that, in everywhere else you don't want that. You want the data to be truly independent. So what you do, what you see is this. Since you brought you brought the topic up i will throw in i suppose and if you guys don't get it it's all right but i'll just do that because it's not really relevant to what we do people often use this word iid in this literature you will often find this word or this sort of word what do we call this this keyword or this acronym iid what does it mean it stands for quite a mouthful independent and identically distributed. So what it means is that if you look at the red points, if that is the truth, the ground truth, that is the distribution. So all the data points that you're learning from should truly have come from the ground truth. You cannot mix two different ground truths and take some data points from one and take data points from another because then no learning is possible. You're trying to, you know, half the data points will try to take you towards function X, one function, another will try to take you towards function x one function another will try to take you to another function g let's say f or g and that won't do common sense says that if you want to model a ground truth it better all the samples must come from the ground truth data generated by the ground truth right so that is the identically distributed part right right? The word distributed comes from the distribution, like how the data is distributed. And there's a bit more probability, etc. We won't wait until but later. Independent means that this sample of data does not depend on the other sample of data that you picked. In other words, don't pick it in a sequence. Don't just go here, here, here, one by one, start picking the data. And quite often, so long as you're careful and all you do is shuffle the data before training, you should be fine. So you must have noticed that in the data loader, if you go back and look at the code, data loader, you will see the syntax. You give the data set, you give the batch size you remember you remember guys you give the batch size and what else do you give you will see that I have given a parameter shuffle is equal to true means for every epoch every time it cycles through the data in other words in every effort you shuffle the data all over again it's generally a good practice to remember and do this shuffle is equal to truth It's generally a good practice to remember and do this. Shuffle is equal to truth. Are we together? I see one question. So the analogy of this image rotation and shift left to right. Is that exactly what so we're talking about the augmentation of the data meaning that is data yes original data is just one image and then you are creating augmentation by rotating it yes okay so in doing that you're actually increasing the more data points as well right yes the rest from one to three data points or five data points or 10 data points. And you do that a lot. Generally, it's a good idea. And what you do is, I mean, if you're not doing data augmentation, you should ask yourself, why not? It's a freebie, right? Usually data is hard to get. But once you get the data, you want to squeeze the maximum juice out of it. So it is worth asking, why am I not doing data augmentation? I should try to do that. Go ahead. Can you do data augmentation with tabular data, or is this just strictly for imaging? And you can do that with interpolation. What you can do, and it's a little bit, I mean, obviously all of these things are fought with a bit of danger. You look at the feature space and if you can, you can sort of get an idea or see, again, this is a circular argument. If you knew the hyper surface that the data is close to then then in a way that is what you're trying to learn but to the extent that you can find a relationship between some of their variables some of the features and you can reasonably interpolate between them yes you can do that see one easy data augmentation for tabular data is they always come with NANDs, you know, some gaps or missing values in the middle of your rows, some rows, some columns of some rows will be missing, like column values. So when you go and put a reasonable value there, it is data imputation, what you call you call you impute a value to that cell when it was missing you are doing data augmentation isn't it yes salvaging an entire row of data that would otherwise have been you know dropped so what's about one heart encoding as far as matrix oh no no that is not that is not data that is just changing the representation of data. It's not data augmentation. Okay. I see a further that means that whatever we use, you know, smart to fix, you know, fix the imbalance data imbalance. So is that a data augmentation as well? Yes. When you use smart, what are we doing? We are actually doing exactly what this interpolation business. We are interpolating and adding points. And so as you know, with SMOTE, the strength is that when it works, it's great. The interpolation is successful. The weaknesses, if you are not careful, SMOTE will completely misguide you and it will actually be performing yeah we saw that if you remember in the boot camp we saw that yeah because this is something smart handling the behavior very nice all right so are we done with data augmentation now uh one way that you could do the next strategy is called batch normalization this is quite interesting every once in a while we think we understand what it does and then we shake our head and think, no, not quite. We don't quite understand what it does because the way it works seems to be not the way we thought it works, right? Or why it works. We know what we do, but we don't know truly, actually, very clearly in uncontroversial manner what it does. You talk to different people and they'll give you different answers. So I'm putting that caveat out because when you read at least these two textbooks that you this lab books and things that we have mentioned they all seem to say something which now we know may or may not be true. That kind of wisdom is dubious these days. So what happens is that batch normalization comes partly to cure a set of problems. So what happens is that your data can get really big or small. It keeps shifting. Suppose you have one layer of data and you send a batch of data here, batch. What happens is that if all the inputs, now this data, the output of this is the input to the next layer, which is the input to the next layer. Now, what happens is that one thing that we know or the reason you cook it up is that you don't want these values to progressively, let's say, keep exploding or becoming bigger and bigger. Right? Generally, neural networks don't perform well when the values are significantly shifted away from zero. You want your values to be somewhere within a narrow band of zero. Close to it, ideally minus one to one or zero to one or something like that. You do want it to be there. But the output of your neural nets, obviously it's a response function, which is a sigmoid. Y hat is a sum activation function actually let me by the way i'll use sigma here because i'm in the habit of using it but remember it's activation function any activation function it could be relo and by the way next time i'll talk about the activation functions right so it could be logit or sigmoid these days people call it sigmoid, in the deep neural network, which again is a use of terminology, sigmoid. It could be tanh, it could be ReLU, Leaky ReLU. What these words mean will gradually come to, but some activation function doesn't matter. So it is the activation of the z where z is a bias plus weight dot x, the input of x. So y indirectly is a function through the activation of this. Remember the box that we talked about, the magic box, inputs come in. These are the weights 1, w2. So here you get z by z is a bias term b plus w1 x1 plus w2 x2 which we essentially write as b as w dot x. Right and then you have this distortion function the activation function that distorts it in something so now when you do that you don't know how big the values come out and then you don't want those big values to propagate through so what you do is at each step you take the output and you normalize it normalize it So what you do is given a batch, a mini batch, you will compute the mu of the batch and this, well, I suppose sigma because I use sigma for standard deviation. Let me just use standard deviation of the b you find that and then you normalize every value every out value that goes into the next layer as um suppose you have a it's called the z valuing you take this let's say that the input will z is used for something here let me just use x prime any input that goes to the next layer, you subtract it and you divide it by the standard deviation. Are we together? So what you do is you subtract the mean and you divide it by standard deviation. So when you do that, typically values will be between minus three and three, most of it. And in fact, most of the values will be between minus three and three most of it and in fact most of the values will be between minus one and one right so data becomes the sizes again reduce so that is batch normalization right so one of the effects of batch normal so guys any questions on batch normalization and why it is a good idea to do it. At this moment, take it as a fact that neural networks or these layers don't like it if the values are too large, inputs are too large. So the normalization, it's happening for the given batch only, so the new and the standard deviation is for that one batch. For that batch, yes. for that batch, yes. So, is this to make the computations more efficient or is it really to determine the behavior itself? Yeah, what happens is if you don't do batch normalization, batch normalization, by the way, was discovered in 2015. Very recently we discovered that it improves the neural nets. What happens is if you bring in batch norm, if you don't bring in batch normalization, especially for let's imagine an activation function like this and suppose the values all become very big. So you are in this region of the irrespective of the weight, you end up in this region of the activation function isn't it your z will become large because your input value is going in a large now look at this what is the gradient here what is the gradient slope is zero zero so what happens is learning is very slow right it is very small. It's not quite zero, but you're creeping along in your learning process. Whereas if you're in this region, close to zero, so here's the zero value of a tan h or sigmoid or a sigmoid function. And then what happens? You have a substantial slope. This is where the slope is maximum. So anywhere close to zero, you'll have a pretty robust amount of learning at each step, isn't it? Because the slope is good. But if it is at zero, doesn't that mean it doesn't need to be learned further? So it should stay at... No, no, no, no, no. This is the input. If the input is zero, it doesn't mean that the weights are zero. No, I understand. But so this is the sigmoid function, output of the sigmoid function, right? Right. So if the slope is zero, doesn't that mean that the output is very close to the real number, the correct number? No, actually it doesn't because what happens is that, first of all, it will never be absolutely zero. It's not learning you are just far off you're creeping your way back towards the right you know picking up momentum in learning very very slowly right just because you saturate the sigma the activation function doesn't mean that you have reached the optimal point. You just have, you know, you have a situation of vanishing gradients. Your derivatives are close to zero. Nisar, did you get that? Yeah. From a vanishing gradient doesn't necessarily mean that the loss function has reached a minimum. It just means that you're trapped. It's just improving too slowly. You do batch normalization, it takes you out of that situation. You're moving along fine. So then the question comes actually that what has this to do with regularization? So initially people thought that you actually do regularize with batch normalization. Now, and there is some effect, but then it turns out actually that, well, maybe not quite. Right. And the situation has become tricky today we know that if you do batch normalization without bringing in residual short circuits it's a concept that we'll learn about later we are basically asking for trouble but that's a topic for later but for now in a very general sense, I think that batch normalization does have some regularizing effect. Oh, by the way, the one more regularizer that I should mention, let me mention is seven. That is sort of not a real technique, but it is batch size. If you take smaller batch size, what happens is that you have a lot of perturbations of the last surface. In other words, because you're learning from only a small amount of data, right? It is taking you all around, you know, but you're not, you're just moving around, zigzagging around much more, fluctuating a lot more than with huge batch sizes. So another way to look at it is, see if you have, imagine that this is your loss surface at this moment with this batch, B1, then B2 comes and it draws a slightly different loss function. Then B3 comes and it draws a different loss function. So do you notice that the loss functions are fluctuating? Isn't it? So then what happens is if you take smaller, the fluctuations are less if you take large batches, but they are quite high if you take small batches. That is actually a good thing because what happens, imagine that you have a local minima. So let me take this white line, let me take this white line and then going this. Now what may happen is this yellow line doesn't achieve a minima here, right? So one batch of data might have trapped you here, but the other batch of data actually happily sends you forward from that point and says, don trapped you here, but the other batch of data actually happily sails you forward from that point and says, don't stop here, let's move forward. Do you see that, guys? So suppose in the learning process you are here, if this entire thing was your complete data set, you could get stopped at the at a local minima but now suppose you are fluctuating your because each batch is a different set of data with different degrees of errors in it so each of these batches of data will draw a different loss function will will compute the loss differently with respect to the parameters and so let's say that the next batch is yellow and it draws the loss surface with respect to the parameters and so let's say that the next batch is yellow and it draws the last surface like this what will the yellow point do so suppose you're sitting at this value the yellow point will say no no you don't stop here you continue moving downwards are we together so uh having a reasonably small batch size is a good thing which is why in new networks, you never take the entire data set as one batch. You don't do what is called batch learning, batch gradient descent. Two reasons, the usual reason that is quoted is that the data won't fit into the memory especially of the GPU right but that what that's a practical reason a more sort of mathematical reason is you're much more likely to get trapped in a local minima if you use batch gradient descent whereas if you use mini batch gradient descent or stochastic gradient descent which is a sort of a limit form where batch size is one you are much less likely to because your last surface is going through motivation subject are we together guys am i making sense guys am i making sense yes i i don't completely so you're saying you can use a picture I don't completely so you're saying she's a picture so you're saying the smaller the bad bad size the worse it is because we can no no no I'm saying it's a good thing the small back to the lost surface fluctuating so if the lost surface is fluctuating let us say that one batch told you that you have achieved a minima. The next batch comes and it draws the yellow line. So what is the next batch telling you? You haven't achieved a minima, there's more to learn. And so it just pops you out of the local minima. Isn't it? It is a good thing. I said one thing I didn't understand is how did batch normalization help us come out of the vanishing gradients? Okay. See what happens is that, okay, let me explain that in simple terms. This is best explained with a tanh or a sigmoid activation function. It's a very simple idea. See, look at the sigmoid activation function. It looks like this, right? So this is your y x is equal to minus x. Or in our term, let me just use the word z, right? Because it is z. So now what is z? Z is bias plus, for a given node it is bias plus w1 x1 plus w2 x2 etc etc right let me stay with two nodes for the time being right so you have a node here node here one two so here the output of this is let's say x1 the output of this is x2 and they feed into, let's say that this particular node here, and there are many nodes, but let's look at this one. So now Z here is what? It is the weight W1, W2, right? Now see what happens. If any one of these values are large, then Z will be large. Now look at this axis, This is your Z axis because this activation is working on Z. This is your Y of Z. So what happens if Z is here? Look at this. What is the gradient here? What is the slope here? Zero. Approximately zero. It's infinitesimal. So the learning is just creeping along. Right? It is not quite zero. So you learn a little bit every step, but you pretty weights are, let's assume that the weights are small. Then what happens? If your value is somewhere here, some small value here, at this point, what can you tell about the slope? It's a finite slope. Yeah, exactly. So these are pretty healthy slopes, right? These are not vanishing slopes. So, so neural net is not good for sparse data set. Neural? No, I said that's a different discussion. Why do you say that? Why do you bring that up here? Because if the data points are way, way too far, then it's just will not learn properly, right? Yes, parts data and neural networks don't quite go together. That's the point we made a little while ago. You can regularize it, except when you're using transfer learning. See transfer learning when you use it, it's a form of biation learning. You start with a very strong prior. Yes, sir. See, when you take ImageNet data, ImageNet, a model that has been trained on ImageNet, let's say, as we did, right? It has been trained on pictures and ImageNet. And let us say that you tweak and you train just the last mile. What are you really doing? You are starting with a prior saying that this complex deep neural network, I know I'm very close to the right answer in the parameter space of weights and biases. This is where we should be, somewhere around this region. And we are going to tweak only some last layer weights and balances. If you really think about it, transfer learning is saying that we are starting with a very good prior, very informative prior, and we are just doing a Bayesian a posteriori model that comes out of just the last layer, our last couple of layers. So if you do that, then small data sets are enough. In fact, that is why I started this workshop with transfer learning. Look at your data problem, ask yourself, is it similar to a problem that has been solved? Is it within reaching distance of a problem that has been solved? If you can map it to a solved problem and you say, well, this is an adaptation of that, then what can you do? You can take the model that already is there and you can essentially treat it as your bias and prior. All you're doing is treating it. So then even a small amount of data will work for you. But we call it, it data greedy algorithms. So even with a small data set in the end will produce good results. Isn't it data? Sanyam Bhutani. No. fine tuning a pre-trained model you are you don't need much data but then to pre-train it in the first place you needed a lot of data yes sir at some point a lot of data was fed into the neural network and a lot of training took place to to really find the minima in the parameter space where the loss minimizes. With that, now you're doing transfer learning. So that's our world today. If you train a neural network from scratch, you better have a lot of data. But the one trick is if you do have pre-trained models for a similar problem, not quite your problem, but a similar problem, then you can transfer the learned model. And even with small data, you're just doing a last mile training. Yeah. So, sir, small question here. So in the last layer, we are teaching, let's say we have only two target variables, or you said, let's say just cow and duck why do we take the prior we just train on cow and duck and just start from there that is so let's take the example of cows and ducks. Cow and duck you can't classify as images those are images there's a lot of information in there. So you need a pretty deep neural network to train it. So the question is, do you go and get a million cow images and a million duck images and train a neural network from scratch? Or do you take a neural network that has been trained, pre-trained on all sorts of animals, but maybe not on cows and ducks, but it has recognized, it knows how to recognize a truck, a house, a tree and so on and so forth. So it has learned to understand geometries of objects and therefore classify them. You want, do you want to take, it's called representation learning and that's one of the topics in the coming weeks. It has learned a lot, it just doesn't know cows and ducks. You want to take that model because then we don't need a lot of pictures of cows and ducks. You just need to do a last mile training. Last layer training. So the last couple of layer trainings at the head. So if the prior model doesn't have no information for cow and duck, so what is the usefulness of that? That's my thing. VASANT DATARANI- They recognize horses and zebras, and monkeys and whatnot. And that knowledge is not that far, ability to recognize a monkey or zebra. A zebra, for example, is not that far from a cow. Yeah, that is the point. Now I understand, sir. It's that problem is a hot dog and not a hot dog. Now I understand, sorry, yeah. Got it, thank you. All right, guys. So Nisargit answers your question about this, right? Yeah. In exact normalization, one of the things we ask is to what extent it regularizes or doesn't regularize. There are a lot of schools of thought actually. I mean, if you read the classic, everyone will say, yeah, it's a regularization. But actually, some people argue, no, it is actually causing instability. Unless you introduce residuals, it won't actually work for you and so forth. So there's a little bit of thinking going on but uh bottom line is should you batch normalize you should always batch normalize it's a good idea but whether you do it for regularization or not is a different question probably not realization you do it for you do batch normalization because it's a necessary thing to do you must do it yeah one second so guys remember that when I always tell you that in a 100 and 200 I've said whenever you do machine learning learn on standardized data scale data isn't it it is the same thing extended forward see each to each layer suppose Suppose this is layer one, layer two. What is the input to layer two? It is the output of layer one, right? If you think of layer two as its own model, then you would agree that it is a good idea to feed it normalized data. Just as input we normalize, we should normalize the input of each of the layers to do that is batch normalization yeah go ahead with the question please so in in this case like when you do the batch normalization and compare it with the normal feed forward network yes node in your hidden layer every node then need to have an activation function every node always has an activation function when you talked in your your previous session about the feed forward network the output node had an activation function not all the nodes in the hidden so i think so let me clarify that see every node with the definition of a node, def of a node is always this. You compute z and then you, then you distort z. You do some distortion function. So in other words, the output Y hat of every node is the activation of Z, where Z is the omega naught or bias or whatever you want to call it. The W1, X1, Wn, Xn. Thanks for playing. This is a gift. Asif, if we introduce, for batch normalization, if we introduce a large set of data again, will this affect the batch normalization? Because your standard deviations would shift and you're you're possibly you're new also. No, no. So your question is, if you increase the batch size to quite an extent. No, no, no. As if if we introduce extra data. Oh, like a large set of data. All right. See, the law of large numbers is there. What it means is that whenever you take the data forward, if the data has been sufficiently randomized, then you don't have that problem. Remember, you're using it to train the weights, right? have that problem. Remember you're using it to train the weights, right? During the training process you're passing mini batch after mini batch. What happens at the end of it when all your weights are fully trained? You freeze all the weights and everything and you just make predictions one data point at a time or one batch of data at a time. That's that. And that's that. All right, so that's for batch normalization. We, let's move forward. We'll come back to it again. This thing, today I introduced the concept. We will be coming back to it again and again. The next thing is early stop. So what happens is that if you look at data, if you look at the training process with early stop, it is the wisdom is that after a little while, see, it is like this. The training loss by, let's say the number of steps keeps on going down. Right. But so in other words, this is the training loss. You keep on training and it keeps going down and you observe that the test loss tends to go like this. Oh, this looks somewhat like your bias way and straight up, right? This is exactly that. You realize that this gap is your gap of overfitting. You don't want the model to learn from the noise because it's so flexible. First it learns from the information and gradually it has no more to learn from the noise because it's so flexible. First it learns from the information and gradually it has no more to learn from the information, no further information left, so it starts eating up the noise and starts fitting to the noise. So one of the strategies that you can use is you could have an early stop. could have an early stop. And what you can do is you can literally, so the way people do is early stop is, you can just take a few epochs and freeze the model, save the model. And if you want to run more epochs, you load the model back from a, let's say, persistent file, and then continue and run a few more epochs and you do that. The, the advantage of early stop is well of course you don't overfit you. That's a form of regularization preventing overfitting. The only problem is, when do you stop. Sajeevan G. And in a way it mixes two separate things, which is why some people frown on it. See what it does is when you are training the model, you should train, you should find out which features matter and what should be your neural architecture and so on and so forth. You should be busy with those considerations. And later on, you should worry about early stop in a second phase or something like that. Like you regularize, you do this, perform, improve the accuracy, etc. With early stop, what happens is you're sort of doing everything in one go. So people say that you break a concept called orthogonal concerns. First, you train a good model, build a good model. And next, you worry about when to stop or how many epochs to run it for. These are two separate concerns. So that is as it is. But early stop is a valid way of doing it. So how would you do that? Suppose you run it to one epoch, two epochs, three epochs, four epochs. You have to judiciously stop. So suppose here you suspect that the inflection point was here. At this moment, you can just save the model or stop the model, save the model, then see what sort of accuracy you get. Then try it on test data. You say, oh, test data, change it. Now it now here you do again again and after a little while you do one more epoch and you realize that you should have stopped here whatever model you saved here was your best model right model and the epoch i was your best model you don't want to go to this and it's time to abort the moment you see the gap increasing again the moment you see the gap increasing again. Are we together guys now on this? So early thought is simply just saying that I will run the machine at each epoch, but not really. At the moment I see the gap at each epoch, I'm looking at the gap between test and training loss. And the moment those losses begin to diverge from each other I stop. What was that term if violating the purity or principle of sticking to like the training data you said is the orthogonal something? principle of orthogonal concerns. See, this is a basic architecture thing. When you're doing something, do just that. Don't do two things at once. Do one thing, then go do another thing. That sort of thing um i have a question um on that is there a way we can just use the elbow method on the training data itself to figure out early stop no so look at this the white line is the is the training data and the loss keeps on decreasing. There is no... But what if at the optimal point there is an elbow instead? No, there isn't actually. Sort of. I mean, you could say that you reach an elbow, but the point is that you don't know whether that elbow has already over fit. You don't know that. So suppose your training loss falls like this and you stop here how do you know that this is where you should have stopped you have not already over it you don't so you just have to use i mean see quite often this will work but you're not 100 sure go back to the lab nice when i say all of these things uh you know, it is all in the labs. Now when you go back and review the labs, do you notice that your, the last lab, when we tried to fit a regression model, it looked like this. Does it look like this guys? With a lot of wobbles? You can see some of it there. Anyway, we'll do a lot of labs after this and you'll see the effect of all this. So this is early stock. Let's go back to our method. So we did ensemble, we did data augmentation, batch normalization, early stock. Now comes the interesting idea called dropout. I'll do the dropout and then we'll take a break and then we'll come to L1 and L2 after that. So guys, are we together so far? These are all, there's a rich variety of regularization techniques and so we are learning a lot. I would strongly suggest after this, read the textbook. The textbook has a chapter. It discusses regularization and gradient descent very well. One thing reading material from the textbook is read chapter four, which is gradient descent. Well, it is numerical computations, but it talks about gradient descent quite a bit, numerical computations. This I mean from which textbook? The main textbook, not the lab textbooks, the deep learning textbook. The first textbook on our course webpage, chapter four. And then I forget which chapter has it. One of you, if you have the book in front of you Just look it up and see where Chapter has regularization in it But do read the regularization from here to review the material 7 Chapter 7 Yes, so chapter 7 you can jump straight there and see try to understand that so now I will talk about what was I going to take one sorry one question on this graph and big comparison between the test test loss versus training loss so is it always that the um the test loss will there will be a point where it will start to go up and then it will keep going it will not like it will come down again yeah that is one saving grace that once you start losing it there is no test loss there's no getting better after a while okay because our assumption is based on that fact that it is once it starts to separate out yes all the training loss it's always going to go up exactly it does okay but that's a very good question that you asked nice so guys now we'll talk about dropout what in the world is dropout? What it means is, it is like playing the game of Cheshire Cat. Do you guys remember the story? I don't know if you know. There's a story for children and it always reminds me of that. It is Alice in Wonderland. So the whole idea is that Alice goes down a rabbit hole and she is in this magical and wondrous land but one of the disconcerting things is that there is this cat that seems to suddenly appear and start grinning at Alice and saying something with the other right and Alice sort of finds a disconcerting that it suddenly materialized out of nowhere and then it disappeared right so there is something of that quality to this dropout business what it says is that suppose you have a layer which has, I will just take eight nodes. One, two, three, four, five, six, seven, maybe eight. Because so that I can illustrate it. And let us say that you, that you're like the input data is flowing in to this layer. What you do is that for every once in a while, every step of the process, right? Whatever your step is, you just nuke some of the nodes. You for example, will go and first of all, you take these nodes out of the nodes you for example will go and first of all you take these notes out of the picture it's almost like these disappear now how many nodes do you see you see only six nodes so the data flows on to six nodes so what happens in reality is the nodes are there but you just switch them off you drop them from the learning you just just switched off. So whatever weights they're holding onto, they'll hold onto. But they will not contribute. They will not contribute to the forward and the backward, the process of the whole thing, the learning part. And then the next time around, you'll go and nuke some other nodes, these two, and bring those two guys back. And then suddenly you'll nuke this and that. And so randomly, you will keep dropping in some sense, nodes from the layer as the learning, as different steps of the learning take place. So it has the same effect. What it does is that, as you can imagine what essentially you the the last surface right it causes little changes in the last surface and those perturbations are good because they prevent you from getting trapped in local minimum so this technique is called dropout that is all the idea there is so dropout is good but remember that it introduces noise or fluctuations in the lost surface step to step you see fluctuating continuously right um and that fluctuation is good so imagine that you have a lost surface that continuously vibrates in some sense if it vibrates then if you are in a shallow minimum, what will it do? If you are a marble, it will just bounce out of that. That is the purpose of it. It just keeps tilting a little bit, fluctuating the lost surface. So that method is called dropout. So as if, does this work similar to the wisdom of the crowds, where by inactivating some nodes you create a weaker learner? Let me think. I know where you're coming from. I wouldn't map it to that. See, like for example, in random forest, randomly you pick a subset of the feature space. Isn't it? There's a little bit of an analogy to that. See, when you build a tree in a random forest, what do you do? At each moment, you don't span the entire feature space. You look at a subspace and you take only those features to learn. Now the output of all of these are like that. They are features. They are features or the inputs to the next layer. sees a random subspace of the so far as this layer is concerned layer l of the l minus one of the L minus one output space. You realize that if you nuke three of these nodes, then essentially three dimensions are gone. You're looking at a random subspace, right? And those random subspaces generally, what does it do? Remember random forests get their regularization from exactly that. You remember that Patrick from our ML 200, isn't it? Yes, I was right. So you have very similar effect here by using dropouts. The next layer is getting not the whole data. It's sort of being forced to get only a subset of the data. the subspace of the data is looking into and learning from so that's the value of it so that's that's what dropout does and guys we have been uh at it for about an hour do you guys want to continue and we finish the l1, L2, or would you like to take a 10 minute break? Yeah, we can continue. Let's finish. Let's finish it. Okay. So then as I apologize, today's session would be longer. By the way, we again have a choice at this moment. We can just summarize what we have learned. We learned quite a bit of that. And next time take the L1 and L2 regularization. That is possibility number one. I could cover the L1, L2 regularization on a Saturday extra session and then Monday we move on to the new topic. Or we just push it to, we either finish it right now, or the other is, we just move it to next Mondayay so any particular opinions or suggestions let's finish it today excellent so uh this one yeah uh i see one question sorry uh one question on this um dropout. So you follow, when do you draw, bring them back? So let's say I dropped out three nodes. The next step you bring them back and then you go and you then you go to other node. So is it that next step or next epoch? That's a good question. It's the next step. Next step, okay. The next step itself you so the thing is that your error surface fluctuates quite a bit and you get the same thing or in my view very similar effect as you get in random forest. Okay. You're looking into subspaces now. Random subspaces and looking taking random subspaces has been a very effective way to regularize or diminish this many many algorithms so now let's come to the word l1 l2 trump so let me give you the root of it see first of all of all, a very quick one. Suppose I have two points. Let's say that you are in a city. Suppose you are at this point, let me call this point X. And suppose you want to go to a point that is, let's say here, Y. This point. So what is the distance between so the question is what is the distance between X and Y how would you answer that these two points now you may be tempted to say that the distance is very simple I just cut across and that's my distance. Are we together? So you can say distance, intuitive definition of distance would be as the, shortest path as the bird flies. You would agree that that's a pretty good common sense definition of distance? Anybody guys? Yeah. Yeah. And how would you compute this distance? What happens is you would say, uh, why? So suppose X is made up of components, actually X one X two X one X two. These are the X and this is made up of y1 y2 you would agree that this distance would be square root of x1 minus y1 square plus x2 minus y2 square you know this is just a Pythagoras theorem namely but this part this much is x1 minus y1. And this part is X two minus y two or the other way around. Right. I should say, why, why actually want to write it correctly. And use a different color, just to emphasize this. So this distance is y1 minus x1 and this distance here is y2 minus x2. Would you agree guys that this is it, right? And so what does the Pythagoras theorem say? The hypotenuse, which is your notion of distance is this right hypotenuse square is the sum of side squares so this is often called the Euclidean distance so Euclidean distance is as the bird flies. But then if you are in a city and you are a Uber driver or a taxi driver, you would beg to differ. You say, well, you know, my petrol or the gas that I end up using and the time that I spend clearly disagrees with this definition because the best that I can do is I can take this part from here to here. This is one part, or I could take like, uh, I could take intermediate parts. I could take this part, go up and then go here. Or I could take different parts. You can imagine, or I could just go up and go here right or any one of these parts if I take they are all of the same length and the taxi driver's distance taxi driver distance what will be his distance it would be the absolute value of the size of this distance and this distance right it will be y1 minus x1 the absolute value you know the actual size of the base less actual size of the side first you go east uh you go east and then you go north and whatever in absolute terms whatever those things came to that would be the distance so that will be the taxi drivers distance would you agree guys right yes distance is very practical you know your odometer will say that your gas cost will say that everything will say that why because the taxi drivers are usually not allowed to rush through the buildings so this distance is uh because manhattan was i suppose one of the first cities laid on a grid this is called the manhattan distance now what has this to do with regularization? It turns out it's quite interesting. It has to do with one thing. And let's connect the dots to regularization. We will ask what is a circle? What is a circle? So one definition of circle that you would agree is, let's say that you're sitting around, you're making a circle around origin. It is the set of all points at unit. So what is a unit circle, which is say, right? So unit distance from origin, right? So in other words, this is zero, zero, and then you can draw, for example, uh, so you find all the points that are at unit distance from the origin, right? So then you then something gets very very interesting very interesting actually what happens is that when you're looking at euclidean this is what you're familiar with you are at this distance and you can fly anywhere the bird flies and your circle looks like this this has been one unit does this make sense guys So in Euclidean with Euclidean distance, this is your unit circle. Are we together guys? Any doubt? This is hopefully a very obvious statement. So this doesn't look very remarkable, but if you take Manhattan distance and you say all points that are at unit distance, now the thing begins to get more interesting. So here's the thing. Four units. So you could, the taxi driver can go one unit here or the taxi driver can go a three fourth of a unit and one fourth unit up or the taxi driver can go half a unit and half a unit or taxi driver can go oneth of a unit and one-fourth unit up or the taxi driver can go half a unit and half a unit or taxi driver can go one-fourth of a unit and three-fourth of a unit up or the taxi driver can go one whole unit up right this is one this is one if you connect these points what you get is this and so by symmetry you realize that you're looking at, and this, you just defined your circle to be the set of all points at unit distance from origin. So you say with Manhattan distance, distance, your circle looks, your circle looks like a diamond. So to make it very practical, suppose you have one gallon of gas. How far can you go and suppose your car can give you exactly 20 miles a gallon let us say you would agree that this would be the set of all points the outermost set of points that you can reach with your one gallon of gas if you're a taxi driver isn't it so this is it so now it turns out that you say well that is very. Your unit circle shape depends upon your definition of distance. So it turns out that the guy who first generalized this idea from Euclidean to this was a guy named, a wonderful mathematician named Minkowski. And so named after him is the concept of Minkowski norm. He says that the distance between two points x and y based on the norm, the n is the norm, is equal to, what you do is you take the gap between x1 minus y1. I'll just take two dimensions for the time being. To the power n plus x2 minus y2 to the power n. And you can generalize it to higher dimensions. This and the entire thing to the 1 nth power. So now let's see, does it work out to the Manhattan and this thing? What happens when you take two? When n is equal to one, what does it become? It becomes d becomes d one becomes x one minus y one plus x two minus y2. This is the same as your Manhattan. Isn't it guys? Now, on the other hand, when you take d is equal to 2, n is equal to 2, the second notion of distance, becomes x1 minus y1 square plus x2 minus y2 square the entire thing square root right one one over two power now you realize that when you take even powers you don't need to take mod you can just take the difference and square it it will automatically be a positive quantity in detail so that is it. Now, what has that to do with the last function? Before I get there, we are very close to the final connecting all the dots. I want to make sure that we understand what the Minkowski norm is and the generalization of the notion of distance to an arbitrary norm. Are we together guys? Yeah. We are together. So now let's connect the dots. And here I will state something without a proof, but I just stated because we don't have that much time and also because for many of you we have given entire sessions to this now let's bring up the idea of the regular l1 and l2 regularization loss by foot. Sorry, you sound is it just me? So what we do here is quite interesting actually. You say that your error function, remember we said that the error function, the loss function, the error function used to be like this, the error surface. This is sum squared error. Y minus Y hat squared summed over I. And if you need, you can even average it. Mean sum squared error. That's what we did in lab one so what do you do you look for the minima here isn't it are we together but suppose this is your XY axis this is your absolute minima this is the origin now I set a budget. I tell you that you cannot go more than a certain distance. I set a budget. You say you can go only this far. Now remember, what are the axes here, guys? This is your parameter space. This is the space of your, let's say that you have a parameter here. Some weight, some parameter is like this, and this is the error function. Why? Because this can be written as yi minus bias minus w, w dot x, let's say in single dimension, now, X, XI and YI are constant. So this error function is actually a functional W. Right, I'm gobbling up the summation and so forth. Actually, why gobble it up? Let's write it down. So you realize that see when you're learning the error function depends on the weight. What what is the parameter value and I'm just keeping BS constant for the time being. I want to show things in one dimension. So Guys, I'll be together. This is an equation. The only thing you can play around is go to different points in the parameter space and see what the error value is. What the total error is. And you're searching for the best point. So typically if you are minimizing, suppose you start here, you will, you will ultimately gravitate to this. Makes sense guys? Guys, am I making sense? Yes. So you would gravitate up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up I say, you know what, you can pick any value here etc etc so then what would happen then something really interesting would happen the lowest that you can achieve is this point isn't it this is i will put w today here and isn't it right because the lowest loss that you get within the region that you can travel is at this point at any other value the error is more am i making sense yes i say you cannot go too far from the origin. I've given you a budget. This is the limit of how, this is the distance that you can go from the origin. But then you have to find minima, whatever the lowest value is within this region. So now you're not looking for the absolute minimum of the error. You're just comparing this value with other values of the errors, right? Other values here. So your gradient descent will stop here. At this point, you'll run out of budget. Isn't it, guys? You've run out. If you're a taxi driver, you have quite literally run out of gas. So when you do that, and I'll state this without the mathematics, the proof, et cetera, behind it, what you say is you have actually regularized the problem. You have prevented overfitting. So what happens here is this point leads to overfitting in complex models the absolute minima absolute minima is overfitting whereas when you constrained it so this problem is called problem of constrained optimization. Constraint means you have constrained the search space to a finite amount from the around the origin optimization. And there is quite a bit of theory about constraint optimization. And I would just explain that in the next few minutes and you'll see it it's very elegant now everything that I'm going to say that I explained in one dimension it can be generalized to higher and I'll show it to you in two dimensions for a moment but this is your best solution this is your best you should you stop here and it turns out that if you stop here you will you will have reduced overfitting to a large extent now the whole question is how in the world did that happen and there's a very interesting clue to it the reason it happens again i'll state it let's peel the onion a little bit and see what is going on see what is going on. What happens is that when you have overfitting your weights, weights or parameters, it could be a bias, your parameters become large. When the parameters become large, it's a classic sign that you have overfit. So at this point, if you look at it, you're too far from the origin. Then you have overfit. On the other hand, so one easy way, so this is overfit. Now why that happens, we'll sort of skip it over. So if you force the parameters to remain small within a budget, you therefore do constraint optimization, you regularize the problem. Now there is a, let's look at it in two dimensions and what it looks like. See when you look at this in two dimensions, suppose this is your b and w, you know, suppose you're writing y is equal to y hat is equal to b plus, right? Think of it, the two parameter space and here is your error surface. What happens is that somewhere there will be a minima, right? And Right. And let me draw the contour lines here. Three contour lines. What happens is that there will be some point, let me just call it W star, right, which is made up of some combination of, let me just use the vector space, some point, a parameter value, which is a combination of the best b and the best w this point around this point if i project onto this plane if i project these error surfaces you will find that these lines are there right these are your contour lines these equal contour lines. These are called contour lines. What are contour lines? It's the shadows on the plane of equal error surfaces. You agree that if this is the error axis, all the points on, let's say this pink line, they all have the same error because they are the same height from the from the plane so when you project it down it will become this not quite accurate but okay it will project down to this right so these lines which are contour lines it comes from geology right but they always show maps or hills using contour lines so we get contour lines for the error surface. And we can find the point of the absolute minimum. But then comes something very interesting. If you constrain the budget, so what happens is we can keep drawing the constraint line like that. But if your budget is fixed, let me put the budget in another color green if your budget is fixed in some way whether it's a let me just take a circular budget here i'm trying to draw the circle in that plane the bw plane let's say that this is your budget and you have to find the minima here somewhere in here so what is the lowest point that you can find would you agree that the lowest point is here for the error right let us draw it out in a in a piece of in a two-dimensional plane and this becomes obvious what happens is that suppose you have a circle and you have this point. These contour lines are here. And as you go out there and out there, they represent greater error. So let me just say A, B, C, D. So the error of A is greater than error of B is greater than error of C is greater than error of D is greater than the best error, which you have E. This is the smallest error. But any line that you take, let's say A, let me take this thing as Q. EQ is greater than this. So what is the best you can do, guys? In this space, within this circle, what is the point of lowest error it is this point isn't it and this is the best solution that you want in the parameter space whatever the value of b here is let me put a tilde here and a w here these are your best values let's say that this is B and this is W and this one which is the absolute minima B star W star you don't take that that leads to wild oscillations it leads to overfitting you instead pick this solution now there is a little bit of a elegant mathematics to that I'll just mention it in the next few minutes. This is only for those who have taken courses with me or who would like to be mathematically interested. See what happens is that this is a constrained surface. What is this? This is a surface whose equation, so suppose you're looking at Euclidean, it is b square plus w square is equal to some constant g g as a function of circle can be written as g b w is equal to this is equal to some constant some value c is your budget or c square if you want to think that c square is your budget, right? Radius is c. Now, the last surface, the error surface is whatever it is. At this point, you would agree that these two lines are tangentially touching, right? So the perpendicular vector to these two, they will be pointing in opposite directions. Would you agree? The point at which the the budget surface touches the error surface just glancingly touches. Then the perpendicular vectors to each of these two surfaces will be pointing opposite to each other. The direction in which they are growing. So how do you grow this constraint surface? By taking a bigger surface. How do you get more error? Go from inner to outer. So this would be, and there's a notation for this, this you call the gradient of error. And this direction is the gradient of G, the constraint surface. So you say that gradient of error is actually a negative of the gradient of g and what is gradient i'll come to in a moment up to some proportionality factor some lambda you don't know all you know is they are opposite to each other but you don't know what the proportionality factor is and you come to this beautiful result and And by the way, this reasoning was argued not just in machine learning. The first person, the mathematician who argued, but doing optimization with constraints was a guy named Lagrange, a mathematician, a famous mathematician named Lagrange. And this lambda is named in his honor. It is called the Lagrange multiplier. Lagrange multiplier. Lagrange multiplier. And if you remember way, way back when you did college engineering, you must have learned about multivariate constraint optimization and Lagrange multiplier, but I've essentially reproduced it here. Now let's see what it means. The error was reproduce it here. Now let's see what it means. The error was y minus y hat square, the usual summations and then if you want to put a 1 over n you can put a 1 over n which serves no purpose. I'll just skip it. Now if you look at this equation it says grad of E is equal to minus lambda grad of g by the way this grad is generalization of a derivative derivative or slope gradient like gradient and the reason we call gradient descent is actually because of this inverted triangle symbol which is the gradient symbol grad symbol del symbol so now this equation can be written furthermore from this thing we conclude that we can write it as gradient of e plus lambda g is equal to zero in other words we can think that we are minimizing this when the derivative vanishes, we have minimized this, right? Minimizing this. So the word that you use is, it is your loss function. Loss function, L typically written in the calligraphic L, right? Is is equal to which is a function of the parameters let's say B and W here is actually e plus lambda G where this term is obviously some square let's say for regression and this is applicable to classification also it doesn't matter what it, but let me just write it as y minus y hat square plus Lambda. Now what was G? G is the equation of a circle in the case of. B squared plus W squared, so people often write B as W not right, so I'll just write it as W not squared W1 squared and what happens if there are more terms it's higher dimensions because w2 wk whatever the number dimensionality of the data is there and so you're saying that it is error term the initial error term your e term i'll just write it back plus r let me write it explicitly y minus y hat square plus lambda summation over all the possible some other index so this is i this is the data index but this is a k um suppose it is p dimensional so k w k square up to p suppose there are p dimensions so that's that's that does it make sense guys are we together okay and this is a bit of a mechanical uh thing this is your last surface now we we took a circle which was euclidean in the case of a Manhattan circle, remember your constraint surfaces, x, like the absolute value should be constant, wi. Let me write it in the simpler term because we use b. The absolute value of b plus w should be some constant, right? Which means that if I write it in the new notation of w naught plus w1 plus w2 generalizing it etc to wk is some constant and so your regression this loss function therefore becomes function therefore becomes last function for L1 norm that is when you take Manhattan distance is your standard error term by the way in the case of classification it wouldn't be the sum squared error but whatever it is y hat squared summed over. And if you want to, by the way, divide by n, go ahead and divide by n, but I'll skip it because it doesn't affect you. Then times lambda in the case of this, now this you can write it as sum over k, absolute value of k, right? Up to p times, this is k up to P times this is so this was a mathematical digression again if you are not up to the if you haven't been with me for a while you might find this a bit rushed it doesn't quite matter just remember that L1 and L2 regularization their main goal is to make sure so so their main goal is to make sure that the terms don't blow up the the weights the parameters don't go blow up like l1 and two goal keep wi all the wi small are we together and when you do that, it achieves regularization. Are we together guys? Yes. And that is the famous L1, L2 regularization. So I will attribute this L1, L2, like if you are completely new to this, it may feel too much geometry and too much mathematics, but you can sort of short circuit and take a flight over all the mathematics and reach the final statement. The final statement is L1, L2. Their goal is to keep the weights small and they managed to achieve it using some very interesting arguments, mathematical arguments of why it works by fixing a budget. And so they introduced their own hyperparameters into the model. How much is your budget how big are you willing to see the size of w grow to what extent can the weights and biases grow you have to set the budget that's a hyper parameter in the model all right guys so we'll come back today we were a bit ambitious i wanted to cover regularization and i also wanted to cover gradient descent and show you guys i mean not gradient descent back descent and show you guys, I mean, not gradient descent, back propagation and show you loss surface. I would like to still show you loss surface in the next 15 minutes, but let us summarize where we are. So we covered L1 and L2 also. We covered six or seven methods, let's say, of regularizing and come up with more of your own if you can think of something. Contribute to the field. So here we are. These are all methods to regularize a model. Regularization is to prevent overfitting. It is to prevent a model from just simply memorizing the data. If you don't regularize, if you overfit, you lose the ability to generalize. And losing the ability to generalize means losing any learning whatsoever. Because learning is the ability to generalize from samples. Are together guys so I will review quickly what all we learned we learned that one of the easiest ways you can regularize is just get more data if you get more data like you see in the green line automatically you have you get regularization so more data regularizes your system you can get more data by just going to the business people and saying, hey, I need more data. Or if you can't, then you can start slicing, taking pieces, rotating, doing all sorts of augmentation techniques based on what is it you're dealing with, text images, tabular data, right? So for example, if you have a lot of missing values and you threw away those rows initially, one good way would be to just go and impute values, interpolate, do something, find the values, predict the values, interpolate it, whatever. Impute some value and now you can salvage those rows, you have added more data, you can also do some amount of interpolation, like for example SMODE does class imbalance, it does by interpolation, so some classes areE does class imbalance it does by interpolation so some classes are very few and so it goes around those data points in the feature space where those the classes which have very few instances are there and just goes and interpolates more points there it comes with its risk of course because interpolation assumes that it truly the in-between region is what you think it is it can be interpolated into it may or may not be true so well anyway those are methods of data interpolation data augmentation that is one the second way we learned about and we learned about this fact that you always assume that your data comes from the same generative force underneath it. And that you have taken one data, one observation is independent of the other observation. This is important. They should not be autocorrelated or things like that, leaving forecasting aside. They shouldn't be. And so you make fundamental assumptions of independent and identically distributed. IID, it's a very fancy term, you'll see it all over the textbook. All it means is that in common sense term, you get bits of data, observations you take, and you take it from the same generative force that producing the data. You don't just hop around to different things. That's that. So when you do that, and of course one easy way of doing that is when you capture a lot of observation, just remember to shuffle it up. Right, so that is that. Now what is a node? A node takes, first of all, computes. Remember, we talked about it using resistors lots of resistors these are your weights w1 w2 for input x1 x2 and then you produce out of this occurrence z and that goes through the distortion function right so a node is both that bag of resistors and your a node is both that bag of resistors and your distortion function or more formally you say that node is first the linear combination of the inputs with the biostem and then so that is all of it put together is a fine transformation and then comes a distortion function or an activation function applied to that if i'm transformation to that Z. Sajeevan G. You get that. So that's something to remember now early stopping approach. It just says that after a little while. Just go and check that stop the training. Just go check how well your model is performing against test data. Sajeevan G. And if you start seeing a divergence between test error and training error pretty much you get a clue that it's time to stop. You stop there. Right? So that's that. The next way we learned is the dropout. Dropout is a little bit like Cheshire Cat. It's disconcerting. Suddenly a cat appears and disappears. Well, here the cat are the nodes. And what you do is is these nodes they tend to show up and then they tend to disappear a small fraction of the nodes randomly they appear disappear what it does is as far as the next layer is concerned in a way it's showing it a subspace of the data right so that has its own benefit we have talked about it in the previous workshops why taking random subspaces is a good deal. Then we talked about L1 and L2 regularization. That is the heaviest of them all. Oh, somewhere in there we talked about the batch size, mini batch size and how it affects. Then L1, L2, I think, where did we talk about the mini batch size did i write it here somewhere i hope i did i talked about the batch size here right so uh the thing is don't take very large batch sizes otherwise you'll start having overfitting because you do want your error surface to fluctuate your loss function loss uh not the loss surface to keep fluctuating reasonably. Otherwise you'll get trapped in local minima. Next we come to L1 and L2. L1 and L2 basically says that you don't want the absolute minima of the loss surface. You want to have the constrained minima, means a minima that is within the bounds where the weights are all within the bounds of a certain distance from the origin. In other words, none of the weights can exceed a certain size, right, or some total size is fixed using a distance measure. In the weight space, you can go only that far from the origin in search of the weight now the question is how far far is defined by Euclidean or Manhattan distance and that leads to L1 L2 norm L1 is Manhattan L2 is Euclidean as we just saw yeah so that is that So that leads to some bit of interesting geometry in the parameter space when we project down the loss, the contour surfaces, we see that we end up with a circle of certain size and the circle is a circle in quotes based on whether it's a diamond for L1 and it's a true circle for L2. So based on that, it will touch upon the constraints of the error surface. And when it touches upon the error surface, wherever it touches upon certain geometric conditions will be true. Namely the orthogonal, the orthonormal vectors will be pointing in opposite directions. And a consequence of that is, so you will have a condition like this, true. That when you refactor, what does it mean in terms of your error and your constraint equation, like for example this, it begins to spill out the derivation of your ridge and lasso regression. So L1, L2 are also called lasso and ridge regression. But in machine learning, in the deep learning community, you just call them L1 and L2. So what L1 and L2 do is they prevent your weights from getting blown up. And that summarizes all the, all the regularization techniques you will encounter. Well, maybe I missed some. If you guys encounter any, you can do that. Now we have five minutes. I would like to take the five minutes to show you a particular, or not show you, but point you to this, to certain visualizations of the lost surface. See, this lost surface are very high dimensional spaces, but if you project it down to two dimensions, how do you project it down to two dimensions how do you project it down to two dimensions you take random randomly you take some weight and some other weight and then you ignore the other all other weights and you just compute the loss at that point with respect to those two weights right so the loss would again be a y-axis, sort of the y-axis in some sense in this case. You end up with a situation like this. If I randomly pick a weight, another weight, and I look at the air surface, of course I'm looking at a two-dimensional surface. And you can visualize it. There are mathematical arguments to show that this preserves the information of the much higher dimension space and the visualizations are not misleading they're actually quite informative i would like to show that a bit to you today let me bring that here Let me bring that here. Let me see if I'm sharing the computer sound. Share computer sound. So if you go here in the workshop, you'll see that today for review, I have added a section here called, what is it called? Last function and gradient descent. So guys, your homework at this moment for today or tomorrow is you see there are three videos here one gradient descent back propagation there are two videos uh please go uh watch those videos they are 10 15 minute videos 10 minutes each or something like that watch these three videos hang on let me increase the font size. Guys, are we able to see this on my screen? Yes, very nicely. So they are all by a group of people who call themselves three blue and one brown. And they put in amazing amount of effort in visualization and in very clearly explaining the mathematical concepts. In many ways just what Khan Academy is to or what Salman Khan is to school children and to early college, three blue one brown is to you know it's a continuation to the next level to a little bit more advanced topics and they do a great job so look into this and then there's a art there's an article that you can read which is there in the this loss and the blessings of dimensionality read this article and there is a paper so this will give you an idea but I would I will just go through it and show you things that I'm talking about this is your gradient descent how do you come down to a minima now this this article is written in a very interesting way. It almost goes in dark, starts by asking what is the meaning of life or something like that. Very nicely done, but in a bit idiosyncratic manner. It says all life is movement and so forth. So you can get a sense of how it starts. So you can see here guys, do you see the, as you can imagine, you don't see the minima, but you can see that big valley that is forming right here. And you can imagine there's a minima down in there. And this is another point I wanted to make. Do you notice that this place is filled with ridges? If you look at this picture a little bit carefully, do you realize that there are local minimas, but not deep local minimas in which you can get trapped? You have a lot of ridges here, lots of ridges. So next time you are hiking in the regional hills in Fremont and Dublin, etc. Observe how often you meet ridges and ravines and so forth. You'll realize that quite often you have that. Now, there are excellent visualizations. I would like to show you this visualization. So it is the same thing. Do you see guys, it's looking into the absolute minimum and it is showing you It almost looks like the hills around the house around around the Bay Area isn't it if you look a little bit But except for this deep deep deep valley Where you want your gradient descent to take you where you throw the sacrifice so and this is how you know you can see the descent happening to a deep minima now one of the things that comes is what if there are multiple deep minima's in your loss function, you know that the loss function in neural networks is not convex obviously, you see a lot of local minima's, but what if there are lots of deep minima's. Now I argued that actually the higher dimensional spaces are much more infested with saddle points, but that doesn't preclude there being many, few not one but a few but deep like local minima is deep enough that it can trap you that has been a question that has been puzzling people until a body of research came out recently and you showed that you know what all those deep minima they're interconnected to each other. In an interesting way, there is a sort of a trail from one to the other and so forth. And what you need is, you need to pick any one of those minimas because they will all give you essentially as good predictions. This is a fairly recent sort of observation that researchers have made in this space. And I don't think there's a lot of general awareness about them. So it also sets mood, this whole worry that what if I get trapped in local minima. First, there are not that many deep local minimas and even if there are, you do hit upon a few. You just have to get into one deep enough minima because that will be your effective model. It will work. So do watch the videos. If you go to this website, you will find a lot of, they have an entire website. These people who did it, they have an entire website. And those of you who are interested in the paper, here is this paper tentatively, I've marked it as the paper that we'll walk through on Sunday. So this will sort of deepen the conversations we have had on the last surface. It's an archive paper, so usually it's a little hard to understand, but I'm hoping that after the discussions that we have had now it won't be that hard to understand. So read that article which is given in data science and then the discussions we have had in the class or do that and on Sunday we'll take this as a reading. And I've mentioned it in the email in this. So this is your reading for Sunday. And I've mentioned it in the email in this. So this is your reading for Sunday afternoon, the paper reading that we'll do this week. The hope is guys that as we move out with the fundamentals, hopefully once and for all, you understand loss surfaces and the gradient descent. And hopefully you got, if you have been with me for ML 100, 200, hopefully now you have finally like every aspect of regularization is clear, mostly clear. And if you're completely new to this terminology, you at least got some sense of it, of what regularization does. In simple terms, it prevents overfitting. does it in simple terms it prevents overfitting you can take a somewhat complex model a very complex model and you can try to beat it down beat down the oscillations dampen it down to a much better behaved behave model right that's the point of it so you do that'll do that. So guys, with that, I will stop. We did not do back propagation. Let us keep it for next Monday. I will end with that and I'll open it up to questions. Thank you.