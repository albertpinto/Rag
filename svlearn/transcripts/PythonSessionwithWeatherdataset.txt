 Guys, you are all aware that all our lectures are being live broadcast on YouTube. So if you want to review it, those live recordings are there on the YouTube immediately. Later on, I replace them with cleaner versions of the recording, more edited versions. But if you want to immediately access them, all of the recordings are there on the website oh no sorry on the youtube channel support vectors so with that i will i think it is one more minute to go we should I'll start the recording now guys. Does anybody have a question from yesterday's session before we start? But one of you confirm that you can still hear me? Yes. Yes, we can hear you. Excellent. So what I'm going to do today is use, today's topic will be pandas. In data science, we use pandas a lot and NumPy, you know, making arrays etc. As we make progress with data, I'll explain some basic operations that you should do with data, clean it out and so on and so forth. There is also the concept of tidy data. Tidy data is a concept that says if you can put data in a long format, then any kind of manipulation you want to do with the data becomes easy. There is a very close analogy between this and SQL. In SQL, if a data can be represented as a table, then writing and even there, if it is in a good format, then doing joins, doing filters and picking a few columns and aggregates and this and that in group wise, you can achieve a lot with a very succinct syntax which explains why SQL has been popular now almost for 40 more than 40 years I don't remember the exact amount but it's almost four decades since it's been around a similar thing is true for the data frame when we create a data frame in pandas in Python and pandas is obviously the default way we do it at scale we do it as a data frame in spark let us say or in flank or something like that if we are dealing with streaming data but today we are not going to focus on streaming data or a large data set we're going to focus on Pandas. So in Pandas, when you have a data frame, and by now we're all familiar with what a data frame looks like. It is rows of data. Each row has multiple attributes. Then we can actually do any kind of data manipulation with a very small, what people call the grammar of data manipulation tidy data and its vocabulary or its language for manipulating the data it's a very small vocabulary it contains as you will see if you understand that vocabulary any data that you encounter you can you can go through a systematic process of canonizing it standardizing it manipulating it getting what you want out of it today we are going to focus on that now those of you who have taken ml100 you will find that in the tabular data chapter there was a whole section on on this perhaps i should just start by referring you all to that chapter and then we'll go into the code and walk you today so far explaining the concept of tabular data so if you go to where do we go to support vectors website I say, are you sharing your screen or I know I didn't start, but it's a good time to start. Okay. Let me start sharing. So I am now sharing my screen. So if we go to supportlectures.com and the ML100 that you guys took, the very first link here is obviously one is the Slack channel. The very first, and by the way, those of you who took that know that all your YouTube videos are here. So it's a hopefully a good reference for all of you. You can go back. How many videos are there? Let's see 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17. Yeah, about 17 videos are there that may help you review the topics that we talk about. But there is this book. Asif, it is still not, I'm not able to see the sharing. Oh, you're not able to see the sharing. Quite interesting. Why is that so? Let me see what's going on. Yes, me too. I'm not able to see the screen. Tell me if you have better luck. Yeah. Now it's better. See, there is this page, those of you who took ML 100 with me recently, you remember that all our notes are here. Just a refresh here. This is the textbook for ML 100. All my lab works are here. We'll go through this again in a moment. It will remind you. Then we missed the first lecture. If you remember, the first lecture was just getting set up and orientation. So so I didn't record it, but thereafter all the other lectures are here. For example, this was in the pre-COVID world where we were holding the lecture in class and right after all the future lectures are, as you can can see we do it remotely or we do it virtually so all of these lectures are here there are about 17 of these so in ml100 we had 17 lecture sessions and i intend to create another page today maybe not today actually sometime in the coming week for ml 200 so that you'll have one page in which you can go and access all the lectures that we have had on the certain particular parts of that for mt but here i'll refer and this is obviously with that so the three kinds of data we deal with are So the three kinds of data we deal with are tabular data, that is a data frame, which is the context for today. Network data, we touched upon network data in ML100, which is of growing importance these days. LinkedIn, for example, is a professional network. Facebook social network and so on and so forth. Then this was the book that we... There's a few who were... Are you familiar with this? Does this look familiar guys? Is this familiar? It's not. Yeah, it's looking familiar. So this is straight from your, and this is shared with all of you. Just in the spring. Here I will go in particular to this concept of tidy data. Remember, I'll just give you a review of tidy data and those of you who did not attend this summer, it would still be useful. But before I do this, let me take you guys through something that you'll find very useful. In fact, since I'm talking about this course, Let me go to this helpful resources. This contains tips, cheat sheets, documentation, glossary, bibliography. Glossary is somewhat sparse but okay let's go through this. It may be useful here. So first of all you know basic tips on plotting. A principle that you should use the least in that you should use ggplot. And how do you embed latex fonts or latex into your... We covered this last time. How can you have a plot in which you can do fancy animations, you can have a production quality plot. So these are tips that you'll find in this book. How do you create a multi-plot, a grid of plots? So there are sample code here to make a grid of plots. It's very easy. And oh, by the way, this should be in pasting it here, the formatting should be in here too. So that's that. The next thing I do is our helpful cheat sheets. See, we have and use a large ecosystem of life. Data science world is data dependent. You will create that other people will use. Your code, your analysis will be useful to somebody. At the same time, you take help from a lot of resources that are available. So this is a collection of cheat sheets. This is for the R library, those of you who are doing it. And I find that these cheat sheets are actually very good. For example, ggplot. If you click on this thing, you will see you get this very detailed . Is there somebody asking a question? I missed that. So this is a, anyway, I find it very useful. So one of the things I do is I traditionally would print it out on long format paper and paste it on the wall. And just, you know, as when you're doing this, it is quite convenient actually just to be looking around and seeing it there. One trip that somebody used to do which also is very good, what they would do is they would put it on the table, all these cheat sheets, different cheat sheets for all the things they use, and then they would do something interesting. They would put a glass on it, a large sheet of glass or a slab of glass so that they could work on their table and literally they don't have to look far for hints for the reference the references are literally staring them under their nose and i found that to be very useful in fact i'm meaning to do that but it's very useful see you will know all these things but are they available on the tip of your finger. That's what this cheat sheets do they make it available. They're not ways to learn it. But once you have learned it, you can quickly refer to It like for example, this is data manipulation. This is what we'll talk about today, not in our but in Python, but you can clearly see from the picture what it is doing. This is filtering the data, looking for distinct values, so on and so forth. So these cheat sheets are very useful. Anyway, my personal tip would be, pin them out or use whatever you have a younger generation, maybe you never print things. So do as you please. So this is the Pandas reference. so do as you please so this is the pandas reference will pay some attention to it today that's a topic are you guys able to see this one does reference on the screen guys is it visible yes it is visible so one of the things we will deal with today is it much things that you find in this sheet. We will use most of it today. Importantly, we will talk about the reshaping of data. When data comes and it is in a shape that we don't like, how do we make it into a tidy data format? It is the foundation, as very well it says in this cheat sheet, a foundation for rambling in pandas. Tidy data was created by Wickham in R actually, in a very influential paper he wrote on how data should be manipulated. Before that, people were achieving the results in all sorts of haphazard manner. Manipulating it, hacking it, and so forth. When he wrote his work on tidy data, it's short, and he came out with the deep liar library, which is hugely influential in art and influenced founders quite a bit, then obviously you can see, you can tell people who are used to tidy data and people who are not. The people who are used to tidy data, you can very clearly see their data analysis. It is very ordered, very structured and very exactly what you would expect. And in a few steps they manage to massage a wrangle with the data into the form that they want to put it in. It's a very short journey so it is worth knowing. So today from a Python and Pandas perspective this is what we will do. So how do we shape the data? Let us go through all these steps then's data summarization we'll talk about. Basic summarization of data is what count the values in a given column. So for example, if you have one of the columns is cities, you would want to know how many cities are involved. It's very simple. You do values count. A question that keeps coming up is what is the length of rows in a data frame? So quite often I see people do a dataframe.shape and you pick the then that shape is a well it's a list like rows comma columns and then you pick the 0 through. That is also true, that works, but a shorter syntax is literally to ask for the length of the data frame, it will tell you. If you want to find how many distinct values are there in a column, once again, you can do that. This is very common, like, what's the difference between unique and value count? Unique will just tell you how many cities are there. Value-bound will tell you for each cities, how many sort of rows are present, right? So you can say Chicago, there's 200 rows present and for San Francisco, 50 rows are present and so forth. A summation, again, summation min max mean variance standard deviation median quanta. These are standard descriptive statistics. So you can do that quite literally as you can see in a direct way. It supports that. Now you have the apply method. If you want to do a transformation of a row into something else or of a value into something else, apply is sort of your Swiss knife for doing it. Very, very powerful. It can do a lot of things for you. Then some of the things we have been encountering are a drop not available. So you want to remember that if you are rich with data, you don't mind dropping a few rows where data is incomplete or some column values are not present. You can do drop a bit. Excuse me today, I woke up a bit late, I'm still yawning. Okay, so a fill NA will fill missing values with some fixed value. and then there is a whole technology of data imputation, right? If you don't want to just fill it with a constant value, what you want to do. There's many imputation techniques which are more powerful, the limit is that. How do you make a new column out of data? This is one of the first things people ask. Suppose you have data, in our case, let's say in the California housing data, latitude, longitude, and from latitude, longitude, somehow you want to find distance to San Francisco or distance to Los Angeles. How would you do that? And the answer to that is, let's say that you have some computation that you can do. That would be, you can do it and then now you can create a new column. Or here is a simple example. Actually, I like this example much better than what I said. Suppose you want to, you have a column called a length and a column called a height in your data frame. So you see this column and you want to add another column, this green column, which is the area, right? Area of let's say some, something. So wall, let us say length and height of a wall. And then what do you do? And you can use a syntax like this. You can create a new column, lambda d data frame, such that length times height. For each element of the data frame, you do this. You could do it in a different way, which is sort of an implicit way. You could just say, you could do it this way. And you have seen me do it this way, the second way. And the other thing you can do is you can bin data into N buckets. So what it, so basically it will make it discretize the data and so forth. Next, there are many things min, max, the max of the row, and so on and so forth. I won't go through every one of them. Let's combine data sets. Suppose you have two data sets. What can you do? You can, well, you can merge. Let's say that ADF, R data frame and B data frame are X1, X2 and this is X1, X3. One of the things you can do is you can join the standard SQL join. You can join based on the column. The common column is X1 here. If you observe carefully, X1 is the common column. You can join on that right when you do join there is a concept of inner join outer join and when you do outer join it can be a left outer join or right outer join those of you who are familiar with data manipulation probably know it but i'll just iterate it. The inner join will keep only those rows of the data which are present. Like if you take a key, a join column, a key, these are called the join keys. So this X1 is common to both. So you notice that ABC ABD. So what are the two common rows a be the first in the second row so if you do a inner join here you notice that the first and the second row are kept if you do an outer join full outer join then it doesn't matter it will keep rows from both the table so it would mean ABC ABD put together would become ABCD so you see ABCD right now the problem is that this one doesn't have an x2 value for D and this table doesn't have a value x3 value for c so those would be replaced with nads not a number like n is not a number but you could choose to say keep the ones on the left must preserve the ones on the left and out and join so then you have a b c here but d gets dropped because the d is not present likewise you could do a join on the right when you do a join on the right let's look at it it is a b d so c won't be here and here we go we have a b d but c is not here right observe something interesting whenever you do a left right or outer join do you notice the presence of nans, not a number. In other words, the value is not present because the other table doesn't have it and so you develop not a numbers. Inner joins, if you start with tables that don't have nans, then the inner join will also not have nans. Inner joins ends up with the least number of rows, full outer join would end up with the maximum number of rows and then the left and the right outer joins would be somewhere in between in terms of the number of rows. Again guys, if you are familiar with SQL, this is a direct translation from that. Then you could do a further filtering joins, which is a bit more sophisticated. You can say that given a data frame, preserve only those rows where X1 is in the other guy's X1, right? So you're filtering the A data frame based on a criteria on the other one. This is a bit more advanced, so let me not go too much into it. And you could do the opposite of that. Say exclude the ones that are common, keep the ones that are not common, not present, and so on and so forth. This is it. You could also do set-like operations. What are set-like operations? Remember, sets do not contain duplicates. So if you do a merge of a Y, so look at the Y and Z data frame, ABC, BCD. If you do a merge of the rows, you will end up keeping only the intersection so the intersection between ABC and BCD is of course B and C so B and C are preserved so the outer merge of course preserves from both as you can imagine now when you do this cyclic operation remember that the other column has to be the same. So these are anyway we can go and talk a bit more about it but let's come to group operations. Group by is very very common so suppose you have a lot of data let's take the example that we are going to deal with. Suppose you have a bunch of cities, about 20 cities. For each of these cities you have a lot of data, temperature data. There's a column called temperature. You don't want to compute the average temperature across all the cities. Maybe that is of some value, it will tell you something baseline, but you're much more interested perhaps what's the average temperature or the median temperature in a particular city by city so how do you group the data by all the temperature data by city and then take the average within that group right so when you group by cities then for every city there will be a group of temperature data and then you take the average or median or whatever it is that you want to do and it would be specific to that particular group or that particular city so those are group by operations extremely powerful again coming directly from sequel then group by is here aggregate sometimes you can just aggregate directly by giving what do you want median mean this and that you don't need to do group by and then sum or something like that you can directly use the aggregate function so this is the richness of these APIs, but we'll see it in action in a moment as a Jupyter notebook. So then you can have all this value shifted by one. There are many, many things, cumulative maximum, cumulative minimum and so forth. We won't go into all of those, but this is it. And now, as you know, one of the big things that has happened with the latest Pandas is in the Pandas data frame, there is a very tight integration with matplotlib and other plotting libraries. So you can do, you notice that these are things that come from matplotlib. They are directly integrated. So you don't have to separately call. You can on the data frame itself say the dataframe.plot.whatever kind of plot you want histogram, scatterplot and this and that. One more powerful thing that came into data frames just as a reminder on this data frame is the fact that you can plug in the back end now some people find matplotlib to be rather vanilla looking rather meh looking then they want to put in fancier back ends for example if you want interactivity you don't directly get interactivity with a matplotlib. You can bring in interactivity with a book. And we did that, if you remember in a previous lecture, we created a plot that was interactive. You could zoom in, zoom out, you could see the value of each point in the data set and so forth. So all you have to do is in the preamble of your data, you have to just specify that you're using the book a back end. Do you remember guys, that one line I think the notebook that I have shared in Slack has that example there. And we did that in the class. Anybody remembers that? Yeah, yeah. So, Bokeh is a library abstraction of on top of Matplotlib, right? Or Matplotlib. Yes, it brings in interactivity. Actually, what they do is they use JavaScript actually. So, they don't produce static images. They do it inavascript another one is plotly plotly is actually a commercial library but they have made sort of a suite a part of it available in open source usually like for example plotly is sitting upon d3 d3 is perhaps the most powerful data visualization library in javascript's a huge, huge following. Plotly is based on that. The good news is that makes it very sweet and powerful. The bad news, I don't know if it is a bad news, but I'm usually very skeptical of companies that will sell you one version of the product free and then another version of the product they'll charge you for because sooner or later they'll start restricting what's available in the open source version. So I have been a bit skeptical, but maybe my skepticism is not justified. Anybody who has more experience about you can speak up. But here's the beautiful thing. All you have to change is the backend. The Pandas Plotting API remains the same. And just with one line of configuration change, you can flip from one to the other. And that is a powerful thing to do. One backend to the other. So this is a basic landscape that I'm going to cover today. Let us do that. I'll start my Jupyter Notebook. I will take the example of the weather data because there I think I have worked out a deliberately a situation which required pretty much all of these things that you have seen here. We'll need to reshape the data and make it into the standard format. We'll have to do groups, we'll have to do group buys, we'll have to do some manipulation and adding of columns and so forth so let's try that example so let's start there we go. There's a few who have read the book, the exploratory data analysis book on the tabular data. You are familiar with it. But anyway, I was talking about the cheat sheet. Let me finish the discussion. These are the cheat sheets that i use guys so i just put the ones that i use fairly extensively and i am inspired by seeing my friend i put it on the table and put glass on it i intend to do that i haven't quite done that but it's useful the rr comes with a lot of documentation. There's a website of documentation. R is probably one of the most well-documented languages and now Python is trying to follow that tradition. You have very good quality articles explaining every language. And there's a journal. By the way, this journal I highly recommend that you guys read if you are doing a coding with R. If you look at this journal, it is not, so let's look at the current issue. The current issue talks about techniques in our package for bi-clustering, fitting tails or modeling regimes, our package for computing duration-based quantiles from the Cox proportional hazard model. So people are announcing new work quite literally here. So here's the thing, and this article is worth reading, a landscape of our packages for automated data analysis. Let's go and look at it for a moment and see the quality of the article. Because on the internet, the quality of articles are pretty valid. So guys, when you look at it, do you realize that it speaks to a high quality? It's been well-written, it has an abstract, it's written in tech, sort of exported to tech, most likely through markdown and they are talking about all of these libraries that some of you some of it you may know some of it you may not know right and just by looking at it what do you conclude the most popular libraries are data Explorer. This is the one that I had recommended and now summary tools. Summary tools I did not know about. So see today I learned something. I could use this tool called the summary tools. The number of downloads is very high. So obviously a lot of other people have discovered it and I was a bit in the dark I still have to catch up on this but I've been using the data experiment so now you look at this article we are doing exploratory data analysis and is this useful in fact it is so useful I will immediately go and print it out for myself so you you see that each of these libraries, they are explaining in detail and they are showing how to use this libraries. What do they give? They can begin and these are powerful libraries. They even doing your principal components analysis and so on and so forth. And all these like we look at expand and so on and so forth and all these like we d looker expand r and so forth see what they do for you uh a decision tree fitted using the explain tree certainly it's something we could have used uh in the r version i'm yet to release the R version. So now what will I do? I'll probably go and incorporate this work into the solution that I'm going to release. So that is the fun of it guys, that you can keep on enriching your arsenal of tools that you use. Remember we did a missing example, this is the missing value analysis. If you remember the missing no function in python has it in black and white well this does one better you have it in color now isn't it and so well in general the R libraries tend to be a little bit more professional, little bit more because the statisticians take it a little bit more strict. Python is moving faster though. In deep learning, Python is the dominant king. Outside deep learning, I would say still R has a superiority over the Python part, which is why I should explore a really solutions in that also. So you see it here. Wonderful article, literally worth reading guys. This world of data science is so rich with other people's hard work and the results of their hard work that it's any hour you give to reading these articles and journals is really time very well spent so that explains for example this r journal what i have done in this notice is all these things that i talk about i put it literally on the left right so then this is a curated list the c10 task views if you want to do something, let's say exploratory data analysis or regression or clustering, somebody has curated the best libraries available to do those tasks, has written a review and accumulated the best ones. Quite often, you can shorten your web surfing time by just going here, picking up the ones from here. These are the respectable high quality libraries you can use for the task similarly for python first of all is uh there are all of these uh tutorials right in the same tradition as r python is following that and coming up with fairly extensive documentation i would would say, I mean, compared to any of these software libraries, you would realize that Python and R both are documented to a much, much higher standard. Lot of effort is put in explaining them very, very clearly. So that's the beauty of this subject. Anyway, I have put all the libraries that I use quite often. And I put it just next to it so that you can quickly refer to it. So do that. Well, glossary is rather sorely missing. The best definition that I have is for mathematics. I hope you'll agree that nothing beats this. Anyway'm joking of course so anyway now I'll come back to this document is available for to us like the one that's your book right were you doing a 100 with this spring or you did you miss it not this is spring way back when you oh yes i have not written it i wrote it actually this spring itself okay i'm sending it for publication at this moment and if you guys i mean as i said actually i'm a bit behind i promise people who want to purchase a copy of this so you must have taken ml100 sometime in your past to be able to buy this book. And this book is going to publication, hopefully in another couple of weeks when I get a bit more time. But let me give you an idea and I'll give it to whatever the printer charges. Let's say that the printer will charge, I guess it's about 30 bucks. So whatever the print, the reason for that is a printing color is expensive and data science is about colored diagrams you know colored plots and graphs so you can't avoid color in a data science book otherwise i can wait takes away half the charm of the book so yeah it gets expensive it's about 30 bucks, 40 bucks, whatever it is at price. I'll make it available to you guys. So this is the book anyway. So I will go in particular to a chapter on. So this is anyway. So for those of you who haven't seen it, putting it here, it's exploratory data analysis is what it is. And that's that's what we'll do today in Python. data analysis is what it is and that's the that's where what we'll do today in python so what do we do in exploratory data analysis there are different ways of doing it single purpose you know you write your pandas data frame dot history plot dot histogram so one by one you within r you will do his or you use this automated data explorer tools so so far guys in the in the lab which automated data explorer tool did we use you guys remember which one did we use profiling yes use that right so that is that and then we have the concept of a data frame which is fundamental And then we have the concept of a data frame, which is fundamental. Once you have a data frame, the R data frame gives you some basic, very high value functions. You can call summary on it,, there's a very, very close similarity, which is why knowing one, learning the other is a child's play. So a grammar of graphics, let's think through it. What does select do? Whenever you have data in a rectangular form, rows and columns, one thing you could do is keep some rows and throw away some other rows. Isn't it? So that is the select. You're selecting some columns, sorry, not throw away some rows, columns. So keep some columns and throw away other columns. So select, very much like the selecting SQL does that. The technical word that you use is data projection. You're projecting it onto a subspace. If you think of it as a real value n-dimensional space, you're going to m-dimensional space, which is smaller than the original space, by throwing away some of the features. Select will do that. Filter, what does filter do? Filter, very much like your SQL filter, you're saying don't keep all the data. throw away some rows or instances of the data based on some criteria. So, for example, if the temperature, if you're looking at a rainfall or something like that, throw away any data where rainfall seems to be greater than a certain amount because maybe they're they are bad readings or accidental readings for example a rain somebody has some uh data gatherer has gathered rainfall in a given day at something like 200 inches you know like 200 inches of rainfall in a given day like a few hundred inches of rainfall in a given day. Therefore, you would say that those rules are bad rules. Let's go down and delete it. That could be a reason to filter. Or if you have data and you say, I'm just going to look at San Francisco. I don't care for other cities. And so you would say, you filter down to city is equal to San Francisco. So that is a filtration of data. Mutation of data, you know you add things, features, columns, et cetera, that's a mutate. Arrange, this is your way of ordering, sorting and ordering data. Sorting is a classical example. You can arrange data by any other means that you choose, but sorting is by far the most common. You want to ascend or descend sort data. And in SQL, you realize that this is sort ascending, sort descending. Summarize the data is a way of getting descriptive statistics about it. So that is summarize about it. The distinct just tells you, removes the duplicate rows. So sometimes people gather data and two people may, you know, in the data set, if other lots of people are gathering data, they'll have overlapping sets of data. So distinct will make sure that you have only unique rows of data. Count will count the number of rows. Then you can sample into the data, you know, you can dip into the data and get a sample. Group by, we talked about group by, you can group by some feature value, for example, city. Take all the temperatures of San Francisco and group them together. Inner join, left join, right join and full join. Exact SQL equivalence. We just talked about it from the cheat sheet. Intersection, again the same thing. Keep the common goals. It's a set operation. And the union and union all is again a ways of merging data frames. It turns out that these basic operations which are there in R are also exactly the operations that you have in Pandas. So that brings us to the concept. If you notice that, you know, I was talking about these methods in R, dplyr, but they are exactly the same as the methods in pandas. Do you see that guys? When we reviewed pandas, we were talking about exactly the same methods. Yes, merge and so forth. Mutate there in R, it's called mutate. In pandas, it's called merge. So with minor name, you know, syntactic differences, it is exactly the same. So today's topic will be, obviously, since this is a Python class, we'll keep it Pandas centric. So what is tidy data? Tidy data says, it's a philosophy about data. It says that if you can put data into a standard format, which follows some rules, data into a standard format which follows some rules then you will have ease of data manipulation you can use a very small grammar and things will you know you can manipulate it to handle with the data with ease so this is a quote actually taken by wickham himself when he wrote his uh paper So this paper that he wrote, well, I didn't quote the paper itself, did I? I didn't. So, but this is his website. Let me click on that too. Yeah, tidy data. And irrespective of whether you work with R or Python, you should read or be familiar with this. So this book, guys, I've gathered all the important references that are really worth understanding in a given topic. So see if you can go back and review this book. I've shared it with all of you as a PDF. But yeah. So what is the basic? And I love this quote, actually. The first quote is coming from Tolstoy in his book Anna Karenina. Anna Karenina is a wonderful and rather heartbreaking story that Tolstoy wrote. Actually the interesting thing is if I remember right Tolstoy fell in love with a girl with a sweetheart and got and they married and in his days of greatest happiness and honeymoon, pretty much around that time or during that time, he wrote one of the saddest books in human literature, the Anna Karenina. Very heartbreaking. And the book starts out with this sentence. It says, happy families are all alive. Every unhappy family is unhappy in its own way. It's an interesting way for a book to start. So then what Wickham is saying is tidy data sets are all alike. It means once you have data in those tidy forms, right? But messy data sets are messy. Each messy data set is messy in its own way. And of course, the messy data sets give you no amount, I mean, endless amounts of heart headache. So the first job of a data scientist is to convert whatever data you get into the tidy format. It is surprising how many data scientists don't adhere to this principle. And I think this is because books, textbooks tend not to emphasize that. So they manipulate data in sort of a hacky way. So what we will do today is we will take the weather data set. This weather data set I've taken from the Cagle website. You will notice that this dataset comprises of many files. There is a file of cities which contains a city's country, which country the city belongs to, latitude, longitude. We'll see that in Jupyter notebooks in a moment, but let me give you a walkthrough before we go into the Python code. Then all other csvs are about one kind of weather observation so the kinds of weather observations are um temperature humidity i believe precipitation wind speed and something else we'll find. A few such observations are there. So when we look at this data, so first thing is, okay, here we go. Humidity, oh, pressure. I missed pressure. Oh, precipitation is not there. I take that back. Humidity, pressure, temperature, wind speed, wind direction. These are the five attributes. How do you read it in a Python guys by now? This one is an R so we haven't done it in Python, but I'll show it to you in Python in a moment You you read it like this Did I give the Python version in this? Let me see. I think in this book I may or may not have given The tidy data. Yes the tidy tidying data with python so let's look at the file would you guys agree that to read data by now this syntax looks a no-brainer figure 212 would you agree guys that this is a no-brainer we are all used to reading data like this yes so this is simple what is the first thing you should do when you read the data? You just go and check how it looks, you know, a few rows. So here if you look at the first few rows, you realize you're in for a surprise actually. This data looks a no-brainer. Vancouver first column is city, country, latitude, longitude. I believe they have about 34 cities latitude, longitude. I believe they have about 34 cities involved here. Now, just as an observation, we talked about it, pandas as a native support for styling and customization. We talked about it, how you can customize your pandas. You see, I've changed the header and so forth. Let's make it look a little bit prettier. But then that looks normal. But then you go and look at the. Look at that thing and you look at. How the data is for cities. And you realize that the data for city is in the wide format wide format means if you let's go and see what it looks like in the notebook weather data so here it is when you go here let me increase the font is this now legible guys from and everybody's desktop looking yes okay so when you look at this data you look at the uh so variable you look at the raw temperature you you do a descriptive statistics you're surprised because what are the columns columns are the cities because what are the columns columns are the cities and let's look at the if you look at this data I don't think I did a preview of this data here if you look at this data and you do a preview you'll realize that it is in a like columns are the cities and the temperature on a given day called a row is a given day and it gives you the temperature for that city on that so the first thing you need to do is you need to put it in a standard format let me we are being rather cryptic let us go and check it out let's take the data let's look at humidity do you see something odd here daytime is the first column each of the city names is the second is the column after that and every day you have let's say a given date and you have the values for it humidity values for each of the cities so now if you think about it does it follow into your tidy data format which says the data should ideally be what is the tidy data format saying data should ideally be in this format. The column should be sort of the observables or the features. It is not. The data is very wide there. Each of the city is in the columns. So it violates this. And it says that for a given city or a given day, all the observations should be in one row that is a tidied data format whereas what we are looking at here where are we here is not in that format you see that columns are not observables are there instead a dimension so you know when people talk of dimensions and measures in data warehousing there's a very close analogy here the dimensions are there as columns but the observations are there in the rows so you want to change this data into a standard format so guys if this word wide format which is a long format is not familiar I would suggest go read my book or go to the tidy data website, make yourself familiar. When you did ML 100 this spring, I covered it in a lot of detail, so I don't want to waste time repeating all of that. But if you have not, it is really important that you go and do that. So what do we do? We take data and we want to put it in the right format. How do we do that? We take the data and we melt it. Melt is a way to pivot the data. In Pandas, you pivot it by saying the row remains this the variable which is which you want to create right is city and then the value of the city and the value name will be temperature if you do that the data becomes in this format you see this is in a way a neater format on a given day in a given city? The observation was this Does this make sense guys on a given day on in a given city this was the observation So these two are the dimensions and this is the measure the observation Any digital signs trying to do it like that? Make sure that the dimensions are your column and then your measure is right here so if you do this for temperature you can look at a few temperatures you can do the same thing for other uh files also so now what will you end up with you would end up with a data frame like this except that the next one will say humidity the next one will say a wind speed the next will see a wind direction and so forth right so now that you have that it is time to do a join isn't it because what you want in reality you want that a data frame like this should have date time, city, temperature. Next column should be humidity. Next column should be wind speed. The next column should be wind direction and next column should be pressure. Things like that. And how do you do that? By joining on these two. So let's go and once you have this you notice that we are doing a join. Is this making sense guys? So let's, let's stare at this. This is a bit of a complex syntax. We are seeing that the ID, the index of a row will be the date time from the original data. The new column we are creating is city and a new value column that we are creating is the humidity or pressure or wind speed or so forth. So it will create the data into the long format from the wide format. So once again, please do review tiny data, it's important. I wish I had time to spend more time on this. But now what do we do? This is a simple join operation. So guys, let's go back to the Pandas data set. This Pandas thing, tidy. Do you see that I use melt? If you look at the cheat sheet, do you see me using melt? Isn't it? Yes. Followed followed by once I have melt I do join of the data okay so how do I join the data somewhere here is a way to join let's go back and say a combined data set right so there are there different syntaxes this This is merge, right? I could have used merge, or you could directly use the word .join, which also is there. I think this one doesn't mention join, but you can directly use join, which is more appropriate here. Join, and then you have to just join on what? and then you have to just join on what? These two column attributes and set index this. When you do that, you will get a combined weather data which will look like this. So do you notice that all the columns have been joined? So in other words, data from all of the files have now been joined to this. This data we can do a cleanup if needed, drop null values. It turns out that you don't have null values, so zero null values. Then you continue on. on no there are some values here so you drop the end so then you ask for what do you do with the null values remember one way is to drop it you don't have some column value is missing you drop the room sometimes you don't have to especially especially for weather. You realize that the weather between two days are likely to be relatively similar, temperature are likely to be similar. If the temperature value is missing for a given city, one easy thing that you could do is just interpolate between the two days. Are we together guys? You can just interpolate the value of the day before and the day after the temperature. If you guess that the temperature today is the average of the temperature yesterday and the temperature day after the temperature tomorrow, you'd be fairly accurate. And that's what interpolate does. Interpolate will fill up all the. Yeah, it will fill up all the first row because how do you interpolate in the first row? Right? And so then you can drop that row. This is the Python syntax. That happens to be. Then you can rename here. I do some cosmetics. I notice that temperature these things have capital letters and so I canonize the name so they all look a bit more uniform great time city temperature and then here I'm just showing the top seven rows of data suppose I want to filter down just to San Francisco what will I do this This is the filtering. When you take a data frame and in brackets you put the filtering criteria here. What are you saying? weather.city, the city column must be San Francisco. Then this data frame will limit itself to only San Francisco. And so for San Francisco we have so many data points. What can we do with these data points now? We can do further things. For example, you can do what you want. For example, you can take the average years or whatnot. So here, what did we do? We did a restriction on the rows. We only want the rows associated with San Francisco. Alternatively, what we could have done is, we could have said, I am not interested with all of these attributes, just give me city, temperature, humidity, forget about wind speed and wind direction and pressure. Just give me these four columns. So how would you do that? You're all familiar with this. Remember, this is how we would create the X, the feature vector vector the feature matrix and the y target variable we have done this in our notebooks isn't it guys the river data set the california housing data set we have so this syntax is very common you pick a few columns you sample into a few rows so now data is limited to these four. How do you find the average weather for each city? See how powerful the syntax is once the data is in tidy data format. You can say weather.groupByCity. So now you groupByCity. And then you say for each of these attributes, temperature, pressure, all of these, find the median. What is a median, guys, German? What is a median of a bunch of numbers? Average, the center value. You sort them. The center value is the median, right? So there are three things mean median mode just to refresh mean is take all the values and divide it by the number of data points average median is sort and pick the middle guy what is more frequent value the most frequent value. That's good. That's mean median mode. And then there's standard deviation. So, Asif, just here we use like a bracket, right, around city and sometime we use like that other bracket, right, I think there is a, is that when you pass the argument, then they use the square bracket let me say okay here this is I wrote a rather cryptic syntax at the break open this box so we have better as a data frame is a function on that data frame it's a group by city you're calling a function you're calling a function. So now what will we get? We will get a data frame in which for each city, there'll be many values, right? And then what we are doing, we are saying that in that, pick only these columns, leave the other columns alone, because that thing will still contain the city column and the other things, right right so it says just pick these columns this is very much like you're passing an argument yeah no this is square bracket is a list of remember the square bracket is always a list in Python square brackets produces a list yes are you're saying that from that data frame, pick these columns, right? And from these columns, find the median of these columns. Right. Now the thing is you got the median, it is worth giving these medians some name, right? These medians that you found. So you say, okay, you know that the first one, second, what each of them were, you give it the column names, because otherwise they were missing column names, you just give it column names. And then this is just called reset the index means, forget about the index column, the row ID column, reset it. And then you sort it by, let's say that you want to find the from the hottest place sort values you can sort it by values in a given column right so when you do that see what do we get so can you tell which is the coldest city and which is the hottest city coldest is montreal right a 281 what do you think the units of measurement are here anybody can guess what are the units of measurement in what is the scientific unit of measurement of temperature scale when Kevin exactly so these are Kevin so if this number looks huge in Fahrenheit you have to remember that these are Kevin's. So 273 is zero degrees. And zero degree Celsius or 32 degree Fahrenheit. So this basically in terms of centigrade, it stands for median temperature being just eight degrees centigrade throughout the year. You would imagine that if you live in 8 degrees centigrade, medium temperature, you would call it a rather cold place. Minneapolis also in the US, as you know, is cold. Toronto is cold, right? And Seattle looks cold. Vancouver, all of these are up there. Detroit, these are up there. Northern cities, they are cold. On the other hand let's go and see what are the hottest places. Miami. Miami is close to 300 degrees, like temperatures are pretty high, isn't it? 27, 25 degrees centigrade median. Houston, you all would agree, Phoenix, Arizona, San Antonio, agree phoenix arizona san antonio these are all fairly hot places tel aviv tel aviv certainly and jerusalem are in the middle east those are really hot places heifer and all these are hot places right so this is it and the same thing if you want to just prettify it, I just showed you how to prettify this thing. So guys, this is using pandas in Python to do data manipulation. The first time you encounter this, and I'm speaking to those of you for whom this is your first encounter with data manipulation. I would again remind you of the advice I gave in the orientation book. Pandas for everyone. That book should be your primary text. Read it. Those chapters are easily digestible. Each chapter you can finish in two, three hours. There are, I believe, 16 or 18 chapters or 20 chapters in that book. Every day if you do one chapter, in two, three weeks you would have finished the book. And just giving two hours or so to read one chapter in the book is not much. Do it, practice it. If you just read it, it's an hour. Practicing it will take you an extra hour. Correlation plot. How do you find co- what is correlation? Correlation is how related are, you know, how co-varying the z, the normalized values are, you know, once you z-value it, it gives you the covariance of two variables. Asif question. Yes. In line 23, Asif, when you set the index, is that a sorted already? The city is already sorted? Yeah, it is at this moment it is already sorted. And then you just reset the index from 0 to 20 to 30? Yeah, you reset the index. They reset index on the city. i mean you make city itself the index so do you notice that there is this index column here right yes okay let me explain what i uh to give you the context it's 21 probably that you are referring to once you have done all of this right data you have sorted the data remember where did we sort the data here you sort it by median temperature right so observe something you have this index column the pandas data frames will always have this index column it is serving no purpose you know it's just a row id what do you do about the row id it is unnecessary because each you realize that a city sorry city is unique in this table city is the unique thing so you don't need this you could have made city the index so two ways to deal with it first is you can go and just say reset index if you reset index then you will see something like this all right zero one two, if you look at the data. But actually these indexes are necessary. So the next thing we do, and I'm just showing what all the things you can do. You say that, forget about these numbers. Let us declare that the city itself is the index. So I made city the index. You say, make city the index column and effectively get rid of the row number as the column, index column. Every data frame has an index column, so make city the index column. You just got rid of one unnecessary column. And so you can then go, and now that it became the index, you don't want to have an index and a city as a column also present now city column is useless so you can go and drop the city column and your data then if you sample into the data you will see a slightly different behavior actually there is something odd if you run this actually this should have been city names yeah you should see city names so there's an active run this now did you get that patrick you remember that you did this in the last course yes my question is as if if i sort and then i just put inside the sort ignore index equals uh true and that will that be the same output yeah it will do that that will work too okay so whether data now merge remember i talked about left join right join well Joins are here as I'm using merge left on right on. So you could do that. City, right on city, you're merging. I could have just used join. I wanted to just show you the syntax for both left and right. Like you could use merge to do the join or you could use literally weather.join if you want. So you're joining another table called the means and the city table and so forth. So now all you are doing anyway, these are minor details. All we are doing is we had the mean temperature of all mean, etc, etc. Every city temperature is useless. Actually, these things are not. the mean temperature of all mean, etc, etc, of every city. Temperature is useless. Actually, these things are not. So temperature, you added the mean value. What is 6th of March 2014 are the temperatures below actually it's not mean it should be medium are they below median or above median they're below median so it is a colder than normal day isn't it on that particular and so forth so this this is data manipulation with pandas, guys. I'm just giving you a flavor of it. This notebook is available to you and of course this book is available online for all of you taking ML100. Soon you'll have a printed copy, all of you, if you are interested. Go review that. We are reaching a stage at which you know enough about the language, you need to learn about the library. So today was the day to do the Pandas library. Pandas is very powerful, you know, this is a course in machine learning, so obviously we are focusing on the algorithm, but I thought we needed to give time to basic uh you know the vocabulary of data manipulation uh by the way approach is maintaining the list of people who are interested in purchasing a copy of the book uh give her your name if you haven't given at some point because if we group if we make a group order with the printer there is a discount they give if you order more than one, more than five copies or something. As an author I don't get any discount, I mean any extra discount, so we will all collectively order this book. I will keep a copy for myself, you guys will all get a copy of it. It's an unfinished book, that's why I'm not fully publishing it. So you will you're essentially buying a draft or printing out a draft copy for yourself. So with that, guys, I don't have anything else. take questions hey as if so this is arithra so this is very useful but uh unfortunately i didn't know about ml 100 uh on time so so i couldn't take it at that time so uh so but so for that reason so i I won't have access to the book. You're taking ML 200. I'll let you let you purchase the book. Thank you so much. Yeah, though I must suggest that there are two things. First of all, you know that it's support vectors, repeating a course is free. You can repeat ML 200 as many times as you wish having said that i would strongly suggest that whenever i offer ml 100 at this moment i don't know maybe in fall you may be in that registering for that yes yes i definitely want to do that but yeah that was my next question when you are going to offer them 100. I'm going to see this batch your back, I will get all the way through this deep learning and all that. Then I'll start the ML 100. Okay, so that will be much later, right? Asif? Yes, that will be much later to three months. So can we purchase the book till then and practice it? Yes, that will be much later, two, three months. So can we purchase the book till then and practice it? Yes, sure. I'll let you. I mean, if you have taken any course with me, I'll let you purchase the book. Okay. Thank you. I've taken it. Asif, I just wanted you, if you could write, we did the training and test data here, right? In Python. Training? How do you get the training and test data in our because I'm able to do it for single single value like you put x-ray and y drag in the same line of code right in when we know yeah it doesn't give you such a elegant syntax okay I'll give you the best that i have that i use okay so it is okay to do it in single lines then yes okay to do it in single okay what you do is you first define a set of random rows then those rows a subset of those as your training data and the other rows not in that sample as your test data. So in that matter is a little clumsy for Python that is scikit-learn that's one of the crown jewels that you can split the data into test and train in a single line. That's what I was being, I've been trying to do it, but I couldn't okay. So I'll summarize what we learned one thing is that i started out by saying in these notes that i've given you guys obviously i apologize to people who haven't taken ml100 to that just reminding you that there's a lot of references those references are very useful make use of the references you know it is surprising even when you're reading email or doing something if a reference card is just under your nose you end up imbibing a lot of it those cheat sheets and those documents the r journal is very good it comes out periodically and there are a lot of these articles that come out. One website that I like is Towards Data Science. I've mentioned that before. I have obviously no association with them. I like you. I'm just another subscriber to it. I tend to like it but I assume there must be other websites also devoted to this. But this one is pretty good actually. So I think they charge me, if I remember right, they charge me a little bit of money to read all the articles, some two, three dollars a month or something. So somebody told me that first few articles are free and then after that you have to go into the incognito mode and you can still read it. I don't know. Personally, I like to support the site. So I actually like paying them and like sort of patronizing all the people who contribute articles. Because apparently their model is that anytime you write an article on this site and anybody can write, and then based on the number of claps you get, you get money. Maybe each clap is a dollar or something like that. So people can actually make a bit of money by writing high quality data science articles and posting them. So this is another. So there are many such websites. Then some people like the analytics Vidya a lot. It is filled with some good articles. You can try reading all that. YouTube is of course absolutely chock full of good videos. More than that, the second thing we talked about was that data, it is worth having data in a tidy form. And for pandas, because today we were doing Python, pandas data frame, if you follow the tidy data syntax, you will realize that manipulating data with pandas is both elegant, powerful and easy. You can do a lot with just a few lines of code and you should always learn the right language or grammar of data manipulation. It helps you a lot going forward. People these days are in a hurry. They come into the field, the very first thing they want to do is how do I write my first deep neural network? And in a way, it's sort of you get onto the hot topic, it's exciting, it's exhilarating, but someday you should go back and fill in your basics. This is filling in the basics. All right, then. Some data manipulation in that system. Go ahead, Patrick. As if I have two tables that share the same, let's say, city column. What's the quickest way to compare if they have the same amount of cities listed or the data is exactly the same? Oh, that's very easy. So see, if you want to see if the number of the cities are the same, all you have to do is do dot unique. You can get the set, you know, you can take the columns. All right. Not unique as if like the exact same, um, the exact same values in the exact same order. Oh, whether the two tables are exactly the same data. So first what I would do is I would sort both the tables. See is basically you should go like this. You sort the columns because somebody may have written the same data in different order. So you sort it in the same order after the two things are sorted in the same order. Then there are I think data frames may have a way of comparing that I don't remember. It's a good question, actually. I don't know the exact answer to it. But one off the top of my head answer that I can give you is I would sort both the data, write it out to CSV and just do a diff on the CSV. Okay, so as if what I did was I pushed the values to a numpy array and then I sorted it and then I compared the values of one table would equal the values of the other table. Yeah, yeah that is just as good. See in data frame itself you can write a little bit of code that will compare it column by column. Once you have sorted the data it will compare it column by column because each column of data is a series. And you can say on the series you can check if two series are equal to series is nothing but a wrap around numpy array and you can check if those two areas are the same so you can iterate over all the columns and just verify that okay thank you i thought there was a one line to do it maybe there is so this question that you asked was very interesting it didn't appear to me a very good question let's explore it or compare two data frames founders differences quick tip there it is somebody has that if he is always works if this is with the frame reset okay here here's he has the data frame one data frame two you concatenate the two data frames reset the index then you group by list of the df columns so you group by those columns one two. And now this is a more complicated record. I think compared to data frames and output the differences side by side, somebody else has So this is it. This is really this shortest syntax. What you do is you take the data frame, this is really the shortest syntax. What you do is you take the data frame and you take only those rows which differ, like where something is different, not equal, right? And then you produce it, that is it. So yes. So yes. This is the way. So guys, I learned something. Let me paste it in there. That today. Yes. This is a slash on stack overflow. So it's a very good question. And this is an elegant answer, I suppose to that it does it in one line likewise can we use join as if you know join would not be the best way of comparing no uh i think if you go to the pandas cheat sheet yeah so on the second page uh combined data set uh so that fun that yeah yeah the ca so that is the difference uh so if you combine data sheet uh the last one all rows in that do not have a match yes yes thank you for pointing this right here there's another way of doing it you just say right here is another way of doing it you just say all the but this is only on a column uh the trouble with this is i know this is only on a column basis so you're comparing the first column to the second column yeah so yeah that is it so you'll have to write a sort of a for loop to iterate over the columns uh that that also does you see that is the approach i was thinking about but this one seems even more elegant it just if data frame not equal to data from which means that it has been spun as data frame is pretty smart internally, it will do the comparisons for you. Isn't it. And so look at this syntax very, very elegant. I didn't know that. In one line, you can find that. Like both approaches. Thank you, Anil. Thank you, Asif. Yes. Very, very good. And people have added more and more to it. Very nice. All right, guys. So that is it for today. Any questions? Was it useful by the way this ponder stuff? Very useful Asif. It seems like data frame just takes value on its face, right? Just integer, character and whatever it is, store as it is, right? Right. Actually, this concept of data frames, the way it is very well articulated in R and in Python. And somebody was telling me that these two communities have joined forces now so that the syntax is even closer to each other. But it's a huge thing. Imagine doing it in C, C++, Java. It's much harder. And there's a reason if you look at Spark, the first version used RDD, Resilient Distributed Datasets. But then the second version was based on DataFrames. DataFrames are a very powerful construct. And today, in most of the things that you do with Spark, you can just write it as SQL. So that is the data manipulation language. You can just write it as Spark SQL. You load the data as a data set as a data frame. And then most of the data manipulation is just SQL. And most of the world of reporting and business intelligence is just that. That's very powerful. So you can do it very elegantly now instead of writing tens of code. You just took else guys? else guys? No Asif that's very useful thanks a lot. Okay. All right so this recording remember I'll stop the recording now it is all live so therefore it is there on YouTube already as you if you want to go back because we went over some material a little bit fast. And what else do we do now? Alright guys, if you don't have any questions, I will this session and give the point here let's take up tomorrow tomorrow yes we have a new topic we will do boosting and we'll follow boosting with uh we will follow boosting with what with support vector machines with kernel machines thank you yes bye guys thank you so much thank you when do you have time for consultation? When do I have time? Tuesday is a bit more open. Should we keep it Tuesday evening? Okay, okay. I don't need too much time. I just needed to run through the code just to see if the thought process is okay. But more or less I understand the code. Nice. Let's careful. Thank you. Tuesday, I'll send you an email. Thank you. Bye guys. Thank you, Asif.