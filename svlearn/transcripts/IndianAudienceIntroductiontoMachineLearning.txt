 All right guys. One second. I want to make sure it's muted. Yes. Today's topic is clustering. This is one of the many very important things we do in machine learning. So far, you have learned about classification and regression. In both of those algorithms, the goal was to predict something. For example, in regression, you predict a number. When we talked of the entrepreneur trying to open an ice cream shop on the beach, the input was all the weather data. The output that you were trying to predict is how much ice cream would the entrepreneur be able to sell on the beach. In the case of classification, for example, you're trying to detect whether something is a cow or a duck. It's a process of identification and that identification, to the extent that you have, you know, you're trying to tell which class it is, categorical variable, is the formal word you use, is it a cow or a duck, or more practically, whether to give this person a loan or not, how much is the risk? And based on that, you make a decision. To give this person a loan, if you're a banker, to give this person a loan if you're a banker to give this person a loan or not and or as a friend is some friend comes by and says I want to take give me a loan of one lakh rupees or something like that and you would want to you would want to know whether it is safe to loan to this person whether there's a reasonable chance of it's coming back and if so do you give him the loan or not that's a classification problem right or trying to for example if i show you a character any character and or some text and you're trying to determine which of the languages the text belongs to is it in the indian family of let's say not indian family of languages they have nagri script base is it a one of the uh for example is it tamil or is it uh tibetan and so forth so when you try to do that, you are again classifying, you're looking at the input and you're trying to speak to our estimate which looks the most likely answer. That's classification. But nonetheless in both these situations, the output is a prediction there is an output and in a sense you're building a predictive model a machine that can make predictions based on inputs right that generalizes from data learns from data generalizes from that to be able to make predictions on unseen data this form of learning is called supervised learning. Supervised part being the training part where you learn from training data. We saw that in the lab for California housing. Today we're going to deal with another area of machine learning altogether. Its purpose is not to make predictions. The purpose here is to just find interesting patterns in the data. When you discover these patterns, you learn something about the data and the forces that might have produced the data. Now there are many things that you can recognize in a data pattern. The simplest of them is the cluster. You may find that data comes to you in not uniformly distributed in the feature space, but is clustered around some points in the feature space. It does happen. Now the question is how often does it happen? We'll come to that in a moment. But to the extent that there may be clustering of data or the data maybe bunched together in the feature space, it is an exercise to discover those clusters if they exist. A different kind of example of unsupervised learning could be, pattern recognition could be, for example, you look at data and you build, you understand what people, mathematicians, use a technical word, a probability distribution. The probability distribution of the data in the feature space. If you can determine the probability distribution of the data quite clearly, it leads to a very powerful effect. You end up with a generative model. Now you can generate data at will. You know, you can, what you have done is you have taken data, learned from it, and you have a generative will. You know, you can, what you have done is you have taken data learn from it and you have a generative model. You can go on generating new data. And new, these generative models these days are extremely effective. They are quite the sensation. For example, all of these things, I won't go into those algorithms like variational autoencoders and things like that, but at the end of it what happens is that they often become powerful models that can generate new data and they're uncannily good data. Sometimes the line between supervised and unsupervised learning gets blurred. For example, some models, it's hard to tell that they lean more towards supervised learning and still they do pattern recognition or they build generative models and so forth. So there's a bit of a continuum between supervised, semi-supervised and unsupervised learning as the term goes. So I tend to use different words. I tend to use the word predictive modeling, generative models, and pattern recognition and things like that. And examples of that is, for example, of the generative kind of models are like, suppose you have a whole lot of pictures, I give you your picture and suppose I take another reference picture. Let's say you take a picture of whosoever is the latest Indian film star. Could somebody give me a name? Somebody who is very popular these days. Amitabh, I think. Amitabh is still going strong, is it? Yeah. Goodness, isn't he a geriatric fellow? Yeah. Goodness, isn't he a geriatric fellow? Doddering old age, not yet, eh? Okay, so let's take Amitabh Bachchan. He was a guy of our generation, my generation. I'm impressed, still going strong. Okay, so let's take Amitabh Bachchan. So suppose you take your reference face and Amitabh Bachchan's face, what we can do, what a generative model could do is it could generate all sorts of faces in between you and Amitabh Bachchan that make you look progressively like Amitabh Bachchan, right? And those faces will be very, very realistic. You won't be able to tell that these are not real faces. So these are called fakes, the fake images. And the fake images are all the rage. You can have, and you can do not just images, you can do videos and you can do audios. One of the videos I'll post to the, our discussions, like discussion is a video that was introduced in the MIT machine learning course it is a president a president of united states former president Obama who is very popular in the US despite the fact that he is no more the president giving a speech the entire speech giving a speech the entire speech was never given by obama but you can't tell that because it is generated by uh it is it is machine learning that's producing that you can have a situation in which you are speaking but you can have every word that you speak come out of the mouth of Amitabh Bachchan, with Amitabh Bachchan's expression, with his accent. And you will never be able, it's very, very hard, actually near impossible to tell that it is not Amitabh Bachchan speaking, what you are speaking. So that is the power and of course the danger of generative models. These days it's become a huge issue with power comes abuse and it is being abused these days. So people create all sorts of fake images and fake videos and so on and so forth. These are called deep fakes. They are so utterly realistic that even machine learning algorithms, powerful machine learning algorithms cannot tell the difference between them and reality. So it's a completely different world we are entering. And so after doing the predictive models, I thought it was time for us to get into this other two kinds of models, generatives and the pattern recognition. But today I'll start with obviously just clustering. There's a third form of pattern recognition. I mean, some more of them. There are many, many things in this space. But I'll talk about one more thing. This is dimensionality reduction. Sometimes, you know, you get a lot of features. People are trying to determine, for example, factors that cause diabetes, right? So you may get all sorts of factors, environmental and life science and genetic factors. Each of them is very responsible for diabetes to a different extent. But then let's take one class of people, type two diabetes, that is, or adult onset diabetes. And the question is, which action that you can take will lead to the person getting or managing his or her diabetes better? So towards this end, you may think of, OK, there's so many things a person can do the person can for example go for a walk the person can do i don't know pranayama the person can do reduce the portion sizes of food eat less sugar and so many many factors you can. And you can enumerate the factors, the hundreds of actions that can do. Each of them in small measures will benefit the person. So when you try to make a recommendation or think about what will help this diabetic, you may be inclined to give a very complicated recommendation. But what you can do instead is look at the underlying theme behind those recommendations. And so let us say that plausibly you come to the theme that you tell the patient, I think more abstract, you don't give an action, but you instead say, do two things, or let's say three things first uh increase your activity level do more physical activity walk more sweat for at least half an hour whatever way you do it whether walking swimming running jogging tennis or whatsoever just make sure that you have about half an hour of sweating in the process, rigorous exercise in whatever form you like. And you should cut down your portion sizes in your sugar by a significant amount. Don't exceed this threshold. And you can give probably a third advice and say, make sure that you sleep at least eight hours a day, eight hours a night. And then, and you have derived these recommendations by looking at the data. But now you're making recommendations along only three axes, exercise, certain amount of exercise, certain amount of diet control, and a certain amount of necessary sleep, which is a far better thing to do. It goes to the core of the issue, lifestyle issues, compared to making small recommendations or very specific recommendations that you must walk and you must swim and you must do this and you must do that. Some of which may not be feasible. For example, if you look for swimming, which may not be feasible for example if you look for swimming there may not be a pond or a swimming pool nearby for the person so uh or a sea nearby so uh in a sense what you're saying is get to the heart of the issue and the in some sense the core factors the underlying latent factors that are behind this feature, you can discover somehow, and they are fewer in number. So while there are multitudes of factors affecting a certain behavior or certain data set, Underneath them all are some few core latent, latent, the word latent is a Latin for hidden factors, hidden factors. And those factors, those few factors are the real things you should go after. To do so, to discover those sorts of factors is called dimensionality reduction. reduction. Now the literature of machine learning has a lot of techniques, beautiful techniques to do dimensionality reduction. It has a lot of techniques and a vast amount of literature for generative models. It also has a lot of literature to discover clusters and data. Each of these are vast fields. Each of them have produced like quite literally tens of thousands of PhDs. world's in themselves and machine learning is now a vast vast field as probably you must be getting a sense of it is as big as the field of medicine say or or any other discipline vast discipline so but today we can't get introduced to all of that and we are going to take an hour hour and a half we will focus on one thing which is the detection of clusters in data this process is often called clustering and the algorithms that find clusters are called clusters so let us start with the very basic principles and i'm going to do that now uh we'll walk our way through, write and walk our way through this whole process. Let us do that. So suppose you have data. I'll take an example. Let's go back to the example that I keep quoting. Let's say cows and ducks. let's say cows and ducks. Suppose you look at the weight of the cow and the size of the animal and the size of the animal, you would agree that all the ducks are lightweight. So let's say that this is X1 and this is X2, the size. And then, so would you agree that most of the ducks data would be clustered here because they are small and lightweight creatures. They're flying, they're birds, right? Whereas the cows are big and heavy. So they will be clustered somewhere around here. So folks, is this looking pretty obvious? If you look in the world, this particular feature space, which in this case is a two-dimensional feature space i hope it is very obvious that you should observe some behavior a data distribution would look like this right so far so good guys right yes simple it's a simple fact that you would observe something like that. Obviously, I don't have actual weight and size data for cows and ducks with me. But I'm sort of giving a motivating example and I hope reality is not too far from this. Now, what you observe is that you have in the data clusters. This is a cluster and this is a cluster. Now the question is, how do you discover clusters in the data? So the human eye, of course, will immediately notice the cluster. The human eye and the human brain is the most the most fantastic computational and machine and a learning engine right if you were to think of it as a learning machine it is the most amazing learning machine that we have and not just humans if you go go and look at your or any of the animal species vision is one of the greatest marvels in the animal kingdom. I don't know about plants, but certainly in the animal kingdom, vision develops pretty early from very rudimentary forms of visions, which may be just sensory to full-blown vision. And someone once, I read from a researcher once that the vision evolved close to 100 to 300 million years ago in the species, in the evolution of animals. So what has happened is if you think from a machine learning sense, the brain is a machine and the eyes as learning machines. These machines have had absorbed data that spans close to 100 to, I don't know, I'll just use the word 100 million, in the order of 100 million, in the order of 100 million years, right? So it has been learning and looking at data for 100 million years. The end result of that is the learning that has come from that is so super efficient that when we look at a picture like this, the human eye recognizes the clusters, perhaps even before you utter the word cluster. It has seen it just in the blink of an eye, it has seen it instant, it sees it. But the question then arises that, that is fine for the human eye, but can a machine detect it? Or can you write code that will detect clusters in data? So there has been a lot of effort in this and a lot of history. See, when you look for clusters in data, if you really think in an eye way, you can say, well, you know, if I have programming background, I can search for things, I can sort data. Why can't I discover clusters and data? It must be easy. But no matter what you do or how you try, you will soon realize that the straightforward programming cannot solve this problem. We know today that this problem is at least NP-hard, right, or maybe NP-complete, is at least NP-hard, right? Or maybe NP-complete, in the sense that programs cannot do it in non-polynomial times. And yet the human eye or any animal eye detects the presence of clusters immediately, right? So if you have a lion and you show a herd of ducks, a herd of cows, it wouldn't take the lion more than a blink of an eye to see the herd of cows and start running towards it. So there is no ambiguity in the minds of the lions. So we do know that dr that clusters can be discovered somehow. The question is how? And that's where machine learning comes in. Machine learning looks at this problem and the answer is, you know what, there is a reason why you can't discover clusters immediately. It has to do with the definition of the clusters like for example is this red thing the cluster or is this more dense area inside that is the cluster you see the ambiguity if you look at the dense area that i have where i have perhaps more points so which of these would you call the cluster there is an interpretational aspect to it. There is also an aspect of granularity. At what level do you want to see the cluster? Do you want to go even further? And let us say that you have points very close here. Do you want to go further and call this little thing the cluster? Right? So what is the cluster? And what notions do you need to define a cluster? Those issues come up and you have to address those issues then only you can discover clusters which is why it is not a problem in normal programming, it is a machine learning problem. Are we together? But it okay so this is this is about the process of discovering clusters but But even before we go here, there is an outstanding problem. Given data, do you expect to see clusters? How often would you find clusters in data? Suppose you imagine that you're taking a broad sweep and surveying all sorts of data, data sets that you have been given, that you find something coming from mechanical engineering, an aeroplane flying, some medical data, some financial data, some all sorts of data, you know, your bugs, your economics, econometric data, social psychology data, everything that you can consider, right? There's a wide variety of data in the world, of course, and an infinite amount of data sets that one can have. It is a question worth asking, how often would we find clusters in this data? So let me throw that problem to you. If I were to ask you, in your just gut feel, what proportion of those data will have the presence of clusters? One percent, half a percent, 0.1 in thousand, one in ten thousand, or something like that. Let's see what our gut feeling says. Would somebody like to answer? Speak up guys, speak up. Anybody still listening? Yes sir. Yes, so make a guess. Yes, yes, yes. Yeah, so throw a guess guys, throw a guess. You'll probably see it one in thousand, one in hundred. How often would you see the presence of clusters in data so one in thousand one in thousand that's a thank you for stepping forward and making a guess marissa anybody else except for the repeat students of course anybody else how often do you think you'll see it let me start taking names since you guys are not stepping forward rahul yes sir it's like uh uh in hundred we can see we usually see the cluster data how many times in 100 one in half is it yeah one in hundred uh anybody else who'd like to have a different answer what about you shri khan one in thousand and uh what about you Kumar? One in thousand. One in thousand. Alright, so Rajeshwari? I think she just joined. Hello? Yes, go ahead Rajeshwari. No sir, I just joined due to network issues, so that's it. No problem. How about you, Suresh? See, we always see a cluster. So I don't know how one in 1,000 or one in 100, how much that would be realistic, but we always see a cluster in the given data that's right uh did you attend the previous lecture point on this i think you did isn't it i've talked about this or is it yes no no no i didn't attend any session it's a very good answer actually surprisingly it turns out that you very often get cluster. I would see that clustering in data is all like greater than 80 to 90% of the time. If you have data, you're much more likely, even more, I would say that 95% of the time, you always have clusters in the data. And more than even 95 would be actually now the more i think about it in the examples very rarely do you see an absence of clusters and the the the reasons are quite interesting actually um it has to do i mean i could argue from a theory called the kiosk theory or nonlinear dynamics, but I'll explain it in a simpler way. See, have you ever gone to a wedding reception in India? Those are big events. When you go there, suppose you're looking from the balcony to people in the sort of the room, the main room where all the guests are sitting. Do you see the guests uniformly distributed or do you see them in clusters? Clusters. Form clusters. Isn't it natural anyway, sort of congregated into small groups and talk and things like that and they're forming. You go and look at a schoolyard. Do you see children just randomly distributed everywhere, uniformly distributed? Or you see them in little bunches, little clusters or groups? You always see them in groups, right? You stand on a highway, you know, now you have highways in bandler and in india so you you stand on a fly it is called an overpass do you have overpasses over your highway like can you stand on a bridge over the highway and look down at traffic flowing yes yeah if you ever do that you will observe something quite interesting. Even when there is no traffic congestion, especially when there is no traffic congestion, when the traffic is congestion, congested, there are no clusters. It's one uniform sea of cars and automobiles stuck and motorcycles and everything stuck there. On the other hand, when the traffic is free flowing, there is no congestion. If you stand on such a bridge and overpass and you look at the traffic flowing, you will observe that the cars don't, or the automobiles, they're not growing, it's uniformly separated out or more or less uniformly separated out. They sort of come in bunches, they come in waves, right? One big wave of cars and automobiles will go, followed by a little gap and then more automobiles will come. And it is worth wondering, so what is happening is that on the highway they are clustered, they're going in little bunches together, whereas ideally they should perhaps be uniformly spaced out, right. It doesn't happen for some reason. It leads to and the reasons have to do with nonlinear dynamics at kiosk theory and things like that. It's very interesting that it happens. More than that, if you just look at something very basic, look at the mails coming from, the postal mails coming from your coming to your house, or sometimes the packages coming to your house and so forth, you will realize that someday you don't get much mail, one or two mails. Some days it seems that the whole world is remembering you, right? You get a lot of mails, but you don't get a steady state of mail. It isn't that every day you'll get exactly three mails, the average number of mail to your postal envelopes to your house. Have you noticed this, guys, that some days you get much more and some days you get much less? Now, when you think about it, each of these letters are coming from completely independent sources. They are not in collusion. And yet, coming from completely independent sources, and yet coming from completely independent sources nonetheless when they reach you they reach you in bunches right they reach you in clusters and clustered in time right a lot of them come on one day another day that you wouldn't get as much and so on and so forth so they come in waves right so when you look around everywhere, you will see that actually data is clustered. Things, events, happenings are clustered. Are we getting the sense? There was a very interesting thing you can do. Even if you make people, like let's say enter a room. I'll take this thing. And this is coming from one of the first episodes of a lovely TV show, which I encourage you all to see. One of the few TV shows which are centered around machine learning, it is called numbers. B. The three is reversed, I believe numbers, right? It may be visible in Netflix, I think, I hope so, or in one of the streaming services that you folks have in India. Check it out. It's an old TV show, but I think it's still very good because each episode introduces, it's a mystery, they'll typically be murder mystery or some something or the other, but the solution of that will come through machine learning through the application of mathematics to the problem. So one of the first episode had something similar to this. The mathematician was trying to show that data is clustered. And when data is not clustered, it means somebody is deliberately trying very hard to prevent a pattern from being visible. So what he gave is an experiment. He asked everybody to enter a room and stand somewhere. And when they stood somewhere, they automatically began to form clusters. They were not uniformly spread all over the room. It is natural for data, for people, for everything to form clusters. So an absence of clusters usually shows something unusual. There are absences of clusters. For example, if you go to the beach and you look at it all you'll find is sand right now you may say that sand too is clustered in some sense but uh but sand is more uniform you probably wouldn't see very well-defined clusters but between the sand you may find clusters of bushes like some green bush or something like that. But the sand itself is more or less through a lot of erosion and so forth in the sea activity. It has become a uniform bed of sand, right? So there are situations where you don't see clustering of data, but it is much more likely that you will see that. And to me, the most evocative proof of this is to just lie down on summer days and look up at the sky. I don't know if it is still true in India, but when I was growing up, that was 30 years ago, 30, 40 years, actually 40 years ago, it was common in India because load shedding used to be common or because in India the houses get extremely hot in summer. You don't feel like sleeping indoors. You didn't in those days. So you would sleep outdoors. When you would sleep outdoors, you would be sleeping in your courtyard or you'll be sleeping on the roof and you'll be looking up at the sky when you look up at the sky on in the night what do you notice you see stars there now are the stars uniformly distributed like do you see a sky filled with random and uniform distribution of stars you don't see that what do you see? Anybody? Questions, I'm sorry. Yes, the stars are clustered, right? And especially if you have a telescope, it becomes much more apparent. We have galaxies, then we have constellations of galaxies. And the universe is not a uniform distribution of stars, isn't it? So that's again an evidence of clustering so these are all just to give you a variety of data that exhibits clustering so clustering is a fact therefore the question remains though how do we discover those clusters and that is the context of today's algorithms the clustering, people call these the clustering algorithms or they often also call them, so the words that people use are clustering algorithms or clusters. You will see clustering algorithms much more commonly used clusterers is more intuitive right but I suppose it's less commonly used and actually you find clusters used paradoxically in more formal literature so research papers and so forth you'll see the word cluster mentioned. Anyway, so today we are going to learn about three kinds of clusters in quick succession. We will learn about partition based and we will do density based. So today would be a bit of a long session because we'll give some time to doing each of them. It may perhaps be the longest of the sessions we have done so if you feel that we are going too long we can of course break it up into parts and do the second part next time okay so um i will start with the first one which is the simplest of them and the most ubiquitous any way you look you'll find somebody or the other has applied K-means clustering. Its strength is that it is simple. Its weakness is that it can be unstable and it may create clusters which are unrealistic. Those are its limitations. Now K-means, the version that I'm going to explain here, it is a very simple version. Usually you use a bit more sophisticated version of it to speed it up, to make it run faster. But we don't want to get into the complications. We want to get to the core of the idea. And to get to the core of the idea and to get to the core of the idea we will avoid all sorts of optimizations so for those of you who are technically inclined you will see that you use data structures like metrics like certain things like metric spaces and metric trees and kds as sort ofD tree is quite common, and so forth, cover ball, cover spaces, O trees, O ball trees, and so on and so forth. There are all sorts of data structures that people do bring into the space, but I wouldn't go and talk about all of that because it would be a distraction from the core idea itself that we will use. So let us go back and bring back the problem of our cows and ducks. So suppose you have data point again, and I'll just make this ducks here. And then ducks, and then this curves. This algorithm would basically says is that it solves half the problem. See with a plus string, there are two main problems to solve. Plus string has two parts, with clustering there are two main problems to solve clustering has two parts a how many clusters are there b right where are the clusters? So discover the clusters. First is decide how many clusters are there. Find out how many clusters are there. Now, when you look at this diagram in the feature space, which is X belongs to here, it's a two dimensional plane. And I'll just keep it abstract instead of weight and size, but you can think of weight and size as a motivating example. When you look at this, I ask you, how many clusters do you see? Two clusters. Two clusters, right. two two clusters right so now k means does not answer the question how many clusters it says you tell me how many clusters to find and i will find you those many clusters no matter what so for example if you ask it to find 10 clusters it will go find 10 clusters in this data so it leaves half the problem to you and solves only the other half the part b of the problem of the thing it says tell me how many clusters you want to see so let's take a let's say that you make the happy guess of two clusters so that is what the k stands for. K is equal to number of clusters. So let us say that you say K is equal to 2. How will we do it? Let's figure it out. Now, so I'll explain the word means here, that is where it will come to so the way you do that is uh it is made up of two a few steps and these are very sort of intuitive steps it says first step one steps pick any two points randomly points randomly as the initial centroid, centroid means center of gravity. Pick any two points. So let us say that you pick, and of course I'm getting to the intuition first. So let us pick some point that is like, which one should I pick? Let's say if you pick randomly, you ended up picking this point, right? And also randomly, let us say you end up picking which point, this, maybe this point. These are two random points that you picked up right so then let me just call it one and two the second point then the algorithm says one to assign every point to the nearest centroid. Here, initially you said centroids are one, two. So if you take every point, so for example, if I were to take this point, which centroid is it closer to? Is it closer to one or is it closer to two? One. One, right? So you say it belongs to one. So in other words, every point votes or becomes the constituency of one of these two centroids. When you have done this, you would agree that the entire data set would have bifurcated into two sets, one belonging to centroid one, the other belonging to the other centroid, isn't it? So with that being true, would you agree that this would all these points would belong here and all these points would begin to belong to centroid two? Does that make sense guys? Then it says that the algorithm basically says that aha,, that's pretty lovely. And you're pretty much done. The third step is find the centroid. And so when you assign each point to the nearest centroid, i.e. build the two clusters, right? Build the two. You build the two clusters. Now, the third step is find the centroid of each cluster. So if you look at the centroid, what is centroid of a cluster? The center of gravity, the center of mass or the center. Basically, it is the central, the mean point, the mean value of x1 and the mean value of x2 of x1 x2 for each cluster so if i look at this picture let us draw the mean would it be fair to say that the mean would be uh let's say somewhere here this is the uh let's say somewhere here this is the this is the mean or the center of gravity or the center of this cluster and the mean of this perhaps is somewhere here would that be reasonable guys yes it would be reasonable so what happens is that you call these the new centroids, find the centroids of each cluster and these become the new centroid, the new centroids, right? And then what you do, and here's the part that is interesting, you keep repeating this cycle between two and three, right? The two-step part, first assign all the points to the nearest centroid and then find the centroid for each of the clusters. You keep doing this, do this, repeat till, each of the clusters. You keep doing this, do this, repeat till, till you feel like stopping. Well, that's a very vague word to use, feel like stopping. Machines don't feel, but I'm just using an intuitive word, feel like stopping. For example, you can have many stop criteria. Stop criteria could be, criterion could be one, that the centroids are not moving. Do not move much. Or it could be that a points are very few points are changing loyalties, like very few, very few points are moving from one centroid to another, right from from one centroid to another, right? From one centroid, from one cluster, I should say perhaps, from one cluster to another. Are we together? So all you could do, or just arbitrarily pick a number or just arbitrarily pick a number or arbitrarily really arbitrarily this is butchering the spelling arbitrarily pick the number of iterations if you're pretty sure how long it will take, iterations is equal to, I mean, whatever, you can say five, 10, whatever, whatever suits you. And now you say, well, if you pick the number of iterations, what if you're not done? It is possible. Generally, k-means class converges very fast. Like for example, in this case, you went from this to this and from a random guess to the correct answer quite literally in one step. Isn't it guys? If you repeat it, you're not going to change your position or your clusters very much. So this is a happy example where the data is clean, your initial guesses were good, right? clean, your initial guesses were good, right? And because your initial guesses were good, you quickly move to the centroid. Now, there is another, but it may not be true. Let us now look at the same data, but with bad guesses and see if it still works. bad guesses and see if it still works. So suppose I take this, I'll redraw this cluster, this diagram and see what happens if the data is not what we think it is. Or I mean, data is what data, of course, I apologize, but the initial guesses are terrible. That's a really unfortunate in making the initial guesses. So this is I hope a close approximation to the original data. Suppose you have this. Okay, this is suppose you were unlucky and your initial guess was just by misfortune, happens to be, let us say these two points. One and two. Well, now what? You realize that you have a problem here. These two are very close to each other as guesses. The question therefore is, will this work? Let's find out whether it works or not. So what we again do is we associate all of these two points. When you do that, you will initially discover that your first cluster is like this. you will initially discover that your first cluster is like this. These points will belong to one cluster and these points will belong to the second cluster. Can you see that? If you assign all the points to the nearest centroid, given one and two as the initial guess of centroid. This is the cluster you'll come up with. Does this make sense guys? You agree with that. So now comes the next thing. Let's find the centroid of the second cluster. The centroid of the second cluster would be the center of gravity here. Would it be reasonable to say that it will be somewhere here? Two. Guys, is that reasonable? Yes, sir. Yeah, that could be reasonable. Now comes the tricky part for one, where would it be? Would it be reasonable to say that the centroid would be maybe in this empty space somewhere somewhere here as a guess would this be a reasonable answer guys yeah but that looks reasonable but we don't have any data points there that is right we don't have any data points there. That is right. We don't have any data points, but that's perfectly okay. Our algorithm didn't say that you can't put the centroid where there's no point. This is one of the things we allow for. It looks very odd, the most empty space we are calling a centroid. But so be it, we do that. So this is the second step. Now let's use a different color. Now that the centroid has moved here, let us redo the clusters. So it is clear that, let's say that this point, if you let's make the cluster for two, you would agree that most of these points would have gone to two. And most of these, the rest of these points and all of this would go to one. Are we together guys? So the journey has been, the centroid has moved. The two has moved to this. One has moved to this. And now the pink or red color, whatever this is, is your new clusters, the two new clusters so far so good here so here actually the one can move to either to the uh second centroid or it can come even to the first centroid no sir there will be a two option for that in that case because uh in the first group so we have taken the complete thing so which is it can come to both the center i feel i don't know i just like a thing uh i don't know see the center of gravity is the mean is the mean of points the location of points so in the beginning you have a lot of points here and a lot of points here so it the so let me just call this ducks, right? This was the ducks, let us say, and these were the cows. You cannot have one move all the way to the ducks in one step, right? In the beginning, because see, look at this, yellow points you started very close to each other. So in the beginning, one, the yellow one, will end up owning half the cows. And the, right? But this, and it will own all the ducks all the cows and half i mean half the cows and all the ducks but then what happens is when you find the centroid one moves to this pink colored one right the centroid is now here manisa are you getting that? Sir, actually, no, sir. Actually, what you explained is like, I don't know whether I'm correct. So here the centroid for the one, first one is the one which you mentioned in the yellow color. I mean, the one. So because the both the thing are close to each other so the first centroid can be either to the uh the one which you have placed in the center and it can either be to the one which is next to it only it can be both right so it cannot be only to one i guess let us redraw let us redo the entire thing over again. I'll erase this and I'll redo the whole thing. Let me see if I can redo the whole thing. Suppose you take these two. Unfortunately, these two are your initial guesses. Centroid guesses. So this is an unfortunate pick because of course, if you look with your eyes they look as bad a choice as they can be isn't it but our rule is we still have to give the points to one every point has to belong to one centroid or the other so let us take this point where my mouse is which centroid do you think that belongs to this point let me just call it a where does a belong to which centroid does it belong to one or two two what about this point the one very close to it where my mouse is this belongs to two two so how about this guy Two. So how about this guy? It belongs to? Two. It also belongs to two. But what about, so let me just call this C. What about this point? D. Which centroid does this belong to? One. One, exactly. So you have switched over. So how do you decide who belongs to that? One easy way geometrically is to take the perpendicular bisector of the line connecting to these two centroids, take the perpendicular bisector, and you would realize that everything to the left of this line belongs to one and everything to the left of this line belongs to one and everything to the right of this line belongs to two would you agree so for example if you take this point which is it likely to be closer to is closer to one isn't it than to do right so suppose you look at this point E, which cluster does it belong, which centroid does it belong to? One. One, right. And continuing that argument forward, here is your perpendicular bisector. These all of these points, these dot points, which cluster do they belong to? Which centroid do they belong to, one or two? Which are they closer to? One. One. Marisa, are you agreeing with that? Yes, sir. Now I got it. Now you got it, right? And so the first time around, your cluster would be something like this. Let me bring back those white points. Your cluster would be something like this thing. This entire thing is your cluster. One cluster, even though it has a lot of empty space and this part is the other cluster, now what is the center so now I'll remove this ABCD maybe they're clutter maybe I'll leave it let's leave it there now ask this question of of the centroid 2 and the cluster 2 so let me just give it a name, cluster two, and this is cluster one. Tell me, where is the centroid for cluster two? Would you agree that the centroid for cluster two would be somewhere around this point? Is that where the center of gravity of those points are does this look reasonable guys yes sir yeah so two has moved here what about the centroid for one the one the cluster one owns half the cows and all the ducks so so far cluster one is a rather rich farmer has a lot of cows and all the ducks. So, so far cluster one is a rather rich farmer, has a lot of cows and all the ducks. Cluster two has only half the cows. So now where's the center of gravity of cluster one? The centroid, where would you put it? Would you say that, yeah, somewhere here. Let's say that it's somewhere here right would it be reasonable to put it here yes sir yeah so now that it is here let us repeat the process let us assign all the points to one cluster or the other so we realize that when all the ducks will certainly go back to one, they will remain with one. What about two? In two, you will see that loyalties of some cows are changing, right? What will happen is, if I look at the perpendicular bisector of this, the perpendicular bisector is going along this line, right? So now these many cows, is going along this line, right? So now these many cows all belong to two. More cows now belong to two. Do you see that? Some extra cows flipped over loyalty to two. And these many, this is the cluster for, let me just call it cluster one and this is cluster two are we together guys would you agree with this so what do you notice that cluster two is picking up points, cluster one is losing the cows. Now let's go over to the, let me use another color now, I'll use blue let's say. Let us find the centroid of two, cluster two. The centroid of cluster two perhaps could be this point. Would it be reasonable to say that? This is two. And what about the centroid of one? Would you agree that it would have shifted to near this? This is one, isn't it? And now, once again, you find the cluster for two right so when you do the cluster for two let's join the two with the line and do a perpendicular bisector here and you realize that what happens now two two ends up owning all the cows. Do you notice now two owns all the cows? Would you agree? And one ends up actually it owns this empty space also, but I won't focus on that. It ends up owning all the ducks at the bottom. So this now is your, where am I? This is now your cluster one, and this is now your cluster two. And now let's find the center of gravity of, let me use one more color. What could be it? Maybe green. I'll take green. Let's see. Green. This is it. Okay. So now what is the center of gravity of this? It would be this. Would you agree that this is the center of gravity of one? And the center of gravity of this cluster is perhaps somewhere here or somewhere here too. Does this look reasonable guys? And you know that you're pretty much reaching the end. Those cluster, it's the, now the points will not be changing loyalties, right? They will all stay within the cluster. And so the center of gravity in the next, or the centroid in the next iteration is not likely to move. And therefore you have detected the clusters. You can say, I can stop now. I have two clusters and my clusters are these clusters here and the clusters here. These are my final clusters. Right. This region is one cluster and this region is my other cluster. Right. That's how you do it. Or actually, more precisely, what it does is it also divides the empty space. So I join these two line between the two. I will have a perpendicular bisector. So you would say that everything on this side, even though there are no points, like logically speaking, we are doing something like this. We are dividing the space into two parts. One belongs to cows and one belongs to ducks right that is why these algorithms are also called space partitioning feature space partitioning algorithms people don't use the word features prefix so often partitioning algorithm, K-means. K-means is an algorithm that clusters by dividing the feature space into regions, one region for cows and one for ducks. Are we together here? So it's a beautiful, the K-means is a very simple and elegant argument for finding clusters in data. You notice that even when you make very unfortunate choices, you can still discover the clusters, isn't it? You have a journey. at the movement of one one move from here to here to here and from here to here right you see how it migrated the centroid of one migrated the centroid of two didn't migrate that much it went from here to to where to here and from here to here and from here to here, not much movement in two. But centroid one moved quite a bit. But at the end of the day, you settle down at a point at which the centroids have been discovered. So that is a power or the beauty of K-means. Now, there are a few limitations. One is, do you notice that if you make bad choices, it takes more number of times to find the answer, isn't it? For example, in the first one, where we made good choices of the guesses, good guesses of the centroids in the beginning, we converged much faster. On the other hand hand when we made not so good guesses it took a while for us to converge to the centroids right so the the run time of your algorithm therefore becomes proportional to a random process you know random choice of guesses that's one limitation There is another limitation which is pretty actually bad, which is, and I won't go into that, sometimes you end up with cluster centroid choices in the beginning guesses so bad that it ends up sitting on the perpendicular bisector of the real clusters. And so as you can argue if you're in the perpendicular bisector you just get to a very unstable situation in which the clusters that you discovered are bad clusters they're not good clusters so for example if you end up discovering a cluster halfway like this this is one cluster and this is another cluster the lower part Now, what happens is when you do that, your cluster centroids don't move. In both the cases, the centroids would be somewhere like here and here, right? So this will become like one and two in that situation. And when it becomes that, there is no further movement, but those centroids, as you can see, they are terrible, right? They hardly, I mean, those clusters are not real clusters. There is no further movement, but those centroids, as you can see, they are terrible. I mean, those clusters are not real clusters. Just one look at it and it will tell you that this somehow is not right. You have picked up half of one cluster and half of another in one cluster. And in the second cluster also you have done the same thing. So these cause instabilities. So people have found ways second cluster also you have done the same thing so these cause instabilities and so people have found ways to solve this problem many clever ways people have found so one clever way they can they can argue is so so i'll just erase this part of this bad clusters that i drew let me just erase this part. These are bad clusters that I drew. Let me just erase it. So one way that you can do is whenever you go, or maybe I'll do it in the next part here. So let me draw this again. I think the previous drawings have become cluttered. Suppose I have this. Again, I draw these points. have become cluttered. Suppose I have this again, I draw these points and I draw these points here. And so let us say that I randomly pick a point, pick a point point let's say somewhere let's say that i pick this point as one one suggestion that often helps is pick the second point which is as far away from the first point as possible so what would that be it cannot be one of the ducks you'll end up picking maybe a cow this one will become your two would you agree this is this seems to be the furthest point this cow seems to be the furthest point from one would you agree with this yes so then what happens is as you can imagine that this will converge very fast because in one hop you will end up declaring this sorry let me use the same color you will end up using this as your first cluster and this was a point and this as your second cluster, and this was a point, and this as your second cluster. And then in one hop, you would get to the right answer, which is that one will move to this guy and two will move to this guy, and the clusters will remain the same. Do you notice this? So it's a clever trick. You can, works it doesn't always work sometimes it works there's no guarantee that this a process itself will work right but it does seem to work quite often and sometimes people use this then sometimes people will do other tricks like for example they'll say you know what I need a way to to measure the quality of a cluster how good the clusters are and so there are mathematical measures one of them is cluster sum squared distance. What it means is, suppose you're given points, four points, right? Or five points, how many? Or did I make six points? Find every pairwise sort of a relationship. I don't know, did I miss something? I'm sure I missed something. Yeah, missed this. I missed this. So you see you have a lot of interconnections between the six points. For each of them find their distance. Let me say d1, d2, all of these distances. And take the average, or take the within square, some square distances, just add d1 squared plus d2 squared, et cetera, et cetera. And sometimes you take the average, you divide it by total number of points, let's say 1 sixth, right? So you take this distances between all the points. And then what you find is, and do it for the other cluster also. So it's a summation in mathematical notation. It will be a summation over each of the clusters. And it will be a summation over every pair of points, distance between every pair of points, d i j, belonging to a particular cluster square d i j square belonging to whichever cluster it belongs to you do that this is why is this important the reason it's important is suppose you make the mistake and you pick suppose the points are here let me just make a few points not too many points to illustrate this suppose your points are here few points, not too many points to illustrate this. Suppose your points are here. I deliberately put them in a line for the time being. Let's say like this. Little bit like this. Suppose you made bad clusters. Suppose you discovered clusters like this and this. So let's look at the cluster one. You would notice that if I try to do paired distances between every two points in the cluster, you will end up with these big lines. Do you notice that? How many long, big lines I'm drawing compared to the other one? And we can keep going with that. So you realize that these lines are much, much longer than in this situation when the clusters are clean. So your WSS here will be large, right? And so if what you do is you pick the WSS cluster, minimize WSS. How can you minimize WSS? What you do is you keep on doing the clustering a few times. Let's say that you do 10 tries. So you build 10 random clusters. And then you pick the minimum WSS. Now, in practice, what will happen is the probability that you'll make a blunder like this is very, very low. So what will happen is at least nine times out of 10, you will pick the right clusters. So nine values will be very close to each other and the 10th value may be off if you're unlucky. It is very, very, very unlikely that all 10 choices or tries would lead to very unfortunate clustering results. So this is the way you can do by increasing the number of tries. And so when you use it in scikit-learn, the Python library that we are going to use for clustering in the next session, you will notice that there is a parameter. It will ask you, K-Means will ask you, how many clusters should I find in data? And the second question that it will ask you is how many times should i try to find clusters right so that it will look into the wss and try to pick the best and so on and so forth so number of tries matters and so are we together okay so that is for k means clustering i i believe oh all right it's already one and a half hours so we probably cannot cover all of this in one day so you know what today i'll stop with um one algorithm because each of these are conceptually big how about this we repeat this and i will early release the code for this clustering so that even before the next time's lab you guys can review the code try to read it on your own and then i'll walk you through this code next time right but so clustering is a very beautiful thing it's a beautiful algorithms in clustering to find clusters in data has amazing amazing scientific value it is heavily used in medicine, in medical fields, medical research. It's heavily used. And I mean, I can't think of any field that does not extensively use it nowadays. And so we learned one particular algorithm, which is the K-means clustering. So any questions, guys? Hello, sir. So here about the clustering what you told us so you mean to tell the number of tries will give us the more accurate clustering model so like because you told that uh when we are working on this clustering it will ask like how many clustering we need to find so for example if you press something like 10 i'm not sure so that shows that there are 10 clustering and i mean the result will be more accurate no no no no see you mix up the number of tries with k don't do that a k is asking how many clusters should i find let us say that you always want to find two clusters but you realize that when you are trying to find two clusters you might end up doing very well one time and not doing so well the next time right so what happens is you need a way to find out how good was your clustering and you can use it to just you know try it a few times to make sure maybe two three times to make sure and pick the best clustering that you achieve for k is equal to two. But the question still remains, what is k? What is the correct number of clusters? This algorithm, k-means clustering, is silent on that question. It doesn't answer that question. It says, you have to tell me how many clusters there really are in the data. If you tell me how many clusters there are, I'll find it for you. And if you make a mistake, if you say, suppose there are two clusters, cows and ducks, right? So here, suppose you give me an answer like five or three, I will find you three clusters and the best possible three clusters in the data. And then don't come blame me because you said three or you said five, but I'll give you the best five clusters. That's what K-means clustering is saying so if we have data like in circular pedifiller I mean two into circular ways the data is distributed. Right. So in that way, this game is able to find the cluster. Yes, yes, very well. So it's a good point you raised, Rahul. When the data, what you call circular, the people use fancier terms when the data has a convex, well-defined convex hull, or when people have a data is very convex. I use a simple term, globular, secular-like. For example, an ellipse will do just fine, isn't it, Rahul? Doesn't have to be a circle, an ellipse could be. So more general definitions are globular, roundish, some form of roundish, or convex shape. When it is convex, K-means clustering works wonderfully. It does a very good job. The trouble with K-means happens when the data is not globular or when the two clusters overlap. So with cows and ducks, it's easy, but what if you're talking about cows and buffalos? You realize that there would be a certain degree of overlap, right, between the two clusters. Or cows and lions. So those are the situations where it becomes a little harder to tell whether a data point you should put, which cluster it should go into and so forth. So those are some of the limitations of K means. And the biggest limitation is it answers half the question. You have to tell it how many clusters to find. Once you tell it that is the K, then it will go find it for you. But it's amazingly simple. Actually, if you think about it, I wrote down just three steps or two real steps, a two and three here, assign each point to the nearest centroid and then find the cluster associated with that centroid and then repeat the process. It can't get simpler than that in machine learning. This is perhaps one of the simplest algorithms and yet also an extremely powerful algorithm. What simple and powerful. It is one reason you find this used extensively in the industry. Very, very commonly used in all sorts of problems in the industry. And as I said, there will be clusters in your data most likely, right? And this answers that important question. How do you find those? So you can say that K-means is a cluster. K-means cluster is what we studied and the process of finding clusters is called clustering uh so one more doubt i have sir so you told that depending on the model for example if i tell i have found some three clustering then if my friend she tells that she has some five clustering, so then how will come to know that it is a correct model? So like we doesn't know the accurate cluster considering right in particular model because my guessing may be three and her guessing maybe two. And finally, what will be declared a correct cluster in that case the the answer to that uh is and that is something i'll tell you about okay maybe i should do that uh it's it's a little techie i mean if you get it it's all right if you don't get it doesn't matter remember i told you this wss measure what you do is you you this method is called the elbow method What you do is you this method is called the elbow method. Elbow method. What it says is for each of the value K is equal to two K is equal to even K is equal to one K is equal to two K is equal to you know 334 You can keep going, right? Five, six, seven, eight, nine, 10. You can keep, what you do is you find the WSS, this measure that we talk about. And then when you do that, then you end up with a plot, like for one, you'll get a certain value. For two, you'll get a certain value. For three, you'll get a certain value. For four, you'll get a certain value. For five, you will now get a certain value for three you'll get a certain value for four you'll get a certain value for five you will now get a certain value six certain value so what happens is these values will still go down keep going down down down but have you noticed something interesting at this point if you think of this as your hand you know imagine that this is your shoulder and this is your hand what is this this is your elbow isn't it if you imagine your hand being like this I don't know are you getting the impression so imagine I won't make such a big hand let's say that this is it these are your fingers these are your fingers I'm not a bit I'm not a great artist imagine that this is your hand with your muscles and so forth what is this point it's your elbow isn't it guys have you seen the intuition here yes sir yeah so what happens is if you look at the WSS plot, you will usually find an elbow at some point and that elbow, wherever that is, that elbow is, let's say it's here is pointing to four. So you would say that the best number of clusters to find in the data is four. So there is this elbow method is a more formal name, mathematical name, scree plot. You can remember the scree plot if you want to, or more intuitively, just think of it, it is called the elbow method in common language. In more formal literature, you call it the scree plot. So this is it. And you can plot it and just look for the elbow and that will that will tell you between you and your friend which is the better answer to pick three or five that you gave the example does that answer your question yes sir thank you that's that Yes, sir. Thank you. That's that. All right guys, so that is it. If you have any questions more, any more questions, you can ask me, otherwise we'll call it a day. I hope you found this topic interesting. It's a very beautiful topic, clustering, and the algorithms here are very, nice uh well just just remarkably uh powerful and simple algorithms so in the previous session you had given us some homework homework regarding that one hot coding so we need to find what exactly is that yeah one hot coding and did you do that actually i was not sure with that but i think it is something related to binary number zero and one where all the coding will be taken to zero two and one one being the higher field no okay one hot encoding so all right let me answer your question. One hot encoding. So suppose, you know, there is a feature X, I, whose values are, let's say that you're looking at the ice cream, you know, go back to your problem of sale of ice cream on the beach. Then your features are temperature this is your x1 x2 is wind speed but x3 is day of the week day of right so your numbers would be here would be a number for For example, this is 21 degrees, wind speed is 17 miles an hour. But this could be like Tuesday, Wednesday, Sunday, right? Then maybe Monday, something like that, the values. So what do you notice and then you have to predict here the y is ice cream sale sale of ice cream are we together this is also a number let's say 150 buckets right 152 buckets sunday will be 300 buckets, Monday is 140 buckets, or things like that. So now the problem is when you try to put it into your algorithm, your algorithms will protest because it will say, well, you know what? We don't deal with textual. You know, these values are not okay. These are strings. These are characters. You know, these are not numbers. Math only deals with numbers. So how do we convert these things into numbers? We have a problem. So what you do is you actually do what is called one-hot encoding. So this x3 is problematic. Why? Because at this moment, x1 belongs to real, x2 belongs to real, but x3 does not belong to or does not belong to real. So the way you solve this problem is you say all right i'm going to create binary features i'll say is sunday is monday is tuesday and so on and so forth is uh saturday i i created these seven features. And so what will happen is for any given data, let's say that this is data 123. Let's look at the first row of data. Which of these values will be true? This is Tuesday, right? I put a one here, 0000, right? And then let us say for the third data set, I would have, it is a Sunday, right? So it would be one, oh, second data set would be Wednesday. So here we go, is Wednesday, Wednesday, right? So here, one, zero, zero, zero. For the third data set, it seems to be Sunday, one, zero, zero, zero, zero, et cetera. You get the idea, right? so now what has happened is this 3 has become multiple variables let me call it x 3 x 4 x 5 x 6 x 7 right and so forth and so on and so forth right now are now these values are the numbers they certainly are numbers right they're zero and one do you see that yes sir yeah and this is called one hot encoding because now instead of a three-dimensional space of input the third one breaks up into seven parts. So it becomes seven plus two. So you will end up with the input space. Now X vector belongs to a nine dimensional space, two of the dimensions being temperature and wind speed. And the other seven dimensions being the one hot encoding. It's Sundays, Mondays, Tuesdays, Wednesdays, Thursdays, Fridays, Saturday, seven of those. And so this now is a number. And now you can continue on with your machine learning. So that is the meaning of one hot encoding. Thank you. That's the meaning of it. I'll stop the recording now. I have a question on regularization. Can I ask? Of course you can ask. Actually in regularization, we are adding penalty to the cost function. function right so in the lausso regression uh how uh it will eliminate the some features one second so please what you're saying is regularization repeat your question i'm sorry i was a bit distracted uh in regular in regularization we are adding the penalty to the cost function, right? That is one way of doing it. In your book, the two regularization methods that I mentioned, ridge and lasso, both of them add penalty terms to the loss function. Yeah. There are many ways of doing regularization, by the way. That's not the only way, but those are the common ways. That's not the only way, but those are the common ways. In the LASSO regression, we are adding absolute magnitude of coefficient. But how it will use it for the feature selection? No, no, don't think of it like that. LASSO is unique, the p the riser because what it does is, and there's a beautiful geometry associated with it. The book talks about that. See, it's a little advanced for this class because there are people in this class who are from non-computer science, non-math background. So I didn't teach it. But the basic thing is this. Lasso is unique in the sense that it throws away some of the features. And so it helps you do dimensionality reduction by limiting your analysis to only a few domain, only a few features, a few dimensions yeah how it will make some feature elimination like i i read somewhere like it's the feature spaces will make it as a zero that's right it does that is see it has to do with now i'll have to talk about the geometry of loss function and so forth how much do you know like where should i start do you understand the of the loss function yes yeah yeah so you understand that okay so then i can explain to you the answer to that once again my writing board again second back my writing board again all right let's try a little here see what happens is suppose you have i'll take the world of two features x1 x2? What happens is that the loss function will form contour lines, right? It will form, suppose the absolute minima of the sum squared loss is here, right? What happens with loss function is, the way the regularization just says, don't find the absolute minima, which is at point, let me just say a right instead a take a less than the best minima don't find the absolute best solution find the best solution that you can possibly find within a region like this in a region like this are we together yeah and when you do that you you be so you know the story in Ramayana of Panchavati Ram and Sita and Lakshman they are in exile yeah one was there and. And so there are a lot of dangers in the forest. So Lakshman draws a Lakshman Rekha and tells Sita don't step out of this magic circle. And of course, the story is that she out of compassion, she ends up stepping out to give arms to this Ravan, the demon, and who ends up taking her away, risking her way. So somewhat like that, think of this region that is in here. You basically set a region around the origin and say that whatever the solution is, I must find it within this sort of Lakshman Rekha, it is within this region of acceptability. One word people use it is within this region of acceptability. One word people use it is that this is your constraint region or the constraint surface. So then what happens is if you can find the answer only here, the best answer that you can find will be, if you really think about it it will be at this point uh at this point right because any other contour any other solution that goes into the region these are contour lines these little circles so each outer circle is is the loss is more than the inner circle so it's a not lesser solution a is the absolute best solution but if i tell you a is not acceptable you need to find the solution within the constraint region then this yellow point is the best point you can come to isn't it yeah but now look at yellow very carefully what does yet that yellow point says what what is the value of x2 at the yellow point what is the value of x2 0 0 so suppose x2 was the weight of the cow or weight of the animal what have you just found you said that in in the solution yeah the weight doesn't matter it's zero that feature has gotten eliminated okay got it that's the intuition behind behind lasso doing a dimensionality reduction because you tend to you know these are these pointy edges you end up punching the lost surface right at one of those pointy edges you're much more likely to and when you are more likely to puncture it at the pointy edges those pointy edges are sitting on the axes so it means that some some feature values are zero at that point and that's how you get dimensionality reduction okay oh thanks yeah i got a clap here nice any other questions guys if not then i'll close this meeting uh prabhat and Prabhat and Ravi, Hurley and Suresh, please stay back. Thank you, sir. You're welcome. Thank you, sir. Thank you. thank you yes if how are you guys benefiting i've taken provide you i remember and murli i don't know if murli was there you were there first time i gave this workshop way at the time in evolve you remember that no i i was not there you would so you are listening to this for the first time is it first are you finding this are you finding this easy to understand yeah yes very much clear and easy relations very simple I can easily understand that Oh gosh, I didn't hear a thing from you guys Murli your sound is echoing Our no No, no, no Hello, is that okay? No, no, no No Murli yes if it is it is really useful and i think we are getting a lot of the things clarified and the the one you explained now the last function that you have, we, that is also, you know, I couldn't find it anywhere. But yeah, I found it clarified. The guy asked the question. Yeah. Me as well. Yeah. Not clear. I could do that book guys. It's a very element book. It's the kindergarten of machine learning. If you don't do that book thoroughly and if you don't have every single line of that book in your mind, then you really will have a bit of a struggle in this field that book was deliberately written to be the doorway to this field as if you said you're going to release the code for the Cayman's clustering some some yeah I see this one one hot encoding you said like the main purpose of that like let us say we can also put like 1234567 right for Sunday Monday 1234567 you can't no I'm telling you know yeah so the main purpose is like because if we put 1234567 it will count the weight is so for that purpose we do or not that's the main reason because the moment you put let's see in math we talk of the ordinal order those are ordered numbers because in order four is more than three yes and that is the problem you want to keep it as just binary either it is or it is not yeah nice guys anyway just wanted to do a check with you guys if you are benefiting from this yes i say we are benefiting means i actually i went through the book earlier but yeah now it is like more clear for us good all right guys and then you are a data science you are part of the data science team so you better pick this up over time it will come to you see machine learning is view, the easiest part of computer science because it's analytical. So there are no bugs. You just reason through and you're done. You're not writing a very complicated user interface code in which when you click this button, maybe something else will happen that you didn't expect. Okay. Enjoy the rest of your Sunday. Bye.