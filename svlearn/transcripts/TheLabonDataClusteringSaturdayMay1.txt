 I should share the screen. Here we go. Today, our topic is clustering. Just to recap, in the last lecture, which was on a Monday lecture session, we covered two things. We covered k-means clustering and we covered hierarchical clustering so before we continue into the lab just a few things to summarize first is clustering is different from supervised learning algorithms it is it is not predictive modeling it is something different what is different about clustering would one of you like to volunteer how is clustering different from classification or regression what does that word mean like in what sense is it different? Like, without the words, in simple words, like, without using terminology, in simple words, what is different about clustering compared to classification or regression? Asif, is it like a grouping of data points? Like, you have different instances of the data points. You're trying to group it yes that is one way of putting it yes that is correct it's the correct answer yes no no prior uh training data set or like instructing the data like how to figure out this data set it just that that too is part of the let's go ahead is it pattern recognition versus modeling yes very good that too is part of the correct answer for all of it like like I think is like so similar properties or features will be in a similar cluster uh so if the properties of those data points are different uh most probably it will be in a different cluster we're not interested in predicting right so basically that's the point so all of you have essentially touched at the answer in the right way let me summarize it in predictive analytics there is always an input space and an output space a target space the input space is the feature space the feature vectors and then you're based on the feature vector for each uh value of the feature vector you're predicting a target value either a real number measure let's say how much ice cream you would the boy would sell on the beach or whether it's a cat or a dog or it's a cow or a duck I mean identify the type the categorical type so it is a mapping from an input space to an output space to learn that mapping or that function this is a mapping from an input space to an output space. To learn that mapping or that function, this is a predictive model. You predict what the target value will be. Now, to do that, you need to start with training data from which the algorithm learns. That is the inherent nature of predictive modeling or supervised learning. Those words are essentially synonymous on the other hand in so-called unsupervised learning or pattern recognition what you have is no output space you just have the data space you don't distinguish between input and output space all the columns of your data if you were to think of it as a data frame they all are part of the feature space so given the feature space the question that you ask in in a pattern recognition is is there a pattern in the data one of the simplest patterns that you can find in the data is that data may be clustered they may be near each other data points based on some measures of similarity for example if you look at lots of elephants and lots of ducks in the feature space of size and weight all the ducks will be clustered around one region and all the elephants clustered around another region. But suppose you didn't know that there were different elephants. You're not thinking classification. Even if you don't, you would still in the data space find two different clusters of data. So those clusters, they are points that are similar because of some reason. You don't know the reason like for example in the case of a cow or I mean a duck and elephant it's very clear they're two different animals but when you don't know when you don't have the target variable itself all you know is that there is something that makes data points sort of flock together or group together in different groups and that is clustering you find those clusters the underlying belief is that between the clustering may or may not be because those points are inherently different sometimes just by the nature of the the physical, they're different. For example, the stars form galaxies. The two galaxies may not necessarily be different in terms of the astrophysical attributes. They just happen to be two different galaxies. They may share a lot of things in common. You don't think of them as cows, I mean, or ducks and elephants. Inherently different things. They just are different. They just happen to be not different. They just happen to be in different groups because of the dynamics of formation. When you see, likewise, if you see the beach and you see children have created piles or clusters of sand, those sand hills are not necessarily different, but they just are clustered. And so that is a subtle shift. When data is clustered or grouped in the feature space, there may or may not be a difference between the clusters. Sometimes there is. And when you have a difference between the clusters, it actually gives you a lot of insight. For example, today, one of the most interesting areas of research is, can you use unsupervised learning in particular clustering to do classification, for example, or regression, classification in particular. So, for example, if you can tell that this cluster is made up of people, let's say you're looking at some biomedical markers and you find one cluster of people and you happen to notice that these people all have diabetes and another cluster of people and they don't have diabetes, you might have stumbled upon a way of classifying by clustering so that is a happy accident or happy circumstance you look for that and so you have used clustering to actually do classification but it is not necessarily so clustering may be done for its own sake you may just be clustering data. So that is what we'll do. The other form of pattern recognition is what we'll do next time. It's called dimensionality reduction. Sometimes you find that in the feature space, most of the data is sticking to a lower dimensional surface. So in a three-dimensional world, most of the data may be sticking to a plane, for example. And when you do that, when you find the plane near which most of the data is sticking, you say that you have, because a plane is two-dimensional in a three-dimensional space, you say that you have reduced the dimensionality of the data. For all practical purposes purposes you can think of data as belonging to some point in the plane and you can put a coordinate system on the plane and say well you know i can give instead of giving xyz i can create a new coordinate system with respect to the plane and just give it with respect to xy on the plane that is that is dimensionality reduction we'll do dimensionality reduction next time but today is the lab of clustering. So usually when you do clustering and when you do it in your books, most of the problems that you are given, and quite often the problems that you get in real life, are very simple problems. And so these algorithms, the k-means clustering and algorithmic agglomerative hierarchical clustering, they tend to work very well. So it gives you the warm and fuzzy feeling that these are very good algorithms and that's all you need to learn. Unfortunately, I would say that most of the standard machine learning textbooks, they give you the fallacious belief that this is all you need to know of k-means and agglomerative and there are some esoteric clustering algorithms but um you can get away with these two in fact these are the only two they cover actually i i don't want to do that so instead of taking simple examples, I've deliberately created an example which illustrates the entire complexity of the problem of finding clusters. We will deal with this particular dataset. I've created this dataset and you'll see it. You will see how different algorithms deal with this dataset. What I want to focus on is where it succeeds and where it fails are we together is this uh uh notes shared like i was not able to find this clustering algorithm yes yes i'm going to share it add it to the tabular data right after i thought i'll walk you guys through it and right after actually I should have done it in the morning but I had a busy morning today I was helping my daughters and so forth so with their studies if you can't download this document from the tabular data is that yeah you can't but you will get your book a physical book which could be much. The reason I'm preventing it is because I'm updating this document every day. So whatever you download is obsolete. You may think that you have the latest doc, but it isn't the latest doc because I've just updated it and so forth. So wait a bit. You'll have a full physical colored bound, heart bound physical book, which will be much more fun than just having a pdf around you so just bear with me for a couple of weeks and then you'll have access to this so all right guys so are you able to read it online though like in your laptop are you able to read the notes properly in your laptop are you able to read the notes properly i hope you guys are able to anil you're able to read the notes online isn't it uh yes okay good yeah so all right so what i have done is taken a data set which i created myself it's called the smiley data set it's there on the github the class github myself. It's called the Smiley data set. It's there on the GitHub, the class GitHub. Go to the GitHub and you'll find it there, ML100. Now, one thing for clustering, for me, whenever I think of clustering, I suppose because I come from a, you know, I have a very strong fondness for astrophysics. To me, clustering always evokes the images of galaxies. So I put this picture of galaxies there. You can see, but naturally you see nature forming clusters and so forth. So we'll do that. So here we are. We are taking this smiley data set. What do we do with any data set the exploratory data analysis we load the data so it's a two-dimensional data yes to division now I have given you a target T variable which you should ignore which actually tells you that if you did the right right clustering then they are actually seven they are actually eight clusters and data should belong, each of which point should belong to which clusters. I've given an answer here. We will see how well our clustering algorithms that we learned is able to deal with it. You know, we load the data. I'll give you the solution today in both R and Python. I will walk through both. R is simpler, quicker, So let me go through R. Loading the data is easy. Summarizing the data is easy. Plotting the data is again easy. So this is the plot of the data. I've colored it by the actual cluster identity of each point. And when you look at the data, what do you see? You see a smiley face, isn't it? Now, this data looks deceptively simple. It's actually not simple. It is hard enough that it brings most of the standard clustering algorithms to its knees. And I'll explain to you why it is so. Do you notice that the eyes are roundish? So they have what you would call globular. But nothing else seems globular the eyebrows would you call the eyebrows globular no they have concavity to them isn't it likewise the smile has concavity to it that this is triangular not not exactly globular it's triangular and then they are outlier points do you see this salmon colored points that is equal to zero, this colored points? They are the outliers. They don't belong to any cluster. So spend a couple of minutes thinking about this data and you will realize that actually from a clustering perspective, you get a clue that this could be a pretty tough data set to deal with. Fortunately, in real life, 80% of the time, you won't get complex data set. 20% of the time, you will get complex data set. And so your basic algorithms won't work. So in other words, if you just know what we learned in the last lecture, it can work quite a few of the times, but often it doesn't work. So the clustering algorithm, what did I tell you? In K means you don't know K. So you have to try out different values of K and see where do you get that? What was the measure of goodness of a cluster, guys? Do you remember from the notes? Within clusters, some square distance. Do you remember that? WSS. WSS, exactly. So what this code does is it computes. It does the clustering for each of the values of K from 2 to max, it go up to 15 clusters. And the clustering itself is just one line. K means give the feature, tell how many clusters you want to see. Now, n start is do it again and again and again just to find the best three cluster or four cluster whatever it is a good idea because sometimes the k-means clustering can come up with crazy answers so you should never trust oh do it once you should do it again and again and take the best one it's just a technical detail so um you have this clustering and then what you do is you compute the wss and you record it and once you compute the WSS and you record it. And once you record it, you make a plot of it. The rest of it, this part is just the plotting. The main code is just line 8, which is doing the clustering. So if you know how many clusters to find, 5 for example, you can do it with just one line. Namely, k means feature, and that's what makes R so powerful and simple. You can achieve a lot in one line. In this 24 lines, we have everything. We try every possible value of k. We also do the plotting of it and so forth. So when you do, and I would suggest guys that do the lab and understand this code. Let me walk through this. I'm creating an empty data frame in which I'll store k for any given k, what is the best WSS I got. Then I write a for loop. For each value of k, I go and do the k-means clustering, find the WSS for that value of k, and store it in the table. Once I've stored it, I do the plotting. I just plot plot it out this is just the plotting code right so let's see what comes out a quick question is there a rule to set the maximum k in the no there wasn't actually this is one of the things you have to look for the elbow right once you have the elbow there is not much point in going too deep after that but where you'll see the elbow is a guess so in the beginning you start with 10 15 20 and then see if you didn't find the elbow keep going all right what is a string as factors uh which fact string as factors oh okay uh when you find like if you can if you get string as factors is just the value like when you store k in this k is numerical wss is double when you create a data frame if you get any string as value right should you treat it as a number or should you treat it as a fact categorical in our case we want to treat it as a number because both of these are numbers numericals yeah so even in the data frame we write it as a number we don't want it to be treated as a string by any chance so string actually this fault is unnecessary you don't need to put it because more more as a cast as factor or cast as integer right yeah it is saying don't cast it as a factor yeah leave it as integers yeah which is what we wanted because these are numbers so when you do that let us see what comes out. Zoom out. There we go. Let us see. So we get this curve. You see this WSS versus K. When you look at this, guys, do you see any elbow here? Where do you see the elbow? One between 2 and 4 or the one between four and six. So I think two and four. Anybody else would like to make a guess? Some other opinions guys. Where is the elbow? Which value would you put your elbow at four and six between four and six the five maybe five huh maybe four maybe maybe maybe three as a Harry prayer said so can choose, but is the elbow very clearly visible? No, not clearly visible. So this is a clear sign that either the K-means clustering is not doing a good job or the data is truly not clustered. Well, in this case, we already visualize the data and we know there are clusters. So this is a clear hint that a K-means clustering is probably not doing a good job. So let us look at this diagram here. I've shown the clustering for each of them. And here, let me make it one page at a time. Zoom out. Continue a single page. Let's look at this. So you look at K is equal to 2 how many clusters do you see and what I have done is I put a convex hull or a shrink wrapped the clusters does this clustering look okay half the nose belongs to the top cluster bottom half of the nose belongs to the lower cluster would you consider this a good clustering guys no no let me consider it a good clustering now look at k is equal to three here it breaks it up into three places now you notice that haripriya you said that k is equal to three look like a maybe an elbow point yes It is a reasonably clear clustering, except that it misses the fact that nose and lips should be separate and eyebrow and eye should be separate. But at least it did somewhat of a clean separation. Yes. Right. But still not perfect. You go to K is equal to four. And now things begin to get really bad. The nose and half the lip go to one cluster. The other half of the lips goes to one cluster the other half of the lips goes to another cluster that doesn't sound good at all now k is equal to five or something i think it was changing as you were saying look like a good cluster and it's sort of here at least the nose is a cluster in itself eyebrow and eyes are a cluster but the lips have become two different clusters and so what do you notice that in each of the clustering there are imperfections and you can keep going down if you go to K is equal to 6 it begins to get worse the lips have three parts if you go to K is equal to 7 something interesting happens I the eyebrow begins to separate from the eye do you notice that yellow and this left eyebrow is separated but the right eyebrow is still the same by the time you go to k is equal to eight lovely thing happens eyes and eyebrows are different clusters as they should be nose is a different cluster but it completely fails on the lips the lips get broken up into three clusters, whereas we know that the lips should be one cluster. Isn't it? And if you go to higher values of K, you'll realize that it doesn't get any better. Do you notice that? Sorry. It doesn't get any better. Would you agree, guys? It gets progressively worse after that. So K is equal to 3, K is equal to five, and maybe K is equal to eight are some possibilities. Let's look at this structure. And K is equal to three is this. Yeah, there is a good bend. K is equal to five, a little bit of a bend elbow. K is equal to eight, not much of a bend. So at the end of it, what do you conclude is it useful you would say maybe i could use k is equal to three or k is equal to five uh they're imperfect but we could sort of use them at some level that's all you can say but not terribly useful level that's all you can say but not terribly useful would you agree isn't like the k-mean is to identify like a spherical uh blob globular yes globular cluster so because it is the data set like it does not have blob like look at the lip or the eyebrows like it's not a spherical blob it's a like a different kind of a blob so maybe k-mean is not the right absolutely algorithm making the textbooks tend not to mention this fact or emphasize this fact that k-means works only when you have globular clusters it actually doesn't work otherwise but in real life situations uh It actually doesn't work otherwise. But in real life situations, you don't always get globular clusters. Oh, okay. So the weakness of K-means, and I use this data to show the weakness of K-means clustering, it doesn't work well for non-globular situations. So that is the point, which is why I feel that the textbooks have slightly misguided you. What about now? Let me use a DB scan. We'll do a bit later. Let's do hierarchical clustering. You remember hierarchical clustering. What do you do? You build those dendrograms. Well, there are so many points that this figure is very hard to see. many points that this figure is very hard to see. But you can, what is the quality of a dendrogram? You can have as many clusters as you want, right? You can have one cluster, you can have two clusters. By cutting here, you can have three clusters, four clusters, and so forth. When you do, using a dendrogram, once again, this is the best you can do which is one two three four five six seven eight K is equal to eight very similar to K means clustering the eight K is equal to eight if you cut it out right this is what you get the very collaborative clustering also so the point is that let me go back to a two view yeah so when you do this uh agglomerative clustering also doesn't help you this is the code by the way huh agglomerative clustering is uh just three four lines of code. The code for all of these, good thing with machine learning is, the actual coding part is very simple. What are we doing in the first line? We are saying that the distance should be Euclidean distance, which is a given. Unless you have a reason not to use Euclidean, you tend to use. How do you know what clusters are good? A measure. One of the measures is called Ward hierarchical clustering measure. It is somewhat like WSS that you use to establish which clusters are good and so forth. And then all you do is you do the clustering. You use this method to find the clusters. And then you plot the clusters. And then you you do is you do the clustering. You use this method to find the clusters. And then you plot the clusters. And then you group the clusters. You can then find the groups by cutting the clusters at k is equal to 8. That's all it is. It's a very simple code to do that. And I'll now show you the Python code in a moment. But the lesson that you learn is neither k-means nor this hierarchical clustering are good. So what works? There is a different theory and a very interesting theory that I'll tell you about, which is called density-based scanning. It basically says that clusters tend to be dense regions in the feature space. Isn't it? This is a dense region. This is a dense region. So it takes a very physical argument. It says that imagine that each point had a mass, right? So places which have clusters in the feature space are places where the density will be high. Density of points will be high so all we need to do is find regions that have high density and call them clusters very intuitive it's surprising that this intuitive argument was discovered very late in the last century i believe the year was 1998 when it was all discovered so when the first of these methods called dbScan came out, which is still very popular, when it came out the paper, the guys who wrote that paper, they were immediately awarded the best paper of the year award in this field. Because this breakthrough was considered, this very simple idea was considered a major breakthrough in clustering algorithms. And in hindsight, it looks like such a simple idea, but then you implement it. So the theoretical aspects, I'll talk about it next time. But how do you do? Well, the lines of codes are even simpler. Three lines of code, three, four lines of code. One line of code does the clustering and what this feature mean epsilon and minimum number of points mean i'll explain to you later just assume that we are doing based on regions that are dense you cluster find the clusters you plot the clusters that's it and then you say okay let's see these are some things like And then you say, okay, let's see. These are some things like local outlier factor. What do they mean? I'll talk about that later. Local outlier factor is something far off and so forth. Is it a, et cetera. So we'll talk about that later. These are theoretical points. Just focus on the first two lines, clustering and plotting. Line number two and line number four because the rest of it will make sense when we do the theory and so when you do the clustering this is what it says the clusters are what do you think this is good clustering it has defined these points outlier points it has left white in color because it is saying they don't belong to any cluster and all the points that belong to cluster it has colored as different points so what do you think of this clustering guys would you think that this is a good clustering uh yes it certainly seems to have hit the nail on the head and one reason i brought this point up this point up so this is actually one reason why these days i mean usually in most situations where i suspect non non-globular structures my first go-to approach is to start with density based clustering these density based clustering are very very powerful or one of the methods i'll teach i'll teach you in the next session is called dental lovely idea it's a field theoretic idea and it finds clustering it is massively scalable Google has one of the fastest implementations of dental and you can imagine that it is massively used at the industrial scale right for large scale clustering exercises. Any my general experience has been that I prefer DB scan or density based methods, then clue DB scan, etc. Any day to K means clustering rarely find situations where the traditional methods K means and hierarchical clustering, they do better than density based clustering very, very rarely, if ever. So this body of work that led to density-based clustering is very interesting. That's the context of our next lecture. But these are- Sorry to interrupt. Is the density-based DB scan better than K-means because it keeps the outliers away from the cluster is that the major reason and that is one the other thing is it handles non-globular clusters very well look at the eyebrow look at this and there's the lips it it dealt with it very nicely isn't it yeah the meaning advantage to main advantages first is it will mark outliers as outliers whereas both gay means and hierarchical will assign every point to some cluster it is very it is blind to outliers right and the second thing is both traditional methods tend to do very poorly when you are dealing with non-globular clusters. DB scan is one density-based method. All the density-based methods, they handle non-globular structures very well. Sir, what if your data is clustered with any shudder, like a target? What if your data is clustered with any shutter, like a target? If your data is a target, like there's a bullseye, and then there's a periphery going around the main centroid? Oh, yes. DB scan can still detect that. And you'd be surprised that the density base works actually very well with that. What you're saying is imagine a bullseye and then a circle of another cluster and an outer circle of yet another cluster and outer circle of yet another cluster and so forth, right? Actually, it turns out that for that situation, the K-means will be a uniform disaster. Arachnical will be a uniform disaster hierarchical will be a uniform disaster deep density density based methods will succeed just out of the bad but see density based methods are not there are situations where even density based methods fail so the field of clustering is filled it's like a enchanted land of very very powerful algorithms and no one algorithm is perfect but each algorithm addresses or is able to handle some peculiarity in the data my general feeling is i rarely see k-means works well when hello i say for some reason i become the presenter Yeah. I think Asif, what's the thing? Yeah, but Asif must have dropped off or something, but you can change the role back to him as a presenter post. Looks like there's a network is showing his side. you you Thank you. you So, as if is not there, I'll ask a question. So. question so so in this clustering you saw the eyebrows and the mouth and everything right so how the what does that actually mean correct they are not associated with any feature sets so how do you quantify these areas and why those points are there correct that part is not clear to me One thing I feel is would be the distance aspect. So the points near one eyebrow are closer to, let's say, some centroid within that eyebrow cluster. And it is far away from the other eyebrows one one point in the other eyebrows cluster so I think the distance aspect that we saw in the last lecture that should be the what is what I feel that should be the distinction between if one point comes in one cluster or comes in the another cluster. Okay. I don't think you can base it on centroid if it's density, because otherwise it would be globular. Yeah, that is also true. But the underlying idea would still be distance, right? I don't know if. Okay, yes, you're right. It's lovely. So we tend to. Get a circular distance around something find the radius, but. It's interesting. What is it is in gaming so in the regular algorithms at least you are quantifying by something, right? This is weight. That's why cow is cow and duck is duck or beak or something. But here you are having clustering, but on what basis, right? So you split the points, but then somebody asks you what are there, why the points here and why are the points there, right? So then it's not clear how you kind of answer that question so one thing I can share with my own experience about this is so the company that I work for we use db-scan there and what my company does is we have a product we have application so if you have that application installed on your phone it will get you connected to any open hotspot all around the world so it has like a global footprint so it may be Google Starbucks or bread or SF for terminal to Wi-Fi so how we how the clusters are formed is let's say terminal 2 has 10 clusters and we use DB scan to do that. And then those 10 clusters would have, you know, hypoth will get you connected to the best hotspot in it. So that's how at least the clustering takes place. So for your question, it is somewhat related to classification as well. So one eyebrow is separated from the nose, let's say. So at least in my case, we don't do any classification it is purely clustering so there's a specific different set of hotspots in cluster number one in terminal two and cluster number two in terminal two okay I think you answer that question there is there is a feature set or there is some sort of basis of the quality of the hotspot, correct? So that tells you which is the best. Thank you. you you Does anybody have Asif's number? I called him but he's not there. So Dennis you take over till he comes. you you you Hello guys, can you hear me? Yeah. Oh goodness, when did you you hear me? Yeah. Oh goodness, when did you guys lose me? I didn't realize. Probably 10 minutes ago. Let me rewind and say the things again. What was I talking about the last time? Did I do the Python part of the I was I going did I do the Python part of the code or not should I do it again okay so we were starting with the DB scan but I did the Python k-means clustering did I do this so but I finished the R part, isn't it? We can't see your screen also now. Oh, goodness. This is pretty bad. I don't know. Oh, and I can't even share it. Not available. Maybe port 9000 and TCP are blocked. Okay. I think Webex is having a problem. Guys, stay here. I will just kill this meeting and I'll come back to it again and I'll pause the recording just to be safe where am I okay am I okay means so doing it in is no different from doing it in our it just seems to be a little bit more verbose in the visualization part. The reason being for visualization, Python doesn't have one-liners like plot and so forth to just do it yet. People are trying to mature the libraries. Eventually, it will be there. But as of this moment, you have to write a little bit more code to do the visualization. But the actual machine learning part, the clustering part, all of those are one-liners. In general, clustering is much easier than, you know, your supervised learning, your classifiers and regressors. In clustering, you will feel that it's a much simpler topic from a doing perspective. So here we go. We have data. You look at the ground truth. And then to do K-means clustering, again, the same for loop. In this for loop, we go we have data you look at the ground truth and then to do k means clustering again the same for loop in this for loop we go through different values of k and find out uh the loop is exactly this more or less the same this finds the clusters and this plots the clusters very almost identical to the c uh i mean sorry sorry, the R code. When you do that, the clusters that you find are nearly identical, I mean, exactly identical in fact, to what you found in R. There's no reason why the same algorithm in two languages should produce different results. It's good. The scree plot, how do you plot the scree? This is it, the plot, the scree plot. We have that. And then once you have the scree plot you realize that there are no clear elbows here the best you could think of is at three or at five and which bears out just to remind you that three and five looked fairly decent three was this the nose and the lips are one cluster each Each of the eyes are a different cluster. Eyes and eyebrows are a different cluster. K is equal to 5 was actually pretty neat. The eyes are a different cluster. Nose is a different cluster. But the lips get split in two. So not quite perfect, but it's about as good as you can get. So this was for Python. And the same thing for density-based clustering. Density-based clustering in Python, again, given the ground truth, this is the same code repeated. This is nothing. So the actual code is just a couple of lines. But then comes the visualization part. Unlike R, where to visualize it is just one line of code in Python, as of this moment, you have to write quite a few lines of code. It's a little bit messier. And so this is the bit of code for data visualization. So here I would like to say there is a tip I will give you guys. When you do machine learning with Python in general you'll find that skeek it learn is very efficient it's very good it's almost like our some little a few more lines of code perhaps but nearly as good as are but the visualization part you the maturity is not yet there so you have to write visualization code using matplot So you have to write visualization code. Using Matplotlib, you have to write a bit more code to visualize the data. So become good with Python visualization. Become good with Matplotlib and CBOD and all of these libraries. Try to spend some time and becoming good at it. Now you may say, well, that's not really machine learning, that's just data visualization. But the fact of life is whenever you work, you will be expected to create this visualization. See, in mathematical disciplines, mathematical results are very hard to explain to businessmen. It is the visualizations that tell stories that makes it accessible to the common man. And so the beauty of mathematics often is conveyed through the visualizations we create around it. Gradually become good at the visualizations. Spend some time and become good at the visualizations. So this all this code, I can talk about it one by one. One thing is you're taking the the points, and you're saying you're setting it to zero, then you're saying, which cluster does each point belong to? Does it belong to this cluster? estimated number of clusters is six. And then what do you do with each of these clusters? By the way, this colors is a color scheme called spectral. There are many, many color schemes in both R and Python. Usually, they come from the Color Brewer package, an excellent package that is sensitive to colorblind people. And they have aesthetically pleasing color schemes. Just use them. Don't invent your own color scheme in general, unless you are really confident of your artistic ability. Most people use an existing color scheme. So once you do that, then all you do is you plot those points here. You do that. So when you look at this, once again, it has succeeded in doing what we did with R. Just to give you a comparison of this code versus this much of code versus the R code, you can see where's the R code? Yeah. You can see that here we achieved it in three lines of code versus the R code, you can see, where's the R code? Yeah. You can see that here, we achieved it in three lines of code, right? One, two, three, four, maybe three lines of code here, and then the local outlier factors and so forth. Visualize the result, two of them, good enough, right? One line to cluster, one line to plot, so much easier. But in Python, you have to do more. Relatively speaking, you have to do a little bit more coding. You have to just get used to the coding. Because if you look at the machine learning part, it is still one line. The rest of it is just visualization of the clusters. So quite often, obviously, you will have your own little libraries of code to do the clustering libraries are maturing more and more people are coming up with good libraries or one library that i suggested to you which is very good to simplify your uh you know the visualization of machine learning models and clustering was the yellow brick library so let me take a moment to take you guys to the yellow brick library which I have mentioned before but might as well mention again if you go to the yellow brick library and by the way I should have used it in the notebook this notebook I created before the yellow brick library so yellow brick now not yellow brick.com yellow brick dot now yellow brick yeah um at least you just kick it yellow brick okay here it is this is the yellow squeak it yellow break by the way these links are there in the document that I have right if you go to the in the end you'll find all the important links you'll see it there so you can visualize us an API if you go to so there's a clustering visualizer and as you can see you it helps you show the silhouettes like for example see the clusters and so forth and are there examples here let me see if there are examples that they've given your color scheme frequently asked questions quick start let's go to the quick start and see if they have examples yes so you'll see a lot of examples here but let's see the examples for clustering they must have examples for clustering about the data this is feature extraction and so forth mmm Yes, clustering visualizers. Well, they don't seem to have examples that I can show, but I will post those examples. So here's the Elbow method. It is showing you that. And then quick methods api references to make the screen plot and then silhouette visualizer but they don't give you a visualization here it's pretty useful it will do pretty much what i did with the code now that the yellow brick library is there i would encourage you to use it and i'll also post code based on that. This is intercluster distance map. How distant are the clusters from each other? So make yourself familiar with these libraries. Also, for the clustering, I think without visualization, it's hard to see what the algorithm that we did has done to the data. Is that correct? Or can we still read it? Yes, absolutely. Clustering, you have to visualize it. You have to take some samples of the data. Even if you have to project it down to two dimensions and then visualize it, you should always visualize the data. Because how else would you know that your clustering is good? Try your best to visualize it if you simply cannot like let's say your data is a 200 dimensions it's hard but then you use techniques like there are certain techniques and teach you next time dimensionality reduction techniques PCA is the simplest of them, kernel PCA, then there is something called T-SNE, then there is UMAP. There's a whole family of techniques. In the next workshop, ML200, you'll do a large number of them. This time you'll just do PCA, the foundational one. They help you reduce the dimensionality of the data and one advantage is you can then visualize the data in lower dimensions. That is all I have, guys. Clustering from a doing perspective, not perspective is quite simple. Especially if you do it with R, it's very easy. Python is also very easy, but you just have a few more lines of code. That's all. Any questions, guys? Asif. So I see these eyebrows and eyeballs and the nose and stuff like that. Right. So in this case, what are you quantifying these points to is it some x or y what what are those values that's the confusing part for me you mean what values the x and y they're just the locations of the points okay so it's it's just one kind of dimension and just location of the points that's right quite literally so for example you know this could have been a picture imagine that a photo is there of a face and I ask you to cluster the pixels you quite literally this is a very simplified picture of a photograph and you're finding clusters in the photograph okay but if you you talked about previously right so you're giving data to this algorithm and you do not know what the data what are the different features of the data that comes then it comes back with one feature set and plots it accordingly correct and so that then you know that, okay, it could be location, could be something else, could be color. Yes. No, certainly. You tell it what are the feature sets, then it goes about clustering. And then the question that comes is, are the clusters good or bad? Are we together? So I'll take another example. Suppose you have a picture in which there are three flowers in the picture now that they have a certain color let's say that the color of the flowers are orange or yellow pictures it's surrounded by green leaves so if you were to cluster each of the flowers will stand out as a cluster, isn't it? Potentially the leaves will stand out as a cluster. But let us say that there's a general background of greenness and the three flowers are there. Surely you should be able to detect the three flowers as three clusters. That's the thing. Got it. Thank you. Thank you. that's the thing got it got it thank you thank you so I'll post a lot of practical examples of this clustering let's do a few real-life allergies alright, that's all I have. Then I'll stop the recording if you guys don't have a question. Hey Asif, have you posted the written notes for last section? Oh, last time's written notes I didn't post. Let me do that right away. I posted the video. I hope you have the video. But I didn't post the written notes. I'll do it right away. And I'll post this recording also right away. Let me stop the recording now.