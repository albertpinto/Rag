 All right folks, so we are scheduled to start at 10. Today is the first day. We'll probably give another three, four minutes, and then we'll start. Since it's the first day, some people are still finding their way here. So let's give them a few minutes. So this is the workshop on it's a comprehensive introduction to data science. That's what we are going to do. This workshop will run for three months. It is going to span 12 weeks. Those 12 weeks would be active sessions, labs, lectures, and quizzes, and so forth. After those three months, the last month, usually people extended, they do the projects. The projects that you do in this workshop are very vital to your developing in-depth fluency in this field. And also for people with different aspirations, like founding their own company, or going out for interviewing and so on and so forth, having done a project brings a huge advantage to this. Welcome, welcome. Breakfast. So, yeah. So with all of those things, there's an extra month at the end which is optional in which you do the projects, we will still be helping you and guiding you and trust me if you get involved in this workshop, you would love doing the projects. There is, if you do the projects, there is actually an incentive. The team that does the best project in this workshop, traditionally, we have a way to reward them and recognize their contributions to themselves. Of course, the fact that they learned and gained something from this place. Well, you get two things. You each get the latest iWatch. Each member of the team gets the latest iWatch. You each get the latest iWatch, each member of the team gets the latest iWatch, and you get, obviously, a strong letter of endorsement from me, besides the certificate that you will get at the end of the workshop. So certificates, of course, we all get, but the winning team gets a strong endorsement from me, along with iWatches. And I just had a new iWatches come out. So that should be an incentive. With those words, I will get started with today's for this workshop. So once again, welcome to this workshop. Are you folks able to see me? I don't know if I'm visible. Yes, yes, yes. yes okay excellent so uh what you don't see obviously are the participants in this uh class they're about um about a dozen here and many of you are in dual roles i suppose you're dialed in remotely as well as here we'll start this workshop is rather fast paced. So let me set your expectations in the beginning. We will have theory in the morning, we'll understand the concept, we will practice that concept in the afternoon. So between the morning and the afternoon, you would go away feeling confident or familiar with one concept. Now, learning is always iterative. You can't learn in a linear fashion. So what it means is, once you have learned the thing, and you have practiced it using a guided lab, we need to reinforce the learning. To reinforce the learning, there are three things in this workshop. The first is there are quizzes. Every week, there will be a quiz. That quiz, you take it on your own. When you take the quiz, it will give you, score you, and it will give you a reality check on how much you have learned. Now these quizzes are meant to be in depth. So they are not frivolous, small quizzes. They're pretty in depth so they are not frivolous small quizzes they're pretty in-depth so you should get some reality check from the quiz and learn a lot a lot of people say it's good interview preparation also the second aspect is you will do a project the project again will extend what you have learned in the lab you will do many many. How many you do depends on you and your team. So for the project, it's mostly code. You will learn to code. Those of you who don't know Python, I hope you attended yesterday's Python session. This field is not about programming. You don't write reams and reams of code. Most of the code you write are 10 lines or less. It is about thinking. And so your project will also not be code heavy, but it will be thinking heavy. It will be outcomes heavy. You'll build prediction machines, pattern recognitions, and things like that. So you will build visualizations. And in the project, you will get the entire experience. You will get the experience of gathering data, of processing data, of data visualization, of predictive model building. You'll build AI models, machine learning models, and you will also learn how to take those models to production. So what it means is you deploy it to the cloud, you create a microservice around it, right? So you will learn about containers, dockers and Kubernetes. You will learn to deploy it into production. And you will have the pleasure of surfacing it as a web application that people can access through their browsers, or through their mobile devices. And you have a tremendous amount of flexibility when it comes to the UI. You have a lot of flexibility in how you do the projects all across the board. You can do your UI in React. You can do your UI in Angular. You can do your UI in something called Streamlit, which is a dedicated UI building, rapid UI building, a framework for data science. It's actually surprisingly easy to build applications in data science using Streamlit. So you will do the projects. So those are the four legs of our theory, labs, quizzes, projects. But there is a fifth leg. It's a sort of a peculiar five-legged table. The fifth leg here is research paper reading. This field is advancing very fast. As it advances, newer and newer things keep happening. To really be proficient in this field, it is always a good idea to be abreast of the latest developments. Now, when these developments come about, usually it takes years before they make their way properly into textbooks. For example, there is such a thing as a transformer in machine learning. These transformers came about in December 2017. It completely transformed the landscape of AI. They are everywhere now. Despite that, there are no good textbooks today. Textbooks which explain transformers and give you its most powerful use cases most of the textbooks that are there are relatively dark of the traditional stuff so what we will do is we will do the foundation but in order to keep abreast of the latest we will also cover one research paper a week. Now, don't get scared. Research paper part, the fifth leg of this workshop is purely optional. It's on a Sunday afternoon. You don't feel like coming to it. Don't have to. And the timing may change. I might make it Saturday evening or some week evening, quite possibly. It is a one-hour thing. The purpose is to explain to you a paper. It doesn't mean that you have to read the paper and understand all of it yourself, but it is that I'll introduce the paper, explain what the breakthrough is, give you a walk through the hard parts of the paper, and after that, you can read the paper on your own and get a sense of it. And it is purely optional. In this workshop, because we are starting out, I will actually not start out with the latest paper, but I'll start out with the landmark papers, historical papers that essentially founded this field. Some of these papers are over 100 years old. Wow. We'll start with that. So those are the parameters. So let me once again recap. This is a three-month, 12-week workshop plus an additional month for your projects. We will do the entirety of data science. That's why it's called comprehensive introduction to data science. We'll start with the basics and we'll go pretty far. It will move at a pretty brisk pace then there are five pillars to this project to this workshop theory which we are doing now in the mornings labs guided labs which we'll do in the afternoon we will do quizzes for you to get a reality check of how well you are projects in which you will work on something interesting and you will optional is a paper reading right which you may or may not attend based on your uh inclination those are the five uh pillars we have a teaching staff here besides me uh so i'll give the lectures in the lab but this isn't it we are i'm a little bit behind in the notifying you of the course web page and the slack discussion groups and so on and so forth those notifications you'll receive tomorrow i apologize this is a season where many of you who have children are taking your children to college and that is certainly my situation so i'm a little bit behind but i'll be sending out those notifications soon to all of you do please actively participate in the discussion groups the uh you will realize that learning has many aspects to it uh i'll take a small digression into methodologies of learning see when you teach and as i've been teaching for 30 years, there are many things you learn along the way. You know what doesn't work, and you know what works to learn. What doesn't work are flashy PowerPoint presentations, and you will not see any of that, because presentations contain facts. A fancy slide will be filled with facts, facts a fancy slide will be filled with facts but google it serves very well for looking at facts when you come to a workshop or when you come to a course if all you get are facts you will suffer from what is called the famous forgetting curve this was discovered around 18 well i'm forgetting the date myself, but I would like to say 1801 in education. But what it states is something very obvious, but in a quantitative way, it says that whatever facts you learn, knowledge will evaporate from your head. Within 90% of it will evaporate within 90 days right so what happens is that you learn something two three months later you remember some things that you learned after a few months after six months all you remember is that you learned the subject when you encounter it that there was a time when you used to know it i if you look back and think how many of you remember for example let me take a just a simple topic that all of you have learned fixed point theorem in calculus does it ring a bell I'm sure it might look somewhat like a term you used to know but none of you could probably tell me what it is today. Isn't it? So that's the nature of facts. We forget knowledge, we forget knowledge. So then what's the purpose of education? What is the purpose of coming here, spending a beautiful Saturday with the sun shining outside? Why are we here away from our families? If we are going to forget all that we learn. Would somebody like to venture a guess? Kate. To learn how to learn and take notes and efforts to help reinforce your knowledge. Yes, Kate shares that you learn how to learn and how to retain things and so forth and very close to that see we forget facts but there is something that we never forget we do not would you like to say something to apply those facts yes again an excellent point see we forget facts, we forget knowledge, but we don't forget understanding. You remember, you forgot all about calculus. We took the example of calculus. But you don't forget what differentiation is. It's the slope of a curve, the tangent to a curve. You don't forget what integration is, this area under the curve. That is understanding but you have forgotten all the formulas of integration and differentiation so what you take back and what you retain for the rest of your life is understanding because understanding transforms your mind forever you're not the same person in subtle ways you change your worldview changes same person in subtle ways you change your worldview changes and you start applying that to your workplace to your life and everywhere around you so what we will focus in this workshop and this is sort of my teaching methodology part we will focus on understanding and i want you to raise your hands and stop or ask questions lots lots of questions. But now, because we have a heterogeneous educational mode here in person as well as remote, what I would request you to do is I will talk in phases. uh those of you who are remote see this this is very useful for classrooms and i'm going to set it up to 40 minutes when 40 minutes are up we will take 20 minutes of questions are we is that is that fair and do send your questions over to on the chat we have tas in this course i would like to introduce the tas teaching assistants we have Kate here, we have Harini, and we have Kyle who is dialed in from India. So there are three teaching assistants that we have. There is a fourth one, Dennis Shen, who is not or maybe is remote. So we have four teaching assistants. I would strongly encourage you to use the teaching assistant. So with that, I'm going to start now in the topic. So this course is on data science. Let us ask ourselves, what is data science and what brings you here? We all hear the word, these interchangeable words are everywhere, machine learning, AI, data science, data mining, statistics. And these are sort of words that seem to have different degrees of overlap. What do these mean? What are they all referring to? Some people make very fine distinctions between these words some people say they are the same thing right uh there used to be a joke at stanford university that machine learning is nothing but statistics wearing a marketing hat right of course that statement was made by a professor in the department of statistics. So he made a lovely cartoon that showed if you have a conference in statistics, the budget would be like $10,000, $20,000. If you have a conference in machine learning, the budget would be a few hundred thousand dollars and so on and so forth. And so there is some element of truth to that, but only some element of truth. Machine learning has other aspects. It has a different history. R. Vijay Mohanaraman, Ph.D.: So what i'm going to do today is going to take data science and sort of break it apart into its constituent parts and we will cover the basics of all of those parts today in the first 40 minutes we'll know what they are. R. Vijay Mohanaraman, Ph.D.: The word data science. The word data science stands for the two words there. This word is a very recent coinage, I believe 2012 or 2014, people started using the word data science for this collective set of activities which incorporates machine learning or AI, which incorporates statistics, which incorporates data visualization, but it also incorporates essential data manipulation and data engineering. It incorporates these days SQL. So these days when you go and apply for a job in data science, you're also supposed to know SQL, especially if you're applying, let's say to Facebook and so forth. So there's a breadth of topics that are now brought under the umbrella of data science. The word data, what is data? I mean we all know what data is, we see it on our screens, but if I were to ask you what is data and its singular datum, one way to think about it is it is one atomic unit of fact, atomic unit of observation, a measurable. And that is something very important. The second word is science, and science only deals with what is measurable. Are we together? Are we together? Now, what happens is when we do, when we are in this field of science, let us say, one of the questions is what is science itself? Science is about, and that is where a lot of people have pointed out that what data science is doing is what scientists have done for hundreds of years. and so why this new name data science why this buzzword and there's a lot of truth to that because all scientists go through the following process the so-called the scientific method they observe you observe certain phenomena and then you take measurements you measure And then you take measurements, you measure. The measurement produces data. Then you build a hypothesis on the data. You ask this question, what is the force that produced this data? What is the dynamics that produced this data? Are we together? And then you create a hypothesis, some sort of an equation, some sort of a generative force description. And then you validate, you make predictions, and you see whether that is true or not. So example is, well, the classic example in science that everybody gives, apparently an apple fell onto Newton's head. I'm told that that's not strictly historically true, but it makes for a great story and it knocked some sense into him perhaps. He's supposed to have discovered gravity. Well, that's a caricature, but whatever it is, he did see apples falling, and then he looked at the moon going around the Earth, and he made a tremendous leap, intellectual leap. And he asked, could both of these be because of the same reason? Is there as common force that's making the moon go around the earth, that's causing the apples to fall and for things to fall? And so he discovered a gravitation. Now, let us frame it in the data science language or the science language what is it he observed certain things right then from the things he sort of deduced that there is a common force but then he had to quantify it into a model into an equation so what what would he start doing? He would start taking measurements, right? And based on the measurements, then he would write down an equation, which would be up to a proportionality constant, mass of the object. Like if the earth is one giant big object, then the force of attraction that any other object feels towards the earth is proportional to the mass of the object and inversely proportional to the square of the distance away from the earth right so in the center of the earth so you you come to that how you need data because now you're getting very quantitative very precise in your hypothesis it's not a hand-waving hypothesis anymore and that is the scientific journey the moment you write an equation an equation is falsifiable right in other words the hundreds of data points may agree with your equation may agree with your theory but it takes only one data point to disagree to completely wipe out your theory are we together and in this regard there are many examples that are given about it for example you can make a hypothesis when we talk about falsifiability a classic example given is you can see most of the swan are white right so you can make a hypothesis that all swans are white it It's a pretty good hypothesis. It is based on observation, but can it be falsified? What does it take to falsify that? One black swan, right? All it takes is one black swan. And of course, indeed, in Australia, and sometimes in Australia, they are black swans, apparently, I'm told. And apparently some flew into Europe at some point, right? So that's the nature of science and that's what we will do. Now in this journey here in data science, we will do the same journey. We will have data, these are observations. With those observations, we will fiddle around with the data. We will make friends with the data. Understand it, this is the first part of the journey. The technical word for this is exploratory data analysis, EDA. And in the history of dealing with data in statistics, this is the first pillar, exploratory data analysis. We will explore data, exploratory data analysis. We will explore data and the simplest way that I can tell you the importance of exploratory data analysis is like this. Well, many of you may or may not have dated when you were young. Right? Well, in this culture in the US, of course, dating is the norm. It's not so much in some other countries, but when you date or just think of friends, or forget dates, making friends, you go to a new college, you go to college and you have new classmates and so forth, and you begin to talk to somebody, are you likely that the person would tell you their deepest secrets in the first meeting, would trust you with that? You don't even know who this person is. It takes many meetings before two people are comfortable and they really speak out who they are. You really get to know them. The same thing is true for data, very much true for data. Whenever you look at data, the best mental frame that I can give you is think of it as a potential friend. This data is something you can be friends with if it were a person. And you're not going to be friends with this person on the first date. It will take many meetings. I am told, somebody told me, I don't know how true it is, that it takes something like 20, 30 hours of knowing a person, two people, before they really get an understanding of who they are. The same is true for data. You will find that you really need to spend quite some time with this data before this data becomes your friend and begins to reveal its secrets and thus the importance of exploratory data analysis i must say from experience of fact when i interview people and in my day job i'm in a position where typically when i interview they have been through three four rounds of of interviews, and they come to me. Often I ask them questions which are very basic in the data itself, exploratory data analysis. And I get people with five, six years, data scientists with five, six years experience, leading teams who absolutely founder at just describing to me what this data is over which they claim to have done an analysis, which is a very sorry state. Why? Because it's not supposed to be in quotes sexy, right? The whole industry is in pursuit with the hot, the shiny ball syndrome. What is the latest algorithm? Oh, today they are transformers oh today there is this there is that and people are in a hurry to show that they have experience with that they have fluency with that but it doesn't work like that if i may give another example of my personal life see when you go to the any of the silicon valley beaches or tourist attractions you will find a very a very peculiar species of people a very special tribe of human beings they will all be well dressed they would have a camera with a huge zoom, right? A very expensive digital camera. And in broad daylight, when the sun is glaring down, they would be taking pictures of all the landmarks. And at the same time, annoying the hell out of their family by saying, can you please stand here? And can you please stand here? And they'll be going far off. And with a long telephoto zoom, they'll be taking pictures. Anybody has seen such uh such creatures or most of you have seen such creatures right so this is photography and this is it looks silly this truly is what happens in data science in this field of ours most people most data scientists are exactly like this person. That is a classic sign of somebody who has just made a lot of money, has bought fancy equipment, but has absolutely no idea how to use that equipment, how to use the tool. It is a pretty sorry state for a human being to be in, you would all agree, and to look ridiculous. So if you're into photography, you immediately spot them. So many things are wrong with that, but we'll come to that. First of all, one basic hint, never take landscape pictures or any kind of pictures, even of people people in outdoors with broad daylight and sun shining straight up straight down at you you will get shadows on their face you will make the most beautiful people look ugly right so you don't do that unless there's a fog or something like that so many other things so the point is that tools do not make for good outcomes there may be a necessary condition but they are not a sufficient condition for excellence and outcome are we together it's a very obvious statement what does it mean in data science in our field do not chase the fanciest algorithm what you will notice is that just like in national geographic some of the best photographers of national geographic they can go into a forest or into a culture with nothing but one camera and one lens, of that culture, or of that forest, or that animal and so forth. And those go into national geographic. All of you are familiar with national geographic picture. So your goal is to be the national geographic photographer. Are we together? So for that, we will focus on understanding and mastering the tools, not on chasing shiny tools. So exploratory data analysis is the very first step that you would follow in this particular journey. And this journey has a long tradition and illustrious tradition. Unfortunately, something that is often underestimated by people in this field so once again i'll illustrate it with one last fact there was a lady with great compassion and energy a few hundred years ago you will deal with her in this workshop in your lab she there was a war in Crimea, I believe it is 1851 or so. Crimea is a part of Europe, close to, I suppose somewhat close to Russia, Turkey and so forth. So in those wars, England was fighting and France was fighting, the Russians were fighting. She went there to help out the soldiers, the wounded soldiers, and she set up a little place to camp. What she observed, and she was a person who would gather data, unusual quality, whenever she would see something, she would write it, make a note of it. She used to keep diaries, and she was truly an inspired person. She noticed something quite remarkable. When she gathered data, she found that in a battlefield, the least likely cause of death is, yes, the least likely cause of death is actually a bullet or a battle wound. Like you know, just being killed by a bullet or by a knife or something like that on the spot. It is the least likely cause of death. The leading cause of death in battle phase is that you will get injured, like Harini said, you'll get injured, you will recuperate in so-called recovery wards or hospitals, and you will die of infection, right, by far, and we will do in the labs at some point, I will show, in fact, if you are on the computer, I invite you to google up this lady's name, anybody can guess, not the people who have attended the class before but anybody can guess what is the name of this lady right it is florence nightingale and with one data which had barely 23 rows or something like that rows or something like that with one little table of data and one data visualization she transformed the world she founded the practice the field of nursing today we say that nursing started with her because she showed how important it is to maintain hygienic sanitary hospitals, where if infection is your biggest enemy, you want to fight infections, you want to create extremely clean sterile environments. And the moment she did that, well, it got complicated actually. The hospital that she built, unfortunately, was under a drain. So it was a minor misfortune, but her concept was right. That that thing led to saving of countless lives. Right. And of course, it founded the entire field of nursing. That should show to you the importance of exploratory data analysis. Sometimes a lot can be said by looking at the data and just visualizing it, right? Because of all the AI engines, of all the machine learning systems, there is nothing so elegant, so powerful as the human eye and the human mind. So in machine learning, in this field, we talk about how long it takes, how much data did you train AI with. The more data you train it with the better it's supposed to get. That's sort of the narrative and we'll review the narrative again but so you see all these cars taking data, driving around and you know trying to drive themselves and take pictures and so forth. Learn from that so you see AI is learning in action all around you. If you think about the human mind, if you think about the human eye, you will realize that tadpoles and little fishies in the ocean have eyes. To the best of our understanding, the human eye, the eyes have gone through at least 100 million years of training, of learning. Think about it for a moment. It completely dwarfs any kind of data gathering that we humans can do, even big data that we can use to learn anything, isn't it? Or train any AI machine. The most powerful AI machine is literally right here, the eyes. Because in the blink of an eye, it knows. That's why we use it. We say in the blink of an eye, it immediately understands what is there and does the importance of data visualization which is a core pillar of exploratory data analysis and we will focus a lot on our data visualizations visualization is where math meets art right you can say when people say a picture is worth a thousand words you will create those beautiful visualizations that will say all that that needs to be said from the data so data visualization is important and it's an important part of our journey as we do it we will continue to master that so the next thing is what are the other things we can do to make friends with the data there is this entire field of statistics. Statistics is looking at data and describing it somehow, extracting certain quantitative measures from it. The word statistics is the plural. Notice the S at the end is a plural of the word statistic. So what is a statistic? So when you look at data, it has lots and lots of items. Let's say you have a million rows of data. And I ask you, describe to me this data. You would be like, how do I describe something so complex? So the first baby steps you can take, you can say, all right, let me give you the descriptive statistics. The average value is this, right? The mean is this. The median is if you sort all of those numbers from smallest to biggest, middlemost guy is the median median is quite literally that on a highway median is the middle line and so with data the middle guy is the median in a sorted order the mode is the most frequent isn't it so they give you some aspects of the isn't it? So they give you some aspects of the data. I hope you remember these basic concepts. If not, please do take five, 10 minutes to refresh them later or reach out to the teaching assistants and they'll help you refresh it. Then there is something called variance and standard deviation, right? It gives you how spread out the data is around the average. Is it really spread out or is it really concentrated into a zone? Isn't it? For example, human body temperature of healthy people is very close to 98 degrees Fahrenheit. The spread is not that much. On the other hand, if you look at people's income, there's a pretty wide divergence, isn't it? And you see, and so let me bring up the drawing board here, writing board. Let's draw some data and try to get a sense of it. Suppose you have data. I'll give you data along. Well, I'll just make two dimensional data to make it more interesting but think of it as just one dimension. So suppose I have data, lots of data like this. Look at this axis. If I were to ask you, what is the mean? Would you say the average is somewhere around? Well, this is the minimum value along the X axis. Let's say that this is the X axis. This is the min. This is the max. Right. And so suppose you add up all of these and you take the average. Right. And so suppose you add up all of these and you take the average. So min and max, they are also statistic. Each of them is a statistic. It tells you what it is. The range is a statistic. Range of the data tells you something. Now, the average would be somewhere around, let's say, would you would you say something like this? I may be wrong, but I'm just for illustration saying the average would be somewhere around here. Does that look reasonable guys? Average is somewhere around mean. A mean is often represented with a mu, right? So there's a sort of rough convention in data science and practically all science, but in data science, and we'll follow it. So do you know the difference between Roman letters, Roman numerals and Roman alphabet, and the Greek alphabet? So English that we write is Roman, ABCDEFG, and the Greeks are alpha beta gamma delta right so there's sort of an unstated convention that we'll follow what you can see and observe we will use roman alphabet for it so temperature pressure things like that measurables are Roman letters, but things that are abstractions or concepts that we create in the field, we will write it with Greek letters. Are we together? So what you will find that anytime we talk about things, we are talking using concepts about measurables, about data. So you will see a mix of greek and roman letters in the diagram in the picture that we make right so the mean is mean something that you can see can i hold a cup of mean in my hand i cannot it is our concept it is a statistic It is our concept. It is a statistic. Therefore, conventionally, people use a Greek letter for it. In the same way, so this is it. Now, this data, one of the first things you notice is, you can also measure data. So, for example, data can be very peaked around the center. If you draw something like a histogram or a frequency plot, how frequent the data is at different values of X. So here, if you make a frequency plot, you would probably see it like this. I'm making it along the downward axis. So because most of the data seems to be peaking around this location, isn't it? Right. So because most of the data seems to be peaking around this location, isn't it? So this this kind of a this is another statistic, a histogram. It shows you the frequency distribution. Right. Or more generally, people use will often use the word probability density distribution, PDE. the word probability density distribution pde right so what is pda a probability distribution what it is is quite literally that think of it roughly speaking as your frequency plot normalized by the total value so so what's the relationship of numbers to probability i'll give you not a very accurate in a frequentialist measure. There's a different definition of bias and definition, but to get started, we'll take simple. So what is probability? We keep saying probability of this and probability of that, and we often mix it up with odds, right, we use the word odds and so on and so forth. So just one minute primer on probability. So the frequency list, so there are two camps of people in the world. It turns out the probability itself is a concept at the foundation of this field and yet there are two different definitions of probability from two different camps. The frequency list and the simpler definition is if you pass a coin, let's say you go about passing a coin and a thousand times and approximately 300 times it was head and 700 times it was tails. First of all, does that look like a fair coin? No, right? So, well, okay. So be that as it may, what is the probability of heads? Right? So, well, okay. So be that as it may, what is the probability of heads? What is the chance that you will get a head if you toss the coin? 50? 30, 30 percent, right? Because remember 300 times it was head, 700 times it was tails in the data, right? It's not a fair coin. for this for this observation says this is not a fair coin if you pass it there is 30 chance it will be heads 70 chance that it will be tails so now how do that chance is your probability but how did you come up with that probability you came to it by saying by counting how many times you threw it sufficiently many times right the word that people use in this feed is you did a long running experiment tossed it enough number of times says that the proportion of heads and the proportion of tails began to stabilize it won't do if you toss it only three times because all three may be heads, even though head is less likely. But if you do a thousand times, it's very, very unlikely that you will get a thousand heads. Possible, it's still possible. And that, by the way, brings about the distinction between the possible and the probable. In colloquial language, whenever you want to prove a point you can always and you know that you're wrong one easy way is always argue from the possible and if you are smart and want to counter others argument always argue from the probable right probable is fact possible is just intellectual fiction. Like, for example, is it possible that this room is actually filled with 40 people, half of whom you don't see, because of some optical illusion? You can't disprove that. You can't disprove that. You can't disprove that. And Occam says, the burden is on you. You made that extraordinary statement, you disprove it. But if you ask, what is, how probably is it that it's true? You would all agree it's not probable. But if you want to just argue for the absurd, you can always argue from the possible. Are we together? Right. So that's a distinction between probable and possible. So probability is proportion in the Frequentialist term. That is the Frequentialist probability definition. Very, very useful. So when you, for example for example here when you take a certain value what is the value most likely to be this right it's speaking here it's speaking at this value right this value of x is most likely to be true right if you take a random point where is it most likely to be somewhere here isn't it around this place where is it least likely to be? Somewhere here, isn't it? Around this place. Where is it least likely to be? Somewhere here. Would you agree that this place, let me call this A and this B, if you just randomly took a point and you don't know which point it is, where would you hazard it? Where would you place your bet? Suppose you had to bet $10. Where would you bet? Is it near to A or near to B? A, because B is rather lonely. I mean, points around there are rather few and far between, it's a sparse region. Whereas around A, it's a dense region. So that is the concept of probability derived from frequency of occurrences, right? You know, when you have sufficiently many data right or longer the problem with this probability definition can there be a problem with this definition there is a problem with this definition which is subtle but it's the heart of amongst other things global warming and whether global warming or is taking place or not so there are people there are various theories some people deny that there is global warming at all some people say well global warming is there but we didn't cause it it's the natural cycle of vr and some people believe that global warming is very much there and you and i are the culprit right so there are three schools of thoughts if you, most of the scientific people are what I, are people who tend to belong to the third camp. They believe that there is global warming, there is data showing it, and we are the culprit, right, for a variety of reasons. But the people who belong to the other camp, they have an interesting argument and they certainly should take data science courses. Dr. G R Narsimha Rao Their argument is, where is the evidence that human beings cause global warming? When you say the probability is very high that will destroy the earth with global warming, let us say, where is the evidence? Because why? Probability in their mind is a frequentialist definition. So what they're saying is that, let's do a long running experiment. Let's take the earth, populate it with people, let them be irresponsible. And then do this experiment 1000 times. See, did the earth get screwed? Oh, yes, oh shucks. Then do it again. Again, it got screwed, oh shucks. And then well, maybe this time something happened. You keep on doing it and then in 1000 again, again it got screwed over shots and then well maybe this time something happened. You keep on doing it and then in a thousand runs see how many times it got screwed, that will give you the real probability of us destroying the earth with global warming. And that is the central argument that if you really boil it down to its essence, one of the arguments that they make is something like this, where is the evidence? Probability needs lots of data to make it. So therefore comes the other definition of probability. You would all agree that the frequentialist definition is not enough because that doesn't address the situation. We don't get to screw up the earth a thousand times or not screwed up. We don't know what will happen. If we continue to behave the way we are behaving, nobody really knows what will happen but life is not about certainties it's about probabilities degrees of belief how strongly do we believe that it is gravitation that's taking the moon around the earth and not some angels pushing it around the earth the medieval belief was it was the angels who are pushing the uh the moon sorry moon around the earth right and newtonian gravity says it is well the gravitation taking it around we all tend to favor the the modern man tends to favor the gravitational theory because of its predictive power and so on and so forth. And the fact that nobody seems to have seen those engines, isn't it? So in the same stream, when you talk about probability, you often talk about degrees of belief or degrees of uncertainty. In the absence of any data, let's say the earth was just fine and the industrial revolution had not happened there was no basis to say that should people if people burnt a lot of coal for example global warming would take place there was no data right at the time you could make both hypotheses you could make a hypothesis that you can burn all the coal you want because people were burning small amounts of coal to heat up their homes and cook their food or you could it could destroy the earth it may not destroy the earth so there are two different hypotheses but gradually as the years passed and we start seeing the effects of it we started seeing entire ecosystems getting destroyed, thousands of species of animals vanishing. Literally us seeing the temperature around us becoming erratic, weather becoming erratic. What does it do? What does all this data do? It makes you believe that very, very likely we are destroying the world are we together but it remains a matter of degrees and one of the peculiar facts of science is in science no theory no hypothesis can ever be proved science is a body of falsifiable theories. In other words, a theory can be falsified, never proven. Nothing can prove that Newton is right. In fact, we know Newton is not right. Einstein corrects upon Newtonian gravity. Is Einstein right? Well, it so happens we know Einstein too is wrong. We know it for a fact, we just don't know what to replace his theory with. And for all practical purposes, it is so uncannily accurate, Einstein's theory. We have not found a single data point to disprove it. All we know is it doesn't agree with the other theory, quantum mechanics, right? Which also is uncannily accurate, but the two just somehow don't seem to come together anymore. So that's sort of a problem. And so we will take that approach, guys. We are reaching the end of our time. So let me recapitulate. We talked about exploratory data analysis so far. We talked about probability. We talked about statistics statistics there is more to statistics so one is probability so by the way the second definition of probability which doesn't need a Frequentialist approach you know which is still a viable definition of probability without needing lots of runs long-running experiments because there are situations in which you get only one chance like global warming that definition of probability is called the biotin probability right generally scientists implicitly are a biotin thinkers these days biotin thinking and biotin machine learning is extremely hot we may not be able to cover Bayesians in this workshop. It's a pretty deep topic, but in the subsequent workshops, we do cover those. So Bayesians talk about probabilities as degrees of belief. The belief gets stronger as you see more evidence. So you see, you have an a priori belief, right? So I may have an a priori belief, right? So I may have an a priori belief that California has a lot of rainfall. Then I come here, the first day I look out, no rain. Okay, a few more days pass and there is still no rain. Do I still believe that California is a very rainy place? No, my belief in the probability of it being rainy Do I still believe that California is a very rainy place? No. My belief in the probability of it being rainy begins to fall. My belief in this being a rainy place falls because every single day that I wake up, I see just the same dry weather until winter comes. So I am disabused of the notion that California is like Seattle or something like that, isn't it? So the Bayesians call it, you go from a priori belief, the word a priori means before the fact, to a posteriori belief. A posterior belief is that which is reasonable to believe after you have seen the facts are we together right so after you have seen the facts your belief now has changed you say it's a dry place looks like a dry place but it is very low that it's a rainy place and that this was just a freak set of dry days isn't it so that is the biation journey to go from a priority to a posterior and in fact all of machine learning is that you can make any a prairie hypothesis about data right so for example aristotle is believed that women, amongst other things, have far fewer teeth in their mouth than men, right? That's a perfectly good a priori belief, but should Aristotle had opened Mrs. Aristotle's mouth, he would have come to a different a posterior belief isn't it so that is the journey of data science and the journey of science so we are coming upon 40 minutes i will open it up now for for questions and we have covered two things importance of exploitative data analysis and visualization, and basics of statistics. Questions. Oh, we are. We won't go very deep into it because there is an entire field of biotin machine learning for which we are not ready yet. It's simple like that. See, frequential is maybe incomplete understanding of probability, but it's easy. Isn't it? It is very intuitive. It helps us convert data and frequency of data into probabilities. So we'll use it for now. There will come a time when we'll become Bayesians. And by the way, a Bayesian comes from a Bayes, a British gentleman named Bayes, who wrote the famous Bayes equation, Bayes theorem. Of course, it's named after Bayes, Of course, it's named after bias because it was actually, who was it? It was Legendre, I believe, a French mathematician who had actually discovered this from what I understand, as some other Frenchman. I think Legendre it was. So there's a very peculiar thing in data science and in science in general. Generally the guy who really discovers something does not get the laws named after the person. It's somebody else entirely. This is called the Stigler's law of eponymy. Stigler is a great sociologist who studied scientists as a tribe of people. He said, if these were a tribe in the jungle, let's study their habits and rituals. And one of the rituals he found is the famous Stigler's law of eponymy. Eponymy means law named after a person. So Stiglitz law of eponymy says that no law named after a person was actually discovered by that person. Right. And then he went on to make a cryptic statement. He said the proof of the law is in the law itself. And everybody was like, how can that be true? How can the law be self-proving and then he revealed later on that actually it was his close friend martin who had discovered this law but because stigler was famous he knew that if he said said it everybody will remember it as stigler's law so that's the stigler's law of course his friend martin was completely in cahoots with him in creating this law it was a joke with them but so it's sort of the history of this field, guys. Yes, go ahead. Question is for exploratory data analysis, what we do. Other than the data, what are the other important parameters or variables for data analysis? See, a very good question. See, when you look at data, the entire, there is an entire piece Asif, excuse me, can you repeat the question for us, please? Oh, yes. Thank you for reminding me of that. What Prashant is asking is when you get data in exploratory data analysis, what are the things that you can do to understand the data? Basically, besides getting descriptive statistics. Yeah, so I'll answer what are the other important parameters or variables? See, the question is not posed like that, Prashant, typically. What you basically say is here is data. Actually, it's a good question. So let me start with that. The question is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying to say is, what I'm trying Manifest. So let me take an example. You have Amazon or some e-commerce site, Walmart. Lots of people are coming and buying things. Right. The manifest data is that this guy had this shopping cart and he bought these items. These are the orders and order entries. These products are selling. That to see the data, first of all, you have the record in the databases. Next, you can do statistics on that. You can do what are the top selling items, what is the average sale price of a shopping cart and so forth. So we can do descriptive statistics, mean, median, merge, histogram. And by the way, standard deviations. And we didn't talk about it. That's what I was coming to. The spread of the data is called standard deviation or variance, and then the higher, well, it gets more esoteric, but the higher moments people talk about, it turns out that mean and variance are the first and the second mode of something called the moments. Mathematicians use the word. There is skew and kurtosis, right? Those are, and I'll explain what skew and kurtosis right those are just and i explain what skew is in a moment skew and kurtosis but the point is that you can have a lot of descriptions but they all manifest the entire industry of business intelligence right have you heard the word business intelligence right analytics right the whole world of analytics, the industry of analytics is nothing but descriptive statistics and top ends and group buys. Like in this location, how much revenue did we make? Suppose you're Walmart. How much revenue did we make from the Fremont Walmart versus the Walmarts in, let's say, Alaska or something like that, Anchorage. So that matters in business. So those are aggregate statistics. All of those are aggregations. You add up those numbers from the different locations. And much of business intelligence is about aggregation and the usual question, the best sellers, the top end. What are the top end locations where we are making a lot of money? What are the top end products that we are selling, the best sellers and so forth? so all of those things are manifest you can do it using sql when we are looking at it if the data is stored in the database you can do it using sql and some small functions and so forth vitables tableau can do that on the other hand data has something else which is not obvious for, why did this user buy this item and not that item? What should I recommend to this person? So behind this data that has shown itself that this guy bought this, there is also the question that this guy did not buy all the other things. Behind that there are there is two different driving forces. What the item is really about and what are the tastes of this particular user needs and tastes of this user. For example, a person living on the 10th floor is most unlikely to buy a lawnmower. Isn't it? But can you infer that from the data when the data doesn't tell you on which floor this person lives, right? So there are hidden signals in the data and the job of machine learning in many ways is to hypothesize and data science is to hypothesize about the forces behind the data the latent forces right behind the data that you see all of machine learning in some sense is recognition of those patterns and those hidden forces when you say we build a machine learning model what you're doing is you're building a hypothesis of that hidden thing that produced this data now there can be multiple hypotheses all of them making equally good predictions right so from a machine data science perspective you say well anything that works that makes good prediction is good are we together so prash, does that answer your question? Yes. Okay. Any more questions, guys? This is Raman. Sorry. Go ahead. Are we going to have any reference material, reading materials that we can sort of refer to or read after the class? Absolutely, guys. That is a significant omission and my scatterbrained activity. I didn't do that. So let me tell you the textbook, the main textbook of this class. It is, are we seeing the screen? It is a website called statlearning.com. In statlearning.com, there is a book, Introduction to Statistical Learning. This book is the bible of this field. It is your best entry into this. Machine learning used to be a pretty hard field and there's a lot of effort to democratize to bring it down to more and more people make it more accessible this is by far the best such effort and we will use this as a primary textbook now these people are stanford professors uh tibshirani and hasty and the two graduate students written and someone is, I forget the fourth name. So this book is really a classic in the field. The second edition has just come out. They make the PDF version freely available. So you can just download this book. We will more or less follow this book throughout this workshop. All right. And I will tell you what the reading material is for each week. So for example, for this week, I would have told you at the end of the class, it is chapter two and the beginning of chapter three. So read chapter one, two, and the beginning of chapter three from this book. We will strictly go along this book. It's sort of a thing. What we will learn is more than what is present in this book, but what is present in this book is a closest reflection thing what we will learn is more than what is present in this book but what is present in this book is the closest reflection of what we are going to talk i will for example follow the notation of this textbook this is for the theory for the lab there are a couple of lab practical textbooks one of them is pandas for everyone right and there's a site and there's another book so please excuse me there's supposed to be a website for the course the course portal. I haven't released it, but on the course portal you'll find all of this information. Unfortunately, I should have had it ready by today, but I'll have it ready tonight. And you should see it by tomorrow morning, a whole course portal with all these resources and material thank you thank you so as if a quick question so going back to your original example so let's say if i am at walmart i'm looking at data points i don't know about the floor and all and i want to see the customers who didn't buy a product so how i will make the call and i don't have so we make assumptions or hypothesis and then we try to see it proves with the data point that we have or we have a different approach that is right so uh sanji what we do is we take the data we hide some data under the pillow right right? And then we give the algorithm only some data to make a hypothesis. It builds a hypothesis. Now we test it out. Is that hypothesis actually working? By pulling out the data that we had under the pillow, the test data, and feeding it to the hypothesis, seeing what does the hypothesis or the model predict and what was the real value. What does the hypothesis or the model predict and what was the real value and by comparing the two we come to the conclusion of whether this model is making good predictions. Are we together. Yes, thank you. So you're waiting for something. Yeah. So I had a question about data and how data collection is actually valuable to us. You gave us an example of like tossing the coin 70% of the time and 30% of the time. If we're giving that data, we would conclude that 70% it's heads. So how do we know that the data that we're getting is actually useful and true to what we're trying to solve? That is a very good question. So let me repeat the question he asked. He said that suppose you toss the coin many, many times and the data that you got, somebody gave you the data. In that data, it turns out that 70% of the times it tails approximately and 30% of the times it's heads. How do you know that you can trust the data? And that is a question of bias in the data. It is a fundamental pathology in the field. See, math can work with the data, but if the data itself is biased, you are fundamentally screwed. So, for example, if I want to prove that all, let's take something completely absurd, right? That people who are teenagers, they are always on drugs, pretty obnoxious hypothesis, right? What do I need to do? I just need to visit specific neighborhoods and then sample into the teenagers there and do their blood test. And then I'll have data that shows that I said, you know, I looked at the sampling of data. What is wrong with that sampling of data? It is not representative of the population. It's a fundamental problem, your sample must truly represent the population. Right? If it doesn't represent the population, right? If it doesn't represent the population, then it's wrong. So how do you remove bias in the data? It's hard, but some of the common sense ways is never trust one sample. Gather many samples, right? And build, train the model and all the samples and make sure if your thing changes. So go to lots of random samples because any small sample will have bias. However diligent you have, if you have sufficiently small amount of data, you will have some peculiarities in the data. So let's say that you have a lot of patients, and this happens in the medical field all the time. Most of the medical research is done with a small number of data points a few patients most researchers will publish oh i have seven patients and this is something that i observed most of the time that research is not reproducible for a very simple reason that thing was just accidentally true for those seven people are we true are we getting the point? And so, I mean, it's not that the researcher doesn't know. He's just saying, I happen to observe this. It may or may not be true in the larger population. But as a conscientious scientist, you need to say that because your sample size is too small, right? So you need sufficiently large sample size, and still it's not possible to remove it. So here is a classic story that is true. It turns out that Amazon came out with a facial recognition system, the Recognitor or something it was called. It was supposed to be used by law enforcement, and in fact some law enforcement was already using it or something, and Amazon was thinking of it to be a very successful product. It could look at a picture and tell who it is and so on and so forth and what type it is. Some people, I think some professors or some people, did a very simple experiment. They knew there's bias in the data, so they did this experiment. They took, in the US Congress, they took the Black Caucus, all the Black congressmen and senators. And they put that picture to this AI machine and said, who is this? And in every single case, the members of the Black Caucus were recognized as rapists, serial killers, and so on and so forth. The Black Caucus was not am amused as you can imagine right now the whole question is obviously amazon did not consciously inject the bias or they did not know that there is bias in the data so how did this fiasco happen this fiasco happened because look at the amazon campus and look at software engineering. Look at this class. How many people here are African American? Zero. The campuses of tech companies is populated by people who are predominantly Caucasians, Southeast Asians and Asians. Isn't it? Whether we like it or not, it's a fact. So when they are creating the software and people are told to gather data, guess what data they are doing? The easiest data set is, oh, employee database. Right? Go get the employee database and let's start training it on that. And so when you do that and then you get data from here and there and when you want to get data from the african-american pool i suspect i don't know if this is really true one of the easily accessible people is you go to the prison system and say can i have your database right or something like that i don't know what really happened who knows what happened uh obviously we shouldn't say something negative without knowing the whole fact but the fact that we know is the system was deeply flawed and it is that started a whole movement called explainable ai in fact in this workshop one of the key topics you learn is explainable ai we cannot trust that the ai is making correct predictions. It can actually destroy society. It can destroy our civil liberties. It is doing that. Not that it can, it is actually doing that. You can go to some other countries which are essentially become police states because of AI and the huge amount of facial recognition and see how it is tearing apart the entire fabric of society. And that is the danger of indiscriminate use of AI, right? So anyway, that's about bias in data. It's very subtle and it's hard to tell. I'll end with a final example about a bias in the data. This story must be, it is hard to believe that it's true, but it is told so many times in the field because it has something worthwhile to teach. So apparently the military, when you go into a theater of war, you don't want to hit civilians. The first rule of the military is never kill civilians. It's a basic tenant. Ethically, it's wrong. And of course it's bad PR. You only hit enemy combatants. Now, they were supposed to find out using AI. They came to a Silicon Valley startup and said, can you tell us by looking at the picture of a vehicle, whether it's a military vehicle or a civilian vehicle? They said, well, that's very easy. Military vehicles look like Humvees, right? And tanks and whatnot. And civilian vehicles look like Prius or something like that. And the two are vastly different looking. We will train a completely nice AI in no time. And so all that we need is lots of pictures of civilian vehicles and lots of pictures of military vehicles. And sure enough enough the military said no problem and one can imagine that some general giving a command to his army and saying troops go take pictures right or vehicles uh civilian vehicles military vehicles and send them and all the soldiers saying yes sir and gathering lots of pictures lots of pictures came about all the soldiers saying yes, sir, and gathering lots of pictures. Lots of pictures came about. The startup immediately trained it. In fact, you guys will do a similar experiment in this lab soon. You train the AI. It was working perfectly in the lab. Military came and tested it with their own hidden test database of new pictures of military and civilian vehicles, which soldiers had gathered over again, and it worked perfectly in the lab. When you deployed that model in the field, unfortunately, it was an unmitigated disaster. It simply didn't work. And everybody was confused. Why is this not working? It's working in the lab. After a lot of digging, it turned out that the cause was obvious, but it was not so obvious. It was obvious only in hindsight. You ask the troops to take pictures of military vehicles. When will they take it? Troops come back the whole day they are out patrolling. In the evening, they come back to their base stations and the base stations are filled with military vehicles. You ask them to take pictures of military vehicles, what will they do? They'll go about in the evening or early morning taking pictures of the military vehicles, isn't it? And they'll send it to you. The ambient light in the early mornings and late evenings is low, it's dark, relatively dark. And they are patrolling in the daytime and while they're patrolling all around them, what are they seeing? Civilian vehicles on the road, isn't it? So they're taking pictures of civilian vehicles. What is the condition during daytime of lighting? Lots of bright ambient light. So all those vehicles look bright. And so what did the AI catch on to? It said, well, this is a very easy task. All I have to look at the ambient light in the picture. And so it was making perfectly good predictions in the lab and it was failing in the field. And so that is a bias in data is very subtle. You always discover it in hindsight and 2020 hindsight is always 2020. it looks so obvious in hindsight the trouble is it is not so obvious the biases and that is one of the cautionary tales in this field it's a huge tale ai can destroy this world and is destroying the world uh we do not know whether ai is the force for the good or for the evil so in other words whether you you folks will be remembered as monsters by your grandchildren, right? Or forces for the good, the jury is out. But that is what it is. And so use these tools that you learned responsibly. One follow up question. So let's say when I'm looking at the coin example, 300 and 700 out of 1000, maybe we can make a guess so let's say I am working for you I get the data I say oh something not good but when I'm working on something I have no context right mainly for pharmaceutical industry or airline industry then how do we infer that the sample which we have or data we have is not sufficient and we need to do more work, right? So even before we conclude, right? Somehow I need to make a decision based on some certain confidence. But if even before I do, I need to know that data looks good. So usually is there a practice or something we just do to go with the fact what I have. Sanjeev I got it, so sorry to interrupt you. The basic point is that the way this field data science works is with this statement. If the data is to be trusted, then these are the observations and models one can build with it it never says that these facts are true it only says that if the data is to be trusted this is the story this data tells and this is what we these are the models that we can build it remains forever the fundamental if remains forever are we together it's a fact of this field yes thank you all right guys so now I will start with the next 40 minutes and we have a lot of territory to cover so guys the way it will work it is 11 uh 24. we will go for another 40 minutes that will put us at 12 close to 12 a little over 12 and then we'll again take some questions and then we'll have the last 40 minute session that will conclude all theory for today we need to do practicals lot of hands-on things this whole workshop is a large part largely hands-on but today's is exceptional today it's a bit theory heavy because i'm introducing you to the field right but get ready that as we move, as we make progression through this workshop, we'll get more and more theory heavy. Go ahead. Guys, is there any person who is not fully vaccinated? Yeah, please. It's a basic rule. We should all be fully vaccinated, right? And generally, the state law mandates a mask. If you taking it out please do it responsibly be sure that you're fully vaccinated right and if in doubt stay apart all right folks so the first part we talked about exploratory data analysis, data visualization, right, and about data and the biases in the data and the risks in trusting any data too much, right, especially small samples of data. So that's an important lesson. Now, what we are going to do is I'm going to talk about a few, go deeper into statistics and bring about some core concepts. I will talk about something called kurtosis, skew and variance, these three things. We'll talk about covariance and correlation, sequence and causation. So these are the concepts that we will learn in the next 40 minutes and then we'll take questions on that. Then we'll take a lunch break after that. And then we'll continue. So see, when you have data, like you see the data on my screen, data could be, let's say, good data could be distributed uniformly. Most, good data could be distributed uniformly. You know, most of the data is uniformly distributed. This is the most common curve that you see in nature. We all call it the bell curve. People call it the Gaussian and so forth. This means most of the data is around the center. This was discovered by a person named Gauss. Gauss, a great mathematician, a mathematician who is and deservingly called the prince of mathematics. As we do machine learning, you will find that Gauss dominates machine learning. He has a lot of contribution or his work, Gaussian distributions, Gaussian functions, et cetera, et cetera, are all over the place. And in fact, the first model that we learn about comes from him. Some of the core concepts And in fact, the first model that we learn about comes from him. Some of the core concepts come from him, right? So, but we'll come to the machine learning part later in a moment. This is it, data that is balanced. One example of this data is, suppose I were to give you a thermometer and I were to ask you, what is the temperature in the room? And each one of you, I gave you a thermometer. Each of you i gave you a thermometer each of you would report a temperature in the room let's say up to two decimal places and you would come up with 69 670 something or in between with different degrees what you will notice is that all the values when you plot them they are centered around some mean value mean this is the mean of those values, temperature values that you'll come up with. Would you all agree that if the true temperature is this mean, some instruments will under-report it, some will over-report it, right? But in general, errors would be symmetric generally. Are we together? Generally you would expect that errors instrumentation or measurement errors are symmetric and most of the values are peaked around the real value the mean value which may be let's say 69 degrees temperature Fahrenheit in this. Common sense so so far actually this is very interesting because this is literally the the origin of the bell curve there was a question posed what is the distribution of errors in measurement so people came up with all sorts of hypothesis one person put it like this they said that the errors are like that. Linearly increases towards the true value and linearly decreases as you fade away from the true value. Another person, which was Laplace I believe, he came up with this. The trouble with this function is this is also pretty close to truth, you know, very very high around the center but not so much the reason people stuck their nose up mathematicians tend to be finicky they just stuck their nose up and said not this why because look at the peak at the top it's pointy generally mathematicians don't like pointy things well Well, more formally, they don't like non-differentiable functions. They only like smooth curves. So that it wasn't. And then came Gauss, and he came up with this bell curve. It has all the properties. It is symmetric. It is bell curve. And actually, you can mathematically prove, and that was his classic proof, that errors have this distribution. proved, and that was his classic proof, that errors have this distribution. And thus came the bell curve. Bell curve's main quality is it's symmetric. Now, let us take something else. Now, typically, bell curve has one quality. It can go to the minus infinity and plus infinity, though it doesn't really go there. Like people's heights, for example, they tend to look like a bell curve, but nobody is negative height and nobody is infinitely tall, right? So that sort of thing, it's concentrated. Now in a bell curve, the most important thing is, is this the bell curve or is this the bell curve or is these are all bell curves? do you see that having the same mean what do they differ the white the green and the pink bell curves how do they differ variance is more or less right the air is more that is true if you look at the plumpness right The air is more. That is true. If you look at the plumpness, the red bell curve looks a little bit like me, spread out. And the green bell curve is narrower, much less spread out, isn't it? So what you see is that the data, because this is the frequency plot in some sense or the probability plot of data, what you see is that the red bell curve talks about data that spread much further out. There's a big variance in the data. The green one is more tightly knotted data. If you were looking at measurement equipment, would you buy the red thermometer, the white thermometer, or the green thermometer? It seems to be far more precise and peaked in measuring temperature, isn't it? So that is it. So another statistic that we discovered therefore, this is called this width. Right, this width is important and it is quantified by a variable called variance. How do you compute variance? You take a value, for example, this point. Let me take this point in a different color at this point. Yeah. A point here. Let me call this value X. So let's think about it. How far it is. What you're trying to quantify is how far it is from the mean right from the expected value so you say x minus mu is a measure but then it could be in the positive and negative direction so typically what you do is if i square it what happens now it's a positive number and generally squaring is a better idea for a whole variety of reasons, which you won't go into, which is the whole work of Gauss. Now, what is this? So every data point, what you do is, now I'll do something funny. I'll put a subscript to it. So any data point, this is X1, this is X2, and in the arbitrary case, it is Xi. xi i the subscript means any data point let the data point be i its location is xi xi minus mu so now if you ask well what is this fatness one of the ways that you can do is you can add up all of this x 1 minus mu squared plus x 2 minus mu squared plus x3 let's say x3 minus mu squared all the way till x n minus mu squared well that adds up there's a problem though the more data point i add the more this total keeps on increasing but our variance is not because variances the spread is not dependent on how many data points you're looking at so what we need to do is we need to divide it by what? N. N, the number of data points. And so this is your variance. So is variance something you can hold in your hand like this cup of water? No. So what sort of letter should be used for this? Greek or Roman? Greek. Right? And so there is a Greek letter letter it's for reasons that you'll understand it's called sigma squared right and the square root of this is called standard deviation now comes a interesting fact which is important and not well understood when you look at so this formula we can let's be fancy now, more mathematical. We can write it as sigma squared is equal to, we use this very peculiar sum notation. This stands for sum, sigma, right? Sum over all values of i, xi minus mu squared, 1 over n. Now, this thing is nothing but this thing. Sum means just go on adding for all values of i, i from 0 to 1 to n, 1 to capital N values. The two are exactly the same. So we just a refresher on mathematical notation. So this is the variance. Now comes a subtle fact. Sometimes you will notice that people don't use n, they use n minus 1. They will write it as i xi minus mu squared. So you will see a formula that is often written like this. Now you say, well, where did the minus 1 come from and what is it signifying go ahead i equals zero i it from one to n the n n data points right i is x1 x2 remember see x1 x2 x3 so that's it what do you mean by zero up there you define i equals to one i was wondering if there's i equals to zero two n that's what there's oh no no no that's explanation no no no it's not that's a good one you're programming you into that no go ahead sanjeev i believe yeah i'm guessing i don't know so so because one of them is the mean no it's not the reason is actually quite subtle see what happens is when you take the you never have access to the entire population so suppose i to tell you what is the average temperature of all people no matter how powerful you are you won't be able to go and measure the temperature of 7 billion human beings. Not practical. So what will you do? Suppose you encounter a species, you have never encountered humans. Imagine you are an alien, you have just landed and you feel like knowing what's the average body temperature of humans. What is the likeliest thing you will do? You will pick a sample, right, or the formal word is sub sampling, but you'll pick a sample of human beings, and you will take their temperature, and then you will come up with their mean, and you will come up with their variance. So it a fact though that what happens is when you take samples of people there are two interesting results and these are this is an important fact for you to remember i'll talk about something called the center mean theorem big fancy word but you'll see that it's not so hard and the explanation to why a minus one helps when you derive your conclusions from sample rather than the whole population that you don't have and you rarely have access to the whole population so what you do is what happens is let's say that people's actual temperature goes in this range true True, you happen to know the real facts. What will happen is when you take 15 people, you will get a data distribution, some here, some will be around here, some will be around here, some will be here, some will be here, some will be here, some will be here, some will be here. Right? Every sample you put, because it is a finite sample, it cannot have the spread of the entire thing. Unless you truly happen to be lucky, your sample contains the min and the max. You realize that. Until you are lucky that your sample has both the min and the max, quite likely your min and the max will be less than the population min and max. What is the insight that you get from that that the variance that you will measure will be less than the variance of the population do you see that guys is that obvious so you say that variance is a biased estimator right in the sense that it is the when you just use one over n, average of the squares, it is a biased estimator. Biased estimator means what is true of the sample is not necessarily true of, sorry, what is true of the population is not necessarily represented well by the sample. The sample has bias. Are we together? That's what it is. And this word is very interesting actually. It's a very common thing in interviews. Most people get absolutely bowled over with their tell, what's the difference between a biased estimator and unbiased estimator? Right? So this is it. The difference is what that estimator, if it says something about the sample, is that really true for the population or the two are different answers, right? So what happens with the standard with variances, you make it unbiased, right? Because, you know, you see the spreads are less. All you need to do is slightly increase it, just a little bit of edge, increase it. Means the number that you're dividing it by, just shorten it by one. A little bit. You shorten it and so we won't go into all the reasons, but it just works. It works for that. So you get an unbiased thing. And that is why one of the common things, oh one second, interview questions is they'll give you you a they'll tell you it's a sample data and then say what is the variance and you fail or pass based on whether you get minus one there or not it's a very common thing to do range it you take this explanation if you're looking at the context of understanding variance apply that same illustration to mean understanding me. I'm coming to me. Okay, coming to me. See, okay, so let me explain that. Let me actually this picture that I drew was a pathetic picture let me do a better job so i'm saying that suppose the value goes in this interval right and you pick points let's pick five points you know that this is a true mean you pick points one two three i don't know four five do I look like five points? One, two, three, four, five. Yeah, five points. Oh, that is right. So the question is, what is true for variance? Why is it not true for me? Right? And the answer is actually quite interesting. I'll repeat what we did with a better diagram. So suppose I have these five points. Look at this. When you look at this, think of the mean. You know that this is the true population mean. Actually, let me give the samples a colour. One, two, three, four, five. a color one two three four five of this population where would you put the mean the sample mean the sample mean you would put it maybe somewhere like this this is the sample mean right and this is the just let me just say this happens to be the true, true population mean. True population mean, isn't it? Now, this is just one sample. And that's the dangers of just doing dealing with samples. Don't trust just one sample. But look at this. What is the spread of this data the min is here the this is the min this is the max and suppose it was not five points but it was a bell curve your bell curve would essentially go somewhere like this isn't it whereas the true bell curve would probably be a little bit like this isn isn't it? Roughly speaking. So you realize that no matter, and then you can take another sample. You take another sample here, here, here, here, here. In this case, another sample's mean is sample mean. So you notice that sample means are not hitting the true mean by nature, by definition. You can take bigger and bigger sample, and it will hopefully tend towards the true mean so that in the asymptotic limit of taking all the points, the whole population as the sample, you will achieve the true mean. But each sample, one one second each sample has its slightly different mean but which is okay which is still tolerable because it's close to the real mean but we'll come to that but you also realize that the variance of this is now here but whatever variance you give to this both both the green and the red sample, or the pink sample, you would agree that they're underestimating the true variance of the population. Isn't it? And so, for a mathematical reason which we won't go into because of lack of time, but clinic hours, remember, you can sit with me and we'll do this on paper. What you need to do is this pink, you somehow need to stretch it out to this size. You get the idea, right? Somehow the sample variance, you need to stretch it out to the whole thing. So how can you do it? You notice that it is some number. It is sum over xi minus mu square over n numerator over denominator what happens if you make the denominator small number goes up it turns out that mathematically i mean it's a just if it's just not a hand-waving argument there's a rigorous way to prove that the right correction that you need to do is just subtract one from the denominator and when you subtract one from the denominator then it becomes an unbiased estimator right so sort of it's sort of the wording that people use uh for these things it becomes unbiased right so whenever you take a variance of the population of the sample always remember to subtract one from the denominator if you have the entire population in your hand then it doesn't matter now in practical terms it means the following when your data set sizes are small remember to subtract one when you're dealing with big data which seems to be more and more true in our situation see guys we're in a paradoxical situation. We are inundated with data. Big data is fact of life. Millions of photos are uploaded every day. Millions and so on and so forth. Everything is now in millions in data points. You look at a Twitter feed, right, Every day, hundreds of millions of messages and posts and this and that is happening. Human beings are a gregarious species. If all of this shows something, it shows that we like to chatter, we like to Twitter, right? And so every time we communicate, data is produced. We live in the world of big data. Instrumentation device, an airplane takes off from New York and comes to San Francisco. In the process, four terabytes of data is produced and stored. You want to do any mining of the data, you know that there's a wealth of information. You can actually figure out if any part is in trouble and do fixes are fit and so on and so on, you can do a lot with the data. So we live in the world of big data. Instrumentation is data. So Raman here has a question. I guess Raman you can go ahead. Oh, yes. Thanks. sorry for the background noise. My question was, I think, as if sort of answered it, as if I was going to ask if n minus one is more of like a political statement that we are trying to say, hey, this is the sample data and not a true data, as opposed to n minus one in a big data set actually makes a difference. Like I have. Yeah. Yeah. So that is the thing strictly mathematically speaking and minus one must be done for samples. Are we together? So that is the math. Math says, if you deal with sample do and. So it's basically a way for me to know it's a sample and not a true data, but way for me to know it's a sample and not a true data, but in actual value that comes out with it, won't be too far from the actual data. Absolutely. And that's the point I'm making that we live in the world of big data. When you're dividing it by 5 million, would you really care that it is 5 million minus one? It's not going to change. So this is an argument often made by people who say that, forget about all these subtleties about a biased and unbiased estimators. In big data world, it doesn't matter. It's true. But not all is big data, guys. One of the big frontiers of machine learning today, the next 10 years of machine learning is going to be machine learning on biomedical data. Right? It is the frontier, next 10 years. We have Varish here who is doing an internship with me on cane for the blind. Right? So this is it, next 10 years. Medical data inherently is small data. Of course, you can say x-ray is big data, but not really. It's one data item, which is extraordinarily large in size, but you don't get many x-rays. One of the reasons, for example, AI failed miserably in creating a diagnostic tool for COVID-19 is not because our algorithms were not good enough, were good, but in the amount of time needed, there was not a proper gathering of data to train the AI. So we reached an accuracy which was very good, but never, could never beat the NOS test, what is that called, RT-PCR. the the nose test what is that called rtpcr right and rtpcr became the gold standard and it is it is a case study actually there's a lot of hand wringing happening in the field in the ai field that how come we missed such a big opportunity we couldn't move the needle at all in the medical diagnostic community it all has to do with data so you therefore when you don't have big data you have to be cautious of this right uh ramon does. So you therefore when you don't have big data, you have to be cautious of this. Raman, does that answer your question? When you don't have big data, when you have small data, you have to use this. Yeah, thank you so much. Sachin. The samples that you show over here, right? Yes. Where you have taken those things, they have some locality where they are around there right now on a global scale if certain things were taken say part of the left and the part of the right at what point do you say that hey you know i've taken everything that is that is covered because you really don't know what this happens so the example is because right which is where we are going we are are going there, center limit theorem. So guys, if it was height... If it was height and you go to kindergarten, all your means... So suppose here is a brilliant person, a brilliant researcher who says the right place to measure height is in kindergarten. And this is how the sample looks their mean would be like this right and another person believes that the right place to measure people's height is in a basketball court right and so in a basketball court you get people's heights like this. So this is another meme, right? So what you realize is samples are biased. Sometimes silly bias like this, the person who went and measured height only of kindergartners couldn't be sillier as a scientist, but maybe that's the only access you have. But suppose you are measured taking height and you know comfortably that they're going to be many, many samples. It is perfectly legitimate to say, okay, go and do the sample for kindergartens, you do it for basketball players, you go do it from here and so on and so forth. And that brings us to one of the most profound results in statistics, one of its crown jewels is the center limit theorem. What it says is when you have a lot of means, you see every, every sample's mean is different and irrespective of how the data is distributed. So for example, the salaries of people are not necessarily high tends to have a bell curve distribution, but people's salaries, as you know, your, your, your people's salaries, as you know, your CEO makes how much? Yeah. So typically, let's put it this way. The CEOs earn in one year, what you, your children, your grandchildren, and your great-grandchildren will make in their entire lifetime. It is a fact of life in modern capitalism. Big company CEOs make that much. So if you look at the salary distribution of this population, it will be like this, salary distribution of this of population so it will be like this very very long tail right so this data so suppose somebody goes and does an employee survey of salary distribution somebody comes up with a curve like this curve like this curve like this curve like this curve like this right so you notice that the actual salary distribution is not symmetric, it's skewed, right? I can make it even pronouncedly more skewed just to illustrate the point, I can make it like this. So do you see it's very skewed distribution of the data. Nonetheless, one of the most profound results in statistics is the center limit theorem, which says that even whatever the skew, underlying skew in the data or distribution of the data, when you take the mean of many samples, right? You come up with lots of mus. So you come up with mu one, mu two, mu three, mu four, mu n. So n people went to n samples right and here is the result that is utterly beautiful no matter what the underlying distribution is the mus so there is one let's call it the royal mu right all the mus will be distributed around it says that the mus will show always a bell curve distribution. That's why it's called the center limit or center mean theorem. That the means of the sample will always show a bell curve distribution. So you can take the mean of the mean as the closest you can ever get to the truth which is why if you remember in the previous election, not this election, the previous election, all the polls were sure with 90% chances that Hillary would win. Right. And then they were unpleasantly surprised. Could you repeat the exit poll from elections, all these pollsters, they work with samples and the answer is yes, they work with samples because they can't go around the whole population. Most of you when you get phone calls, telemarketing phone calls to give your opinion, what do you do? You promptly keep the phone, isn't it? So they get very small samples, they make conclusions from that. Usually in marketing, they not only do research, they market themselves. So they claim to be more accurate than they really are and the results show. So polls are very unscientific means of coming to the truth right and generally they are not enough polls so there was a company a 538 you know a website called 538 which tries to do exactly this it takes it it does meta learning it takes the results of the different polls and how much they can be trusted as some sort of, well, it is more complicated machine learning, but a very, very, very naive way and incorrect way, but still meaningful way to say it is, they sort of take the mean of means, a weighted mean of means of some sort, right? To come up with a balanced sense of what is really happening. But even 538 can be wrong. In the Hillary election, they were wrong. In this election, 2020 election, they got it right. So you never know. But if you have data, see politics is complicated. Human emotions come in. Whenever human emotions come in, you can't see the truth. Somebody in the Indian system of thinking, there's a lovely metaphor that if you have a bucket and you have pebbles in the bucket, nightstone pebbles in the bucket system of thinking, there's a lovely metaphor that if you have a bucket and you have pebbles in the bucket, nice stone pebbles in the bucket, and you have water, if the water is turbulent, you can't see the pebbles clearly. The water needs to settle before you see the pebble. Usually you say it about a clear lake, right? To see the reality, which is a pebble at the bottom, the water needs to be still. Of all the places, politics is a pebble at the bottom, the water needs to be still. Of all the places, politics is not where you find still waters. So you never know the truth. By the way, that's an opinion, that's not a scientific fact, so don't take it down. So, well, okay, that's mu. So this is the center limit theorem we got introduced to. Pretty good fact, isn't it it how many of you knew central center limit theorem square you knew of course good and so lovely thing actually the first time you come across these theorems you're amazed that it's true that it's true for any distribution so now you know why you should take lots of samples and take the mean of the mean right and how to deal with the variance now i'm going to do quickly two more concepts. See, what is the difference between this distribution of data or frequency distribution or density plot and this A, B? And let's go down. And C, which is. What can I say about a most of the weight is on the left side B most of the weight is on the right side and C most of the weight is in the middle. Right? So that's sort of it. So here's a way you remember it. You call it this thing. You call this Q in the data. Word is Q. Q. Are we together? This is the skew in the data. And here is a lovely way that you can think of it. Make it into a little. Which way is a lovely way that you can think of it. Make it into a little... Which way is this one looking? This bird, imagine it's the beak of a bird. Which way is this bird looking? In A, looking to the right or the left? Right. Right. Right skewed data. So you see, this is a Right skewed data. So you say this is a right skewed data. What about B? Which way is the bird looking? Left. So this is the left skewed data. Now this is something at least maybe I'm slightly dyslexic or whatever. I always get confused. So whenever I see data and I have to say which way way the skew is i always put a dot and make it a bird it becomes it is that our children's slide exactly which way would you fall down come down the slide and this of course is no skew okay now skew can be measured mathematically and this is your homework. I give you the expression for variance. Take this as an exercise. Figure out what is the mathematical expression for skew. It is the third moment. It's called the third moment of data. Mean is the first moment. Variance is the second moment. Skew is the third moment, but we won't do moments, but find out the formula for the skew. Now that is skew and finally there is another fact. Look at this data. It is even with skew, let's say it's right skewed. It is like this versus, it's like this versus sorry it's not like this let's say that the data is well i got the green wrong oh sorry it's like this or it's like sorry it's like this now this is this is green, by the way. This is in between stage of gosh, I'm getting my colors all mixed up and red. And these are the distributions. The density distributions. So within the yellow. Yellow, the green and the red, they all have the same skew, the right skew. Yet you can see that there's something different about them. And the variance. Variance is different, yes, very good. There's something else. See, what happens is, look at the red. It seems to have a heavy tail. Most of its mass is in the tail, isn't it? Look at the yellow one. The tail is rather flimsy. It looks like a kite, isn't it? Light tail. And for bell curves, it becomes, it's sort of, it is this. There is a word for this. It's called kurtosis right the leptokurtic is light-tailed l for lepto alpha light light-tailed then there is i think the word is am i getting it right platy kurtosis right it has a big tail actually does platypus have a big tail heavy tail i don't know uh look let's look at the picture of platypus is the opposite is it it has a short tail okay so not platypus okay but but so so the how heavy the tail is matters. Now why does it matter? Why would we care about kurtosis? Actually the whole e-commerce world relies on the fact that data or items that sell and how much they sell tend to have heavy long tails. What it means is, and we'll take a small break with this, we're coming to it, and then we'll do, in the next session, we'll do covariance, etc. See, what happens is, imagine that, let's take books. Amazon started out as a bookstore. Think of the bookstore around the corner, the mom and pop bookstore, right? A mom and pop book store right a mom and pop bookstore can have how much how many books it's a one-room shop and they are usually filled with wonderful books it's a treasure hunt i love going to those bookstores and but they can they can only afford common sense says that they keep the best sellers top end books that sell in each category. But the trouble with top-end is that yes, they sell a lot. They belong to this part of the bell curve, the top-end thing. So hang on, the distribution for top-end is like this. This is how those distributions are. These distributions are called inverse power law. Power law. Often you use it because they all are 1 over x to the n distribution. If you ever plot 1 over x, 1 over x squared, 1 over x cubed, you'll see this kind of a behavior. They're also called the Zipfian distribution distribution which is a slightly different formula but essentially coming to the same thing zippian distribution and most people call it the common people calls it the long tail distribution what does it mean a very few items are best sellers in books right A very few items are bestsellers in books, right? And videos and songs, right? So ever log into YouTube anonymously, and what does it throw? It throws probably the most popular videos. And those are absolutely terrible, horrible to look at. You're ashamed that these things are the most popular in the world, right? And you want to move away because where does our taste go? Our taste is the taste of a few other people. There are other like-minded people out there. But most people, their taste belongs somewhere here. Occasionally, a little bit of the best seller, but mostly somewhere in the long tail. An example would be we like books on machine learning i hope by now right when you go to a bookstore you would love to have books on machine learning do you think machine learning books are bestsellers no machine learning book has ever sold in millions or hundreds of millions right those numbers are unheard. A bestseller is like 10,000 copies, right? So people write machine learning books as a labor of love, not because they're actually going to make some money out of it. This is true of most textbooks. Textbooks are labors of love. Somebody gives years of their life to write the textbook. And we pay more price, let's say $50 for a textbook, but trust me, that $50 is nothing compared to the years of hard work that has gone in writing the textbook and the decades of learning to produce a man who could write those textbooks. So anyway, textbooks don't belong to the most popular. They belong to the tale, long tale, isn't it? Right? And so music, like look at Netflix. We watch Indian movies here in the diaspora. The Indian diaspora watches a lot of Bollywood movies. Netflix started out as a US company. What was the total market for Indian movies? Indians apparently are 1% or so, 1.5% of the US population, the Indian diaspora, or even less. I don't know what that is. Politicians say, population-wise, we don't matter. You must have heard the statement. We just don't matter. We are not sufficiently large to matter or to even be worth canvassing for our votes and so forth. But we watch a lot of Bollywood movies. They are not in the best sellers list. They are way down. And so now going back to the bookstore, a small bookstore can only keep the top, let's say, 20 books per subject. Then came Barnes & Noble and Borders. What did they do? They built a big block, one of those big boxes. They filled it with more books and brought in a coffee shop and a kids' play area. And what did they do? They completely wiped out the little guys. Why? Because they could keep more of the front top sellers. They could perhaps keep the top 150 books per subject, 100 books per subject. But the whole value proposition of Amazon as the best bookseller was, think about it, counterintuitive. 20 years ago, you would not have picked up a book and paid for it without actually looking into it, isn't it? We like the touch, the smell, the weight of books in a very tactile level. We like to read into it, browse it, and then we buy books, isn't it? Books are not cabbages. We don't just pick them up and throw them into our grocery cart. We do that. And yet here comes a company that dares to say, we can sell books online and they succeeded for many reasons one of them is the recommender system the other will come about because they can make better recommendations but one of the reasons is a online retailer has no limit they can fill their shop with the whole long tail isn't it because every time somebody buys a book they can call the publisher of the big to ship the book to the to the buyer do you see that right they have an endless near endless capacity or inventory now of course amazon sells a lot of things the last time i was told, they stock and sell close to 300 million unique items, I'm told, or 100 million unique items or something. Just the number is mind boggling. There's no physical brick and mortar store that can possibly have that. So long tails matter. And the whole e-commerce world, the success is where is the weight? The weight is here in the long tail. So the thing is, it isn't that it's the long tail economy. It is the fact that it is the long, heavy tail economy. The tail better be heavy, worth monetizing. And that's what it is. And thus, I hope it drills into you the concept of kurtosis with that i notice my time is up we'll take a 20 minute break and guys this is the second theory session we'll have just one last session and after that the rest of the day will be there immediately available the whole video will be on youtube right it will take time see here's the thing by evening this thing is being recorded you will get the zoom share link but a polished video cleaned out video usually takes up to a week for five days right then with all the right support vectors logo and you know a bit of jingle and whatnot and cleaned out will come in a week right three four days but this will be this is immediately available uh zoom takes well for such a long recording Zoom will take approximately one hour to process it, one hour later after today's session, it will be available. And our teaching assistants will post it for everyone. Questions. What's this interesting guys, all of these concepts? concepts excuse these are very elementary concepts today is basics we're getting deeper and deeper into it but today was statistics essentially right but we are getting now we'll touch the the shores of machine learning by the time we leave i'll say this is raman sorry is there a difference between machine learning and deep learning is there a difference between machine learning and deep learning very good there a difference between machine learning and deep learning? Very good question. Let me answer that question with a figure. See broadly, these are intersecting fields. There is AI. Artificial intelligence was the pursuit of intelligence and machines. We'll talk about it actually a lot in the next week or maybe today itself by the third session. about it actually a lot in the next week or maybe today itself by the third session inside it machine learning has robotics it has machine learning sorry ai has learning within machine learning one popular algorithm one family of algorithms that is highly successful these days is called deep neural nets right and using our more sort of a popular marketing term is deep learning so deep learning is a class of algorithms in machine learning right we will learn it we will learn where it works well and where it doesn't work so well so far these days right so you'll learn all of that does that answer your question ramon yes it does thank you i had a question about variance um so in the formula that you showed us it had one over n uh so do we assume that um all these data points are uniformly distributed because we are see there is a word see you need to take so now we are getting deeper into the knowledge it's a very good question actually what do you assume about data you first you assume that the gatherer has made a best case effort. It's a sufficiently random sample. But let's say you random sample from people and you're looking at heights. Each of the sample, you expect that if you take good random samples, they will be something called IID, independent and identically distributed. Means each sample will show the same shape, identically distributed means each sample will show the same shape identically distributed and they will be independent of each other each of the data points right so you don't you don't take consecutive values of the data deliberately you take random or identical independent and identically distributed that That is why the word is IID. Independent and identically distributed. It is the gold standard data should be iid right so it shouldn't be that one sample you take from a bell curve another sample you take from something completely different it shouldn't be like that does that answer the question was that was that yes yes good any other questions guys before we start into the third session? Would you like to take a five-minute break before we get into the next session? We're done with basic statistics. Now we are going to touch the shores of machine learning using covariance and correlation and so forth. So let's take a break. So, in the previous session, theory session, we learned about a few key statistical measures. We learned about, for example, kurtosis, we learned about skew, we learned about variance, we learned a little bit about how to fix the variance value of when we deal with samples. We learned a little bit about center limit theorem and we learned a few definitions of statistics. Now, I'm going to, those were all descriptions of what I call scalars, right? The word scalar or something, basically temperature, pressure, temperature pressure etc one dimensional data right one dimensional data is a fancy way or scalar is a fancy way of saying basically numbers right given a bunch of numbers they will exhibit those attributes mean median mode variance right skew kurtosis all of those things are descriptive statistics of a bunch of numbers. But then data comes often in, and one dimensional numbers are also called scalars. Let me, I'm just starting the timer. Yes, they are called scalars. What are scalars? Can I increase the font? Yes, indeed. I can. One second, please. So this is our textbook. But at this moment moment I'm not projecting the textbook I should go instead to this. Is it looking small. It's good. Okay, so, little bigger. Okay. So, So scalar, we all have a scale. I used to have a scale here. We all know what a scale is, a linear scale. From the scale comes the word scalar. When you measure the length of something, it's a number. And of course, now you generalize from that. Temperature is a number. Pressure is a number. One-dimensional data. But in real life, data comes in many dimensions, right? We will start with two dimensions today to develop intuition or develop statistics of two dimensional data. So suppose you have data along X and Y, right? A classic is suppose for everything, for liquid or for gases, you all must have heard the law, the inert gas law, the ideal gas law, sorry. PV is equal to NRT, right, at some point. You must have read that, but we won't focus on that. So imagine that temperature remains constant, you just want to keep it. So don't focus on that. So imagine that temperature remains constant and you just wanna keep it. Then you take a balloon and you, or you take a piston or something and you compress it, you decrease the size of it. What will happen to that? Pressure is going to increase, isn't it? So things like that. So what you can say is that if you plot the data, let's do that. You plot the data let's do that you plot the data in two dimensions and you will see an observation that is different values of volume and pressure you may see so i'm just making a graph it's not it's not high fidelity to the mathematics but just making an argument. Suppose Y is a pressure. This is the volume. X is the volume of some amount of air, right? And you keep the air finite, the temperature the same. You squeeze it, then pressure goes up. Look at the tires, right? And the opposite is true. You expand it, the pressure goes up look at the tires right and the opposite is true you expand it the pressure goes down so this happens to be two data two-dimensional data we will use this sort of data as examples and ask ourselves what is the appropriate statistic we are searching for when we are dealing with two variables x and y the most obvious thing that comes is is one dependent on the other when we see data like this we tend to say well y depends on x right it's a common thinking because we go through high school and they're usually why is a function of x. Right. But so you say now we are going to be mathematically more precise. We'll ask this question. What is the relationship between x and y? We won't say that x causes y. And just just be careful, guys guys like i'll give you an example just we all know this we all have heard the statement causation and correlation are different things correlation is not causation today we are going to be more precise and understand it but i will be even more precise and ask a simpler thing sequence is not causation. We know what causation is. Did X cause Y? Right? For example, it was raining and your clothes were outside in the backyard. Did the rain cause it to be wet? You can possibly say yes, there's a causal relationship. Right. But let's say that I came to Silicon Valley in 1999, I believe. And right away, there was a dot com boom. did the economy boom here because i came no so so that is an example that sequence is not causation simply because something happened after something else y happened after x doesn't mean that x caused y are we together it is when you think about it in the abstract it makes a lot of sense but when you look at real life all around and every day you see claims otherwise people always take credit for something right something good that happens afterwards after they did something right so for example when trump came to power there was a whole thing that economy was booming so did he cause that if you listen to one political party of course he caused that if you listen to the other political party in the United States, it had nothing to do with him. It was the previous presidents and their policies that caused it. Likewise, when you look at the pandemic, for a while it seemed in the US that, Biden? Or was it because of things set in motion by the previous president? Or was it because of neither of these two? Because of independent scientific facts, right? The pharmaceutical industry doing its job and whatnot, right? So it is hard to tell these things. And different people have different interpretations. But there is one thing that is definitely true that just because why follows x doesn't necessarily mean that x caused y. Are we together. This is something. It's a common fallacy putting it in the abstract it looks very obvious, putting into real life every day you see claims to it right so be careful of that so now with that out of the way we will talk now we'll focus on causation in data science here's a basic fact it is very hard to create a causal relationship between x and y you need other facts you can make a plausible argument that there may be a relationship and x may be causing y but it is not always so so without yet explaining mathematically what correlation is because colloquially we all seem to understand correlation i'll tell you a story which is, I understand, it happens to be true from what I understand. At tobacco, this, the south, in US tobacco, tobacco was big, the tobacco industry. We spoke of the big tobacco. Then gradually the scientists began to do experiments and they came to, they seemed to see a relationship between smoking and lung cancer, right? So they began to cause alarms and say, people should stop smoking and the big, bad tobacco industry. So tobacco industry retaliated, they had their own setup. So they hired their own special scientists in their own special lab who could see no relationship whatsoever between smoking and cancer right and they published their own papers that happens even today by the way with plastics with this with that one of the easiest ways create your own scientists in your own lab and make sure that they produce the results that you want them to produce right and completely confuse the issue. So now you have it from being something leading to a fact, it becomes in-course contrabotion. It just muddy the water quite a bit. So anyway, that was one technique. But after a little while, the scientific data began to mount. More and more, there was clear relationship. People began to say that there was a high preponderance of lung cancer amongst the smokers versus non smokers. So this thing went to court many, many times. So at one point, it had, from what I understand, it had gone to court. And it looked that data was winning scientific data was winning and big tobacco was in trouble so they thought it's time to bring in the big guns so they got one of the great fathers of statistics to fly in from england so sir fisher and you will encounter his work here in this course truly a remarkable statistician, but perhaps not so remarkably ethical. He was flown in to be in the court. And so in the court, he was asked, do you see the correlation between tobacco, smoking, and cancer? And so, so far, they were all saying, no, we don't see it, we don't see it. He straight away said, of course I see it. So there was ton silence and people said, finally, the big tobacco industry has accepted that smoking causes cancer. Right? And he said, no, I never said that. All I say is there's a correlation between smoking and cancer, lung cancer. So people showed this graph of data that say, look at this, there's a correlation. So he said that, yeah, it is correlation, but why do you assume that smoking causes cancer and it's not the other way around, cancer causes smoking? And he successfully argued that, see, the reason people smoke is because these people had latent cancer in them and they were suffering they didn't know that that they had cancer they were suffering and to alleviate their suffering they started smoking right and so tobacco industry was doing such a big social service giving these guys cigarettes. And that is the error just because X and Y look related on a graph in data. You cannot tell which way the causation goes. You need signs, you need other things to tell. So never try to infer causation. You can make a plausible argument that maybe there is a reason, but you have to go down to the biochemistry of what smoking actually does to human tissue and derive your proof evidence from that, not just from correlations. That is something to remember. There are many people in statistics who tend to argue that you can find causation in data. I tend to be very skeptical. Right? But there are bodies of people who believe there are right ways of finding causality in data. I don't believe that. So we will throw causality out of the window. Here is a homework thing. out of the window. Here is a homework thing. In the world, there are hilarious correlations. So I'll give you, to drive home this point, I will tell you a very obnoxious correlation, which hopefully you'll never forget. You know that India became free in 1947, right? It's a third world country. It was under British dominion. Likewise China, which was under British dominion, 1945 is when they became free. Now here is a fact. The population, you know, India has a population problem supposedly. The population growth curve of India is very closely correlated to the population growth curve of donkeys in China. Well, that's an uncomfortable thing. Do you think one could cause the other? It is pretty silly to think either ways that either the Indian couples or the donkey couples are taking hints from each other. So it can't be, it's silly. So correlations can be. So whenever you see correlation and you ask what is this relationship to causation? It could be the following. First of all, there may be a relationship. When it rains, you can see that clothes tend to get wet, you find more wet clothes, especially left in the backyard. In that correlation, there is causation. So there are correlation situations that imply where you know there's causation, it doesn't imply causation, but there is causation, you can work out the physics of it. There are situations like this, where there is a cause but it's not obvious it is silly to find obvious causes the reason would somebody like to hazard a guess not my past students would anybody like to hazard a guess why this could be true why such a weird fact exists So, see what happened is that I gave you the hint in the beginning, both of these are British colonies. Both of them when they became free, they were a nation of beggars starving beggars, you know it, it is a fact. The country had been devastated by imperial the colonial experience people were hungry and starving the governments of both these countries despite their other problems one thing they did remarkably well in the last 60 years or so 70 years they have done an extraordinary job or so, 70 years, they have done an extraordinary job of managing to pull people out of utter starvation and poverty. They're doing an incredible job in both places. People are rising out of poverty. They have food. They're not starving. Fewer and fewer people are starving. They are resources. When they are resources, it's a biological fact. There's some, I think it's called Malthus law or something. When there are resources, populations increase. So remember, when there is food, not only humans but donkeys get food too. Horses get food and cows get food and everybody gets food. So what happens when there is abundant food? Generally, population tends to grow when there's enough resources. It's a biological fact. And so population of India, and I could have contrived this example by taking anything. I could have said, I don't know, India's population versus the mice population in yet another colony or something like that. Generally, when resources are there, population increases. So there's an underlying cause. There's history behind this fact. Then there are third kind of correlations, which are purely fictitious. They're just adventitious correlation. You take any stock market, right? And any stock, right? And then you look at some facts. Let's say how many dolphins jumped into the sea, how many penguins jumped into the sea from the Antarctic on that given date, from an island on that given day. If you keep account of the number of penguins that jumped into the sea, I bet you there must be some stock which seems to be correlated to it. There's so many companies traded, isn't it? Now, you can't say therefore that the right way to boost the revenues of this company is to somehow incentivize those penguins to jump more into the sea. Do you see that? It looks completely absurd. And so the third form, the third thing that happens is correlations are purely adventitious. They have purely accidental and accidental correlations abound all around us, right? And people have created websites with hilarious correlations abound all around us. And people have created websites with hilarious correlations that they have found in completely different sets of data. All of it driving home the point, never, never confuse correlation with causation. So one of the homeworks that I invite you to take, and obviously post it in the discussion group when you get the access to discussion group, post some hilarious correlations. Tell a story, tell something that you observed. Or tell about a fact that illustrates that sequence does not imply causation. Just because Y followed X, it doesn't mean that X caused Y, right? So things like that. So discuss it so that it sinks into your mind forever. But now we'll talk about correlation and covariance together. So I will show you three kinds of data. Now we are changing the topic to, we will understand a word covariance and covariance and correlation are the last theoretical topics for today correlation covariance co is together Co is together as in couple, right? Co, co-occurrence, two things occurred at the same time. Variance means varying. So let us say that you are looking at some data like this. Let me give this data A. This is A. Let me give you another data, which is like this. And X and Y are assumed. So remember, this is x this is y x y right like this and another data which is oh maybe i'll do zoom out a little bit oh why am i not able to? Okay. All right, let me go here. Let me make another form of data here, which is like this. Suppose you have data points. And X and Y you can in your mind make it whatever you want for the time being. Would you agree that these three plots, B and C, A, B, C, they show different relationships between the data, between A, B and C, between X and, I apologize, between x and y they show a different relationship would you agree with this isn't it intuitively your eyes can see it what would you say between x and y do you see much of a relationship would you say there's a very strong relationship between x and y knowing x can you predict y no so let's take a particular value of x let's say this value of x right you see that the y is spread all over this from here to here, isn't it? Y value could be anything. You see that? There is not much of a relationship between x and y. The same thing I take here. And what do you notice? The y value is much more localized, isn't it? You can pretty much tell what Y is. So what is the difference between Y value being spread out and Y value being localized? I'll give you a real example from my experience. One day I was running a Spark job, a big data job. We had to predict the value of something after a lot of computation the job finished and the results were it said that the value is it is 100 confident or 99.7 percent confident that the value is between minus infinity to plus infinity right do you think that statement actually conveys any information? There's any entropy in there? No. Useless, right? It has no information content. But let us say that the same job had finished and said with 90% confidence, I can say that the value is between 10.5 and 11. What about that? Is it conveying more information? Yes. So from that perspective, if this is the value of X and the value of Y is within a more narrow band, are you conveying more information? Isn't it? You predict the middle of these values as your prediction, you're doing pretty good there, right? There is a relationship. Now, likewise here in C, is there a relationship between X and Y? Once again, given X, the same value of X, Y is localized to this little region, right? Those values of observations are seen. So value is somewhere in there. You can just take the average and predict that. But now let us say, let us observe this. When X increases in A, does Y necessarily increase? No, there is no overt relationship. In in B when X increases, Y increases. So they co-vary one increases the other increases. And does the word you say that they have covariance. You can measure how much they increase together. Are we together? If X is increased, how much, how closely related is the increase of Y, the varying of Y? What about C? When X increases, does Y correspondingly vary? It doesn't increase, but it decreases. To coin a term, it contravaries, right? Varies in the opposite direction. But it does vary. So now comes a question. You say that B and C show covariance. If you just use the word one varying cause the other to vary, either increase or decrease, B and C show covariance, covariance, A doesn't. Now let's try to create a mathematical framework to discuss this. So do you notice that B, suppose the value is, the value here is both in the positive quadrant, right? X is positive. Y is greater than zero. Is this true? y is greater than zero. Is this true? Right? x less than zero means y is also less than zero. Would you agree that this is true? It is true. So what can you do? It gives you a hint. And what about this? If x is greater than zero, y is less than zero. If x is less than zero, y is greater than zero y is less than zero if x is less than zero y is greater than zero so let's try to come up with a mathematical measure that do and by the way i have deliberately taken data so let me just say data centered at origin just just for simplicity. The center, the average of X and average of Y is zero. Center is zero. So now one easy way is you could do this. I could multiply. I would say that here X times Y is always positive. Isn't it? times y now there are some points there are some points in the off quadrants also where x times y is negative but far fewer than points where x times y is positive would you agree this is and here x times y tends to be less than zero for negative covariance right and therefore now but here also there are some points there so if you want to just see is the data varying positively one with the other or varying inversely one increases the other decreases one easy thing you could do is for all the data points compute x1 y1 plus x2 y2 what will happen is for positive covariance most of these values will be positive a few will be negative because they fall in the other quadrant right these values negative quadrant but generally the sum total that's a total will be will tend to be positive will tend to be greater than zero for Paul in the case of b isn't it total will be greater than zero and just because they're n data points it's a good idea to take the average of these points so this is the definition of covariance covariance for centered data is essentially the average of the products x i y i if the data is centered are we together so in this case uh as if shall we use n minus 1 again or? No, no, no, no. So because look at the intuition, all we are trying to get a sense is overall are you positive or negative? So you don't do that, right? So forget about the n minus thing for all other things except variance, right? So here we don't worry about that. So let it be there, there right let's keep it simple for the timing so this is your total now comes an interesting question do you notice that what about covariance here covariance x y will be approximately what will it be if i add up the product of all of these excellent data points What will it be if I add up the product of all of these X and data points? Zero. Close to zero. Here, covariance will be greater than zero. Here, it will be less than zero. And here, the covariance of the product of X and Y will tend to be less than zero. to be less than zero right so we seem to have come upon a nice way to quantify how data varies how x and vary together co-vary isn't it so this is the base this is actually correct it has one assumption data is centered are we together in this formula data is centered so now it's a minor correction and this is the way guys when people create concepts in mathematics and you see this formula, data is centered. So now it's a minor correction. And this is the way guys, when people create concepts in mathematics and you see this formula in your books, many of you may have encountered covariance. Maybe some of you looked at the formula and said, gosh, where did this come from? Like did somebody just pull it, some mathematician pull it out of his hat, but see, this is the way you can reason through any concept or unpack it and understand it so now what i'll do is what if data is not centered as if you mean uh centered as in centered at origin okay is not centered content is not centered at origin which is most of the time the way data is right so for example if you take children's height and weight it won't be centered around the origin isn't it because height and weight are both positive attributes so let us say that the data is just for the sake of one argument, it is like this. And I'll take a simplified picture of height and weight. Let's say children x height y is weight. Now how do I quantify that? It's not really good to do x times y what what you need to do is you say well that won't do let me go and put the coordinate system here i need to put the coordinate system in the dead center in the center of gravity of the data right in other words if i sit in the middle of the data, then this picture becomes the picture above. Isn't it, guys? B, this picture looks exactly like B. Let me. No, sorry. Well, I made it too small. Yes. Do you notice that? Let me call this picture D. You see that D is equivalent to B with yellow coordinates. Isn't it? So what you did is you moved to the origin, you moved your reference frame, your coordinates, to the center of gravity of the data. Isn't it? What is the center of gravity? X mu, mu x, mu y is the coordinates of this origin with respect to the other coordinate system, the original coordinate system. You just go to the average of x and the average of y coordinates, right? Once you just translate to that, d begins to look like b, and so the same argument occurs. Now, d, in the general case, obviously, you won't get zero-centered data. You will get data-centered with... So in other words, generally, in the arbitrary case, mu x, mu y are not zero. They may be something else. Height and weight, for example, are not equal to zero. So therefore, what do you do? You center it. How do I center it? You go to a new coordinate system, x prime, which is equal to x minus mu x. Isn't it? The mu of x. x minus mu x isn't it the mu of x and y is equal to y minus mu of the average of y do you see that if you go from the original coordinate system if i go to the new coordinate system i just need to just translate a little bit move a little bit isn't it so that's what it is move a little bit, isn't it? So that's what it is. Now, covariance is actually, once you have gone to the center, the same rules apply. Covariance of x, y is actually covariance of x prime, y prime, you know, center. That is the way you define covariance. And therefore, it is the real definition of covariance is this. It is the sum of all covariance is this it is the sum of of all i what is it x prime y prime right xi prime y i prime in the in the center of center of gravity reference frame you do that which amounts to what it amounts to in reality is this therefore covariance of x and y is equal to the average of sum over x i minus mu because that's what x prime is and what is y prime y i minus mu right you see that this is the this is covari. It's nothing but the product of those two variables multiplied, but in the right reference system. And that is the definition of covariance. And that's where this, so this is the real formula for covariance. Are we together? If these products are broadly positive, you'll get a positive number, positive correlation, otherwise negative correlation. But let me ask this question now, we have five minutes left, I'll ask this question. Covariance. If there is covariance, you know that there's some relationship between X and Y. Positive or negative. What happens if covariance is zero does it necessarily mean that there is no relationship between x and y think about it for a bit if there is no the covariance is zero data could be well that's not a function cross wouldn't do but you're close but Reason a little bit further. See, covariance is different here is different here it's here is negative right and in the yellow regions it is positive right in the red regions it is practically zero right so locally the covariance changes but if if you take the entire dataset, you will find the covariance of XY is close to zero. Isn't it? Think about it. The negative relationship and the positive relationship pretty much balance themselves. So from that, there is a takeaway lesson. An absence of covariance, and correlation is pretty much the same thing standardized when we talk about it absence of covariance or correlation does not mean an absence of relationship are we together all it means it means is an absence of linear relationship are we together an absence of covariance is an absence of linear relationship. Are we together? An absence of covariance is an absence of linear relationship. So covariance is this concept, very simple. Out of this is derived another concept which is that of correlation. And we'll come now to the final concept, correlation. It is used, even kindergarteners know what correlation is in a colloquial sense, but let's be a little bit more precise. See what happens is, if you weigh an elephant or many elephants, you will get values. And let's say that you're measuring the weight in grams, you will get huge numbers, thousands, right? Or millions of grams each of the elephants will weigh millions of grams is that correct but if you take their temperature what will happen or let's say that you measure the forget let's say you measure their height forget them let's say you measure their height in meters what kind of numbers will you get three three meter two meter right height of elephants so those would be small numbers but their weight in grams because the unit is not appropriate for weighing elephants, you're getting huge numbers. So when you plot out, if you say, is there a relationship between the height of an elephant and the weight of an elephant, and you know there is, because when elephants grow, they become heavier too. So the relationship would look something like this. Because the height is, the weight is high, most of your data would be, and to the eye, it would be hard to tell that there's any relationship at all. Why did we, you know that actually with height, height does increase height, weight. As the weight increases, the height increases, weight increases and vice versa for growing elephants. But from the picture, you can't tell. It's not so obvious, isn't it? You wish that the picture was more like a more balanced picture, more like a picture like a more balanced picture more like a picture like these isn't it so how do you make data which are in different units sort of when you bring them together to surface the real relationship what you do is you standardize the data one way to standardize the data is you say that hey you know what the first of all I need to Center the data one way to standardize the data is you say that hey you know what the first of all i need to center the data centering the data is easy if you take any data point xi minus mu what have you done you have centered the data right you have moved your coordinate system here now are we together but the spread the in grams the weight of the elephant seems to spread very far. So in other words, your data is very ellipsoidal. In one direction it spreads a lot, in another direction it barely spreads. Isn't it? So you need to standardize or normalize the data. So what you can do is you can divide it by its standard deviation. So when the standard deviation in the data is small, right, you divide by a small number, the result is bigger than dividing a quantity by a big number, isn't it? So for any data point, when you do this, given a data set, every data point, when you standardize it, you get, traditionally, you represent the standard data as Z, ZI for any data. So ZI is taking the data point, subtracting the mean, and dividing by standard deviation. So what it will do is this ellipsoid will begin to look more round or whatever it is, right? The real relationships will start showing through. This data will look more like this. Height and... So you're at the center. Well, okay, height is not... So what will happen is in standardized value, you'll actually get negative heights and negative weights because of standardized value. So height X. Oh, no, this was weight. Y is height. So when the data is looking like this, sorry, when the data is looking, write it in bigger form. What what it does is no matter the units in which your data is written, it now becomes standardized, right? Like this. And when the data becomes standardized like this, even visually it's better to see and see the relationship. So covariance with standardized data is called correlation. Pearson correlation is the standardization is covariance of standardized data. In other words, correlation, which is usually represented by a little r or by rho, I'll use rho, rho is equal to covariance of zx and zy, you see. And so if you were to write it out as formula, it would be 1 over n, sum of i, n sum of i, x i minus mu over sigma y i minus mu over sigma. It's just sigma x, sigma y. That is it. That is the formula for co-relation. I hope I've taken you through a journey in which it all looks very obvious you know i'm developing these formulas from first principles is it making sense guys okay so i am out of time i'll take questions now any questions anyone Any questions anyone? Is there a methodology to get how much to shift x or y to get to a standard level? So is it based on data or is there a procedure to do that? No, no, this is it. Just go to the center of gravity. It means x minus the mean of x, y minus the mean of y that's it it will put you in the dead center of the data okay got it yeah all right guys so uh any other questions if not we'll take a lunch break it's 1 18 i'd like to take a new one oh right right i should also say a mu of x and mu of y oh right right I should also say a mu of x and mu of y good of course mu of x and mu of y good any other questions guys so today guys we end today we didn't get to the machine learning part per se I would like to take just a little half an hour more after lunch to at least effectively, to put it metaphorically, touch the shores of machine learning, right, so that you get the flavor of the whole journey. Let's do that and then we'll do labs in the afternoon and we have a lot of labs to do right so let's take a break it is 1 18 should we meet at 2 45 it will give you 80 minutes for lunch 240 or 245 right 245 okay yeah all right guys so let's meet yes you're welcome 245 pacific time let us meet in in the meanwhile bon appetit go ahead sachin so i wanted to share something with, since you mentioned lung cancer, right? Yes. I wanted to tell, one was when we were doing the lung cancer and the fever thing, we kind of went through the medical literature that where they looked at. So basically showing that if you're a smoker, you have cancer, and the other way around also non-smokers also get cancer so there was like the communities with people even now that's number one the second was what had happened was they very initially when the cigarettes came out they saw that the fetuses were smaller and cigarettes were encouraged to ladies saying that you will have an easier pregnancy though they knew that it was because of smoking and when they started noticing the problems of when you do that you're pulled out this is like first seven or eight years after cigarettes were kind of so they went heavy after ladies wow so then they realized what the problem is the third one is related to hydrocarbons so basically when you are close to two wheelers and things like that the partial combustion that happens that is known to be 50 or 100 times more lethal than actually smoking cigarettes wow if you're near like in front of a gas station don't make sure that your kids and all are not near the exhaust to say bye to you because that's like you're basically wow yeah it is interesting I should remember that or where are mass near gas stations I should remember that. Oh, where are Mars near gas stations? All right, guys. So I'll see you guys in an hour and 20 minutes. We are going to start now. I was going to get into the practical part. It's 2.45 Pacific time. get into the practical part it's 2 45 pacific time i'd like to do at least some setup of the environment so that from next time we can do the labs now the setup that we will need for this particular workshop are in three different directions we will set up for data science in actual real world in production. We need three, it's sort of a three main pillars. We need to have the data science tools in place. We need to have data engineering tools in place. Like basically, how do you manipulate data? Today we deal with data. Quite often the data that we deal with is at scale, large-scale data. So along this workshop, you will find that some of the emphasis is on dealing with data at scale. And the third is cloud. Today, predominantly, work is done of production applications are deployed in the cloud so to the extent that cloud is the new dominant platform to deploy applications and products we will emphasize the cloud quite a bit the particular cloud that we will select is the google cloud platform gcp so whenever i say gcp i mean the google cloud platform we will take baby steps first in setting up our environment locally and then we'll play with the cloud and gradually we'll learn to do some of the data engineering stuff like for example deal with containers, Kubernetes, how to take it to production and so forth and we'll also do some of the labs in Spark when the data sizes are huge we'll retrieve data using Spark data using Spark and we'll manipulate data. The Spark gives you MapReduce functionality and we'll learn to retrieve data from a variety of big data sources. Particularly, we'll learn to get data from a BigQuery and possibly Bigtable also. So it's part of the journey. In the beginning, we take baby steps. We will take data from CSV files, then gradually we'll start bringing in other data sources. We'll bring in data from databases, SQL, and then we'll make progress from there. So that will be our journey. And likewise in the cloud, in the beginning, we will start by using something called the Colab. Colab is a simple sort of a data science notebook we'll use. And on the desktop, we will use Jupyter and gradually we'll learn containers and there also we will use Jupyter notebooks. Finally, when you do the projects, we will sort of modularize the code into good libraries and then we will use PyCharm as a tool. Now, that is a whole lot of buzzwords that we are going to be involved with. So let me start simply today. Let me first stand. All right. So the very first thing that I would invite you to do is visit our website. Let me bring this thing here. And share a different screen. But if I share the screen, it is not going to be reflected. Oh, no, I made a mistake, actually. I'm getting used to the screen sharing here. Give me a moment. Cancel. We'll let the same screen be shared. So let's visit a site called, let me see. called let me see oh by the way this is while our corporate website is supportvectors.com the student portal is studentsupportvectors.io you should soon receive information and when you go there I'll just give you a brief overview of how our tool website looks it uses a learning platform called moodle when you log into this this is for example for another course you notice that there's a whole wealth of resources training material and books and quizzes and whatnot so there is a course portal that you'll get a link to as soon as you get a link to the course portal please immediately start using it there'll be a discussion group and all of that I'm running a little bit late with those but I will catch up and take care website that all of you invite all of you to do anaconda.org anaconda.org see if you can visit this website when you do visit this website you will see a download button and I'll explain what it is. But first start the download on your machine of this Anaconda. Now when you deal with Python, Python is a snake so it is quite common for people to name their libraries around all sorts of snakes so please go there and download the library in my case let's see where it got downloaded Y is individual edition. Okay, here. You'll get this button for your own platform, whether it's Windows, Linux, or Mac. You will just click on the download. It will lead to the download. Now, I'm not going to, after that, click on it. But in your case, the executable that gets downloaded, please do run the executable. And while the installation is in process, let me explain what you should see. By the way, once the installation is complete, then run the application of Anaconda application. And when you do that, you should see something that looks like this this is the anaconda it's called the anaconda navigator are we together so this is a goal from a lab perspective at this particular moment we are going to install anaconda we are going to run it we are going to reach this now what is anaconda but this is a software that let me this is a sort of a well packaged distribution of all things data science with python that you will need, in fact using R also, you will find that it installs the Python libraries, the Python language, the R language, which is also used for data science quite often. And then something which we will use quite a bit, it is called the Python notebook. Now, there are many things that you will see here gradually by the time this workshop finishes you'll become quite familiar with many of these right but for now we will just focus on these jupiter notebook will be our primary focus today The Jupyter Notebook will be our primary focus today. So I would like to invite you all to install it. Let's take about 10 minutes to install this and then we are going to have some fun with it. I'll just wait for you all to install this on your machine. I'll ask if I already have python running on my uh laptop so if i install with anaconda is it going to override or is it going to have like a virtual kind of environment for our words how does it work so you can install anaconda and the python in a separate location if you have a previous version of python which version of python do you have i have three python three or three then you might as well install i mean the only time people worry about is if they have two but since you have three just install anaconda and you will end up with a fresh version of python along the way okay there's a lot of other things that need to install as well jupyter and everything if you don't have. That's right. So you need to install everything. Anaconda will install everything. Okay, sure. In fact, since it's going to be a long session, I think I'll... We're just sitting here. Let's start now. Okay. If I'm assuming that Jupiter, like your Anaconda is installed, please launch something called Jupiter notebook. When you install, when you launch a Jupiter notebook, well, obviously in my case, I have a lot of things already present. So not a great way to start. So I will, anyway, I will walk through. Well, what you do is, once you have launched your Jupyter notebook, this sort of situation, you should see something that is more or less empty in your case. Let me know if you see something like that. Or you could just go into a directory of your choice. For example, I can go into documents. And I can create a new folder right and in the new folder you can start putting uh let me let me just drag it here so as if you have to go to the directory of choice or i or we can create a directory at the root itself yeah create wherever you like wherever you have access to. What I would suggest is, find a place that you like. I'm sorry, that you like. This machine I haven't used in a while so that we familiarize myself with it. Folders. So let me go into documents. I'm here in documents. Let us say that I create a new folder. Right. I created a new folder. Where is the new folder gone? Yeah, untitled folder. You can go here. And it's a little bit clumsy, but you can rename the folder here. And it's a little bit clumsy, but you can rename the folder here. Rename it to something that you like. Let me just call this Lab 1. Lab 1. Then you created a new folder wherever you liked in your file system. Go into the folder. Once you are in the folder, the thing here is you click on this new you see where where this new is. It's a in an unusual location. Typically we are used to finding file new on the top left in Jupiter. It's actually in the top right near the top right. Are we all seeing it guys? Is there anybody who's who needs help with this so i'm completing my installation so i'll just type a little distract on there okay so i know i will help you and we have tears will help you i'll just continue here for a moment so you just say you're creating a python 3 notebook which means you'll create a note something called a notebook written in python 3 the first thing you do is you go to the top and you can just give it a name right you can say first steps let us say first steps right I tend to put like this, first steps. So you created a notebook called first steps. Now, what is a notebook? Notebooks need some explaining to do. So let me explain what a notebook is. See, we, some of you, quite a few, have written code in some language or the other. Typically when you write code to a person not familiar with the code or that language in which you have written it, it might look like gibberish, very hard to decipher, right? So very hard to decipher, right? So we in programmers typically tend to put comments and so forth into it. In data science what happens is the thinking is quite different. The thinking is that when you are analyzing data, you create a notebook the way you would do it in a science lab. So imagine that you're in a physics or a chemistry or a biology lab. What do you do? You have a lab notebook in which you write copious amounts of notes, and then you have a table of data, and then you have some graphs. Isn't it? So that is our thinking when it comes to this notebooks and data science. You have a lot of descriptive text. Then you have just a few lines of code. Then you have the results of the code. Right. Whatever the code does, the output of the code right there. And then you have more and more explanations, more comments. Right. So we are going to follow that thinking out here. So when you follow the thinking, the first thing you do is in this notebook, there are two modes. One is the cell can be either in text or it could be is the cell can be either in text or it could be, when you do a cell here, right? So a cell can be a markdown, something called a markdown, or it could be Python code, right? So we will come to markdown in a moment. At this moment, I seem to be unsure whether I should sit or stand, myself getting a sense of it. But let me first explain it here, come closer to this and explain. Do you see this dropdown here? Code, Markdown. We will limit ourselves to these two we'll either write code or we'll write markdown so now the question is what is markdown markdown is a way of writing formatted code you know with head headings looking bigger and some things being italic, some things being bold. And it is one of the simplest ways to write code which has formatting. It was designed to be a very easy learning curve formatting language, not like HTML, which is a complicated syntax or text, which has a complicated syntax takes a long learning curve, but quite simply it gives you just a few commands. So I'm going to start first with that because all good notebooks, they should have a heading. So we are going to add a heading here. Now the way it works is if you put one hash mark it is a sort of like a title font right so our first steps we say our first step and then we can put some descriptions. This will begin the workshop by taking the very first steps using a data science notebook. So you notice that I'm just writing plain English. This thing, when you hit enter or you run, this is one unit. Jupyter notebook is just made up of these little units. They are called cells. Are we together? These are called cells. So you just created a cell here and then there are very shortcuts but in the beginning keep it simple just go uh you run this cell etc etc and then you have insert you can insert a cell below right and here i'm going to do some very basic stuff today. So I'm going to say, let's say X is equal to 30, right? And Y is equal to 40, let's say, right? And then I can say Z, let's print out X plus Y. And then when I hit enter, when I run this cell, what do you see below that? It has done the computation. It is showing you the output of that code that you run. Now obviously this is not yet data science. This is very elementary arithmetic, but this I hope just gives you a sense of how this notebook works. So what is the use of all of that? See you could do. At this moment, I'm not importing the bringing in the rest of the Python here because I wanted to illustrate the basis. When you go to this notebook, you can go to file and you can do a print preview and what happens is that you will get a nice html page that you can actually publish as a blog a data blog and in fact in the community now when you go and search on the internet you will find lots and lots of jupiter notebook results are published as that you could also go and say download as as html etc but here's something that will that will really interest you you could do you you could do as pdf right and then what happens is well in that particular case i don't have something called a latex install on this machine because actually this is a new machine i have to do some setup but assuming that your setup is there it will produce a beautiful html version of your notebook right which is not exactly publication quality but close it's a good draft right now what do i need to make that happen you need to install a software which is called tex or latex latex latex is pronounced latex it is uh on your respective operating system there will be different instructions for doing that on windows people often install mic text or live text or one of these software do you see mic text if you have this software installed and then you can export data as or your notebook as pdfs but that will be a digression today i will just stick to the basics this is your jupiter notebook right now the tradition here is you should create and this is one thing the the engineering or the coding mentality is to only have code cells you know know, Python cells. The trouble with Python cells is that then there's no explanation of what this code is doing, right? Jupyter Notebook gives you the idea, just as all code in other languages, C, C++ must be commented. Here, you can do even better. You can have a pretty good explanation. You look at the output and you can interpret the output give an interpretation to what you found in the data right so real notebooks they look slightly differently so what i will do is today because it's just the basics i will stop here and i will repeat the same exercise in Google Cloud. Very basic thing. We'll do it in the cloud. To do it in the cloud, what do we do? We go to Google Cloud Console. So let's say Google Cloud Console. I keep forgetting the URL myself. And when you do that, go here to Google Cloud. Let me do that. Switch over to my support vector site. Now, the question is, do you have an account with Google Cloud? I would strongly encourage, use this time to create an account in Google Cloud, right? Let's take five minutes to make sure that all of us have accounts in Google Cloud platform. So go to the GCP platform, consult it, and then go register yourself or create an account. Most of you should have implicitly an account. If not, just go create it. Remember folks that if you get stuck, we have people here, some good data scientists who are helping with this workshop. You can reach out to them. We have Arini here, Kate, and we have kyle three people and there's dennis also i didn't see him here today but he's there so you can reach out to any one of them and they'll help you with the labs now today i'm going very slowly because it's all new to you. But going forward, we will pick up speed, we'll pick up quite a bit of speed. You will end up creating a lot of notebooks. In fact, after this Google thing, I will show you what a basic notebook looks like once you have done that. Because I have not explained the data science libraries like NumPy and SciPy and Scikit-learn, three libraries that we will use at this moment, I'm hesitant in writing any complicated code. But from the next time, we will, I will, obviously next lab will give to understanding these three libraries, Pandas, these four libraries, Pandas, NumPy, SciPy, and Scikit-learn. So we'll get more into proper data science from the next workshop. Somebody has a question so i'm not able to i got the google the cloud.google.com but in jupyter notebook the one that you did i'm not able to create the folder if i go and expand the drop down there is no option to create the folder even if i go to let's do that why don't you share your screen switch over if you don't mind but make sure you write anything private don't keep anything private on your screen and then share your screen with us and I'll guide you through so now what you do is you're already in a file, you have successfully managed to create a file. So I am here right and I want to create a folder. So how do I create a folder? New folders. Give me a second. Your thing is coming in. I need to move your screen around. On the right side I see a new button. Click on that. But right now I am the document right? So I want to create a new folder. Yes, so why don't you click on the new and do you see folder there okay okay okay now you you have a folder called untitled folder just click on that and click on rename rename just above it just above where your rename yes your mouse was there rename and rename it to whatever you want okay okay by the way these are very basic gradually you learn some powerful ways of quickly doing all of these things now go into lab one click into lab one and go in there now again go to new python 3. and there you are now what you do is where you see the word untitled you see the at the very top untitled. Yeah. Click on that and just put whatever name you want to give. Say four steps. Yeah, that's right. That is good. And so there you are. got it are we together so i will share my screen now and remember for all of these things we have like for example kyle was stepping in to help you people are eager to help you just take the help which screen am i sharing what are you guys seeing on my screen at this moment excellent so once you create that you'll have to create a little project for yourself you know at the top you will have your projects you can do a new project go create a project for yourself so i'll create a project let's say one more project i will call it first steps did i spell it right yes first steps i am creating this project and of course when you create or use gc in the beginning, you can use your credits. Actually, since I use GCP for production, a lot of massive production, I've forgotten some of the what are the rules they will apply, or how they will tell you that you're about to use up all your $300 credit. Hopefully it is automatic. Once you have the project, now you can go select the project, what you do is you can go to the project, we are here. Once you're in the project. Have you all reached this part guys? created a project called first steps in GCP. You create the project by quite literally you see this drop down here click on this and then you say at the top new project where my mouse is and once you do that just go into the project from the drop down, select that project. You can switch between the projects simply by selecting them. And typically you end up having many projects for different purposes. So just create a project like this. Are we here guys? Anybody for whom I should wait now? We are all here, right? So all right, so here comes the fun part now. We click on this, you see the hamburger at the top left? So you click on the hamburger and once you're in the hamburger, what you do is you can pin it. You could, yeah. So here is it. Once you pin it in this left-hand menu will be there this left side menu is very useful on the google cloud console you will see lots of things so cloud is a big world for you it's a really big word world and and what you do is we will go down to a section called can somebody guess what section will go down i'll let you just browse through this first do you find a section called artificial intelligence? And under that you'll find something called notebooks. So see where my through the menu where it is all pointing to so far so good guys you can click on this or alternatively there's this very powerful search bar at the top and so because we are talking about notebooks you can just click on notebook right and then Oh, this is enabling the notebook API actually. So sorry. While this is being enabled. So you will find that if you type the word Jupyter, there's a lot of documentation and tutorial visualizing like this is again big data stuff visualizing big query data in jupiter notebook data product jupiter notebook is very very instructive to read through some of this documentation so i will just happen to pick one of these for a moment so as you, where do you go to look at the notebooks? I am at the Artificial Intelligence tab, that menu. So which one you click? Just pick notebooks. Artificial, yeah. Just click notebook and then you'll go through some steps. Actually I'm thinking that for today maybe it's not necessary because I need to, I should probably create a video of all the steps because there are quite a few steps we are going through. So let us do something simpler. Once you are in this, you have a Google account. Let's do something simple and for this, I'll create a proper video so that it's easy. So, go to collab.research.google.com. And we will do something even simpler. Here, you're going to collab.collab.research.google.com. This This is actually free notebook service that GCP gives you, Google gives you, and you can do most in the early labs of this workshop in the CoLab. CoLab is actually very good and there is a free version and there's a paid version. If you give $10 a month to them them they'll give you a slightly better more powerful machines behind the behind your notebook right so let's let's go first with colapho today because today we are doing some very basic stuff okay are we all here are we all seeing this Are we all here? Are we all seeing this? Yes. So click on the new notebook. And when you get a new notebook, you again have a Jupyter notebook. So, for example, you can do a text section in which you can say, for example, now I have to turn sideways to see. And then you have a button next to it, which edit double click to enter or edit and then you can hit enter. It will go it will keep doing the normal things you have a run button next to it and so on and so forth. So you can again do the same thing, X is equal to, I'm sorry, I have to bend back that way to see what it is, but this is very basic, is equal to 40. And so X plus Y, you can do, and you get the result 17. now this was supposed to be a markdown cell it is control m so So this is called Collab. At this moment, I'll just leave it as that. This is Collab. Collab takes a little bit getting used to. It's a little different from Jupyter Notebook. It's based on Jupyter Notebook, but you'll have to get familiar with Collabtics a little bit getting used to it's a little different from Jupyter Notebook is based on Jupyter Notebook but you'll have to get familiar with Collab right and once you become familiar with Collab it becomes very convenient you don't have to use your laptop you don't have to use anything you can do all your exercises in the cloud right so today what I'll do is So today what I'll do is, I should have created, but I will create some videos that help you set up your Jupyter Notebook in the cloud, your Collab, do some basic exercises with Collab in the cloud. And today, just play around with this basic tool, see what you can do, write a little, write some text, write some this. I haven't introduced you to any of the data science libraries so far. The libraries that we'll talk about are NumPy for arrays, SciPy for some statistical functions, Pandas for dealing with data, Pandas is very, very important, and then we'll do scikit-learn for machine learning. We will do PySpark for Spark using Python. And we will also do some data retrieval using BigQuery and so on and so forth. So all of those things we'll start doing from next time. This time it's just very basic. What I want to do though now is just show you and it's here itself what a typical notebook sort of looks like when you create a notebook this is in collab when you create a notebook it again looks like this you notice that there's a lot of text and followed by a cell. Right? There is a cell. There's again a lot of... There's some code, there is some text, there's some code. You can do a plot. Let's just focus on this. Don't bother about the code. The code is a little hard to figure out. Let me increase the font size for a moment. So if you look at this, there is some code here which produces a plot and we can for the time being ignore the code because we'll learn about numpy next time it produces a plot and this in this section these three things you see the gist of a jupiter notebook you see text titled You see text titled, marked up text, formatted text. You see code with syntax highlighting, just as you would see in an editor. And you see the results of that code, the visualization that comes from the code, right? So this is your Jupyter Notebook. Now this contains one thing, introduction to Pandas. Do you notice that? One notebook is introduction to Pandas. If you want to prepare for next time, I would say consider this as your background reading so that when we meet next time in the lab part we you are somewhat familiar with what we are saying right so read it and i'll explain it in detail what these things are when we meet next time so i say a quick question so if i want to do that how do i upload my csv file so that upload my CSV file so that I can read. So it's a very good question. So the question is how do we upload? So one of the nice things is, do you see it here? There is, uh, you have your folders files. Do you notice an upload button and your Google drive button? Suppose you have your files in Google drive, you can go and directly get the your google drive you can run it and once you run it will ask you for your authorization code and so on and so forth go to this url you go to this url you will authenticate yourself, see me authenticating myself, I will say this, copy this code, then go back, enter this code here, it's a little tricky because of security, you have to go through this basic steps. And when you do that, what will happen is all your google data will become mounted in content slash content right sample data let's go up it says it is in slash content so we go to content here and do you notice that you have your drive right and so I won't go into it because it will start showing a lot. I'll just quickly do that. There's a lot of files that you see. So everything that you have in your Google Drive will start showing up here. So Sanjeev, you got it, right? So let me repeat it again. Yeah, this Google Drive. And so what happens is it mounts your google drive for you that is one way to do it the other way to do that is just use the upload button do you see this upload yes and then you can pick a file from your machine and upload it and then if you upload it then how do you get the location of the file it will say here yeah it will give you the location it will tell you it will show you the files uploaded okay okay i'll do that forward but you have to it's just becoming familiar with it and seeing and playing around with it a little bit the best thing to do is you know is the play around and as you get keep getting stuck reach out to us we'll keep helping you so at this stage you know what i would say is the most important thing is to become very familiar with collab very familiar with jupiter notebook on your machine right and just play around with it a little bit and see where it does and the the homework for this week is lab homework is just read this notebook this notebook is the pandas right introduction to pandas right this is you may not get all of it out of it but i'll give you a small introduction to what pandas are see we always think of data as a CSV, as a grid, as a spreadsheet. Isn't it? It has so many rows and columns. A SQL query returns you a rectangular data set. So many rows and so many columns. Isn't it? So that in the language of data science is called a data frame this language this data frame word is everywhere anytime you deal with data these days the right terminology is data frame for example spark when it talks about big data and gets it from the database it calls the retrieved data a data frame, right? Or data set or something like that. In the world of R and Python and Julia, in the world of data science, the data frame is the fundamental data structure. Every data, once it is represented as a data frame, you're ready to do data manipulation on that data. Are we together? Now, the data frame and its manipulation is done conveniently using a library called Pandas. It's a very powerful, very easy library, enormously useful. It is pretty much the first few lines of code you write most of the time you load data into a panda's data frame you do some cleanups some visualizations and so forth okay so this is your reading now in this reading what i would like to do is i would like to just give you an example of what a real code will look like in due course of time. Let me see. I have it. I'll just open one of my notebooks, Jupyter notebooks. Now, where did I put it? Okay, maybe it's in the other browser here. Okay. I'll show you, this will be one of your first labs next time. So this is a data set called Galton families. It's a famous data set. And one of the milestones in this data science journey at some point, of milestones in this data science journey at some point. It's a rite of passage. Everyone, when they learn data science, they deal with these datasets like IRIS dataset, the Galton dataset and so forth. These are great ways to learn about this field. It's a very small data. It comes, and we'll talk a lot about it the next time. So I won't tell you more about this data, but I'll just give you a flavor of how these data science libraries look, or notebooks look, sorry, generally. You will see a lot of imports. You're saying, it is a way of saying, I'm going to use this libraries, right? These are the Python libraries that I'm going to use. Now this is using, this line of code here is using the pandas do you notice that import pandas as pd so pd becomes it becomes a nickname a short form or a variable name for the pandas library you're saying from the pandas library use the function read csv the read csv function does exactly what you think it does. It reads the data because the data is tap separated rather than comma separated. It will do that. You can see the data what it looks like. It can it has columns that family their families so I'll tell you what it contains. It can take it goes and looks at families different families measures the father's height measures the mother's height and measures the height of each of the kids each of the children so each of these row is a child in a family and it says that in the first family how many children are there there is one male and three daughters, one boy and three girls, and these are their respective heights. Obviously, this family has four kids and the first one is a male. And that is why male is equal to one means it is a male. Female is equal to zero means it's not a female. And so obviously these are in this data set mutually exclusive. Only one of them is one right so this is the data you want to describe this data there's a head data.head shows you the first few rows of the data head means uh heads are gone quite often used in the unix world and so forth it shows you the headings the first few rows you could have done tail and all of that but okay head is there tail in funnels yeah it is right okay so there we go then describe remember we talked about descriptive statistics whenever you get data your first instinct is to get some descriptive statistics on it so this gives you the descriptive statistics it says their families how many families are there 898 families are there is to get some descriptive statistics on it. So this gives you the descriptive statistics. It says their families, how many families are there? 898 families are there. Like how many unique records are there? 197 unique family records. Then this doesn't make sense, father's height, right? And is this standard deviations it gives you in the heights i hope what are the units they seem to be inches i i think so yes then likewise mother's height and then the kids height the height and how many kids there are and so on and so forth so number of kids in a family it turns out that the maximum is 15 and the minimum is one right and the median number is six so obviously we are talking of a different age right in those times this used to be the fact you can look at some information about the data when you do that it is again giving you what is the data type it's saying is float numbers objects are strings like the gender is male or female isn't it so it's a it's a thing are there null values in the data we'll talk about all of this later the families unique families that you can Now, the first thing you would want to do is group the data by family, isn't it? Each family's data. So you can group it by family and then you can do a describe on it. So you will get more reasonable values well more of it it says plot out the height the values of the father and the mother and kids number of kids that are there and the height so you you realize that the the center The center of this seems to be around 70. The center for women is around 65 or 63 or 64, whatever it is. So right off the bat, you look at this data and you can conclude, if you didn't know, that men to be slightly taller, fathers tend to be slightly taller than the mothers. Isn't it? In general, in average. And then then kids well obviously the the fractional numbers don't make sense here this plot is not terribly useful for that reason because kids are in whole numbers but it seems that a lot of people are having about five kids now height Now height. This is the height of the kids. It is around this value, right? Average height, irrespective of gender. Then this is a bit of code. None of this code, you should worry about how it came about, but just look at it. You are just saying for men, plot the heights and for women plot the heights. These plots are called violin plots right it just looks pretty right what it shows you is how many is the frequency distribution of men and women with respect to their heights and you can see that men tend to be around 16 8 17 i suppose it's inches and women tend to be a little bit less than that, the spread. So right away, this gives you, this is data. So what we are doing is we are doing exploring the data. Are we together? Yeah, I just have a question. So each cell is individual and we cannot get information from one cell to the other. How does that work? No, no. So any variable that you have created in the cells above you are available to you afterwards. So it is just like code. Anything you have declared above is accessible to you, but anything below you is not yet accessible to you in that cell okay so we can import it in one cell and that is imported throughout everywhere yes exactly imported a pair plots give you a two-dimensional representation this is a little more complex but i'll explain this to you what it shows for example let's look at this uh this histogram it shows that with respect to height there are two different plots of of kids who are male and kids who are female and obviously the men seems to be slightly taller and a little bit more shifted towards higher values right now these are density plots i will gradually as we do we'll have a whole session on data visualizations and various kinds of plots so we'll cover all that but some of it i hope makes sense to you that for example when you look at the kids height one set one circle is for uh female and one circle is for male like boys and girls so it just what what these mean this full a curvy circle is these are control plots we'll come about it we'll talk about it in detail later so these are some visualizations I won't go into all of them these are the heat map all of them these are the heat map and correlation plot how correlated are the values for example how likely is a kid's height how much is it correlated with the father's height if it is a kid's height so let's say how much is it related to father well quite a bit here right so things like that you you get all sorts of information there uh without going and all of these things today is just a sample guys i don't want you to read the code too carefully because we are going to do it line by line oh yeah data.tail you can see the bottom of the data uh you can standardize the data remember i talked about taking the z value so this is what it is I talked about taking the Z value. So this is what it is. The rest of it goes into regression and so forth. Yeah, we won't go into it. So these are the basics of data, loading the data, getting some basic visualizations out of it. And this whole notebook will make a lot more sense to you. And you'll be doing a lot of these things when you have gone through the next session which the topic for next session is regression right so if you want to prepare for the next session the reading material for today's session the review of today is chapter one and chapter two right and just review those sections that i have taught there are things in those chapters that i have not taught at this moment you can ignore that but the ones that i have taught do that and if you're adventurous try to get an early start on chapter three right read a little bit on chapter three but chapter three is what we'll do the next time in chapter two and chapter three we'll do the next time go ahead is the statistical learning textbook remember that is a core textbook review from that textbook right now what i will do is we are getting to four o'clock I'll just take questions. And I'll let you go today. Today was just an introduction. And the lab was of course very lightweight just making you familiar. But it is essential that you set up your Jupyter notebooks, and you become familiar with Colab. And if you get stuck, please keep reaching out to us, we'll help you. Very, very important that next time you have this basics behind you, because next time the pace of the class will be much faster. Today was introductory, but we will move much faster and we'll go much deeper into things. Having said that, I'll summarize today. In the theory part, we did essentially descriptive statistics. We learned about all of these mean, median modes and so on and so forth, standard deviation, variance, skew, kurtosis. We learned about covariance, correlation. We learned about none of these having implying causation right and we learned the relationship between correlation and causation there may or may not be a relationship between them right so x may cause y x may have no relationship whatsoever to y even though there's a correlation. Or both X and Y may have an underlying cause, a hidden cause. Right. So this sort of thing. So we did all of that, that that is the sort of thing we covered. Then for the lab, we just did some very basic stuff, set up Jupyter and set up, just played with played with collab just did x plus y is equal to something right basic we learned that jupiter notebooks have text sections and they have code sections text and code that's all we need to remember and when you run the code it produces output it may produce a graph it may produce a table and so forth what are we going to do next time in theory we are going to do linear regression right which is chapter three of your textbook and chapter two of the textbook we'll do it properly at the same time in the practical part we are going to get really serious we're going to learn numpy and pandas deeply and a touch of scikit-learn right numpy gives you the data structures pandas puts you gives you the data frame numpy gives you the array pandas gives you the data frame and scikit-learn is the main machine learning library in python when you're not doing deep neural networks right when you're doing when you're dealing with tabular data structured data as we are dealing with it think of it as csv data then the main library that you use quite often is the scikit-learn are we together right so that's sort of the summary of all that we did today. I'll open it out to questions. I think all of you look tired, which is why I feel like ending a bit early today. So one question, one thing, everything that I taught is the basis of a quiz now i will be releasing this quiz sometime well of course we have to get the whole course portal uh to you and so forth but then in that you will start seeing every week a quiz take the quiz seriously this week we don't have a project but the homework assignments that you have is look for funny instances of correlation and causation, funny instances of just correlations. Do some reading and try to read the introductory notebook on pandas that is available in Google Cloud. Are we together? those are your steps right so wait for notifications you will receive email with a lot of information on the course portal and on the slack and so on and so forth so that is all for today folks just a couple of questions so um do we have the link to the slack Just a couple of questions. Do we have the link to the Slack? No, we have to send you all the information. So expect an email with all of these details. Nice. Soon. Sorry, please go ahead. No, go ahead. A quick question I have is when do you use Jupyter Notebooks on your laptop? When do you use Colupiter notebooks on your laptop when do you use collab on the cloud see the basic rule is if you have a powerful laptop and you have set everything up locally you can work locally if you don't have a powerful laptop or you're using your work laptop you're better off being in the cloud okay got it so uh is there a need for GPUs or CPUs are good enough for all the things? For this workshop, in the course of this workshop, comprehensive data science, you don't need GPUs. OK. Right. CPUs are good enough and GPUs you'll need for the deep learning workshop. Different. Cool. Thank you. Thank you. workshop different workshops cool thank you thank you also sure any other questions guys hey i have a quick question so uh for the google cloud thing do we need to set it up or collab is good enough no i'll give you instructions for jupiter the google cloud setup google cloud we are going to do a lot actually i started out by doing jupiter notebooks there but uh not yet uh first become familiar with uh collab and then see collab will have some limitations the the instance that you get won't be that powerful then you give them ten dollars it becomes powerful for comprehensive data science science, this workshop, Collab is good enough. But nonetheless, I will make you familiar with other workshops, sorry, other tools in Google. So for example, you learn to have your Jupyter notebook in Google, you learn to do Docker containers, and you learn to do things like that. That will come much later in later labs right but at this moment collab is good enough and in fact you can do the entire workshop in collab okay cool thanks any other questions all right guys so if that is that let's call it a day. I will see you next week. Let's start a session on time.