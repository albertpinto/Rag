 For those of you who prefer watching it on TV or on the big screen. All right, folks, the last time when we talked, we learned about an important algorithm, which was k-means clustering. K-means clustering is a very powerful way to cluster data. We saw that taking just a few iterations you can quickly converge onto the clusters and you can sort of stop. The steps of K-means were just randomly pick points and then declare them to be centroids, then assign all the points to the nearest centroid, and then recompute the centroid as quite literally the center of the points, each of those. So I hope you remember what we talked about and how we did the k-means clustering. Very fascinating algorithm, Its main aspects are, it is perhaps the most popular clustering algorithm used and very well studied. Its behavior is very well known. Now, the problem that clustering, K-means clustering has is, many problems actually, everything comes with advantages and disadvantages. So the disadvantages are, it doesn't answer the question, how many clusters? So to answer that question, what you have to do is you have to try many different values of the cluster, and you have to look for the elbow in the screen plot. And whenever you see the bend, the point of diminishing returns, you say that is the optimal number of clusters. Right? And the way you do that is the plot is, the x-axis is, of course, k, the number of clusters. The y-axis is actually the within some square distance, you know, the pairwise intraracluster distances squared. That makes your WSS. And at the end of the day, you take that point where the WSS stops decreasing rapidly. And you know that that is a number of clusters. So to the fact that a k is a hyper parameter of the model means you have to tell k, you have to yourself say let's take this k and compute. It sort of poses a bit of a limitation. What if you said I don't know how many k's are there and you draw the screen plot. You're using up a lot of computations. People have asked often, are there other algorithms? So there is two more algorithms that we will talk about. One of them is hierarchical clustering. It is also called agglomerative clustering. And the other one is called density-based clustering. Both of these are very interesting ideas and today we are going to develop them. The hierarchical clustering is, I will go through it somewhat rapidly because it is used quite a bit especially in the medical sciences and so forth. It's one of the classic algorithms. I would like to spend more time with density-based clustering. So we'll allocate time accordingly here. So we're going to do agglomerative clustering. By the way, this is chapter 10 of your book. It is also called hierarchical clustering. And the second thing is density-based clustering that we are going to cover. And these are all completely different ways of looking at data and coming up with the clusters and that's the magical thing about machine learning, you find very powerful ideas coming from very different directions, a whole variety of ideas. Which all help in solving a problem. So what is hierarchical clustering? Suppose we let's go back to our data set. If you're looking at weight and size, you know that all the ducks will be somewhere here. All the cows will be somewhere here. The point is how do we discover it in this method? What you do is you begin by taking all of the points and you just just line them up you have all the points here then what you do is you find the points whose distance mutual distance is the least d i j is smallest d i j is smallest and here i'm using d i j is the distance between so first let me define it distance between point i and point j is by definition this notation d i j so you find the two points which are closest to each other maybe it is these two points who knows something like this you take these two points then you take points so you by you you keep all of these points in a row sort of sorted by distance or something then what you do is you find the next two closest points so these two are close you you make a relationship you link the two then there would be some what are the next the next closest points that you can find i'll put them next here right and then keep finding the next closest point and the next closest point so what you do is you keep pairing up the points which are close to each other as close as they can be and then what you do at each point when you every time you pair a point you replace a point like this with the center of these two points so center of these two points would be let's say uh somewhere here so suppose i take uh let's take let's exaggerate these this value right so suppose you have this point here. What would be the center? This would be the center, isn't it? Where the cross is. So what you do is from this list, you delete that two points and instead replace it with the center. You consider this to be the new point or this to be the new cluster, right? And so you keep on doing this and you keep on joining points and what you end up with sometimes there are three points that are all together or four points that are all equally distant from each other and then they will all start joining you know they will be uh joining and maybe there is one more point and they will all join and maybe there is one more point and they will all join. Maybe they will all join at some level. So bottom up. Yeah, that's right. You keep accumulating points and you basically, you get bigger and bigger clusters and you treat these. So the way you think about it is that, you can think of each point as a cluster or all the data is one cluster but they are all these in between stages of uh you know gathering points together nearby points together and marking them as clusters when you do that one nice thing that happens is you won't have the problem of k-means. You will very quickly be able to identify this sort of separations very, very easily. So you would, for example, if you were to cut it here, you cut this graph here, there are only two legs, one small leg, one big leg, or maybe I'll add more elements to this here just to just to show the point right so you you are able to find you want two clusters you can see two clusters you want three clusters four clusters you can see as many number of clusters as you wish and that means a lot to especially in certain fields because clustering as i said is also a matter of opinion like let's say that i will exaggerate this picture of the cow let's look at the cow cluster so the cow cluster internally may be like this So even though the cow cluster may be like this, you clearly see a pattern here. There is actually cluster within cluster, isn't it? Cluster within cluster, isn't it? And so you can go deeper and deeper and you can further argue, for example, that this is another cluster inside this cluster. And that kind of intuition is very well captured by a hierarchical clustering algorithm, right? So you can pick and choose like how big or small you want your cluster to be. And in biological systems, it means a lot actually to be able to do that because there are cells in the body and the cells are here. And quite often you can find little clusters of cells, but it depends upon what resolution you look at. If you look very fine grained, you'll want to consider some things as clusters. You step out, you want to consider something else as clusters, right? A bigger group of cells as clusters and tissues as clusters and so forth. So you do that and it's used in many, many fields, hierarchical clustering. The idea is quite simple, and it is very commonly used also. I think after k-means clustering, the most commonly used clustering algorithm is hierarchical clustering. There is no magic to it. It only leaves one point though unanswered. Suppose I have two clusters. Suppose I have two clusters. So suppose I have two clusters. These three points, or these points, are merged into one cluster. This point, I merged into another cluster. Now I'm trying to compute the distance between these two. So how do I compute distance? We know how to compute distance between points, right? This is the distance, dij is the distance between points. But how do you compute the distance between clusters? clusters and it turns out that you can have multiple answers so for example somebody may say it is the shortest distance between the two okay sorry i take this back it is the shortest distance well that doesn't look quite straight line, but imagine it's a straight line. You could say it is the shortest distance, shortest pairwise distance such that one point belongs to one cluster, the other point belongs to the other. Somebody else may come, no, no, no, I don't want to look at it like that. I want to look at it as the largest distance between two largest pairwise distance. And you can say, well, that is my measure of distance. A third person can come and say that no no it is actually the distance between the centroid centroids of the clusters right you could you could define it like that and you could have one more definition, you can say, hey, you know, let's do one thing. Let me use a different color for this blue. What you can do is more rigorous, you can say, find every the average of every pairwise distance, where pair being a point in a point belonging to cluster one and point J belonging to the second cluster, right? So you're finding the distance between these two and every pair. So what happens is roughly speaking what it means is suppose you have points like this then you're finding first of all this will have three lines this will have three lines uh this will have three lines and so you would take the one over every pairwise distance. So how many distance pairs are there? Three times three is nine, like one over three square times all of these distances, you know, all the nine distances. And you would say that the average of the pairwise distance that is the distance between cluster so are you seeing that guys when we when we think about distance between clusters we could be coming up with all sorts of definitions defining the distance between cluster right so that raises the question so which is the correct notion of distance which one should we use anybody has a comment and by the way you can think of more notions of distance there are many such anybody has an idea distance between the centroids distance between the centroids rich okay that is a valid answer but that actually in practice that even though that sounds the most intuitive it leads to a serious problem it's called the inversion problem i won't go into that but just take it as a fact that in practice while in theory it sounds the most intuitive in practice actually it doesn't work that well it sometimes works very well but occasionally anybody else any other opinion so we have the yellow the pink the green and the blue for my shortest distance sir the shortest distance the yellow one okay we have one vote for the yellow one vote for the green anybody wants to go for the pink and the blue blue blue so one is for the blue anybody for the pink? I think so, blue. Blue, we are gravitating to. So here's the answer. Blue is actually a pretty good answer and a common use value because it sort of ensures that the answers are not very unstable. The only problem that it has is because you take averages, averages have a problem and the problem with average is that outliers blow it up right what do i mean by that so suppose you work for a reliance company or some i don't know i can take any Indian company, not to single anyone out. And the CEO of Reliance makes a huge amount of money. So if you take the average salary that an employee makes in Reliance, or any of these large companies with heavily paid CEOs and owners, what will you find? What will the average be skewed by? It will be heavily skewed by the extremely high profits and salaries that the top tier, the executive staff and the owners get, isn't it? Are we together? Yes. So here in the US, for example, if I were to take a company, I remember at one time, which company was it? This company is no more, Siebel. The CEO of the company made a billion dollars. So when you average the executive salary and the CEO salary, you add it in the same pool as the rest of the employees then it suddenly turns out that we all seem to on average be making a lot of money but were we we were not we were making pretty mainstream salary with respect to the industry a little bit higher but not what the average would tell so in other words average is a problem it it suffers from outliers those people the ceos those executives they have paid huge just way out high amounts of salary right it is quite at least in the us is customary i don't know about india i'm just guessing um So in US, for example, it's customary that suppose you are a, let's say vice president, and then there is a director, and in between there are a couple of associate directors and associate VPs, I mean, so senior directors and associate VPs and all of that intermediate layer. But broadly speaking, if you look at a vice president and a director, it is easy to see that sometimes the total compensation of one is more than, it can be easily two to three times the other. Right. Then between the vice president, and I'm talking about not my company where I work but Silicon Valley average with the pattern that people have found Oracle and many other places that have worked and it's very common and between that level and the next level the difference is not just a factor of two three but typically it's a difference of at least 20 you know the next earns 20, what you would earn, what a VP would earn in 20 years. Then you go to the next level, and it turns out that the next level earns, the CEO will earn something like, he will earn another like 100 times what the C-level staff is making, right? So their salaries will be in hundreds of millions and so forth, it's very common. So when you start trying to take the average of that, you would realize that if you looked at the average and somebody published it, you would feel, well, I'm not making that kind of money, isn't it? So outliers skew the average, that's an example, right? So it's a statistical fact. I just created a very colorful story to illustrate a point in statistics, which is that average, the simple average average or it is called the arithmetic average average is susceptible uh is so except i'm writing i'm misspelling it susceptible, today my spelling is off, is prone to be adversely affected. This is a statement you should remember. It is broad statement beyond just clustering and so forth. Adversely affected by outliers. Right? That's a fact. So something worth remembering. So what do we do? Well, you can then say, I can come up with a different measure. I won't take the arithmetic mean, I will take the geometric mean. Right? Or you can say, I will take the harmonic mean. Now, these are things in India, at least we learn in high school. Matt, you guys remember arithmetic, geometric and harmonic means? So let me just take an example. Suppose you have points X and Y. This is what arithmetic mean. Would you agree? Yes. So we just literally call it the mean when people say mean or average. That's what they mean the geometric mean is does anybody remember what that would be it would be x plus x times y so x y x multiplied by y to the power half. In other words, it's the same thing as square root xy. The harmonic mean is very interesting. One over, let's say the harmonic mean is h. The statement is one over h is equal to 1 over X plus 1 over Y. So if you think about this, what does it come to? Therefore, H is equal to X times Y divided by X plus Y. Are we seeing it, guys? So this is the harmonic mean. So these things you have done. And now I can generalize it to many things. For example, if you generalize it, it will become x plus y plus whatever divided by number of points. And this will become the x, y, z, whatever divided by the number of points one over n right the nth root of that and so on and so forth you can do the same thing whenever here it generalizes very easily right so people people can do i don't know you can come up with all sorts of arguments and say i will use this or I'll use that and whatnot. So there's a name for that. And by the way, I haven't seen many people use geometric or harmonic means. They mostly stay with the arithmetic mean, but I'm just giving you for the sake of argument that you can come up with your own set of concepts in this space. So what we say is that each definition of distance is called a linkage function. and the statement is that these are all valid notions of distances so then how do we decide which one to pick could anybody tell make a guess how do we decide which linkage function to pick when we are doing agglomerative clustering or hierarchical clustering make a guess guys. Today we have a very small audience. Interesting. Okay, so would one of you like to make a guess? No, data is king. This is the one rule you have to always remember. Data decides. So what you have to do, and this is, so in other words, linkage is what is called a hyper parameter of the model. You have to try out different linkages and see which one works best answer is try out each linkage function and see which one works the best in other words leads to the best clustering the way that you are looking for it are we together and by the way this is a broad theme later on if at some point obviously if you continue with me there is a theorem actually it's called the no free lunch theorem. It's one of the most fundamental results in the theory of human knowledge. It basically says that no one approach will always do better than another approach for all datasets. So in other words, for some datasets, some approach is better. For other datasets, another approach is better. Which is why machine learning literature is rich with many many approaches some approaches are used more commonly some approaches are used less commonly so for example you can look up into scikit-learn the library and see what the hierarchical clustering algorithm what does it use by default? I'll leave that as an exercise, as a homework for you. Also see how you can change from one to the other. So that is that. But at the end of it, these are all different flavors of variations or dialects of the same algorithm, which is hierarchical clustering. Now, hierarchical clustering though is more expensive than the k-means clustering. The reason it is more expensive is because it requires a lot more computations, very, very computationally heavy. Well, in today's world, of course, we don't care for so much unless the data sizes are huge. And see, it's a balancing act. If your data size is a small hierarchical clustering, would still converge pretty fast. But if your data size is huge, the difference between k-means and hierarchical will be noticeable, right? So you'd have to be more careful of which one you use. So these two are the great classics in the field of clustering, these two algorithms. They are not, like in many ways, their single biggest benefit is they are simple to understand. They're simple to visualize. One great thing with this agglomerative clustering is you create this diagram. These diagrams are called dendograms. Dendrogram. Dendrogram. Let me write it in small letters also. Dendrogram. And dendrograms give you a lot of intuition of what is happening with data, right? So people love it. People print out these charts, these dendrorograms and they'll post it to the wall of their uh you know a meeting room or their research group and so on and so forth very intuitive this is this great strength right so that's nice about it now comes the limitations of these algorithms. Besides being computationally more expensive, it's still, it is not that expensive in modern hardware. If the data sizes are fairly reasonable, these are still considered relatively inexpensive algorithms. They work well. The limitation is that they tend to work well only for They work well. The limitation is that they tend to work well only for bulging clusters, clusters which have a bulge all around. The word that we use is convex clusters or globular clusters. If the cluster is not globular, then you have a bit of a problem. These methods, they don't work so well. And in the lab that we will do the next time, this will become very apparent. We will do a particular lab with this data set, SMILI data set, and you will see the strength and weakness of each of these clustering algorithms. Are we together guys? So we would move on then to a new class of algorithms which actually in practical sense is very very powerful though it tends to be a bit expensive. These are called density based clustering. And it would basically says is that if you think if you attach a physical intuition to data, let's say that you have data like this. And you have your cows and dogs. And you have your cow. So here's the thing. Now, what it says is that if you consider each point. as think of it as a grain of sand right as a grain of sand so then what will happen is somewhere near the center of the cluster what happens these regions are dense or inside of clusters are dense like this Right. But what about this? These areas, mostly empty. So not dense. You can say lightweight. Maybe there'll be one or two stray points that might be here. Some unusually big big duck some unusually small cow they may be there but these are the regions where you don't find many points you won't find many grains of sand are we together guys so now if you use this intuition what you're saying is to find clusters is to find areas where density is high right which are dense regions dense regions lead to this so with that thing in mind how do we find these dense regions how do we find uh the basic goal is how do we latch upon these guys right so there is a method to it and the method that they take is a little bit abstract so i'm going to explain it to you but if you don't fully get it that is all right but just remember the main intuition is simply you just find places which are heavy in the feature space and lots of sand a lot of sand has accumulated there and you can sort of weigh in some sense or get a sense of density of the region and from that you can discover which points where the clusters are does that intuition make sense guys yes yeah so it's a very simple intuition but the way you bring it down to mathematics, it gets a little bit technical, not very technical, but it starts out as some two abstract notions. It says that take a point, take a point I, right? Then around this point, I draw a circle. So let me try to draw a circle here. I am not terribly good with drawing, but let us make a circle of radius epsilon. write this funny e it is the greek letter for epsilon is the greek equivalent of e now when i have this the region inside is called the disk or the region inside the circle is called the interesting word is there epsilon neighborhood of point I right of the point I, right? Of the point I. So far so good guys? Right, let me use XI because you know, we have been using X for data for features. So XI, right? Of this point, so is this is this looking simple? The region that I have shaded is the neighborhood of this point. So next, what you do is you compute how many neighbors does this point have? So let me color the neighbors with something else that we color it red. So suppose it has neighbors like here. Yeah, yeah, yeah, yeah, yeah, yeah, yeah. So it says count the number of neighbors. Number of neighbors of xi right inside its epsilon neighborhood well you can count the number of neighbors let's say that they're about how many neighbors do i see here uh one two three four five six seven eight nine ten eleven twelve thirteen thirteen seven, eight, nine, 10, 11, 12, 13, 13. Here it is 13, right? You may then make a rule. So now comes the part you can, of defining a dense region. You can say if, and this point is the most important. If the number of points, people often write it in code as n, it is n, if n is greater than equal to some minimal value, some threshold, let me just call it n0, n0 being, let's say, 10 points. n zero being, let's say, 10 points. So in other words, if you have 10 neighbors in your close neighborhood, right? If for a point, psi, then, then what? n is equal to, Then we call it an interior point. So think about it if you were thinking of houses. If you step out of your house and don't have to go very far suppose this is your walking distance right of your morning walk and you encounter a lot of neighbor neighbors there a lot of houses there you know that you are in a populated area if on the other hand you go for a walk and you can't see any house except maybe one or two neighbors what would you conclude you're certainly not maybe one or two neighbors, what would you conclude? You're certainly not in a densely populated area, isn't it? In other words, you're not in a very clustered area. Does that intuition make sense, guys? Now, we have an interior point. And now we are almost there. The way the argument is built up is pretty elegant actually. It says that two points i and j, these two points, two points xi, xj, are, and they use this word directly reachable reachable if and only if by the way this word i f f the misspelled if if you want it stands for if and only if. It's a very common term used in mathematics. Means it is a true almost, it has to be both ways. If and only if J is in the epsilon neighborhood of Xi. Are we together? In other words, at Xi, you have to draw a circle and j has to be inside. Right? If j is outside, it is not directly reachable. So far, so good. So, we are almost almost there we are building up the construct so what have we built up so far the concept of epsilon neighborhood the concept of an interior point an interior point is a point that has sufficiently many neighbors within its neighborhood then we are creating another concept of directly reachable two points are directly reachable if one is within they both are within the epsilon neighborhood of each other right so because if our J is in the neighborhood of I then you can imagine that I is in the neighborhood of J right both ways the statement would be true so these are directly reachable. Then comes the next statement. Two points are reachable. The way they do it is sort of an interesting definition. Two points, xi and xk, xi, xk, are reachable let me just use the word reachable just reachable One choice is they are directly reachable or they are reachable through intermediate intermediaries, right? Some people in between. So what are intermediaries? So suppose I have a point I, and you have a point K. Now these two are, they're still reachable, so long as they are points like J, like, you know, U, V, W, X, right, P, Q. So what happens is that each of these distances, each of these distances, if it is less than epsilon, each of these distances, if it is less than epsilon, less than equal to epsilon, means what? That j is directly reachable from i, u is directly reachable from j, and this long chain continues. And using this change, you can, a chain of reachability, you can reach k. Right? Are we getting that, guys? So two points are reachable, either if they're directly reachable or they can be reached through intermediate hops. You can go from I to J, J to U, U to V, and so on and so forth, right? They become reachable. So now comes the, and we are done. It turns out that if you just use these ideas, which are just abstractions of what we mean by dense points with radical abstractions in simple ways, we're done. Now let's go back to our clustering algorithm. We get back to our cows and ducks. And this argument is by the way very elegant and you'll be surprised how well it works suppose you have cows so what you do is randomly pick a point point to see let's say that there are some points here also some point some point to check if it is an interior point it is an interior point so you randomly pick a point so what can happen you can either be unfortunate and you'll pick this or you'll be fortunate and pick something like this. Maybe you'll pick, let's say this point. Do you see this initial point first? Suppose you pick that first point. And now you say, own every point reachable from the first. So what will happen? From here, you can go here, you can go here, you can go here, you can go here, you can go here, here, right? Here, here, here, here. You can go here right here here here here you can go here here here you can go likewise here here here here and you can go here and you can go here what just happened guys you ended up reaching essentially every point in the cluster did we not in the cluster, did we not? If only we make reachable hops, we check which points are reachable from this point, you agree that we would end up owning the entire cluster. Guys, am I making sense? Is it straightforward to understand? So you say you own this point. So now you look at the remainder of the points and you play that game again. So if you're unlucky, as I said, you might end up with picking this point. This point has, does it have enough points in its epsilon neighborhood? It does not, right? So you mark it as an outlier, outlier, this is an outlier, this is an outlier, this is an outlier, this is an outlier. These points will all get marked as outliers. But then after a little while, you'll hop into, you'll end up picking, let's say this point, right? And so once again, you make reachability arguments, it will just fan out in some path like this, right? This, you can imagine this, this, this, this, this, this. It will end up owning this cluster, right? So this will become cluster, let's say, A, and this will become cluster. Sir, actually, I have a doubt, sir. Is this somewhere related to probability in that case, union and intersection? Not really. This is a very clear deterministic algorithm. You say, i am just making this much jump you have a notion of distance and you just go that much distance doesn't have to do with probability directly okay so now what has happened is the beauty of this and by the way this db scanScan has many density-based algorithms, has many implementations. What I just explained to you has a name, dbScan. And it's quite interesting actually that it showed up, if I remember right, this algorithm was written by two or three people in either 1999 or 2000, literally at the turn of the century. And the moment they wrote this paper with this very simple idea that we can discover clusters like this in the field of machine learning and AI and data mining and all that, they were immediately awarded the best paper award, the best paper of the year award worldwide. So that was quite in the professional field, that's quite a prestigious thing for a research paper to have such an impact. And so, and the idea is very simple, you know, you understood the idea idea which basically says that find a points which are near each other and all of them put together is a cluster right so this was db scan now db scan had a limitation which limitation is who decides you know remember that we have two hyper parameters, epsilon and number of points, right? The neighborhood region definition. Epsilon is the radius of the neighborhood region isn't it number of neighbors and not the minimum number of neighbors for a point to be considered an interior point to be considered an interior point point interior point and so if you think about it if you take your notion epsilon very small then every point in the system will look like an outlier, isn't it? Because you can't reach from any point to any other point. Isn't it? So suppose every, you said that your epsilon distance to a nanometers, and let's say that all of these things you look at, as you see on the writing board, they seem to be millimeters or centimeters apart. So you see that if two things are not less than a nanometer apart, they are not neighbors. So what will happen? All your points will get declared as outliers. So that is an extreme case but now when we bring it closer to reality what happens is that if you're not intelligent about choosing your epsilon, the clusters that you form are not good clusters. Right, you'll end up with lots of tiny clusters. So epsilon, and it needs to be wisely set. The other property is the number of what, how many neighbors do you need to have before you consider a point to be an interior point? That also is a matter of judgment. And as you change or play with that, then your cluster shape changes. If you say there must be at least 50 neighbors, then you may have difficulty finding clusters at all within a neighborhood. So things like that, you have to be intelligent about it. So what happens is that the limit, one limitation is that you have this hyper parameters. The other limitation that you have, which is harder actually to deal with is that this algorithm is computationally, does not scale, right, very expensive, large data. So the first limitation, the hyper parameter problem, they actually saw there was a variant of it, which most people don't seem to remember, but I'll just put it there. It was called optics. Don't worry about it, but they tried to remedy some of the defects of. can but not many people use optics what people do is they like dv scan very much and they are just intelligent from experience they learn to pick good epsilon and good number of neighbors i mean together and by doing that they sort of get away with it one advantage of using dbScan is it also can find outliers and finding outliers in data is a big deal, right? So for example, if you're having a credit card transactions and in some feature space, some points are standing out like outliers, what does it mean? You need to be suspicious of it. You need to go check, is it a fraud, credit card fraud and so forth. So that is the value of outlier detection. And dbscan's property is it not only clusters, it marks or discovers the outliers. So as such, it is very heavily used. Often it is my algorithm of first choice while working with certain data sets. It's a success. So there were many, you know, 20 years have elapsed and 20 years is an eternity in machine learning where research papers keep coming out practically every day. You get dozens of them coming out so now there has been a lot of progress and there is a new algorithm which is called den clue a den clue then clue and this is actually heavily used for example google uses it a lot internally Google uses it a lot internally in its search and many other things, web crawling and so forth. What this algorithm does is it takes a slightly different approach. It's more mathematical. It deals with something called the gradient. Gradient of the density. Gradient of density. And then it uses something called called and I'm just mentioning these words because you might encounter when you read about it in blogs etc it uses a hill climbing algorithm now what is a hill climbing algorithm and what is a gradient? I'll just give you the physical intuition. The math is straightforward if you know calculus, but I'll assume that at this moment you don't have an appetite for doing calculus. So let me go back to the like this, and data like this. So suppose you start out at a point like this, here, right? Any random point in the feature space, maybe here, even far out, let's say you're here. You pick a random point in the feature space and then you ask this question and so pay attention to the intuition i'm giving you imagine that each of these points imagine that each of these points Each of these points is a little lamp, right? Daya. In India, we have these beautiful little diyas, right? So imagine that they're beautiful. These are just tiny little lamps or candles or candles. Right? Glowing. So one very literal thing I'll tell you, you know, when you're traveling by train, when I used to travel by train in India, over long distances, train would go and this is obviously 30 years ago, it may not be true anymore. The train, I don't think I've traveled in train in India for decades. So anyway, the train would go through rural areas, you know, agricultural areas where there was no light, it would be dark but when you come to a major city you could tell that you're one hour or earlier because the strains were not very fast you could already tell that you're reaching a big city by looking at the glow in the horizon the city lights from a very far off distance you wouldn't see the actual light but but you would see the glow, the collective glow of the whole city lit up. And you know that you're finally coming to a major city. And as kids, of course, we used to get excited because when the train stops at a major city in those days, it used to stop for a longer time you could get you could get down walk around a bit get some snacks pick up a magazine at the stall the bookstore if there was one and so kids could have a lot of fun basically right so um we would look out for that. So something very similar. If you look at this and you ask yourself, which of the two regions, where is the light the brightest? So you would agree that light is pretty bright here in this cluster, I'll just mark it here. And light is light, the collective light would be brightest here right let me call this region actually a and b you you would argue that at a and b the lights is brightest because see light is reaching here from all of these points right it is getting the light from all of these points but this is in darkness relatively because the light fades out by the time it reaches here but look at the point a it is getting light from all its neighbors not to mention the fact that it itself is glowing so if you have ever lit up a lot of deer and a lot of candles, you know that really in the center of the heap the light is brightest, isn't it? So go with that intuition. So now what happens at this point move a little bit in that direction where taking a step forward leads to the and this is the crucial word leads to the greatest increase in light to the greatest light, right? So if you think about it, you're getting light from this direction, from A and from B. Maybe you want to make a step somewhat like this, right? Does it make sense, guys? We can make a step like this, Right? Does it make sense guys? We can make a step like this. Right? So let me call it X1, this is X2. You can make a step to X2, right? And then you keep making a step like that. So what will happen is you'll keep finding your path. If you're here, you'll end up there. Would you agree guys? So this is X33 this is x4 and finally you'll end up at b so there's a word for a and b the these brightest spots and by the way i'm using intuition here the papers will use much more formal language. These brightest spots are called attractors. They literally attract you. Imagine that you're a moth and you know you're getting attracted to its light in some sense. So the word attractor, of course, has a more mathematical origin. We won't get to that, but if you want to remember, if you just want a mental picture to remember, think of yourself as a moth starting at X. And so moths gravitate to light and you have just gotten to the brightest spot. So our A and B, and these are A, B in this picture. When you discover the A and B, so now you reached B. Now again, I start with another random point, let's say here, let's say here. And this time around, you will make steps that will finally take you to A, right? And so you have discovered A and B are the attractors. Once you have discovered the attractors, divide up the points between them. So once you have the attractors, the attractors, do one thing. Use the same density-based argument. What you do is find regions which are dense enough. Dense enough means sufficiently many points are there. So find the region, the region around the attractors the attractors that are dense enough in a very sort of a qualitative way dense enough we can be mathematically more precise you can say the density must be at least so many points per square unit area or something like that we can pick that that's your hyper parameter in some sense do that and then the remaining points that remain they are outliers right so density based algorithms always have this advantage that they not only produce clusters they also tell you what the outliers are so this argument is denlu and it's a very powerful argument. You use a lot in production, actually. So it is based on the concept of light gradient. You know, I can write the whole thing in a more mathematical way, but it will come back to the same thing. But I won't do that because of our current audience so any questions guys is it simple to understand the intuition of den clue are we understanding den q guys i don't know how many people are still here in the audience yes it's easy right so this is it you know you have three but now you have learned three clustering algorithms three classes of one is k-means clustering one is hierarchical or agglomerative clustering and one is density-based clustering now each of these clustering algorithms they have a lot of variants so k-means for example lot of variants. So k-means for example, there are many variants of the k-means. There are ways to make it faster, there's ways to make it not get screwed. We talked a little bit about that. Now there are certain data structures called kd trees or metric trees, right, that make it much faster to compute that. So there is a like you know you know, K-means is being the oldest, is perhaps the most researched of the clustering algorithms. And people have come up with all sorts of variants to make it better, especially because it's used so much in the industry. Then likewise for agglomerative clustering or hierarchical clustering, you realize that you can create all sorts of linkage functions. Think of this, this should be my definition of intercluster distance and based on what you pick. Now you can go ahead and implement it. And then you will come up with different sorts of clusters, you can see which clustering method. And then there are density-based methods. Density-based methods starting the first paper was DBSCAN. It is still the most popular. I believe scikit-learn has a DBSCAN implementation we will use in our lab. Now, then it's very, very effective. It density-based clustering not only find clusters, they also find outliers.liers now db scan doesn't scale very well to large data set so then people use gen clue which is a more sort of intuition is very simple but now do you understand what i mean by hill climbing you're going in the direction of greatest increa you know increasing light If you think points of highlight is the top of the mountain, you're going in that direction. The gradient of the density simply refers to the point that at each step you take must be in the direction where the light increases the most by taking that step. So there's a word for that. So you see that you take a little step in the direction of the greatest increase. And the greatest increase is mathematically given by the gradient. This operator is called the gradient. It is just a multidimensional generalization of slope. What is slope in y is equal to fx remember calculus in multi dimensional calculus is called the gradient but we won't go into this fancy stuff for this session. Suffice it to know that it tells you where gradient is most and which direction, I go to? It'll go. So it's the common intuition is suppose you're on the hill at the bottom of the hill, the direction in which the gradient or the slope is highest. What happens when you walk along that direction? Besides the risk of falling down, suppose you successfully managed to climb up the steepest part of a hill, you will reach the top in the shortest part time or shortest distance is that true guys right you will reach the top in the shortest distance so that is why you you go in the direction of maximum gradient and quickly you go get go and find the attractors and once you have found the attractors, the rest is easy. Around the attractors find the region, which has at least a minimum amount of density. And you say, this is a cluster and that's a cluster. Like you can find the two clusters and all other points become outliers. So this is DENCLU. DENCLU it turns out scales very, very well. You can sample and can scale scale you don't have to deal with the entire data set you can take a representative amount of data and you can get away with it and so forth you can do all sorts of games but even at scale it works very fast right so it is used in large implementations and so that is it guys, that finishes our discussion or understanding of clusters, clustering, finding clusters, which is the clustering algorithms. Or these clustering algorithms are also called clusterers. Let me write the word clusterers. Clusterer is nothing but a clustering but actually here i have a doubt sir so regarding this gradient slope we studied even this in linear regression like uh finding this rope and finding negative values and then uh positive uh values the same thing now sir even this is the sensitive thing the difference is uh in linear regression you did gradient descent means you came down the hill as fast as you could so you went against the slope okay this is climbing but that is coming down that is it the only difference is a sign which direction you go other than that it is exactly the same okay these are actually the gradient slope equal to something like x minus alpha slope i think so here it will be x plus alpha soap in that exactly exactly that is it you just change the minus to a plus and you have a hit what what is a gradient descent becomes a hill climbing or gradient descent this is see i use the hill climbing as a sort of a intuitive term it is also called gradient ascent right ascend is to climb increase the climb height so that one is different this one is ascending all you have to do okay so when there is a question related to linear regression, then we need to use the X minus as alpha slope. But when there is a question related to cluster, it will become X plus alpha slope. That is right. That's exactly. Okay, sir. Thank you. All right, guys. So we are coming to the end of our time. And that finishes clustering that also finishes pretty much the three core areas of machine learning to get started if you review the book this is chapter 10 i would encourage you to read this and review this uh with this you get at least a broad understanding of the field we will do a lab in this and in classification the next two weeks so that you have some practical experience. But this is it guys, much of machine learning revolves around these topics. There are other things also there, but these are the big stones, these are the big hills in the field. And much of it is about these three things. Now there are fancier and fancier algorithms to do classification. There are a lot of algorithms to do regression. Now there are a lot of algorithms to do clustering. And it's a vast literature that will forever keep growing. And you can't catch up to all of them. Even I don't know all of them or even most of them i just know some of them in each of these areas and as you spend more time in the field you'll gradually pick up more and more of these algorithms but they will often fall in these three categories there are more for example there's a recommendation system there is dimensionality reduction there are other things that people do, but there is language translation. Like how do you convert from English to Hindi and Hindi to Tamil and whatnot. So, but broadly, when you look at the field and you want to get started, it's actually straightforward. It's three things. Classification, regression, and clustering. And if you get these three things under your belt, you're making good progress. Any questions, guys? So, should I stop the recording?