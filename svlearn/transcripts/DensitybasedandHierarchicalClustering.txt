 All right folks, the last time when we talked, we learned about an important algorithm, which was K-means clustering. K-means clustering is a very powerful way to cluster data, we saw that taking just a few iterations, you can quickly converge onto the clusters and you can sort of stop. The steps of K-means were just randomly pick points and then declare them to be centroids, then assign all the points to the nearest centroid, and then recompute the centroid as quite literally the center of the points, each of those. So I hope you remember what we talked about and how we did the k-means clustering. Very fascinating algorithm, its main aspects are, it is perhaps the most popular clustering algorithm used and very well studied. Its behavior is very well known. Now the problem that K-means clustering has is to many problems actually everything comes with advantages and disadvantages. So the disadvantages are it doesn't answer the question how many clusters. So to answer that question, what you have to do is you have to try many different values of the cluster and you have to look for the elbow. Adi M. In the screen plot. Adi M. And whenever you see the bend the point of diminishing returns you say that is the optimal number of clusters. And the way you do that is the plot is, the X axis is of course K, the number of clusters. The Y axis is actually the within some square distance, you know, the pairwise intra-cluster distances squared. That makes your WSS. pairwise intra-cluster distances squared. That makes your WSS. And at the end of the day, you take that point where the WSS decreases that stops decreasing rapidly. And you know that that is a number of clusters. So to the fact that a k is a hyperparameter of the model means you have to tell k, you have to yourself say, let's take this k and compute. It sort of poses a bit of a limitation. What if you said, I don't know how many k's are there and you draw the screen plot. You're using up a lot of computations. have asked often are there other algorithms so there is two more algorithms that we will talk about one of them is hierarchical clustering it is also called agglomerative clustering and the other one is called density based clustering both of these are very interesting ideas and today we are going to develop them. The hierarchical clustering is, I will go through it somewhat rapidly because it is used quite a bit, especially in the medical sciences and so forth. It's one of the classic algorithms. I would like to spend more time with density-based clustering. So we'll allocate time accordingly here. So we're going to do agglomerative clustering. By the way, this is chapter 10 of your book. It is also called hierarchical clustering. And the second thing is density based clustering that we are going to cover. And these are all completely different ways of looking at data and coming up with the clusters. And that's the magical thing about machine learning you find very powerful ideas coming from very different directions a whole variety of ideas which all help in solving a problem so what is hierarchical clustering suppose we let's go back to our data set if you are looking at weight and size, you know that all the ducks will be somewhere here. All the cows will be somewhere here. The point is how do we discover it in this method? What you do is you begin by taking all of the points and you just line them up. You have all the points here. Then what you do is you find the points whose distance, mutual distance is the least, d i j, is smallest. And here I'm using d i j is the distance distance between so first let me define it distance between point i and point j, point i and point j is by definition this notation d i j. So you find the two points which are closest to each other. Maybe it is these two points, who knows, something like this. You take these two points, then you take points. So you start by, you keep all of these points in a row, sort of sorted by distance or something. Then what you do is you find the next two closest points. So these two are close, you make a relationship, you link the two. Then there would be some, what are the next closest points that you can find? I'll put them next here, right? And then keep finding the next closest point and the next closest point. So what you do is you keep pairing up the points which are close to each other as close as they can be. And then what you do at each point, when you, every time you pair a point, you replace a point like this with the center of these two points the center of these two points would be let's say somewhere here so suppose i take let's take let's exaggerate these this value right so suppose you have this point here what would be the center What would be the center? This would be the center, isn't it? Where the cross is. So what you do is from this list, you delete that two points and instead replace it with the center. You consider this to be the new point or this to be the new cluster, right? And so you keep on doing this and you keep on joining points. And what you end up with, sometimes there are three points that are all together or four points that are all equally distant from each other. And then they will all start joining. They will be joining and maybe there is one more point. And they will all join. point and they will all join. Maybe they will all, uh, they will all join at some level. Yeah, that's right. You keep accumulating points and you basically, you, you get bigger and bigger clusters and you treat these. So the way you think about it is that, uh, you, you can think of each point as a cluster or all the data as one cluster, but they are all these in-between stages of gathering points together, nearby points together, and marking them as clusters. When you do that, one nice thing that happens is you won't have the problem of k-means. You will very quickly be able to identify this sort of separations very, very easily. So you would, for example, if you were to cut it here, you cut this graph here, there are only two legs, one small leg, one big leg. Or maybe I'll add more elements to this here, just to show the point. so you you are able to find you want two clusters you can see two clusters you want three clusters four clusters you can see as many number of clusters as you wish and that means a lot to especially in certain fields because clustering as i said is also a matter of opinion like let's say that I will exaggerate this picture of the cow, let's look at the cow cluster. So the cow cluster internally may be like this. So even though the cow cluster may be like this, you clearly see a pattern here. There is actually cluster within cluster, isn't it? And so you can go deeper and deeper and you can further argue, for example, that this is another cluster inside this cluster. And that kind of intuition is very well captured by a hierarchical clustering algorithm. So you can pick and choose how big or small you want your cluster to be. And in biological systems, it means a lot actually to be able to do that because there are cells in the body and the cells are here. And quite often you can find little clusters of cells, but it depends upon what resolution you look at. If you look very fine grained, you'll want to consider some things as clusters. You step out, you want to consider something else as clusters, right? A bigger group of cells as clusters and tissues as clusters and so forth. So you do that and it's used in many, many fields, hierarchical clustering. The idea is quite simple and it is very commonly used also. I think after k-means clustering, the most commonly used clustering algorithm is hierarchical clustering. There is no magic to it. It only leaves one point though unanswered. Suppose I have two clusters. These three points, or these points, are merged into one cluster. This point I merged into another cluster. Now I'm trying to compute the distance between these two. So how do I compute distance? We know how to compute distance between points, right? This is the distance, dij is the distance between points. But how do you compute the distance between clusters? clusters and it turns out that you can have multiple answers so for example somebody may say it is the shortest distance between the two okay sorry i take this back it is the shortest distance well that doesn't look quite straight line, but imagine it's a straight line. You could say it is the shortest distance. That is pairwise distance such that one point belongs to one cluster, the other point belongs to the other. Somebody else may come, no, no, no, I don't want to look at it like that. I want to look at it as the largest distance between two largest pairwise distance. And you can say, well, that is my measure of distance. A third person can come and say that, no, no, it is actually the distance between the centroid centroids of the clusters right you could you could define it like that and you could have one more definition you can say hey you know let's do one thing uh let me use a different color for this blue what you can do is more rigorous you can say find every the average of every pairwise distance where pair being a point in a point belonging to cluster 1 and point J belonging to the second cluster right so you're finding the distance between these two and every pair. So what happens is, roughly speaking, what it means is suppose you have points like this. Then you're finding, first of all, this will have three lines, this will have three lines, this will have three lines. And so you would take the one over every pairwise distance. So how many distance pairs are there? Three times three is nine, like one over three square times all of these distances, you know, all the nine distances. And you would say that the average of the pairwise distance, that is the distance between cluster. So are you seeing that guys? When we think about distance between clusters, we could be coming up with all sorts of definitions defining the distance between cluster. So that raises the question, so which is the correct notion of distance? Which one should we use? Anybody has a comment? And by the way, you can think of more notions of distance than many such. Anybody has an idea? Distance between the centroids. Distance within the centroids. rich okay that is a valid answer but that actually in practice that even though that sounds the most intuitive it leads to a serious problem it's called the inversion problem i won't go into that but just take it as a fact that in practice while in theory it sounds the most intuitive in practice actually it doesn't work that well practice while in theory it sounds the most intuitive in practice actually it doesn't work that well it sometimes works very well but occasionally it gets anybody else any other opinion so we have the yellow the pink the green and the blue. Shortest distance, sir. The shortest distance, the yellow one. Okay, we have one vote for the yellow, one vote for the green. Anybody wants to go for the pink and the blue? Blue. Blue. So one is for the blue. Anybody for the pink? It's the blue, I think, sir. Blue, we are gravitating to. So here's the answer. Blue is actually a pretty good answer and a common use value because it sort of ensures that the answers are not very unstable. The only problem that it has is because you take averages, averages have a problem. And the problem with average is that outliers blow it up. What do I mean by that? So suppose you work for a reliance company or some, I don't know, I can take any Indian company, not to single anyone out. And the CEO of reliance makes a huge amount of money. So if you take the average salary that an employee makes in Reliance, or any of these large companies with heavily paid CEOs and owners, what will you find? What will the average be skewed by? It will be heavily skewed by the extremely high profits and salaries that the top tier, the executive staff and the owners get, isn't it? Are we together? Yes. So here in the US, for example, if I were to take a company, I remember at one time, which company was it? It is company is no more Siebel. The CEO of the company made a billion dollars. So when you average the executive salary and the CEO salary, you add it in the same pool as the rest of the employees, then it suddenly turns out that we all seem to on average be making a lot of money. But were we? We were not. We were making pretty mainstream salary with respect to the industry, a little bit higher, but not what the average would tell. So in other words, average is a problem. It suffers from outliers. Those people, the CEOs, those executives, they're paid huge, just way out high amounts of salary. Right, it is quite, at least in the US it's customary. I don't know about India, I'm just guessing. So in US, for example, it's customary that, suppose you are a, let's say, vice president, and then there is a director, and in between there are a couple of associate directors and associate VPs. I mean, so senior directors and associate VPs and all of that intermediate layer. But broadly speaking, if you look at a vice president in a directive, it is easy to see that sometimes the total compensation of one is more than, it can be easily two to three times the other, right? Then between the vice president, and I'm talking about not my company where I work, but Silicon Valley average pattern that people have found Oracle and many other places that have worked and it's very common and between that level in the next level the difference is not just a factor of two three but typically it's a difference of at least 20 you know the next level 20, what you would earn, what a VP would earn in 20 years. Then you go to the next level and it turns out that the next level earns, the CEO will earn something like, he will earn another 100 times the the C-level staff is making right so their salaries will be in hundreds of millions and so forth it's very common so when you start trying to take the average of that you would realize that if you looked at the average and somebody published it you would feel well I'm not making that kind of money, isn't it? So outliers skew the average. That's an example. Right? So it's a statistical fact. I just created a very colorful story to illustrate a point in statistics, which is that average, the simple average, or it is called the arithmetic average, average, the simple average, or it is called the arithmetic average, average is susceptible, is, I'm writing, I'm misspelling it, susceptible, okay, today my spelling is off, is prone to be to be adversely if affected this is a statement you should remember it is broad statement beyond just clustering and so forth adversely affected by outliers right that's a. So something worth remembering. So what do we do? Well, you can then say, I can come up with a different measure. I won't take the arithmetic mean, I will take the geometric mean. Right? Or you can say, I will take the harmonic mean. Now, these are things in India, at least we learn in high school. Matt, do you guys remember arithmetic, geometric and harmonic means? So let me just take an example. Suppose you have points X and Y. This is what arithmetic mean. Would you agree? Yes. So we just literally call it the mean when people say mean or average. That's what they mean. The geometric mean is, does anybody remember what that would be? It would be x times y. So x y, x multiplied by y to the power half. In other words, it's the same thing as square root x y the harmonic mean is is very interesting one over let's say the harmonic mean is h the statement is 1 over h is equal to 1 over x plus 1 over y right so if you think about this what does it come to therefore h is equal to x times y divided by x plus y are we seeing it guys right so this is the harmonic mean so these things you have done and then now I can generalize it to many things. For example, if you generalize it, it will become x plus y plus whatever divided by number of points one over n right the nth root of that and so on and so forth you can do the same thing whenever here it generalizes very easily right so people people can do i don't know you can come up with all sorts of arguments and say i will use this or i'll use that and whatnot So there's a name for that and by the way I haven't seen many people use geometric or harmonic means. They mostly stay with the arithmetic mean but I'm just giving you for the sake of argument that you can come up with your own set of concepts in this space. So what we say is that each definition of distance is called a linkage function. And the statement is that these are all valid notions of distances. So then how do we decide which one to pick? Could anybody tell, make a guess, how do we decide which linkage function to pick when we are doing agglomerative clustering or hierarchical clustering? Make a guess guys. Today we have a very small audience. Interesting. Okay, so would one of you like to make a guess? Want me to do it? No, data is king. This is the one rule you have to always remember. Data decides. So what you have to do, and this is, so in other words, linkage is what is called a hyperparameter of the model. You have to try out different linkages and see which one works best. different linkages and see which one works best answer is try out each linkage function and see which one works the best in other words leads to the best clustering the way that you are looking for it are we together and by the way this is a broad theme later on if at some point obviously if you continue with me there is a theorem actually it's called the no free lunch theorem it's one of the most fundamental results in the theory of human knowledge it basically says the no free lunch theorem. It's one of the most fundamental results in the theory of human knowledge. It basically says that no one approach will always do better than another approach for all data sets. So in other words, for some data sets, some approach is better. For other data sets, another approach is better, which is why machine learning literature is rich another approach is better, which is why machine learning literature is rich with many, many approaches. Some approaches are used more commonly, some approaches are used less commonly. So for example, you can look up into scikit-learn, the library, and see what the hierarchical clustering algorithm, what does it use by default. I'll leave that as an exercise, as a homework for you. Also see how you can change from one to the other. So that is that. But at the end of it, these are all different flavors of variations or dialects of the same algorithm, which is hierarchical clustering. Now, hierarchical clustering though is more expensive than the K-means clustering. The reason it is more expensive is because it requires a lot more computations, very, very computationally heavy. Well, today, in today's world, of course, we don't care for so much unless the data sizes are huge. And we see it's a balancing act. If your data size is a small hierarchical clustering would still converge pretty fast. But if your But if your data size is huge, the difference between k-means and hierarchical will be noticeable. So you'd have to be more careful of which one you use. So these two are the great classics in the field of clustering, these two algorithms. They are not, like in many ways, their single biggest benefit is they are simple to understand. They're simple to visualize. One great thing with this agglomerative clustering is you create this diagram. Dendrograms. Let me write it in small letters also. Dendrograms. And dendrograms give you a lot of intuition of what is happening with data. Right? So people love it. People print out these charts, these dendrograms, and then post it to the wall of their meeting room or their research group and so on and so forth. Very intuitive. This is its great strength. So that's nice about it. Now comes the limitations of these algorithms. Besides being computationally more expensive, it's still, it is not that expensive in modern hardware. We are, if the data sizes are fairly reasonable, these are still considered relatively inexpensive algorithms. The limitation is that they tend to work well only for bulging clusters, clusters which have a bulge all around. The word that we use is convex clusters or globular clusters. If the cluster is not globular, then you have a bit of a problem. These methods, they don't work so well. In the lab that we will do the next time, this will become very apparent. We will do a particular lab with this smiley dataset. You will see the strength and weakness of each of these clustering algorithms. Are we together, guys? We would move on then to a new class of algorithms, which actually in practical sense is very, very powerful, though it tends to be a bit expensive. These are called density based clustering. And it would basically says is that if you think if you attach a physical intuition to data, let's say that you have data like this. And you have your cow so here's the thing now what it says is that if you consider each point think of it as a grain of sand, right, as a grain of sand. So then what will happen is somewhere near the center of the cluster, what happens? of clusters are dense like this is also dense right but what about this these areas mostly empty. So not dense, right? You can say lightweight. There'll be maybe there'll be one or two stray points that might be here. Some unusually big duck, some unusually small cow. They may be there but these are the regions where you don't find many points, you won't find many grains of sand. Are we together guys? So now if you use this intuition what you're saying is to find clusters is to find areas where density is high, right, which are dense regions. Dense regions lead to this. So with that thing in mind, how do we find these dense regions? How do we find, the basic goal is, how do we latch upon these guys, right? So there is a method to it. And the method that they take is a little bit abstract. So I'm going to explain it to you. But if you don't fully get it, that is all right. But just remember, the main intuition is simply, you just find places which are heavy in the feature space. Lots of sand. A lot of sand is accumulated there. And you can sort of weigh in some sense or get a sense of density of the region and from that you can discover which points where the clusters are does that intuition make sense guys yes yeah so it's a very simple intuition but the way you bring it down to mathematics, it gets a little bit technical, not very technical, but it starts out as some two abstract notions. It says that take a point, take a point i, right, then around this point, draw a circle. So let me try to draw a circle here. I am not terribly good with drawing. But let us make a circle of radius. Epsilon. Yeah, this symbol, by the way, it is pronounced whenever I use the word. Right, this funny E, it is the Greek letter for epsilon is the Greek equivalent of E. Now when I have this, the region inside is called the disc or the region inside the circle is called the the interesting word is there epsilon neighborhood of point i right of the point i so far so good guys right let me use xi because you know we have been using x for data for features so xi right of this point so uh this is this is this looking simple the region that i have shaded is the neighborhood of this point So is this looking simple? The region that I have shaded is the neighborhood of this point. So next what you do is you compute how many neighbors does this point have? So let me color the neighbors with something else. Let me color it red. So suppose it has neighbors like here. Yeah, yeah, yeah, yeah, yeah, yeah, yeah. So it says count the number of neighbors. Neighbors of xi. Inside its epsilon neighborhood well you can count the number of neighbors let's say that they're about how many neighbors do i see here uh one two three four five six seven eight nine ten eleven twelve thirteen thirteen 13. Here it is 13. You may then make a rule. So now comes the part of defining a dense region. You can say if, and this point is the most important, if the number of points, of points, people often write it in code as n, it is n, if n is greater than equal to some minimal value, some threshold, let me just call it n0, n0 being, let's say, 10 points. So in other words, if you have 10 neighbors in your close neighborhood, right, if for a point psi, then what? Then we call it an interior point. So think about it. If you were thinking of houses, if you step out of your house and don't have to go very far, suppose this is your walking distance, right, of your morning walk, and you encounter a lot of neighbors there, a lot of houses there, you know that you are in a populated area. If, on the other hand, you go for a walk and you can't see any house except maybe one or two neighbors, what would you conclude? You're certainly not in a densely populated area, isn't it? In other words, you're not in a very clustered area. Does that intuition make sense, guys? Now, we have an interior point. And now we are almost there. The way the argument is built up is pretty elegant actually it says that two points i and j these two points two points x i x j the r and they use this word directly reachable And they use this word directly reachable. Reachable, if and only if. By the way, this word IFF, the misspelled if, if you want, it stands for if and only if. It's a very common term used in mathematics. It means it is true almost, it has to be both ways. If and only if J is in the epsilon neighborhood of xi. Are we together? In other words, at Xi, you have to draw a circle and j has to be inside. Right? If j is outside, it is not directly reachable. So far, so good. So we are almost there. We are building up the construct. So what have we built up so far? The concept of epsilon neighborhood, the concept of an interior point. An interior point is a point that has sufficiently many neighbors within its neighborhood. Then we are creating another concept of directly reachable. Two points are directly reachable if one is within, they both are within the epsilon neighborhood of each other. So because if j is in the neighborhood of i, then you can imagine that i is in the neighborhood of j. Both ways the statement would be true. So these are directly reachable. Then comes the next statement. Two points are reachable. The way they do it is sort of an interesting definition. Two points xi and xk are reachable. Let me just use the word reachable, just reachable. Reachable if either one choice is they are directly reachable or they are reachable through intermediate intermediaries, right? Some people in between. So what are intermediaries? So suppose I have a point I and you have a point K. Now, these two are, they're still reachable so long as there are points like j like you know u v w x right p q so what happens is that each of these distances each of these distances if it is less than epsilon, less than equal to epsilon, means what? That j is directly reachable from i, u is directly reachable from j, and this long chain continues. And using this change, you can, a chain of reachability, you can reach k, right? Are we getting that, guys? So two points are reachable, either if they points are reachable either if they're directly reachable or they can be reached through intermediate hops you can go from I to JJ to you you to me and so on and so forth right they become reachable so now comes the and we are done it turns out that if you just use these ideas, which are just abstractions of what we mean by dense points with radical abstractions in simple ways, we are done. Now let's go back to our clustering algorithm. We get back to our cows and ducks. And this argument is by the way very elegant and you'll be surprised how well it works. Suppose you have cows. So what you do is randomly pick a point. point to see, and let's say that there are some points here also, some point to check if it is an interior point. It is an interior point. So you randomly pick a point. So what can happen? You can either be unfortunate and you'll pick this or you'll be fortunate and pick something like this maybe you'll pick let's say this point right do you see this initial point first right suppose you pick that first point and now you say on every point reachable from the first. So what will happen? From here, you can go here, you can go here, you can go here, you can go here, you can go here, here, right? Here, here, here, here. You can go here, here, here. You can go likewise here, here, here, here, right here, here, here, here. You can go here, here, here. You can go likewise here, here, here, here. And you can go here and you can go here. What just happened guys? You ended up reaching essentially every point in the cluster, did we not? If only we make reachable hops, you know, we check if other, which points are reachable from this point, you agree that we would end up owning the entire cluster. Guys, am I making sense? Yes. Is it straightforward to understand? So you say you own this point. So now you look at the remainder of the points and you play that game again. So if you're unlucky, as I said, you might end up with picking this point. This point has, does it have enough points in its epsilon neighborhood? It does not, right? So you mark it as an outlier outlier this is an outlier this is an outlier this is an outlier this is an outlier these points will all get marked as outliers but then after a little while you'll hop into you'll end up picking let's say this point right and so once you again you make reachability arguments it will just fan out in some path like this right this you can imagine this this this this there's this it will end up owning this cluster right So this will become cluster, let's say A, and this will become cluster. So actually I have a doubt, sir. Is this somewhere related to probability in that case, union and intersection? Not really. This is a very clear deterministic algorithm. You say, I am just making this much jump. You have a notion of distance and you just go that much distance doesn't have to do with probability directly okay so yeah right so now what has happened is the beauty of this and by the way this db scan has many density based algorithms has many implementations. What I just explained to you has a name, dbScan. And it's quite interesting actually that it showed up, if I remember right, this algorithm was written by two or three people in people in either 1999 or 2000, literally at the turn of the century. And the moment they wrote this paper with this very simple idea that we can discover clusters like this in the field of machine learning and AI and data mining and all that, they were immediately awarded the best paper award, the best paper of the year award worldwide. So that was quite in the professional field, that's quite a prestigious thing for a research paper to have such an impact. And the idea is very simple, you understood the idea, which basically says that find points which are near each other and all of them put together is a cluster right so this was db scan now db scan had a limitation which limitation is who decides you know remember that we have two hyper parameters epsilon and number of points right the neighborhood region definition epsilon is the radius of the neighborhood region isn't it number of of neighbors and not the minimum number of neighbors for a point to be considered an interior point to be considered an interior point point interior point and so if you think about it if you take your notion epsilon very small then every point in the system will look like an outlier isn't it because you can't reach isn't it because you can't reach from any point to any other point isn't it so suppose every you set your epsilon distance to nanometers and let's say that all of these things you look as you see on the writing board they seem to be millimeters or centimeters apart so you say that if two things are not less than a nanometer apart, they are not neighbors. So what will happen? All your points will get declared as outliers. So that is an extreme case. But now, when we bring it closer to reality, what happens is that if you're not intelligent about choosing your epsilon, the clusters that you form are not good clusters. You'll end up with lots of tiny clusters. So epsilon needs to be wisely set. The other property is the number of what, how many neighbors do you need to have before you consider a point to be interior point? That also is a matter of judgment and as you change or play with that then your cluster shape changes. If you say there must be at least 50 neighbors then you may have difficulty finding clusters at all within a neighborhood. So things like that. You have to be intelligent about it. So what happens is that the limit, one limitation is that you have these hyper parameters. The other limitation that you have which is harder actually to deal with is that this algorithm is computationally does not scale. Very expensive. Large data. So the first limitation, the hyperparameter problem, they actually saw there was a variant of it, which most people don't seem to remember, but I'll just put it there. It was called Optics. Don't worry about it, but they tried to remedy some of the defects of DB scan, but not many people use Optics. What people do is they like dvScan very much and they are just intelligent from experience. They learn to pick good epsilon and good number of neighbors together and by doing that they sort of get away with it. One advantage of using dvScan is it also can find outliers and finding outliers in data is a big deal right so for example if you're having a credit card transactions and in some feature space some points are standing out like outliers what does it mean you need to be suspicious of it you need to go check is it a fraud credit credit card fraud and so forth. So that is the value of outlier detection and dbScan's property is it not only clusters, it marks or discovers the outliers. So as such it is very heavily used. Often it is my algorithm of first choice while working with certain data sets. It's a success. So there were many, you know, 20 years have elapsed and 20 years is an eternity in machine learning, where research papers keep coming out practically every day. You get dozens of them coming out. So now there has been a lot of progress and there is a new algorithm which is called Denclue. Denclue, Denclue, and this is actually heavily used. For example, Google uses it a lot internally in its search and many other things, web crawling and so forth what this algorithm does is it takes a slightly different approach it's more mathematical it deals with something called the gradient gradient of the density gradient of density and then it uses something called and I'm just mentioning these words because you might encounter when you read about it in blogs etc it uses a hill climbing algorithm now what is a hill climbing algorithm and what is a gradient I'll just give you the physical intuition the math is straightforward if you know calculus, but I'll assume that at this moment, you don't have an appetite for doing calculus. So let me go back to the same picture once again. Suppose you have data like this and data like this. So suppose you start out at a point like this here. Any random point in the feature space, maybe even far out, let's say you're here. You pick a random point in the feature space and then you ask this question. And so pay attention to the intuition I'm giving you. Imagine that each is a little lamp, right? Deer. In India, we have this beautiful little deers, right? So imagine that they're beautiful. These are just tiny little lamps or candles, right? Glowing. So one very literal thing I'll tell you, you know when you're traveling by train, when I used to travel by train in India over long distances, train would go and this is obviously 30 years ago it may not be true anymore the train I don't think I've traveled in train in India for decades so anyway the train would go through rural areas you know agricultural areas where there was no light it would be dark but when you come to a major city you could tell that you're one hour or earlier because the strains were not very fast you could already tell that you're reaching a big city by looking at the glow in the horizon the city lights from a very far off distance you wouldn't see the actual light but you would see the glow the collective glow of the whole city lit up right and you know that you're finally coming to a major city right and as kids of course we used to get excited because when the train comes it stops at a major city in those days it used to stop for a longer time you could get you could get down walk around a bit get some snacks pick up a magazine at the store the bookstore if there was one and so it kids could have a lot of fun basically right so we would look out for that so something very similar if you look at this and you ask yourself, which of the two regions, where is the light the brightest? So you would agree that light is pretty bright here in this cluster, I'll just mark it here. And light is light, the collective light would be brightest here. Let me call this region actually A and B. You would argue that at A and B, the lights is brightest. Because see, light is reaching here from all of these points. It is getting the here from all of these points, right? It is getting the light from all of these points. But this is in darkness relatively because the light fades out by the time it reaches here. But look at a point A. It is getting light from all its neighbors, not to mention the fact that it itself is glowing. So if you have ever lit up a lot of deer and a lot of candles, you know that really in the center of the heap, the light is brightest, isn't it? So go with that intuition. So now what happens at this point is you figure out in that direction where taking a step forward leads to the, and this is the crucial word, leads to the greatest increase in light, to the greatest light, right? So if you think about it, you're getting light from this direction, from A and from B. Maybe you want to make a step somewhat like this, right? Does call it x 1 this is x 2 you can make a step to x 2 right and then you keep making a step like that so what will happen is you'll keep finding your path if you're here you'll end up there would you agree guys so this is, this is X4, and finally you'll end up at B. So there's a word for A and B, these brightest spots. And by the way, I'm using intuition here. The papers will use much more formal language. These brightest spots are called attractors. They literally attract you. Imagine that you're a moth and you know you you're getting attracted towards light in some sense. So the the word attractor of course has a more mathematical origin. We won't get to that, but if you want to remember, if you just want a mental picture to remember, think of yourself as a moth starting at x. And so moths gravitate to light and you have just gotten to the brightest spot. So our a and b, and these are a, b in this picture. When you discover the a and b so now you reached b now again i start with another random point let's say here let's say here and this time around you will make steps that will finally take you to a right and so you have discovered a and b are the attractors once you have discovered the attractors, divide up the points between them. So once you have the attractors, the attractors, do one thing. Use the same density-based argument. What you do is find regions which are dense enough. Dense enough means sufficiently many points are there. So find the region, the region around the attractors that are dense enough, in a very sort of qualitative way, dense enough. We can be mathematically more precise. You can say the density must be at least so many points per square unit area or something like that. We can pick that. That's your hyperparameter in some sense. Do that. And then the remaining points that remain, they are outliers. So density based algorithms always have this advantage that they not only produce clusters, they also tell you what the outliers are. So this argument is Denclue and it's a very powerful argument you use a lot in production actually so it is based on the concept of light gradient or you know now I can write the whole thing in a more mathematical way but it will come back to the same thing but I won't do that because of our current audience so any questions guys is it simple to understand the intuition of Denclue are we understanding Denclue guys I don't know how many people are still here current audience. So any questions guys? Is it simple to understand the intuition of DENCLU? Are we understanding DENCLU guys? I don't know how many people are still here in the audience. Yes, sir. It's easy, right? So this is it. You know, you have three, now you have learned three clustering algorithms, three classes of one is k-means clustering, one is hierarchical or agglomerative clustering, and one is density-based clustering. Now, each of these clustering algorithms, they have a lot of variants. So k-means, for example, there are many variants of the k-means. There are ways to make it faster. There are ways to make it not get screwed. We talked a little bit about that. screwed. We talked a little bit about that. Now there are certain data structures called KD trees or metric trees that make it much faster to compute that. So there is a, like, you know, K-means being the oldest is perhaps the most researched of the clustering algorithms and people have come up with all sorts of variants to make it better especially because it's used so much in the industry then likewise for agglomerative clustering or hierarchical casting you realize that you can create all sorts of linkage functions think of this this should be my definition of intercluster distance and based on what you pick now you can go ahead and implement it and then you will come up with different sorts of clusters you can see which clustering do you like best right so those that's the hierarchical clustering method and then there are density based methods density based methods starting the first paper was db scan it is still the most popular i believe scikit-learn has a db scan implementation we will use in our lab now implementation we will use in our lab. Now, then the it's very, very effective. It density based clustering not only find clusters, they also find I find outliers. Now, dbScan doesn't scale very well to large data set. So then people use Genclue, which is a more sort of a intuition is very simple but now do you understand what i mean by hill climbing you're going in the direction of greatest increase you know increasing light if you think points of highlight is the top of the mountain you're going in that direction the gradient of the density simply refers to the point that at each step you each step you take must be in the direction where the light increases the most by taking that step. So there's a word for that. So you say that you take a little step in the direction of the greatest increase and the greatest increase is mathematically given by and the greatest increase is mathematically given by the gradient. This operator is called the gradient. It is just a three multi-dimensional generalization of slope. What is slope in y is equal to fx? Remember calculus? In multi-dimensional calculus it's called the gradient. But we won't go into this fancy stuff for this session. Suffice it to know that it tells you where the gradient is most and which direction should I go to. So the common intuition is, suppose you're on the hill, at the bottom of the hill. The direction in which the gradient or the slope is highest, what happens when you walk along that direction besides the risk of falling down suppose you successfully manage to climb up the steepest part of a hill you will reach the top in the shortest time shortest distance is that true guys right you will reach the top in the shortest distance. So that is why you go in the direction of maximum gradient and quickly you go and find the attractors. And once you have found the attractors, the rest is easy. Around the attractors find the region which has at least a minimum amount of density. And you say this is a cluster and that's a cluster like you, you can find the two clusters and all of the points become outliers. So this is definitely been to it turns out skills very, very well. Amit Singer- Right, you can sample and can scale. You don't have to deal with the entire data set. You can take a representative amount of data and you can get away with it and so forth. You can do all sorts of games but even at scale it works very fast. So it is used in large implementations and so that is it guys. That finishes our discussion or understanding of clusters, clustering, finding clusters which is the clustering algorithms or these clustering algorithms are also called clusterers. Let me write the word clusterers. Cluster is nothing but a clustering. Actually, here I have a doubt, sir. So regarding this gradient slope, we studied even this in linear regression, like finding the slope and finding negative values and then positive values. It's the same thing now, sir. The difference is in linear regression, you did gradient descent, means you came down the hill as fast as you could. So you went against the slope. Okay, this is climbing, but that is coming down. That is it. The only difference is a sign, which direction you go. Other than that, it is exactly the same. Okay, these are actually the gradient slope equal to something like x minus alpha slope, I think. So here it will be x plus alpha slope in that case. Exactly, exactly exactly that is it you just change the minus to a plus and you have a hit what what is a gradient descent becomes a hill climbing or gradient descent this is see i use the hill climbing as a sort of a intuitive term it is also called gradient ascent Ascend is to increase the climb height. So that one is different, this one is ascendant. All you have to do is Okay, so when there is a question related to linear regression, then we need to use the x minus as alpha slope. But when there is a question related to cluster, it will become x plus alpha slope. That is right. That's exactly okay so thank you all right guys so we are coming to the end of our time and that finishes clustering that also finishes pretty much the three core areas of machine learning to get started if you review the book this is chapter 10. i would encourage you to read this and review this. With this, you get at least a broad understanding of the field. We will do a lab in this and in classification the next two weeks so that you have some practical experience. But this is it guys. Much of machine learning revolves around these topics. guys. Much of machine learning revolves around these topics. There are other things also there, but these are the big stones, these are the big hills in the field, and much of it is about these three things. Now there are fancier and fancier algorithms to do classification, there are a lot of algorithms to do regression. Now there are a lot of algorithms to do clustering. And it's a vast literature that will forever keep growing. And you can't catch up to all of them. Even I don't know all of them, or even most of them. I just know some of them in each of these areas. And as you spend more time in the field, you'll gradually pick up more and more of these algorithms but they will often fall in these three categories there are more for example there's a recommendation system there is dimensionality reduction there are other things that people do but there is language translation like how do you convert from english to hindi and hindi to tamil and whatnot But broadly, when you look at the field and you want to get started, it's actually straightforward. It's three things, classification, regression, and clustering. And if you get these three things under your belt, you're making good progress. Thank you.