 This paper is quite interesting actually and it sort of follows the work of a few other papers all of whom try to ask an important question. We had the classical NLP pipeline. If you remember, and so far we have done that, using spaCy, we were doing the tokenization, the lemmatization, the tokenization, and then the part of speech tagging, and then the extraction of the dependency parsing and so forth so we did all of that and named entity resolution recognition so all of these are part of the pipeline of things that you do in nlp successively Then came the transformers. When you deal with transformers, and so before that I must also say that when you deal with the traditional thing, what you do is you, one of the things you do is you go from one hot encoding of words to word embeddings. You remember doing that and you can pick your word embeddings. So all of these things are part of the classical NLP pipeline. Then came transformers and transformers in many ways changed the game of the pipeline, the NLP pipeline. For one thing, you replaced embeddings with contextual embeddings. Even before that, there was ELMO, which was the first paper on contextual embeddings. What it says is that we took the word stand, I believe. The stand can have many meanings. So each meaning is derived from the context around it, whether we are talking about a stand or to be able to tolerate or to actually stand up. It depends on the context. So these are homonyms. And this phenomenon of words taking on many different meanings in different contexts. I think the word for that in and it's polysemy. So that is there. And so things like Elmo started creating context aware embeddings of words. Then came the transformers. And it basically said that, let us learn the embedding on the go as part of whatever task it is that we are trying to do. Let's say we are trying to do sentiment analysis or you're trying to classify whether it's a topic in science or whether it's a topic in history. So you can do that. So those are learned word embeddings and inherently they are contextual. So they capture that property of it. Then people began to ask that, see in the transformer, what we are doing is there is no pipeline. You take a box, you feed it and maybe I should draw a little picture here so are you folks seeing what i have on my screen what you have is a transformer transformer based. Let's say a classifier. I'll take some example. Say classifier. What you do is you pass the words as it is, W1 to WN. This is your sentence. And if you are using but, of course, you will have a separator followed by another sentence. Optionally, sentence, my sentence, okay. Sentence two, prime intents. Okay. Sentence two prime WN prime, right? You feed this along and into the system, you feed it, then there is position embedding, E1, E2, all the way to E n, then E1 prime all the way to E n prime. You have the embedding, which themselves are learned as part of the transformer process, and then you you fill it into and this is all part of example when you're doing BERT. You go through all those attention encoder layers, attention encoder layers. layers and when it comes out you take the first and by the way there's also one of the cls token and so there's a cls ecls here and what you do is you just take the output of the ecls this last output of the cls for classification feed into any classifier this vector that comes out of the first one let me just call this the intermediate xcls vector that comes out of the first token any classifier you can use logistic regression or softmax or pick your thing. You can throw in whatever you want. Classify it as you like. You can feed it through a few feedforward layers and so forth. At the end of it, you can do your classification. Say you're doing sentiment analysis or whatever it is. So this architecture we have covered, isn't it? Now the question is, and we just did it, I believe on Wednesday. So the question here is, where is the pipeline? What happened to the natural or the much more common understanding that NLP is a pipeline of things. You clean the word, you remove stop words, you do stemming, right? I mean, not either stemming or lemmatization you do all of that but then you do the part of speech tagging you do uh you know all of these things the word embeddings the the the you know dependency graph different dependency graph parsing and you use all of that in a typical pipeline but the big question is where is the pipeline is the classical pipeline transformers seem to be almost magical they are saying forget about the whole NLP the classical pipeline everything that we used to do, and you did that in your previous prior labs, we don't do that anymore. So that's basically the thing that we were using, in which we created the pipeline is now saying, forget that. In one stroke, there we go. Right? So that it begs the question then, two things, that does it internally somehow reconstruct the pipeline that we are familiar with? Or does it entirely think in a different way, which is not at all like a classical pipeline? These are the only two options, isn't it? So the options are possible options, I can say. It thinks, this transformer thinks completely differently. And so our classical pipeline is now proven to be not as effective. That is one possibility, let's just say, possibility number one. And there's a possibility number somehow, somehow, somehow, rediscovers or rediscovers or reconstructs the classic pipeline, the classical pipeline, it sort of does it in approximately what we do, not necessarily rigidly, but quite similar to that. It reconstructs the pipeline, somehow rediscovers it. So what it means is that as you go from layer to layer, layer to layer, et layer, you should be able to see that the pipeline, some things, the early things, for example, the basic things like named entity recognition and so forth, or the part of speech tagging, they should be there in the earlier layers, and more higher order representation should be in later years. After all, deep learning is representation learning, isn't it? The whole point of deep learning, just to step back a little bit, deep learning is representation learning. Right. And this is very apparent when you talk of images, what happens with CNNs, etc. The earlier layers discover primitives in images, right, in images, example in images, right? In images, example, in images, edges, et cetera, vertical edges, this, that. You discover that. And later layers, successive layers, build a higher order representation. And then it begins to build up a face or a cat or truck or whatever it is, higher order representation. It does. That is the whole point of deep learning that at each layer it is in many ways it is many many neural networks if you think of it as a single hidden layer network and the output of this layer hidden layer one layer is fed into the next layer each layer has its own unique responsibility and the responsibility of each layer is to do find the sort of representation it is responsible for. It is a little bit more complex. It is based on the representations discovered from the previous layer. And it is feeding that into the next layer, which will build even a higher order representation from that layer. So I hope this is a summary of what we have known so far in this workshop. So the question therefore is, is something like that happening when you use transformers? One would imagine that, right? So there are, one would, it is very natural to argue or say that surely earlier layers are discovering some basic primitives, like CNN's discovered geometric primitives. And now the earlier layers in this transformers, they must be discovering the lower level or the basic primitives of a natural language processing or linguistic processing. And those lower level things are things like part of speech tagging and so on and so forth, the named entity recognition and so forth. And so each layer is feeding something and the next layer is building a higher order representation as the task, as the information flow takes place. Now the whole question is, is it happening? So is it happening along the classical Is it happening, right? So is it happening along the classical pipeline approximately, or is it an entirely new way of thinking that the transformer is doing? So what this paper shows actually is that the classical understanding of NLP or linguistics is actually correct, broadly. actually correct broadly. The transformer internally without ever being told all of these things essentially rediscovers those pieces on its own, which in many ways is a remarkable validation that the theory or the subject of NLP works, right, and the transformers essentially follow the same trodden path. And that is the gist of this paper. So we are going to do that now. So that is one aspect of it. But then you might ask, if transformers can do that, and the classical pipeline can do that, then what is the value that these transformers bring to the table? Why are they more effective? The moment transformers came, BERT came and all this, in one fell swoop, they managed to take the top position at that time on a whole range of about 10, 12 tasks in the blue benchmark, all sorts of tasks, different tasks, it began to do very well and take the leading position. So how did that happen? Surely it is not just reconstructing the classical pipeline, it is doing more than that. And what is that more that it does? It is doing more than that. And what is that more that it does? It's worth understanding that. So this paper, in some sense, gives you, sheds light on the more that it does. And the more that it does is a couple of things that it does differently. One is that if earlier layers make a mistake, because they have a much more localized view of things. The successive layers which are looking at the bigger and bigger picture, they tend to correct those mistakes. In a classic pipeline, you don't get the chance once you have done a named entity recognition, that is it. You live with that it may be right it may be wrong but it is that okay but one of the things that emerges here is the successive layers go and revise the understanding from the previous layers in view of more information and that's a remarkable thing that it can do that and we'll see that that is one of the examples that this paper quotes and it a remarkable thing that it can do that and we'll see that. That is one of the examples that this paper quotes and it is quite remarkable that it quotes that. So we will go through that. The other thing that this paper says is that see when you're looking at basic things like a part of speech etc etc, those activities are focused or responsibilities or roles are focused in a very specific layer. So, you know, some earlier layers, some later layers or something like that. But the moment you talk of semantic aspects, you know, the meaning of the sentence kind of thing, those sort of tasks, they are not so localized. The entire transformer, all the layers are collaboratively figuring out the meaning, the semantic aspects of the sentence. That is an interesting part that see, you can do some parts, low level stuff, you can have localized responsibilities, but the meaning, the semantic aspects are much more spread out. So these are essentially the lessons from this paper, and that is its value. So that's the main thing that this paper is trying to say. Now, we will go over the details in a little bit. But before we go over the details, I would like to just introduce you to some of the underlying concepts, because this paper will use certain concepts, especially about the semantic representations. So let us, let me bring up our webpage and take a few examples to clarify some definitions. Hang on, All right. Now I'm done. Did I keep my pen? There's the pen. So we will use this Allen NLP, which is something that we'll use in due time. This is just to review some concepts. So you know that we did named entity recognition. Do you remember named entity recognition? What was it supposed to do? Locations, people, organizations, etc. So let's use this one, Elmo and NER, and we will take some text. For example, let's take this text. This shirt was bought at Grandpa Joe's in downtown, deep learning. So here, let us, before we do that, what are the named entities here? Grandpa Joe's, isn't it? It's an organization and deep learning seems to be a city, isn't it? In downtown this. Let's see what it comes up with. And you realize that it's a very deceptive sentence. I mean, you would, if you are naive, you may mark deep learning as just a subject, you know, but here, the context of the sentence presents it to be almost, it's a city named Deep Learning and downtown Deep Learning. And so you see that it has figured that out. And if you do this, it will tell you where is it, which words it is focusing on. So to figure out that Grandpa Joe's is an organization, which words did it pay attention to? Do you see this, guys? The saliency map tells you where was it? What did it consider very important? This shirt was. And how did it figure out that deep learning is a location? location well it turns out it figured out just like this it still picked up on these words now so anyway this is a just a way of seeing what is happening uh again grandpa joe's organization it paid really a lot of importance to Joe, right, to figure that out. And then deep learning, downtown deep. So you can imagine that the word that follows downtown would be the most important word. Whatever it is, it will weigh towards a location. And so it seems to be that. And so there's more inter, I mean, so there's a little bit of visualizations you guys can play at it and you can write your own sentence i won't do that or maybe i'll try some other sentence it's carrie my preferred candidate is carrie moon but she won't be the next mayor of seattle so now moon usually is not um it's not an unnamed entity but so let's see if this recognizes it and it did it said kerry moon is a person and seattle is the location very good so this is named entity resolution uh dependency parsing of course you're familiar with um you guys remember what dependency parsing is we did that in our labs we did entity recognition also in our labs. So Asif, what is the package behind Allen NLP? Is that a transformer that's working there or is it something like spaCy? No. So here is the thing. Allen NLP is an emerging NLP library that works in conjunction with spaCy very closely. It sits on top of spaCy and transformers both. So what it does is, this is why this project is relevant. See, it's a triangular situation. There was the spaCy tool that you guys are using. There is transformers as in Hugging Faces and so forth. Then there is Allen NLP, which tries to so forth and then there is allen nlp which tries to derive the best of both of these and create a layer on top and then things are moving around for example spaCy 3o which is about to come out uses and some people are already using so for example praveen amah here he uses it already the sort of not a production version but early version beta version now specie itself has assimilated or gone full transformer right so it has support both for classical nlp and for full transformers embedded in it so there's a lot of convergence in the industry right as if lnlp is built on space here right it's our extension if LNLP is built on spaCy, right? It's our extension. No, no, no. It is built on spaCy. spaCy, right? Yeah. spaCy and Transformer. That was its value proposition. And it's one of the things in due course will introduce all of these high-powered libraries. So you can look at this. This is your your dependency parsing some keys and so forth now we are used to a more spacey kind of a diagram so i won't go into that there is something called constituent parsing this is different let me explain it with an example. Let's take this first one and run it. So this sentence is first, this is the full sentence. Then you break it up into sort of phrases or sub phrases. So here appear Winkler died at 81. You would agree that the semicolon decomposes the sentence into two logically, this is a good logical break to make phrases independent. We know that in languages, of course, semicolon play that role quite often. And then you have immortalized age 61 in full stop. So it managed to break it up into the two Here it were clauses. You just use the word sub phrase or constituents these two constituents so that is the meaning of constituent parsing Now let's take a bigger sentence James went to the corner shop to buy some eggs milk bread and bread for breakfast. Now what happens? We do this and And we realize that there are actually no parts. There is James, who is a proper noun, went to the corner store. This entire thing is one logical constituent, right? So it won't, it will try not to, it will take logical units together, right? Keep it there. And it it is tough you can imagine that this is not just token tokenization or so forth so it's one of the exercises you do in natural language processing let's take another example if you bring dollar ten ten dollars with you tomorrow you can pay for me to eat too can you pay for me to eat too now what happens if you bring to eat too? Can you pay for me to eat too? Now what happens? If you bring $10 with you tomorrow, certainly makes one subphrase, comma, can you, but these two sort of hang out, pay for me to eat too? This is one phrase, right? That it took out, right? process is called constituent parsing. Now there is another job. So you can imagine that constituent parsing is a little bit higher level abstraction. Suppose I take this one, semantic role labeling. What semantic role labeling is, and I'll just read out the definition, which I think says it best. It basically is like who did what to whom, extracting the meaning of a sentence. That's what it does. So here they say semantic role labeling is the task of determining the label predicate argument structure of a sentence right so another label latent the hidden predicate argument structure so what is it that is being said essentially of a sentence and provide representations that can answer basic questions about sentence meaning including who did what to whom right so let's try a look this sentence oh so now let's look at it the keys so that answers what needed to access the building so building is more sort of the whom part right uh i don't know sort of a purpose it gives you the purpose needed is the verb and the argument is the keys needed to access the building. So now you notice that you're not just breaking it into constituent parsing parts, but you're doing more, you're getting the structure of the sentence out. Are we together guys? And you can do further things. The keys which, this is a reference word, which. Now let's try something else. So guys, are you understanding it? I'm just sort of showing very fairly, hopefully basic things. Let's take this sentence. However, voters decided that if the stadium was such a good idea, someone would build it himself and rejected it 59% to 41%, right? So obviously, whichever city it is, the voters don't seem to like the idea of paying for a stadium. Let's see what happens when you do that. Look here. Right? You can see that it picked up the verb decided. By the way, remember the specie LLP we used to say that at the root of a dependency parsing is? What is at the root of the dependency? Verb. Verb, exactly. Right? And so you see that here again. However, it is a discourse word, voters is your, if the stadium was such a good idea, someone would build it, right? So this is it. It picks up the sort of the meaning aspects from here. That is it. Then there is the one last thing that this paper uses or talks about, and I'll bring it here. That is it. Then there is the one last thing that this paper uses or talks about, and I'll bring it here. It is a coreference resolution. So coreference resolution, sign back in, not now. Let's take an example. Paul Allen was born, here it is, born on this blah this, attended this school, private school. And let me just give the sentence. So now see what it has done. It has created these core references, the blue ones. Do you see? It has figured out that all the blues are Paul Allen's, that is correct. At the same time, it has its relationship to all of these other entities. So let us look at the definition of coreference based on this example. It says, coreference resolution is the task of finding all expressions that refer to the same entity in the text right so is that happening what is the entity here paul allen right so everything that refers to paul a is here, all other entities that refer to him. So what other entity is there? Seattle is an entity. Does it say something about Paul Allen? Certainly, that's where he was born. Then Allen is, of course, the same thing. It is pretty clear that it picked up that he is again, Allen itself, he, he, he. And then it is within this, all the phrases, all this circles or sort of boxes tell you things that relate, right? Or refer to Paul Allen. And I'll let you read the sentence and hopefully you will agree. Paul Allen was born on January 21st, 1953 in Seattle, Washington, to Kenneth Sam Allen and Edna Faye Allen. he befriended Bill Gates two years younger with whom he shared an enthusiasm for computers. Paul and Bill used a teletype terminal at their high school, Lakeside, to develop their programming skills on several time sharing computer systems. This is of course a part of history as you know. I believe Paul Enner is no more, right? He's passed away. So this is it. So now we can take another example and see the legal pressures facing Michael Cohen. And let's try a look at that. This is far more recent. And now it picked up that if you are talking about Trump, then the core reference of Trump is only in this green area, right? And we read this, the legal pressure facing Michael Cohen are growing in a wide ranging investigation of his personal business affairs and his work on behalf of his former client, President Trump. In addition to his work for Mr. Trump, he pursued his own business interests, including ventures in real estate, personal loans and investments in taxi medallions. So, you know, these sentences are deliberately very tricky. But you see how many his and how many pronouns are scattered everywhere and you have to be able to relate all of them properly so this is again a higher level abstractions the core references the core reference resolutions that you have to do it's a higher level task to do it right it's not just tokenization or something like that right it's a tough thing to do for example you can't just do it by. It's not just tokenization or something like that. Right. It's a tough thing to do. For example, you can't just do it by programming. I mean, traditional in creative programming or something like that. These are high level machine learning tasks or NLP tasks. So this is the meaning of coreference resolution. So now that we understand these things. Let us now go back to our paper. So now that we understand these things, let us now go back to our paper. Are we together guys? Do I need to explain anything more? So what are the things we learned? Coreference resolution that relates which things in the sentence refer to it. Semantic role labeling is essentially extracting the meaning or getting to the meaning of the sentence. Constituent parsing is sort of building logical semantic subunits of the sentence right in many ways dependency parsing you're aware of sentiment analysis i hardly need to know um the name entity resolution of course you know what it is locations person organization and so forth so this is it so with that review in place now let's go to the paper and let's try to read this paper any questions Let us read this abstract. This abstract is actually the summary that says it very well. This is important. Let's read it. Pre-trained text encoders have rapidly advanced the state of the art on many NLP platforms. What are we talking about pre-trained text encoders? Transformers. Right? Transformers also act as pre-trained text encoders. BERT for example is only encoders, encoder part of the transformer, isn't it? So next he says we focus on one such model, BERT, BERT is something we learned on Wednesday, and aim to quantify. So we focus on one such model, BERT, and aim to quantify where linguistic information is captured. And this is the sentence that is we need to focus on. Sorry. Oh, yes. Within the network and I wish there was an easier way to do that. Here it is. this sentence that we have is the gist of it so it is asking the question all of these linguistic information about a part of speech and this semantics and this and that where is it where exactly is it there in those attention layers and coda layers right it is worth asking so then they say that then they produce their finding they say we find that the model represents the steps of the traditional nlp pipeline in an interpretable and localizable way that is the crucial sentence i would just put it with the underlying we would say that this part, what is happening? I wish I got better with the, give me a second please. Display show high tools. Full screen mode. No. All right. I will. Is this what it is? Oh, that I clicked on the wrong thing. Sign yourself. Okay. What should I have been clicking on? Close. Okay. Edit And the text images. Alright guys, I'm just getting used to Adobe's way of doing things. So give me a moment. Fill in form. Now we don't want that. Comment. What I want is to highlight some lines. So close. Yes, we are back to it. We find that the model represents the traditional NLP pipeline in an interpretable and localizable way. It means you can go and look at the layers and see what it is doing. You can bring back interpretability to it. And this is important. And it again goes to the topic we were talking on Wednesday, that can deep learning models be interpretable? We saw that to some extent the CNNs can be, you see what the filters are doing. In the same way now in the Transformers, this paper shows and a few other papers show that the pipeline you can recognize the traditional NLP pipeline the pieces of that pipeline in an interpretable and localizable way localization means you can say this particular encoder layer is the one that is doing and this layer these two layers together are doing most of the let's say part of speech tagging is doing and this layer these two layers together are doing most of the let's say part of speech tagging right or named entity recognition and so forth you can be very specific you can't just you can do that right and that the region's responsible for that and this is the next thing you know in the in the classical pipeline of course you do tokenization before you do classical pipeline, of course, you do tokenization before you do lemmatization and before you do a named entity recognition, you know, part of speech tagging, and then you do named entity recognition, right? So that order is preserved, which is a remarkable thing that it preserves that order. But not always though, not always. And that is where the magic of this paper, of the transformers comes out and this paper brings it out. It says that the part of speech tagging parsing named entity recognition and semantic rules and then coherence. So it gives you the order part of speech tagging, you know noun verb, et cetera, et cetera. Your dependency parsing named entity recognition should come only after that, makes sense. And semantic rules, you know, that's a higher level abstraction and core references. We remember that when we looked at the example, these are much higher order or sort of higher order tasks to do. They're more semantic or meaning based. Whereas part of speech tagging is not about meaning so much as about the atomic things. Is this a verb? Is this a noun? And things like that, right? So this is what it is. And now they go on to say that qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, which is a remarkable thing. So this is the sentence. It goes beyond that. It says that transformers not only recover or reconstruct or rediscover the classic NLP pipeline, but reveals that model, this part, that the model can and often does adjust this pipeline dynamically, revising lower level decisions on the basis of disambiguating information from higher level, higher level representation. So what it says is that suppose suppose you come across a word, and the word, one example they give is Toronto. In the sentence, once you get the word Toronto, the earlier levels, for example, a named entity recognition will recognize Toronto as what? As a city, isn't it? Would you agree, guys? It's a geopolitical entity, it's a city, isn't it? It's a, would you agree guys? It's a geopolitical entity, it's a city. Yes. Yes. But if the latest sentence implies that we are not talking about a city, but Toronto happens to be the name of a sports team, all is not lost because the later layers, which look into the meaning of the words, based on the context may revise that answer and say that no it's the name of a sports team because a sports team could also be called toronto for example so that is the power of transformers in in many ways right that that highlights one of the great things about the transformers. And so this sentence is important. This sentence. Often adjust the pipeline dynamically, revising lower level decisions. Based on the basis of Asit sir, can you increase the resolution? It's difficult to read. Is it? Oh, I apologize. What's the resolution of your screen? Let me let me do that. I didn't realize I say said that earlier when that happens. Is that better now? A little more maybe. Oh that's nice. Okay. Yeah, better than earlier. How about this now? Yeah, best. Not much in the screen now. That's a little too much. It's a little too high resolution. Yeah, this is better. Glass is on too. OK. I guess it depends upon what sort of screen you have. I apologize, guys. My monitors are 43 inches and 49 inches big. So you can imagine that I tend to forget that on smaller monitors it may be hard to read. So always catch me on that. So all right, I'll reread this because some of you were having a hard time reading it. This abstract is worth going over again. Pre-trained text encoders, think transformers, especially BERT, have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the framework. So you're asking, OK, can we go and look and specifically tell where is the linguistic information? So, for example, where is the part of speech tagging information stored? Which layer? Which of the attention layers? And where is this named entity recognition happening? In which layer, one or two layers or three layers are involved with that? Is it localized or is it that the entire transformer does everything collectively? entire transformer does everything collectively? Those are the questions to ask. And they say, you aim to quantify where linguistic information is captured within the network. So this was a goal of their research. Now, what is the finding? We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way. It means they can actually identify this layer, so a few layers and layer or two layers so forth and say okay this is doing part of speech tagging much more than other layers, this is doing this, this is doing this, so you could localize certain activities of the pipeline to certain attention layers encoder layers right and localized and that the regions responsible for each step appear in the expected sequence and that is the next important thing when you find where things are happening you realize that the earlier layers and the later layers the things that that they do, they follow the classic NLP pipeline. In other words, part of speech tagging comes first, parsing comes, dependency parsing comes next, a named entity recognition comes thereafter, semantic roles comes after that. Now you're getting at the higher level, meaning aspects of things. So semantic roles comes then and finally the the who which part this part is talking about who and what and so forth so then you go into the core reference part of it right um that comes and so you see the progressively higher order abstractions or tasks coming through isn't it Or recognitions coming through. I don't know what word to use, concepts being built, so forth. Then they say qualitative analysis reveals, I mean, when they look at it, it sees that the model can and often does adjust this pipeline dynamically. Revising lower level decisions on the basis of disambiguating information from higher level information so the example they gave is a sentence that contains the word toronto but the sentence is deliberately crafted such that toronto refers to a sports team smoking toronto so what will the early layers do what will be the name part of speech tagging and the named entity recognition do they will all point the fingers towards or push it towards being a city a geopolitical entity but the later layers which understand begin to understand the meaning of the sentence they will revise that understanding and say, no, it is actually, it is a sports team. Right. It's an organization, not a geopolitical entity, not a city. It's a team, basically. So that is what they show in this paper. Now, we can go over the details. I don't know how much of the details I should go over. Is it still readable guys or it is completely unreadable? It is readable. Okay, good. So this builds upon prior work. One of the work that it really builds upon is this particular work. People have created some instruments or some tools to be able to probe a layer and see what is it that it is doing, right? You could do it through some sort of ablation study by saying, if this layer was not there, then what would be the performance of this transformer? What will it fail to do or not do well? So they do, that is one way of doing it. And then they use a couple of other ways and we'll read through this and you'll see what they do. The other way and is okay so why don't we read through this paper is worth reading through. It's a short paper. So pre-trained sentence encoders such as ELMO have rapidly improved the state of the art of many NLP tasks and seem poised to displace both static word embeddings and discrete pipelines as the basis for NLP. Basically all of this, basically the main burden is that ELMOS and thereafter the transformers change again. While this has been a boon for performance, it has come at the cost of interpretability. So this goes to that question, these transformers have looked very uninterpretable, black boxes. So it has come at the cost of interpretability and it remains unclear whether such models are in fact learning the kinds of abstractions that we intuitively believe are important for representing natural language. Or are they simply modeling complex co-occurrence statistics? Are they doing something very complicated internally? Are they doing something very complicated internally? So a wave of recent work has begun to probe the state of the art models to understand whether they are representing language in a satisfactory way. So people are starting to ask those questions. Much of this work is behavior based, designing controlled test sets and analyzing errors in order to reverse engineer the type of abstractions the model may or may not be representing so far so good guys it's very simple they're saying that we're going to take the model and then try to see its behavior how does it change and we'll design some experiments to figure out what is what. Then they say parallel efforts inspecting the structure of the network directly to assess whether there exists localizable regions associated with distinct types of linguistic decisions. So there's also people are also trying to look at the network directly and to assess you know is, is this first layer doing this? Is that second layer doing that? Right? So they're asking such questions. Such work has produced evidence that deep learning models can encode a range of syntactic and semantic information. Right? In other words, this is a long and short way of saying what we have been saying, and that more complex structures are represented hierarchically in the higher layers of the model. Hopefully, guys, this is clear by now. This is what I have been explaining. See, one question trying to relate to the previous lecture that we had right when we were looking at the model then there was a notion of multiple layers and then there was also the notion of multiple heads oh okay that's right that's right so so so the thing is Okay, that's right. So the thing is, you can just assume that at one layer, right, the earlier what is happening in any... So for simplicity, because all those multiheads have essentially the same information flow, just assume that it is one head to understand this paper. Paper is basically focusing on it as a single head with multiple layers. That's basically how the paper is describing it. Logically, logically. Even though there are multi-heads, because it's a repetition, what happens in different layers are quite along the same lines. It doesn't invert the whole thing. It's along the same lines. So stay with this. So then it goes on to say semantic information, et cetera. And the more complex structures are represented hierarchically in the higher order of the model. We explained that we build on this latter line of work, focusing on the BERT model and use a suit of probing tasks derived from the traditional NLP pipeline to quantify where specific types of linguistic information are encoded. So this is what we are going to do. Building on the observations at lower layers of a language model encode more local syntax while higher layers capture more complex semantics. Which is the way it should be. And this was already discovered by Peter et al, earlier research, which was already beginning to show that sort of the syntactic information is in the lower layers, and the semantic information is in the higher layers. Semantic is the meaning part, syntactic is noun and so forth, parts of speech, etc. That's one easy way to remember. Syntactic versus semantic. So semantic is a more higher order meaning based thing. So they come later. So that work has already been done, as they point out, that lower layers of a language model encode more complex syntax while higher order layers capture more complex semantics. We present two novel contributions. First we present an analysis that spans the common components of a traditional NLP. So basically they are looking at all the entities in the typical NLP pipeline, all the parts. we show that the order in which specific abstractions are encoded reflect the traditional nlp pipeline we have gone over this we show that the order in which specific abstractions are encoded repress reflect the traditional hierarchy of these tasks also we have done secondly we quantitatively qualitatively analyze how individual sentences are processed by the BERT network, layer by layer. We show that while the pipeline order holds in aggregate, the model can allow individual decisions to depend on each other in arbitrary ways, deferring ambiguous decisions or revising incorrect ones based on higher level information. So this again goes on to that, think of that example, the Toronto example, and you'll pretty much begin to get the sense. And there's a couple of more examples we'll go through. Basically what it means is some things which are not very clear or become, look wrong after from a semantic perspective the syntactical recognitions may be wrong so the semantics once a semantic understanding emerges it may go back and revise some of the syntactic recognitions so edge probing is a technique our experiments are based on edge probing approach of Blau, which aims to measure how well information about linguistic structures can be expected from Yaa. So edge probing decomposes structure prediction tasks into a common format where a probing classifier receives spans. So what you do is you take spans of the sentence, blah and optionally another. So Bert of course takes two, if you remember, two sentences typically. But they do here also, it's something like that. You can take two different spans and predict, must predict a label such as a constituent or relationship, relation type. So you must associate a label to a span of the words. Oh, by the way, here the span is not necessarily the sentence, I take that back. It could be just any combination of two, three words or a single word. The probing classifier has access only to the per-token contextual vectors within the target span. So now it goes into a lot of the details which I don't think are terribly relevant. You can talk about it. So here they're talking about eight tasks from the edge probing suite. So these are important. Part of speech, we know what they are. Constituents, you know, recognition, we did that in a little bit. What are the pieces, the sub phrases that this is made up of? Dependencies of dependencies of course we understand very well we have done dependency party entities is named entity recognition semantic role labeling we just went through that right and uh co-reference also right yeah yes right after that a core reference right and there's one more semantic proto roles right uh srp which we should just assume it's another higher level abstraction semantic thing so these tasks are derived from the standard benchmark data sets and evaluated with a common metric micro average f1 to facilitate comparison across stars so they have some devices and tools to do that i won't go too much into that because now we're getting into the implementation detail perhaps we shouldn't much into that because now we're getting into the implementation detail. Perhaps we shouldn't get into that in the very first reading of it. So they take the tokens and they look at it at two different levels. There is something called scalar mixing weight. Let me just talk a little bit about it. This part, the metrics, this part is important. I really sort of ignore it, what came before. We defined two complementary metrics. The first is scalar mixing weights. It tells us which layers in combination are most relevant when a probing classifier has access to the whole bird model. So okay, let me give you a little bit of a background. What they do is you take a trained bird, you freeze all the weights, you make sure that while you're doing all this probing and poking around the bird, you don't accidentally make it, you know, become aware that you are probing it, you know, change its weights while you're probing it or learn anything. So how can you do that? You just make sure that the weights are frozen, isn't it? Are we together? Or one way in PyTorch, how would you do that? You put it in the eval mode rather than the train mode, right? Models. So when you have done that, in that particular case, the weights are not changing. And now you can go over probing everywhere. And then when you looked at this, so the scalar mixing weights tells us which layers in combination are most relevant when a particular probing classifies access to the whole model. The second is the cumulative scoring. So this one tells you, you know, which two, three layers are important. Now the next one is a different metric and this is important. The second cumulative scoring tells us how much higher order, how much higher we can score on a probing task with the introduction of each layer. So what it means is suppose you have layer one, layer two, layer three, and suppose your task is, let us say coreference, which we do realize, maybe Elastic's very basic entity, named entity recognition or part of speech tagging. You would imagine that the first layer would contribute a lot, right, to this task. Then you add the second layer, your performance improves. The question is differentially, how second layer your performance improves the question is differentially how much did your performance improve when you went from one layer to one and two layers right and then you go to the next layer and again see how much improvement you got so the differential improvement by including a layer and not including the layers so far, right, that is given by the cumulative scoring. Are we together? So what will happen is the early things like part of speech tagging, the early layers will contribute a lot. And after that, it will just taper off. You don't expect more successive layers adding much to it. So from that, what do you conclude? Suppose you see a huge difference coming in, in the beginning as you add layer and layer after layer to part of speech tagging, but after a little while it dies off. You, what conclusion can you draw? Perhaps a conclusion you can draw is that much of the work has been done, understanding of the part of speech is done. The later layers are not adding any value to it it's not altering the answer right and so you have localized it you have said that okay these are the layers that are doing that particular thing so that is the gist of this what they are saying so remember scalar mixing weight just tells you which couple of layers it it sort of tells you probes into it and tells this is it and the second one tells you that after how many layers it it sort of tells you probes into it and tells this is it and the second one tells you that after how many layers it is done you know now you're moving on to higher order higher order things so these are the two things now there's a bit of a mathematical expression and so forth one quick interruption here is if i think i'm still uncomfortable with the intuition behind what are the metrics measuring because it looks like the metrics are actually by the use of the word probe they're going in between the layers and they are capturing the intent signal from there and then doing something with it but that is right they're looking into it what they're basically see that you can do it two ways like you could just remove a layer and see what happens. So what you do is you start with the first layer and you ask, is it able to, just as a classifier, is it able to do part of speech tagging correctly? You say, well, it comes up with certain accuracy. It will make mistakes. Then you add one more more layer and then you realize that the accuracy has gone up the f1 score which is a which is a harmonic mean of precision and recall it goes up then you so then what happens is that your f1 score keeps improving but at some point it will stop improving right after that the more successive layers that you include in your probing the cumulative score won't improve and so you say okay I reached a saturation point means successive layers are not really involved with this activity then the other one which is the scalar weight business what is it called the scalar mixing weights what the scalar mixing weight does is it called? The scalar mixing weights. What the scalar mixing weight does is it answers a different question. Which combinations of layers are most relevant for that task? So suppose the cumulative tells when you have reached the end of that task and mixing weights is a different probe, which very overlapping, but it tells you, it tries to be more precise and says, these are the layers that are involved. These two things. This will become very real when you see the example in a moment. So there's a bit of mathematics and these are layers and so forth. I won't get into that too much, but it is best given by an example. And so what you see is that as you go layer after layer after layer, do you notice that part of speech tagging starts early, right, in the earlier layers. Then constituent part comes a little bit later, dependency parsing comes a little bit later. Entity recognition on the other hand starts a little bit earlier than this. The rest of these, now these are semantic layers, they come distinctly after the syntactic layers, the first positions, constituents, dependencies, and entities. They're more like syntactic processing. And then come the semantic processing layers. So you see early signal of what is happening here. That's what they're trying to say. Now, accumulative scoring also, when they say what is happening where, you can see. So I won't explain this, actually, because it's best explained. In the first reading, guys, I would say that you can skip all that because this paper, if you read it very carefully, it can take a lot of time. And for me, we won't be able to finish it in a one and a half hour time slot. So let's look at this example that shows what is happening. So the x-axis by the way are the layers. These are the 12 layers. Now look here. If you look at the named entity by this layer, by seventh layer, you're already coming to the conclusion, let's read this, you're coming to the conclusion that it is what? Blue is organization. In the beginning, it said it was a GPE, torrentor. Do you notice that? It was initially saying it's a city. But as you move forward, is it still saying city? No. Gradually, the fact that a team is an organization, that begins to increase. The moment you use the word smoked Toronto, then by the time you go to this semantic relationship thing, it is pretty much a certainty. Argon is basically saying, you're pretty much a certainty. R1 is basically saying, it is you're talking about a team here. You notice that how the blue rapidly increases and it comes to a different conclusion. Now it is telling you that it is, well, what you initially thought was a city is actually not a city when you put these two together. And then it has come to this conclusion. Now let's read another sentence. China today blacked out a CNN interview that was something or the other. So look at the word today. If you are just looking at the words, what is today? You would expect that it is a time thing, isn't it? You would expect that it is a time thing, isn't it? Are we together? Yes. You're talking about time, date, so forth. So in the beginning, you would very clearly start saying, hey, it is a today's a noun and so forth. You won't realize that you need to couple it together with China. And then gradually, as you keep analyzing it, you notice that after a little while, it will revise its answer. And it may start saying, China today looks like a proper now, which is most likely some newspaper, today blacked out or something. So this is, this is a gist of it. Actually, this Toronto example is cleaner. Toronto is smoke Toronto. The sentence was, what was the sentence? It's a very specific sentence that It's in the beginning of that page. He smoked Toronto in the playoffs. So if you look at it, it's a deliberately deceiving sentence. You're trying to see how the transformer will recognize it correctly. A naive approach would have just tagged Toronto as a city and stopped at it. And the value that transformers bring, in some sense, is that they let you revise that answer and which perhaps gives us intuition as to why transformers suddenly give us a much superior performance than the classic nlp pipeline isn't it because in the classic nlp pipeline you don't get to revise your answer revise your judgment later evaluations but here you can revise it the other thing that is there and i wouldn't get into somewhere in here i forgot where it is uh is there the fact that maybe in the next few pages uh in the appendix it is there let's go and look at the appendix there are a a lot of graphs. It takes some time to absorb all of this. But they take another example, BERT-BASE, BERT-LARGE, more or less the same discovery process that you will see. And see, you realize that as you go to semantic layers, they tend to be, you see that these are semantic layers. Sometimes it takes all the layers to be involved in making a decision. They are all, you know, for semantic layers, the whole transformer thinks as one brain. That's one way of thinking about it. It is a whole brain activity. Whereas these syntactical things are somewhat like low level things like, you know, hearing a sound, seeing something more localized in the brain, in human brain, right? So there's a close analogy to that. The syntactical stuff seems to be more localized. The higher order, the most semantic things are more spread out. It takes the somewhat like that. My intuition is that which may be wrong, but we don't take it as it is. It's more a whole brain activity for the transformer. So and it goes over many, many examples. So you can read the paper in greater detail and work through that thing, all the metrics that they derive, but the burden of this paper is the fact that a transformer essentially reconstructs the classical pipeline, which is good because that adds tremendous interpretability to it. We are all familiar with those intuitive concepts of parts of speech and all of that and it seems to be doing that. But the other part which is interesting I found is that its ability to revise the answers from the previous layers. And I found that very interesting that it does that. It gives you some intuition, at least a little one reason of why transformers do so well and that is it guys we have about six seven minutes Thank you.