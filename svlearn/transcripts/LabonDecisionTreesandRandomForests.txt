 Give me a second guys. All right guys, so I'll restart the recording. So today to recapitulate what we'll do is solutions to the homeworks. We have the river data set, the flat data set, the California housing data set, and the breast cancer data set. Today I'll walk through the solutions to these, first using just feature engineering, then using decision trees and random forest, the two methods we learned in the last two weeks. If you missed either of the two sessions, remember the decision tree recording is online and did I I believe I'm yet to put the random forest recording online it's probably online because of the live YouTube so what if you can confirm oh yes it is life I can see it's like yeah so all of them are live and so please feel free to go and recap recap on the material or review the material after the class in case you miss it. So with that preamble, let me go straight into the lab part of it and I'll share my screen. There we go. Give me a moment. All right, guys. Are you all able to see my screen? Yeah. Yeah. moment. All right, guys. Are you all able to see my screen? Yeah. Yeah, entire browser you're able to see isn't it? Okay. And in this browser, what I will start is with the data set. So to recap, here, actually today too I'm doing it in Python. Maybe the next solution that I do I'll insist on doing it in R first, showing the solution in R first in the interest of making sure we cover both. In any case I'll post the solution, the notes in both. So here we go. We have, so just to give you what will be a progression, this is a exercise partly in careful feature engineering, seeing how far it takes us. We used logistic regression the last time to see whether we could solve the problem. Now, when you look at this data, if you recall, just so this is again, okay, let me go from the beginning. These are standard imports, right? By now, you should be familiar with the inputs. By the way, I posted at least a partial, most of this notebook into our Slack. You can pick it up from there. So it is all there. So this is there. can pick it up from there so it is all there so this is there here i load the data by the way is the font big enough on your screens or should i need to increase it anyone would like to give me the feedback increase the increase the font okay how about uh hang on this because it is good How about, hang on, this is good. 125 is good. Okay. So maybe a little bit more, I think. A little bit more, right? Okay, I can't. Yeah, this is okay, I think. 150% Yeah. Okay. Are you guys also seeing a green line here? Yeah. I don't know why that comes. Maybe I'll unshare and share again. So stop sharing. And I'll share again and hopefully this time we'll be luckier. Otherwise, I'll share the entire screen. All right. So are we your screen is not scaling to entire screen like what we see. Oh, really. But the font everything is, you know, like clearly visible, you know, like It's not actually hindering anything else. So it's fine. Yeah. So is it showing again like in the top one quarter of your screen is that what's happening? Yeah one fourth part of the screen a dotted red line. Oh no. One of you just take a screenshot of your entire screen and post it to Slack. I'd like to see what really is going on. Yes. I'm worried about it. So Zoom is supposed to be the best of the lot and it seems to be unfortunately also giving us issues. I can try sharing my entire screen. Should I try that? Oh yeah, Anil shared this screenshot. There is that. Give me a second guys and so I can see. Anil, in your case uh you're seeing the entire thing or you're seeing it in corner one corner of your screen you're doing fine isn't it no the entire screen is there okay so you are fine so somebody who is actually having problem could you please uh share a screenshot i'd like to see how it looks yes so may i suggest that anil you have a 4k monitor isn't it uh no it's a laptop is it a 4k screen uh it's a 4k screen yeah yes probably that is it guys some of you if you're still having a 1920 by 1080 screen my guess is that's where the problem is happening yeah yeah minus you can adjust yeah i think you can adjust the screen between speaker view and just move it left and right and you can see the full okay uh in your case also you're seeing it fine isn't it it looks fine on your screen too arithria so i want to see it on somebody's screen where it's problematic all right guys i apologize for it it is a unfortunate reality without um is it completely unreadable? No, I think we can read it. It's just that for me, at least I am on Mac. And for me, just on the two sides, there is a black strip means, you know, the screen is not coming. But that's fine. Yeah, it is not the full screen. But we can see the whole thing. I think you can see the whole thing i think you can proceed maybe i don't know if anyone is in a situation where he cannot read at all i'll do something see if it is better yeah sorry i wasn't mute yeah it's fine for me yeah now it's good i think i did something i don't know what you did but i shared now it fills the whole screen. All right guys. So let's get started. So you remember we load the data, we drop the null values and so forth. Remember the pandas profiling is a great tool to quickly get descriptive statistics or preliminary descriptive statistics. It is not everything don't always completely rely on this this was the flag data set we got the descriptive statistics on it and then it gives you quite a bit then remember the first thing we do is we extract we separate if you're doing supervised learning either classifier or regressor the first thing you have to do is extract out the target variable from the data and now you have a separation between the feature set the features that feed into or the depend the independent variables and the dependent variables another way to say is input and response capital x by convention is the input capital and little y is the response, why is that because X would be a matrix because there are many features producing the input. Raja Ayyanar?nilro?a.: And a row is like the many roles so it's a matrix whereas y is a column vector matrices a capital in general convention people follow and why they write it as a. in general convention people follow and why they write it as a column vector. Actually, I take that back. In neural networks, all those conventions go out to the window because everything is matrices. All right. So we visualize this data. If you remember, this was a theory of logistic regression. Basically, what we said is log odds is the distance from the decision boundary. By the way, I have a video on our website, on our YouTube channel, about distance from the decision boundary, you might want to consider watching that. It is quite relevant in the context of, let me show that to you. It's a very short video for 15 minutes. Why do I call it distance from the decision boundary? It is because youtube.com youtube.com slash supportlectives. If you go back, go down here, you see linear classifiers, distance from the decision boundary. This is the video I'm referring to. And you will realize that at the end of it, let me go straight to the end. Yeah, you will come to the conclusion as you do here that this equation or this beta naught, et cetera, et cetera, here W, it represents a distance from the decision boundary. In other words, this thing is the distance from the decision boundary. All right, so that's a little bit of theory for those of you who remember your ML100. We'll build a model. If we try to naively build a logistic regression model, as we realize, it's a disaster. It's a baseline model. It just degenerates to the baseline model where the majority class is the predicted class or the zero R model. Next, we try it with polynomial regression, polynomial logistic regression. We expanded the feature space with polynomial terms. When we expand the feature space of polynomial terms, we get a pretty complicated model. You see how complicated this model is. And the fifth degree polynomial. But then if you try to look at how good this model is, still the accuracy is only about 69% for the zero, which is the real positive test case here. There are fewer zeros here. 85 for one, but it doesn't. So then we do a more careful analysis by extracting the river, modeling the river with a line. Oh, where's the line gone for the river? Okay, seems to have not run the visualization. Look at the model diagnostics, model diagnostic look good. These were the model diagnostic. You remember we talked about having a homoscedasticity in the residuals and the residuals must show a bell curve distribution, which you see on the side. All of those are good things, a good correlation, positive correlation between prediction and reality, which again is here. The Cook's distance to show if any points of high influence none of them seem to have they are all doing well and finally we are here it is we visualize the model over the data so we now have captured the river the center of the river and so it helps us extract the feature namely just the distance from the center of the river and we are done after that we just have to build a logistic regression in one variable just look at at this. We are using a feature space of just one variable. We ignore all the input variables and we are just using d, which is a new variable, the feature that we extracted. And when we do that, we get a model that is actually already from the confusion matrix, begins to look good. And when we do the classification report we suddenly see that it has very good accuracy 84 percent for the zeroth class generally overall precision the the harmonic mean of these two is pretty good actually this is 89 percent so 89 percent accuracy is a pretty impressive accuracy even for the zero the positive test case we get 84 percent pretty good accuracy we can see how the precision and recall is for each of the two classes for the zero positive classes this 85 82 83 and so on and so forth pretty good the roc curve again if you remember what matters is the area under the roc curve when we look at the roc is the area under the ROC curve. When we look at the ROC curve, the area under the ROC curve, it's an impressive 96% of area under the ROC curve. Pretty good. How often do we get the mixture? So for blue, we misclassified some blues as there we misclassified some blues as yellows, the sand, and some ones we misclassified, very few number of ones we misclassified as zero. So this is it. So now the question is we learned about decision trees, isn't it? The theory of decision trees, just to recapitulate, we learned that a decision tree goes about recursively splitting the region it uses either genie or entropy as its impurity index or disorder measure and then it uses information gain as a criteria to split where to split i hope this time you all understood information that reminds me i have to send you a survey guys but do please attend that respond to that survey like you notice if you feel you didn't understand something I always repeat it right so that's how I discovered it. So now that theory when it comes to practice what does it become? It is a pleasant experience it becomes just three lines of code one line to this is just importing the library this is to build the classifier do you notice that you build a decision tree and you fit it to data training and training and training data you make predictions you get predictions and then what do we do You make predictions, you get predictions, and then what do we do? We look at how good those predictions are. We look at the confusion matrix. The confusion matrix looks pretty good. The principal diagonal has most of the elements and off diagonal doesn't seem to have that many elements, which is very good. If we do a classification report, you see an accuracy of 87 percent not quite as good as our feature engineering but still respectable right so this model is not really as good as the previous are doing it carefully by hand but still it's a respectable measure now when I build this decision tree, one of the things I didn't specify here is how deep was the tree. So we can specify actually those parameters. So there are a lot of parameters that we can use to tune the tree and perhaps it will make it a little better. Then, once we do that, we can create, as you notice, where am I? Decision tree on the river dataset, right? So this is it. Overall accuracy of 87%, very good accuracy. And then, so it just shows how these nonlinear methods or decision trees are quite effective and you see the reason why when decision trees came onto the scene people they really went overboard with it you know they got there were exaggerated statements that this is all you need to do and she learned and so forth those times have changed of course today a decision tree is not considered the state of the art by any means but it is there nonetheless so yeah go ahead yellow brick classifier any reason for this yellow bit classifier it is a library that helps you visualize models okay model diagnostics it helps you visualize model diagnostics so here we go we look at this so guys look at this decision tree produces our ROC curve like this for a moment just stare at it and tell me is this better or worse than what we did in future engineering compared to this what do you think guys our future engineering ROC curves are better or the decision tree ROC curve is better one is better yeah it's much superior and it also shows guys that if you can do feature engineering it is the gold standard machines will only try to achieve that sometimes they are successful so i'm quite often but it comes close when you try to visualize this decision tree remember the whole value proposition of decision trees is that they're interpretable i'll let you decide how interpretable this picture is how many of you feel this is a highly interpretable picture i'll open it in a new tab a highly interpretable picture. I'll open it in a new tab. Where is it? Yeah. And I'll expand it. Even in its expanded form, how many of you are delighted that this looks like a very simple explanation of that Java dataset? You wouldn't feel that, isn't it? It's a . It is almost a black box, though in principle decision trees are highly interpretable. In practice, the moment you give it a non-trivial data set, you begin to say all you see all interpretability going out of the window pretty quickly. Whereas a feature engineering, it was so easy for us to interpret. We had an intuitive explanation. We said distance from the center of the river. That is the difference. This is why good feature engineering is the gold standard. We will do this now again with random forest. When you do it with random forest, once again, we studied the theory of random forest. You use a lot of trees and so on and so forth. You see me do that. We'll grow a lot of trees. How many trees will we grow? We'll grow a forest of thousand trees. When we grow a forest of thousand trees and by the way number of jobs in parallel. This is the degree of parallelization. How parallel you make it. So the more parallel you make it the better. So for example if I say number of jobs that will run in parallel is 100 right obviously if your machine has that computing resource then by all means you go and do it and if you do that notice that this decision this random for is built on my machine in 1.6 seconds but don't expect the same on your machine. If you have a gorilla machine, it may finish even faster. If you're running it on a laptop, expect it to run for a minute or so. Is that a standard for 1000 trees? Or how did you pick that? That's a very good question. See, my basic rule is I never take more than 400 trees or 500 trees. Not a good. The other rule I use, I count the number of predictors. Here the number of predictors were 11, right? No, sorry, not 11. Here the number of predictors is 2, right? Number of predictors are 2, okay. I multiply 400 by number of predictors. Excellent, okay. I multiply 400 by number of predictors. So it's a, see, we live in computational plenty. Sort of go overboard. There's nothing wrong with 200 times to 400 times to 800, so I approximated it to a thousand. It quickly ran through. But do we it to a thousand all right it quickly ran through but do we really need a thousand trees probably maybe i could have gotten away with 400 i usually don't go below 400 okay the default is 100 and the reason uh the secret learning all these things have a modest default is because uh they are trying to make sure that the code runs the library runs on people's laptops and some pretty underpowered so that's that so what did you mean by number of predictors is two or they didn't get that. The data set. What is the dimensionality of the ? Oh, OK, OK. That's cool. OK. So that is that. So now that is all I'm talking about here in the random forest. And so you run it. This is the level of parallelization that depends upon how good a machine you have so model diagnostics is again simple the same model diagnostics that we have been doing so do you notice that guys certain things become boilerplate in every analysis you'll do that once you build a model if it is a classifier you'll you'll need your confusion matrix you'll need your confusion matrix, you'll need a classification report, and you will do the ROC curve. Right? So once you practice this, you will use it for the rest of your life. That is the value of it. While this is good to do, I must also say that when you interview people and you see people from different programming houses or different data science houses come to interview with you most often you find that people are sloppy they don't actually do the model diagnostics carefully so you have to just learn to do it right once and then do it all right so with random forest you see that the roc curve is is it looking better than the decision tree guys yeah yeah so can you can you guess why is this better than the decision tree what is wrong with decision trees there is a limit to the uh what to say extent or extent or the branches of the tree. We are limiting it. Actually, the problem with decision tree is exactly the opposite. It tends to just go and overfit to the data. Remember, it fits the data hard. And so you have variance errors. But a random forest, because it's an ensemble of lots of trees, it reduces your overfitting problem. Because it's a voting machine, different trees are built or looking at different aspects of the data, it is very hard to overfit because each tree gets to see only a small window of reality. So even if it overfits, it will overfit to a very small window of reality, never to the whole reality. So random forests, therefore, are much more robust against overfitting. And it gives you a clue that the reason the decision tree failed is because it was an overfitting problem. And that showed up, right? Look at the tree it built. When you look at a tree this complicated, what do you conclude from this very, very complicated tree, which is practically impossible to read? What you conclude is that it has gone in overfit to the data. Yes, that is it. See, complexity, unnecessary complexity. We are able to explain this data in one line, distance from the center of the river. But this decision tree needs such a complicated explanation to explain it. So that's a classic illustration of overfitting. Then, random foils certainly seems to do better. But even if you look carefully, you'll notice that the ROC curve of this is only 95 percent. Remember, hand-created, your feature engineering gave you 96 percent. So Random Forest does come close and you can tune it and try to tune it and so on and so forth. right so what you can do is a you can try to take one tree from the random forest right I just took the first tree from the random forest inside and said show me how does it look and you realize that each of the trees still looks pretty complicated isn't it guys but because you are doing an ensemble over this comp all these different trees your answers are better now one of the things that you get in decision trees and random forests is a overall very rough measure of how much did each of the features matter in helping you decide about make a decision whether something is river or not is a river point or not it is called feature importance how important is some feature just to give you an intuition let's say that you are selling ice cream on a beach right then you may be temperature is an important feature, wind is an important feature, day of the week is an important feature. But suppose in the data somebody has given you the number of penguins in Antarctica who jumped into the sea that day. Just because you have that as a column or as a feature in your data, it doesn't mean that it's a very relevant feature. So in feature importance, its importance would be close to zero and the more significant features would trump. For example, temperature would trump and so on and so forth. So that's about feature importance. In the same way, if you talk about classifier, we are talking about deciding whether something is a cow or duck, you would all agree that size would be a pretty important factor, isn't it? But when you're looking at the cow or the duck, right, think something fairly irrelevant. Let us say that whether this, if they have stripes on their body or some sort of a straights or color lines on their body, whether the color lines are horizontal and vertical, perhaps doesn't matter. You can take some other irrelevant feature like how much mud is there on their feet as far as you can make out. Probably may not matter because both of them may or may not go through mud right so that is the value of feature importance in determining the answer in predictive models now one of the things i gave you guys is the flag data set and i give you a hint that flag is just a small twist on the river data set so how many of you saw the flag data set on the river data set. So how many of you saw the flag data set? I did. Yeah, there is you did. And anyone else who did? You did. So I'll walk you through actually. It was just a small puzzle. See, when you look at the flag data set, you visualize and it looks like this does it look remarkably similar to the river data set guys yes yeah it looks very much like the river data set so now what do I need to do except that if you are above there if you are on the one side of the river you are this greenish color if you're on the other side you are the yellow color isn't it so when we compute the distance from the center of the river the rest of the feature extraction is exactly the same but the only distance is do you notice that you don't take the absolute value this is the only one line change in the river this one was absolute here it is just as it is. Distance from the center, positive distance will make it green line, negative distance will make it yellow line. That's all. And when you do that, you again do your analysis, exactly the same analysis, the same feature building of logistic regression. You notice, except that now it's a tri-valued thing and here is your confusion matrix looks pretty good i would say the the principal diagonal is heavy and red the off diagonals are pretty light right and the number of mistakes you are making are very very low and if you look at that classification report you have a close to 89 accuracy fairly impressive accuracy right so with feature engineering just this is it once you develop the art of feature engineering right you can use the intuition you develop in one data set quite often into other data sets right obviously here i engineered it to be such that you could use the intuition from one to the other let's look at the roc curve would you guys agree that the ROC curve here is pretty good? Yeah. Right, in fact, it is 94%, 98% and 99% ROC. I would consider those pretty impressive ROCs here. And then this is the beauty of feature engineering by hand. I mean, when you feature engineer, very rarely do more black box algorithms come as close to it, but here we go. It depends on the context also. Sometimes it's hard. The reason you use black box algorithms is you can't feature engineer. You don't have the intuition. You can't visualize it, right? So you let them do the feature engineering for you. So here we go. The number of mistakes, how often, the river points are sometimes confused as green, sometimes as red, right, and so on and so forth, one and two. But there's a little bit of a mistake here, a little bit of a mistake here. Now suppose I try the decision tree classifier on flag, the code remains exactly the same, no change except that I'm feeding in the river data set here. When you look at this, what is our confusion matrix is here, looks pretty good. You give the accuracy, do you know that the accuracy is only 86% guys? It is significantly less than your hand engineered feature. Yeah, feature engineering exercise, isn't it? And when you do the classification, that's your classification report, and this is the visualization of it. Once again, when you look at the decision boundary, I mean the ROC curve, you're 85% 91, 93 good, but nowhere close to the 98 and 99 person kind of numbers you were seeing with feature engineering. These are the number of mistakes, right? And I'll leave the last part, doing it with random forest as an exercise for you guys. From here onwards, do you think you can complete the rest of the exercise guys, doing it with random forest? Yeah. Yeah. So, fill in the gaps. I've deliberately left some things unfinished. I'll be posting the solution to Slack, right? And obviously, please don't post it on the public web. This is otherwise I wouldn't be able to teach the next class. They would say already all the solutions are there. They wouldn't be able to teach the next class they would say already all the solutions are there they wouldn't even try they'll just take the solution and just say oh i'm done shouldn't be like that so all right this is it now any questions guys i sort of went through this uh this is the way ideally you should create your notebooks This is the way ideally you should create your notebooks. So by the way, Dennis, what method did you use to find? Was your method for the flag of what I used or was it some different method? Oh, actually, Cindy is our code on Slack. Yeah, no, no, but could you explain what approach you use? Was it the same or slightly different? Oh, for the flag one, I first classified, no, I changed the label for either one or two into a consistent label, like saying like I changed all the twos to one or changed the also wants to do oh and then just did absolute value um okay classifications and like a set of a parameter if your lowers ends of midlines and your you know either one or two and then that way was actually biased against um like two's apart um, it's biased towards the direction of like which way you initialize it as. So I did it twice and stacked the predictions together and then got like a 90% accuracy. Interesting. Okay, I'm going to read your code. So guys, some of you have sent me your notebooks, I apologize, I've been rather busy uh now fourth of july is coming and i'll be reading it over that and i'll give you guys feedback on your notebooks i said one quick question on on when you did random forest on river here how many no how many subset of features one like there are only two features here did you yeah that's right so what happens is that in a situation like this it will pick up one feature at a time it will just build a tree of one feature and uh as a same question with a different part so when we select like thousand then a thousand decision trees for the random forest method how those trees are different like when we have only two features oh yeah the reason is that even if 500 trees pick one feature and other 500 trees pick the other feature remember they are being given different subsets of the data bootstrapping is happening right okay they given different subsets of the data bootstrapping is happening right okay they're getting different parts of the data that's where it is so it was a really problem it was there just to teach you guys the importance of feature engineering it was just a didactic data set i created to bring out the importance of that. How do you prune the tree for decision tree and code? Yes. So the pruning of the tree is built in here. Let's talk about it. The next example, which will be more complex next time, next week, let's come to that. What you do is you force it to limit the depth you say max depth is equal to let's say for actually in this in this problem itself I do so you'll see it you'll see it just give me a minute and you'll see this happen Vaidhyanathan Ramamurthy, Where did I suddenly come here. Vaidhyanathan Ramamurthy, At this moment, are we done with the river and that isn't looking simple like are you are you feeling more comfortable with the solution. Vaidhyanathan Ramamurthy, Right i'm getting a lot of. Vaidhyanathan Ramamurthy, yeah okay so. Yeah. Okay. So any feedback guys? Is it looking simple now? The river dataset and the flag dataset? Any feedback? Like this thing, all of this code, is it beginning to look familiar now? Nobody is giving a feedback. Yes, very good. Yeah, I hope it is. If it is not, remember that we have a Q&A session. Come by and I'll help you get there. So the next data set that I will take is an interesting data set. But before we take it, actually, it is 7.55. I'm wondering if we should take a little break see guys why don't you do this i have posted the river data set to the um to the slack i can post it again would you like to take some time let me let us do this let me post it to Slack guys. And you use the time to browse through it for 15-20 minutes and then we'll continue. How about that guys? Make sense? Make sure that it is comfortable for you because you guys are not saying anything and if it is then we will continue from there So, if your file just says river, but it contains both river and flag. Yes, it is called River careful, but it contains river and flag. Three methods feature engineering, decision tree, and random. And maybe you can take up a task, take this, and then I've deliberately given you HTML so you can't quite run it. So if nothing else, you'll have to copy-paste it into your Jupyter notebook. You'll learn something. You'll get some practice. Then I've left holes in it for example i haven't done the the random random process you can go complete it i haven't done the feature importance for the flag you can go complete it so there's a little gap try to go finish fill in the gaps guys and see how so can you please explain where you use the absolute function for feature extraction. See, in the case of river, because whether you are above or below the river doesn't matter. The only thing that mattered is how far you are, isn't it, from the river. Whereas in the case of flag, it mattered whether you are up or down. Up was one color, down was another color, another class, so you don't use the absolute function. Did you get that? Yeah, okay. That is it. It is as simple as that. See guys, if I may say something, much of data science is about just thinking about the data. It's not so much programming the programming when you enter this field it looks all new and strange and so but by now you just reading the solution you see after a little while it becomes very easy and repetitive you get one solution right you'll be using the same methodology for your next and the next and the next. Quite often you use the same process, similar processes, but this field is all about thinking about data, cleaning, preparing the data, feature extraction. That is where the magic is and sometimes you can just think and solve a problem rather than actually hack it through code. So, all right guys, spend, it is eight o'clock. Let us meet at, 20 should be enough, maybe not. 30, how long? It will take you guys some time to read through it. One of you who has opened it, give me a sense of how long it will take you guys to read. Do you do read? It hasn't come in my slack yet how is that possible it is there i see it here do you see it on my screen yeah it came yeah it's there check check your data science are you checking that yes i'm in that so the last message i've brought is the zoom details okay probably it's not i don't know it's in data science 2020 group harini yeah i'm looking the same at the same thing prachi but yeah i think i will reload it and see okay yes is anybody else Anybody else having problems? The rest of you. Jaishankar, are you able to get the river this? Yes. So, give me an estimate. So, Asif, so what is the purpose of the HTML code in the like the first line? Oh, good that you asked. It is one of my secret tricks. Okay, I don't know if it matters to you. Let me explain that. So is it for visualization, like for making some visualization work? No, see what happens is, by default, the fonts and the Kalesk, the fonts that Jupyter uses, I find to be not the ones that I like some people like they find it clean and nice I like my own fonts right so do you notice that my title is in a different color and font the text is in a different color and font like if you look at my text here right wherever the explanations are given I'll give you an example you look at this this is more of a textbook font isn't it are you are you seeing that in it yes so I like to see explanations in the form that I'm used to seeing the way you write in text book so I like to put my own custom fonts and so forth you don't have to do that so the beginning part is my you notice that i've given my own font here laura yeah that is all i'm doing i'm setting up my fonts i like the laura font much more so that's what it is here so i i tend to go between the LoRa and the Literata font. Both of these are Google fonts, they're beautiful fonts. And I sort of go back and forth through that. It's just a bit of aesthetics, no other purpose. You can ignore this. Even if you delete it, the whole thing will work. So how are we supposed to open it like using Colab or? No, it's HTML. I've deliberately made it hard for you so that you can't just directly use it. You'll have to read the code, understand it and then write your own notebook. Aritra, you have to write every line. Or you have to copy paste every line from the HTML. Yeah. You can copy paste, but the whole idea is that see, if I give you that solution as a notebook, then guys, you'll only have good intentions of doing it. You'll never do it. Right. So I made it just slightly harder, giving you the whole solution, but as HTML. And at the end of the workshop i'll give you the notebooks also when the class finishes at the end in three more weeks i'll give you the original notebooks so you can hold on to them but ideally you would you shouldn't need them because you should create your own notebooks looking at the solution all right guys so one of you who is looking at this notebook, give me an estimate 20 minutes or 30 minutes. How much do you think is reasonable to study this? It's a pretty long notebook. I think 20 minutes. Okay, so we'll meet at 8.25. It is 8.05 and we'll meet at 8 25 it is 8 of 5 and we'll meet at 8 25 percent i will put the recording on pause for the time being Hey, I have one question. So, yes, go ahead. So I mean, how are we supposed to read it like I opened it with a notepad but then it looks on like, why don't you share your screen I'll help you. Okay, let me share my screen. Any any web browser you can open it in any like web browser you have like ie or chrome or safari anything when you downloaded arithra does it have an extension html yes yeah so just if you double click it should automatically open in your browser yeah it should open in a default browser like for your machine okay just double click on the file okay oh yeah okay okay so i'll see you in 20 minutes and obviously you know help each other make sure that you study the whole thing guys don't just study the whole be comfortable you'll need this for the next data set that we are going to deal with it's a real-life data set sort of California housing and there it would really help if you have read this. um so Thank you. E aí E aí E aí Thank you. E aí Thank you. Thank you. E aí E aí E aí Thank you. Thank you. E aí Thank you. Gracias. Gracias. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Kansai International Airport I'm going to make a E aí All right, guys, can you hear me? Yes, Asif. We get time to study the study the notes. Do we need more time or we can start now? Hey, Asif. I did not go till that far towards then. I think I will need more time, but you should not wait. I think you can go ahead. But there is one, I have a question here on number 15, right? Number 15? Yes, I'll go down to number 15 in the careful, okay. Number 15 is... Model the river with polynomial regression, or polynomial regression what is i think you are trying to do the feature engineering there right so y underscore reg right y underscore reg is are you missing out what i'm doing here is, okay, this needs explanation. I'm taking the river data set. Do you notice that what I did is, from the, so, okay, hang on. Look at this. What did I do? I limited the river only to data points that are t is equal to zero. Only to the, this, you notice that data are limited to this right okay i just give it a name x underscore reg y underscore reg just to say that this is the limited data segment only for regression because remember the the yellow points are gone yeah and why did I remove it? Because I can't do a regression line if I keep all the points. I need to just draw a line through the river. In other words, I want to draw a line through the river, through the center of the river. So I filter down to only these points, and I'm trying to fit a line only through this that's right oh so that is why your output there is the other variable yes that's it okay so x2 is essentially an equation between x2 and x1 so x2 is like the y and x1 is like the X. Absolutely X two is a X one. So I see why I have this question about that line, line 15, line number one. So why is the X capital? I know that it's probably a matrix and Y R D G is small because it's a vector, but x1 and x2 are similar right so i just follow the convention your point is valid the in this particular case it doesn't make sense because um this is also just a column vector now isn't it right so you could have used little x but it is just a practice in data science that the input has that capital x associated and that's why you also put like two square brackets for the word x1 whereas you put on that is right i forced it in to look like a matrix of just one column one column. No particular reason. I mean, sort of these are more like conventions in the field. You get into a habit, you know, when you do this, you don't even think. Because like trying to do this, like fitting polynomial fitting myself, of course, without looking at your code. And so i was like looking up online like how to fit a polynomial and all that so i was trying like polyfit and all those things yeah but it was somehow not working out for me but i see you use it completely like you make a pipeline and all those things so so i mean it's very hard for me to have guessed like that we have to do something like this so yeah that is the whole point of taking this course that i'm trying to teach you the right way of doing things and everything you're learning is new you could probably not have guessed but because you tried and now you see the value of this isn't it right right yeah this is like completely different from what what i was doing so right and now that you know this way by the way this this will become the your new standard way of doing it in machine learning in data science what happens is people make a big deal out of pipelines. That data comes, it gets transformed to something else, it gets transformed to something else, and from there it gets transformed to something else. So you have a pipeline through which data flows, which is what I'm doing. Actually, because it's linear regression, I didn't really need to do the scalar, but I wanted to introduce you to the concept of a pipeline. That this will scale the data this will make it polynomial right are we together and yes so what is the function of the scalar i mean i sorry i missed that part this is the x minus mu over sigma is oh okay okay it's like basically you normalize everything that is right z-back test people statisticians call it zero and there are different scalars there's bit back scalar and so forth there are many scalars there's robot and so forth. So see, my job is, the way I see it is, I'm introducing you to all the big areas of what people do, right, by using these examples as illustration, getting you introduced to this, so that using this notebooks, once you understand it, you can use this for the real life problems. understand it you can use this for the real life problems okay one of the feedback that i have gotten from people is they actually use this code in the notebook in their workplace sorry thank you Anything else, everybody, before we move on to the housing data set, guys? One more very basic question in the top, if you go, you know where you have all those import statements there? Yes. Not this one, the one below here. What is this percent? Is is it is doesn't look like a comment to me what is this percent matplotlib inline these are called the magic commands what it is saying is single percent is saying that go and do well it's sort of a very python and j Jupyter specific thing. You're asking, you're giving it a command that matplotlibs all of its output. Like you shouldn't have to say when you draw a figure, right? You shouldn't have to say plot.show. If you give it a line, it means that at the end of every visualization, I don't have to say plt.show. Otherwise, what happens is you would have to say plt.show otherwise what happens is you would have a plt.show that's all so that is why this this particular kind of a preamble you will find in most notebooks right so you know what i have done i've given you guys the sort of imports that i would highly encourage make it a template use something like this in your work. And another thing, Asif, I will basically, I don't think I don't want, I don't want to take a lot of the time because I did not even go through the whole thing, but this kind of small, small questions I will have, maybe I can put in Slack or somewhere, and maybe, you know, some Saturday session when you have something we can ask this, you know, it's, you know, we put in the template and then if someone asks us, you know, person something and what is this, that I just took it from a template is not a good answer. It's not a good answer. I need to know, we need to know like what are these things. See when you do a lot of analysis, you tend to be, you know, you develop some good best practices for yourself, what you consider good practices for yourself. And you tend to use it a lot, like for example, commented in COCO, and so on and so forth. So on the left, I've given as much explanation as I could. Maybe you could add it to this so that in future you'll have it. is that and anything else guys before I go to the housing data set any other questions okay so I'll go to the housing data set see guys we are all and again you notice that first thing I would like to show you is that I'll just scroll down casually and you will realize that once you read the river data set carefully, all of the analysis in the field of data sets will look at least somewhat familiar to you. This I hope looks familiar, the imports, with some minor variations, some couple of other things I might have imported. Loading the data. Does loading the data look exactly similar, guys? Familiar to you? Yeah. Sweet. Now I wanted to introduce you to something new, which will be sometimes, you know, if you notice that we have been, we have been doing a describe of the data and so forth. So one of the questions is, can we add a touch of style to it? Make it look prettier right it's a question that people ask you know that this is a notebook can i make the notebook look prettier and you by now you must have realized that i tend to put a bit of premium on it i tend to like things which are a little bit, you know, with a touch of style to it. And so you notice that what I have done is, and this at this moment, if you are familiar with HTML, this will look extremely easy to you. A table is made up of a table header, a table data, table rows and td. tds are for cells. So what I have done is I have added the standard CSS styles here, what you would call the CSS styles. So you can actually go in style. By now, data.describeall.transpose. You must be very familiar with the syntax from the river dataset. I'm just giving you a way to polish up and style your notebooks. Would you feel that this has a bit more style than the vanilla form as opposed to this? Compare the two. This versus this. Much better. Yeah. That is it. So that is the code. This is a question a lot of people ask. How do you make it look pretty? How can you make this table look pretty. So well, here it is. As if any record this Oh my goodness. Did I stop the recording. I apologize. Is thank you for reminding me actually Alright, so I'll go from Once again, quickly. So we're going to do the California dataset. Much of it is familiar territory. The one new thing that I introduced is how to style tables, any table. These are CSS properties. The beautiful thing is that the Pandas data frame has an integration with CSS styles. You can style the dataset and it looks different. It looks pretty, I suppose, than the vanilla version without any styling. Well, my styling is not particularly pretty, but if you are really good at CSS, you can make this table, I suppose, even prettier. Yeah, with all the cool colors. Yes, you can do it with all sorts of cool colors. I put salmon here, but you can do some different colors. Light blue or something. Yes. So it's not a Jupiter specific command, right? As if I mean, is this like a HTML table that the python is giving out with that command? Exactly. So it is emitting out a HTML table. See, one thing is there, Jupyter and Colab Colab is based on Jupyter, are pretty much the D-factor standards in data science. If you do, the one thing that you may graduate to is PyCharm. PyCharm has the support for notebooks, this style notebooks, Jupyter style notebooks. So all of them will respect this, this HTML output and so forth. They will all respect it. So now, moving forward, one of the things you do is, this is a new thing I'm introducing you to. See, when you get data, guys, you don't know that data is clean you should assume that real-life data is not clean this is a real-life data so a little bit of a background on the California housing data see in California one of the perpetual obsession especially in Silicon Valley's are the house prices going up or down? Because most Americans and certainly most people in California, their biggest asset or their principal value or retirement money is often stored in the house. It is the house itself. So as the market goes up and down, the individual people, they see their fortunes go up and down. And people worry if the house price is crashing, people get very excited if the house prices are going up, unless you're on the fence wanting to buy a house. If you want to buy a house, of course, you'll have a crash. But the moment you buy a house, you want the house prices to boom. So a real data set from the 1990s, I think 1990 data set, of California. You see how the house prices were then. Now 1990 is just 2025. I think it's 1990 or 1995. I forget. This 1990 or 1995. I forget. This data was gathered. And what looked like an expensive house then will look like a, I mean, for the same money today, you could probably buy yourself a shed or something like that. California houses, the prices have really gone up. So this is not a time series data, temporal flow. It is very specifically a snapshot in time data for California. What were the house prices then? This dataset is available in Kaggle. I must have taken it. I'm guessing I took it from here. It is a dataset that is published actually. And it is also mentioned in this book that I recommended you all to read. So it is the same in your textbook, the Python one. This is discussed. So you can go back and read a chapter there and see. And actually I have not, I'm embarrassed to say that I haven't really looked at how he has done it. Perhaps he has done some things better better you can go and tell me about it so well this is a command what it does is it finds if data is missing in some of the rows and columns it is real life whenever you gather practical data you will find that will find that some rows will not have all the values present. They will be missing values. It's the nature of it. So suppose you're taking the weather, your temperature, pressure, whatever it is, wind. And let's say that the guy is doing it manually. Just forget modern instruments for a moment. So you can easily imagine that if it is too rainy, the guy won't go out and take all the measurements he'll just run back or some measurements may be wrong somebody may delete it and so forth. So things like that sometimes you don't have all the data for all the rows all the feature values for all the rows. So you need to inspect that first unlike the river data set of flag data set which were didactic data and so there were no missing values. In real life data you will get noise, you will get imperfections. So you need to acknowledge those imperfections and study it. First thing is just as an exploratory data analysis let let's look up here. What are the columns we have? Longitude, latitude. It gives you the coordinates of a house. Actually, it's not at the level of a house, but it's at the level of a city block, but it gives you the coordinates of a city block. House, excuse me, median age. As the name suggests, it gives you the median age of the houses in that block. So some house may be very recent, some may be older, but generally in a block in US, you know, most of us live in row homes, right? the entire neighborhood sort of comes off the ground at about the same time. So median house price, median age is a measure of what the middle age is. Sometimes houses have a huge variation, new and old. Total number of rooms in the house again is a pretty good measure, worth looking at. Now it looks very odd, the total number of rooms is minimum is two and maximum seems to be a huge number. Why is this possible? Because it's not the number of rooms in a house, it's the number of rooms in a block. And if the block has a lot of apartments, high-rise apartments and so forth, tenement housing, you may has a lot of apartments high-rise apartments and so forth tenement housing you may have a lot of rooms this is the total number of bedrooms available typically number of bedrooms is a pretty good estimate of the number of people living there right because you see the number of households is also too many. That's right. Yeah. That's how you configure them. Yep. Right. Households. And yeah, so you can see that and then you have the households, population, median income, like you know, what is the median income of people living in that block. And the target variable is we want to use all this data to predict the value of a house what would be the value the selling price of the or the estimated value of the house median value in the whole block city block this ocean proximity is important as you guys know in california it is pretty expensive to live on the beach right we have a lot of coastline but the coastlines tend to be more expensive than in life inland and so ocean proximity matters if you live on the island for example catalina island or somewhere it could be quite quite expensive uh it's worth looking at so those are the features. Just looking at this value, can you tell which of them is a categorical? Portion proximity. Right. So next thing you do, you look at the missing values. In missing values, if you see a white horizontal line, it means data is is missing so which of these features has some rows in which there is missing data guys for bedrooms bedrooms is one and if there isn't something else it's barely visible right uh how many rows of data there are and so on and so forth it's a measure then so it turns out that there is a total bedroom and total bedrooms that's the only thing where any data is missing right and if you have total bedrooms missing then sometimes it affects all of these that's about it doesn't mean much now i taught you guys about this thing what was it about profile the pandas profiling so you can use the pandas profiling good thing with pandas profiling is some of the things that i did by hand for example missing value analysis in an abbreviated form it does it so when you look through this pandasarnes profiling is pretty good. It established that most of these are real valued, but it was smart enough to indicate that ocean proximity is categorical and there are five values. What are these? You are less than an hour from the ocean, you're inland, you're near the ocean, near the bay or you're in an island. So there are five homes or five rows of blocks that seem to be on the island so which is the island in california alcatraz alcatraz is one certainly yes but i don't think any i don't know if anyone lives there catalina catalina exactly yeah so then you can look at the interaction, for example, how does longitude and median income latitude and median income vary. Clearly you notice that latitude. Asif, can you please quickly recollect interaction? Is that collinearity? No, it just is showing you some sort of a correlation between… Correlation only, okay. That's correlation. Because there is correlation below, so I'm not… That's right. So from this, it may not be very obvious, but you can see that San Francisco Bay Area is filled with people with a certain income value and then there are people in Los Angeles who have that income if you go longitude now you see that also it's a little harder to see longitude wise but at certain longitudes are the most medium income here, median income here, longitude and median income. Again, you see this value that Bay Area, so the two big income areas are Bay Area and Los Angeles. You can sort of see that. This is the correlation, you can see correlations are there. Very prettily it draws out the correlations. the correlations right and if you want to understand what these different correlations are you remember that I taught you guys Pearson correlation all you have to do is toggle this and it will explain to you what these correlations are kindle towering so forth if you want me to explain it I'll be happy anytime to do it in Saturday ask me but today I want to move a little bit faster so you can see that it gives you some sort of a missing value analysis. Pretty much it says that none of the rows are missing except for, well, why is, did it not show total bedrooms? Well, this does not look right. Okay. It gives you a few sample rows, last row, this is it. Then let us, so now I want to introduce you to some techniques. One of the things is when you get geospatial data, it's always a pleasure to visualize the data, right? And to visualize the data is actually very easy. Do you notice what do I do? Let's go through this code. I got the minimum and maximum longitude of the latitude of the the longitudes minimum and maximum latitudes using this what can I do I can find the center of the map isn't it yeah yeah it does there's this light these three lines make sense it's very simple all I'm trying to do is the center so what I will do now is I will create a Yeah. Yeah. Does this line, these three lines make sense? It's very simple. All I'm trying to do is the center. So what I will do now is I will create a map and Folium is an excellent library to create maps. You see in one line, I'm creating a map. I'm just giving it the central location, longitude, latitude, and I'm giving it the zoom level. You can start out with the zoom level. Of course you can zoom out. So if you notice that with my mouse, I'm zooming out. This is interactive map, these are live maps. It's a pleasure to actually be able to create it. You realize that with just a line of code, we are able to create it, one line. And on that map, which will be an empty map, now I need to project our points. Each of these points, I projected it. Our data points, I projected it. Longitude. That color has to be like in hex code? No, you can put whatever color you want. Okay. You can put red, blue, green, whatever. I must have been fooling around with something. I like this color. Though these days with coronavirus, this color is a dangerous color, right? Things are not going well. Anyway, so this is it guys. Do you see how nice it is how quickly you can create maps in um in in data so you know this is these are all the tools of data science people use it is good to know how to make maps if you have geospatial data right but you don't need to make maps you can just draw a simple scatter plot if you just did a scatter plot of the data draw a simple scatter plot. If you just did a scatter plot of the data, you would realize that you would get exactly the same thing as if I just made the scatter plot of points. And let's look at the scatter plot. I'm making a scatter plot x and y are the latitude longitude. I'm coloring it based on the median house value, which means that if the value of the house is high, it will be more red. If the value of the house is low, it will be yellow. So yellow, orange, red. Color maps again, let me remind you what color maps are. Whenever you create scatter plots or plots in your code, you have to be careful of multiple aspects. One is the aesthetics. Together they should look pretty. They shouldn't be jarring to the eye. The second is disability. A lot of people are color blind. So people have sat down and they have created this color map of good color, the color, you know, the color ranges, which fulfill both the needs. There are enough contrasts, they have the color blindness sensitive, at the same time they give you the aesthetics and the beauty. So this is nothing but a scatter map, scatter plot. And yet, and then the size of each point is how much population there is and the color is how expensive the house is so if you look at this map can you see guys that places so deep red is expensive places they also tend to be high population centers isn't it which makes sense cities are densely populated and the median home values is very high there. Is it making sense, guys? We all know about homes, we all know cities, so it makes sense. Can you tell where is San Francisco and where is Los Angeles in this? San Francisco, minus 1.2. Yeah, that's right. You can literally see, see, this is not a map, this is just a scatterplot of data. And yet you can see a lot here, you can see the San Francisco area, you can see a little bit of the Santa Barbara area, you can see the Los Angeles area here, and the San Diego area. Not only that, you can also see Interstate 5 run through it. Do you see it guys effectively? This line here, those of you who have been now let's correlate it to the map and see whether we are right. So look here guys, do you see this? Yeah, right, all the cities are along this. All the cities are along this. And actually it's not, I don't know if it is interstate five. I think it is. Yeah. But yeah, there you go. I think there's also a California route that goes through this, that may be more through the cities. I think five tries to avoid the cities i think five tries to avoid the cities you can overlay the map and the scatter plot right yeah yeah you can do that and you can do you can do so anyway i give you some at this moment i was just trying to give simple code so you see this is your standard scatter plot i've just prettified it a little bit yeah see on the right hand side that you know 100,000, 200,000 that is basically your C. Median house value. So in those days in 1990 perhaps, half a million used to be the cost of houses, very, very expensive homes in Los Angeles and San Francisco. Today, what do you think 500,000 will buy you? Those areas. You have to go to Tracy. Yes, that's right. I mean, you need to literally add a zero there. It's close to 5 million in those places. I was just looking at the house prices in Atherton. I think they all are six, seven million and so forth. So, by the way, any rich participant here who lives in Atherton or something like that? So, all right, so I put this side by side now, huh guys? You can literally see the scatterplot in this map. They look so close to each other, isn't it? Now, the other one thing I realized that the ocean proximity is a categorical variable. So I'm explicitly changing it to categorical. Instead of a string. It's just a good practice to do that. Any surprises so far, guys? Anything that needs explanation? Does this statement look very easy? These two things. By now, is it looking very straightforward and easy, guys? It is a categorical, so I'm making it a categorical. Yeah. Yeah. Then this is the describe. By now now you must be familiar with it we are describing it then what do i do i i draw histograms of each of the variables when i draw the histograms can i just shrink the font size a little bit so i want to see the histogram in one single shot yeah so what is the histogram in histogram you can choose the number of bins what does alpha 0.4 mean guys who would like to enlighten me what does the alpha stand for is it like how transparent it is exactly transparency transparent it is exactly the transparency level how saturated it is alpha one means completely saturated opaque alpha of zero means completely transparent invisible so 0.4 why did i do that by the way guys why did i not have it why did i if i don't put alpha what will happen one yeah these will be very saturated colors it adds aesthetics you know hard style then now what why did I do this X rotation is 90 degrees Why did I do that? Can you tell me what it is doing? X-axis is, you know, like the numbers are rotated 90 degrees where you can read it well. Yeah. And if you don't do that, then it will overlap. Yeah, otherwise never. Exactly. Great. And figure size 2020 means I'm giving it a rectangular shape. Just enough big. You can decide what your figure size is. 2020 here was good. Maybe I could have made it a little bit bigger. Now, the pairs plot is interesting. This is, again, those of you who did ML100 with me now, what pair plots it. It is nothing but the scatter plot of two variables taken at a time so the way to read it is a little more complicated let's take this what is the relationship of longitude to population do you notice that there are two humps here can you explain why there are two humps? Why is this peaked at two places? San Francisco and California. San Francisco and the LA area. Yeah, the bimodal. Bimodal distribution. California is bimodal in most of the things. So it's quite interesting that most people of California live in these two cities and the rest of California is empty. People often say California is very expensive, very expensive. Actually, if you go away from these two population centres, California home prices can be quite, quite reasonable. Yeah. Yes. So in case you have an urge to run away to the middle of nowhere, or maybe in the middle of Arizona or something, remember you don't need to run that far. There might be, people might do that if this trend continues like this, people might even do that. Yes. continues like this people might even do that yes so you can see once again near the sea how often are people this is a box this is a color category plot given latitudes how much of values there are once again you see that there people are They're people of sort of distributed all along inland at certain inland. People are at all sorts of latitudes. Near the ocean people are at they're closer to this latitude, the 35 degree latitude, which is closer most likely to San Francisco area. I mean, the Los Angeles area. Near the bay is of course all San Francisco population and so forth. It's just a dish. So these are the plots guys I'm introducing you to in case you had not introduced to that. What I would say is as you study this notebook and those of you who did ML100 of course have gone through this exercise at a more leisurely pace but those of you who joined ML200 directly these are the landmark or that's stereotypical or ubiquitous plots that you make in exploratory data analysis so become familiar with the syntax in the, it will all feel rather magical that you're invoking some magic mantra or some obscure shlokas and something is happening. But gradually this will all become very real. It will begin to look very easy after some time, because just standard Python code. If you want, I can explain any one of these code to you. python code if you want i can explain any one of these code to you uh like for example here do you agree that this one line is self-explanatory it is saying go make histogram do bins are 50 what is the bins is equal to 50 mean 50 buckets exactly 15. so when you make histogram of a category of a numerical variable you have to put values into bins because you're counting how many values fall in each bin. You take 50 bins, alpha is this we already talked about, figure size we talked about. So there's no magic to any one of these. Likewise, the pair plot. This is a bit more going on here, but still it's four line. I'll explain it to you you make pair plots between this alpha size is 80 all of those are self-explanatory then you say that along the diagonal make the histograms do you see so these histograms are the same as the histograms here then the next thing it says is that what is that now what is the color in the histogram that I give you is the ocean proximity how far you are from the ocean that's the coloration then in the bottom upper diagonal on the upper side of the diagonal the pair should be scatterplot and in the lower part there should be something called a kernel density estimators. Those are sort of, think of it as some, at this moment, KDs will deal with them much better, but think of them as, these are well probability density sort of thing, but think of them as some sort of a heat map at this moment. A very rough idea would be heat map, but we'll talk about this and learn about this in more detail when we do the workshop, the boot camps. Are those numbers, like a table of numbers? In between? No, no, no. These are just the legends. This picture is very dense. See the thing is, we made this picture of size, this size. When you have so many variables, you want to blow up the size of the picture so that everything is well separated and you can see everything. So you would make this picture to be more like 100 by 100 and put it on the wall. Then all the things will show up properly. But it also shows what happens when you have too many variables, that you have to, you cannot put everything in a single visualization. This is already beginning to push the limits, isn't it? Imagine that there were 30 variables, but then the pair plot would be horrendous. So what you do is you make it in pieces, you pick a few important ones and you make the pair plots and there is nothing like you have to see all of them in one place right i mean you could when you're analyzing going deeper you can always go one by one right it's so it's okay and then make scatter plots absolutely yeah you can do that and And now, these are called beautiful. If they look pretty, these are called violin plots. They literally give you the box plots, but violins are a little bit prettier than box plots. They give you counts, like median household age, how many homes, and this is why, remember that ocean proximity is the categorical, right? on the color so there's value so this household median age you can pretty much see that the median age in this is the ocean Catalina is pretty old homes are there you don't have new homes most likely they don't allow new homes to be built it's an island they must have very strict regulations right so this is it and the same data i just show you that you can do it as a regular box plot see with all of these things and when your present data guys, people underestimate the value of clean visualizations. Visualizations if you clean it up, it sort of creates a bit of aesthetics. I haven't done much you can do much better than this. But for what it is, now all of these things that I'm doing box plot, category plot, violin plot, none of them are hard. These are all features there. You have to just go use it. You have to know that those things exist and you have to go use it. Account plot, how many homes are there? So most homes it turns out are less than one hour from the ocean in California, which makes sense. California is a long and narrow state. Most homes are inland. This is probably the Central Valley. Maybe there are, but not that many. Then near the bay, near the ocean, this is it. Now correlation. You realize that any two variables could be correlated. It shows you the degree of correlation. Now we already saw that in profiling in the Pandas profile. So I won't go more into that. This is the... Oh by the way, here I wanted to show you something. Quite often, you know, when you write documents you want to write it in LaTeX. People often ask how do I put the output into often ask how do I put the output into a research paper that I'm writing something. So it's very easy. Any data frame in pandas all you have to do is do to LaTeX. Now those of you who know what LaTeX is would find this magic mantra very easy to read. If you don't know LaTeX, don't worry about it. LaTeX is a language in which in the mathematical communities of computer science and math and physics, et cetera, most papers are published. So this is the syntax for that. Now the same correlations, we show it as a heat map here. We're looking what the high correlated are. So I'll let you ponder over it age is median age is highly correlated negatively correlated with uh let us say with many things uh total number of rooms does that make sense guys median age is negatively correlated with total number of rooms and total number of bedrooms. What does it say? New houses are smaller. No the opposite. The bigger the house, the older the house, the smaller, the lesser the number of rooms. Yeah yeah actually if you go to the older house you will see the bedrooms were much larger. Right. Yeah, actually, if you go to the older house, you will see the bedrooms are much larger. Right. So the number of bedrooms were small as a result. Yeah. But these days, if you go to the houses, you will see bedrooms are very, I mean, they like to, because the, you know, the price is based on number of bedrooms. Right. So they increase the number of bedrooms. Increase the number of bedrooms and tiny bedrooms. Okay, guys. So that was it. So now let's come to regression. I taught you guys decision tree and the random forest. Let's see if we can do that. A very simple, if you read the river data set, you'll find this easy to understand. So we need to do some data pre-processing. First pre-processing we need to do is that we need to do one hot encoding of the data. What does that mean? One of the predictors was ocean proximity. It's a categorical variable, right? So we need to convert it into a numerical variable. And the way you do that is you take all its values. So, for example, are you within one hour of the ocean? Yes, no. Are you within proximity? Are you inland? Yes, no. Right? within one hour of the ocean yes no are you within proximity are you within are you inland yes no right are you you know so on and so forth are you on an island yes no are you near the bay yes no are you near the ocean yes no so what have you done you burst out you have burst out all of these into this. Different columns, different features. This is called one-hot encoding. So you can do one-hot encoding of the data. Once you do that, the other thing that I'm doing is I'm dropping the... All the rows that don't exist. Is that a good practice? Actually, you have the luxury to delete or drop rows that are defective only when you have a lot of data. Here we have a lot of data, 20,000 rows are there, like 20 plus thousand rows are there. But if the data is sparse, you don't do that. You do something called imputation. You try to fill in those values and keep those rows and use them anyway. It's like when your car has a nail in the tire, there are only two things you can do. If you're rich, you go and change the tire immediately. Well, if you're like you and me, then I suppose you'll want to take the nail out. Unless the tire is old, you would like to have it patched. So it's like that. Each of your row of data, you would like to patch it up by imputing some value, filling in those gaps with some value. The imputation is a topic. Usually these little things or practices we do in the bootcamp, we don't. So, but here, we just delete it gradually as we make progress. I'll teach you guys imputation. Then we impute it. Now notice one thing, guys. This data. Do you see skew in the data? Is the data symmetric or you see a lot of skew in the data, guys? Skewed. Yeah, the data has a lot of skew there. One more thing you notice that this particular thing, household median age 50, 50 plus. At 50, they have sort of clubbed all data that are 50 or plus into 50 plus category. So you have to watch out. It will screw your analysis, and you'll see how it screws your analysis after a little bit. I leave this as an exercise for you to deal with this part. One way you can do it is not take the data, 50 plus, take it out, and so forth. But we'll deal with it. This data is skewed. Is it right skewed or left skewed? Remember the trick guys, if you think of it as the beak of a duck. Right skewed. Right skewed. The beak is looking this way, right? The duck is looking this way. So it is right skewed. All of these are right skewed. Some of them are very strongly right skewed right and median house value the target variable also seems to be right skew what can you do about it guys how can i take care of this cube well you could do the whole power transform and find the right transformation to normalize it but a cheap way is just take the log of these this one and this one and this one and this one whatever you think needs a thing you can do as a pre-processing you can take a log if you take a log it's a trick and again i'm talking about things that at one point i've taught in depth to you guys but suppose i take the log of it that is the log transform by the way this whole thing most people don't pay attention to and you will see what happens if you don't do this your r squared value will be about six to ten percent less right but if you are careful and you do this and i invite you to do this guys comment out this code run the analysis see what your uh what your model actually what your model metrics are performances and then do this and see your model performance it is quite interesting actually that most of the notebooks that i saw of california dataset which have been posted in Kaggle and other places, they did not do these transformations. If you are using, it is always good to do that feature extraction or data preparation carefully. When you do it, you get the extra bit of accuracy, the extra bit of performance, not actually, extra bit of performance you will get. extra bit of performance you will get so now when you do the transformation do these things look much more normal guys are they better right this queue is gone right so now that it is gone what have I done so far I have massage a pre process the data the data pre-processing after the data pre-processing then i go and so what were the things that i did in the pre-processing i did let us summarize what all things i did i mentioned it here convert the categorical to one hot encoding drop the missing values extract the feature space and the target x y right all three things then finally I also do the log transform because I look at that so this is the value of drawing those histograms guys they're not just one thing you do as a ritual you use it to make some decisions about data and once you do that you now take this and use does this look totally uh easy to understand guys what am i doing and then i'm going to try a slew of models on it first i'll do a simple linear regression okay when you do a simple linear regression, do you remember this lines of code guys? This is straight from your river dataset. You just see the linear regression fitted. But now you notice that, things are beginning to fall into a repetitive pattern here. It's more about thinking the code begins to look very easy and familiar. It's exactly the same code. When I run this code code i get a mean squared error which is a fairly low actually and i get a coefficient of determination which is 69 69 r square for this data is actually i would consider fairly impressive if you had removed the outliers it would it would become even. I leave that as an exercise for you. So one basic thing is that outliers, there's not enough data, not enough information to make a judgment call for them. So you could do one thing, you can remove the outliers and say, I'll make a separate model for outliers, something else, and let me make a better model for the inliners, inlineline data data that is not outliers let me leave that as an exercise for you guys so i say where do i see that outlier information in the graph oh it is uh all here see if i look at this graph here look at the histogram do you do you notice that all these outliers are there in the box in the box whiskers plot right so anything beyond the whiskers do you see the this is the box these are the whiskers anything beyond the whiskers are outliers so latitude has outliers well that's not harmful but a total number of rooms total number of rooms has huge outliers right do? Do you see that? The main box plot is here and then there is a long tail of outliers, total number of rooms, total number of bedrooms, population, they have outliers. Households have outliers. And let us also go and income has outliers, right? So let us go and verify whether that shows up in the original plot do you notice that this tail soft but it goes to 6000 why does this box plot why does this histogram go all the way to 6000 we can't see it but there must be some one or two outlier values here isn't it in all of these there are some outlier values sitting here like for example total number of rooms 15 right hang on households let's look at the household right in some city block their massive number of households in certain median income some people seem to have a fairly high median income even after you know pretty high median income i i'm assuming that this isn't thousands or something like that but whatever it is look at the population it goes all the way up to here some blocks are really heavily populated so the data is filled with outliers. Do you see that? Guys, do you see that? It is a clue, like you ask yourself, why is this histogram going all the way till here? It means that there is data that I don't see. So if you chop off the outliers guys, your analysis will become better. And I leave that as an exercise for you it's also one of the things people forget to do actually in fact it was just uh in the break i was looking at the california housing and i was looking at some of the notebooks and what people have done even the more celebrated notebooks uh yeah, the kernels that people have been, by the way, you can learn, I highly encourage you to go and read some of the highly voted kernels that people have created and see what things they did that you missed because you can learn from it. At the same time, you can take pride that you did something more carefully that a lot of them missed so you you know this is the way for you to get a reference of how good you are yet these people certainly think they have done a great job which is why they have posted their notebooks here right so go check it out guys you'll be pleasantly surprised that by the time this workshop ends you would realize that your notebooks look better than most of the notebooks there at kegel anil do you agree uh yes yeah begins to get better and so forth all right so we are making a linear regression model and we achieve an r square of 64 6 about 70 percent or 69 points up let's say give or take at this moment guys remember what did i teach you in ml100 it is not enough to know that you have you're getting a decent r squared or a decent mean squared error what you need to do is what you need to be able to do a residual analysis and see all is well so you begin to see able to do a residual analysis and see all is well. So you begin to see that this is your residual analysis. When you look at it, first thing you notice on this side, do the errors have a bell shape? Do the residuals have a bell Yes. Yes, they do. Yes. Yeah. Here, there is a little bit of a pattern here. By the way, ignore this. This is coming from the limitations in the data itself. Because remember, there was a hard cutoff in the data in one of the fields. This is where this is coming from. You can ignore that. But it is mostly homos, there is mostly a sense of homoscedasticity, except for these outliers are screwing it up. They seem to be there more prominent in the lower predicted values than in the high predicted values. So if you remove the outliers it would get better so it is not perfect but it is pretty good after all we are using a linear model for a very complex data set what is the relationship between prediction and reality guys this is the relationship between prediction and reality does it make sense is it a positive or negative correlation Is it a positive or negative correlation? Positively correlated, isn't it? Why prediction and reality are positively correlated? They're along the principal diagonal. So you can see the deviation. See, a perfection would be if they were along this gray dotted line, and they are along this dotted line. Well, not quite perfection, but pretty good. If you're searching for points that can be of high leverage high influence it gives you a sense of high infant points nothing pathological all looks good now this was your linear model guys do you notice that just a good application of a linear model took you this far by the way if you had not done all of those log transformations etc., your numbers would not be so good. They would be much worse with a linear model. Try that out. So 69.1. Now you expect, you ask yourself, if I had regularized the data, would I get a better answer? It turns out regularization does what it suppresses overfitting i haven't uh obviously i apologize for those of you have just joined uh overfitting means you know the data has high variance overfitting there is a method to suppress it and you suppress it by different degrees of suppression and see which one is the best and then you whichever is the best you use it ah it has gone to 69.12 well 69.11 versus 69.12 pretty much the same if you look at the coefficients of the model let's look at the first three coefficients uh point one sixty five point one sixty four 165.164, do they sort of match the previous one? 0.165, 164, then 003 and 19, minus 19. So, 003 minus, so you realize that the coefficients are almost all the same. It means that the data does not have, your linear model does not have overfitting, right? And it sort of makes sense. Linear models tend to have underfitting problems, not generally overfitting. So well, that is what it is. If I had made it a polynomial model, so I leave this as an exercise, guys. See, somewhere, I taught you polynomial regression. See if you can extend this and make it a polynomial model. Be ready that it will be a big computation because even if you go to quadratic form, 11 square is close to 110 combinations, right? So you will have an explosion in the number of variables that will show up. C, whether it gives you anything. But when you do a polynomial, you must use ridge regression or LASA, whatever it is, to suppress that. Better is actually use elastic net. It's a combination of both. Can I leave that as an exercise, a homework for you guys to try? Let's do that. Then we talk about decision trees now because we learn decision trees the week previous week and random forest this week let's look at the code for decision trees guys I trust by now this code looks obvious I think I build a regressor I give it the data to fit and then I use the data to predict the labels. So far so good, guys? Yeah. Right? These are your predictions. Labels are your predictions. So people use Y hat or labels, whatever you like. Labels is more typically for classifiers. I tend to use Y hat. I don't know here. I wasn't being too careful. So I just used the word labels. Any variable. So there we go and what is the r square I get from a decision tree 67.72 is it better or worse than linear regression worse worse actually and look at the complexity of the model this tree built so it is illustration of what principle Something about lunch that I keep talking about. Anybody can remind me, what do I say? Free lunch. There are no free lunches. NFL, right? Exactly. It's an NFL theorem. No one algorithm is in quotes powerful, more powerful. Every algorithm has its place under the sun. Sometimes one works better, sometimes others. So don't let anybody tell you that decision trees are more powerful than linear regression or something. Linear regression is too basic. It was a point worth emphasizing so I'm mentioning it to you. You can visualize this, by the way, this is hopeless. The built-in visualization from Python this kick it learn is not terribly illustrative that for visualization of graphs and trees graph is as the best if you use that and I can do show you what that becomes let me see that becomes That becomes Okay. Oh, I think I removed the could show up this. I didn't want to run this now, it will take a bit of time. This is our, if I use yellow brick, the same thing, do I now see a pattern? Does this look much worse than the linear regression plot guys? It does, right? It doesn't look as nice as this. It is actually a little bit worse. But good news is that I still have a more or less normal distribution of errors. Here you can see that this line is a little bit more off the diagonal than the previous one so let's try a luck with an ensemble method now remember i told you that ensemble methods can be more powerful let's try random forest the forest is just made up of trees i gained a thousand trees uh 20 well parallelization is 20. it executed in 5.79 seconds 20 is something you guys can probably run on your laptops if you have multi-core i typically run it with 100 it runs faster but it doesn't matter so here we go you go and do the predict again why not this code guys i hope it's beginning to look very easy do you notice that between the decision tree regressor or linear regression all I had to do is change this word the algorithm name it has become random forest regression I apply it get the labels get the predictions and what do I get ah I get an R square of 83.6 is it better than linear or worse than linear better than both better than both isn't it that was 69 and about 70 percent and this is close to 84 percent so for this problem the random forest is certainly the the best of the three algorithms that we have tried. Isn't it guys? This is what people tend to miss, they tend not to do the residual plot. But if you look at it, isn't the residual also looking much better now here? There's far more plasticity. The residuals do have a normal shape, normal Gaussian distribution. Much better. So here obviously the random forest is the winner. Remember that in the river data set the careful hand generated feature extractions were the winner. Here there are too many variables and it's harder to do feature extraction but there are feature extractions you can think of. So let me tell you and by the way one of the Kaggle notebooks did it brilliantly some fellow has done it he said that one good feature would be or she said the one good feature would be the distance to the metropolis nearest metropolis so distance to Bay Area or distance to Los Angeles whichever is smaller and he made that a predictor and he came up with a beautiful visualization. Look up his notebook will be the classic I wish I could have found an open and showed it to you. I leave this as exercise for you to find out. So you can do a lot of further feature extractions. You can make it out. And the other thing you can do is join this data with other data. For example, you can join it with educational level. Add at any given latitude and longitude or city block. What is the educational level? You would realize that educational level has a huge influence on the home prices. Birds of a feather flock together, right? So you can go on creating more and more features by joining this data with other data sets. And that is where the magic is. Most of the time, if you're given data, you get more data, not by getting more rows of data, but by getting more features into your data set somehow. You manufacture features or you take data that is not there by taking a view of labor statistics data or something like that, economics data, weather data, something or the other, and you bring it in and see how it goes. School rating. Go ahead, Abhijeet. No, I was just saying school rating. School rating, yes. Yes, yes, excellent excellent school rating is a huge determinant yeah so we can get like the zip code and then based on the zip code we can get the school yeah exactly we can get the overall school rating right in california the schools are failing so it's like almost an arms race whichever few school districts are good everybody wants to go there and the house price just skyrockets in my own case i have a illustration i have a house which is much more modern bigger nicer in vallejo hills and i have a house here in fremont as you know right uh proximity matters but also the fact that the school district in fremont is superior means the house price is practically three times over for a much worse house actually much older house yeah so it's an illustration of that so anyway when you visualize so one of the things i did is a forest is made up of trees so you might be curious let me look at into one of the things I did is a forest is made up of trees. So you might be curious, let me look at into one of the trees. So here it is inside the forest. There are lots of trees. They call estimator underscore so member variables are always hidden with an underscore. Now there's a convention, the output variables have an underscore here at the end and here at the end and there in uh skicketland so i just took the first tree no reason why i should take the first tree could take any one tree but do you see how complicated it looks guys do you think there is any interpretability here there is so i say what is it exactly swing so So one tree means what is it exactly swinging? Oh, look at this. There is this root node, splitting into two nodes, splitting into two. Oh, okay. Oh, they have shown the trees like okay, now I get it. Okay. It is practically indecipherable. Okay. So that's how it is. Some pruning is needed. Yes. So one of the things I introduce you to today is a very rough measure. You may say, all right, what factors are really important? What really, which factor is more important than other in determining a house price? This is a 1990s. Let's see if the results agree with our intuition. Feature importance. Now, by the way, remember, guys, that this is a very rough and ready measure. Usually, feature importance is more local phenomenon. In certain regions of the feature space, certain factors may be more important, which is why this feature importance, which people tout as a big virtue of decision trees and random forest, obviously it's worth knowing and I'm explaining it to you, but don't take it with a huge grain of salt. Remember that's a very rough approximation. There are better means of feature importance and explainability and those are the shapely values and and making local approximate models and so on and so forth we'll come to that line uh in the boot camp of course there's a few attended you know that we covered a whole day to that uh for shapely and by the way those are non-trivial things shapely got the nobel prize in economics for economics for one statement effectively, one big statement, how much value to attribute to a player in a team. If you think of each of these features as player in the team that is together getting a sort of a home value, high home value or whatever then how much credit should go to each feature that is shapely value we'll talk about all of those of course in the boot camp uh or maybe next time uh see there's a this by now you must have realized that this is a lot to digest are you guys feeling that that we had a long yeah yeah it was very overwhelming today yes so i don't want to bring in even more so i'll leave it at the feature value so at least for me i don't know about others so yeah so let's keep it to that and see does it make sense so the biggest determiner of home value seems to be the median income of people living there right well that doesn't seem to be very uh useful because obviously expensive homes can be afforded by rich people isn't it the home like our income determines the house we buy right the banks will tell you that you can't spend more than 30 percent of your income as mortgage payments it's the upper bound it's a healthy bound. After that you get into dangerous territory. So obviously you expect a median income to be very crucially related to the target variable which is the home value. It determines the home you can afford. The second is and so people of, if a house is priced at 1 million, you can ask yourself, what is the mortgage on 1 million, right? And let us say these days interest rate is what, 4%, right? So 40,000, that comes to about $3,200 a month means your income has to be a hundred thousand or a hundred thousand plus you know 120 000 roughly speaking for it to be 30 percent of your i mean 40 right 40 times 3 is 120. so your income better be 120 to 135 for you to be able to afford that million dollar house minimum isn't it do you see guys how the computation goes? So this sort of makes sense. I mean, I don't know, maybe, by the way, is my explanation right? I made a very rough and ready over the top of my head computation. Hopefully. Okay. That's that. So the second is ocean proximity inland. Are you inland or not? We know that that is true. The more inland you are, the less the house price. Would you agree that that's true for California, guys? Broadly. Then the next factor seems to be latitude. Very true. What does latitude and longitude speak about? Whether you're in the metropolis or not, Bay Area or LA area or not isn't it guys yeah location matters and anybody doing real estate will tell you so what do these things see median income is more of a chicken-and-egg situation expensive homes are afforded by people who can afford them so I would sort of discount that but if you look at these three values, ocean proximity, latitude, longitude, what are they saying? They're speaking to the truism that any real estate agent will tell you. So they have a saying, a cliche, they say that there are only three factors that matter in a house for a house price. It's location, location, location, and location. Yeah. Right. So those are all the three facts. And you see that said in so many ways out here. Oh goodness. There is a call. So guys, I need to go attend a meeting now. So I will stop here. This is the last one. The other factors are there, so on and so forth. Median age of the house matters, population, number of rooms matters, and so on and so forth. I apologize, guys. I have a meeting waiting tonight with India. I need to get into that. I will stop here. So this one you are going to post, right? I have to post all of it. So I'll put it to Slack. Again, guys, do not