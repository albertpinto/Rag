 Okay, so in this, this is exploratory data analysis. What does that mean? We have been talking about that. It is looking at what the data is saying, not making predictions, not searching for hidden patterns, but seeing what is quite literally there in the data. And if you summarize the data, visualize it, arrange it properly, you can see quite a bit of information in there. It is remarkable how many things you can see just by looking at the data and that is where the history of this field is. Like, for instance, Nightingale's example and all these other examples are of people who just gathered data the right data and once they gathered the data it was simply a question of seeing the data and that is exploratory data analysis so this is why i quote galileo here at the top which and this code means look to me all truths are easy to understand once they are discovered the point is to discover them and that is exploratory data analysis this is the definition given by John Tukey perhaps the father of modern data analysis with his landmark book in 1973 or something like that and this is the definition that you get so anyway in this thing i've rearranged this thing a little bit i put the exploratory tools first there are two ways of doing data analysis by using each of the functions and so on and so forth and these days there are all these automated data exploration tools like for example the data explorer in r and tools like for example the data Explorer in R and the pandas profiling in pandas and if you are in the cloud environment each of the cloud environments are competing to create their own specific automated data exploration to be upload the data and then give you some basic exploration of it immediately. So this is it. All data exploration starts with a data frame. We start with the data frame. We manipulate all the things in the data frame. A data frame is a rectangle. It's rows of data each end with columns. You think of it as a spreadsheet, it's sort of like a spreadsheet. Now, data frame support comes built in with R, but in Pandas, in Python, you get it to the Pandas frame. Now, when you do that, you have a data. Now, what do you do? There are all sorts of ways you could be manipulating the data. After some time, people thought about it it and they said that see we need two things we need a standard representation of data which is called the tidy data it's a pretty important concept tidy data once you have data in a tidy format it's sort of you can apply a mini language to it with a specific small grammar and that mini language is the language of data manipulation. So people call it a grammar for data manipulation. It means you just have to know a few functions, a few commands, a few things to operations on the data and if you know those operations they usually call the verbs. With those verbs you can do any complicated thing by a judicious combination of things or sequencing of these. And some of these verbs are mentioned here, you select. What does select do? It just picks a few rows of the data I mean select columns of the data and people call it data projection and there is data restriction which is the filter then there is mutate mute it helps you add new features alter existing features you can add new columns to the data a range is a way typically example of arranges sorting you can rearrange new columns to the data. Arrange is a way, typically an example of arranging is sorting. You can rearrange the data based on certain criteria. Summarize the data gives you a summary, like for example the median of any of the columns and so on and so forth. Distinct obviously removes duplicate rows, counts the number of rows and from machine learning perspective and by the way today we are really going to get into machine learning now, sample and sample and these are ways to get fractions of the data, group by is to group by a certain feature. So these are basic operations then you have inner join, left outer, right outer join, intersections, and unions. Now, those of you who are familiar with SQL would realize that there's almost an exact parallel to the SQL grammar. All of these things are part of the data manipulation language, DML of SQL. And it is very great. Yes? Can you make the percentage is 75.5 so the text is really small to read at least for me can you make it 100% 100% should be okay I am perfect so these are the statements that you can use. And familiarity with SQL, those of you who are familiar with SQL know that this has almost a one-to-one mapping with what you do in SQL. Those of you who are used to functional programming, either in Java or Scala, would once again recognize that all of these verbs, these are essentially things very very close to functional programming. It's a, there you have the map, the, you know, the flat map, the reduce, the and summarizes like sort of the reduce and so forth. So count, you know, sort and so forth. So you know sort and so forth these are all things from there we close analogy to see clean order so that is that you do I have a question here so then when we deal with the data sets does these grammar the commands that we do to explore the data set varies depending on the sample size no not at all and that is the that's the point we are saying it is let me repeat what I said if you just learn this basic grammar any data set irrespective of its size its shape you can manipulate first thing you do you bring it to the so called tidy data format a standard format which is easy and I'll explain that and then once you have been then you just need to need know this very few words and you're fine now it is little you know in every language you have some basic words and once you have mastered the vocabulary the common vocabulary you can write you know a legal document you can write a novel you can write a technical document you can do anything the same is true here with data it's a very important concept bring it to a standard format and then once it is in standard format you have tamed the beast. After that, you can do whatever you want with just a few operations. It is one of the things actually that's not emphasized strongly actually in most of the machine learning books that there is a systematic way and if you approach it, it becomes very, very easy and very powerful and it is language independent whether you're doing it in R or you're doing it in Python or you're doing it in Julia or you're doing it in Scala Java doesn't matter it is the same essentially the same grammar in minor variation essentially the same grammar and you just need to bring data to a standard form that is for the tidy form. And then you'll be able to manipulate that. Okay, thank you. The only difference that would be that suppose you have data at vast scales, then you wouldn't create a Pandas data frame. What you would create is a so-called a PI spark data frame libraries which are more conducive to huge size data but the grammar is more or less the same yeah that that's what I was asking so okay the underlying tools might be very but what you're saying is the comments that we use to explore students stays the same exactly and so you need to do is master one thoroughly which is why i started this whole thing with data exploration and it is the first time by the way in the previous times i would just start straight with machine learning and what i realized is that as people did more and more advanced stuff, I would see that they could do the machine learning part, but they were all struggling with just manipulating the data, data wrangling. And so this time I decided that it's a very important thing and I shouldn't miss it. And that is why it is. That's why I'm here to write it down for you guys. So what is tidy data? Tidy data is actually a very intuitive way it says that each row should be one observation so for example let's take weather at any given hour on a given date in a given place let's say fremont you observe the temperature the wind speed the wind direction the pressure So these are different measures or different features of the weather. They all help quantify that one observation, weather observation. And so it should be there in one row of weather data. The columns of data should be of course the features, like for example when we talk of the weather, the features should be of course the features like for example when we talk of the weather the feature should be the you know temperature pressure humidity wind speed wind direction and so forth those would be the features of the weather intersection of a row and a column is of course a cell a cell is obviously what is the temperature on a given at a given hour on a given day at given city say Fremont. It would be a value let us say it's 57 degrees Fahrenheit. So that's what it would be. So it seems that what is the big deal? Isn't all data like that? the answer to that is actually data doesn't come like that data comes in all sorts of ways and it comes in so called white format as you will see in an example it comes in a lock in a format that we want guys one second I'm getting a call from support vectors custodian So, in this particular example, you'll see that the weather data did not come in that form. So, our first job is to put it in that form. This is particularly true, data comes nested, you know, as JSON. So it will come as hierarchical data. XML data can be hierarchical. Data comes in all sorts of formats. So your first task of data exploration is to put it in a tidy data format. to put it in a tidy data format. And it's important to do so. So do that. Once you have done that, then after that, dealing with it becomes much easier. So here I take the example of weather data. This weather data set, and here's the link. Guys, actually work out this labs. And I didn't release the solution so far, but I'm going to release the solutions today I will post the both the R and the Python notebooks Jupiter notebooks to our ml-100 so you can follow through but ideally try to do it on your own before looking at the solutions. So here data comes as you notice we are getting one two three four five six spreadsheets. This is very common. You want to do an analysis data you'll have to pick up data from many different sources. Usually it is not clean data somebody has given it in the format that you want. You take data from all of those and then you start your journey. First step is make it tidy in the format that you want. So what can you do? Let us first look at the data for any one thing. For example, let's look at the data that is there for in the city data what does it say this is the city name and tree longitude latitude very useful that is the city data then we look at that so is it in the tidy format if you look at this data guys would you say in a tidy format one data item is one row datum is one row and the columns are the features of that datum. Anyone? No, it's not in the tidy format. Why would you say so? Let's say take a city like San Francisco. Don't we have all the features of San Francisco in columns? The country, the latitude, the longitude. Not by country? No, no grouping by country. It's just looking at rows of data and asking is each row about one particular context? One particular... It is tidy. Yes, it is tidy. We would say that this is tidy data. The way to do that is to just read one row and say is it about one thing? Vancouver, Portland, San Francisco, one city. These are the features of the city. Which country the city belongs to? What is the latitude and longitude. In contrast if you look at the temperature data then you get a bit of a surprise. What you see is on a given day you are getting these cities in columns and for each column for each column you are getting a value which is the temperature on that day. If you're alarmed with 284 it is in Kelvins. So to make it centigrade just subtract 273 and then it will become centigrade and then of course you can convert it to Fahrenheit, maybe a good exercise to do that but more scientific data is in Kevin so this is it now would you call this tidy data for example for a given city on a given day is the data one row the context is at a given time in a given city, I'm getting the temperature. That should be one row. So would you say that it is in that format guys? No it's not. See look at it. There can be arbitrary many cities. What it means is that this table can be arbitrarily white it so happens that in this data that they only what 32 cities but in technical terms I mean if you think about it the number of cities can be arbitrarily large so it can be a very very wide data set and the context of San Francisco the same example if I want to know in San Francisco on a given day what is the temperature I will get it in a one row the row contains the temperature of all other cities also so it is a concept actually it takes some time to grasp guys. Wide but long. People who are very good with Excel for example they call it pivoting, pivoting the data. So a long format and wide format people often use this like because when you look at the Excel spreadsheet or when you look at data frame you can immediately get a hint of it because the the wide format data will be very wide and not so long whereas the long format will be very long but the number of columns will be relatively smaller. But is so the the way we say the above data set, it's not a tidy moment because it depends on what key you've selected to analyze the data, is it not? Yes. So the key here is the context. And let me use the word context. I want to know about San Francisco at this hour on this day. at this hour on this day so will that data will the data be there in one row because my context or my key is San Francisco at this hour today from that perspective it is not enough in the format that we would want. It is in a wide format. And that's a good point that you brought up. It's context dependent. And this guy is to do this so that you get a way to handle it. So what do we do? It turns out that you can do this data, you can convert it to long format quite literally. Where am I? One line. So in this line of code, I hope there's a way to minimize this. Let me take care of the multiplication. And move this aside. Good guys. So you take the raw temperature data, which is this data, and usually for these set of operations these libraries both are in Python funders they have excellent support you can convert from one to the other format quite literally with one line but you need to make yourself familiar with the syntax of the play around with a with a few examples, see if it becomes second nature for you to do that. What usually will happen is first time you'll read it, it will be confusing a few times, then it will begin to make sense, then gradually you'll write your syntax down in a notebook or some online notebook and keep referring to it and after a little while it becomes second nature. You know what to expect and you just use it so this is it in one time we convert it into ID data format you see that you take the raw temperature and you pivot it longer and what do you do? By city and temperature. The value in the cell has to be the temperature value, the column has to become the city, and date time has to be left untouched. So when you do that, data comes now in this form. So here are a few rows of data. Now let's look at this data and see is it tidy data? If I want to know, for example, what was the weather in San Diego on 1 p.m. on that particular day, 18th of June, 2013, can I see it as one row of data not see some of the data for other cities or other times do you see this guys would you say that this isn't ID format it's in the long tidy format isn't it long, tidy format, isn't it? Is this making sense in practical terms? Read this, guys, and in your book, the two books that you have, one of them was the Pandas, you know, every man's introduction to Pandas, everybody's Pandas book. And the R book, the R book that I referred to you, both of them, they do have a chapter dedicated to tidy data because it is an important concept. Not many books talk about it so carefully, but it is actually very useful. In my team, when people do data analysis it has become for them a second nature to quickly come to the tidy format and realize that after that everything becomes easy so I'll show you how easy it becomes what we did with temperature we can do the same thing with the you know all the other things like pressure humidity wind speed wind direction all the other temperature attributes or features we can do the same thing so we will end up with all sorts of files different csv files all of them in the right format in the tidy data format but then suppose we want to stitch all of those together into one file one data frame the weather data frame what do I need to do I can join on the date time and the city across all of these files so I will suddenly end up with and this is the inner joint that you see me do here those of you familiar with sequel would immediately recognize the syntax if it was a sequel you would say temperature inner join on pressure in a joint a pressure on blah blah this field it's a very similar syntax you do join, and at the end of it, you end up with this wide table. Not this wide table, sorry. With this complete table, which is still in the tidy data format. It's a long table with lots of values, because as you can imagine, the combination of date, time, and city, you will have a lot of values. You put all those values together, and you get this table are we together days so far I need some guys let me yes sir yes so read this the things guys are taking some trouble to write it out because see and what I'm doing is over the years you know you do all of these things in practice you leave teams you begin to see a pattern of what things are in practice more useful and I'm putting all of those things here so yes we we can do that. We have data and now what can we do? Once data is in a tidy format I want to just give an illustration how easy it is to do things like this. Now what I'm going to do is find the median temperature, median pressure, humidity, wind speed, wind direction. All of these I'm going to find for every single city so every city has lots of readings across different days I'm going to take the median of all of these values and a show as a table because I want to know in general is Chicago cooler or warmer than let's say Florida some state some city in Florida Orlando so when I want to do that just think if I had to find the mediums in a data which was not tidy which was in the old format like this format how tedious it would be you would have data across multiple files and each you would have a huge trouble managing the columns and doing it in general I mean you can do it but it would surely not be as elegant as an as simple it is literally one liner you realize that one single line of syntax well I've broken it up into multiple lines for readability as you should but it is still one statement one assignment in one line we have computed it all you see that guys and not only that I have even sorted it by our temperature medium temperature so again those people who have been doing spark or function programming this should look pretty familiar and those people who have been doing Spark or functional programming, this should look pretty familiar. And those people who have been doing SQL, again, this should look pretty familiar. Group by, you all know, summarize. Once you do group by, then you can find all of these values. This is a sort at the end. If you were to look at this table, for example, would you be able to tell whether, let's take Montreal, which is the coldest city in the data set in general? This is a basic example. Can you tell which is the coldest city? Montreal. Montreal. And you can then go down to the bottom and see which is the warmest city on average. It gives you some intuition, which is the, for example, if you look at wind speed, you realize that, all right, you have a fairly good wind out here in this place and mother cities are not that windy for example portrait seems to be not so windy it's so forth so you can answer these questions you can look at humidity and see which is the most humid city and answer questions now I did this exercise in R. Those of you who realize that this is R language. But how different is it to do it in Python? So let us just review the R code. What we did is in R first we read the data. Let's see. Yeah we are reading the data. Let's look at its Python equivalent. Here I'm reading the data. Let's look at its Python equivalent. Here I'm reading the data in Python. Does it look more or less familiar, guys? Read.csv has become pd.read.csv. Looks simple to read and easy to understand, guys, in Python? Yes, sir. Similar. One of the beautiful things is that if you follow a structure, like sort of a grammar data manipulation in a tidy data format, you will become equally fluent in both R and in Python, quite literally, because these are just two dialects of doing the same thing. Now one of the things you notice that I tend to create these tables with a certain bit of aesthetics. It's something that you should do. See ultimately you use your data analysis to tell a story. To the extent that we do that, here is an example. This is a very lovely package in our it is called cable extra you can maybe cable extrapolate the right pronunciation is and what you can do is in a very systematic way it is like CSS what CSS is to HTML you can do that and it does essentially the same thing another covers I guess it produces CSS of some sort so that when you look at your tables, you notice that my tables have headings and they have a row, right? This is literally how it is produced by the R code. So it is something we're doing because again, I started out by saying that with data you tell a story, whether you just do exploratory analysis or you build sophisticated prediction models or a clustering and pattern recognition at the end of it you need to bring it down to the level of you know real people people who are not data scientists yeah it has to mean something so you have to tell a story and that story is the story that is buried in the data. You have to uncover the story and tell it. And so presentation matters. Professionally, it matters a lot. How good your presentation is. One very nice way, a mathematician friend of mine used to say that we tell we write theorems and all of these propositions and theorems and corollaries and so on and so forth and it is all very abstract but the moment we visualize it and we put it out all of it suddenly becomes very real so the beauty of mathematics is in visualization, especially in data visualization. So take it like that. It is the art of our fields, you know, how well we present it and visualize things. Let's do that. So here I'm going to do the same analysis in Python. This is it, the city data. I'm using the syntax of pandas. Now now how do i beautify a table in pandas well it turns out that in pandas it comes the ability to style it in html comes built into pandas it's one of the things people don't realize that you can style in fact when somebody asked me a question how do you show what was the question and then it was you isn't it you asked the question how do you show what was the question and then it was you isn't it you asked the question how do I produce that graph in a table how do I show a bar chart building yes and if you remember it was just a one-liner that I sent back to you and that is the power of these libraries, guys. In the right way, they have all these hidden gems in them. You can beautify it, you can literally embed spark graphs and bars, little bars in the data itself, in the tables themselves. And it sort of comes alive. It becomes more informative there is a I mean ultimately it conveys you're trying to explain or show something to people and it helps you convey that so this is it now how do you pivot or how do you convert it from the wide format of data to the long format? Very simple. The specific Pandas thing is called melt. It's called melt. But do you see, guys, that it is exactly the same city and temperature? Do you see how similar the syntax is between Pandas? And let me show you the our syntax just for reference our syntax yeah guys look at this raw data and pivot longer eight times city can take a few fields you mentioned isn't it now you go to the syntax for R and do you see that it is exactly the three fields you mentioned isn't it now you go to the syntax for R and do you see that it is exactly the three guys daytime city can take you it helps you pivot it into the long format and then the joint syntax looks practically identical the only difference is are we use the word by and here the attribute is on very small differences between the two which is one reason you should uh i insist that people learn both the languages in this entire so once you have this data and you have cleaned it out like for example data you you change it to capital letters and so on and so forth and you sort it by city you can see all of that you can see data again the same way formatted and how do you find the median for example the same statistic exactly the same you group by all of these you find the medians in a couple of lines of code and that is it very easily you have done that that's just one question so in um in python we are actually just getting different csvs for temperature date humidity everything right right so but what if the the everything right but what if the table itself is already created but it's in the byte format how do we get it back in the tidy format each of these things was already there once you get used to this you know this pivoting pivoting you have to do is pivot it into the long form so even if it's already there we can melt it into using yes the word manage pivot these are all synonyms and these are very powerful things in fact when I interview people for in my team I'll start out by deliberately giving data in the wrong format and just see how they handle it do they just struggle with that and do a machine learning exercise with that struggling all the way through or do they actually it's just a very basic thing to see how skilled people are in data wrangling why is data wrangling so important in machine learning you know fun stuff the machine learning fun believe it or not it will take the last 20% of your time 80% of the time goes and data ranking gathering data wrangling with it putting it in the right format ID format what you are learning is actually believe it or not here's another way of saying it. In real life you have, if you do these labs, you would have learned 80% of what you would do, 80% of the time you would spend doing this in your place. Literally, it's a fact, it's not an exaggeration. You would spend the better part of your life in data handling. So do not be in a hurry with machine learning. This is where you need to become so fluent with this. You see the data and as a second nature, you can immediately go about cleaning it and pivoting it. And Nisarg, you were there in the, Nisarg and Anil, some of you were there in the bootcamp in the Nisargananil, some of you were there in the boot camp in the previous thing, you remember that there's more to it. A missing value handling, outlier treatments, outliers and specialty novelty treatment, isn't it? And anomaly detection and so on and so forth. So there is a lot, but we have started dealing with the data. We'll do more after this. So this is it. So this is tidy data, guys. With that, then I go into the example of Florence Nightingale. She is obviously a most remarkable character. Not only is she the mother of nursing, considered the mother of nursing, because she started the profession of nursing literally as a modern profession. It started with her. She created the first nursing hospital in nursing school and she learned all of that from this data that she gathered during the Crimean War. And what she observed in this data is far more people die of course from malnutrition and bad medical conditions of those days, lack of hygiene, lack of hygiene was a huge thing, dirty places, so than from actual bullets or in wounds, battle wounds. It was a remarkable and eye-opening discovery and by the way she is quite remarkable so she's also considered essentially the founding or certainly the pioneer in data visualization this work visualization remember predates computers is hand-done and she did it and she went and presented it to the powers that be in London and of course the rest is history the The whole practice of modern nursing started. So this is that and if you actually look at this data you will be surprised. So I'm doing this now I'm taking examples. How do you read this data? By the way one thing that people ask how do I get my graphs and plots to look so good well I wouldn't consider them too good people who are experts with this aesthetics they do a much better job than mine is probably better than the notices so here are some of the tricks I use latex in the text and so on and so forth and you will see some of that in effect so i already set the figure size i said the style of the plot gplot and so forth so i'll take an example this is the way you would do it in pandas now in this notes what i've done guys is i've alternated between r and python i've deliberately done one example in python, then another example in R, another example in Python and so forth. Except the weather data set which I did in both, just to illustrate. And the idea is you should also do it in both the languages. In fact you should do each of these examples in both R and Python. So here we start with Python because you can tell the Pandas data plane. This is a summarization of the data and if you actually write the data, it's just 24 rows of data. Do you see that? So essentially you can say that modern nursing started with 24 rows of data. Isn't it quite remarkable? One of the things I wanted to say, but of course I'll leave it to you to do the exploratory analysis for that. And you can do that or you can just use the data profile, you know, profile report, find us profile report command and it will produce the whole report. And those of you who were here in the last session that we held physically at support vectors, you realize that you had finished this part. Likewise for John Snow. John Snow have given you a hint, do it in R. You see the data is in R. You can use these datasets. By the way, all R datasets can be imported into Python directly. Skicket Learn gives you a direct support for our datasets. They should know. So there is a very, very strong cross-pollination between R and Python communities. They're like, you do something, I copy. Then I do something, you then i do something you copy sort of almost completely at parity so i don't get caught up in all these debates of which is more powerful do both so likewise this is the galton data set again i believe i've used Pandas. So again, alternative Florence was in Pandas, Python, Galton, I left it, I mean, the John Snow's data set, called it instead I left it as exercise for you in R. This is again in Pandas. And very simple, Galton's work is regression towards the mean. It is this concept of regression towards the mean is profound, guys. Its influence or its impact is way beyond just data science. The great, one of the greatest historians of statistics and a great statistician in his own right, Stephen Stigler, who was also a great sociologist, great sociologist. So he used to study scientists as a tribe. If you consider them as a tribe, what are the behaviors and manners and practices of scientists and so on and so forth. Very interesting fellow. So he has written one of the books, The Seven Pillars of Statistical Distance. Regression towards the mean is one of those pillars. It is also a concept that our mind has tremendous difficulty grasping. When it is staring us in the face we don't see it and we ascribe all sorts of causal reasons for it. And so therefore this data set, it is we must at our time do the Galton data set sometime in our life. It's really worth it if nothing else to drill this into our head what is the passion to do. So this data file by the way is again there in our GitHub and and whether if you want to go and look at this article site do you see that in blue these are all hyperlinks. If you were to click on this it will for example open up you see you can you well okay it takes you straight to the article the original article and you can read online for free and so on and so forth. I suppose I have an account, I didn't notice. But if you just Google for this, many people have posted the PDF online for it. So this is it. And then Stiglitz article by the way is very very good. I cannot, I must highly recommend it it's a lovely article once again this is this is again freely available download PDF so if you download this PDF you can okay okay I have accounts on the other machine but yeah you have to do all this thing you just google this up and you'll find this article available maybe on a Stiglitz home page or something. That is that. So what is this data? You have families of the father, mother, gender of a child, the height, male, female, and so forth. So you have this data. You can compute statistics of the data. And by the way, is this in tidy format or not? Let's take a quiz. Is this data in tidy format? What would you say? What does each row of data speak of? It speaks of one family. Yeah, one child in a family. So if you look at one row, it is one child, one offspring. It says that it belongs to family number one. So the family number one seems to have four children. Zero, one, two, you know these four children then in each of them the father's height is the same this is the mother side but each child is this point seems to be a male is meaningless male is one female is zero it is just a encoding height of the child and obviously this speaks of this family has how many children these are the heights of the so each row is a child it is in tidy data form we can describe the data again to see what is the average and so on and so forth so for example this these are all in inches let's look at the average height 50% the mean height so clearly the mean height is 67 it is the average double both male and female right there the data seems to have a little bit of a asymmetry about fifty one point seven percent of the people I'm boys and forty eight point two person seem to be girls as children and then this is the average height and so on and so forth in general what can you tell from this data our father's taller or mother's taller by just looking at this data others the average weight is much more than mothers so this is it guys you know you look at data this looks like very trivial thing to do it is worthwhile doing that and then of course you make graphs and so on and so forth. So you see me here make a graph and once again I assume, so by the way guys, when you make graphs it may not look like this. So why are my graphs looking different? The reason for that is it has to, and I'll come, actually hold that part in your mind because it is in the document. When you read it, you'll see this in the chapter I mentioned about. So this is, do you see how minimalistic it is? The grid grids are missing and so on and so forth it's quite minimalistic and these days we believe it should be like that i will talk more about it so this shows you the high distribution clearly the men are taller than women on average right so there this is a violin plot which is very much like a box plot just more a prettier version of the box plot. A little bit different actually, it's not exactly what it talks about, but I won't go into the technical details. Then see what happens typically is that you want to know the histogram, you know the frequency charts of the father by height, what are the different heights of the fathers and mothers mothers and the scatter plots by mother and father height. So clearly you see that there is certain degree of correlation between mother and father height. Not a very strong correlation but there is some correlation. And likewise, Likewise, the height of the children are related. Then male tend to be taller than the female, again, once again. And these are called density plots, kernel density plots. What they give you are the contour lines. They give you a sense of how the data in two dimensions are grouped together. It is a topic that I will cover one day. Intuitively it is like if you think of all of these points of men and female as clusters of points, where is the peak of the cluster? Like how densely are they sort of located? That's what this kernel density is. These are called kernel density estimators. We'll talk about it more when we get into the machine learning. At this moment take it at face value. Then we look at the correlation. What is the correlation of the factors with each other. Do you notice that any child height of a height is, is it ever perfectly correlated with the height of the parent? Father or mother, what's the correlation of a child's height? The height. Height's correlation with father and mother is just pretty small, right? 28% and 20%. These are small correlations. So you do and that shows in this data, you can see that the correlations are there, but they are very small data is scattered. So I've given a homework here, which is a is a sort of worth saying. I'll just mention what the homework is. Notice that the correlation between an offspring's height to a parent's height is never perfect. A perfect correlation would be what value, guys? values of one except the diagonal which is self correlations. So now the thing for you to do is to reason from there and explain the phenomena of regression towards them. So this phenomenon can be explained the roots of it is in this the fact that the correlation is not broken. Now if you recall I've given this explanation in the class. I've given it to you using correlation and then I've also given it to you in terms of the regression equation. Think through, see if you can reason it all the way, recall the explanation. This one exercise is really worth doing because it's an observation. It will tell you an observation that most people don't know. First of all, most people don't know that there is regression towards the mean and then if they do know regression towards the mean, they don't know why does it happen. So the statement that you should discover is regression towards the mean is a direct consequence of imperfect correlations. So once you have thought it over, then you can look for a very detailed article, this particular article of Stigler. Absolutely definitely worth reading. And again, I'll click on it and it should take me, well, it takes me back to this journey and then there is a book by a noble laureate called daniel bahneman thinking fast and slow fast and slow i don't know if any one of you have read it very very interesting book he talks about two kinds of mind, how our mind reacts. A fast thinking mind and a slow thinking mind. Usually fast thinking mind is the one that quickly gets into trouble because it sort of comes to the wrong conclusion very rapidly and doesn't realize that you have used your fast thinking to come to a bad conclusion. You need to do some slow thinking and so forth. It's a very interesting book. So if you want to read it, he talks about regression towards the mean. So in the community that looks at data, this regression towards the mean is a big thing, guys. Really worth knowing, which is why I started this entire workshop series with it. We'll never again talk about it because we have to move on to other things. Today is the end of data exploration. In fact today I would like to request a longer session till 10 30 because we have to do machine learning but for what it is worth I want to cover this document. So pandas profiling once again. So what does ponders profiling do? One line of code will produce pretty extensive report. And you should do that. It will produce a summary of the data. It will produce the correlations. And it turns out that there are many kinds of correlations. I'll let you explore those and due course of time, one by one, I will be talking about that. But I've introduced you to the correlation by Pearson because he was the student of Galton. He formulated that concept of correlation and so forth. So you can see all of these histograms and everything. I won't go into that. Do it in your labs. I certainly do that so you can see it. Next we move on to iris datasets. So the previous one we did in Python. So certainly this one merits R. This is it. I'm doing it in R. There's a very interesting history to this dataset, Iris. Nobody, like all the data scientists in this field, Almost everybody has started out in learning machine learning by doing this exercise early on on IRIS. At some point, very early in the learning process, they have all studied the IRIS data set. At this moment, we limit only to the exploratory analysis, no machine learning, but this is it. What we're doing is perhaps the most quoted data set in various articles in the feed. Everybody comes up with a new algorithm and he uses it quite often. It came about as came about, there's an interesting history. A biologist just literally walked out to a colony of iris flowers. And in that colony, he found three species. And he collected 50 data points on each of the species. He collected their sepal length, sepal width, petal length, petal width. And he got the data. So you can just go and get the Irish data it is again I posted it once again if you just review the data you know just in the raw form you see how land it looks you can pretty fire it's a bit and just bring in the library title verse it will import all sorts of things in the world of ID data you can include be player you can include GG plot which is the grammar of graphics how to create beautiful and all of that so here it is yeah you guys are familiar with this code by now here I'm doing so what have I colored I've covered the last column because that's the species that is the species and these are the attributes of that species so is this data a tidy data format guys again once again if we were to ask this question what would you say is it in tidy data format yes yes right because this is a sentence of iris citosa and then we have the four measures the four features it is entitled data so how do i do deal with this data? Well, I haven't done much. I've just taken an example. If you have it in tidy data, you can easily summarize the data. What we did before. By the way, in the flower, I've put the pictures of the flower. And so you have to look a little bit carefully to see the sepals and the petals. This is the sepal and this is the petal. It's a biologic thing. The big ones are the sepals. So when you look at the median, it's just worth observing. Let's look at Iris sitosa. Do you notice that sepal length is five and sepal width is 3.4 and then petal length is 1.5 and petal width is minimal so if you take the ratio of these two 1.5 divided by 0.2 you realize that it's a seven and a half times bigger petal length in general is bigger in iris citrus. So you can look at this picture and see is the petal long and narrow. You can't make it out but this is the long and narrow petal. It speaks to the data. That's what it is. You can look at the picture and verify if these things are true or not. And if you look at virginica, it has the widest petals. You know, of all of the acetoac has the widest petals and then the sepals, widest sepals. And the second widest is, well, they're almost close to each other in the sep the petals part virginica has white petals and i suppose this picture is too small on your screen to see but you do see the width yeah so it's good you know comparing the data and seeing does it make sense or not next comes to visualizing data now visualizing data is a topic that I would say just as you know people can do data manipulation in a haphazard way whichever way they hack their way through. But you can do it in a systematic way. In the same way when you have to make plots you can make these plots in sort of a haphazard way, hack this, hack that, and put a plot together, or you can do it in a systematic way. So there were so many plotting libraries, and every library had a different API. And then came a landmark book, which is in a little abstract. It is called The Grammar of Graphics. It's actually a very good book. little abstract it is called the grammar of graphics actually available it talks about how all graphics can be considered sort of like a language and you can speak the language with a very few words you know with a very few with its own fixed grammar just as you can speak English with a fixed grammar and if you do that you can systematically build up a graphic any graph any plot you can systematically build up so that philosophy influenced actually at least two very uh popular open source projects very popular one of them in the JavaScript world is called d3 d3 yes those of you who don't know about d3 I invite you to a google it up at some point and see what d3 is it is perhaps the most widely used data visualization library it is based on the grammar of graphics and the other in the in the world of our environment it showed up as the library called GG plot so GG plot and here I've given the code to show her layer by layer you build up a graphic as simple as this we are showing the histogram of several length by car actually I should fix this picture is not really legible but that's that so how do we actually is it because you go to you zoom different view perhaps. Zoom. View not changed. And let's look at this. No, it's fine. Okay, that still could be all clear. If you look at this diagram, guys, what do you see? We are looking at a feature called sepal length for the three species of iris. For each species, we make a frequency plot, a histogram, right? Histograms and frequency and obviously is bucketized by some bins some from let's say five to five point one how many how many of iris it also data samples of our so this is a frequency plot now let's see how it is built up those are called histograms first of all you think of it as layers like it is in Photoshop you first take the first layer you get it is the data layer you just give it the data in the second layer which is the aesthetics you specify your axis what are you specifying the only thing you need to do is provide the x-axis which is the second one so you just picked a feature a column and then you picked fill is species like what is the fill color and you do that and then so this is just specifying this this and the color but what are we going to do now we happen to do a geometry so the way a grammar of graphics things a histogram a scatter plot a line and all of these these are just different geometries to represent the data and just by changing one line of code the geometry you choose we can represent the data differently so here we choose histogram. Now you notice this alpha is equal to 0.5. There's a few, I just thought I'll let you know. Those of you who are good with CSS and HTML know what alpha is. What is alpha? Could anybody tell me? Transparency. Yeah, it's the level of transparency. One of the things you should do is if you fade it out a little bit, it looks aesthetically more pleasing. It looks a little bit bold colors if you don't get transparency. So I would highly encourage you to always play with transparency to create a good effect. And then you notice that I have added a theme to it. So it's the next layer. And after this geometry, these things are not really needed it is good to have I added a theme called tuft what this theme is and what in the world tuft thing is will come to it a little bit they're coming close to it and now we add a label so here label I've given count as a label, otherwise it wouldn't have been there. So you see that how systematically I'm adding, think of each line as a layer, data layer, aesthetics, geometry, theme, and then catching up the theme, you know, fixing the axis labels and this and that. You could add a title and whatnot. So this is the grammar of graphics. Believe me, guys, people have created absolutely stunning visualizations using the grammar of graphics. D3 and ggplot, by using either of these two, people have created beautiful, beautiful visualizations. In fact, if you go to the b3 website and the person who created this is Michael ghost on amazing amazing pillow in data visualization and if you look at the gallery of visualizations that you can do examples look at this all of these things can be done with just a few lines of code and a very sort of intuitive set of lines of code all of these visualizations so I would highly recommend that whenever you want to create a data visualization visit it see how it is done, learn from it, and then do it. And whenever this code is, thanks to Michael Bostock, freely available, if that code suffices for your need, you can take it. And a lot of the places people just have used this. If you look at these visualizations, you would remember how many website you would recall that there are many many websites you have visited which actually use these things and there are many many websites devoted to this this sort of thing and any one of these let's go into any one of these so I said this is a JavaScript notebook yeah these are all JavaScript done in JavaScript right so suppose I take some boost it's a hierarchical representation of data let us see d3 at play I mean the grammar of graphics at play here what do you do you take canvas the very first thing you do is put a data layer do you see these guys a data layer you put the data layer and then you put the fill you know color fills and so what do you fill it up with etc etc right and you give it some text what is the text that you would do and then after that you're attaching is the text that you would do and then after that you're attaching do you notice that these are all styles the rest of it is styling and so append yeah you're appending something you're doing some well if this one is a little bit complicated but I hope you discern some of the structure that you talked about data there then aesthetics then then you know teams or you know color and title and so on and so forth you can do that and obviously people have created a ggplot libraries to ggplot showcase. Our package showcase. Let's do that. I don't think this has that. This is just a, these are highly well documented libraries. People have done that. Beautiful visuals. Alright, I'll just click on one so somebody has taken a data set and look at this yes he does see how he builds the data he takes this file he reads the data. He takes this file, he reads the data, then first thing he does is he says okay ggplot. Then he is going to bind it to a geometry. Where's geometry? But then you have to give it data. At some point you have to give it data. Usually I give data in the ggplot itself and then add geometry. After that aesthetics. What are the x and y-axis? It leads to this. Then this looks pretty ugly, black and this. How can you make this chart better? You start now adding, make the dots smaller, adding colors. Then obviously the next things will be, and you can scale it by log, you can do a scale distribution and so forth. I assume at some point he'll get into colors, I hope. Yeah, he's getting into colors and so forth. And do you see how systematically the visualization keeps getting better? At each point he adds one more thing. To go from one to the other, he keeps adding things. So it is very additive. You can add sentence by sentence, and then it keeps getting better and better. You keep putting layers, as people who are familiar with Photoshop would say. So this is an example of ggplot. I would highly recommend you learn ggplot. And there are many plotting libraries where it gives you a systematic way of doing it. And suppose you want to show not just sepal and sepal in but you want to separate the histograms so you notice that here all the histograms are mixed up in a single graph it's a composite suppose you say separate them out all you have to do do you notice that you take the previous code and you just add one more line one more layer called facets it says by species separate them out are we understanding this give me some feedback are we able to understand this does it look familiar yes it's clear yes and it's very powerful guys to do that you can do that and then of course I suppose you want to say I want to do I want to make a grid plot of looking at this by sepal length sepal width petal length petal width so there are many libraries that help you make that have extended ggplot you can do it with ggplot it's a little bit more code but people have written libraries on top of ggplot to make it easier and one of those libraries is cowplot and here's this usage of cowplot to do that and there is also gridplot and other libraries and grid extra and so forth people have added so pick your favorite and do that now one question that you should have in mind is we are talking of all these libraries of r and python where do i go looking for those libraries? And we'll come to that in a little bit. You can make, oh, this is a box plot. If you have done courses in statistics in high school, these are your box plots. How do you draw the box plots? This is the way. Very simple. What do I do? You make four plots, one for each attribute, separate length, separate width, etc. etc. and you do this. Now you must notice that this is rather bare compared to the typical thing people have. They'll have all sorts of grid and so on and so forth. This minimalistic style is highly incorrect in this case. So now that we have done that we can do bivariate statistics. How hard is it? Again very easy. We can we are still talking about diaries. So we create a correlation. Just this is the use the car function it will produce the correlations why only one to four columns because there are four attributes the first four columns are the attributes the last is the species then when you default make a car part it will look rather ugly I mean you know the principle sort of what I call principle of least in what people what what that says is that you need to have a high data increase ratio which is that data should take more ink and clutter should be minimal. So correlation is a symmetric you know the correlation of sepal width to sepal length isetric. The correlation of sepal width to sepal length is the same as correlation of sepal length to sepal length. So no point in repeating any one of the boxes. So you notice that some boxes are missing. Redundant boxes are missing here. And how do you do that? Let's again look at this very simple plot. People say, okay, how did you get that? When I do a default cor plot, it doesn't look as pretty. So the answer to that is, let's see here how I do it. By the way, this is the clustering order. It's something I do in cor plots. Ignore this part. First of all, I do the correlation. I give it the data. Then I say that only have lower, because there is no need to have both upper and lower. If you don't give it, then you can show everything. Outline color white. Where is that? It is hard to discern, but there's white line separators to the boxes. And obviously, here I'm going blow by blow, gradually, a little more faster. Then one of the things I'm going blow by blow gradually and more faster then one of the things I'm doing is I'm suppressing the x-axis title and y-axis title is redundant and then I'm giving it a theme in the theme I'm saying all right the element size should be all text should be 16 points font, and all titles should be larger font, pretty font. So does that show foot, guys? Is this bigger font than the rest of the text here? Yes. Yes, Asif. So that is it, actually. I'll make it more like 150. So this is it. So this is how you make this, use libraries. And then you can do more than that. Suppose you want to do this pair plots that I was talking about. Remember we did the pair plots in R, I mean in Python. So how would you do the pair plots in r in a pretty way equally pretty way this is a you use a library called gg galley again based on gg plot and uh what do you think guys does it look nice it does right i have a question so the sepal length to sepal length should be a correlation of one right so i didn't understand why there are three histograms there okay so here right it is a sepal length each is a histogram of one species so it was uh virginica and the other one okay got it got it those are the histograms and this is you literally see the distribution so you know you see this you see this histogram here and you see the data right here you see that the data goes only this far right for the pink one it goes this far yeah and here it goes much more so this is it guys this is uh do i need to do more? Oh yeah. So after all of this example, I thought I should leave some things for you guys. So this is the California house prices data center. Can you guys please repeat whatever you have learned in these notes and apply to the California housing data center. So in California everybody seems to be very obsessed with house prices because for the vast majority of people, middle class people, it is a fact of life that our single biggest asset are our homes. If we have a home. If we don't have a home then that's a different story. Quite often our homes are our biggest assets. So when the economy does its ups and downs we get worried and we often keep looking at our house price. Zillow is a very popular website and there are many other websites to give you an estimate of the house price. So I thought this data would be interesting to you guys and this data is from the 1990s. You can even learn something about how house prices were in 1990s and how house prices are spread out in California today. So do this analysis in both Python and R and guys send me your notebooks, email me your notebooks. I will review and give you feedback. Are we together guys? And if you need help, reach out to the TAs, reach out to Dennis, Prachi, Dennis, reach out to them whenever you need help, or reach out to me directly. So now there's a, that's it. Well, now I'll give you some basic tips, guys. This is an important tip actually, the who is Edward Teft. He's a professor at Stanford and he has had a tremendous influence on the display of quantitative data. His book, Visual Display of Quantitative Data, literally by that name, was sort of groundbreaking. He gives popular workshops, pricing is perfect. Very good workshops actually here in Silicon Valley and everywhere. If you guys get a chance, attend his workshop. You'll get a completely different perspective on how graphs or plotting or just data visualization should be. One thing that may interest you, this document, do you guys like the formatting of this document anyone likes the format of this document this document itself follows edward cuff's recommendations yeah presentation i did go to his presentation oh you did go how did you like it um it was good i don't know how much longer he's going to be giving presentations though. He's getting pretty old. He is indeed. So Edward Tufts says that graphic excellence is that which gives you the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space. Do you see how much he emphasizes brevity and a minimalistic approach what happened guys is that we used to actually when we were growing up my generation distinguish between what we call a scientific diagram or a graph from line graph or bar graph or a diagram of a machine used to be minimalistic. It would show things clearly, they would be labeled clearly, but you would not add shading to it. Those of you who have done machine drawing would remember that you would do the plan elevation side elevations um well today is the world of cat camp so perhaps you don't know what machine drawing is but if those of you who do remember that it was a minimalistic approach all scientific data is minimalistic represented minimalistic in journals but somewhere along the line know, a data visualization gotten into the commercial world and in the hype of marketing and whatnot, people started adding eye candy and clutter to the graphs. Unnecessarily, the bars began to look 3D. They began to develop gradients, you know, color gradients. All of you have seen this, isn't it? began to develop gradients, color gradients. All of you have seen this, isn't it? And all of that, and soon when you looked at a graph, you weren't sure whether you're looking at a scientific diagram, like a plot, or whether you're looking at eye candy or art. And there was overwhelming, I mean, I don't know, it was actually pretty much a mess many many tools that help you do data business intelligence tools and whatnot they are to be blamed most of the data analytics tool at one time used to fall for that sort of eye candy in the widgets. You could make the graph, unnecessarily a bar chart three dimensional. You could add third dimension, shade, gradients and whatnot. It is completely against the grain of scientific data visualization or display. Keep it minimalistic. Your point is to convey a story and information, not to create eye candy. And one particular story that Duff takes which is quite illustrative is the Challenger disaster. Many of you know that at one point, the shuttle, it ended up, it blew up on the launch pad itself. So when it did that, people investigated what happened and why it happened. And it was the physicist Richard P. Feynman who founded the reason quite simply. He took an o-ring, what we would call a gasket, and he put it in cold water. And then he shows that when the temperature is low, it becomes brittle, it breaks. So what had happened is that the seals had been compromised, and so fuel, a highly inflatable material had leaked out and there was a fire. So then the whole question is how is it that it was not caught? After a lot of investigation it turned out that actually there were a couple of engineers who had been saying that this is a problem. So they presented it to their management, but they presented it using a visualization that is hopeless. And their entire presentation is so convoluted, so complicated, that the upper management thought, OK, no big deal. We can go ahead with the launch. So from a data scientist perspective, did those guys succeed in their core mission of their data analysis? Would you say they succeeded? The analysis was perfect. It even showed it. It even made pictures and diagrams to show what was wrong or what could go wrong. But from the way we look at it as a data scientist it was a singular failure because it failed to tell the main message the data was telling a story and they could not convey a very simple story which was that at low temperatures the o-rings will get compromised. As simple as that. It could have been conveyed with simple data and simple visualization. You see how it is and how tragic the consequences are when you don't do it the right way. So something is sort of a cautionary tale. Then so this is about so he talks about it which I've talks about the data integration. How much ink did you spill to show the data versus the total ink used in the graphics? Did you add a lot of shading and third dimension and eye candy and lot of tick marks and this and that, grid marks and so forth. So avoid that. Ask is it absolutely necessary to show what is here? If it is not absolutely necessary, take it out. Now it turns out that R is generally more mature, being an older framework, so there obviously there is a tough theme that comes built in. You can add it to ggplot as we have been doing in the code. In matplotlib it's a little bit harder to do that but you can still do that. People end up writing their own functions, themes for matplotlib which mimics the tuft style. And if you guys are interested I can give you the code from mine. You'll find my code sooner or later. The other thing I do is the text that is there. If you look at the text that is produced by these plotting libraries, it is, in my view, very substandard. It is not the high-quality text that one expects in a publication-quality scientific journal. I mean, a scientific journal, if you write an article and expect it to be publication quality, it should meet that standard, aesthetics matters. So in that world, generally latex, there's something called latex, it is considered to be the gold standard. Most physical sciences, mathematical disciplines, physics, math, and chemistry, and so forth, quite often, they all tend to use LaTeX. And computer science also, all the research papers tend to be written in LaTeX. So people like the formatting of LaTeX, it has a professional appearance. The good news is you can actually do it in your code. do it in your code. You just have to remember to first of all on your machine have the LaTeX installed on Windows on every operating system it can be installed do that. If you're willing to do that then and I've taken this example in Python here. Do you see that I've read some data set. Data set 2. By the way this is part of your next lab and we'll do that at some point. Divi set 2, I'm going to visualize it. So first of all I do this normal aesthetics and this and that. Then you notice that I'm doing some things. Now usually I do it not for every plot but I do it in the top of my notebook so that it applies to all the graphs that will come, all the plots that will come all the products that will come in that point these are some of the things i do i believe you see this crucial line use text is a little different if you don't have text installed on your machine you will have to remove this line so what does it do now it turns out that r actually it comes up with something and and this whole Mac.lib comes up with this lovely thing called Python. Python in Mac.lib has this lovely syntax. If you wrap a text with this R quotes, a string is just quotes. Font is just in single quotes. This is what you would call a string in Python, isn't it? But notice here here we have done something strange we have put R before it when you do that then it is interpreted as a thing written in latex in the language of later and this dollar X dollar is a latex syntax what it says is X and is a mathematical term Y is a mathematical term so write X and y in mathematical mythology. And then I'll give you an example of what it produces. This is what we are aiming for. Now I don't know if it shows through but when you print out, let me go to 100%. Well not particularly somehow on this web, it's not showing coming out in full beauty as it would be intended. But do you notice guys that this seems to be, there's a title or deliberately put a random title. So this seems to be a sine wave function. Now this is mathematical right? Y is a sine px plus pi. That's the equation for a sine wave. This is the data. Obviously I've added some aesthetics to it, a transparency. But you know sometimes to make a point you need to put arrows and notes. These are called annotations. To put annotations. So these are annotations I put. And you notice that this notation here, X feature, it is when you zoom into that you will notice that this is actually fairly professional. Well it doesn't show, I hope it shows you well, but you notice that it's sort of written in good fonts. All of this text is written in good fonts. So the code to do that is here. This is the code that will help you do that. All of these things. This is how you create an annotation. So there are two annotations, both of which I have. This is the title. And you notice that in the title, again, I have put LaTeX syntax. LaTeX, by the way, I must warn you, it has a pretty steep learning curve. While it is nice to say that you can do that, as they say, Rome is not conquered in a day. So as you're entering data science, don't try to do everything at once. So if it is new to you, by the way, is there anybody in the audience who is already an expert in LaTeX? way is there anybody in the audience who is already an expert in latex? If not, well it is new to most of you. It's a language in its own right. It has a steep learning curve. It is a language for typesetting. That means typesetting. A beautiful language. It was created by the great Donald Nook. He got so frustrated by the fact that mathematics couldn't be properly written in journals that he sat down and created tech. And it is by many measures, perhaps the one library that has no bugs for the longest time. It's a huge library and used to offer $100 to anybody who would find a single bug in the text code base, you know, the implementation of text. And it became very, very hard to find any bugs. So it is practically bug-free. It's one of the few software that is actually as close to bug-free as is humanly possible. There is a legend, that is sort of an understanding that things will keep getting fixed, but when Donald Knuth dies, whatever version is there will be declared the final version and all remaining bugs that will be discovered will be called features. Sort of a joke in the community,'s a tradition and I believe they are going to follow that but tech is a brand I mean some grand language many of you don't know but if you do want to get introduced again slowly there are lots of tutorials on it we can do that and so again in now in Python how do you create a grid of plots we saw how to do that in R. Well, here it is. I've given an example code of how you would do it in Python. You see that, right? In matplotlib, this is a creating and I'm reading four files by the way in a loop. This is a for loop. Oh, by the way, I should have shifted it. This is wrong. I'm pasting it here. The indentation went wrong. So as a Python code, this should be indented a little to the right. I'll fix that. Anyway, this is the Python code. We read four files, and for each file, just plot the data. But the point was to illustrate how would you create subplots. This is it, guys. And all of this, by the way, observe syntaxes that are used . By the way, where are all these data sets? As a reminder, once again, it is here. The margins, you'll find all the references. And then we are done, guys. Another thing, some helpful cheat sheets. You're entering a vast world of data science. It helps to have some good cheat sheets. Now, there are many, many cheat sheets. What I do is, and I'll give you a way that I recommend people to do. When you're learning this, print out these cheat sheets and just put them on your wall or on your table, paste them to your table. So what happens is that whenever you need it, you can just directly look at it and you get it. Or you know you can, if it helps, one tip that I give to people is because these are PDF documents, you can convert them to images and so forth, make them as the background to your convert them to images and so forth. Make them as the background to your laptops, desktops, workstations, whatever. Just make this cheat sheet the background image. As you are doing data science, as you are creating your own code, it is very easy to refer. You just have to look at the background of your computer and that does that. And so there are all these lovely cheat sheets people have spent quite a bit of effort making this like example pandas it's pretty extensive library but if you look at the cheat sheet do you see this guy isn't are you able to see my screen days so this is it you know in one, all the key operations are mentioned, how do you create a data plane, three ways of creating data plane, method training, reshaping data, remember the pivot, the melt, it is there. How do you concatenate, How do you join? And so forth. How do you summarize data? So it really helps having these cheat sheets with you. Because it summarizes a lot of learning. You learn and then you refer to the cheat sheets. I've tabulated all the major cheat sheets that you will need and also implicitly the libraries that you will need, I consider essential to this. Now where do you go looking for documentation? It seems to be a vast world. Now I've documented the two things in R world. R has a very amazing tradition. All of these libraries, they contain not only a user documentation, they contain something called Wignets. You can just use the help command in R and it can give you fairly good documentation but you can use something called the vignettes these are a treasure trove of helpful guidance on how to use these libraries and then there is obviously the R documentation website here we go you can go here and you can search for for example any package for example here there is a new package called cyto miner or car fight package let's go and see the top five packages what are the top five packages ah there is a package called open land look at this tool for analyzing, for the analysis of land use and cover. It's a time series package. So when you see how well it is documented with examples, and graphs and so forth, and quite often the code that you need is all there, right? And the open waiting community you to use it. What will happen is this will be your starter code and then you'll add things to it. Are we together guys? So this world of data science is highly, like very well documented. There is a concept of literate programming, which means that instead of explaining what the machine should do, which is what people do when they write code and they put comments, your entire code should be aimed at in your documentation. The whole notebook, the Jupyter notebooks and R Mountain notebooks, should instead be geared towards explaining to another data scientist, what is it that you have done in a reproducible manner give the explanation show your analysis you should be able to run the analysis have reproducible results and so that is a big theme in data science so anyway that is that so i've given the documentations here likewise now, now there is a lovely journal. Okay, there are two more things. There is something called CTAN task views. If you want to say, hey I want to do multivariate analysis, which is what we're doing, where do I go and look at it? What are the best libraries available there? So CTAN, which is a repository of all these libraries, they have spent some time curating the best of the libraries for each of the classes so you can just go there and pick what you need for each of the purposes which is as simple as that not only are they there somebody has taken time to put it together there is a lovely journal that comes out about twice a year it contains articles and those articles are generally high quality let's go and check what is there today. Journal. So let's look at the current issue. How do you connect R with D3 for dynamic drafts? Oh very good. D3 is very very popular. Let's go and see what these people have done to connect R with D3. This is it. It is saying this. I can open this. I hope you guys can see that these are not just shoddy documentation. These are very well-written articles explaining things. There is a library called Turar. And using this, you can integrate D3 into your work, into your R code. Welcome to the Turar Shiny app powered by D3. So he's given an example. It's pretty extensive. Now you can just use this. You like it and you can use it and then there is everything is there, bibliography and everything. So in other words, these are very well done and everything. So in other words, these are very well done documentation and journals in this space. Likewise, in the Python world, Python is a younger, data science in Python is a younger enterprise. Almost all the documentation is just online. So the big libraries, PyTorch, you go to this. Fast.ai, they have a course. At some point, I highly recommend you guys to do that, but not now. Or if you want to get started, why not? How would I learn TensorFlow? So I put it in this document. All you have to do is click it. And likewise, Cricut Learn, which is something we'll do as soon as we get into machine learning. Pandas, we're already familiar with now, and NumPy, Skipy. Just look at these libraries sooner or later. You will become familiar with this. By the way, as you guys read this document, if you find typos, do please force to just snap them. So this is it. And then I don't think I have anything else. The rest is bibliography. This is a collection of all the references that I made. So guys, this is a walkthrough of that. When you do this, then I would like you to try it before I post. Now, I will post the solution R notebooks and Python notebooks. Are you guys ready? Should I post the solution or do you guys need some more time to do the labs? Do all these analysis on your own. Any feedback guys? Give us some time to try as if okay take another week and then next week I'll release the solutions to all of those so guys I'd like to take a should we take a five minutes break or ten minutes break in quarter and so forth and then let's get back to theory how How about that? Let's finish our bias way and straight off on regression. Today I want to be done with regression. Let's take another hour and finish it. Are you guys game for it? Sure. Sure. Sounds good. So I'm stopping the recording, pausing the recording for about 10 minutes. We'll regroup at 9.40. All right guys, let's resume with our lecture. If you can see my screen, we are talking about linear regression. So you realize that the error term we wrote down, I wrote, I took the liberty of writing this therefore. So the error term is this. Let me remove the mouse from here. This is the error term. Now to go back and recapitulate our theory, suppose this is your hypothesis. And this is your X-axis sale of ice cream, the temperature. Temperature and the Y axis is sale of ice cream sale. Of ice cream. This is the example we took. And let us just mark the data point somewhere around this. I think enough data points. So we realize that this is maybe the mouse has become too big now. Are you guys able to see the pointer? It's not visible. No it's not visible. So my mouse is not visible. Okay, I'll have to figure out how to make it work in the mix But I am just basically making drawings in the graph So these are residues guys So we remember that, right? And so this is nothing but the sum of residue squared. Now I explained to you something called the line assumptions or the Gauss-Marco assumptions and so forth. And while that was correct for one dimension, when we go to many features, the more formal statement of those Gauss-Markov assumption, it is more complicated than that. And we will talk about that at some point. But not today. Today, we want to finish this. So guys, we say that the sale of ice cream does depend on the temperature. That's a hypothesis. Baseline hypothesis is there is some relationship. And it's a linear relationship and we are finding it. What is the opposite of that hypothesis? When would the entire modeling exercise be useless? The sale of ice cream is not dependent on temperature. Exactly. When temperature has nothing to do with the sale of ice cream is not dependent on temperature. Exactly. When temperature has nothing to do with the sale of ice cream. So for example, if my x-axis was not temperature but it was the number of penguins who jumped into the Arctic, Antarctica, and Antarctica jumped off into the sea on that given day, well you would plausibly argue that that has nothing to do with the sale of ice cream on your beach. So just because you are given data like x and y, you should not just assume that y is equal to some function of x. Do an error term. You should not just assume that. That may or may not be true. So the null hypothesis is. Is that. There is no relationship. There is. No relationship. No relationship. there is no relationship there is no relationship no relationship relationship between features and target variable which is x and target variable y There is no relationship at all. So if there is no relationship, what is the best Y that you can predict? What would be the Y value that you would like to predict? You see there is no relationship. So what is the best I can do in predicting the value of Y? Anybody would like to guess random yeah is and just take an approach which just takes the average of the middle of all the Y values right just go and take the average value Y bar so irrespective of what the X is predict y bar so this green line is the null hypothesis now you would agree that the green line would have its own error term of the null let me write null with a phi symbol this is a symbol for null I suppose is the is basically the sum squared residues sum squared residues of yi minus y bar square isn't it not the prediction because your prediction is always and not y bar in fact no y just the y bar are we together guys and so any model that you build anything that learns from the data any hypothesis that you think what should be the baseline criteria that it is successful that it has learned something would somebody like to guess that what would be a proof that your model is actually worth it what is a baseline condition you can think of later than the hypothesis yeah better than that right So you would like to say that your this guy, he of your hypothesis is greater than greater than what? Greater than me of the null hypothesis, isn't it? In fact. The sorry, it's not greater sorry now smaller it should be smaller isn't it your hypothesis should make smaller errors in prediction than the null hypothesis would you agree that this is is a baseline condition that yes the question is and we will use this baseline to define a term which is defined. Now, suppose your error hypothesis is zero. You have no errors in your prediction. Would that be a good model or a bad model? Unrealistic. Unrealistic. But let's say in this term you would say close to zero you would say well that sounds pretty good on the other hand if your hypothesis is the same as the null hypothesis you would say it's not doing any better so we need a number a measure a measure which is equal to zero when E hypothesis is equal to E null hypothesis and let us say it goes up to how good the measures of measure when the hypothesis the error can at most be epsilon or zero the smaller the hypothesis error because then you have gone from certain null hypothesis error to absolutely zero so let's have a sliding scale zero is it's static one is a great model so any real hypothesis realistic hypothesis should be between zero and one so if you just observe this you can see that the improvement so the measure of improvement measure improvement measure improvement is would you say that this would be a measure of improvement how much did how much smaller is the null hypothesis I mean the hypothesis compared to the null hypothesis in errors? Would you agree that this would give you some measure? If hypothesis error is the same as null hypothesis error, you have no improvement. Does it make sense yes yes and so what we do is we take this as a measure except that it makes sense to divide it by the null hypothesis why because it gives you a proportionate improvement proportion of portion i'm sure i've misspelled it. Proportion of error reduction. Would you agree with that? Right. And now let's see, does it fulfill this green criteria when error hypothesis is equal to null? I mean, hypothesis error is the same as null hypothesis error. We get what what zero in the numerator and so this becomes zero that that looks right when hypothesis error is absolutely zero in itself e5 minus 0 over e5 would become 1 this looks to be a good measure and indeed it is a good measure there is a name for this measure. This measure. So let me write it. It is called it is the famous R squared. We call it the R squared. And for historic reasons. Of your hypothesis. This there is a name for it. It is called the coefficient of coefficient of determination are we together it's a fancy word and r squared is in linear regression and only for linear regression it is perhaps the most often used measure the most often used measure. Are we clear guys? So this is our score. Can we really quick go through the hypothesis how it is it should be less than that? Yes it's a very good question yes so let's take this example do you realize that the green line let's see the mistakes green. Why is the green not? You know, let me fix the color so these notes are there. So look at the errors from the green line. Do you see how much bigger errors it's making? The green line is making these big errors you see that guys that in average the green line makes bigger mistakes isn't it that's the whole point its predictions are vastly wrong was thatna who asked this question? No, it's Manisha. Manisha. Manisha, so do you see that, right? That the green line is way off. It is just saying the same answer irrespective of what the X value is, whatever the temperature is. It says you're going to sell this Y bar amount of that. So it will make much bigger errors. It just has nothing from the data Saying that the sale of ice cream will be the same no matter what temperature Exactly. So you would assume that if you have a theory if you have a hypothesis or a model about the true relationship It should beat the null hypothesis, right? It should beat the null hypothesis. It should beat the no relationship story. Which is why we say that the error of the hypothesis has to be less than the error of the null hypothesis. Otherwise this hypothesis is meaningless. So hypothesis is the error of a hypothesis is just a random selection. No, of this line of this best fit line that you made to the data. So what you do is remember, you learn, you try to find the best line going to the data. But if it is best, it better improve upon. Otherwise, what is the meaning of best? Otherwise, what is the meaning of best? Best line through data should at least improve and have lesser mistakes, lesser error compared to the. Is that getting question? Sir if you sir, if you generate a new hypothesis, does that mean the next time you want to generate another hypothesis, you have to reset the null hypothesis or do you hypothesis remains the same it is the flat line average so what really happens is that the machine will leave the null hypothesis alone it will try to find the best fit line to the data remember the gradient descent we talked about yes sir ultimately find the best fit line to the data remember the gradient descent yes sir immediately find the best line to the data then it would say all right now that I've did it let me see how good is my model you ask how good is a model one of the easiest things first thing in the class is how much have you improved upon errors of the null hypothesis oh so the null hypothesis will stay static during the learning process yeah it stays because it is just a fixed thing okay so you don't reset you don't reset the null you know you fixed another fixed yes you don't do that if you have therefore this kind of measure model. Unfortunately, this measure, people are so fixated on this. It's so influential. They get a good R squared and they start jumping around thinking that they have made a good model. But actually, and one of the lessons I want to mention is one metric does not prove that you have a good model. There can always be a better model. The fact that you got a r squared which is greater than zero means you are on the right track. It may not necessarily mean that what you're doing is the best thing doable. There may still be reducible errors by using a better hypothesis. I'll give you an illustration. Suppose your data is like this. This is part of your lab. Next time slide. Suppose your data is like this. and how many bends there are in the data guys to band site and so if you try to fit a straight line the best straight line would be something like this let me call it the hypothesis one the best straight line hypothesis. Order 1, linear hypothesis. Now you realize that this linear hypothesis will make less mistakes than the null hypothesis. So R squared of the H1 will be greater than 0 for sure. It would have made some improvement upon the null hypothesis. But is this the best hypothesis? Or perhaps somebody can come and draw a hypothesis, a third degree polynomial hypothesis. A two-bench means third degree. So let me call this H3. What do you say? what you will find that R square of H 3 is greater than R square of H 1 which is both greater than 0 so H 1 is moving in the right direction H 2 is even H 3 is even a more progress in the right direction isn't it guys so you have to know that just because you've got an R squared which is greater than 0 doesn't mean that you have a good model. You should look at other metrics. So what other metrics we look at, we will talk about when we do the lab part of it. But today I want to quickly end with a topic we left unfinished last time, which was the topic of, does somebody remember what was the topic? Bias variance. What is a bias variance? What happens is when you build a model, let's say that you look at this model. Let me just take this as an example. This third degree polynomial. See I could have fit an even more complicated model. Let me take a more complicated model. More colors. What color should I take? Yellow. Suppose I took a model of fourth degree of seventh degree. Something like this. Let me call it H7. Do you think between the white hypothesis that H1 and H3 and H7 which do you think is the best? Anyone? The H3 is the best but guess which will give you the lowest R squared if you try to compute the data. Seven. Yes. What will happen is paradoxically you will find that R squared of H7 will be greater than R squared of H3 is greater than R squared of h1 and all of them will be greater than zero well you say well that sounds unfair so this r squared thing this coefficient of determination is misleading us now isn't it what happened this is an example of why you shouldn't just just the r squared but there is is more, where you say that, okay, I can do a game. First thing I do is I take the data and I break it up into the train data and the test data. Great. And by the way, in Python, these are one line. Easily it will take your data and create two subsets of data, one for training and one for testing. So what you do is you look, you fit your model, the trained data, fit model. Asif, quick thing, because visually looking at it, the R square of H7 seems to be doing much worse than even the R square of H1. Yeah, it's because I should have cheated a little bit on a lot of points on it now what do you say now okay yeah okay so there you go right so what it is doing is it's fitting onto the noise. The yellow line fits onto points that are actually, while most of the points are going through this, it is fitting onto the, it's too flexible, basically. Obviously my depiction is not so good, but the point that I'm trying to make is a yellow line is too flexible. It will just try to fit to the data, overfit to the data remember the overfitting problem so now the thing is you need a test to tell that you are overfitting so what you do is you find the error of each hypothesis on the train and you compare it to the error on the test oh well that is bad because this is strain and test. Okay, let me just use strain and test. So now one of two things will happen. For the red hypothesis, because you have randomly split the data, what do you expect? E train to be much less than a test or approximately the same as test. It will be more or less the same because you have a good model. What will happen for the white one is that, again, same thing will be true. E train and E test will be high. E train will be more or less e-test but for the yellow one something weird will happen what will happen is e-train will be much much less than e-test means your model did very well it just sort of comes out with flying colors on the training data. But the moment you try it with the test data, it blows up in your face. The test error is very high. Are we together, guys? That is a sign, and this is a sign you should watch out for. Because when this happens, probably overfit. Overfit. Fit to data. when this happened probably overfit overfit fit to data you have over fit your model to the data and that is what we are going to avoid now I said that the learning hypothesis let's go back and review our a bias variance straight up the line hypothesis is it simpler than the ground truth, yellow the green lines being the ground truth, the data. Would you say that the line hypothesis is simpler than the actual reality or more complex than the actual reality? Simple. Simple right. So it is making mistakes because it is very inflexible. It is just a straight line no matter what and that is the best straight line you can capture right and so you see that the hypothesis first hypothesis suffers from now let me use the white color the hypothesis one I'll just write it down here. Hypothesis one suffers excessive bias, bias error. It is the errors of being simpler than reality, being too stiff is this. Hypothesis seven suffers from high variance errors. Right, overfitting errors. So now let me give you what happens. There's a trade-off actually. So suppose you have data, you don't know that a third degree polynomial would be best. What you do is you take the errors and if you look at the training error, what will happen is that as your sorry training error no this is not the x-axis this is the training error this is the complexity of the model think of as degree polynomial and it degree. That would be one example of degree polynomial. That's one example. Because if you do polynomial regression, the higher the degree of the polynomial, the more the complexity of your model. So what will happen is that as you train, your training error will keep on decreasing. Are we together guys? But your test error will behave quite differently actually. Your test error will behave like this. Now let's try to understand what happened. Why is this blue line continuously going down, the training error going down with more and more complex models or higher degree polynomial how would you explain it anyone guys to noise yeah it's fitting to noise it's very flexible the more uh you make it the more it will you know reduce the errors by going through more and more points of getting closer to as many points as possible. But then why is the error going up? Because you realize that after a little while it is overfitting to the data, isn't it guys? But it has overfitted to the training data. Because the test data is different from training data. This test data is separate from training data. If you only listen to training data, training data contains noise. If you overfit to the noise, then on the test data, you'll start, you won't do so well. The error rate will start going. Are we understanding this concept? Anybody? Okay. Sir, are there certain data that you can actually prefer more variance errors than bias errors? Or are you really supposed to just meet somewhere in the middle? See, this is a very good question. Your question, I'll rephrase it. Is there any data in which you say, let there be variance errors? No, you want to reduce the error. But there is a different answer to your question. When would you make more complicated models? This is one of the central problems. When will you think that a very complex model, it is okay to build and I don't worry about opening it. The answer to that is big data. You have a lot of data, variance also disappear not only the bias errors disappear variance errors disappear in complex mode so both by variants begin to vary there won't be a situation where you would prefer one kind of error more than the other but when you have a lot of data you have the luxury of considering much more complex models and you can see this deep neural networks are very complex models they are uh their hypothesis space parameter space is sometimes half a billion parameters you know we are having difficulty developing intuition here today in one dimension can imagine development in parameter space that actually two dimension it are not be taken but can you imagine developing intuition in half a billion dimensional space highly flexible model that can fit to think but we dare to go there because amongst many other reasons one of the big reasons is we have today a lot of data so we can tame the problem of variance by having complex model we won't have the problem of bias but we can we will we fear the problem of high waves overfitting it is a problem we will always have to deal with overfitting but the more the data the more it brings down overfitting it is a problem we will always have to deal with overfitting but the more the data more it brings down overfitting i see sir so my question was really uh because because um we're we're between complexity and trying not to overfit but are there situations also let's say you have a two-step two-step machine learning model where the first step you actually want to over fit let's say you want to get the boundaries of a picture or something before you get the finer details situations where you can afford the extra complexity to over fit the extract the extra variance errors that come with more complex. Okay. So bear with me and see if this answers the question. The next thing that I was going to explain. It may answer, if it doesn't then raise this again. But I understand where you're getting at, where you're getting at, what you're referring to. So guys, let us look at this the errors come from two sources the bias errors and the variance errors so as you increase the complexity the bias errors reduce you realize that your model is flexible so bias errors keep going down let me call it a bias errors and we are now talking about on testing data on testing data and the same with the bias errors are we together guys on the test data actually let me remove it because you guys are going to refer to these nodes. Let me put it in small complexity on testing data. So remember the graph on the left is let me call it B and A. B is on testing data only. Your bias error keeps coming down. Your variance errors keep going up. You agree right? As you make the model more and more complex your variance errors will go up will go up does this make sense guys and because your total error is made up of here remember the total error is what e bias squared plus e actually let me just say bias squared let me just put the bias squared down right here bias squared plus E variance plus the irreducible noise you remember this relationship guys the real thing is it is made up Now, we cannot do anything about the epsilon. Can we do anything about epsilon? We can't, right? It is the missing information. But we are playing between bias and variance by dialing up the complexity of the model. But the total error is the sum of these two. So what will happen to the total error? The total error will go like, if you add up these two so what will happen to the total error the total error will go like if you add up these two things it will go like this do you see it guys actually let me use the red line and because then you will see total error do you see this guys that bias error comes down variance errors goes up so there will be a particular point the perfect where the complexity of your model at this point let me call this point p what happens at complexity p let me write them at let me write the word down. So the best model that we could fit to the data is at this complexity. Because that is where the trade-off between, given the trade-off trade-off between bias and variance that bias square people just say bias variance but actually is bias square i won't say bias so this is called the bias variance trade-off. This graph above is the bias variance trade-off. The fact that in reality as you make your model more and more complex, variance errors will keep rising, bias errors will keep coming down. And so there is an optimal point at which the error would achieve a minima. And that is the model that you're looking for. Because what happens at that point? At that point, the complexity of the model, complexity of the model. Why is bias square? This is the mathematical thing. If you grind the mathematics, the way the bias is defined, total error is the square of bias. That's very see variance is sigma squared. Right. Remember variance? Yes. Yeah. So it has the square built in, but bias doesn't have the square built in that's why you need to square it the model approximates the complexity of the ground truth this is an important statement complexity of the ground truth. And this is the point that when you have met the bias we have straight off and found the best model, right, that model P at P the complexity of the model approximates the complexity of the ground truth understanding that I'll be happy to repeat so sir this is the general truth no matter if it or simple data this is general ruling yes you have to take a model let's say a polynomial regression given a line suppose you take degree one you can find the best fit line you take degree two you can use gradient descent to find the best fit parabola and best fit a degree curve and so on and so forth but what you cannot have machine do easily is tell you what is the best value of n so that is why in a polynomial regression n is called the hyperparameter of the model. So remember that in an equation y is equal to or y hat is equal to beta naught plus beta 1 x and I'll generalize it to higher degree polynomial x squared. to higher degree polynomial x squared. And the parameters are. Guess what the parameters are, guys? They're not either one, beta n isn't it? Are we together? These are the hyperparameters, because the parameters of a hypothesis isn't it polynomial hypothesis and these are the parameters but in this there is a hyper parameter What is the hyper parameter? What is it that you just directly learn by breaking the standard? It is n increases the more the complexity measure the more complex the model becomes what you have to do is you have to successively make uh polynomial models of higher and higher degrees but then you know when do you stop the good news is it is easy to stop because what will happen is your test error will first decrease you know test error decreases sorry decreases and then it begins to increase so at this degree whatever m is and there that would be three let us say you know that you have reached the count truth this the moment you notice that your fourth fifth degree you notice that errors are increasing time to stop you have found the best keep on dialing up the complexity while the test error is going down but after, the moment you notice that you test error has started going up. You say now it's time to stop. So I think I go to go in here. The only two never we have is the degree of radiance right biases is anyways being minimized by gradient descent for that degree no no no no no bias and is independent of confuse the two see yes and variants are associated with the complexity of the model whatever complexity you take suppose you take a fourth degree polynomial gradient descent will find the best four degree polynomial to fit your data. you have no no that's E train yes this thing right so now going back to hypothesis one hypothesis seven and hypothesis three right we said for for hypothesis 3, E train would nearly be equal to E test. Right? But that is the closest to the truth. So let's forget about that for now. Hypothesis 1, which is the white line, we said again, E train will be equal to E test. but here the bias will be very high so what happens is most of the errors hypothesis one is coming from high bar and you can't do anything about it you can't learn that from gradient descent because the best line it is still the best line gradient descent has found hypothesis one right if you insist on a straight line that is the best straight line that gradient descent will take you to and that will suffer from high bias errors yes that's what i was trying to form up in my mind that we will get a polynomial or we'll get a what you call a fit by the best fit using gradient descent for that degree for that and that fit will inherently come with its own bias error which you cannot bias and variance error both bias and variance error but you don't know that's in various yeah gradient descent is not able to tell is this error bias or variance but you don't move bias and variance gradient descent is not able to tell is this error bias or variance it's lying to that all it says is total error i reduced as much as i could so i'll say so there's no able you know equation for bias and variance right no there is a equation but this thing doesn't help you yes it doesn't it is a recurring problem guys in machine learning at this moment we are doing you know we are just learning to crawl before we run this is linear and polynomial regression by the way these are the most effective and with surprisingly that this simple models work so often even at this level already realized that you can fit gradient different you can find the best parameter but hyper parameter means you have to build lot of models and space you have to do your bias and there is no magic you don't know the ground truth right you don't know what is the complexity of the what can you do you have no choice build models of varying degrees of complexity try to fit it at me which work best where does the bias way in straight off has the optimal then you know wherever it has optimal point you look at the company you look at the degree of the point on aha so this is as close to the ground truth as we can be so sir like so other hyper parameters would probably be how many you would want to jump how many what you want to do like like like how how many times how many times you you move up a degree or does it does it automatically just go one degree up at a time you should do that see what happens is that polynomial regression like the story is that you would take first degree second degree third degree and you would keep making a plot like this where is that plot you will keep making a plot like this you will be just observing the test and the in the beginning the test error will keep going down then it will reach people in this case it didn't think you will get the best of the lowest test error and after that you go to degree 4 and 5 and again the test begins to climb up the moment it begins to clean up you say okay it's time to stop i found the best model at degrees my question is sir if you're working with big data sir would it would it be time consuming to start with the low degrees if you know it will be moving high degrees or is that really the process yeah see the big big data complexifies the situation Yeah, see the big data complexifies the situation. There are some people who would say that see, we have big data. So we can start with a very complex model, and the big data will tame the variant errors also. Bias won't be there in a less likely to be there in a complex model. model variance errors will be suppressed by a lot of and and we'll do that exercise I didn't see that but therefore start with really complex model it has become very fashionable by the way these days even it's like you know you have to put a nail in the wall instead of using a hammer you go and get the best earth mover or something like that it doesn't make sense so people no matter what they will always pull out very sophisticated models then start talking about gradient boosting or random format deep neural net the problem is the more coffee model you build the more human beings lose interpretability example or even manageability if I told you you you are a diabetic you go to your doctor and you say what should I do I sugar has gone up and how do I get my sugar down and manage my and the doctor gives you he gives then in parameters in the model and he begins to say that in the morning your breakfast should be no more than 400 calories and no less than 200 calories. Your lunch must be this. Between breakfast and lunch, by the way, remember to walk one and a half miles. And you give a very complex set of instructions. You have set the value of 10 parameters for that. And say you must sleep this much and so forth. Would you go to that doctor? Or would you go to another doctor who has a two parameter he will say exercise more and eat less so think about it a bit of the new models is more effective getting the point? Yes, sir. We need the simple, it is called the Occam's razor principle. Occam's razor principle states that for a given data, in a whole set of very complex explanations, but the simplest explanation is the definition of the correct answer. In science there are no correct answers, but we define correct answers the simplest explanation that works That's the Occam's razor principle Even in the presence of big data It is our responsibility as data scientists to search for the simplest model that works and that is a tremendous practical applicability and at some point I'll tell you stories from literally the law courts of companies getting sued because they made complex opaque models and they led to all sorts of biases. I mean, in the political sense, you know, gender bias, discrimination, race discrimination, and so on and so forth. And those guys were not aware of it. They weren't aware because the entire model was so complex and so opaque that, well, it was that. And that begs the question that why did you not try to fit simpler models to the data? Actually, from a legal perspective, a lot of these very fancy models are not even acceptable in certain areas. You have to put it to simpler models. So that is another aspect of the story from a real life situation. This is assuming you don't you don't drop data right you're still using the same data it's just the model as simple all right guys any other questions guys we have asked you go ahead just just to clarify you have given a higher level equation but uh you uh you're saying that there will be a uh certain equation through which you can get the bias square itself when you choose a n degree polynomial, you can still get the pieces of the equation, just the bias, variance, and the lower error. All three of them get it through. Yeah. See, you can compute the bias and variance. These are just formulas. You can do that. So, for example, at this model, right? Sorry, where am I at this point? This point is behaving very strangely why is it behaving strangely give me a second guys yeah this point at this point what happens you can measure the bias error this much is the bias error isn't it sorry this much is the variance error. Where is the bias error? Okay, my line was really crooked. Bias error, this is not bias error. The bias error was here. I don't know what happened to my line. This is bias error, right? So you can see that I can measure the bias error and I can measure the variance error. Isn't it? They are the same. The bias and variance errors are pretty much reaching. I thought you you you can only measure the red color right the total error is the observable error. So this is it you can notice gradient descent in during the gradient descent process but once you have done your gradient descent you want to see what is really happening see the trouble is you have only test data having the formulas of bias and variance doesn't prove very useful for linear models it works in real-life situations you wouldn't know you really deep neural networks you don't know what's happening so certain real real life, the goal is not always to hit point B. It's just to reach the model that's the most effective for your use case. Exactly. You find the model that's most effective. You're in search of the best point given the bias . Where does the trade-off work out? So guys, in practical practical terms you often do that example the we exercise the better our health but but the more time we spend exercises the less time we have to play with our kids and our kids become unhappy you know they say that after coming back from work, you do have a trade-off. So, at some point, but if you don't exercise at all, you'll get sick. At some point, you have to find a trade-off. What amount of exercise you can do and get the maximum help. I mean, as much help as you need it, at the same time, you get time to play with your kids. You know, we make those trade-offs all the time. So it is somewhat like that. Bias and variance trade-offs are like real trade-offs we face in our life. Any other questions, guys? This has been a long day. So a couple of action items guys. If you're not on Slack, you can reach out to me or reach out to Achi or Glenis and they will add you to the right channels. And the other thing is, do follow Slack. I'll be posting messages there. You must be receiving mailing messages. I will send out another survey after this class to see how you understood each of the concepts. Whatever concepts are not very clear, I'll always answer them. So with those words, I'll conclude today's session. Those of you who have at least concluded the recording of today's session, after that, I'll make it an informal Q&A. Those of you who want to stay back are most welcome to stay back.