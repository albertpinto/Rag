 The last time we started covering clustering. Clustering is an unsupervised learning exercise. It's a part of pattern recognition. This is quite different from predictive modeling. In predictive modeling, we look at, we are essentially trying to generalize the relationship between input and output as some underlying function effects such that we can use that function that generalization or that function as a relationship to make predictions in the future now the target variable can be numerical in which case it's regression it It can be categorical in case it is classification. Cats and dogs classification and. Uh, the, the amount of ice cream you would say. Is a question more precisely what would be the home value. Of your home based on certain parameters is regression. Isn't it whether or not given a test you have covet is a classification problem. Based on that system, extra parameters and so forth on the other hand, pattern recognition is not making any prediction. Just saying there are hidden patterns in the data, and those patterns are worth discovering. They need not even be hidden. But for a machine, it's a little bit hard to find out. As I mentioned, the human eye has gone through at least 100 million years of evolution. So for us, we immediately see patterns as human beings, and probably most mammals, most animals see patterns as human beings and probably most mammals most animals see patterns clusters immediately it's a wonderfully intelligent machine the human eye or the eyes of animals but machines of course don't have eyes they have to discover patterns like mad They have to learn the presence of patterns. So that is the field of pattern recognition. In pattern recognition, we talked about quite a few things, clustering, dimensionality reduction, anomalies and outlier detection. These are all exercises of pattern recognition. They look at certain aspects of the data or certain pattern in the data we focused and we are still focusing today so far on clustering clustering is the exercise in which we look for clusters in the data I mentioned that clustering is not exact science because how many clusters you see depends on the granularity or the resolution at which you're seeing. And sometimes it can be ambiguous how many clusters there are in the data, especially if the clusters are not very well defined. And then there are clusters within clusters. You have subclusters and so forth. So all of that makes for the very interesting subject of cluster detection or clustering now how do we do clustering we do k-means clustering k-means is the exercise in which you say I don't know how many clusters there are tell us how many there are that is the value of K if you tell us what K is then it's a simple exercise to find the clusters its primary purpose is to just pick k initial points now you can randomly pick those initial points or you can use a little bit more intelligent heuristics of example one heuristic could be if you're doing a two means clustering, you take a point, a random point, and then you take another point as far away from it as possible. So that would be the choice of initial points. Then what do you do? After you have the initial points, declare them to be the center of the clusters, or the center of gravity, or the centroid of the clusters. One intuition that I ask you to have is think of each data point in the feature space as sort of a mass point in real space. So then you would see areas that are wherever the clusters are, every cluster will have a center of gravity like a centroid and you these initially what you do is you find the centroids then the remaining task is to take every point and ask which centroid it belongs to which is equivalent to saying which cluster it belongs to because the identity of the cluster is the centroid so once you have done that now you realize that initially you started with a guess now what you do you need to have a cycle a loop that repeats the loop is once you have found the once you have partitioned all the data points into clusters now for the clusters recompute the centroid the true center of gravity now for the clusters we compute the centroid the true center of gravity you do that and once you do that once again reassign all the points to the nearest centroid and you keep repeating this cycle till you notice that they're either the centroids are not moving or points are not changing loyalties from cluster to cluster and there are many variants of k-means clustering that use one or the other you can keep doing that so that is the algorithm of k-means clustering there are many adaptations to it for example instead of k-means you can take the median right uh instead of the average, the middle guy and so forth. So that would be the mid-doid clusters. You can, instead of mean, you take the median. We all know what medians are. Sometimes the big problem is that you have a million points and assigning one point at a time to each cluster can be rather slow. You can expedite it using entire regions of the feature space and associating it with asteroid there are those ways to accelerate gaming's clustering there's a vast body of literature starting with the oldest method kd trees a data structure called kd trees and then over the over time people have adapted and created a lot of these uh structures of partitioning the feature space into regions. And those are called metric spaces. And what they do is they make K-means plus 3 pretty fast, especially when we live in the world of big data sets. Now. Asif, is the distance different, like when when I use came in versus the K, medium. No, no, you still use the distance generally and that is a very interesting point to raise. What is the notion of distance between 2 points? As far as key means is concerned. It is silent on the question of distance. all it says is that so long as it agrees with the basic notions of distance whether you use media in manhattan whatever it is it doesn't matter right any notion of distance now just to wet your recollection what is the notion of distance i think that may be worth reviewing so let let me review that. Technically or mathematically in any feature space, any Rn space, if you define a function, a function a function that follows these three criteria that fall that follows the following criteria or condition that the conditions one is that function d of a point to itself the distance of a point to itself is zero in other words if two prime if two points or let me mark it as y if two points in this feature space if the distance is zero if and only if X is the same as Y in other words those two points are co-located they are the same point in the feature space the other thing is that distances are always a positive definite greater than zero and obviously this condition comes to the fact that when they are the same it is you and this is the symmetric this is built into it the distances are symmetric you know the distance from your living room to the kitchen is the same as the distance from the kitchen to the living room is the distance from the kitchen to the living room that sort of thing no and the other distance is that it is called the Schwartz inequality which says that suppose you have a point X and Y in the feature space then if you take any arbitrary point at the point Z then distance between X and Y will always be less than equal to the distance from x to an arbitrary point z plus the distance of z to y right so this path will be longer than this path the shortest path between these two points so this is the shortest path. The shortest path, there is a name for the shortest path. It is also called, shortest paths are called geodesics. It's a term that you'll see me here speak of. People who are mathematically inclined, and especially machine learning textbooks tend to be, and papers tend to be, you will often be surprised by the word geodesic. Geodesic simply means the shortest path. And generally, geodesics imply that your space may not be very linear. It may have curvature and things like that. Otherwise, you would just use the word straight line. What's the difference between a geodesic and a straight line if you look on the surface of the earth surface of the earth is like an apple shape right so uh the shortest distance between two points is a curve it's an arc it is not a straight line. So the shortest distance of the geodesic is not necessarily a straight line. Something to remember. Of course, if you could go through the earth, then you could do a straight line. But if you are bound to the surface of the earth, the way an aeroplane flies or a ship sails, then you have to go along the geodesic. That's the meaning of the geodesic.. That's the notion of a distance. The most common distance that you can think of are Euclidean distances, of course. Euclidean distance between X and Y. It is equal to the square root of. It is equal to the square root of. What is it? X one minus Y one square plus X minus Y two square. Go on. Let's suppose you're dealing with N dimensional space or three dimensional, whatever it is. X and minus Y and. Squash. Do we all agree? Like in two dimensions, it would be just X X. That's right. I just X X that's why I use x1 there's no reason for yeah so x1 component x2 component are we together guys assuming that the axes are x1 x2 x3 right this look obvious, guys? Can I have some feedback? Yes. And if you remember, I talked about the Manhattan distance. is often called the d2 distance the d1 distance between x and y is equal to x1 minus y1 this x2 minus y2 yes X n minus y n absolute value so this is the Manhattan distance and if you remember long ago I talked to the generalization of this to, I mean, cove ski distances. That the. It's a very simple generalization of this between x and y, x and y are let's say a two point distance in arc m, this is sorry, in I m, is defined as, or what is it defined as? It is defined as x, What is it defined as? It is defined as x, the component xi minus yi to the power n and the entire thing, obviously summed over, and the entire thing to the power 1n. So here it is, for Euclidean, it's a square root. Square root is the power half half and this is called a generalization of the notion of distance. So you can play with many distance norms though. From what I know, most of the time you go with just in case you can have these are the distances that you typically deal with but with the notion of distance in machine learning or in mathematics is much broader anything that follows the Schwartz inequality is a proper definition of distance so anything that for us not just the Schwartz conditions are these things these are the definitions of distance. Are we together? And this is the way you should remember it, guys. We are taught in high school only Euclidean distance. But generalize beyond that. It's a very straightforward generalization. Distance is any function in feature space, scalar function, that follows these rules can be considered a distance function. So on a feature space, you can define all sorts of distance functions. And in particular, the Minkowski norms, many, many norms are defined in this space. Yeah. Asif, can you also clarify between difference between norm and metric? You mentioned metric some time back. So what's the difference between those? Another word for distance is metric the function D all the metric the the that itself is a metric the reason this word gets a little bit tricky is that see a lot of this literature is borrowed from theoretical physics so what happens is when the space is curved right or when there's a curvature in space let's say on this on the surface of a sphere or things like that so then people often use the as the. D, X, new D, X, new. So, in other words, if you take two small points, which are like, in which the distances are very small distances between is so this is the Delta D a small distance between two points let me just call it the first and the second point in this Delta X okay so what you do is or DX you can think of it as well I use the word D here which is unfortunate because D also stands for derivative. But let me just call this delta distance. So here, this quantity is called metric tensor. It's a little bit of a mathematical thing. I'm giving it to you in all its mathematical glory, I suppose. This is your mathematical thing. I'm giving it to you in all its mathematical glory, I suppose. This is your mathematical tensor. What it means is that it is not so straightforward. Certain distances are stretched out like a rubber band. So, people often distinguish between a definition of the distance and the metric, but for very small distances, this quantity follows the Schwartz inequality. And so this entire thing is called the metric. So people use the word metric in machine learning a little bit more loosely than the way they would use it in theoretical physics or in uh in for example theory of relativity in space time sort of i'll just have a quick uh clarification uh the manhattan is now m equal to one uh the euclidean is n equal to two uh these two is kind of in the physical world we are able to imagine when i only draw a line between two points that tends to be n equal to two but what would be n equal to three i just wanted to get a little idea because i can't even think about it okay so let me give you an intuition see euclidean distances as the bird flies the Manhattan distance the word comes from Manhattan because Manhattan is laid out on a grid right so the city of Manhattan is like this so suppose you have to go from here to here Euclidean distance would be this but that's not how that would be as the bird flies. But a taxi cannot go like this. A taxi is supposed to go like this and Uber or a taxi go like this or it would go. It has many parts associated with it. It can go like this. with it it can go like this it can go like you know this or any one of these parts it can take intermediate parts people but whatever part you take it will be at this distance plus this distance so the taxi drivers distance is the Manhattan distance or the first norm as the bird flies is the second norm the third norm is actually a little bit harder to distinguish a little bit harder to tell because to be how should I put it it's a little it's a bulbous it's a if you look at the center and you look at all points that are Manhattan distance one unit distance apart circle is like this the this is Manhattan circle in Manhattan distance would actually to our eyes look like a diamond shape and in Euclidean distance of course it looks like the familiar circle but in higher distance it takes to be more bulbous it begins to look like this as you increase the the norm it begins to develop this extra curvature now in this it looks weird but what it means is that it sort of shrinks the space between two points the absolute value of distance will be much much less are we together and I see that distance green it's much larger right the distance between those two points is counting the thing is that if if even if far away points are actually at unit distance in the in the fourth or fifth measure then what it means is that somehow the far off points have been pulled in so think about this if you're going by the red path it is a pretty long path when you go with a green part in a way this distance has been pulled into a shorter distance isn't it the value is shorter the distances keep thinking the absolute value of the distance thinking it's almost like find some some magic way to tunnel from one point to the other just as for a taxi driver cannot but you can imagine to the taxi driver the bird is somehow tunnel through the I got it. Okay. I got that. I cannot imagine, but I got thank you. Yes, so a couple of points I want to ask and again, I don't want to delay your process a whole lot. But maybe if this offer, like, I could take offline 2 points. 1 is the right? Like, from Shanghai to San Francisco in one hour so that could be something I wanted to see is it because of the third normal form is talking about or about his flight so his point is slightly different what he is that see suppose you are here on earth at this moment it's very counterintuitive argument when you when you fly from here and you go along this path you get a lot of see what is preventing you from going fast there are two things in anane one is that the air drag so to overcome the air drag you have to burn a lot of fuel isn't it but then you can go to higher altitude to reduce air drag the trouble with going to higher altitudes beyond 33,000 feet in airplanes for example is that there is not enough air so there is not enough air to even have a lift really is that the aeroplane or what does it do the airplane flies because the engines are let's say that this is the engine the jet is sucking in air and it is pushing it down yeah if you look at the engine you'll see a slight angle here and the flaps what they do it because of the flaps and the shape of it it basically what you have is action and reaction this pushes the so the air pushes the engine up or the whole airplane up that's how airplanes fly so you need enough air to push back if you go to too high altitude the drag will be less but there won't be enough air so your airplane will literally stall and start falling right so the point with uh that alan musk is making is that it is time to go beyond this low altitude flights this is calling it like a rocket taxi right exactly so what he's saying is from here you just take a lift off you lift off and you go straight into the ionosphere so you don't care about air air at all you literally go above the stratosphere into the ionosphere and in the ionosphere you're not relying on the air to take you forward you instead relying on rocket fuel the action reaction you're burning fuel like a lot of fuel the how does a rocket go it emits out a lot of hot gases lot of hot fuel right and the thrust is taking the rocket forward. So the idea is that if you go into this, the speeds that you can sustain, they are more in the order of 10,000, 20,000 miles an hour. So, for example, a typical, for example, the space station, how long does it take to orbit the Earth? 15, 18 minutes. Exactly. It's less than half an hour. So you have literally, you can see a sunrise or something like that. You literally orbit the earth very, very fast. Right? And some of these, you know, flying satellites, low orbit satellites, they also go half an hour later so what it means is that if you go here then most of the time is this time is practically zero because earth is actually a very small planet once you are out here zooming very very fast right your bigger risk is not can you go fast but will you just fly off the earth altogether and so the most of the energy goes or time goes in going out of mother earth escaping the gravity and then later on of course falling back in right falling back in is a control thing it's dangerous but if it is controlled it's not that time consuming but the real time goes in accelerating out of earth's gravity and so you can actually go from here to Shanghai and any of these things in the limited amount of time our half an hour whatever it is that's being projected and most of the time will not be the space travel time it will be the time to launch and the time to gradually land slowly land so they're talking about if you look at geodesy this is smallest distance in some hands you usually don't call it geodesic you see that in view of the accelerations and things like that it is still the best part that you can take if you if you just lift out so here's the thing what you're doing is if you think of Earth as flat let's say San Francisco here and Shanghai here what you're doing is you're going up then you're taking the straight shortest path and you're coming down but the extra the see geodesic would be normal geodesic would be like this but you're not doing that because this is too much air drag and you can't go fast because you're using air flight and you can't have a rocket go through the air because the rocket will literally burn itself out as you know when rockets enter the airspace at very high speed the cone is burning actually burning here you need a very high temperature ceramic corn at the head of a rocket so that the rocket doesn't burn itself out and actually the return vehicle rather doesn't burn itself out that is actually the biggest part of the space right the reentry it is absolutely burning hot it's a fire so you can't have a rocket that goes through air you have you can only have a rocket that escapes the earth goes into the ionosphere travels and comes back and that's the whole point of this the new approaches people are trying they're saying that we are in the post aeroplane either airplanes are way too slow and we need to go beyond that if it has a fundamental limit you can't go beyond a certain height or altitude because there is not enough air and you can't go beyond a certain speed with these jumbo jets you guys probably know that right once you come close to the sound barrier there's massive sonic boom and the airplane itself goes through huge stress you can't take a normal aeroplane like a Boeing something and try to reach the sound barrier the entire thing will just destruct itself there'll be too much damage so you need to make a very very hardened plane small planes that can cross the sound barrier so which are not transport vehicles which are just military vehicles so the space I mean air travel has leased a fundamental technology limit we need to use space for travel anyway that's a digression for the destiny yeah but it's see what happens is that when you think about what distance is it's a very fascinating subject Einstein thought about it all his life what what does space-time mean what does distance mean and he came up with the theory of relativity and that work is still continuing my work in the doctoral work was also related very much to this so obviously if you ask me questions in this space I can just go on talking for hours but coming back to it in K in K means clustering what we do is we just find a way to determine or identify whatever number of clusters you want to see if you want to see three clusters it can find three clusters and then the question remains what is the optimal number of clusters and that's a hard question to answer the best way you can do it is you have to look for the so-called elbow you have to draw the scree plot you have to look for the elbow in the script if you don't see clear elbows as we saw in the lab in saturday it could be either because there are no clusters or because k-means is not the right algorithm to find the clusters k-means has one big limitation it looks for globular clusters roundish clusters convex like clusters if you don't have convex like clusters like as we did in the lab and then this k-means doesn't call and the whole family of k-means doesn't quite work. Fortunately, there's a last class of problems where it sort of suffices. So, it's a very popular using clustering algorithm. It has spawn a whole sort of a sub field of clustering in itself. The enormous amount of variance of K means clustering. the enormous amount of variance of K-means clustering. I haven't talked about it because it's an introductory class, and we can go on about it. We can literally have a 10-week workshop on just clustering methods. So there is expectation maximization, and there are many, many variants of K-means. Expectation maximization would say, what if a point, we don't absolutely assign it to one centroid. You say that 90 percent it is in this and 10 percent it is in that so when you start distributing the probabilities the generalization of of k-means becomes expected in maximization and there are many many things now agglomerative clustering is different agglomerative clustering if you remember it breaks up the space into what it takes all the points calls all the points are cluster to begin with and then it starts agglomerating it you know joining the points together and making bigger and bigger clusters and as you rise up you have the choice that at the end of it you have just one big cluster so depending up, you have the choice that at the end of it, you have just one big cluster. So depending upon where you cut the dendrogram, this hierarchical representation, you can get as many clusters as you wish between one and the number of points. So in many fields, in medical domain especially, this is considered very, very useful because people often look at the problem at different levels of granularity. There's a lot of dynamics going on there in the body, in the chemicals and chemical reactions, metabolic reactions and whatnot. So there's a lot of interesting things happening there. But today, so that's a review of last time. Today I'm going to teach you a completely different kind of clustering called density based clustering. So what do we do in density based question density based cluster takes the same analogy. Which is that if you have points in a cluster, think of all of these points. So, actually, if we take bigger size points to illustrate. Suppose you have a situation like this. And let us also take a few outliers, things that don't really belong to point to either of the clusters. So you notice that there's a very peculiar situation. You seem to have two clusters what you would ideally like to do is say I have this cluster and this cluster and the rest of the points are outliers right density based clustering is very good at doing that but the way it does it is it applies the note the notion that points have mass and so wherever the points are, wherever you have clusters, you have a much higher density in those regions, in cluster regions. That makes sense, isn't it? For example, if you take a unit space, there will be far more points in the unit space in a cluster than outside it then let's say here or here does that intuition make sense guys yes yes it's a very basic common intuition and so we use that intuition and this work was quite interesting it came to the end of the last century you would imagine in hindsight that this could have been discovered much before, but surprisingly, it actually came quite late. And it's based on some very interesting mathematical arguments. The first variant was DBSCAN. This paper was written by, I believe, two or three writers, and this paper was considered such a fundamental improvement upon clustering algorithms that it was immediately awarded the best paper of the year award in the field of machine learning if i remember right so what is this it actually takes a little bit of an abstract uh perspective on clustering so i'll take you through a slightly abstract mathematical journey and you may for some time wonder what am I doing? Why am I talking so abstractly? But just bear with me. It's not a long journey, just a small journey. So we will establish some basic concepts or notions. Concepts. One concept is the epsilon distance. The second thing is a direct neighbor, direct Then interior points, there will be boundary points and these things will become very intuitive in a moment. Boundary points and then outlier points. So these are all attributes of each data point. The epsilon distances. So, let's start with the very basic. We say that use a cutoff of it like from a point you are, you have a very small lamp and it can shed light only within a certain distance. Or another analogy would meet that imagine that you have at the point at each point at any given point you tie a string and that string is only this long so you can visit only this distance you can only visit a circle of radius epsilon isn't it from point X visit This looks obvious, guys. You cannot go outside this. This is your region that you can visit from X. If all you're allowed to do is go epsilon distance. So far, so good, guys? So that creates the notion of the point x. So far so good. Now you ask this question. Is another point, so let me call this point x0. Let me mark this point x0. Now you ask this point, in this region, the epsilon neighborhood, does another point exist? Let us say that there is such a point. Let us say that this point exists, x1, which I'll put at this point. So this distance, you would agree that distance from X 0 to X 1 is less than epsilon right would you agree yes so if this is true if and only if this is true we say X naught and X 1 are direct neighbors, directly reachable. Reachable from one another. They are direct neighbors. But then imagine another point like X to what about X to what can you say about in the direct neighborhood X 0 what's the answer to this guys no it is now so the answer to that is no it is But x2 is in the epsilon neighborhood of x1, isn't it? So for example, let us see this. If I make an epsilon neighborhood, oh, actually, it may not be true. Let me redraw x2 and make it a little bit closer to make my argument. Otherwise, it will be outlier or something. It will become too far. So let me put that point here, x2. So x2 is outside the direct neighborhood of X 0 but it is if I look at the epsilon which is this much I realize that the epsilon neighborhood of X 1 is this are we together where this distance is epsilon. Right. So X2 belongs to, let me just mark it like this. X2 belongs to the epsilon neighborhood or direct neighborhood of X1. So we say, So we say that x2 is reachable from x0 and directly reachable from X1 right so how can I go to X2 I can take this path I can take an indirect path would just happen I can take a much more indirect path I can go from X to X 1 and from X 1 to X 2 do you see that it's a two-hop journey X to X 1 and from X1 to x2 this is possible do you agree guys if you look at it so they are reachable from each other but they are not directly reachable X naught and x2 are not directly reaching. So this is all we need. Now comes an interesting point. We consider, we consider or we define an epsilon neighborhood, neighborhood of a point to be dense. This is the word, dense. If we can find some neighborhoods n directly and neighbors inside the neighborhood. What just happened? I have no idea. I'll rewrite it. So, for example, if I have this point X and I draw the epsilon neighborhood and I have points I'm sorry our points X if you call this X 0 X 1 X 2 X 3 X 4 and let's say x5 x6 you can clearly see that this looks x7 suppose I have n is equal to 5 I'll take some number 5 so are there 5 points in this neighborhood the number of points in the epsilon neighborhood of x0 are how many points do we have we have seven points isn't it so if the number of points in the neighborhood of a in the epsilon neighborhood of a point exceed n and we can pick n a sensible value of n 3 5 whatever then we consider that region dense now does it agree with common intuition dense regions are for example dense housing when you live in the suburbs what is dense you you can practically share you know books are stuff with your neighbor by stretching your hand out of your window and your neighbor catching it in the Bay Area that's what that would be considered dense living on the other hand but if you're in the middle of let's say Idaho or you know the heartland of US any one of these states in Wyoming then your neighbor maybe you have to get into the car and drive half an hour to one hour to visit your neighbor, right? So those regions are not densely populated. They are very sparsely populated. Whereas if in a certain diameter, if in a certain radius, you can find a lot of neighbors, it is a dense region. Do you see how it agrees with common intuition guys any feedback days are you understanding this is a very in so how you define n yeah so those are the hyper parameters of the model we need to pick sensible values of n and epsilon that's the that's the tricky part based on the domain based on the situation the hyper parameter of the model are hyper parameters are parameters so you have to agree on a sensible epsilon and n and n people often also call it the endpoints BTS number of points that you must have in the neighborhood right this word is also used but I'll just use any now the thing is if you so once you have chosen this now what can you do you do the following the algorithm is this. Take an arbitrary point. Take an arbitrary point. Let's take it. point excise then find it and find it's epsilon neighborhood. and in its endpoints in its neighborhood word if not It may be an outlier you don't know right if not it is not It is not an interior point then what you do you can just sample around and try another data point, another data point, till you find an interior point, a point which is, whose epsilon neighborhood is dense. Are we together? Then find an epsilon, so find an interior point. epsilon neighborhood this is it sooner or later you'll stumble across an interior point now what you do is from the interior point reach out to every point that you can reach out directly or indirectly find all points find an interior point find all points reachable from point x. From x. So from x. From x. Into your point x. Are we together? So what it means is intuitively, let me show you what will happen suppose you have a cluster like this let's take an example and this some outlier points suppose you end up with an outlier point you'll end up ignoring it right sooner or later let me color the points that you'll end up with you will end up with this point right let's say that your epsilon is this much this is the size of your epsilon so what will happen uh let me draw more points here i'm sorry uh more points here there's this there's this there's this it you make it dance so what will happen soon suppose every point that is reachable from this point you start coloring red you'll start end up coloring these guys red isn't it guys huh you'll end up coloring these red now from those points again more points are reachable and gradually all the points that are reachable will end up becoming like this are we together so find all the points reachable from X any so you can start with any arbitrary interior point and you'll end up with the same coloration isn't it and you won't end up reaching this cluster at all let me call this the cluster a and the cluster B you realize that if you start with the point any interior point in a you will end up coloring all of a but not B or not the outliers this is looking obvious guys yes it is that. So what you do is you declare it to be a cluster. Declare it to be. We just found a cluster. Then what you do is you take another point again, try to find an interior point. Sorry. Let me just pick another interior point. Is it like you keep changing the centroid and measuring the distance from the chosen point to all of the points? Yeah. So what we do is let's stick with this point. So suppose you take this interior point. What will you do? You will find a circle here. You will end up saying, OK, suppose three is my number of neighbors I need is three. I end up coloring these as my neighbors. Then from each one of these, again, you draw another circle. You will end up picking up more points. And then from this, if you end up at this point, we'll get colored because you're continuously drawing circles. Each of the interior points, so you will end up gathering all the interior points. Does this make sense? Does it make sense? My question is is so the circle is of the diameter epsilon right? Radius epsilon yeah. Radius epsilon so which means that okay the cluster just keeps expanding as much as possible right? Yeah so in the beginning you don't think cluster what you do is you reach to every point reachable from my interior point it you're in that hmm so you will end up picking all the points and you declare those points to be your cluster okay Okay. Okay. We found another cluster. So you keep on doing it after a little while, you won't be able to find like these points removed or whatever points remain after that, you'll realize that they don't fulfill the requirements of being interior points. Because they don't fulfill the requirements of being interior points, you just declared them to be outliers so asif so for this outliers so if let's say there is another point close to it so let's say there are only two points so will it be a considered a cluster or no no I'm sorry repeat the question if there are only two points so let's say like between A and B, there is one point, right? So let's put another point very close to that. A and B. Yeah, between A and B, cluster A and cluster B, there is one point, right? You mean this point? Okay. Let's put another point very close to it. So is this considered a cluster? No, it is not considered a cluster but i'll tell you why let me first color it properly i'm getting my colors wrong okay so you're seeing these two points right right now let me just give them name pi sigma pi sigma are not a cluster because this is not an interior point what is is the definition of a point? Suppose your n is equal to five. Does it have five neighbors? It doesn't have five neighbors. Okay, got it. Yeah. So this will still be considered an outlier point. Since Since. Suppose if there are three points here, let's say pi, rho and sigma. Right. Now you declared n equals to two, then this will become a cluster. It will become a cluster. Yes. Then it will become a cluster. So that is why it is very sensitive to the does not have n neighbors. So that's the point for something to be considered an interior point and therefore the starting point of recognizing a cluster. You must fulfill the conditions of an interior point. You look at the epsilon neighborhood and you must find at least n neighbors if you don't do that now the problem with this algorithm db scan was widely successful but it has to do me I mean the main problem that it has is the hyper parameters like based on the hyper parameter epsilon and then the clustering is very sensitive to it like for example if a number of neighbors is two then these two outlier points that we talked about are pi and rho they'll become a cluster isn't it but for a pa for n is equal to three or more a pi and rho don't are not a cluster they're outliers. And what we realize is that the shape of the cluster also tends to be a little sensitive, quite a bit sensitive, to epsilon and n. So that was the weakness of dbSCAN. It was that it's very sensitive to the hyperparameters. But while that weakness is there, it's sort of very effective. It works with non-globular clusters and so forth like we saw in the lab last time Saturday we did the lab and we saw it it works just fine with non-globular clusters so that is DB scan guys now trouble with DB scan it it doesn't scale to big data you have massive number of points in your children a vast classroom it's a little slow to scale so people who are searching for more scalable algorithm and they came up with another algorithm which is which is also density based clustering but these days was thought very well of again clue then so then clue is already into the include to doctor and this and that these days was thought very well of again. Then, so then two is already into the include two doctor and this and that, and Google has an implementation, very high speed implementation and so forth. This is one of the very beautiful density based clustering algorithms. I'm going to talk about it. Let's take a five minutes break and get back in some water. Let's have a five minutes break and we'll come back here in the meanwhile let me do of course the recording for a moment so far for DB scan when it scans a point it the algorithm just rewards it rewards a point it doesn't penalize a point for not being part of a cluster yet right because if it picks if it picks a boundary of a cluster as an integer point, it might not have enough in its neighborhood, but it might be reached by another point, right? That is right. If you look at this point, this point does not have enough neighbors, but it is reachable from one interior point. You see that you can reach from this to this, isn't it? And so these points, they're calling out the point. And so when you discover this, you will. With India points and boundary points. You start from a point and ultimately you'll stop at the boundary points. Are we together? Yes, sir. So if picks if the boundary point is an arbitrary point first it will not penalize it it will just skip it and say it can just reference the point back when it realizes it's a boundary point okay that's why it takes time okay see. See, what happens is, if you just happen to pick either a boundary or a point, right? The point, it won't have enough neighbors. So, you know, that you started with the wrong guess. Let you go about hunting for a point. Now, that into your point might ultimately find out and reach this boundary point. And so then that into your point will end up owning the boundary point in its own cluster so did you get that Patrick it's a simple art it's a very simple approach but you can make a little bit of a side so sir my only question there is what if that that boundary point is close also to another cluster let me answer you let us take this sort of a situation to illustrate your point suppose you have a situation that you have boundary points link you have points like this and then you have this point and then this point and then this point these points are here now the question is and this is where the dbScan becomes very sensitive to what your epsilon neighborhood is. Suppose you start coloring points. Suppose we start going about coloring our points. Where is the red point go? So suppose you start here, you'll end up coloring all of these points then the question comes you reach this point you have reached this point from a deer point isn't it you went like this you reached here so it is still part of the red cluster the question is what is the size of the epsilon neighborhood suppose your epsilon neighborhood is this big then what will happen it will end up grabbing this point isn't it even though it was a boundary point it could reach this point because it could reach this point it will start fanning out here. And so it will end up discovering that there is actually only one cluster. Because everything is in the epsilon neighborhood of, let's say that this was your initial point. Everything is reachable from that initial point. Do you see that, Patrick? So you would say that you just have one cluster if your epsilon neighborhood is this big. On the other hand, if your epsilon neighborhood is this big, only this big, then or just this big, then what will happen? This bridge cannot be crossed from here to here is epsilon. You can't cross over to the other side, to the right-hand side, isn't it? And so what you will end up with in this case is this as one cluster, and this as another cluster. Yes, sir. I understand. What I think, I understand what the point that crossed over what if that's what if that's a boundary point for either of the clusters it can get it can get selected to either the cluster or the right cluster based a secret if if you start here and you fan out the only question is is this point within epsilon let me let me name these points alpha beta is beta within epsilon neighborhood of alpha or not alpha is certainly a boundary point it doesn't have enough neighbors but is alpha and beta reachable from each other directly if they are reachable from each other you started from this nucleation point you'll end up grabbing all the points the once you have grabbed a point it doesn't remain for another cluster to also claim it as its point you realize it it is out of scope now for the next cluster i understand sir but if beta if beta it looks like a bow tie so the if beta is the middle point and it's it yes it there's not enough to breed to find out to another side so what it's to another side so what it's a central point central for the time being yeah so the only thing that matters is whether from alpha, so suppose you start from this direction from alpha, can you reach beta or not? You can pick an answer either you can or cannot. If you cannot, then beta stays sitting there. Likewise, suppose you start from here and you reach gamma. Either you can reach beta or you can't reach beta. Let us say that both from alpha and from gamma you can reach. Let us say that this was your starting point. From alpha you can reach to beta. From beta you can reach to gamma. And from gamma you can reach to everything else. You can reach out to everything else. So therefore you have just one cluster patrick are you getting the point yes sir i understand i understand from from alpha you can get to beta but but beta going to gamma also if they're assuming alpha and alpha to beta and beta to gamma equal equidistant that means that means you can you can pull the whole cluster it pulls the whole cluster of course see you keep finding out every point that is reachable from a point that has already become reachable. Is now part of your cluster, so let's make it very real these days. We talk about a thing, right? So there is 1 person. He infects 5 people who infects 5 more people within 6 feet of him. It doesn't matter that infection will keep on spreading. There is nobody within five or six feet. Remaining isn't it so suppose fifty people have gotten infected. When will the growth of that infection stop when nobody comes within six feet of those fifty people? That's when that cluster will stop forming okay sir then both of these both the entire boat I will get infected as one cluster okay sir I understand so as long as a point gets converted into that cluster, it can be, it can serve as another point to reach. Okay. Okay. And then if it reaches a point that can scan again within the epsilon, it will convert an entire cluster. It will convert the entire neighborhood as other as the same cluster yeah and actually infectious disease is very intuitive a thing that you start with one initial point which is an interior point think of him as the you know the case zero the patient zero and he starts infecting so the person will keep on infecting till there is nobody to infect why because all other people are too far away so all the people that the person infected directly or indirectly that whole infection is one cluster and in fact that's how you think about it in epigeniology as one cluster so that's that so as you can see this is very sensitive to the choice of epsilon the size of your neighborhood and the number of points you expect in the neighborhood which has been a weakness sort of a very unlike sort of a sensitive to that it's not quite a weakly uh ask if another question uh so between like dv scan and k-mean what will be a situation when we should be using k-mean over dv scan see generally right k means is cheaper in lower dimensional spaces K-means is cheaper in lower dimensional spaces. In very high, I mean, see, here's the thing. K-means is relatively cheap because you just have to find distances to points and find the centroid, right? But for each point, you just have to find the distance to the centroids. But in a dB scan for each point point you have to draw the epsilon neighborhood and see what other points are in the epsilon neighborhood isn't it so computationally db scan is more expensive however db scan in general gives better clusters broadly it tends to give better clusters but the trouble with db scan is is very sensitive to uh epsilon and n so now i'm going to tell you another algorithm which is actually superior in the sense that it is massively scalable very very fast and is not so sensitive to the hyper parameters so that that thing is called dentally if you guys can I mean if you have to they tend to like if you're computing computing resources can sustain it preferably go with density based cannons they're superior they give you better clusters their lessons unstable if your clusters are non-globular right so to establish dentle we need another mathematical concept a concept of influence function influence right what is the influence function Think of it as like this. Suppose you have a source of light. Imagine a little lamp. Now the lamp will shed light only up to a certain distance, isn't it? further and further away from the lamp, the light from the lamp will keep decaying, isn't it? So if you were to look at the intensity of light based on distance from the center, distance from lamp, from flame, of the lamp, would you agree that the distance would go something like, it will decay. It could go like this. It could go like this. It could go whatever it is, but it will decrease with distance. The light intensity, the light intensity, and it's a very general, by the way, light intensity. We know exactly how it goes. It goes one over R squared. But imagine some other sort of influence. But one thing we are sure that further you go from the lambda, less the influence. Does that intuition make sense, guys? Or another way to put it is you can think of this point as a heavy point and it is pulling other points towards itself and so forth. I don't like that analogy, but OK, let's just use the light analogy, light intensity. functions, the distance between X and Y, let's say that X is the source and Y is some arbitrary point, is some decay function. These are usually called the influence functions. Now comes an interesting thought. Suppose I have three points. One, two, three. X1, X2, X3. Right? Three points. And I'm looking at the total influence at a given point X on the point X from X 1 X2 how will you do that you would definitely say that it is equal to FX whatever function it is X X 1 plus X X 2 X X 3 would you agree this is the X point or the light intensity at this point X let me color it something else at this point will be the sum of the influence from the three points right do you agree with that, guys? The total light intensity. So suppose you are sitting here and trying to read a book. You would agree that the light intensity will come or the luminosity will come from three sources and all three lamps will contribute a little bit to the brightness of light which is helping you read a book at x. of light which is helping you read a book at x does that make sense guys yes sir and so you can say that the influence and so i'll write this equation the total influence at a point arbitrary point at an arbitrary point you say arbitrary point X in the feature space Rn is summation of f and all points xi that are available. Let me just call this the total influence function. The total influence at x would you agree guys actually let me use a symbol that people tend to use or rather I tend to use, I'll just use a phi a total influence at this point, just to distinguish it from the f, f is the influence from each of the points, would you agree guys so far so good, now the question is what do you distinguish it from the F. F is the influence from each of the points. Would you agree guys? So far so good. Now the question is what do you choose as effects? One common choice that people love and it doesn't really matter which influence function you take so long as it decays with distance or the word that people use is it's a kernel When you go to heaven 200 you see me use the word Colonel so long as it is a kernel. It is fine one common decay Influence function That people uses this the Gaussian function the bell curve kind of a function they will take the function X X I the distance is equal to e to the minus D the distance between X and X I distance squared over 2 Sigma squared now Sigma is sort of the measure of the neighborhood that you are willing to consider it doesn't matter so just think of it as a sort of a hyper parameter or actually doesn't make that much difference okay we'll keep it there for the timing now what is the nature of this function it it decays with distance. So far, the points don't contribute much influence. Nearby points compute a lot of influence. So does this look familiar, guys? We have done the bell curve so many times that this should look very familiar now. The normal function, the Gaussian Gaussian function let me just write up to a normalization I'm forgetting that I'm forgetting the normalization coefficient right I forget the normalization it's not mentioned so it's a number that will go from one to zero now comes a argument that makes it very powerful see what happens is that you can suppose there are two source two sets of points let me make two clusters. And another cluster, let me just take it like, oh sorry. This is terrible. One second, I'm trying to make it green in color. Why is this not becoming green? Green in color. So you have this. Now think of a point here. I'll take a red point here or white point here suppose you have a point here this is this point is your X now you realize that it is coming under the influence of both the clusters but if you look at the in you ask an important question in which in which direction from X should we go for maximum increase of influence of phi x. The phi x. So in other words, I can go here. I can go here. As I go here, these points will have different amount of influences on them. But the question is, in which direction should I go to have a maximum increase of phi? So that direction you would agree would be a direction like this, isn't it, guys? Somewhat like this, isn't it? This would be the direction if we make it like this, somewhat like this. Would you agree? Because at the top here, this is the most illuminated point. And this is the most illuminated point of the green cluster. The nearest cluster here will exert far more influence than a distant cluster on the point X. So if I come closer to the blue cluster, very rapidly, the light will increase as I come closer and closer to the blue cluster. Whereas if I move closer to the blue cluster, very rapidly, the light will increase as I come closer and closer to the blue cluster. Whereas if I move closer and closer to the green cluster, the light will increase more slowly because the green cluster is far away. Does that make sense, guys? So that is the intuition. That's a very important intuition. And you can say that more mathematically you can say that you can take a part of call great like like a hill climbing algorithm climbing and climbing what you do is you follow the next point. The next point, let's say that X prime is equal to the previous value of X plus take a small step. And take the gradient of phi X. Now, does this look very familiar, guys, this equation? It is the opposite of gradient descent. If I replace the plus with a minus, what does it become? It becomes the equation of gradient descent, isn't it? This is the gradient. this is the gradient of the influence function so in other words you take a small step in the direction in which the gradient is the highest you know the maximum increase in the influence function does this journey make sense guys I hope this by now should look really yes sir it's clear straight and so what will happen is you will end up very quickly rising to a point and I'll mark it as a red color or let's give it some color that i have not used yellow on white doesn't look good okay let me use this color you will end up with this point here this journey will very rapidly take you to this point and then suppose you start from another arbitrary point let's say here very quickly the journey will take you into this to this point these points are called these are called attractives these are the points of attraction that is where the light intensity or the influence Would you agree, Dechi? Local maxima. In other words, the following condition will be true. In other words, the following condition will be true. This is a bit of calculus. If you know it, it's good. If you don't, don't worry. The gradient will vanish at maxima and minima, the gradient vanishes. In other words, it's flat on the top of the hill or the bottom of the valley. You remember, there's no slope there. There is a second condition, which is the Hessian, which I'll just mention, but you don't need to remember that. The Hessian is negative. Hessian is defined. In fact, if you're interested, I will just tell you what Hessian is negative. Hessian is defined, in fact, if you are interested, I will just tell you what Hessian is. It is d phi dx1 square. Suppose you do x1, x2 in two dimensions. If you do the Hessian of a function phi, which is made up of just two dimensions if you do the Hessian of a function phi which is made up of just two dimensions let's say like two dimensional space then it would be a phi square d x1 dx2 d square phi dx 1 x2 and this is a this is a bit of calculus if you don't remember don't at all matter and frankly it's of no practical use at this particular moment but it is just worth knowing that there is such a thing as a Hessian because the literature keeps talking about it it is a generalization of the second derivative if you go back and look at your old high school textbooks you realize that at the maxima the Hessian stands to be the second derivative tends to be negative and so the same thing applies here maxima of these so forget the mathematics all you're doing is you're finding intuitively the points of highest luminosity when you find the points of highest luminosity then then what do you do? You set a cutoff of how much influence you have, you know, what is the influence at a given point and you put a certain level of cutoff on the influence that if the influence is below a certain threshold or the light at a given place is below a threshold, you'll consider it to be a dark spot and all the illuminated points are clusters right whereas points like this are not in the cluster because they are not sufficiently illuminated does that make sense guys reasoning here very simple reasoning you keep following the light and if you try to find the nearest brightest light spot in essence when you reach that you have found an attractor or the Hilton the most illuminated point and then that will you that the zone of illumination is a cluster now why is this algorithm very or very very scalable or fast or for one thing you can see what you can do is you can start with a sample of points and with a sample of points you can find the attractors once you have found the attractors all you have to do is for every point you can just assign it to the nearest attractor the computation can go very very fast from that in fact you don't even have to assign the attractors once you have found the attractors forget about the points you can just say that you have found the clusters and if somebody asks you which points belong to the cluster then you can easily go and compute it that okay these are all the points that belong to my cluster and those are all the points that belong to that cluster and so forth right so it's a very fast algorithm. It scales very well, which is why it's well considered, actually, in big data circles, in scalable circles. One reason, for example, the likes of Google, et cetera, keep having high-speed implementation of this. So this is all there is to clustering. We did three methods of clustering k means clustering hierarchical clustering and density based clustering so then clue it's a little bit hard to find python implementations but you do find c implementation c++ implementations are there you can use it what happens is a lot of these dental algorithms are made into executable libraries you can just fire it onto the data set and run it and so forth sometimes so you can use it or maybe I don't know I have not checked the latest situation where the dental is available in Python or not maybe if you guys find a den clue implementation let me know oh yeah somebody has implemented a dental in python now comparing clustering algorithms okay then to to get a python then to algorithm for python so yeah some people have done it they have done it in pure Python generally when you do it in pure Python as you can imagine Python is a slow language so it doesn't scale very well to the computations but typically when you use kick it learn or you use a pie torch or tensorflow but Python is just a thin layer over underlying C C++ implementations. Then it runs super fast. So if you guys find an implementation that under the covers is a high-speed C C++ implementation but has a Python binding to it, let me know. I've been found it. I use it straight from C C++. I haven't found it. I use it straight from CAC++. So, well, that is it, guys. That is the topic of density-based clustering. Now, I was hoping that we would finish clustering a little bit earlier. Then I could have talked about dimensionality reduction. But let me keep dimensionality reduction for the next time. And let me take now only questions in clustering. If you guys have any questions in clustering, you can ask me. Sir, when does DENCLU break off an area into more than one cluster if they're close to each other? See, once again, there's a signal you have to choose what is the minimum density or the minimum influence which you want to do the cutoff right so that is controlled by that Sigma and so forth so you have it is again a choice and based on how much of where do you cut off it makes all the difference it is up to you so let me let's get this see for example is this why is this suppose I have a white point here is this also part of the I'll put a question here is this point a part of the cluster or not it depends upon where you where you where you put out that cutoff on the influence total influence if you see my total influence should be at least let us say 0.1 I'm just taking a number 0.01 then this point may or may not fall into your cluster it may be declared an outlier. It can fall into your cluster. Are we getting it? That's it good thing with dentro is see, it's a field theoretic argument people who come from. Field theory people like me, we get very excited when we see functions like this right a function that operates this influence function that operates on the space the entire feature space and this function is continuous differentiable and you can do calculus on it we get very excited because what it means is that we are not doing discrete mathematics now we're doing continuous mathematics and in general you can do it lightning fast you can find your answers lightning fast even when you have a large scale data and that goes a little bit on to the technical side read up the papers on denture then to to dot oh let's see if you can understand it usually then to most people find it a bit scary but without some background but now that you have a background you can actually why don't we do that right now let's search for a paper on dental I'll do it together with you and let us see if we can make head and tail of it together I'll just arbitrarily pick a paper on dental let me share something share the main screen share the main screen suppose I look at this are you guys seeing my screen are you seeing the whole screen or only a limited part of the screen seeing a quarter right left quarter okay let me solve that quarter screen problem or maybe the other way I could solve it is not sharing the entire screen but by sharing only the browser share of browser which one is a big share still a quarter still a quarter ok then let me share this is a serial limitation of Webex give me a second I'll have to decrease the resolution Srini is this a problem that you guys are aware of She needs. Is this a problem that you guys are aware of? For four K monitors, it becomes a bit problematic. Work your resolution. We search for denture then if we can see your screen oh you can't see my screen okay how about now can you see my screen now yes yes so if i look at den clue and just say a tutorial so it's always good by the way one good website is not to go to google but go to scholar.google.com usually what happens is I have a special icon if you see this icon here in my Chrome I keep a special icon just for Google Scholar because it's a very quick way to find high quality papers on any given topic so we'll do that now topic so we'll do that now. Introduction to a DENCLU. Let's see what comes up. DENCLU, citation, PDF. The first one. So here is a paper on DENCL employees a cluster so let's just scan through it and see if it is beginning to make sense there's a Gaussian kernel which of course we understand and this is it see look at this it is looking at the influence functions and trying to find the maxima of the influence functions. Now, does this make sense, guys? This update rule make sense? It is nothing but the gradient, which they have normalized it by dividing it with. They've normalized the gradient by some quantity. But it is very similar to our update rule isn't it just adaptation of the upgrade the total influence at a given point is again there this and you you take a journey to that I mean this language here is much more complicated little bit more complicated than the way I explained it to you but this is it do you see this guy's it all you're doing is find these attractors, these points where the light is shining the brightest. And that is it. This will go into a lot more. Actually, the notation of this is a little bit more complicated than I would have liked. Maybe I should write a little article explaining it in simpler terms. Dental Clustering Algorithms. A new clustering based on KNN optics. Okay. Dental IM, a new app. Yeah, so usually these things should be good comparative study of various clustering algorithms in this thing because well it explains dental it will hopefully do it in a simpler way yeah density and all you do is I don't know what dot and dot mean they don't seem to use their peculiar notations, which is what makes these things actually most of these ideas are very simple ideas, but they tend to get very complicated when written out as papers. Okay. This one we already looked. Let's look at this guy. This is not in CGI. It will ask me to log in with IEEE. let's try you down there to let it exceeded the server is gone okay clustering techniques for large data sets about the tutors client let's let's see how good tutors they are clustering techniques for large data sets application to marketing hopefully they'll come to K means we know linkages are your density estimations this is the thing we are coming to yeah so do you guys see by the way them the minus is not visible here where the mouse is but you see that this is exactly what I explained to you guys the density function the influence function do you see this guy's the total influence right it's accurate that's a very basic idea and what it means is that but this influence function has peaks those peaks are called attractors this one doesn't seem to have mentioned that is it actually it's surprising how little good literature for explaining to people is available from some of these algorithms which are used massively so maybe I'll write some notes I'll add a chapter on getting to you to our notes explaining it and of course do refer to this video with was this explanation clear that is the way I explained it to you on the writing book yes yeah so we basically explaining that's all it is if you think in terms of light shining it will make it very obvious to you think of the total light shining at a given point from all the points if you treat each of the sample points as a little lamp casting a light or you can think of it as stars so you ask at any given point what is a total star light and which point is maximally lit. Those are your attractors. That's all. The intuition is very simple. Unfortunately, when you write it in formal language, then a lot of notation creep in and makes it a little hard to understand. But that's all there is to it. There's nothing else. So that's all I have on clustering. Now, this weekend, Saturday, I would like to introduce a topic called dimensionality reduction. And I would have liked to do the theory today, but I think we have gone on. Are you guys game for another hour and a half? Or should we keep it for next time? We can do it today. Are you guys up to it? for another hour and a half should we keep it for next time we can today that's okay anybody yes sir let's go all right then let's again take a five minutes break and then we'll start on the new topic and if it begins to get information overload let me know then I'll pause the recording for a moment. And we'll meet again in five. Let's meet again in 10 minutes, and then we'll have a long session.