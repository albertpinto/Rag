 üéµ All right folks, so last time we talked about trees or decision trees. Decision trees were the first algorithm that were, I would say, in the class of nonlinear algorithms. Before that, all the things that we learned so far in ML100 and the previous lectures would draw a straight line to bifurcate feature space into two halves or maybe three halves or four halves partition it into multiple halves using logistic regression and linear discriminant analysis a quadratic discriminant analysis and so forth I mean and this is only true for those of you who attended the previous workshop but the bottom line is if didn't, the bottom line is we would take the feature space and we would partition it into components or pieces or subregions using linear decision boundaries or linear hyperplanes as the decision boundary. Decision tree, on the other hand, was the first algorithm we encountered where you could make all sorts of complicated decision boundary. And some of those I talked about is the wisdom of crowds. There is actually a theorem in mathematics that was discovered a long time ago. It says that if somebody's chance of guessing an answer is just a little bit more than 50% right so in other words the person is a weak learner or just about slightly gets it more right than wrong then in a systematic way you can harness the wisdom of crowds you can take a lot of these weak learners and, you can create an ensemble, a crowd of these. Then you can aggregate their wisdom. How do you aggregate it? If you are doing regression, so suppose you want to ask people like I did, like I told you, I used to do that in class when we used to physically meet in the before COVID world I would give you guys a jar of stones and ask you to guess how many stones are there invariably almost all everybody would be wrong maybe one or two people will be close to the right answer but the average of those answers was uncannily close to the right answer the reason for that is all of us have a bias. We have certain information in our decision and certain bias, right? So we may guess either optimistically or pessimistically, but then as you add up everybody's answers and average it, the average of the errors cancel out, but the average of the errors cancel out the and but the average of the information remains the information and so this the right answer begins to stand out that is the wisdom of crowds now why weak learners I talked about this question this thing that weak learners are better than stronger it turns out that strong learners have a higher tendency to get correlated with each other and tangled with each other in their decision making. So in other words, they begin to think alike. When two learners are thinking alike, they are redundant. You can just use one rather than two. So for example, if you need to get an understanding of whether there's global warming or not, the last thing you want to do is get two members from the same think tank. Because all the members in the think tank have been looking at the same data and coming to the same conclusions. So either both of them will strongly favor the statement that human beings are causing global warming or both of them will strongly favor the statement that you that human beings are simply not the cause of global warming its natural cycle of the earth you see my point right so if you really want the wisdom or you want the truth to emerge, you want to take intelligent people who are making, but not necessarily pundits, you want to take just intelligent people who are capable of thinking on their own. Enough, just more than 50% right or a sense in them. And then you take a collection of them and you'll be surprised at how uncannily right they are. So the basic ideas are you need weak learners, you need a lot of them. And then comes the other aspect of it. See what happens is if all of them are reading the same book, they'll reach the same conclusion. So the way you ensure that the learners don't get correlated or entangled is, in their learning, is by giving them different windows into the data. So one metaphor that you can use is blind men and the elephant. So you have a big elephant of a data but to each blind person you blindfold them and so each person gets to see and feel only a part or rather feel only a part of the elephant so once one feels the trunk one feels the ears one feels the dummy one feels the legs and so forth but if all of them are willing to agree that they are all correct the picture of the elephant will emerge isn't it or more precisely see whenever you take a picture a photograph a photograph is a two-dimensional representation of a three-dimensional thing you take enough photographs of a thing you will be able to reconstruct it in its full three dimensionality so long as all the photographers are not taking photograph from exactly the same vantage point if they are all taking pictures from the same vantage point you would really not know what is out there on the other side right or on any side except the front from which they're taking the picture. So for them to get collectively be able to reproduce the building in all its three dimensionality, you need to take an ensemble of those photographs from different vantage points, seeing different realities, subset of reality. See, when you look at a building, you don't see the whole building. You see a subset of the building, the part facing you. And that is very similar to what you want to do. You want to give each learner only a subset of the features. So suppose there are 10 features there, you want to pick a small number of features, two or three, and give it to them. So the way, if you remember i motivated it was with the story of or the metaphor of a teacher taking a bunch of school children to the to a special zoo or a meadow in that meadow there are only two animals cows and ducks right now the teacher being highly scholastic she begins to explain in intricate detail how, what are ducks and what are cows. So she talks about the fact that cows have cows are big compared to ducks, that they have a swishy tail and big horns, right, and they have four legs and hooves and so forth. so forth. On the other hand, when she talks about the ducks, she emphasizes that they're small, feathery, webbed-feet, beaked, winged creatures. So you may speak a lot about the attributes of a duck and the attributes of a cow. Imagine that these children are all pre-kindergarten children, right, or preschool children. You have taken them and you're explaining so much what will happen is different kids will latch on to different attributes one may remember for example that if it is big it must be a cow if it is a duck it must be small and she may also remember that if she sees a tail it better it is probably a cow right things like that if she sees a tail it better it is probably a cow right things like that if there is no tail then it's probably a duck some other child may latch on to the fact that webbed feet is it look at the feet are they webbed or are they hoofed if you see who it's a cow if you see web webbed feet it's a duck right or she may notice that their beaks is there a beak if there is no beak it's not a it's not a duck so different kids in that a crowd of collection or school of k a class of kids would latch on to different pieces being little children they're not going to understand all the anatomical differences that the teacher pointed out and so now the teacher shows an animal and ask what is it let us say that the animal is at a distance then the to the kid who is looking for size that cow may look rather small and may confuse it to be a duck right but let's say that the duck is flying around the other child may happen to notice that it's a winged creature it flies occasionally and so it may come to the conclusion that it's a duck so all of these children they will come to their own conclusion between cow and duck using what they have learned right they are looking at different subsets of the reality but collectively if you take an the the majority vote you ask the those children what is it? Cow or duck? Let's take the majority. Whatever it stands for, the majority stands for, you will notice that time and time again, that will be uncannily close to the right answer. And very infrequently they will be wrong. Whereas each child would be often wrong and that is the point of ensembles the other point so that is a point C here the point is not only should you let them look at different aspects of the reality you should also let them see or learn from different samples of the data because Because the data may have, any data sample may have some peculiarities. So just by accident, it may so happen that overall the majority of the animals are white in color or something like that, or brown in color. Right, and so you might begin to draw conclusions like brown things are, or, you know, brown things are cows and know brown things are cows and ducks and so forth and ducks are not brown and so forth but that cannot be really true so if you give subsets of the data you sort of protect against against an overall bias of the data right so that's the point of giving different subsets of the data to different learners to learn from so all of these things put together those are our main four points a you need be clear this you need many of them you need different learners looking at different aspects of the subsets of the features and D you you give them different samples of the data. At the end of it, you want to make sure that the learners are not correlated. If they are correlated, then the bully feature becomes important. So suppose there is one feature that always misguides. You see the problem with decision trees has always been that if there is one, if you make your first split wrong you get completely screwed thereafter you know it's a fatal it's a fatal decision the rest of the tree takes you down the wrong path so but then the root bifurcation that you do you based it by scanning all of these axes let us say that one of the feature is dominant and for whatever reason it misleads because either the data is wrong or biased or whatever reason and that feature always misleads. So what will happen your decision tree will always come out wrong but if you take you create lots of decision trees like let's let's take the example of decision trees those learners can be decision trees but they can be anything else suppose you take a lot of decision trees but to each tree you only give a few of the features so what will happen is the majority of the trees will not get that dominant feature that is screwing up the analysis. So some trees will make a mistake because they got that feature, majority of them won't. So I'll illustrate it with a number and an example. Suppose there are 100 features. It is a good idea never to give more than square root of the total features here the total features are 100 what is the square root of 100 10 so to each tree you give no more than 10 features so let us say that there is a bully feature and you create a few hundred trees by the law of large numbers it will happen that only 10 percent of the trees one tenth of the trees will get that bully feature. So one-tenth of the trees will get the answer wrong quite often. But then that's the beauty of it because you are assembling before you are aggregating the results of the ensemble, 90% of the trees will come back and give you the right answer. And so the bully a feature gets muted it gets suppressed it gets neutralized do you see the point guys so that is it now how do you take the aggregate it's very simple if you're doing classification as we talk you take the majority if you are doing regression you take the average of what they are saying so you ask the children like for example I would ask you guys how many stones are there in the jar whatever number you say we take the average you want to know what's the temperature outside you ask all the children what do you think is the temperature outside and then you take the average value and that's your prediction of temperature outside are we together right so that summarizes what we have talked so far guys please stop me here if you have not understood so far are we good so far guys okay did you follow any part of it so far? This must all be looking a bit strange. Yes, Asif. Yeah, getting it. You're getting it. Very good. Anybody else has a question, please ask me. Otherwise, let's take, it's 8.30 now. Should we take a 15 minutes break and regroup at 8 45 let's get some water and so forth and let's get together at 8 45. asif i just wanted to ask you when you said you know we need a mechanism to keep the bullies who can dominate neutralized is that is that the way we like when you gave only a part of the features to each decision tree? Is that achieved by that? Exactly. In fact, the rule is square root. If you have p features, you give more than square root of p. In fact, that is one of the mathematical theorems. It's a theorem about data projections. Okay. You just need square root of p at most features to get the words like, you know, isometric embedding and so on and so forth. I wouldn't go into the mathematics, but take it as a fact that it works. Okay. Thank you. Behind all of this, you know, I've given you the intuition, the gut feel or the picture. Each of these are deep wells of knowledge mathematical knowledge so you can keep drilling deeper and deeper into it it's a big rabbit hole okay if just want to make make sure just my understanding this Gini index is giving like the proportionality right of when it derived from there is derived from proportion and and ROC curves are also derived from proportionality right that you know true false is different it's about model metrics it is about the precision and accuracy or rather a false positive versus true positive so in other words as a side here I don't see curve is suppose you take a certain cutoff in the data like you know see suppose you have a probability that it's a cow mm-hmm suppose you can say cutoff is half if it is 50% probable market as cow or you could have taken 1 4 even if it is 1 4th probable that's a cow it's a cow or maybe you need a lot more assurance 3 4 so you can you know the probability X where you put the cutoff determines the rate of false positive and true positive okay right so if you're not careful see a bad model right that is making random guess will have equal rate of true and false positive yeah there's a do less right but a good model will achieve very low right for a very low false positive rate it would have already achieved a very high true positive rate which makes sense right you want a good model that is that is a high true positive rate for a very low false positive rate so So this is the ROC curve. So if like after partitioning, if Gini index gets better, the ROC will get better, right? Oh yes, of course. That's how they're connected, right? Yeah, in that sense, they're interconnected. Yeah, totally. So yeah, that's a good point. Suppose you're, so this is your square box, right? This is a pretty bad baseline. You can actually do worse than baseline, but let's say that in your first split, you're getting an ROC like this. Then the second one will give you an ROC, a better ROC. A third one will give you ROC like this, and a fourth one will give you ROC like this. And hopefully a very good model will eventually give you a roc like this so let me mark it zero uh so let me call it the the the first split one two after the two okay impurities keep getting better and better right you need to keep betting better and better yeah okay that's the intuition fire keeps improving thanks a lot here so this a lot. Yeah. So this is your ROC curve. So this is a good observation actually. Yeah.36. So let's make it 8.50. Let's meet in 15 minutes. And I'll pause the recording for now. And we will restart the recording if you guys want to talk without worrying about being recorded this would be it control shift So we'll talk about the concepts of bagging and boosting. These are two important topics. So we'll talk about this, that today we limit ourselves to back this word bagging is actually a combination or the what's the word portmanteau of two words bootstrapping compound compound compound yes bootstrapping and aggregation well it's an often interview question that people ask what what is bagging see usually people don't know and sometimes even interviewers don't know they just talk about the mechanics of it but it is actually the compound of these two words bagging bootstrapping BAG do you see that so what is bagging and what is bootstrapping? One way to, so let's talk about bootstrapping. What is bootstrapping? Bootstrapping is, and I'll give you a very rough idea. I won't go into the formal definition at this moment. So suppose you have points, data points. Let me mark this data points with a certain color. What color would suit us? Let's go with this color this time. So suppose you have a data points like this. And this is a sum total of your data points. And you want to generate training data out of this and give it to one of your learners, right? So I told you that don't give the same data to each of the learners. right so I told you that don't give the same data to each of the learners what you should instead do is a pick a different subset for each pick a different subset of data so for example to one you can give this data to another you can give this data to another you can give this data to another you can give this data to another you can give this data do you see that right and you realize that different people different sets different samples let me call it I K L M. So sample I and sample any two sample given I J sample I intersection sample J is a final in other words samples may have overlapping elements are we together and you obviously take random samples so this process is bootstrapping are we together this is bootstrapping and I'm giving you just the whole flavor of the whole thing at this moment let's go with that so far so good and nature of bootstrapping is some people will say you take a sample and then you replace it back into the bag and then it's like you know if you have peanuts in a jar you put your hand in the jar you shuffle and you take some peanuts out look at it and then you don't eat it up you put it back so that the next time you again put your hand in the jar and you take another bunch of peanuts out and some maybe may have belonged to the first time you took the peanuts out. So that is bootstrapping, roughly speaking. Then there is aggregation. Aggregation is simply put just for regression, take the average average prediction from each learner and for classification and for classification by the way guys is my handwriting reasonably legible? Are you able to read it? Or it's still? Yes. It's okay. And for classification, take the, if you have vote of the learners. So for example, if you if the learners are little children that you have taken to the middle, you look at an animal and you ask each child, is it a cow or a duck? And now look for the majority answer right and let us hope that you took an odd number of children so you always get a clear majority in the same way if you are taking regression so suppose you're asking them for example for the temperature whatever the prediction of temperature is and is, one child will say it's 72 degrees. Another will say, no, it's 60 degrees. Another will say it's 80 degrees. And you're asking them to predict the weather as it would be, let's say, one hour later. So they will all make their predictions, take the average of their predictions. And the average is the correct answer answer is the answer that you use you say that it is the answer of the ensemble or of the crowd the crowd has answered in the crowd says the temperature is likely to be 80 degrees so do we understand aggregation also now? So these two put together makes bootstrapping. Sorry, not bagging. Bagging is bootstrapping plus aggregation. Did we get that? Now, that is bagging. Boosting I'll keep for the next time. But let me mention a very popular algorithm that is bagging. Boosting I'll keep for the next time. But let me mention a very popular algorithm that uses bagging. It is called, can somebody tell me what it is called? Random forest. Random forest and its close cousin, which is slight improvement, but somewhat lesser known. There are many variants of it. For example, there is extremely randomized trees, etc. But let's go with random forest. So what are random forests? Random forest is the criteria is the learners are the trees. Take lots of them. Take or create lots of learners my basic rule is I start out at greater than 100 and I go up to typically you know based on that situation 400 or sometimes a thousand like lots and lots of learners we live in the world of computational plenty. Go at it. The second thing is, see, to each learner, tree learner, give bootstrap sample of the training data don't give it the whole data give it a bootstrap sample of the training data by now can I assume we understand the word bootstrap sample just to remind remind you, this is it. This red thing here is the bootstrap sample. Randomly go pick a small sample of the data. The next the feature space B Rd ie R not D let me make it P R suppose there are P features P there be P features P can be five or ten or whatever hundred features or p features let the features must be then then give to each learner only a subset let me call it s which is much smaller than obviously than equal to p of the features so you can give it you know all the features to the of the features but ideally less than p features that you see the random voice algorithm doesn't put a restriction but math says and I won't go into the math is the whole random projection theorem but I'll just leave it as a more intuitive word just say that the wisdom of math says math says s should be less than actually the square root of P so I'll just leave it s of the features the math says yeah ask s of course maybe so P are the features but actually you should not take so for example if there are hundred features what is the square root of hundred okay so take only ten of the features that's it and then you build a model and then the last thing is aggregate the results train each of let decision tree models models right then for inference let each tree learner make its own independent projection a prediction make its own prediction prediction y-hat right so each of them will make their own independent prediction right so let me let's say that the learner is L and you're making a prediction on the data set I. They will all make their own prediction. Right. And the last step is aggregate. The predictions. Get an overall prediction to get an overall prediction this is a sequence of things you know what have we done in random forests let's look through the steps we are saying you take the so you need to pick some learner in random forest you use trees which explains the word forest what do you call a whole bunch of trees growing next to each other you call it a forest right now I'll explain the word random in a moment so now you create a lot of trees obviously for it to look like a legitimate forest You better have a lot of trees More or more practically you need a lot of learners. So you need an ensemble. So this is your ensemble piece Once you have the ensemble now what happens to each ensemble? You need to give it data. Each tree, you need to give it data to learn. Give it a bootstrap sample. Don't give the entire training data to the tree. Once you have done that, now what you do is you, even from that sample, you don't give all the features. You take a random projection of the features right so the word random comes from that a random subspace random is a Sam subs random subspace of the data space projection space or projection. So the random in random forest is not that you take a random sample that would be bootstrapping. It is the random projection that you do or the random subspace that you pick up of the features or the feature feature space that's what you do and so do you notice that these words they sound rather formal when they are written in textbooks but they actually have a very simple interpretation now all of these learners there's no there's no reason they need to wait on each other so they can all learn in parallel in fact the better learn in parallel the more unaware each is of the other the better you don't want them to be to get correlated and finally you draw an inference let each tree so it suppose you want to make a prediction on a data you show it an animal let each of the tree tell is it a cow or a duck and then you take the majority or if you ask the tree to tell what temperature is going to be tomorrow at noon let each of the trees make an inference and then you take the average prediction so that is the aggregation part so you notice guys that we had to put a lot of ducks in a row right we took the idea of ensembles we took the idea of ensembles, we took the idea of random projection, we took the idea of bootstrapping, and we took the idea of aggregating. So we had to put four ducks in a row to get this algorithm, random forest. It is a beginning of what our world is going to be going forward. If you have done ML100 and so far, you have been dealing with simple algorithms based on one idea. Random forest is your introduction to a complex algorithm where you have to put many good ducks in a row to get a good algorithm. Many ideas come together or strands of thinking come together to form the big picture, the big or the full algorithm it has many good components to it random forest the limitation of random forest is limitation you making those predictions it will just shrug its shoulder and say well I like to make that that's what I learned now what exactly did you learn it won't be able to tell it's a black box model right all you get is the X vector goes in as input and y hat, xi goes in and y hat comes out as a prediction. Are we together? You can't open this box and ask what is there. How is it making the prediction? Explain it to me. It is not a model that has a parameter this way you say it's a non parametric model right it's too hard you cannot say that there is beta naught beta 1 beta 2 and so on and so forth and so these are the knobs I can use to explain to the user why it made why the machine made that prediction it's quite a loss to be able to lose interpretability so it degrades it so interpretability is a big sacrifice you make sorry that word interpretability sacrifice interpretability but not completely there is one silver lining one silver lining so we better take a silver colored pencil I suppose for silver lining is the silver color it looks like it so there we go we have a silver lining silver lining is one of the one thing that you have in linear models or straightforward models is interpretability you can compare the size of the coefficients and get some sense of how relatively important each feature is That is preserved feature importance Feature importance the relative importance of features now I used a lot of words. Let me explain it. Rough global average level is preserved. But forget the rough global average word for a moment. It will tell you that suppose you're talking about diabetes and let's say weight is a factor, age is a factor, right? And well, weight or size of the person and maybe the two put together make a BMI but let's skip that and your age and your and and a few other factors genetics and so on and so forth and you're trying to determine given all the factors is the person likely to have to be at risk of diabetes or not and when you build a model let's say that you successfully build a model in which XI goes in and you you predict true-false then this model this random forest that you have you can ask it that in making the decisions how important were each of these factors each of this effective contributory independent of factors, age, size. So it might come back and say that, hey, you know what, let's say weight matters most. Weight matters most and then is greater than age. Let's just say, I'm just putting, I don't know, a doctor may disagree. So size, I don't know a doctor may disagree so size I don't know what the right order globally would be something like that but there is a problem with this random points gives you that the trouble with this is that it gives you sort of an average doubt feature in that on the whole this is what it is it doesn't capture the nuances of prediction so for example you know when in different regions of the feature space for example this region of the young please mute yourself okay so suppose you're looking at the young people here then for the young as you know unless you have type 1 diabetes so let me just make it make it type 2 adult onset type 2 diabetes unless so children tend not to have type 2 diabetes right even if they are relatively overweight they tend not to have type 2 diabetes unless there's a genetic predisposition to it right so you might say that for young genetic factor is here genetics is overwhelming is more important but even but as a general population you may you may agree that for people who are older over 40 greater than 45 now suddenly what begins to matter a weight weight and size of course together making up the BMI matters and let me throw in exercise here exercise matters exercise matters things like that they begin to matter much more right I don't know medically I'm quite likely to be off, but I'm just using this to illustrate a point that in different regions of the feature space, for different age groups or different weight groups. So for example, if you take all people over 45 in the same BMI, now all of a sudden genetics will begin to matter a lot or the level of exercise will begin to matter a lot or the level of exercise will begin to matter a lot. So feature importance is actually a very complex and nuanced topic. And it is almost very poorly understood by most people doing data science. They don't understand the sheer complexity and nuances of feature importance. So anyway, I'll stop today with the statement that whenever you talk about feature importance and even when you read these books by all these publishers, they actually can be quite misleading. They will tell you that in random forest, etc., you have feature importance coming in. Remember that those are very rough measures global measures feature importance is a deep topic and whenever you look at feature importance so for example a given user suppose a user is a person is 25 right the person does like at that particular moment what do you think is the risk of diabetes what is more likely to cause it right there are multiple things you know that the young people are active they're on their toes always so you'd probably advise them to be careful about their diet on the other hand as people get older and older, their exercise level goes down, but their food habits remain more or less stable. So now you want to emphasize, well, they're not likely to suddenly start overeating and assuming their food habits are reasonably modest. Now you want to start emphasizing exercise, right? Because people become sedentary as they as youth fades away so things like that what you advise or what factors matter more it's like asking that how much does Y change is something as that you know if Y is a function of X how much y changes for each of the individual predictors in a given give for a given value of the x vector right so it's a local feature because you're essentially comparing roughly speaking the partial derivatives in the local region. You're looking at the gradients locally, gradient of Y with respect to each of these features locally. So this is getting a little messy, forget about that. But just remember that while people extol the virtues of feature importance in random forest in these models, take it with a grain of salt. Feature importance is more nuanced. There's a few, I noticed some of you are here after having done my boot camp last November, December, so you would remember that we devoted an entire day to the nuances and nuances of feature importance and it also goes to the heart of something we are trying to do these days, recover interpretability of black box models. So, but anyway, this last part, by the way, is out of scope. So if you, I just gave you a teaser for things to come later on. If you didn't get the whole thing of it, don't worry at all. The only thing you should take back is the feature importance is there in a very rough way in random forest random forest is a complicated model it is built out of some four you need to line up at least four ducks in a row you need to use the concept of learners and sample of learners those learners should be weak learners you make them weak by giving them a random projection of the data right a random projection of the data, right? A random data projected to a subspace of the feature space. You give each bootstrap sample. Then finally you aggregate the results. All these things are efforts to make sure that the trees are, or the learners are de-correlated, right? That's what you do. Now, one remarkable fact about random forest and a dirty little secret is, I said that number of features should be less than the square root of p. Now here's the dirty little secret. Quite often, if you create a forest in which each tree is given only randomly one of the features, those are called stump trees, right? Stumps, a forest of stamps no branches it performs remarkably well quite often it performs as well as a more complex tree are we together so that is it so that is random forest what it addresses is this tendency in decision trees to fit data hard to go and overfit to data decision trees always have the problem of overfitting people did all sorts of clever pruning algorithms and then came random forest and then random forest created a quite a tremendous debate in the community people asked that machine learning used to be not just about making predictions but about interpretability or providing some inference about what happened why is it happening you lost that you lost interpretability was the sacrifice okay today we of course say that it is on a case by case basis. Sometimes where only performance matters, you want the best model that you can build. So for example, if you want a self-driven car, you just want it to perform well and keep you safe and keep the other people on the road safe, prevent accidents. You probably are not too concerned on what the interpretability of its decisions are. We just wanted to make the right decision. But on the other hand, in many of the situations, for example, if you're trying to give a job based on algorithms and how perform people performed on a test you want to make sure that the models are highly interpretable you're not willing to take a black box that says like an article that all right this guy is better than that guy so take this guy don't hire that guy because then interpretability of paramount importance because there may be biases if the data is biased your machine learning algorithm will be biased and a black box model you you will not know easily that there is a bias in your predictions also because there was a very subtle bias in the data in fact companies have been sued for doing this in US you can get sued because we have the protected classes you cannot you need to guarantee that you don't discriminate against the protective classes race gender age disability and so forth right and so in the history of this is without taking names there was a company of big company is a big company that was sued because it could be demonstrated that it was discriminating against the protected classes they came they're not deliberately doing it they are feeding all the data they have into the algorithm and the algorithms are blind and impartial and so they are making predictions therefore it is right the judge did not quite agree he says no it doesn't matter the fact of the matter is there's evidence that your process is creating discrimination is doing discriminatory hiring but therefore the circumstances of how you produce that doesn't matter you're responsible for the wrong decisions they lost the case and they lost they had to pay a lot of huge penalty so obviously obviously in these situations, now you ask why it happened. It had to do with the fact that the data had inherent bias. The algorithmic training had bias, therefore. All of those things creep in. So there are situations where you can use black box models. There are situations where you cannot. And nowadays we are entering a world in which we are saying, well, we cannot but help use this very complicated black box models, deep neural networks and so forth. But we want interpretability nonetheless. And so we are trying to create a whole new wave of of body of you know research and tooling and stuff to restore some semblance of interpretability and explain ability on top of black box models so that's a way promising areas those of you who did the bootcamp with me remember will remember some of it and in the next bootcamp which is a deep learning it will become a topic of paramount importance a very hot topic today so with those words I will end now any questions guys on random forest before we close the session today I'm yet so in tandem forest every tree is decision tree right sir? It's a decision tree yes. Thank you. Asghar is there a recommended sample size to do random forest? Generally it will do the right thing internally. It will look at your training data set size and it will pick a sample size. The basic rules of sample size is you try to take a reasonably big sample but you don't try but you make sure that you don't take the whole set itself and you know suppose you have 10,000 points and what's this square so long as you're taking more than 100 points or so in a sample. It's good. It's a good sample. How does Random Forest deal with high correlation between features? It is actually very good. Let's say that the features x1 and x2 are highly correlated. Right? The beautiful thing is one tree will get only x1 another tree will get x2 so you don't have the problems that you have the pathologies that you have in linear models but doesn't it or account the effect so what happens is that the beautiful thing happens is that when you ask for the importance it will ascribe equal importance to x1 and x2 because some trees will be using x1 some will be using x2 okay that's how it is so the problem of multicollinearity therefore is moot when it comes to random forest that was one of the big reasons people you know when random forest came in they were a runaway success every bit as if we lost you he was using it you take a ticket and use trees boosting you hear of all this cat boost XG boost and so on and so forth and we'll talk about boosting the next time next Monday remember guys no it was for everyone yeah audio, audio was dropped, I think. We lost you in between us. Oh my goodness, okay, so I'll say that again. What I was saying is that a lot of the mod, if you go to Kaggle and you look at the winning entries for tabular data competitions, wherever you get tabular data, not image or sound or text, but the simple tables of data, you will often find that the winners are in the leaderboard. You'll often find entries of teams that are using one of the ensemble methods. They're using random forest or gradient boosting, you know, this cat boost, taxi boost, whatnot. They'll be using things like that. Very, very often you'll see them. You'll see at least some entries in the leaderboard coming from this. So this is a very popular algorithm. Obviously, like with every algorithm, people do get carried away. But there are people who solve every single problem using a random forest. So that's that. Actually, it reminds me of something very interesting. My good friend, Dilip, who presented in quantum computing yesterday, he and I used to have a joke. So he would say that we have to solve, write code. So he would say, wait a minute, he would open the code editor, write int ij, and then he would say, now tell me what the problem is. Because he would always need the variables ij, it he would say now tell me what the problem is because he would always need the variables ij's like that so nice after all these years when I think of random forces like that a lot of people swear by it they'll say okay let me first write instantiate a random forest and then you tell me what the data is and what should I fit to it is very effective and now we are entering the space of these powerful methods very very practical and very usable and you use it a lot become good at it guys and you'll it will solve a lot of your problems will do after this will do boosting and it's implementations actually both can boost a boost and all that and then after that we'll do boosting and its implementations, XGBoost, CatBoost, AdaBoost, and all that. And then after that, we'll go to something, yet another very powerful idea called kernel machines or kernel methods. We'll do support vector machines and so on and so forth. So in this workshop, of course, we are moving fast. We're moving through rather powerful algorithms. Theory is done of this. Next time, remember, Wednesdays are for practice. We will do lab work in random forest. Lab work is entirely different kettle of fish. You'll realize that lab is very easy. You just instantiate the random forest and fit it to data. But we'll learn to see its nuances. It's important to know some of the nuances involved and we'll develop practice in that. Any questions guys before I end? Karsan, just one question. This weak learner, right? Like in this case, you're showing them only weight or age, not everything, some subset of features, right? Like in this case, you are showing them only weight or age, not everything, some subset of features, right? And they are doing prediction based on decision tree algorithm? In random forest, yes. But in general, you don't have to. You can create a bagging of anything you want, any other learners you want. You can make a bagging of, you know, you could use logistic regression. So let's take this example. You want to do a linear regression. Remember in linear regression, you have the fundamental problem that if there is multicollinearity between the features, you don't, right? You will start seeing explosion of variance. You start seeing pathologies. Your models are very unstable and they exhibit variance inflations. Very sort of slight change of the data will cause them to build entirely different models. So one easy cure, just take an ensemble of linear regresses and give them a random projection of features you will realize that lo and behold that problem is solved so but all of these learner are using same algorithm right to learn yes but now I'll teach now the next thing I'll mention I think you sort of anticipated where I was going even you can ask this question why why can i not mix a few decision trees a few uh linear regressors yeah things like that different algorithms why can i not do that if i'm doing regressions i can mix 20 different algorithms and instances of them and likewise forifiers, I can take a whole bag of classifiers, different kinds of classifiers and do it. So yes, you can do that. When you do that and you create an ensemble of heterogeneous classifiers, you actually get very, very powerful prediction machines. In fact, that is one of my own dirty little secrets. Hardly a secret, it's an open secret because I keep insisting that everybody should do that. Never solve a problem with one algorithm. Use every good collection of algorithms and lots of instances of learners from each of the algorithms. Do the same process instead of trees, replace it with those, with a heterogeneous bag of learners from different algorithms. And when you do that, it is surprising how much efficiency you get because the weakness of one algorithm is made up by the strength of the other algorithm and so forth. And you get very good thing. So bagging is one, we can do boosting. And the third thing is stacking. Stacking is something i actually i find are very very useful i'll stack predictions from different machines right different learners and then put a learner that learns from the predictions of the different learners that is stacking will gradually come to that I don't know whether we'll do stacking in the practical methods but we'll certainly do it in the boot camps so here in random for events with an ensemble ensemble means an ensemble of weak learners using decision tree okay thanks in random places, because the word forest is forest is forest trees. Alright guys, so this is it. I will stop the recording. Yeah, go ahead. So, so when you say heterogeneous classifiers or heterogeneous algorithms, they're like the example you gave right so you know, you know, the 100 features and then you have, let's say, 10 learners or 100 learners, like that, getting 10. So, you distribute those algorithms equally among the learners. So, let's say there are 10 learners and you're using five so two of the learners i mean generally what happens is in those areas my experiences and this is the experience see this is where you know each just like each artist has his own style each data cycle will have his own rules of thumb and their own experience my experience is i don't try to be very clever there i just take a bag of learners and run with it. Different heterogeneous learners and run with it. I'll throw the same number of learners. But I'll make sure they all are doing their own kind of projections and strategies.„ÅîË¶ñËÅ¥„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åó„Åü you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you Thank you.