 All right, guys, so welcome back to our, this should be our fifth week, the lab of the fifth week. This week we covered regularization. Today is Saturday morning in California, and probably Saturday night in India. So I will just give a very quick recap. The easiest way to regularize data, regularize a model is just to bring more data into it. If you have a lot of data, obviously it solves a lot of problems. More data makes your model better. I have to share the screen. Yes, I have to share the screen. Are you guys seeing my screen now? Are you guys seeing my screen now? Yes. Okay. So we can take, just as a recap, if we take a model like this, you can see that there is a huge amount of wrench phenomenon here. If you take a five-degree model, it's good. But the moment you go to higher degrees, you start seeing this range phenomenon, this huge oscillation towards the end. But you can regularize it by bringing in more data. The more data you bring in, you notice that these oscillations are getting suppressed. And more data. And we bring in even more data. And you can see that even though we are dealing with the 10th degree polynomial like getting something like 100 data points it pretty much smooths it out and you can see it even more and by the time you throw a lot of data at it it begins to reflect it begins to reflect what you hope the model would say, isn't it? So this is the regularization by throwing more data at it. And then when you have more data, it is rather robust against overfitting. So you can increase it to, let's say that you can go to 14th degree. You see a little bit of range phenomenon towards the tip, but broadly speaking, it's still doing pretty well, pretty good. So, and up to a modest degrees, like 10th degree, it's doing quite well. So today we are going to do a lab in which we will see how range regularization has the same effect. We learned that quite often data is, it's a luxury that you can't afford. You can't get more data. Practically, it's not feasible. So when it is not feasible to get more data, which is quite often the case, it turns out that there are mathematical ways to still regularize the model and to prevent overfitting. One of those ways we learned was the ridge and the lasso regularizations, two ways we learned, and the L1 and L2, they were called, if you remember the theory of it. So I'll just quickly recap the theory for this. theory for this. We said that we don't want the global, sorry, we don't want the global minima, the global minima gets too far from the origin, but what we want is a constrained minima within a surface, within a certain distance from the origin. So, oh by the way, I made a mistake here. I made quite a mistake here last time when I made this blue line. So there's a correction here guys. It doesn't bulge out like this, I realized later on. For Minkowski distance greater than two, or greater than one it is not like this the way it is is uh let me see yeah so the asymptotic limit is it's a square it goes where this is n is equal to infinity but what happens is as you increase n beyond the circle, it just develops more and more bulge. It will become like this. It begins to approximate more a square, not that flowery shape that I built. That didn't make sense. not that flowery shape that I built. That didn't make sense. This is the Minkowski norm. So now in constraint optimization, what we are basically saying is we will stay within a certain distance from the origin. We'll make sure that the coefficients don't blow up. And whatever solution comes there is good enough for us. So you may say that, well, you know, don't you lose some optimization? You don't lose because, you know, you're training the model on a part of the data, sample of the data called the training set. If the data is sparse, it is overfitting to that. So you don't want it to learn too much. You want it to learn enough because you know that the test data may look quite different. And so that was the whole theory for that. I wouldn't go more into that now because it can be a long session. So we'll do this today. We'll do a lab on this. Now, just as a preview, if you want to understand this lab you should have done all of these notebooks which I trust you guys have done. You know what wrench phenomenon is and so forth. So we will start with the goal of this lab is it's a simple lab. It shows the effect of regularization. So what I'm doing is I'm deliberately creating a sparse number of data points. I'm creating 10 training data points and 10 test data points. So let me walk through this code carefully. This is linspace. This is the NumPy way of saying create 10 points between 0 and 1 equally spaced, linearly spaced. So you can imagine that the values would be 0.1, 0.2, 0.3, 0.4, and so forth. What I'm doing for X test is the same thing here, but with one addition. I am just wiggling it a little bit. It will be a little bit away along the X-axis randomly. I'm shifting it a little bit so that it's not sitting on top of it, the test point. And for each of the, so how do I create a y-train, y-train and y-test, both of them I create by taking, well, the sign of it, right? And adding tiny bits of noise to it. So you can see me adding some noise here to the data. This is the noise. And so you can plot it out. The Indian red color, the salmon color, these are the training data, and the midnight blue color is the test data. So we have just created a data and now we know what the model is. The model is a sine wave, right? We want to see how well our regression, polynomial regression can capture this, right? So when we try to do that, to do that, we will, and now I want to introduce a very neat way of writing code. So pay attention, guys, to this syntax. This is a new syntax I'm introducing. I mentioned that whenever you get data, you should do what? You should scale the data, isn't it? Always. Now, in this case, data scaling was not really necessary because data is within a very small range, zero to one, isn't it always now in this case data scaling was not really necessary because data is within a very small range zero to one isn't it x-axis is from zero to one y-axis will be from minus one to one so scaling is not necessary but i want to illustrate the point that how would you scale the data so you would you would take a standard scalar you. You also know that you want to build a polynomial model, isn't it? Polynomial regression model. So you create a polynomial regression model of a certain degree, let's say 11. And then once you have polynomial features, you want to feed that data into linear regression. So guys, does it make sense? First, you take x, expand x to x, x square, x cube, x all the way to x11. Now you have 11 columns of data, 11 features. And now you take a regression model and you fit to this data. So there are three things we want to do. Scale the data, do polynomial expansion, and then apply linear regression. So if we do it like in code, you could do that. You could first apply the scalar, and you would have to do it for the test data and the train data, and it gets tedious. So this brings us to the concept of a pipeline and machine learning pipelines are very very important for creating data pipelines and or machine learning pipelines and the the way to create pipelines is actually very easy. So if you if you pay attention to this bit of code are we all able to see, able to read this bit of code are we all able to see able to read this bit of code guys yes right so what we are doing is in the pipeline we are putting steps we are saying the first step is the standard scalar by the way i tend to you don't need to give it a name you can just give a comma separated list of steps but it is always nice to give it a name when you name your models right and in fact i want to show you something what happens when you name the model split cell and i will just run this part and you can just say model let me just do one more thing here model do you see that it points out in detail what your pipeline looks like? So then you need to fit your data. You need to train the model on your training data. Remember that in scikit-learn, the method is fit. remember that in scikit-learn the method is fit so observe how the model the pipeline has the same structure as what you are familiar with linear regression like this is the uniformity in scikit-learn it makes it very easy you can pretend as though you're just doing linear regression but actually what you're doing is you're executing this entire pipeline of steps i would i would strongly advise you to become familiar with this syntax. It's surprising actually when I look at people's notebook or if you go to Kaggle and different places how infrequently people utilize pipelines. But once you get used to pipeline syntax you should invariably use it because all of these things are necessary. For example, scaling the data. Now, one thing that I which we didn't have missing value, but you can have missing value imputation. What do you do if certain values are missing? Right. You need to handle that. So there are many pre-processing steps and then you build a model, right? So all of those you can make into a pipeline, all right? Pre-processing steps into a pipeline. And then the pipeline behaves as though it is the model. You can fit to the data, which is what I'm doing. You notice that I'm fitting to the data, but then if I fit to the data, I know that the coefficients belong to the regression model, isn't it? So how do I access that? You can do model. And this, please pay attention to this crucial line. Yeah, model.name steps. So all the steps are there in a hash map, because you made this hash map. Do you notice that? This is your map. because you made this hash map. Do you notice that? This is your map. Key value entries are there. So you can say named step, and you can give the name of the step and say, give it the coefficient. So it will produce the coefficient. Now, we took a model of 11 dimensions. So how many coefficients should we expect? Anyone? How many coefficients would we expect? 12. 12, yeah, because we also have the beta naught, the intercept, right? And sure enough, you'll see that these are three rows of four coefficients. Now, observe something quite carefully. Do you notice that these coefficients, some of them are significant, but not too large, but they are still large. This is 10, 8.9, isn't it? This is minus 3. This is minus 5, and so forth. This is plus 5. No, well, not plus 5. This is 10 to the minus 2, actually. So no, no no no no so i take this back actually most of these coefficients are small um relatively small but not uh but you'll see you'll see something interesting when you plot it out um as if i think in cell number 75 line number two is wrong x test is equal to x testxtest.resave. Oh my goodness, yes, yes, yes. I saw, I did screw that up. Thank you. Thank you for pointing that out. Let's go fix it and rerun this whole thing. That's great that you noticed it, xtest. Much better, right? So let's observe carefully. I was writing this code just today morning. Let's see if I missed anything else. This is a most common mistake I make. I mix up my test and train data. This looks okay. Y test, Y train, yeah, this looks okay. Let's run it from the beginning and see. Looks okay. And this pipeline looks of course okay. Then Xtrain is on the Xtrain, Xtest. Yeah, also is wrong. Yeah, it should be on the Xtest. Xtest, yeah. So this now is better. Yeah, you fit the model to the data x train x test let's do the model diagnostic and when we do the model diagnostic we realize that oh boy our what is the coefficient of determination is rather poor here we continue this model is clearly a disaster isn isn't it? Let's plot it out and see what it looks like. When you plot it out, so this part I hope, I don't, so this part, let's go through this systematically. This part I hope is clear. What does R2 stand for? Could one of you tell me? What is R2? It is literally the capital R squared, right? It's the coefficient of determination. When the coefficient of determination is negative, what is it trying to tell you? It means that you are worse than the null hypothesis, isn't it? Null hypothesis will give you a coefficient of determination of r squared of zero. And by the way, this is one of the misconceptions people have. They think that the r squared goes from zero to one. Good models are closer to one, bad models are closer to zero. No, you can actually do much worse than the null hypothesis, as for example you're doing here. So this model is clearly a disaster, right? So let's see why it is a disaster. It's a disaster because of, you see these big oscillations. What are these oscillations called? Range phenomena. Exactly. And you notice that it has overfit the data. It seems to go through. Look at all the red points. This model goes through every single red point, isn't it? It is going through every single of the red points. Now, what happens if you do a ridge regression? And by the way, this is the strength of the regularization, the lambda parameter in scikit-learn is called the alpha parameter. Let's try it. And the point is, you should play with it. By the way, guys, this notebook is on your website and your class. We will do a lab. I want you to play around with the alpha values and see which is the best alpha value. And when you do this, with regularization, it looks like this. Does this look much better, guys? Yes. So just to compare it to this, see how overfit it is. And it seems to be seeing a lot of patterns in the data, which we know are artifacts. But when you regularize it, and what was the R squared here? R squared here was a disaster, right? Now, here, the R square is much better, 90%. And so observe something else here. Do you still see wrench phenomenon, guys? Slightly. Yes, slightly. At the periphery, it is still there. So remember one thing. Never trust predictions At the periphery, it is still there. So remember one thing, never trust predictions near the periphery of your data ever, all right? So suppose you are training the data on the range of zero to one, make sure that you make a prediction only in the range of 0.1 to 0.9. Like never trust it to the full range of the training data. It's one of those practical advice that you don't hear often mentioned, but generally I'm very cautious. I would not make prediction in the entire range of the training data. Because I know that near the periphery, many things can go wrong. Is this clear? So this is practical advice, guys. I don't know. I don't find it in books quite often, but it's there. Take it as a wisdom. Now, with this, I think I wrote some notes, which I will go over. Observe how the model is a simpler one and more closely agrees with our intuition. Do you also notice that this model does not go through all the training data or the red points? Does it go through all the red points? No, it goes through red points only near the periphery, near the outer perimeters, isn't it? Where you know that it's very hard to prevent overfitting and range phenomenon. that it's very hard to prevent overfitting and range phenomenon. But in the interior of the data that is 0.1 to 0.9, you notice that this is a pretty good model. Right? So this point actually is wrong. Let me delete it. When you look at this model you can see that it is a much simpler model let me do one thing because that code that i put on the course website had a bug let me put this code on slack so i want you guys to do this as a homework right now or classwork right now what do we do we will or classwork right now, what do we do? We will upload from your computer documents oh boy comprehensive intro and this is called regularization. So guys, let us do an experiment. There is a way that the machine can automatically detect what the hyperparameter alpha should be, right? It's a very simple technique. You could have done, and I'll mention what that technique is. Instead of saying ridge is equal to this, you could have said, let me comment it out, and you could have said ridge is equal to ridge regression. Ridge, CV, cross-validated. And let it find out the best alpha. When it does that, it will come up with something like this. You would hopefully agree that this is actually not the best model. Isn't it? Here, cross-validation is not giving you too good a result so i want you to do one thing play with it so this line i'll comment out go there in your code which i put it on slack run it with different values of alpha try alpha is equal to so the values of alpha to try are, let me just say, try with, try with powers of 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.1 0.2 0. I mean three 0.4 0.5 right all the way uh play with this guys and tell me which Alpha gives you the best results uh as if I thought Alpha is the lambda kind so i thought it is for minus infinity infinity it can take any value but after seeing your values it's look like it can take only from zero to one yeah there is the regularization constant so you always give it a positive value another thing of it is it's related to the radius of your circle remember yeah so you don't want the radius to be negative it is related to that basically it controls how that will be inversely related to the radius of your uh then why it cannot be greater than one then you can make it greater than one so go ahead and make it 10 see play around with it got it yeah play with it guys and tell me what is the best alpha that you like and each one of you tell which is the best alpha you like. So do this guys, please do this. Don't just stay there. I'll give you guys 10-15 minutes to do it. And I will do that thing for you. For example, you said 10, right? Regularization 10. See what happens. At 10, you get literally very similar to what the uh regularization cross-validation gave you right but if you take it to be 0.001 very very weak regularization you you realize that some amount of uh wrench phenomena is suppressed but Ranj phenomena is suppressed, but it still is not as simple a model, right? It seems to be, it is not the way that you expect. So this is how you determine or you play around and tell how you can make it. But I want you to play with it and see on your own what you like play with it guys and tell me what is the best alpha you you find i shouldn't give you the idea as if if you don't mind can you drop the other comment which you wrote now only that rich CV also. This one you want me to put it on the. Yeah, slack. Slack. Okay, sure. So that is cross validation is a topic that I will talk about at some point. Is anyone trying? Prabhat, you're trying? Yes. Good. Find out the best value of alpha at which you like the solution. you you you you you you you you Thank you. you you you Thank you. anyone you you Thank you. you you you you you you you guys someone please pick up what values did you like i'm not seeing any submissions here as if so i tried my adjusted r square after adding three four zeros it basically My adjusted R square after adding three, four zeros, it basically fizzled out. It's in the sense afterwards there is no use. And after that, even though I tried, right, the graph almost looks similar after adding more than four zeros to be true. More than four zeros means you're not regularizing at all. Yeah. So it's like at three or four zeros zeros you get the max at what you might get then yeah i think the so look at your r squared coefficient of determination yeah and look at the plot where do you feel that the plot is smoothest and best fits the data uh first time i felt like r square is showing very wrong after seeing the plot i think at ferris it's very bad in the middle it was really good yeah what about this one can you guess do you notice would you say that this is the best can you guess do you notice would you say that this is the best way there is no oscillation nothing can you guess what value i put uh i can see you have put 0.5 so i was thinking how can this can happen i thought the lesser the alpha it's better i thought i don't think i put 0.5 but you can try 0.5 and see if it helps. See guys, the whole point is you discover the answer on your own. If I tell you the answer, then you don't learn, data science is all science is experimentation in the lab, guys. This is your lab. Experiment and find out what works. If I tell you, then what happens is you'll not develop your own skill. So I want somebody to guess what I might have put. Is anyone trying? Prabhat, you're trying? Asif, I'm facing with issue with LaTeX. Disable LaTeX. Go to the file support which is common and disable LaTeX. Use LaTeX is equal to false use latex is equal to false use text is equal to false okay weird even i kept 0.05 the same thing happened to me to be true which is almost a similar gap graph as you i still have wrench phenomenon at the the first red point but at the end it made smooth like this yes all right if i don't mind can i see your coefficients no why not the whole point is no you should you have to try to incorporate yourself no no i'm not asking to see rich what the value give i want to see the coefficients of those uh 11 degree polynomial okay with my coefficients are here yeah that they are all very very small they are almost other than the intercept everything is smaller than one in size that is a clear sign of a good model okay here yeah i was thinking that yeah after seeing the graph i thought the other questions have must have fizzled out i have two major and after seeing the graph i thought the other questions have must have fizzled out i have two major and you have only one that's the reason i still have one more ranch phenomenon instead of two anyway guys uh i want at least one more person to try and give me some value and then we'll move on Jason to try and give me some value and then we'll move on um I got a value like I was trying in the low areas like around 0.01 to 0.1 I think the closest I think 0.01 ended up like overfitting to the data and right in between I mean like 0.05 ish was pretty good like it was yeah more smooth but it was still pretty close in there that's right show your screen share your screen and show okay um it says i can't share while you're sharing oh you can't share uh now share your screen so i started off with 0.01. Yeah. If I run it. You can see like it kind of goes really close to these points and you see like this kind of thing. Yes, still overfitting. And that 0.1. Will be over. Yeah. Like these are kind of out and these are farther out um it has over damped yeah and that 0.05 um it's very smooth and the points are pretty close that's right so 0.05 i was wondering like um about this side it's like this tail is yeah. So close to the periphery all bets are off because polynomial regression, the wrench phenomenon is so strong that somewhere or the other you will have. You shouldn't trust the data near the periphery, but if you forget the periphery and you look at in the region from 0.1 to 0.9, you will see that your model is doing very well, isn't it? Yeah, the fit is very good in this general way. Yeah, so it depends because it's a random variable. What you should do is try in your case, try 0.65. Just play around with it. 0.065. You still got the same thing, huh? Did you run the rest of the code? Yeah, I ran the same thing. Did you run the rest of the code? Yeah, I ran the whole thing. Yeah, 6.065 is similar. Yeah, see what I get. And obviously, because there's a randomness factor here. So which screen am I sharing? Screen one. This is screen two. Okay. So in my case, I took this value by taking 0.05. When I took 0.05, I got this curve. Remember, these are random points and random initializations. So your values will slightly differ because your data is different from mine. All right, guys. So with that, we'll move on to another topic. I say still I have one more question. My question was after playing with alpha, right? Let's say the graph gets better. It doesn't mean that R square is getting better when R square is better graph is worse turned when R when uh like graph is better than R square is worse then R square is a wrong metric right okay first of all the question that you posed is very very nice but the answer could be improved upon. Would somebody like to give a better answer? Remember, what did I say about the different metrics? Should you trust any one metric always? No. So R squared is just one metric, residual analysis, all of these things. If each metric in itself, if you just stress that, you will be misled, right? So balance everything out. It should not only be good r squared, but when you plot it out, it should like, especially in two dimensions, you have the luxury of plotting it out. It should agree with your intuition that this is a better model. You should look at the residuals and so forth. So when you look at everything, then make a judgment call. Never make a judgment call based on just one metric. In fact, that is the biggest flaw that you find data scientists commit. I mean, error data scientists commit. they get fixated on one. We talked about a p-value, right, in classification and in regression also, that for the longest time, a lot of people, they would publish research papers saying, this relationship must be true. See, my p-value is less than 0.0, less than 5%, right? uh less than five percent right so how significant so but that's not true no one metric should be trusted too much it is the it is the when all the metrics agree with each other the balanced view taken from all the metrics is gives you a good model always so ask if that alpha value yes how do you know from where to where to try out oh so there is a way so that brings us to the whole topic so good question what albert asked is how do you know which values of alpha to take so that brings us to the topic of cross validation because it's a whole topic i want to cover it properly i will punt on it for today but i'll just give a very simple answer what you have to do is people do all sorts of things they put random values and try they do grid search grid search is the oldest way of doing it and the reason I'm not mentioning those ways is because there is actually a much more efficient way of doing it that uses Bayesian inference for hyperparameter tuning. Alpha is a hyperparameter and there are some very elegant ways of dealing with it. I want to cover it as a whole topic. We'll do that at some point. But for now, just go hunt for it all over the place. Random search or grid search, whatever you want. Yeah, go ahead. Yeah, so still one more question which I have is, so let's say now the data was very small, so you can plot it out and see. Let's say the data was very small so you can plot it out and say let's say the data was very huge and you cannot see the plot or it's too complex you cannot figure it out so is there any any metric so you can figure out let's say the range phenomena is zero so you know this is the best graph you can get or let's say yeah other way around a part of it is what we are going to do the next session which is dimensionality reduction so there are ways of knowing see when you are flying blind when you're in higher dimensional space you can still do residual analysis you remember you do have access to residual analysis you can still yeah which is a big big tool that most people don't know about, unfortunately. You can still look at your mean squared error and try to minimize it on test data. So there are things that you can still do. And you can still do aggressively try dimensionality reduction and see how well the data in the model visualize in lower dimensional spaces. For example, in two dimensional space, how does it look and so forth. So that's the topic for next class, next week. All right guys, so if we are done, I would like to move to another topic today, which I thought I would cover. Where is that? Remember, I've been showing you plots like this. When we did the California housing and the breast cancer data, you have been seeing these plots, which I call the kernel density plot. So today, I thought it is an important topic for which we'll never get to devote an entire week. is an important topic for which we'll never get to devote an entire week, but because regularization was a relatively shorter thing, I will use this next hour to talk about it. The topic is very interesting. It's called density estimators. What does density mean? Density means probability density. If you say the, if you say that I find some data at some point, what does it tell you? That it doesn't say that right at that point data must be found. But is that if data is there, there is a reasonable probability that data could be in that neighborhood, isn't it? So for example, if you see a duck somewhere, it doesn't mean that that's exactly the place in the water where you'll find the duck, but it means that in that neighborhood, you're likely to find ducks. Would you agree with that? So the presence of data at a given value induces you to believe that in the vicinity of that value, the data can be found. believe that in the vicinity of that value the data can be found right so when you look at a distribution of data and from from limited data you ask this question that if the data was infinite asymptotically a huge amount what would your histogram look like right That's the question we are going to answer. So we'll take it with a simple example. So look at a pond. You have a pond here where there are ducks and geese. They have flocked into the water. Then this picture, did I take? Okay, it is taken by Jonathan Pillett. Okay, I took the picture from somewhere, from Unsplash actually, and this is the credit to the photographer. So suppose you have to count how many birds of each species are there in the pond, right? An intuitive approach would be to keep two piles of stones, one labeled duck, another labeled goose. Each time we observe a duck, we find a nearby stone and add it to the duck pile. And likewise, when you see a goose, you add it to the goose pile, right? So then you'll end up with two piles of stones that will give you the number of ducks and the number of goose. Make sense, guys? As a kid, if you were counting counting and so let's see how many goo how many geese do we find one two three four how many ducks do we find lots of them one two three four five six seven eight nine i think i count nine or ten something like that we'll find out eleven actually eleven Where did the 11th one come from? 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11. Yes, 11 ducks and four goose. So if you recall, we can plot it out as a histogram, right? Now it's a bar plot. I count how many ducks there are how many goose there are i deliberately made it in the horizontal axis so that you could see the comparison much more 4 versus 11. so now if i ask you what suppose you see a bird in the pond what is the probability that it is a goose in the pond what is the probability that it is a goose what would you answer four divided by so basically number of goose divided by number of goose plus number of duck exactly that would be your the divided by the total it will give you the probability of a goose and likewise you can have the probability of a duck isn't it it? When you divide it by the total, you would say there is a 73% chance that it's a duck and about 26, 27% chance that it is a goose, right? Simply by dividing it. So you're going with a hypothesis. You're going with the hypothesis that this sample of data that you're looking at is representative of the population, isn't it? Means it has been taken from the population. The word people often use, it's a technical word, they say IID. So the presence of a duck is independent of the presence of other decks and Goose in that pond in that sample so each data point is independent and identically distributed means they represent the same underlying reality that the population does the population that you can't see isn't it And what we are trying to do is by looking at the sample, we are trying to say something about the population itself. So the best we can do is if we were to trust this sample, we would say that the probabilities are like that, right? Now all i've done is divided it by the total to get the probabilities now what happens consider a univariate variable whose values are continuous for example people's heights are continuous isn't it weights are continuous and which is why when you stand on the scale twice over you get two different values quite often. And they are fractional values if your scale is capable of fractional values. So suppose you create a histogram. What do you do in the histogram? How do you show the frequency or the likelihood of a value being something? The easiest way is you first discretize a continuous variable, weight or age. So you may say, well, you know, instead of taking age down to the second millisecond, what I can do is I could say age by years, one year, two year, three year, four year, and so forth. So what have you done? Let's say that the human age goes from zero to let's say 140 right i believe that's about the age of the longest living known person a recorded person so 0 to 140 so what can you do you can make 140 bins discretize it bin it right or you could do you could say, well, maybe I'm looking at below 25, between 25 and 50, between 50 and 75, between 7,500, and so forth. You can choose the size of your bin, the discretization or quantization intervals. When you do that, then what you can do, each data goes into one bin isn't it for example if you say how many years old you are we've been people into this different years and then you count how many of people how many data points you found in a given bin so imagine literally visualize in your mind little bins little um every time a person turns out to be 54, you go to the bin that is meant for 50 to 60 and put a pebble there, right? You put a, and now that bin has one more pebble and they will all add up, right? And all you do is you count how many there are in each bin. So that is the intuition of a histogram that will produce data. Let's say that you did that. You took 100 bins, and you did that. And one of the things you can do is you can convert it into a probability by dividing it by the total, isn't it? Right? You take the histogram. You need not take the histogram. A regular histogram would be like this. So suppose you counted 100 bins. You got a value like this. So it turns out that the most frequent value is around four and around four, they are close to 550 or 575 data points. Right. This is the most probable value. This is also pretty probable. It's around 400, a little shy of 400. So these are probable values, whereas minus two is very unlikely, plus six is rather unlikely, right? So you know, looking at this, that if you divide it by the total, you will get the probabilities, right? You'll get the probability, that is it. Probability density. So the way to do that is, unfortunately, Matplotlib makes it very easy. If you say density is equal to true, it will make it into a probability density. It will automatically divide it by the total for you, right? So when you look at this data, just ponder over this data, it gives you the probability of value being between belonging to a certain bin. But as you look at this picture in your mind's eye, you can easily see the outlines. If you look at the silhouette of this of this graph, see where my mouse is going. Can you imagine a continuous, along these points, do you see a continuous curve in the limit of lots of bins? Do you see a continuous curve? That would be a better reality, isn't it? It is, this data leads you to believe that if you had even more data this entire thing will begin to look much more smooth right so that is see how the shape of the bin responds to the number see here's what we do See how the shape of the bin responds to the number. See, here's what we do. When you take very discrete values, you get binning like this. You decrease the size of the bin, it begins to look like this, the blue one, the middle one. When you decrease the size of the bin, and so increase the number of bins, what does it do? Do you see that as you increase the number of bins, you practically begin to see the silhouette, a continuous silhouette, except that it's not continuous, still it's pretty jagged here. But in your mind, you can see that if there was lots of data, far more data than you have, this would perhaps be a continuous curve. So that continuous curve is called the density estimator. What is it trying to tell you? What is this sample trying to tell you about the entire population? So that is where going from discrete to continuous is what we'll do. So the idea is this. You assume that if I find a data somewhere, it is trying to say that data can be anywhere in that neighborhood, more likely at that point. And as you go further and further away from it, perhaps less likely. The probability decays of finding data the further you go from that point, isn't it? probability decays of finding data the further you go from that point, isn't it? In very common sense terms, if somebody notices that you are where you are currently sitting, which is probably at your home, right? What does it mean at some particular location? Let's say you're sitting in front of a laptop on your study desk, would it be reasonable to say that there is still a reasonable probability that after a little while, we'll find you somewhere perhaps in the house, not too far from the desk? There's a much higher probability than finding you two miles from where you are sitting now. That probability will be much less. Isn't it? So ask if you're getting multiple bell curves here. Oh, yeah, yeah. We are coming to that. So the thing is, no, but just take one point. So each point has, in fact, like a zone of influence of where it is likely to be. Isn't it? So that zone of influence of where you say that not just this point, but it is likely to be, isn't it? So that zone of influence, so where you say that not just this point, but it is likely to be around it and the probability of it being somewhere there peaks at where you actually saw the data, right? So that probability curve that you make around each point is called a kernel. Are we together? It is given by a kernel function. It is typically written as k. So suppose xi. Xi is a data point, a location at which you actually observe the data. Means at a different location, there is still a probability of finding some other data point, but that probability gradually decreases. Isn't it? So if and how quickly it decreases or doesn't decrease is controlled by or usually there's a hyper parameter there called the bandwidth. You may choose to decrease it very slowly or you can choose to decrease it rapidly. You can say if you go too far you're not going to find, you're not likely to find data. Or you can say well now if data is here it could be like when children are on a playground they're running all over the place. Then you can say well if I find a kid here the kid could be somewhere pretty far off on the play field. Isn't it? On the other, so reasoning like that, reason it like that. So there is a hyper parameter, which is called the bandwidth. It is typically written in the literature as H, right? And so, a scaled notion of distance therefore, is the X minus Xi. So at some point, close to the data point, Xi being the data point xi being the data point and you say scaled by the bandwidth right and so the most intuitive way that you can think of is the bell curve right or put a bell curve around that point it's a smooth how does a bell curve look it peaks at that point isn't it and then slowly decays so put a bell curve at that. So bell curve is a very common kernel that is used. By now, you must have noticed that machine learning people, they absolutely love bell curves, or more formally Gaussian distributions, a Gaussian distribution or a normal distribution. But we'll just call it a bell curve. This is the ubiquitous bell curve. So now, suppose I do this, comes an interesting thought. Each point that I find, I put a little kernel there. So when there are so many points, a thousand points, I will have a thousand kernels, a thousand little bell curves. And so if at a given value of x arbitrary value of x i will have probabilities are the influences of each of the points each of the thousand points isn't it this point is saying the probability of finding a value here is 0.3 this point is saying the probability of finding a value here is 0.1 so what is the total probability of finding a point there? Data point there? 0.3 plus 0.1 is 0.4, isn't it? So they're additive. All of those probabilities add up. And that is what happens when you do that. So this is, of course, your bell curve. This is what a bell curve looks like. So this is a lovely kernel, right? It says wherever you actually found the data, it peaks. As you go away from the data, it sort of dies off, right? But it's not the only kernel. There are many, many kernels used in the literature. And I invite you to go find these kernels, one of them exponential kernel, cosine kernel. So think of a cosine wave what does it do it also decreases from one to zero isn't it so you can think of a cosine top hat then Ipanich I'm never able to pronounce this guy's name the Ipanichinkov Ipanichinkov's kernel is actually happens to be one of the after Gaussian it happens to be the next most common kernel used in the literature. Right. So that's that. So you could use whatever kernel you want. Generally, what happens is the choice of the kernel doesn't matter so much. But given a kernel, all of these kernels will have a hyperparameter, the bandwidth, like how far the influence goes. And you have to play with that. And that bandwidth gives you a bias variance tradeoff, as you'll see. So here what I do is I show you the bell curve for different values of H, different values of the fatness of the variance of the curve. So if h is very small, you can see it's very peaked. If the h is equal to 5, you're saying, okay, the influence will go a little bit further. h is equal to 10, you're saying influence dies even more slowly. h is equal to 20 says, well, influence goes still pretty far, minus 50 to plus 50 distance, right? So what does it do to the whole kernel's plot? When you take, so this whole thing, by the way, you can say, can I compare these to each other? The same thing plotted here. I plotted it to have relative comparison. You can see h is equal to 1 is very peaked. h is equal to 5 is the blue one, a little bit more spread out. h is equal to one is very peaked. H is equal to five is the blue one. Little bit more spread out. H is equal to 10, even more spread out. And H is equal to 20 has a pretty wide field of influence, isn't it? It's casting its net pretty wide. So when you use that to compute the kernel, one of the lovely things that you can do, but by the way, the beautiful thing is that these things come built in in scikit-learn. This comes under the neighborhood methods. You're saying if a point is here, then in the neighborhood you can find other points. So this is the kernel density. This is the introduction to the topic of kernel density. We choose a kernel of this, we take a bandwidth and we fit the data to it. When we do that, you see that, and you can plot it out. This is a little bit of mechanics of the code. How do you do that? You create it, you create samples of the data, then you take the log density, and then you take the log density, and then you take the density, it turns out that Gaussian, this particular function will give you log density, and then you have to exponentiate it. The mechanics of that library, but then it looks like this. Now, guys, pay attention to this curve. For a moment, stare at this curve. And now think, is this a good asymptotic limit of what these curves are tending towards? As you increase the number of bins from the left to right, you come here. Right is this purple one. Would you say that if you had even more data, the asymptotic limit of it would have been this curve, isn't it? And that is what a kernel density estimator does. It says, even though you don't have infinitely many points, but by taking a basic assumption, so that's your induced bias here, that if you find a data somewhere, there's a non-zero probability of finding data in the neighborhood also. With that assumption, you can come up with a density estimator plot without having infinitely many data points. But there is a catch here. There is a bandwidth parameter, which is the hyperparameter of the model. Let us see what it does. Here, for the same data, I plotted out the kernel density estimator plot for different values of H. So when H is zero, H is zero means it is your plain and simple histogram. If data is there, count it. Just adjacent to it, there is no likelihood of data, right? So it is a plain, simple histogram. This is exactly what you saw, that purple graph that you saw, isn't it? So look at the top left. And would you agree that this is exactly the same as this, the rightmost, right? So when H is zero, when you're not applying any kernel whatsoever, you get this top right. But as you increase H, little values of H, do you see that it's smoothing itself out? Gradually, it gets smoother and smoother. But then after a little while, it over smooths. Do you notice that after a little while, it is getting over smooth. So you look at the bottom right. What do you say about the bottom right? Bottom right is when this influence of a point is going pretty far means you're saying if this point is here then very likely the point will be here here at so many places so in that case would you consider h is equal to 100 a good representation of the probabilities no you wouldn't consider that yeah that's right so what happens is that these two represent the two extremes, the top left and the bottom right. They represent two extremes. And you have to use your judgment because it's a hyperparameter. You have to use your judgment to decide what is a good value, right? Where would you like to stop? And so if you had to do that, where would you like to stop right and so if you had to do that or where would you like to stop like anyone where would you like to stop uh i would pick up uh 0.5 0.5 0.5 or maybe this one 0.21 look at this uh 0.2 0.2 it's a personal job 0.25 look at 0.25 yeah here is 0.25 0.25 and look at this curve you know there is a very steep valley here right do you see the steep valley where my mouse is so you you can't overlook the steep valley. So which one of them reflects a reasonably steep valley? 0.25 does. And in the vicinity of it, you can pick one, a 0.17 or somewhere near that. 0.5, the valley is not as steep. Do you notice that? Right? Your valley is not as steep. Oh, okay. Sorry. i picked up the uh this one right yeah it's 0.2 yeah 0.29 i picked up okay 0.29 yeah 0.25 0.29 these are your reasonable ones and there are no perfect answers but that's a pretty good answer right at some point that you have to know and this is the thing with the hyperparameters, you can then cross validate, you can take a test data and see, are you seeing a similar distribution or the probabilities are working out or not. So this is the concept of kernel density estimator. And all of these plotting libraries they give you so now let's go back and look at the auto data set that we played with in the auto data set you have all of these histograms do you remember right the horsepower how does the probability density of the horsepower etc etc as histograms so the logical thing to do is what to now convert these things to kernel densities. And so when you convert it to kernel densities, they look like this beautiful curve, which in some sense is more informative than just histograms. Would you agree, guys? So let me put both of these together. Would you feel that kernel density estimator is in some sense capturing the information conveyed by the histogram? is in some sense capturing the information conveyed by the histogram. Right? And that is its value. That is its value. And you can then superimpose. You can also do the kernel density estimator by some categorical variable, like, for example, the origin, country of origin. Is it an American car? Is it a European car? Or is it a Japanese car? These are the three origins, if you remember, in the auto data set. So you can split out the kernel densities by specific categorical values, but you can also do multidimensional kernels, two-dimensional kernels. The same argument applies, and it comes to beautiful plots. For example, if you look at horsepower and miles per gallon, what do you notice? These are example, if you look at horsepower and miles per gallon, what do you notice? These are areas, if you were to imagine, it is basically a two-dimensional plane and hills rising above it, right? Bell hills rising above it. So the higher the bell hill, the more yellow it is, right? The more bright it is, right? So this is represented in 2D plane as contour lines. So what does it show show most of the cars are most of the cars are here would you agree most of the cars are this region and this place this bright yellow place is what is it it is of approximately well in those days used to be 75 horsepower and they used to give approximately 25 30 miles a gallon which by the way was a sensible car in those days in the 80s right people wanted high mileage gas was already getting expensive. If you remember, those were the years when we had one of the earliest oil prices, the crisis in which oil prices were shooting up, gasoline prices were shooting up, and everybody wanted efficient cars. Then there's always the enthusiast crowd that wants high horsepower vehicles that doesn't give you too much mileage so it would give you 14 15 miles a gallon those cars are still there V8 right so if you're one of the enthusiasts like I tend to be I tend to like I tend to love cars and sports cars so despite my age so I have a car which is like this. Well, except that horsepower is not 150 anymore. My car has 600 horsepower, but it still gives about 13 miles a gallon, 14 miles a gallon. So this is another peak. So you can literally see a reality here. See this kind of grass speak a lot. They tell you that the bulk of the people are sensible people driving They tell you that the bulk of the people are sensible people driving economy cars, like fuel-efficient cars. And they don't mind the fact that the machines don't have high horsepower. It's good enough to run on the highway at 60, 70 miles, 80 miles an hour. Then there is the enthusiast crowd that likes a little high horsepower machine. Do you see that, guys? So that was the reality. This is even today, the reality, the numbers have changed. The scale, which is going only up to 200, now goes up to 600. So these are the contour lines for it. And you see the same kernel density plots. So I hope it gives you the intuition to these very, very useful plots called kernel density estimation plots. Now you know what they mean. So a kernel is the zone of influence around a point. The probability that if if a point if a data is seen here that it could have been just as well anywhere in the neighborhood at that point isn't it that's what it is and so here we go we break it up by categorical variables etc etc and that's so questions guys this kernel densities are a beautiful concept i wanted to give an hour specifically to this so i thought we'll go over it if the concept is not clear please ask me kernels by the way are a big big thing in machine learning you'll come across in the next class one of the big themes will be kernel machines right in vectors, well, you are here at support vectors. This is named after a kernel machine called support vector machines. The word, the support vectors comes from support vector machines. One of the most beautiful algorithms that you have out there. Support vector classifiers, support vector regressors, support vector clusters also that many people don't know you can use it for. And we'll deal with all of that in the next workshop. And I am done. Questions, guys? That's it for today. And I'll take questions.