 Today we have to do the lab for automated machine learning. Now automated machine learning, we covered a bit of theory. I'd like to finish that theory to some extent. I may not cover all the details of the TPE, the Tree Structured Parse and Estimator. We'll keep that maybe for Saturday or sometime later because otherwise we'll get derailed. Today our main goal is to do the labs. Now automated Now, automated machine learning or auto AI, auto ML, many such words that people are using, it's a very active area of research and development. You see so many, many papers coming out on it. You also see a lot of libraries coming out of it. I mean, there's an entire website, I believe it's autoML.org, that catalogues all the different autoML libraries that people have created. I have posted that. You'll find links to that on the course portal. And when you go there, you'll be surprised how many libraries are coming up with it. Even for a library, mature library like scikit-learn which we have all been using, it comes as a surprise for you perhaps to learn that people have been creating automated machine learning versions of scikit-learn. They're saying let's make something that is so easy to integrate with scikit that people don't even realize that the learning curve is close to zero. And so I'll pick one of these libraries. Many there's auto scikit learn the scikit, SK opt there is scikit optimize and sk learn optimize and so on and so forth. There are many of them. And then there are independent libraries, like, for example, Hyperopt, and then there is the TPOT, and many mature and very, very good libraries are emerging in this space. A lot of excellent work is being done. Now, while all that excellent work is being done, when you play around with these libraries, you realize that a lot of them, not all of them, but a lot of them have very rough edges. Those libraries are nowhere as mature as PyTorch or Scikit-learn. So the mileage varies. You try and what will happen is today can be one of the more frustrating labs for you in the sense that a lot of these very promising libraries they will tell you pull the code from GitHub right the library literally from the GitHub clone it from there and then do this and that installation step and then if all the stars in the sky are aligned it might work on your laptop. So in other words, they have a lot of rough edges and it takes some amount of effort to make them work. So with that being true, what I did is instead of picking the most sort of dominant libraries, and I'll talk about them and go through some examples, I will talk about a few dominant libraries. I picked the ones which are the least frustrating because our goal is to learn to walk before we run, right? So I picked the lab based on simpler libraries, gets the work done and are promising, very promising. They too are in the whole race. And so we'll proceed from that. And I'll take some examples from that. What you will learn is that it's a very rich area and lots of people are coming. Now, while for many things, for all other parts of AI, I have sort of, I have a distinct and huge advantage in the sense that I've been doing it, practicing it for a long time and carrying it forward. When it comes to AutoML, the things are happening so fast that quite likely the tables are turned. Once I teach you what I have to teach, the theory, of course, I will teach you properly. But when it comes to the practice the actual libraries that people are creating and so on and so forth this is an opportunity for me to learn from you guys and let us form and I want to do this lab in a slightly different way there will be some things I'll do in common and then what I would like to do is make this lab into many projects. You guys have already the project teams. What we should do is divvy up the many, many libraries that are doing auto-embed, and each team can pick up a couple of those, post it that we are going to create examples using these. And let us all evaluate all of these libraries. It is not a simple task to sit and evaluate all the libraries, nor does one person have the time to do that and go through the frustrating setup. But if we do it as a project, if we all come together and one group takes two or three libraries here and does pretty much the same example, MNIST or this or that. Another group takes something else and then they do a few of those. We can quickly create notebooks that cover the whole gamut of these emerging libraries. Are we together, Dyson? So that is the approach I would like to take. That way we will all enrich, it will be collective effort, we'll all enrich it will be collective effort we'll all enrich we will create a collection of those notebooks on our course portal and with those notebooks we can all benefit from each other without going to the frustration of installing all like two dozen libraries or three dozen libraries that are emerging in this space so does that sound like a good plan guys that are emerging in this space. So does that sound like a good plan guys? Yes. Also frameworks that we've learned about too? Yes, yes. All these libraries and frameworks, yes. Okay, great. And also guys, like the paper, like for example, I believe, was it you Kate who brought about the AutoZero, AutoML0? No, I know about the Newton framework company that has a framework. Newton is very good. So guys, at this point, what is happening is things are just in a process of exploding, just sudden explosion of interest in libraries and projects and companies getting into it. So let us bring or help each other in coming up to speed. Who pointed out AutoML0? I forgot, but AutoML0, I think Pradeep it was you. Yeah, I see I'm Praveen. Praveen, yes you did. So let's do one thing. I see, in fact, I was thinking that I'll cover AutoML0 for this coming Saturday, coming Sunday paper reading. Let's read that blog through. I think we'll review that blog. It's very interesting paper. It's very early work. The success is still far off from a very practical perspective, but very promising work in the AutoML space. So let us review that. So as you guys explore this topic, and I would tell you that take this topic very, very seriously. It can be your differentiator from the crowd. At this moment, not many people have woken up to it. It's not something that you find gazillions of text with this, you know, O'Reilly books and Manning books talking about it yet. They're beginning to get into this space. So, and it's very fertile, very new. And if you use it in your workplace, you do have a certain and very distinct advantage. Some of these are very easy to use, so we should do that. And so they share these resources. Like for me, it was educational to be shown Newton that Kate brought and this auto ms0 that Praveen pointed out. It was quite the thing and I literally sat down and did my due diligence on that in the last couple of days and I learned something. So this particular topic is such that we can all learn from each other and I can explain you the foundational theory which I am strong in but I can explain you the foundational theory, which I am strong in, but I can't keep track of all the emerging libraries and projects in this space. So let us help each other out on this. Make it a collaborative effort. With that said, are you guys seeing my screen? Yes. OK, so what I will do now is I will quickly review the theory and want to give much of the time to the practice. But I think as as I could see, the theory has been a little bit confusing for people who don't have a background in in Bayesian thinking and so on and so forth. So let us recap what we did and let's finish the discussion we were doing the last time. So if you have been doing ML 100, 200 and you have been reading any of these O'Reilly Manning books, you must have heard or seen these techniques, grid search. And most of the people recommend grid search. Interviews, they ask you how would you do the best hyperparameter search? And you are supposed to dutifully answer that you'll do a grid search across the hyperparameter space. Grid search was the standard for quite some time, till somebody realized that actually, if you instead of grid search, you do a randomized search, If you have a budget for, let's say, 25 points, instead of picking, and let's say that you have two hyperparameters, H1 and H2, instead of creating a grid of 5 by 5 values, creating 25 values, so this should be 5 by 5, 25 values, instead, it is much better to take random points, right, because random points will search the space better, especially if one of the hyperparameters, the last function is rather insensitive to, the objective function is insensitive to, right. So then what will happen is if you do grid search and let's say one of the hyperparameters doesn't matter, then you have searched only five hyperparameter values for the one that really matters. But if you do randomized search, you will end up picking 25 values for each of the hyperparameters. And so you have a richer coverage of the hyperparameter space so this is what we discussed and then let me just review what just in case we have forgotten i keep speaking of the hyperparameter space first of all how does hyperparameter differ from parameters so let's say that you have a neural network, many of these. So this is it. So the weights and biases, the weights here, biases in this, these are the, what are these? Are these the hyperparameters guys? What are these? Are these the hyperparameters guys? Parameters. These are parameters. And how do you find the optimal value of the parameters? The model parameters? Back propagation. Excellent, yes. So the good thing with that is we can use a first order methods and maybe up to a little bit of second order for doing optimizer. We covered this on Sunday, the various kinds of optimizer that we looked at the second moment and so on and so forth. Not second order, but the second moment and a gradient and gradient square and so forth. So at the end of the day, you can find, use all of that to create an optimizer, which is some variant of this, some variant of w next is the value of w minus alpha grad of the loss function. Now, this is the base of the simple vanilla version and then you complicate it for Adam and RMS prop and you you throw in more like for example you throw in the momentum base or exponential smoothing. Asif you used the term Adam earlier also can you just you maybe also. Can you just give maybe explanation for the momentum? See, on Sunday we talked about the fact that you can do exponential smoothing so that suppose you have a last surface like this. If you just follow the gradient from mini badge, your journey will be very erratic. Well, I'm making making i'm sure i'm making some weird mistakes but it sort of is oscillating a lot when you apply the were you there on sunday yeah i i i got the smoothing effect you know all that yeah so it will just create adam i don't recall that so what happens is when you take in smoothing there are a couple of ideas that come in one is that you need to do the smoothing which is through exponential exponential smoothing right or momentum you apply a bit of momentum means you have a certain momentum in a direction even if you are being pointed to another direction you make a certain momentum in a direction even if you're being pointed to another direction you make a slight shift in that direction but not too much you listen only a little bit then you have to do a little bit of a bias correction right remember that if you have a lot of points and you in the beginning it will start low and so to lift it up and do this these are all you know implementation details it doesn't from a conceptual perspective it doesn't really matter then you also the RMS prop and Jeffrey Hinton the great Jeffrey Hinton in this book in one of his courses actually, he surprised everyone by releasing a new idea to improve the optimizers but instead of in a paper he just casually mentioned it in one of his courses that he was teaching. Proc, which also looks at the Delta L square. It looks at the second moment. It's called the second moment in the language of physics and so forth. And then somebody said that, hey, we are doing it. This is a good idea and this is a good idea. And what happens if we do a two plus three? Put all the good ideas together to create one big optimizer that benefits from everything. And that became Adam. So what it stands for, I suppose, is adaptive momentum or something like that. Momentum, I think. Not quite sure. So this is it. And then Adam is no more the state of the art. Now the latest is Adam W. And then that also may not be the state of the art. The thing is moving forward. And then we come to the world which we covered on Sunday of learned optimizers. We're seeing that why bother hand creating these optimizers? Learn the best optimizer. And that is still in its infancy, optimizers. Let's learn it as create a neural network to learn the best optimizer function. Because ultimately, if you look at it, W next is some function. If you write it as W minus, even in the simplest form, it is some function of W alpha gradient of L. And then you generalize from that. You say that, well, suppose I throw in the second term also. It would be F is a function of W alpha gradient of l and gradient of l squared and of course the loss itself let's throw in the loss itself and by the way there is also a beta when you bring in the rms prop you'll realize that somewhere in there there is a beta also that comes in one more hyper parameter comes in actually hyper parameter this is the momentum term, momentum, momentum methods. They all have one more hyperparameter for alpha, beta. This is your learning rate. So this is basically a very quick summary of the optimizers. And the F is supposed to be included in the parentheses with everything else? Oh no no no, it is indeed not. Thank you for pointing that out. f is a function. I'm saying f is some very complicated and unknown function of all of this, right? So we are in the business of discovering unknown functions or approximating it using the neural network, isn't it? So approximating it using the neural network, isn't it? So now the thinking is why handcraft those optimizers? Why not treat the optimizer as an unknown function of all of these inputs and use these to now discover or learn the best F, right? And that is the world, that is the latest and greatest, I mean, I wouldn't call it greatest, is the bleeding edge of research that let's have learned optimizers. And obviously, success is very mixed in this moment. It is, you wouldn't, let's say that it's a great promising idea, this work in progress. So San Sanjay does that answer your question yes yes thank you all right guys so we go back and we ask ourselves that if we shouldn't if grid search is like if somebody like let's put it this way if grid search if we say grid search is the right way we are literally wearing a cap on our head saying I am a dinosaur I'm obsolete right a better idea is let's say randomized search which is again good let's say but what can be better than that so that brings us to this world of hyperparameter optimization. The basic idea of hyperparameter optimization comes or the core, the heart of it, I mean there are many methods of doing hyperparameter optimization. One is for example randomized search. You are doing that but we are going to take a much more nuanced approach and it turns out that when you don't have a gradient to do gradient descent on so let's ask that question again so i'm going to summarize it again in simple forms see guys what happens is that for parameters you can do gradient descent Right? Because you know gradient of the loss. But gradient of the learning, the loss, with respect to the hyper parameters of the model. Are we together? In fact, it doesn't make sense. So suppose if one of the hyper parameters is which particular algorithm itself to choose, or how many layers to have in your neural net, you can't really get a gradient gradient simply doesn't exist of the loss function with respect to this like there is no concept of gradient of the loss function with respect to which of the algorithms you picked random forest versus decision tree versus svm versus you know cnn versus feed forward network and so on and so forth. There is no such concept. So what do you do? This is where the wonderful, wonderful world of Bayesian learning comes in. Now those of you who have been with me in the math class, of course, I have given you, we did remember we did all of this little interesting experiments. Why did this show up suddenly? We did this interesting experiments where we, you know, I don't know if you guys remember, I gave you a target, I gave you something, I think a ball on a thing, and I said go hit it. Do you guys remember? I said go hit it. Do you guys remember? From a distance of about 20 feet on the stage, you had to throw and try to hit it. How many of you remember that experiment in class? I do. I remember. You guys do remember. It was fun. So very literally what you saw is that before you did that, you all had a guess on how often you'll be able to hit the target, how easy it will be. You started with the prior probabilities and some of you felt pretty confident. Yeah, you'll be 80% of the time I'll be able to hit the target. And then as you did more and more trials and then you realize that it was actually quite a bit harder to hit the target, you came to a posterior belief that the probability or the proportion of times you can hit the target is much, much lower. And so we went through this process of learning, right? That is Bayeshan learning, in which every time you throw and you hit it, you say that my probability goes up. Every time you miss it, you say that my probability goes down. Right? And so gradually you come to the right, with enough number of trials, you come to the right probability that is the actual posterior probability that you will hit it, realistic. And the interesting point there is you could all have started from very different prior beliefs. Somebody is optimistic, somebody is pessimistic, and so on and so forth. But as you keep throwing, doing your target, trying to hit the target, you converge on a stable posterior, which is the reality. And that is the beauty of the magic of biational learning. It is the most efficient journey to converge to the right hypothesis of the most optimal posterior hypothesis. Given data, each data point helps you learn. And how did we do that? I'll just review that concept very, very quickly. So yesterday I took the, or day before, i took the example that uh let us say that magically you have uh ended up and i'll now take three cases instead of four or maybe three four number one the other two is mostly raining three is sparsely raining or mostly rainy mostly let's just say uh medium half, half, something like that. Or four is mostly sunny and five is desert. I believe we took this to be California, this torizona or something like that so what we are seeing is mostly reni is saying the prior probability of prior probability is equal to one probability of range remember i use pi for prior and mostly reni is let us say that i set it to 0.9 medium let's say to be 0.5 percent chance. Half the days it rains. Mostly sunny is, let's say, 0.1. And desert is mostly like prior probability is 0, isn't it? So this is your prior probability values. Now what happens? Now you start seeing evidence. You, as part of your, I believe we concocted the story that you have just been put onto a plane and you have landed somewhere in the witness protection and you have no idea where you are. So the first day you notice that, so let's say that the evidence says 10 days and you have no idea where you are. So the first day you notice that, so let's say that the evidence says 10 days of one, two, three, four, five, six, seven, eight, nine, 10. Let's say first day you have rain, sunny. Let's say rain, rain, rain, sunny. Rain, rain, sunny. And we can throw in maybe rain, sunny, rain, or sunny rain. Let me just put it, no particular reason. I'm just putting in random values. I wish if you were doing it in a Jupyter Notebook, we would let it generate the random values with a certain posterior probability. So let us say that this is the evidence you are seeing, right? So tell me the first day it rains, what happens to your five hypothesis? Which of the hypothesis won't survive? So if the likelihood on the first day, L1, is it rained. So how likely is the desert to produce that outcome? What is the probability that you'll see rain in a desert? Zero. Zero. What is the probability that, what was the like, like if this hypothesis is traced mostly sunny, what is the likelihood of seeing this particular day of evidence? Well it is just 0.1, this is 0.5, 0.9 and this is 1. So already you notice something interesting has happened. When you multiply these two, you will get numbers which are in a way, and when you normalize it. So what you do is, let me take you through one step. You multiply this. So this is the posterior of one, first day posterior. One times one is this. So let me call it unnormalized posterior. 0.81, 0.25, 0.01, right? And this is 0. When I add up all of this unnormalized posteriors. So this is, this is what I call the bias box, right? Think of it as a toolbox. Plumbers use their wrenches and other things and biotins use their bias toolbox to get to the right answer. So you can add up all of these. patients use their bias toolbox to get to the right answer. So. You can add up all of these, anybody can quickly give me the answer. One of you, could you please do it on the calculator and tell me what it is? Point six, two point. Zero six, two point zero seven, is it correct to like my answer says is two point zero seven? Yes, 2.07. Is it correct? Like my answer says it's 2.07. Yes, that's right. And now divide, tell me guys that if I multiply or what I do is I take each of these probabilities and I, well, okay. I'll just let it be here for the timing. You get an unnormalized posterior probability like that. The way to normalize it is just to add it up and divide it by, so that all the numbers add up to one, right? So for example, if I divide this, this will be approximately half, right? Posterior P1 will be, it is likely a little bit greater than half it will be a little bit less than maybe let's say that this is a 0.55 0.52 is this reasonable this will be approximately 0.4 this will be approximately 1 8 0.16 this will be approximately 180.16 this will be like vanishingly small one out of hundred 0.005 i mean i'm just doing rough calculations in my mind if one of you have a calculator you can quickly verify whether i hit the right ballpark or not. Does this look okay? If I divide each of these numbers by 2.07, will I get that posterior? Yeah, please, but first one should be little less than half. Oh, yes, yes, you're right, actually. Absolutely, thank you. It should not be more than half. It should be a little less than half. Let's say 4.8. 4.9 actually. Do it actually. One of you, could you just do it on the calculator and tell me? It's 4.8. Come again? 0.48. 0.48. Okay. 0.48. So there we are. So are so guys now think about it what is it saying it is saying that you have 50 likely that 48 percent chance that you're in the rainiest place 40 chance that you're in the mostly rainy place right and um place, right? And okay, actually, the way to do this was likelihood, no, I multiplied it wrong guys. See, in the beginning, the probability that any of these happen is one fifth. Oh, that is a screw up, big screw up. See, in the beginning, your prior was, prior is not this. Prior, hypothesis is 0 is 1, hypothesis is 0.9. So these are your prior hypothesis. The prior probabilities of this, all of these are the same. So let's do the calculation again. screwed it up i apologize so can somebody do this calculation for me this is what happens when you try to do everything in as we talk so this is 0.45 this is 0.25 this is 0.05 and this is zero as if yes just a small suggestion when you're putting the biasing box this could be a very handy tool can you please name the label of the columns properly so when we revisit the box okay you you want me to draw it out in a big way huh let me do that anyway i made some mistakes here uh let's do that sorry for wasting time guys with this mistake that I made. But just think this is very powerful stuff and we will forget if we don't remember it properly. Rainy place. So guys, I'm giving you a crash course on Bayesian thinking with an example, mostly rainy. Let's say H3 is medium. Right. So the values you put is P is equal to 1.0. P is equal to. What is it? 0.9. P is equal to 0.5. Let's say that these are hypotheses. H3, H4 is mostly sunny. H5 is desert. Desert probability is 0.0. So what you do is you do this, and let me draw it in huge big things so that we go through this exercise. And if you see this, you'll see why, when you can't do gradient descent, your best option is to do Bayesian optimization and why it has created a whole revolution here, H1. So I'll just write H1, H2, h3, h4, h5. The prior probability of each of this, prior probability of each of this is one fifth. When you don't have any information, you could have landed in any one of these five places. Would you agree? Like your plane just landed and you have no clue. And you are told that you are in one of these five places. So how would you spread your belief? You would spread your belief equally amongst the five hypotheses, isn't it? For all you know, you might be in a desert. For all you know, you might be in a rainforest. Correct. So 1 fifth, 1 fifth, one fifth, one fifth. And guys, you guys can help me if one of you quickly open up a, you know, Python notebook. You can do these little calculations for me that will expedite our work. So let's say that the evidence that we saw was this, the first day it rained. the tea put the evidence back up again rain sunny rain sunny sunny rain right so this is this is our evidence that we are going to work with so let us not look at the whole data let's say that only this data became available to you right first day it rained so now let's do the likelihood of one just one day what is it the first day likelihood is what is the likelihood of seeing rain in a place where your prior probability like your uh a prior probability was a p is equal to one guaranteed to have rain right are we together here please yes right so what happens now is that you are in a place which is very likely to have happened this again actually those values that we did were not way off i think I lost it. If there is a 90% chance of rain, this is it. If there is a 50% chance of rain, one. And the chances of seeing rain here at all is zero. So this hypothesis seems to be already about to be thrown off, isn't it? So we do something called unnormalized posterior disaccalibration. It is just a calculation step I'm giving you. What you do is multiply these two. So this will become 0.25, oh, 0.20, one fifth, sorry, two zero. Can somebody tell me what this will come to? 0.18 right and 1 10 to 0.1 is this correct right and this would be point 0 come again point 0 to 0.02 and this would be zero so let us go add it up 20 plus 18 is 38 38 plus 10 is 48 48 plus 2 is 50 0.50 right now divide each of these numbers with this so 20 over 50 is 40%, 0.4. 18 over 50 is the same as 36 over 100. 10 over 50 is the same as 20. And this is 0.4. And this is 0. So your hypothesis, H1, so you say that in view of just the first data's evidence, my posterior probability, posterior on day one, is already something different. You're saying I have a 40% chance of being in Seattle, I mean, sorry, in the rainiest place, 36% chance of being here. So what has happened guys? Do you notice that these number, initially it was just one fifth,, just point two. This number has increased or decreased the first one, the rainiest place. Increased from Yeah, from one fifth. Yeah, sorry. From one fifth, it has increased right point to this has also increased. This has remained the same. At this moment, this, 0.2. This has also increased. This has remained the same at this moment. This has decreased. This has really decreased. So if you think of probability, unit probability as one bucket of sand, initially the sand was distributed amongst all the five hypotheses. But now what has happened? This place has lost all the sand this place has mostly lost most of its sand and already the sand is you know evidence is decking in favor of the upstairs thing are we together yes now let's say next day it turns out oh goodness it uh it was sunny so the moment it was sunny what is the moment it was sunny, what is the likelihood? Let's look at the L2. And now we will treat this as your prior, let's say, just for the sake of argument. What is the likelihood that you would see a sunny day in the rainiest, in a place where it rains every day? Likelihood is zero? Does that make sense guys? Yes. Likelihood of seeing a sunny day where it rains 90% of the time is 0.1, right? Likelihood of seeing a sunny day in a place where it rains half and half is still 0.5. Remains half. The likelihood of seeing a sunny day in a place where it rains only 10% of the time is 0.9. And this one is 0 already. So this is already out of the game. So H5 has already gone out of the window. Now let's look at the unnormalized posterior after day two. How are things looking like it? Multiply these two, you get 0.4. Multiply this with 0.36. You get what? 0.036 is this correct and i think i'm sorry absolutely absolutely thank you for correcting that that's a big deal actually it becomes zero and that is that is the main point that are coming to this becomes 0.1 and now what happens to this guy what is nine zero can somebody tell me the answer to that 36.0.036 or zero zero three six no no 0.036 0.036 right so this makes sense evidence is equally balanced now now if you look at the total of this uh it is like 100 plus 30 72 0.172 so let us divide the this so now this is your posterior belief after two days of evidence so tell me guys what is 0.036 divided by this guy it's 0.209 that's a point 0.2 you divide 100 by 172 how can it be that that is wrong it is totally wrong yeah yeah i mean divided by one twenty two divided by one seventy two is one x zero point six point two one is the first one point two point one is two two oh yeah so yeah point two one uh point five eight point five eight and point again yeah point two one zero point two one so now look guys what has happened the first one has gone to zero the last one was already zero because once you achieve zero it is not going to get any better. Right? So now where has the sand moved? Two days of evidence, rain, sunny. What seems to be the most likely hypothesis? Which one is winning now? Medium rain. The medium one, right? That is half the days are rainy, half the days are sunny. So things are beginning to look different now. Two hypotheses is gone. Now let us say that you have another three days of rain. So maybe I'll bring in one more day of rain. We can do one day at a time. Do you see how evidence is getting better and better? Initially, you started with one fifth probability. Now you're getting 60 percent close to 60 percent sure you are in a medium rain place. Right. Let's see what happens if it rains one more day. For the for the second day, why did we skip the h5 because ultimately the posterior is zero right why bother finding the likelihood when you'll be multiplying it by zero suppose you found it out okay suppose it was sunny this is one the probability of sunny day is likelihood of it is one but what is the what is the thing you're multiplying it with you're multiplying it with zero isn't it see guys we are multiplying these two things right these two columns because this is the new for day two this is the new prior this is the likelihood prior times likelihood will give you what zero times one even though there's a very strong likelihood of being a desert because it's sunny but you have already lost that hypothesis completely correct okay understood this is it so you're seeing how the sand is getting compacted guys you know you started with uniform probability and already it's becoming like what is it becoming like would you say that is become like this of the five days one two three four five look at the five hypothesis is this the probability distribution yes looking like and so you see how how your belief shifts and changes you can bring in another so now let's do something more. We have another day of rain. At this moment, the half and half is looking strong. Actually, because it's getting tedious, can I do two, three days together at the same time? Let me at least do two more days together, just for the fun of it. Rain, rain. So in the first hypothesis, the hypothesis to probability of rain is 90. So it will be 0.9 times 0.9 is 0.81 likelihood. This is half and half, right? Half times half will be 0.25 likelihood L2. And this would be rain, rain would be this would be rain rain would be uh 0.01 0.1 times what 0.1 is 0.01 and this is gone we don't even bother about it now we can add this up 81 plus 25 is 106 106 plus 1 is 107 right so 1.07 now we need to divide this numbers by so what is the unnormalized posterior up2 say that again it is l3 right d3 oh right right you're right l3 thank you thank you so now tell me which is the unnormalized posterior normalize it the unnormalized likelihood first of all unnormalized posterior multiply these two guys this needs calculator please do that for me 0.76 0.76 what about 0.58 times 0.25 0.23 and what is 0.21 times 0.25. 0.23. And what is 0.21 times 0.01? 0.01. That can't be right. Yeah, I mean, yeah. 0.0021. 0. I got the H2 as 0.16. It has to be very close to 0.01 this one right points so guys what is this 0.01 huh yeah but how is it ending with one instead of two uh it should be 0.16 right the first one at the edge two uh 21. so it has to end with 0.01 to make it all equal to one right summation goes to one yeah isn't it point zero zero two yes no no don't you cannot have the summation don't guess guys compute exactly and tell me yeah well it's not you know nine actually zero point one is zero zero two right at this moment i don't think i can trust any of the calculations you guys are coming up with so what we'll have to do is open up a jupiter notebook and do the computation which seems as if the denominator is one actually so point it should be very close to point zero01 actually it's 0.009 okay guys are you understanding what I'm saying this is 1.07 right I'm dividing each of these values by 1.07 so yeah so all right so this is now likely no no, no, sorry, not this. I am just multiplying these two together. 0.21 times 0.81 is what? 0.76? What is 0.58 times? I made a mistake. Yeah, instead of multiplying, I did the normal expression. Yeah, that's why it's not. So what is 0.58 times 0.25? It cannot be 0.23. It defies belief. All the numbers are wrong then. Yeah, these are wrong numbers. So can you please multiply these two and tell me what it comes to? Yeah, one-fourth of that is. It should be around 0.14 or 1.45. 0.58 times 0.25 is 0.145. That is it. That's more believable, 1.45. So I say the above one is like 0.16. And the fourth one is 0. Third one. You mean the third one uh fourth right like h1 is zero and so from that perspective yeah yeah yeah h4 is point zero zero two yeah that's what i've been saying zero zero two now add up these three numbers what do we come up with the total uh 0.3169 3169 and now tell me what is the posterior belief after the third day divide each of these numbers yeah by the denominator 0.534 five three four and then point uh four five eight four five eight and then point zero zero six yeah so guys do you notice that this one is zero this one is zero and this is on its way to zero the hypothesis four is getting rather shaky because we have three days of rain one day of sunny it doesn't look like a mostly dry place it doesn't seem that we are in california isn't it and now something else has happened compared i'll ask you to compare posterior p2 versus posterior p3 what has happened hypothesis two is now beginning to lead, isn't it? Yes. With one more day of rain, hypothesis, yeah, two more days of rain, hypothesis now two is beginning to lead, hypothesis three is beginning to fall behind, and hypothesis four is pretty much on life support now, right, at this particular moment. And so do you see, and does it agree with your belief? Right at this particular moment. And so do you see and does it agree with your belief if you had if you saw this data. Is this how you would spread the belief, you would say that hypothesis two is more likely than hypothesis three. Right. Hypothesis one and five are dead. Hypothesis four is mostly dead. And what happens if I add one more day of rain, then this will keep on increasing. The next day, we had another day of rain. So do you see how biation works, guys? You just take the evidence. You start with any prior you want. But as you work your way through the data, now what happens to your probability? So now this is your day two, data now what happens to your probability so now this is your day two p2 now what does p3 look like where is it p3 now looks like what p3 looks like this is dead this is dead one of them is practically nothing the no hypothesis two, hypothesis two is winning, it's more than half. Hypothesis three is lagging behind. Hypothesis four is almost disappeared. And hypothesis five, two, three, actually four is almost close to zero. But very hard to tell that this is not zero. three actually four is almost close to zero or very hard to tell that this is not zero and this is four hypothesis five do you see guys how your probabilities have changed what's your probabilities have changed and does this make sense and that on the only viable situation looking at this data is either it's very rainy or it is medium rainy guys I hope you're seeing that this is very intuitive oh yes yes this is it and you know you know you can play this game more and more and ultimately what will happen is it will gravitate it will keep now keep will happen is it will gravitate. It will now keep oscillating, and it will equilibrate at the right solution. But what do you notice? A flat distribution like this. Sorry, like this. There's a P, and this is 0.2. there's a p and this is 0.2 all the hypothesis one two three four five initially you had equal belief in them now you have already abandoned belief in three of them more or less and now you'll see more evidence so what happens to your understanding of the situation you you begin to have a better and better understanding of where you are would you agree days and that is by Asian reasoning that is the gist of by Asian reason that's why by Shin is so important it's a we are all by Asians we just don't know it we reason like this. When you say from experience I have learned, if you learn something correctly, then you have learnt it the Bayesian route. If different people have different priors, they would still arrive with all the evidences and the likelihoods they will all arrive. Do you want to try that? Do you want to try that experiment? Oh, no. It just occurred in my mind. Yeah, it is. So I'll let you do this. So suppose you have a strong bias that I must have landed up in the desert, right? So let us say that you start with hypothesis one probability, one, two, three three four or five let's say that your hypothesis one that you are in the rainy place is very little and never set any value to zero let me just say zero zero zero one right hypothesis two that you are in a very rainy place is also low let's say 0.1 10 hypothesis 2 that you are in a very low places. I mean, medium rainy places also very low. And your belief that you are in a mostly rainy place. I mean, mostly dry place is a little bit more. Maybe let's say 0.2. And let's say that your belief that you are in the desert is like, okay, if I want to add all of them it's like four nine nine nine nine something like this right you can start with this prior but what will what will the data do to you you want to play this game again with this kind of a prior you will notice that you will essentially start coming back to this you will take a little bit longer to come to the answer right but you will get to the same answer after after you have seen enough data i see right and that is the magical thing just think about it for a moment it doesn't matter if you're wrong in the beginning so long as you are willing to learn from data, from experience, you will come to essentially the same truth, the same posterior belief. Do you see how lovely it is? And just some very elementary mathematics. So, Arshiv, it is good to start with a randomized prior in to arrive at the conclusion fast. No, no, no, you don't. You don't. I'll give you an example. See what happens in a random completely uniform prior, like, you know, equal belief in all the hypothesis. Oh, equal belief is what I was about to say. Yeah, equal belief. When you start with equal belief, you call it, there's a word for it, it is non-informative prior. Non-informative prior. What you're saying is I really don't have any hunch, right? So I have no reason to believe one hypothesis more than another. reason to believe one hypothesis more than another. So my belief is equally spread amongst these. But usually what happens is that you come with some belief. So I'll give you an example. Suppose you have to get a surgery done, right? And it's a risky surgery, open heart surgery or something like that, some very risky surgery. You are going to a person in a hospital which is poorly equipped and there is a very young resident standing there, right, with his knife in hand a very young resident standing there right with his knife in hand and asking you to come into his operating theater and he'll act on you at that moment and then there is another doctor let's say at another hospital let's say ucsf and he's also a cardiologist a heart, and he is also, he's like old, is 45 years old, has 20 years experience of surgery, and he's also holding a knife in hand. Now, before any other evidence happens, like whether you go under anesthesia or so on and so forth, suppose you had to guess a prior probability guess. What is the probability that you'll come back alive under the first case versus the probability that you'll arrive, come back alive under the second case? Would you assign the same probabilities? No. No, your prior is therefore informed. You have a hunch. You know something that the math doesn't. And so that which you know before the data, before you see the data, it is your prior experience. That's why the word prior is used. That which is your prior guess based on experience is your prior. So you give different values to the two hypothesis, the two prior hypothesis, you give different probabilities, prior probabilities, and then you let the data speak for itself. It is still possible, you know, that that surgeon sitting in a top notch hospital may have blundered through all his patients and that's what the data will show. And that young resident the data will show. Right? And that young resident sitting in that a poorly acute hospital is actually brilliant at surgery. And you just didn't know that. When you see the data, your belief may shift. But before that, just looking at the two situations, you come up with a prior belief in which you give more importance to one versus the other. Right? So guys, this is a detour. I took you through Bayesian because the AutoML, the foundation is Bayesian optimizations. It is important to know Bayesian learning. We did a rather little bit fast-paced intro and of course we didn't go very deep but i wanted to show you the magic that is variation how you go so the way you say this is posterior posterior belief or posterior hypothesis belief is proportional to prior belief believe, modified by, multiplied or modified by likelihood of data, likelihood of evidence. How likely is this evidence, how likely is your hypothesis to produce this evidence? That is what we did. Remember the L and we multiplied it. So when you just keep it as a proportionality, you're looking at the unnormalized posterior. And if you want to do that, then you have to divide it by the sum of all, sigma of all to make it a complete probability. So this is the heart of Bayesian learning. That you take a prior belief no matter what but so long as you are willing to learn this is the learning part you are coming to a posterior right so now let us let us bring it back to our world how does it relate now to what we are doing oh goodness today is becoming more a theory session than lab guys i do I do want to do the lab. So can we extend the session today? Can we do it, let's say till 10.30? Yeah. You can do it till 10. Till 10, not 10.30, huh? 10.30 is... Yeah, 10 would be good. Yeah, we'll just keep it calm. Okay guys, so maybe we'll break it up into, I'll cover the rest of it in the future. But so now let's go back to it. See, you have a hyperparameter space. H1, H2, hyperparameters. H3. Hi is the hyperparameters. Hyper, oh this is a bad spelling. Hyper hyper parameters now h i the h i now let us say that for example h1 could be the learning rate h2 could be quite literally layers in the how many layers in the network. There's count right? X3 could be your regularization. Parameter. Something like that. It doesn't matter, so you can keep going down. Now you realize that in this wasp is actually I just screwed myself. I made it a three dimensional object. And it's hard for me to now on a two-dimensional place make it so let me chop off one but you should assume that there are many many hyper parameters huh h1 and h2 i'll just keep it to this too so that you know we can draw for each point of the hyper parameter space any Any point you take, it is a combination of H1, H2. Any point X is a combination of some value of hyperparameter one and hyperparameter two. Would you agree? Isn't it? Yes. So now comes the very simple argument. Now that you understand biation, this entire reasoning will become very clear. So let the evidence be D. Evidence or data. So think MNIST, for example. MNIST is there, right? Let us say you have an algorithm which has these two hyperparameters. So you do this for choose a hyperparameter, choose X randomly or carefully or whatever, etc. Whatever you're making initial guess of X, optimal X, right? You know that. OK, so now what happens is you happen to create an objective function, create an objective function. That's the word we use in this literature, objective function. Objective function is this box, F. X goes in. What is X? It's some combination of hyperparameter one, hyperparameter two, goes in. And what comes out is this. The loss that you have. The residual loss for this data set D, residual loss for the data set D given X. If you take the MNIST data set, you run it over the validation set if you want but or test it i won't i won't talk about testing let me just call it data but you run you find the total loss on the data given this value of x after the training has happened means you go through your epochs go learning of parameters, right, or optimizing parameters, optimizing the parameters, not the hyper parameters. So no matter what you do, see, ultimately, the loss that you're left with depends on two parts, how perfectly you fit the parameters and how good hyper parameters you have picked now parameters you can optimize very easily because of your gradient descent and back prop and all the lovely magic isn't it if you're doing neural nets if you're doing support victim machines whatever it is you can train it basically the parameters can be trained right and after you have trained it you would still be left with a residual loss the optic this is the loss left loss that remains after training would you agree this is the loss you're left with after training given this hyper parameters so far so good guys this hyper parameters so far so good guys yeah so now with this fact being there and now the statement is that see guys what the problem is this objective function is unknown you know that in some sense the loss is given the data and given this hyper parameter is and we write it as y is some unknown function of x some unknown function of x Now what happens? If it is unknown, how in the world do you discover it? You can't. Can you? If I tell you that there is some very complicated and unknown function of X, now go guess it. What will you do? But by the way, here is the data and you have to guess it. What will you do? But by the way, here is the data and you have to guess it. What will you do? The only two things you can do, you can keep taking a lot of values of x and seeing what y comes out. And from that, you may start noticing some pattern, isn't it? Are we together? So let us do that. Let us say y. And if you don't mind, I will just make X as a one dimensional thing at this moment, even though it is two dimensional. But imagine that at some point. So what happens is that suppose I pick actually Y pick that. Okay, maybe I'll do that. Suppose there is a, you pick a point here. You find that the loss is this much for this value of x1. You take another point here. And you take, I don't know, another point, where should I take it maybe here it's like this i'll just take three points and now you know the y values you you don't know f f is an unknown function but what what are you tempted to do tell me what would you be tempted to do just looking at these points continuous try to connect all of them you try to connect it right so what you do is so for example if you make a smooth curve something like this you may be tempted to do this right you may be tempted to do this you may be tempted to do this, you may be tempted to do this. All of these are going through these points, isn't it? Any one of these lines could be your guess. You know, these guesses that we make, what we do is we create something called surrogate function. Surrogate function. And the word surrogate comes from surrogate motherhood. Let's say that you can have a child not necessarily from the true genetic mother, but the child can be born from a surrogate mother, isn't it? For example. sort of somewhat similar interest. So what you're saying is we can actually, instead of trying to find the minima of this, because the problem you're trying to solve is the best value of the hyperparameter is the argmin of x. That value of x that minimizes the function fx, except that you have no way to minimize calculus is out of the window isn't it there is no gradient so forget about doing gradient descent we still have to solve this problem so what you do is you start making guesses like this you these surrogate functions you start creating right so let me just call the surrogate function Sx. Right? And these are simpler. You deliberately choose simple, nice, smooth functions. Do you see this? Right? But there is a problem. There are so many of these functions you can fit through two points. Right? So one way you can think is you're trying to put a polynomial through this. As an example, I mean, usually you do it slightly differently but one example you can do it or you can throw some gaussian mixture so whatever it is you can do now what happens is another data point comes in let us say a data point comes in like so when you look at this right so let me actually color it differently, guys. To bring home the point, I'll just do it differently. I think I, what is it? This. Another hypothesis says, what should it say? Maybe it has to go through these three points. So it may say this you see oh sorry I should use a different color my bad one second let me use some other color yeah so I'm just making some hype some giving you guys an intuition guys so all we know that these three points exist for which we know the loss now we can fit in a lot of these lines through it curves through it it, functions through it. Now what happens is that you can now distribute your belief or faith in these three lines asymmetrically. You may have more faith in maybe the yellow line, maybe you have reason to believe the red line or you have some reason to believe the green line you can do that guys isn't it you can choose to have asymmetric belief in the three colored curves the three surrogate functions would you agree yep right now what you do is it's all right then you bring in uh this is this this by the way this functions because of the great curve in higher dimension as you can imagine it's a surface in fact in our problem of h1 h2 itself because it's a three-dimensional problem it's a surface the word for this is also response surface response surface so now what it is saying is guess what looking at this hypothesis if i have to check then find the next point where i should do next cap what is a good candidate where i should uh you know train train the algorithm again and see what loss I get, apply the objective function to. What would be a good candidate? Looking at this, what would you say? Where would you put it guys? If I ask you to pick a point that that combination of the hyperparameters where you want to test for a good loss, low loss, which point would you pick? Looking at this data. Pick something beyond that range to get a better feel for the shape. You mean something here? Yeah, that or zero point or more points. It's not enough points with only 30. Right, so see one thing you could do is you can say that you know these response surfaces they seem to imply that this seems to be the neighborhood where which looks promising based on the three curves we view isn't it at this moment so and the kid your point that you made is let us look it up at some other point let's say here so that that will help us choose between the green the yellow and the red right and those are the two approaches they are called and this is a classic you know ideas coming from reinforcement learning and so on and so forth algorithms um We'll talk about, see guys, the theory here is very elegant. They're all just multi armed bandits and whatnot. I will talk about which I haven't taught you. So we won't get into it. But you do a trade off between two things. A trade off, trade between two things, exploitation versus exploration. So what happens is when you say, let us pick the next point in something that looks promising, right? And suppose you pick the next point here. Let me take an example. A, you have gone in favor of already picking a good point in view of your exploitation. You're exploiting the knowledge you have so far. Exploration is saying, hey, you know what, maybe we should search the rest of the hypothesis space and see what's happening there. So that we don't want, you know, we don't know what's happening here, all of these places so let's go and explore more of the hypothesis space and suppose you pick a point let's say that you pick a point i won't take this suppose you pick a point you you're trying to determine what value there is um i don't know someplace here let me just take b what if it it dips back down? Maybe it's not possible. It is possible. So what happens is, by the way, these A's and B's are here. This is A, so I shouldn't say that. I should mark, you're right. It's a good point you made. I shouldn't put it here. Ultimately, this is the hypothesis X direction for hypothesis. So let's say that you have another hypothesis B, completely far away. At this moment, what is happening? You are beginning to pick. You have points here, points here, points here. And these three values are tempting you to pick A. If you pick A, you're exploiting the likelihood, the the the likely zones of a good good why on the other hand you say when you say no no no we need to look around a little bit more let's go to B instead you're doing exploiting the exploration you're saying let's explore a little bit more right to get more feel for how the this response surface looks like or the surrogate function looks like. Do you see the distinction between choosing A versus B, guys? B, we don't know what the value of Y will be. It could be anything, right, along this. Because there is so much uncertainty. We haven't explored that region at all. But around A, we have picked two things to the left of A, one point to the right of A. So when you go for A, you are actually going for a likely good place. That is exploitation. The opposite of that is exploration. You say, no, no, let me first go cover my, you know, cover my bases. Let me go and check areas which I have not explored yet in the hypothesis space. Are we together guys? Do you see why each of these have their pluses and minuses? Tell me guys, anybody who has not understood it since nobody is responding. I think there's merit in doing both. A little bit closer to the mic. So, you're saying that there is merit in doing both? Yes, there is merit in doing both. See, you may be tempted to go for exploitation. You may be, so I'll make this situation more extreme for you and then you tell me. So suppose the situation is like this. I go and I really find that A, the Y turns out to be low, right? So now what happens? Your loss, there's a potential, another loss value. What color should I take? Blue. Now you might be tempted to go with this. So now do you pick this minima, C, right? C or you pick B anymore. Do you realize that evidence is beginning to mount more towards C? Right? So at that moment, exploitation seems to be the better choice as opposed to exploration. So what you do is you have a function. It is called a selection function or an acquisition function. People use different words for it. Acquisition function or a selection function function is job. So now we have three things guys. We talked of the objective function, we talked of the surrogate function or the response surface and now we are bringing in one more function or the acquisition function. So many more are bringing in one more function, the acquisition function. So many more actors, many more cooks are entering your kitchen now and helping you cook up this best model. What does this acquisition function do? It is the one that tells you that given this tradeoff, what should be the next point? What should be the next X, X, I, H1, H2 to evaluate? So good acquisition, and there is a bit of mathematics that people use. I will just mention it it but it's not that's technical detail it doesn't matter there is something called expectation improvement like how much how much expectation you have that your loss function will improve and it is given by an integral y star minus y and your current posterior curve so whatever curve that you have so far your surrogate function right y given x dx so this is your surrogate so far and this is your ei and what is y star y star is just some reference value. You say I wish it's a threshold value. You can pick a good threshold value will be the lowest loss that you have observed so far in your experiments. That could be a good and now you evaluate this right and so it turns out that if this if this turns out positive you're likely you are moving towards a place where there will be, this is a dy actually, I was trying to say x. For this given x, so you pick a point based on how much improvement you expect this acquisition function or selection function expects to get by jumping to that point. So there is a bit of mathematics. I won't go into that. By the way, this is the classic formula of expectation of any posterior. But any anything the delta, right, where to go and if I go, how much do I expect? See, let me put it this way. Suppose you have a function Px, fx, and the probability, there's a probability distribution this. The expectation value of fx in statistics is called this. This is basic statistics, the definition of expectation value. So I don't want to review those statistical concepts at this moment. So just ignore it because we need to do a lab. It's already 8.30, 8.40. So this is it. And so what happens is that you hop around, you use this exploitation versus exploration and so what happens is that your region of uncertainty which in the beginning was quite high, let me pick this, region of uncertainty should be which color this hardly shows what color can I take this bright okay none of this is helping me too much so let me just mark okay see here's the thing gradually what happens is that sooner or later let me market here suppose this is really true hang on white okay and what happens after some time is that you have done more experiments one of your one of your guesses would go like this another of your guesses may go like this but they begin to converge you see all those different uh hypo you know uh sort of things that can go through all those points they begin to look alike am i making sense so let me just say they're going through this right here they're going through like this they all are converging and beginning to look like this so what has happened is you have essentially created a good surrogate function by by continuously trying to adapt to the data the few data points you have you have created a good the data, the few data points you have, you have created a good surrogate function, which is, let's say that ultimately it all begins to approximately look like this. Right, and remember that your axes are L versus X. So now all your data seem to fit more and more this surrogate function. What can you conclude from this guys? Can you find the best hyperparameter combination? Where is it? It is at the minima of this function, isn't it? This yellow function. This is your X star. This x star is argmin of fx, the actual objective function. Would this be a good answer to that? Are we getting it guys? So what we did is that we don't know the objective function, but we created a surrogate function and it seems to fit the trials very well. And we use that surrogate function then to, which we can minimize because it's a function of our choice. It's a nice simple function of our choice. Now we can just apply calculus or whatever gradient descent or whatever to the surrogate function and find its minima and wherever the minima is we claim that that is our best combination the best hyper parameter tuning at that moment am i making sense guys so see guys this is a very intricate, a very lovely reasoning. If you're familiar with Bayesian, you look at this thing and you just smile because it's yet another illustration of the power of Bayesian ideas and Bayesian optimization. At this moment is extremely hard for hyperparameter tuning and it's at the heart of, at this moment it is at the heart of this whole automated machine learning in which you have models that tune themselves. So, Asif, the only intuition that's a little hard to grasp on this one is, when we think about arriving at the minima for that function, there's a mix of discrete and nondiscrete variables. Like if you take in the case of a hyperparameter like a choice of an optimizing. For the time being, so the discrete is where that tree part of the tree, you know, tree structured parser and estimator comes in. I would say that in the first early stages then, just forget the fact that anything is categorical or anything is discrete. Just first get the intuition for continuous hyperparameters. Once you get it, it's a small complication or extension to make your intuition fit with this. Don't worry about that. But if you just assume, get the intuition right. Just get the intuition with completely continuous functions, hyperparameter values, which are real valued. Yeah. OK. Right. But your point is valid. I didn't take you through that. See, we have limited time, guys. Do you realize that every week we are covering big swaths of, like, literally some subfield of machine learning, entire subfield we are covering in a week. And yet this is one of the longest workshops you'll ever go through, 18 weeks. So we have to move fast, but at the same time, we have to focus only on the core ideas so that other things you can pick up on your own. So far so good? Yeah. So therefore what we did is that, we said the objective function is unknown, but we can come up with a surrogate, which is a pretty good approximation that will do the job just as a surrogate mom is completely able to do the job, you know, taking that, you know, taking, keeping a baby alive and giving birth and so on and so forth. So what happens is that the surrogate model has effectively given birth to the best hyperparameter, which you couldn't have gotten from the original objective function. And the three pieces of the game were the objective function, which is literally pick a hyperparameter, compute the loss, do your network training and come out with the best loss. But then the surrogate function, which is your literally surrogate. And then the last is the acquisition function or selection function, which does the exploitation versus exploration trade off. And it uses some expectation value and so forth. So now what happens is this is the core of Bayesian optimization. There are many implementation details. Yesterday, the day before, I said I would do the Parzen estimator. Actually, forget it. In hindsight, I don't want to go too much into it because we have gone technically deep enough and those implementation details frankly don't matter. So the two, three leading areas are, one is just to random forest regression, right? So you see all of these are one is just to random forest regression. Right. So, you know, you see all of these points just fit a random forest regression model to it to find the best fit model. We can do that. We have done regression, isn't it? That seems like a no brainer. Another thing popular is, of course, something called Gaussian processes. So when I was making all of those curves, actually, this particular way of explaining it goes to the heart of something called Gaussian processes. So when I was making all of those curves, actually this particular way of explaining it goes to the heart of something called Gaussian processes. It's again a subfield of machine learning and very biotin, extremely beautiful indeed. Obviously it's not something that we covered in ML 100, 200 or or 300, or 400. But I alluded to it in our math course. I can give some references. It's lovely, but it's again one of those things you go deeper into if you want to really master Bayesian. Now it turns out that as this automated machine learning becomes very relevant and central, maybe it's a good time to learn about those. So this kind of reasoning that many, many curves, each with different belief you have in them or faith you have in them, it's a Gaussian process explanation. Then there is the Parzen estimator explanation. I'll give you a rough idea. What it takes is that, actually, maybe I'll talk about that. But if you don't get it, because I need to get to the lab, you take a threshold value of y, y star, the one that I was talking about, and then you create a function, which is two curves, one Lx, which always does below, one gets greater. And then what you do is you try to, in some sense, converge from this, you sample from this. And there's a bit of explaining to do, magic happens a biation magic happens and ultimately doing the same thing this will optimize itself right the tree part of it comes from what you asked, the discrete part, when you have categorical aspects. So these three methods at some point, try to read up on your own. And at the risk of this session, lab session, becoming a pure theory session, I think I should stop. So I'll today take you through two libraries. One uses the Gaussian processes and the other uses the this one tree structured people don't use the word tree structured a parsons estimators TPE everybody calls it TPE most people don't remember what TPE stands for so I will go through libraries in the lab that covers these three so guys it's it's 849 do you guys want to take a 10 minutes break before we come back for an hour of lab? Yes, please. Is there something for us to download from the portal? No, unfortunately I didn't get time to upload anything. So let us do it as we go along. Oh, by the way, guys, was this explanation useful? Like, did it help clarify your understanding of things? Did you get a core idea of what Bayesian reasoning is and why it works? Very useful. Yes. Yes. Yes. The last part, I think we need to get better at it. But the first part, totally. Yeah. All right, guys. So I'll catch up with you exactly at nine. Okay, so we'll just open a Jupyter notebook and put stuff in it together? Yes, I will help you do that. Okay. So. All right guys, so on the portal, in a course portal, you'll see that I have put quite a few resources from a long time ago at the beginning of this course but this list is obviously we need to augment this list with more and more items if do you notice that we have a wiki here if you go to this wiki guys each one of you can add content you know you can go to the edit and you can start adding new contents that you find and if you can do that like for example the new tent or whatever you know you could add some resources i really appreciate it or if not then just just post it to slack and i'll keep adding it to our class page as i said this particular topic is moving so fast that the only way we can should we be opening say that again is somebody say something no okay so the only way we can progress is if we collectively can this notebook be anywhere or do you want it to be in the homework directory structure i didn't ask you learn book. It is a Kate, I don't understand your question. What is it about notebook? Well, we're opening a Jupyter notebook. Not yet. Not yet. We are not we're just looking at the web page so uh at this particular moment all i'm saying is that as you guys find resources either keep adding here or uh add it to just put it on slack and i'll keep adding it to the course portal yeah if i don't think we have access to add the link no you have access to editing it. Just click on one of you, just click on the Wiki. You can't add a link, but you can click on the Wiki and then go edit and put things there. Can you click on the Wiki please? You know this participant contribution to AutoML? Could one of you verify whether you are able to click on this, it will take you here. And here you can go to the Edit tab and you can start editing, adding things could one of you confirm that you guys are able to do that? Any volunteers? Is that under hands on lab and solutions, the development environment for AI? Oh, it's only view only like for me. My the view option is the edit option is not there. Yeah, view only like no no. Okay, it's all right. Let's forget that. I don't know why I couldn't make it editable i'll see what i can do maybe it's a setting let me see if i can oh yeah participant let me see so edit settings permissions maybe what is this instructor delete comments comments, create new wiki page. Yes, activity. It seems to be there. Okay, I'll figure it out. Let's not lose time over that. Just post it on Slack and I'll keep adding it to that. But these are some of the links. Now, let's go to the facts. What I will do is, there is a whole wealth of articles and coverage on this topic, but let's start with something that will sure shot work. So Yes, I'm better. Yeah, maybe one more. Yeah. My screen seems to have frozen. Okay. Is it any better? Yes, this is good. Let me do one thing guys. I'll put it on the other screen and share that for some reason today zoom we just updated zoom and it doesn't seem to like zoom was popping me in and out like three or four times just now yes yes i think that the new update to zoom isn't the best for some reason. So please bear with me for a moment guys, let me share a different screen. Stop sharing and share. We may have more luck with that screen. All right. Not, not particularly for some reason, my browser is not. It will come back up. Yes, it just started cooperating again, but it took a while. I can't, guys, it's too big a font I'm not not able to see. Can I reduce it a little bit? That's fine. Yeah. It's very hard. I'm sorry if I missed stuff because I was popping in and out of Zoom. Where are we now? Do I open a brand new notebook? No, you don't. It should be in one of the . Just follow along. And just do not do anything. Just follow along that's all so you can make it smaller that's it okay okay i'm just waiting for this thing to respond yeah it could be smaller no i'll have to again go to the yeah okay guys wait wait my browser is simply not responding with this ever since we got the new zoom update today afternoon things are going pretty badly okay this looks hopeless place. Alright, let's go to Firefox. Otherwise guys, I'll move over to the Linux side because I know that in the Linux side, there is no problem. We work just fine. If the notebook is available, we can open it locally. It is not available on the internet. Oh, yeah, I mean, yeah, some of these notebooks are available. One of them that's available, I don't know, we will see which some have posted some I have not. So today, I'll start with the first one. SK optimize. Of course, I'll upload everything in a bit. Yeah, Firefox seems to be behaving better. Now, let me try increasing the font. Guys, is it better now? Okay. So, guys, remember, I'm not asking you to do anything at this moment. Please don't get busy doing something, just pay attention. So I'll take the, to motivate this example, I'll take the obviously MNIST dataset because we can carry this dataset through the different libraries. We know what MNIST is, it's a lot of digits, like hundred and digits and we have to classify them to be between zero and nine so we have done that in the beginning we'll just go load the data scikit has already the data built in we can load that data exactly now there is no automated machine learning here what i'm doing is i you we realize that when we deal with support vector machines there are many hyper parameters support vector classifiers many hyper parameters first is the kernel itself do we take a linear kernel a polynomial kernel a Gaussian kernel a vf kernel or what is it which kernel do we take right let us assume that you took a Gaussian kernel. Now still, there are two more hyperparameters. What is the budget, the c, the cost, and the other is the gamma. Remember, e to the minus gamma d, the thing. So what is the gamma? And this is, obviously those people who haven't taken ML 200 with me may struggle a little bit, but just take it as a fact from me that support vector classifiers have many hyper parameters two of the most important being the cost and the gamma right and of course which particular kernel you choose so by default it is rbf the gaussian curtain so we'll stick to that so typically what you would do is you would instantiate the classifier and you would put a guess of the cost and gamma isn't it guys you would get some value for it am I making sense yes you would pick some value it doesn't matter and even you pick some and then you and at this moment you're not doing grid search right and so you would run it the rest of the line should look very easy as we fit i hope looks straightforward y hat looks straight forward this thing looks straightforward right you fit the bottle you get the predictions then you get the classification report, and you seem to be getting variable amount of accuracy. The F1 score varies from 61% to the high of 98% for three. Three seems easy to identify. The overall accuracy seems to be 86%. Right? to be 86 percent right so now you know that you that is that is respectable what happens if i change the gamma let me change the gamma to uh let me let me just play around with it what if you make the cost to 10 right and i run this again Now your accuracy is about 87%, not much change. What happens if I change the gamma to let's say 0.1? Oh, now your model has become distinctly bad. Do you see this guys? So when you do randomize search, you can put it in a loop and you can try out randomized cross-validation search across different values and so forth. But the whole point of automated machine learning is, is there a way to use bias and if you remember the scikit-learn comes with cross validations it comes with grid grid search cv it comes with random search cv these are the two things you're familiar with guys isn't it but now i'm going to introduce you to what this library does and that's a nice thing about the SKLearn, SKOpt, right, the scikit-optimize. The lovely thing about it is that it completely conforms to that naming convention. It is almost as though you don't need to know the whole theory of automated machine learning. Somebody just whispers in your ear and says hey don't use random CV random search CV or a grid search CV instead use bias search CV do you see this then what you do is in the bias of CV you set up the parameters like of the hyper parameter search specie you say that search in 10 to the minus 6 to 10 to the plus 6 in log uniform means if you take the log of it it will go the minus 6 to 10 to the plus 6 in log uniform means uh if you take the log of it it will go from minus 6 to plus 6 right log c so choose it to be from 10 to the 6 10 to the 5 10 to the 4 10 to the 3 10 to the 2 10 to the 1 10 to the 0 10 to the and you know 10 to the 0, 10 to the, and you know 10 to the minus 1, all the way to minus 6. So you're taking about 13 values here. So you're actually scanning a pretty vast range of scales for the good value of C. Likewise for gamma, you're picking values from 10 to the minus 6 to 10 to the to 10 from 10 to the minus 6 to like 1 millionth to 10 and in a again a log uniform space if you're taking a polynomial kernel you're taking degrees from 1 to 8 right 8 possible degrees and there are three kernels so guys can you imagine what a large computational space you're looking at uh 13 times seven times eight 13 times eight times eight times three right if you were doing a grid search you would be doing a whole lot of computations a grid search you would be doing a whole lot of computations would you agree guys yes okay so grid search would be uh brutal and then they can do randomized search that also will be not now look at this this code is exactly like what you do with your scikit-learn no difference and so when you do the lab it's just so easy almost you might even wonder why did i teach you automated machine learning all i could have done is just say use this instead of whatever else you have been using but they made it this easy you fit and then you come up with the best code so it says that look at this the accuracy now is 98.66 compared to what we were getting low values we are getting in 88 even with good values let's say 0.01 we were getting at most 87 percent right so compared to that in one fell swoop it has managed to come up with this code quickly are we together and it finished in 26 seconds 27 seconds which is pretty impressive that it managed to find the best value and if you look at the value that it found you would not have guessed it look at it guys what do you think about these values it is saying that two kinds of support vector machines will work with two kinds of kernels will work the the the best we are getting we seem to be getting is with polynomial kernel in the polynomial kernel you can have the best value that you got was for cost this much 1.27 and for degree 3 right and if you use a rbf kernel it happens to be the gamma for that happens to be 0.00034 that's right is it making sense guys do you see how easy it is to do it? Yes. So the SK opt package that's also part of Skate learn. Yeah, that's right. And so you'll see, and I'll give you I'll take you through the documentation page for in a moment. Right. so then you can do it with more like for example you can say let me try more things let's say that a more detailed example in which i give all sorts of model parameters to search for make it more and more complicated and then run it it will run again even a sophisticated run just takes 43 seconds which is nothing compared to doing a massive grid size and svms are not easy now we are using linear svms all sorts of things and then once again it comes out with the best model value right for the classifier and now it is coming up with the value of the classifier with all of those things which is different. I mean see the point is that this kind of optimal values you could not have discovered so quickly either with grid search or randomized search, isn't it? That is the only message here. Obviously if you try to do it on the iris dataset, it's too trivial. So it will very, very quickly come to the solution. So this is just one library. Let me now show you something about this library uh now let us hope that my one question go ahead uh so when we are calling this bias search uh the parameters that we are giving the c and gamma um are those fixed or those can be anything that depending on the no no no fixed or those can be anything that depending on the no no no see here is the thing where do these parameters come from c gamma kernel because you're using scikit-learn so here is what you would do you would go we are using scikit-learn's support vector machine isn't it so let's go and find scikit-learn SK learn SVC right documentation for it we go to the documentation and see what are the parameters here do you see the hyper parameters C kernel degree gamma so you have to stick to these names okay that, that is it. That is why they have made it very close integration with that. So now guys, let me take you to the website of this. Where is the website of SQL and optimize. This is our SQL code. So there's so many libraries I want to show you. I have that open here. Oh goodness, it's not worth searching for it. So I'll just go and search for SKlearn. Optimize. It's a lovely library. You can go there. So this is the Bayesian search cross validation. There is a lot more functionality there. I just got you started on this. So please go to this website, look at the examples and look at the user guide. It will take you some time to master it. But if you look at this website, you can clearly tell that it doesn't rise up to the standard of scikit-learn or pycharm i mean pi torch and so forth for either maturity or documentation right it's sort of work in progress and that's the nature of all of these all of these libraries so you can go to this. You can just Google guys, can you please Google scikit-learn hyphen optimize and bookmark this page. So the example that I took is literally this example. You see this example here. I put it in a notebook, which is why I thought it's pointless me handing you notebooks when the notebooks exist on the web and you can take it. Literally from the home page so please let me know when you have all bookmarked this scikit-learn optimize website i will also put this website on the slack for the time being let me put it on slack Slack for the time being. Let me put it on Slack. I'll of course add it to the website course portal. So if you would be nice to live on the way of the say that again please. Okay I'm searching I just wanted to check. Okay. Okay, I've got it. Yeah. And I've also posted it to the Slack. So this is it. So is, I gave you a contrast between using their library, their code and not using it at all, just doing it the traditional way that people do machine learning. And they will pre hand instantiate this and try some random values or do some good or randomized search. So you can try all that and then you can try this. I'll post this notebook in a moment. Now let's look at that and then you can try this. I'll post this notebook in a moment. Now let's look at the next library that can do that. Okay, first of all, I'll just go through my other notebooks. If that is good, actually, there is one platform I like very much, which is the Axe platform. I feel that this is, of all things, quite mature, very mature, relatively speaking. So they call themselves the Adaptive Experimentation Platform, AXE. This platform you can install by just doing pip install axe-platform. So I'm showing you how you can do a Bayesian optimization on any objective function. See, when we talk of loss for a neural network, et cetera, it is there. But as far as the Bayesian optimizers are concerned, they don't care. They just need an optimization function. So just to motivate and get you guys started, I took a very simple example that I hand created. guys started, I took a very simple example that I hand created. Some of you may know about the sinc function, sin x over x. I created a two-dimensional version of it. I just hand wrote a function, which is this. I have visualized it here. By the way, this notebook I'll paste, and so rest assured that all of it will go to your website at the end of this class. So rest assured that all of it will go to your website at the end of this class. So this is, what does it look like? Do you see that there is a unique minima there? All in blue, yeah? So it is a highly nonlinear surface, but it does have a unique global minima, like absolute minima. Isn't it, guys? Maybe I'll make it bigger. Look at this picture, guys, even bigger. I'll make it even bigger now. Again, this thing is frozen. It's a nuisance. We can see it, Tasif. We can see it. Unfortunately, this browser keeps freezing on me. Chrome with PyTorch. Let me try my luck with this. With Firefox, that seems to work. Are you maxing out on your memory? Chrome takes a lot of memory no my i mean i'm looking at machines this is a laptop with 64 gigs of ram okay so that's not likely to happen um anyway let me let me do it through this because here we have a more better luck okay Okay guys, so this is code. This is very simple code from matplotlib visualization 3D visualization. So in this, the important thing is that we have one big minimum and we have to search for this. So, and we will do it without doing gradient descent or any calculus whatsoever, just using biasesian optimization. So you know that the solution is this. Let us see if Bayesian optimization can find it. So we say that there are two parameters, X1, X2. Remember this formula X1, X2. Two parameters, X1, X2. I gave it a range minus five to five etc and then i say that here is my function the one minus in car remember this function i just encoded this bit of uh equation i just encode it into a function and i say uh here is my this is the definition of a function nothing magical here guys is Is this looking easy? This is literally that equation writ large. Yeah. Okay, P of course being the parameter map, right? It is the hash map of the parameters. And you're saying, look at this. And then this is the crucial method in the axe, which is why I like it very much. It has one optimized method. You give it the parameter space, the search. So parameters gives you what? The hyperparameter search space, the ranges of it. This is the box in which you have to search. X1 from minus five to plus five and X2 also minus five. So it gives you the space to search evaluation function this is the actual function objective function so this is the objective function will minimize and are we minimizing or maximizing so we are minimizing minimizes to in one fell soup it will do it and see how long it took it took took 6.67 milliseconds, which is really impressive for such a non-convex surface. Do you notice how non-convex it is? You can easily get trapped in one of these local minima, isn't it? Nonetheless, with Bayesian reasoning. And so this brings up the fact that, see, remember I told you the trade-off between exploitation versus exploration? If you just did exploitation, let us say that in the beginning you randomly started here and a couple of points here, what would have happened? With pure exploitation, you would have stuck to this minima. Because exploration is there, you also look at the other parts of the hypothesis space, I mean the hyperparameter space, you manage to get somewhere else, which took you gradually to the real minima. That is why the trade-off between... And how many trials it did? See, in 20, just 20 trials, I mean 20 points, 20 instances of searches in the hyper parameter space it is already able to tell you that x1 is 0.c we already know what the solution is solution is zero zero zero right x one x two and z are all zero now let's see what it is coming out with it is saying that x1 is this x2 is this out with it is saying that x1 is this x2 is this is it close to zero guys almost right would you consider this a reasonably good solution with just 20 trials yeah pretty impressive isn't it and it is saying that the best value of the objective is uh this this one, 000339, which is again zero. And it has pretty much guessed, come to that. And see guys, think about it, completely without doing any gradient descent or calculus, just through Bayesian reasoning, it has come there. That is the magic of this, guys. Now, this other example that I've taken from their website they take a more complicated example and see where we know by doing calculus on this equation that the right answer of x1 x2 is 1 and 3 right let's see what it comes out with it comes out with x1 is equal to 0.964 and x2 is 2.9944 a Pretty impressive for just 20 steps, isn't it? Yes. I see. What is the exit criteria for this right now? Why did it stop at 20? Because you said 20. Oh, my default 20. Unless you set the number of steps to more, it will do 20. OK. Yeah. So that is it. Those are all configurable parameters. But I see I've simplified it so that we can understand. And then you read up the documentation and you start throwing in more. All of the other configurations you can change. How many of you, I mean, see, if you never have seen Bayesian reasoning, right? This is thrilling. All through the whole thing, machine learning so far, we have just been seeing the magic of gradient descent. And now you're seeing something else that also works very, very well. Isn't it? So obviously, those of you who took math with me remember all thisesian, Markov chain, Monte Carlo and so forth. Bayesian is just magic, guys. If you Google the word, what is the most important algorithm of the 20th century? Can anybody guess? Not people who have taken my course, math course. Anyone of you, can you guess which is the most important or most remarkable breakthrough algorithm the most powerful algorithm of the 20th century can you guys guess anybody is it this is it that what would it be harini So the naive bias? No. Anybody else? Monte Carlo? Who said Monte Carlo? Shiva. You're close actually. Give the full thing. Anybody else? What does it do with Markov and bias? I can't hear the person who... Markov and bias. I can't hear the person who Marco and bias. Marco and? I am just trying to mix Marco and bias, something related to it. Marco chain Monte Carlo? Marco chain Monte Carlo it is. The metropolis algorithm. It is literally, and the reasoning is very close to this your bias and reason just google it up the most important algorithm that you can do and see and see come up on the drop list something which is the chocolate system it is a tragedy that we are taught all sorts of algorithms in college but we are not taught what is arguably the most powerful algorithm. And that is the magic of biochimists. Most people go through their entire career of computer science without ever doing it, but realizing the power of it. Sankar, can you mute yourself? Yeah, thank you. All right. So now that I showed you the beauty of it in a simple statement, let me show a more complicated example with the axe platform. This example is taken quite literally. So I'm posting all of these files by the way, this is their notebook, and this is taken from their notebook. So what is it? Now we will look at the axe platform applied to pytorch. Bring it to deep learning. How do we do that in the deep learning context? No different. You just do some imports. Now that you understand it, remember what do we need? We need an objective function guys, isn't it? Do you agree? We need an objective function and we need a parameter space to search. These are the only two things we need. Are we together? And so that's what we do. So in our language, what is this? This is a, like, given some parameters, you will return basically the loss for that, right? Evaluate some loss for that. So here they're taking a cnn by the way the cnn comes is a built-in cnn that they have in their tutorials it's a simple cnn and they are going to apply to the mns data set right so so in your case and this is your homework guys replace the cnn with one of the networks that you have already done in your prior labs. For example, you have a lot of autoencoders, you have a lot of these things, isn't it guys? Classifiers, this, that. So go replace this network with one of the networks you have done in your previous lab, you yourself has built, the rest of the code will remain the same, just be careful about your learning rate, et cetera, et cetera etc and then this is your hyper parameter range right that is it you set up your objective function your hyper parameter range and you run it and what does it say it says something quite surprising it says that for mnist data set you need to set and by the way they're using a stochastic gradient descent with momentum. I just explained momentum to you guys. So learning rate is of all things a number you would never have guessed. I'd certainly not have come through grid search. You see it is 0.000288, right, is your best learning rate this is your momentum the best momentum is this right and if you use that then your accuracy on the mnist data set it turns out that with a very simple cnn is already 93 percent right so so if i see uh 20 significant digits. Yeah. Amazing isn't it? Well, I wouldn't go that far. I don't think it needs to be that accurate. No, the learning rate is 20 significant digits beyond that yeah yeah i wouldn't go that far i mean basically you have to take it with the with the thing that okay uh if i am here 0.0028 is maybe this let's not go let's not get carried away but okay i think for every step i think it's it's uh it's dissecting by one digit actually it knows the value for the one digit and for the next for every step i don't know something like that something like that is learning it that. It's learning it. See, remember, it's doing the same thing I taught you what it is doing. There is the expectation improvement. There is an acquisition or selection function that helps you choose the next point. So this Toon CNN notebook is going to end up on the portal and then our homework will be to replace the network with something we've done. Yes, that's what it will be. And I mentioned it, by the way. I'll take you through it in a moment. So at this moment, just follow it, guys. Just follow through with me. So now one nice thing I like about the Axe platform is do you notice that it gives you a very beautiful rendering, a color map, of the loss with respect to learning rate and momentum right so where is the highest score that you see somewhere here right gosh okay you know that the best values are here You know that the best values are here. LR is equal to zero, 10 to the minus four, right? 10 to the minus four. It is somewhere in this region. I wish it was, okay. There is a way to prevent this from happening. Compare data on Huber versus data on Huber. Okay. closes data on a hopper. Yeah, anyway, you can see that the closest, the reason with the highest score right here, it is just giving 16, 18, unfortunately. So what it has done is, these are the points at which it has sampled. It continuously seems to be going there i wish i could avoid it um and the value of momentum was close to 0.28 isn't it to it so it's not letting me go there but you're here ah i'll just Terrible. Okay, I will not go there. And again, where is your lowest standard error in this white region, which is again the same place, 10 to the minus 4, 2 into 10 to the minus 4 and 0.28 or so learning rate. So you can literally see that the island, you know, the best hypothesis space is this. And do you notice something? Where are most of the trials done? Where are most of the dots? Is it in the best spot or in the worst spots? Yeah, interesting. Best spots. Best spots, right? So it has the nose of a greyhound isn't it quickly it sniffs out that region in the hypothesis space which are optimal and you can see that the hypothesis space is very very complex do you see how many local minimas are there in the hypo i mean sorry not the hypothesis space i keep saying wrong things parameters hyper parameter space there are multiple minima sitting there isn't it yeah and yet it has managed to do most of the trials in the right place so this is the beauty of you know the the trade-off between exploration versus exploitation so let us say that it started here yeah look at this the first trial was done here right the fourth trial it seems to be have been done here why is this saying zero zero zeroth trial first trial fourth trial right after that where is the fifth trial it's here so it did this is your exploratory part and then sixth is where let's say where did it do the sixth trial yeah sixth trial is here and so uh the moment it found this point which seems to have a better like a value better score let's see where it picked the seventh point ah look at this it has picked the seven point quite close by where is the eighth point i wish i can find the eighth point so on the top the top ah yes this is an example of exploration or exploitation going from seven to eight. Which is it doing at that moment? Is it doing exploration or is it doing exploitation? Exploration. Exploration. Right. It is exploring. And when it explores, it realizes after a little while that this doesn't look as good. So the ninth point then goes to, let's see. And this is the thing, guys. You have to observe these plots very careful. Now look at this. Ninth point is here, which is better than the sixth. And where was the seventh point? I forgot where seventh was. No, this is eighth. I wish we could number these points. It was explicit rather than hovering over it. Yeah, see seventh is here. Ninth is here, right? So you're in this island of stability here. It pretty much seems white. Now after that, tenth, eleventh is here, tenth is here, it pretty much seems white. Now after that, 10th, 11th is here, 10th is here, 11th is here. So now what is advantageous to do exploitation or exploration at this moment? You would lean more towards exploitation, isn't it? So 11th, let's see where the 12th is. I can't find the 12th one. I think 12th is on the x-axis. Oh yeah, 12th is here. So this seems to be shifting a little bit towards exploitation. It went far and then 13th 14th but you guys get the bane idea right 15th 18th due latest that the latest 16 15 16 18th are all here but it also does exploration even then it keeps jumping out also 14th is here instead right so and ultimately it's doing more exploitation after a little while than exploration. Let's see what is the 19th point. So 18th is here. 15th. So 18th is pretty good. So anyway, I don't know where the 19th point is gone. Oh, maybe here. Oh, here is 17th. Do you see how it is doing exploration far and wide? 13th is here. We were searching for it. It's hiding somewhere else. We'll find the 19 point hiding. But in any case, once you have these points, do you see that your surrogate function is continuously making this contour plots? And looking at it, you don't pick one of these points, do you see that your surrogate function is continuously making these contour plots? And looking at it, you don't pick one of these points as your best answer. This is what you would do in grid search or randomized search. Instead, what you do, you find the minima of the surrogate function, isn't it? Once you have drawn your surrogate function well, you find the minima. Your minima says that it is somewhere here right right somewhere here would you agree that the minima would be somewhere here and you pick the minima of the surface the actual surface and that is the beauty of it guys and and that's why i like this access platform is I like it because it helps do you realize guys that it is helping you see what is happening how the biation thing works once you once you learn to interpret it how many of you find it fun interesting it's very interesting right the only thing is you have to know the theory before any of these graphs make sense to you. Asif? Yes. Have you read the book, Honey Bee Democracy? Honey Bee Democracy? Yeah. No, I have not. Send me a link to that. I'll purchase and read it. What is it about? Talk about it uh it's basically uh you know like in the winter they kind of split nest and uh a group of the honeybees have to carry the queen to a new nest and uh they basically get 500 you know like a distinct number of bees to kind of like do the exploration and they sent out maybe like one fifth for one you know expedition and they will come back and show the other bees what they found and then like at the next time you know it will be like another batch but there are cases where they fail and the whole nest just like die off wow yeah like once they find like two perfect nests and uh uh they'll go different ways and then like they will notice that they lost each other and they'll uh come together again and the process the queen is lost and the entire colony is just gone so are there like so is there like fail situations for the patient it is possible if your surface if the see the true objective function which is an unknown if it is extremely bizarre, like very complicated, but then it is possible that in spite of like if you don't do enough number of trials, you will end and you will be stuck with a suboptimal solution. But what works is that, see, you're doing a trade. See, those of you who know simulated annealing and so forth. Right. There is a pursuit of the optimal based on what you know, but there is also a probability of sort of evaporating out of that, jumping out of that to a random place, just to explore. And because of a trade-off between these two things, these things actually work quite well. So I was gonna say the same thing as if i was going to say genetic algorithms so it's a question of i mean the dennis's quill question that he asked if there's no notion of a failure here right you will end up with something slightly less optimal yes that's all yeah but that also is still the good thing with biogen reasoning and the surrogate function is that is still helping you draw your surface, the response surface. You need all points, not just the points near the minima, to be able to faithfully draw a good surrogate function. So the only thing you do is you can, do you notice how quickly it converges? Isn't it impressive? In just four trials, it has already reached a very high level of accuracy. What is accuracy measured as? Classification problem. How many mistakes, accuracy, how many correct answers you get in the classifying the digits so this is the accuracy of the final model right no no no the model yeah the model yeah final model for this the at the fourth trial the accuracy is already 90 percent of the model let me let me see so the iterations are trying to pick the hyper parameters and then you're building the model using that hyper parameter and what you're seeing here is the accuracy of the model that you built no it's the other way around see for each combination of hyper parameter you train the model to its best value. Then you see its accuracy. So in the first trial, the accuracy was low. So you must have picked a really bad set of hyperparameters. Second time, it was a bit better. In the third experiment of hyperparameters, the model that was crafted had a better accuracy and it is already reaching an ascent it is already at 90 percent by the fourth trial then the fourth hyperparameter combination that you pick when you train your model using the fourth trial value already you're achieving a very high accuracy once you train the model with these values i think that's okay that's what i was saying i don't know if i worded it differently yeah you happen to say the opposite way actually it's okay i i think you meant this but yes this is what it is and then you notice that it's incremental very slowly it's drifting to the optimal value but can you imagine getting so close to the right answer, except by just fluke if you were doing just grid search or randomize search or something like that. One major problem with those algorithms is you will never have the best solution. You'll be somewhere in the neighborhood of the best solution. Whereas this takes you much further. These values that are coming out, they are not the kind of values you would pick normally in a grid search okay so that is the value of this now you can apply if i had a quick question in that you said you would train the model completely using that one set of hyper parameter and then it would do the biation search on different you know set of that hyper parameter but trains over the whole training data for every point that was there on the graph it has trained on the whole training data right that is right. So remember, look at your objective function. That will reveal it. Let's think about this is our objective function. What do we do? Given one hyperparameter point in the hyperparameter space parameterization, what do you do? You build a neural net, take the neural net, you train it with the data you know with those hyper parameters set you know the training data you know this is your standard uh pi torch code you give it the training loader you know data loader you give the hyper parameters you give the device cpu or gpu then you evaluate this is the one where you evaluate for let let's say, accuracy. Right? This return evaluator will return the accuracy of the model. My question is, why do we need to go the whole training data? Or so as to say, why do we need to go? Can't we batch by batch try out something much more effective? It's not good enough, no. See, the thing is that to train a model, once you have frozen the hyperparameters parameters you need to run it through a few epochs no okay got it right you need to otherwise it'll you know it would be you prematurely stop you don't know whether you're fully trained with that model got into the optimal value for those hyper parameters or not okay that's the point so guys this is it. I'll post this code here. Here is your homework. Replace the CNN network used in this tutorial with some of the networks that you built. This is it, a very simple change. You realize that the homework that you will do will be just one or two line change. Import the sv-learn library and then the right import from sv-learn library. It is then import this particular network that we have built. We have built quite a few neural nets. Stick it in there and see what hyperparameters it learns. And you will experiment with it. Become familiar with it, guys. This is good. This space is very, very alpha at this moment. Things are rough and so forth. And I found that the X platform is pretty good in terms of maturity, reasonable maturity. And the one thing I liked about it is this. I love this plot. It's a very nice dynamic plot. It shows me what is really happening with the data. So guys, we're getting to 10 o'clock, and you guys wanted me to stop at 10. Let me show you some other things. This is just one of them. Then there is AutoSKLearn. AutoSKLearn, and again, actually, let me not even try with Chrome. God, that's what happened. Let me go back and do auto-sklearn. Very promising, people are talking about it and this and that and so on and so forth. My experience wasn't too good. When I tried to install this, it failed. But according to their website, it should have worked. It didn't. But according to their website, it should have worked. It didn't. Right. But if you did that and you ran this code after that, it should have worked. If you look at this code, this code is no different from that. The only thing is that you you instantiate an auto-SKLearn classifier. So their approach is different. They are not saying do this Bayesian search cross-validation. They are doing it, but they're doing it under the covers. They are saying pretend as though there is a new kind of classifier called auto-sklearn classifier. Feed your data into that and you're done. So you don't have to know anything about Bayesian optimization. They will take care of everything under the covers. That is their approach. Then there are other. Let me show you. Auto Keras is Auto Keras works. Right. So people who do TensorFlow and Keras is very easy. So here is it. I tried it on the California data set. I think they give you unstructured data. You can also do it on the image classification data actually let's go and look at the image classification data no different see you notice that all they do is aka image classifier fit and evaluate nothing special nothing special right so all of them tried for simplicity but their maturity levels are at this moment very able so auto sk learn does work autopy touch though is try that where is it it is really empty okay i didn't put it so this is the thing guys uh your homework so now we should come to the homework since only a few minutes are left is there something that i missed auto keras auto sk learn autopy touch okay so there are left is there something that i missed auto keras or to escalate or to prioritize okay so there are more guys uh what i would suggest is for example i missed an important one teapot now the other important one i missed is hyperopt and those are all significant uh maybe i've opened those so i post some articles. This is a good article for you to review Bayeshin and see if you can use this to understand it after the explanations you went through. So I will post a lot of information to the portal along with these notebooks. And this is auto-escalant learn right in theory it should have worked right in practice it didn't so that's a separate thing but then there is the hyper opt hyper opt is very good actually this will work uh yeah this notebook i didn't do because all i had to do is walk you you through so hyper opt is actually very well regarded it uses um the this method the way that i mentioned the tpe parsing opt-in estimators so nowator. So the code is very simple. So let's see if this code makes sense. Good old pandas and NumPy. So guys, the theory is you just need an objective function, create an objective function that you like. And they have taken some polynomial here. Right? This polynomial looks like this shape. So where is the minima somewhere between four and five four and six right five maybe a little less than five 4.8 or something like that it looks like the minima let's see if it is able to discover it you give the domain in other words the parameter range here you're searching from minus five to six, the range you will search for. Now, how different is the library? Let us go and see how hard it is to use. So you create a few trials, you decide, this is just keeping track of the history, trials and so forth. But the main thing is fmin, look at this. You first come up with the function, minimize this function, come up with a surrogate function, and then try it out and find what it is. Just a couple of lines. It's a little bit more involved. At the end of it, it comes out with, yeah, our guess was right, around five. It comes out with, yeah, our guess was right around five, right? It comes out with this. So if you really think about these libraries, they all are doing, what are they doing? They are picking up the objective function, the domain, and after that is just applying it, right? And that you need the surrogate function and the acquisition function and implementation details. They're using TPE and that is it. So this is for Hyperopt and when you notice it, when they do a lot of these trials, all those trials seem to gravitate towards the red line which is the truth. The mean is that the values, the peaks at 4.8. So more of it when you read through it, but it's straightforward. So this is for Hyperopt. Is there any other library that I wanted to show you guys? Yeah. I think what about SigOpt? Yeah, see, there are many of these. Keypot is to be considered SigOpt. That one I haven't played with. Okay. So guys, why don't we do that? Could each team propose by tomorrow, each of your teams, project teams, propose a few libraries that you will investigate, post it to Slack. And so you realize that investing all of these libraries is not hard. All the APIs are simple. It is a maturity level, you'll have more struggle installing it than actually running it, right? Or understanding the code, understanding the code will be straightforward. Remember all of them just have an objective function, parameter space to search, and then some way to search it, some optimized method. Oh, looks like Intel bought a go up recently. Oh, OK. Yes. So with that, I'll stop, guys, I'll post all of this. I should have posted some of this to the portal before, but I'll post it guys tonight or tomorrow morning. But do please form teams and do it. How many of you feel that this is a lovely thing and we must always use it? Seems like a good idea. It's a good idea guys. It's one of those things that most people haven't woken up to but i don't see a reason why they shouldn't and why everybody shouldn't be using it so we are in a different world forget about your grid searches and all that and especially if you look at the sk optimize and so forth they have made it so trivial for you. Just replace the word grid search with the word bias search. That's it. And you're on the go. So as if in the practical way that you use this, what I'm assuming is you need to somehow make the problem smaller, get a representative problem that you can put through the AutoML path to find the right hyperparameters because if your neural network takes let's say a few hours to train and you're trying to do AutoML with that network then you're looking at a few days right no actually that is where it shines how How would you explain that? Let me just say, what happens today is these neural networks run for days. So there is no feasible way that you can do 1000 experiments of grid search or randomized search. Because even if it takes 10 days you don't have 10 000 days that is the professional lifetimes of people isn't it 30 years so uh you can't do that now did you notice how fast the bias and optimization converged you saw that right graph within four or five trials it was already near the right answer. Okay. So that is specifically the place where you would use automated machine learning even more. In fact, that is where it is designed for. So I think the absolute measures that I was using in my head is a little big, right? So what you're saying is, yes, you have to go through the neural networks whatever time it takes, but if you hadn't used this base in learning, it would have taken even more. Right, right. It's a difference between one month, like let's say that you have to do four trials and each trial takes about eight days, right? Versus taking 30 years, your entire professional career to find the best yeah okay student model the differences are just mind-boggling differences that is the first statement that I said then that is not the so you still work with the full network there's no concept of making it smaller yeah yes yes you still work with the full network so guys the point is that you know we are entering uh now as this workshop finishes i brought you to essentially the cutting edge of ai we started with the foundations you're realizing that i hope by now that we have covered all the topics and i'm leaving you at the cutting edge the two topics remaining time series you guys had insisted from the last years that we covered it, and it has some use in deep learning, we should learn it. So I wouldn't be here for the next eight to 10 days. So Dr. Chandler will take over and he'll do the time series. Then when I come back, quite likely, I'll take off wherever he is. We have one last thing, interpretability of AI models. I'd like to finish that. And then we'll just focus two weeks of labs, you know, projects. All of you finish your projects, guys, and present to each other. See, you have already started doing. You guys are learning from each other. And that is what I want to encourage in the last two weeks. I will be just moderating over and giving you guidance. But I want to just see in the last two weeks you guys finish your project. Asif, are you releasing the second project? I will release the second project, yes. Okay. So. Asif, I have a question. So the Bayesian learning we used to optimize a particular model, but how does it relate to AutoML? Because we said AutoML... The Auto part of it is finding... So there are two aspects of the AutoML journey. One is the hyperparameter optimization is at the core of automated machine learning. Automated means automatically tune the model to its most perfect hyperparameters. And an extension of this idea is neural architecture search, which means that, for example, in the case of COVID, I showed you how many layers to have, in the case of COVID I showed you, how many layers to have, where the residual connections would be, those are also hyperparameters in some sense at the neural network level. And you can even design the neural network itself as a hyperparameter optimization problem. That's a neural architecture search in some sense. It's a very rough way of putting it. a neural architecture search in some sense it's a very tough way of putting it you got that yeah yes okay if it's on topic where i had a slightly different question not on topic no no no uh exactly the nature paper that he talked about and asked us, and it's related to Premji's question. There was mixing two entirely different things. Okay, on yesterday, we covered this paper which was on learned optimizers, right? There we are doing something similar. We are basically saying, forget about Adam and all this. Let's learn the structure of the perfect optimizer using a neural network in its own right. That thing, that part of the research is very immature at this moment. This is not, we're talking of a slightly related or different topic, learned optimizers. Learned optimizers, we are at the beginning of a journey. Got it, okay, so I confused it. It's not all the high pass parameters, just the optimizer alone. Optimizer itself, the hypers parameters, just the optimizer alone. Optimizer itself, what should be the structure of the optimizer? Got it, okay, thanks. That's a different thing. All right guys, anything else? Not on this topic. So I noticed that they're installing a whole bunch of libraries once in a while, right? Like right now for AutoML, there are a few things we're going to install yes but i have not heard you mention uh the p e and v pi n right oh the virtual ends the virtual environments yeah should we be looking at it would that make our lives simpler any thoughts see the basic thing is i have avoided some of the you know rigor that you enforce in the workplace one of them is that whenever you start playing with some code or something it is always a good idea to do it in a virtual end so that your host operating system is not sort of screwed up right so suppose you get one of these libraries and they're very immature you don't know what it will do to your host system and the rest of your python libraries it's always better to do your experimentation in a new virtual end yeah generally you should always use virtualv. This is a general tradition guys to containerize or just to isolate. For example, nowadays we don't do bare bones deployment of anything. We use dockerization. Even if you test a software we had installed, we would go create a docker image and take that around because it becomes portable, and take it around. So in the same spirit, when you create a virtual and when you do the setup installations within it and then do that at the end of it, you can export all the required libraries of it in a requirements file and somebody can reproduce your environment by just running the requirements within their own virtual environment. So when we do the virtual environment, does it create a copy of existing what we have or it will create a fresh copy where only what we ask is going into it yes it will create a fresh copy where only what you ask goes into it okay besides the python language itself but everything else you'll have to start from scratch so what you end up doing is you create something called a requirements file and you run it you do pip install minus our requirements dot txt and whatever requirements are mentioned there that environment will populate itself with those libraries and you know you don't see any issues with working in a conda environment with that right yeah your corner gives you the ability to create. I'll show you, hang on, since you asked this question. Yeah, you can create, I have almost 10 environments. So you can create a new environment or you can clone the environment. Yes. And the question came to my mind looking at when, as you've said, all these libraries we're gonna look at are kind of immature, so I don't want any of them downgrading other libraries when they try to play with them yes i think you are already fatally lost because you are working on mac okay guys so i'm kidding by the way don't have apple come and sue me so you see that when you have your environments here So you see that when you have your environments here, then you can go and creating new environments. You just go at the bottom. Do you see the bottom left-hand corner? Do you see the create here? Yeah. Just create a new environment. That's it. I won't click on this. It will ask you for an environment name, it will ask you what packages, etc. Then take a requirements.txt in which you have itemized all the Python libraries you want and run it and they are done. Okay, so that's it. All right guys, I'll just stop. Thank you.