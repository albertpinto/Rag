 So in machine learning NLP stands for which of these obviously it stands for natural language processing I thought almost all of you got that right. So given a body of text T in a human spoken language X in order to translate it to some other language Y, which of the following is true? So one of the things is it used to be very hard actually to teach machines to do language translations. They were very very poor at it. One of the big breakthroughs that have happened with the coming in of deep neural networks and support vector machines both is that some of these things as the more advanced algorithms came these things became possible. Now there's been quite a bit of theory like people who did computational linguistics or language modeling, natural language processing without AI, they used to have all sorts of statistical models to do to bring about something like that. But with the coming in of deep neural networks, it has been a game changer. The first neural network architectures that could do it were called the recurrent neural networks. And in that generation, were called the Recurrent Neural Networks. And in that generation, you have the LSTMs, GRUs. But in the case of recently, actually, more recently, there has been the breakthrough of transformers. Transformers were discovered, or the paper on transformers came about in 2017 December. Since then, the last three years, 2018, 19, 20, two and a half years, we have had a sea change in natural language processing. Today we can do a whole lot of tasks with transformers and their variants or their combinations with RNNs and so forth that have a far higher accuracy. You see this being talked about in the market. You see chat bots everywhere, every company trying to incorporate it, knowledge summarization. If you have tickets, bugs in your system, how can you find duplicate bugs and so on and so forth. Lots and lots of people are busy doing that. And the efficacy of that is just astounding these days. These natural language processing, these transformers are able to even write code. So one of the questions that arises today is, the GPT-3 for example, is you can describe a UI in English and it will translate it not to German or French but it will translate it into a pretty good HTML and JavaScript that is bug free. And that is quite amazing, isn't it? Soon they may be writing the code for us in a few years. So that's the scope of this. Today, translations can be imperfect. If you do translations, you see sometimes it can be hilarious. But at least by reading the translations, most often you get a sense of what was said in the original language. Go lunch so that's a big problem right I mean like across geo-distributed data centers and you know data privacy all those things so does I'm just wondering does this DNN solve any of those problems or it's still that onus is on the data collection is still very much there that problem is still very much two things have happened pre-pro it is called pre-processing all the steps you do before you do the machine learning the training of the machine learning model is called pre-processing pre-processing has always been a very manual task. Programmers would do it. They would write, some of you may remember the world of ETL, extraction, transformation, and loading, things like that. And today we call it data extraction or data ingestion and all sorts of words and data munging. So these are more or less synonymous in a broad sense. There is a lot of data transformation. The reason is data will never come to you in the way you want it. You have to do a lot of traditional engineering to get the data into the format that you do. And that has not changed. What has happened though in recent terms is, and you will learn that is the crux of AI, that people used to do feature extraction like what is the input features that will go into your model the X vector the one of the remarkable achievements of deep neural networks or deep learning is that it does something called representation learning which means that you don't have to do feature extraction once you have vectorized the data then it can automatically discover the relevant features that is one of the most amazing discoveries in fact that is what brought back the neural networks back to life in 2006 the great breakthrough that Jeffrey Hinton made was with representation learning and so forth. And deep learning became a big thing again. In fact, it became a big thing. The word when we say deep learning, we refer to deep neural networks. And it is fairly reasonable to say that a good starting point would be Geoffrey Hinton's work in 2006. So we have moved the needle a little bit back. Now representation learning is there. But can you take care of the ETL jobs and so on and so forth on its own? Once again, there is some progress. See, in business, they're common entities. There'll be a user. There'll be a user can be the buyer or a person in, you know, employee of a company, whatever it is. And when you store it in databases, especially of the relational kind or in a company whatever it is and when you store it in databases especially of the relational kind or in mongo or whatever it is people tend to give very common attribute names mongo document field or relational databases column tables columns they will have things like first name last name and this and that one of the things that people have done and are doing now is they're using deep neural networks to inspect the data and infer what is it, what is this column, what is this attribute, by looking at the data and inferring it, by looking at the values and inferring it. So that it is not completely automated, but there's a lot of effort to cut down on that pre-processing task. effort to cut down on that pre-processing task. And there's significant progress. I would say that, see much of the ETL and other things, they will certainly get accelerated. With this AI techniques, people are applying it to inspecting data and discovering the usual suspects, the user entity, the user columns, the user fields, first name, last name, and so forth, and so on and so forth, you know, items. So there is a consolidated effort to have AI permeate the world of ETL and so forth. And I would say, if I have to guess, see ETL is a big manual problem. It takes a lot of effort to make. There's a reason why businesses like Informatica and others, they have such a huge following. They make tools to make it easy. Ultimately, you can do all the ETL in pure SQL. Informatica just simplifies the creation of the SQLs by a visual designer and things like that, and then runs an ETL server to do the transformations for you. So in that space, AI is entering by making just accelerating the process, discovering the usual things and so forth. But at this moment we are nowhere close to having it figured out from plain language what should be done with the data in doing it. Though there is a lot of research that's happening there, academic research, we are still a little bit far from that um but just like um mapping onto like you said in the um automated features for extraction right would it be fair to say something like uh in distributed systems for example uh large-scale distributed database there are tons of logs being generated so many logs trying to figure out what, I mean, root cause of any kind of problem, like security attack. Yeah, root cause analysis, AI is very effective. In fact, I- So, if I had to say something like, you don't need to worry about that. So basically, which fields to pick from the log, but this deep learning would auto extract it for you like don't worry you know techniques there are many many techniques of AI that is people are applying to log analytics and with with with fairly high degree of accuracy they can pinpoint the root causes often or at least point you in the right direction narrow down the choices so the applications are there people are doing it reading frameworks to do it I don't know how much of it is commercialized but people are certainly writing papers and things like that on that the next question question three what is XAI? Of course it stands for explainable AI. It was just a terminology stuff. So we covered that. The fourth one is transfer learning is what it is. Taking a model that has been trained on one task and using one set of data and then applying it to another related task, of course. Like we did it. We did ImageNet was an example of transfer learning right we just transferred the problem to our stuff. And sometimes you can't just use the model as is you have to train the last few layers, you know, that is called fine tuning and we learn to do that also in the coming weeks. fine-tuning and we learn to do that also in the coming weeks. So AutoML is what? So this is a question I hear a lot from business. You talk about automated machine learning and the first thing they'll say, oh Google AutoML. No, it is not. Google has nicely captured the buzzword AutoML as the name of one of their services, cloud services, but no. Automated machine learning is the subject of creating machine learning is the subject of creating machine learning algorithms that tune themselves for best performance the best architecture and so forth now CIFAR 10 classifier was the lab class that I gave you guys I hope did you guys get a chance to play with it a little bit massive yes do the lab if you don't do the lab you'll never gain confidence see both theory and labs are important please do the labs carefully later on review it and make because we'll move forward and do a lot of code here in this workshop practical code. So alright, so the CIFAR one of the homework that I had given in CIFAR was see if the error rate decreases with the number of epochs. It does. Right. And the other is if you change the learning rate it affects the efficacy of it. You can screw up the the the accuracy of your model by taking too big a learning rate. screw up the the the accuracy of your model by taking too big a learning rate and those were the things meant for you to discover by doing doing the home lab works so do the homework guys and do the homework please question number seven which of these topics are considered a part of automated machine learning? The answer to that is, oh well, a neural architecture search is the one I think you missed. But other than that, neural architecture search, hyperparameter optimization and meta learning. We talked about these in our lecture. Then XAI stands for extremely large AI models. No, it does not. Obviously, this question almost all of you would have gotten right if you have been attending the lectures. The next is inception V3 is one of the pre-trained models available that can be used for image classification. Well, we did it in the lab on the Wednesday lab and if you did the homework, of course, you must have seen in the lab we saw that inception was one of the models in fact it was the one giving the highest accuracy for the examples that we took so it is one of the pre-trained models available we did that in the class question number 10 which of the following are some of the publicly available pre-trained models available for tasks such as image classification. So the answer is obviously I treat a lot of things. I don't know if Delbert net exists. I don't think so. Then Alice net is also just a spin on Alex net to confuse you. There is no such thing as Alice net. There is no such thing as Pythagoras net and image magic is a Linux software to do image processing, not a neural net. So here, these two answers are correct, but AlexNet is missed out. Google Inception should be there. GoogleNet and Inception both should be there. So three of them are missed out. The correct answer is AlexNet, GoogleNet, RestNet, Inception, VGG. One of you are asking a question. Did I hear a question? All right, I'll move forward. Given a body of text, we can apply statistical and AI techniques to estimate the overall readability of the text by humans. This was a lab. I hope all of you got it right. We literally did it in the class. Yes you can. Deep neural networks represent the state-of-the-art in machine learning. As such they supersede and always outperform all other classic models in this space. This is actually a very popular misconception. Deep learning wipes the slate clean. It's the only thing that matters. All we have to do is do deep learning. Deep learning is the cutting edge for sure, but it isn't the only cutting edge. There are many areas of research. Not only that, there is such a thing as a no free lunch theorem. Those of you who did ML 200 with me would remember that I emphasized the no free freelance theorem quite a bit. No freelance theorem is one of the most profound results. Not just for AI in machine learning, but to the theory of human knowledge. What it says is that no one model will on average outperform another model. So every model has a space under the sun. There are certain data sets for which one model will do better than the other model. So that is a point. In particular, for smaller data sets, actually neural networks tend to do pretty badly quite often. So remember that. And one easy way to check this is to go to Kegel and see for the various competitions which model won, which algorithm won. You'll realize that deep neural network doesn't always win. Deep neural networks perform inherently complex nonlinear transformations of the input to generate the predictions. As such, it is a full-serving to try to explain their behavior or predictions all we can expect to do is train them to high accuracy i hope if you attended our sessions last week you would realize that this is certainly not true we expect ai is our black box model but what we are doing is putting in every effort to make them explainable putting in every effort to make them explainable, impose interpretations and explanations for them. So black box models are a dangerous thing and one of the civilizations, one of the realizations recently has been how catastrophic the impact can be on civilization if they are not carefully understood and their biases surfaced. Therefore there is considerable effort to have explainable AI. There is a growing collection of powerful techniques that help understand a neural network's behavior. The correct answer is false. I believe we are at the end now. Transfer learning means taking a model trained on one set of machines and running it another set of machines for a given data set. I hope you all realize that is certainly false. Transfer learning is taking a pre-trained model for one task and applying it to another task. Related task. When a model learns from training data, all its hyperparameters get trained. I take optimal values for the task. Why is this false? Because it is the parameters of the model that get trained not the hyper parameters hyper parameters by definition are the God-givens to the model it never tries to tune that and that finishes this any questions before we stop here this one was a easy one isn't it I'll stop the recording here you