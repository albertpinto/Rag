 So good morning, everyone. This is Saturday, April, I believe, 21st, 22nd, April 22nd here in California. Yesterday was one of the important Indian festivals, so happy to all of you. Because of that, I noticed that in-person attendance is a bit low but a lot almost everyone has dialed in remotely so um I'll see who is there right now here is uh Sukpal and Praveen and Sushinder they are here and I think Dennis is also here. A couple of folks are here, a few folks. Almost everyone else is remote today. All right, folks, so to get started, last time our theme was, we continued on the theme of transfer learning and we delved into fine tuning models. What did it mean? In transfer learning, there are two phases, to recapitulate. The first phase is someone trains a model for a particular task. And generally, when you train it for a particular task, especially these large language models, these transformers, it takes vast quantities of data once it is trained for a particular task the second leg of the journey comes in you ask you ask the question that for my particular task can i use this pre-trained model Can I use this pre-trained model with all its weights and biases established? Can I use it for my task? Now the answer could be yes, if the task for which it was trained is exactly the task that you have. It may be a qualified yes if it is the same task, but the data on which the pre-trained model was trained was more generic or was different from your specialized dataset, in which case it may work with a certain degree of accuracy, but may not work as well. We saw that as an example for tweets and sentiment analysis. If you take a sentiment analysis engine that's been pre-trained on generic data, it gives a certain degree of accuracy on financial tweets. But if you take a model which has been pre-trained specifically on financial data, you get much better accuracy on your own financial tweets. So that makes a difference which data was the model trained on, even though it's the same model. What went in in training the model, because what went in in training the model ultimately determines the values of the rates and biases of the checkpoint. So when you download the checkpoint is it's more optimized for the data set. It was trained on now a third, typically what happens is you encounter situations in which you want the model to perform very well on your specific domain. You need to do some domain adaptation or fine-tuning of the model. You have specific label data and you want to wanted to learn a little bit more and be specialized for your problem set. Now, this process of fine tuning have many causes. First, because you have specialized data and you wanted to become adapted to that domain. Another reason could be you got access to more data and you want to train it further. Or it was trained on an adjacent task. It wasn't trained on the task. The transformer wasn't trained on specifically your task. And now that your task is different and your data is different, you want to go through a few epochs of training specifically for your task, right? And zero-shot learning isn't working good enough. So these are some of the reasons why you would want to fine-tune the model. So again, to recap, there are two stages to transfer learning. The pre-training step, which somebody trains a model on a more broad set of data. And usually the pre-training is very, very expensive computationally in terms of the resources it takes, in terms of the time it takes, and not just computer resources, but also in terms of the data. So for example, BERT-BASE-UNCASED is trained on just about all the English text you can scrape off the web. Now the question is that if you train a model, let's say a transformer on such vast quantities of data, what is the task that you will give it to train? You need to pick a task that is generic enough, that's powerful enough so that you can train it well, but at the same time it should be adjacent to a lot of useful tasks. So the original BERT paper, for example, trains the BERT model on mask language model. So what it does is it takes existing sentences, wish model. So what it does is it takes existing sentences, paragraphs, and then it randomly goes and masks some of the words in the sentences and trains BERT to reproduce those hidden words, masked words. When it does that, you realize that there is no way a transformer could have done that with any degree of fidelity without understanding the semantics of the language, the content. And because it learns to figure out the semantics of a sentence, that trained transformer now can be repurposed to a variety of actually useful tasks like question answer, like classification of that, like sentiment analysis, like continuation and so on and so forth. You can use it for many, many things. So that is how you train a model. Now these trained models, pre-trained models, are trained through a process of self-supervised learning because nobody can create a trillion rows of data, label data, despite throwing armies of people to label data. Labeling data happens to be a very, very expensive and time consuming process. As you probably know, today, companies that do the labeling of the data, while it may look tedious or low tech, they're actually making, minting money. They are melting gold because there are so many AI companies that want label data. So it's the old, you know, the gold rush in California with the gold rush king. The companies that really got rich were the ones making the shovels and the jeans for the process, the tooling. And so this label data companies are doing extremely well from what I understand. If you're in a hurry to make money, go start a labeling data labeling company. Yeah, the scale AI. Scale AI and yeah. It's a very good business. Very good business, yes. So, but when you, when you pre-train one of these large models, you can't even despite that, you can't really get a lot of trained data. So what you do is self-supervised learning. You take a task in which you can create, in a sense, labeling out of the data itself. If you mask a few words in a sentence and you ask it to tell you what words were masked, you have essentially created label data. There's a masked sentence and there is an inferred word or words that have been masked. So you have created label data to do supervised learning. It sort of is paradoxical that you can do supervised learning with data that you have just scraped off the web and that is not labeled. Now, having done that, when you fine-tune a model, you don't need so much data because some of the harder things is already done. The transformer is already trained to understand the language. So if you want to, for example, fine-tune it for the medical domain or finance domain, etc., you just need a few samples. And how few depends. It's a case-by-case situation. And how few depends. It's a case by case situation. Sometimes, I mean, on the one extreme is zero shot. You just try to give it new labels and you say classify into these labels. It can work surprisingly well if the vector embedding of the target space is effective, the semantic vector embedding. And we learned about that in the very first lab, how to do vector embeddings, semantically aware vector embeddings. So you could use that to do zero-shot classification as an example, or you could give a few examples, few-shot classification. The few-shot classification is something that we will actually use today. We will go more, one of the examples we'll do today is a few-shot learner and see how that works. It's quite fascinating how these transformers, they can learn from just a few examples after that. And then of course, if that doesn't suffice, you have to give it more examples. But generally, you don't have to give it billions of examples to trade. You need a very finite data set. Sorry, guys, today, you will see me John is because spring allergies, it's, it's sort of drained me, and I need more oxygen. So because I need more oxygen. So because I need oxygen, I tend to yawn. That is not an indication that I'm in any way sleepy. I'm quite fresh, but I apologize if it looks like that. So to recap, we did in the last session, we did fine tuning of the models. We took a model and fine tuned it for purpose. So let us recap what we did. We took a dataset, which is the glue MRPC dataset, which is, is the second to state given two statements is the second one genuinely a re paraphrasing of the first one. Right? So, so that is why it's the Microsoft rephrasing corpus in the glue. It's part of the glue benchmark. How did we do it? Let's repeat what we did. We took a pre-trained checkpoint and which checkpoints did we take? We just took the bird base uncaged, which we have been using quite often. It is a sufficiently large model. It's not small. It's not like the still bird and yet not so large that it wouldn't fit on our workstations and our laptops. To recap, whenever you get two sentences, the first thing you need to do is you need to tokenize it. And because the transformers don't take sentences, strings, they take essentially vectors as inputs. All machine learning algorithms take vectors as inputs. So what you need to do is replace a word by its ID, index. You need a dictionary in which the key is some number, value is a token, a word by its id index you need a dictionary in which the key is some number value is a token a word actually not word but if you recall we went deeply into it those are word tokens word pieces right not necessarily word so it will do that and once it has the id associated with an id would be a one hot vector embedding right and so you get the one one hot vector representation in a vocabulary so far so good guys i hope by now we have covered this territory quite a few times and this is the nth review of that of the basics uh any questions so far the data collator that you used here with padding uh can you just elaborate a little bit about why we needed to use the data collater here? Yeah, what the data collater does is that it will do basic things to the input. Like, for example, when you get a data set, like when you have a data, so think about this. When you have a data set loader here, we are loading a dataset that has 3,668 rows. It's made up of sentences. Now, when you feed it into a transformer to train it, or even for inferences, you don't want to feed the entire dataset because you'll have an out-of-memory error. And even otherwise, a batch training, even if there was no out-of-memory error, a complete full batch training is generally ineffective. It is far better to learn from many batches of data. So people, when they say batch learning, they actually mean many batches. It's an abuse of the term, but they mean many batch. So somebody has to take, so imagine data is a pile of sand and you are bringing it, a pile of coal, let's say, and you're feeding that coal into a fire. So somebody needs to take a shovel and take one shovel full of coal and put it in the fire. Right. That thing that makes many batches out of data and feeds it into the fire in this in this idiom, the fire being your transformer. Right. And the coal being the data set, the original data set. So the in-between agency, that is the shovel that brings many batches of coal and feeds the fire with it, is the data collater. For example, it may notice that some sentences are longer than others in the mini batch. So what it could do, for example, as added functionality, is it could make all the sentences the same token sequence length. So it could pad it with empty tokens so that it truly looks like a matrix, a proper even matrix. So that is the purpose of a collator. So this is it. You have a data loader that can become tokenized. But when it becomes tokenized, the sentences. So what you do is you take a mini batch, you feed it into the token. You take the entire data set. It becomes a whole lot of tokenized data now. But you don't want to feed all the tokenized data, you need to take mini batches of it and feed it to your FHIR or feed it to your transformer. So the person that does it is the data collator. And it will along the way also pad the sequences of tokens to make them all the same length. So Moushmi, does that answer your question? Moushmi Anandani- Thank you. So that's what the data collater is very useful. If you folks remember in PyTorch, that's what we do. The dataset API in PyTor python inherently gives you an iterator that iterator brings you many batches of data to feed into the training into the model it's exactly the same thing what is iterator there is data collator here right in the hugging face language in the hugging face language. So, and I apologize for those of you who haven't taken our deep learning course. By the way, deep learning course, two announcements. I'm postponing it by a couple of weeks simply because we are extending this course by a couple of weeks. So one of you who is friends with Shah Nawaz, could you please tell him that we are postponing it till NLP workshop finishes yeah as it's up we'll take care of it please take care of that and uh so you have also registered you are aware of this so uh so deep learning course will start there for two to three weeks later let's finish this course properly we still have some territory to go some beautiful things to do can i ask a question before you um go forward yes so different llms um so different companies have their different language models a lot of them are trained with the different number of data points and then the second dimension we have some that are labeled and some that are not labeled. So some have labeled and not labeled. I know you mentioned ScaleAI labels them, but ScaleAI also does another, has another section that does fine tuning of these models for them and does research work on that. So what are some of your thoughts regarding that? See, it is true that, so there are two ways of looking at it. First is companies are coming up with adaptations of the transformer, their own homegrown adaptations. For example, GPT-4 is not just scaled up, but there is a bit of reinforcement learning thrown in, I mean, in the chart GPT by the time you come to it. There is the GPT-4, but but there is there is a bit of reinforcement learning thrown in i mean in the chat gpt by the time you come to it there is the gpt4 but then there is also the interpret ai and a bit of reinforcement learning and so on and so forth so you throw in your special flavors of things that makes it uh unique for you but even when you do that you do feed it your data. So a close analogy is two people, you take twins, right? Or you take two people, two brothers, who are sisters who are two, maybe four years apart, right? Yeah. Now, they may be two sisters, but it so happens that the mom was eating different food and was in different environments when those kids were growing up, those daughters were growing up. And when they grow up, they learn from maybe they go to two different schools and learn different facts. So at the end of the day, what will happen is first genetically, there will be variations. Secondly, their development, what they were born with may be different. One may have gone malnourished because the mom was going through a patch of economic hardship. Another may have come out as a really well-fed baby, healthy baby, and so forth. So they may be fundamentally different. And then the education, if they go through different education, different experiences in life, and you know that the two sisters, no two sisters are ever alike. 20 years later, when you meet them, you find that they are different people and very similar to the AI models. At the end of it, every company is doing some variation in the architecture. So they're inherently slightly different. Though fundamentally, frankly, I don't think that all things considered, there has been any very big breakthrough since the original transformer paper. We have just scaled that same idea up, added things to it. There have been fascinating things done. I don't want to devalue that. Very fascinating applications of the transformer. But the transformer architecture, it hasn't been reinvented in any fundamental way. But a lot of different data has been fed into it to create this pre-trained models and people are precious about it. Next comes the question of the scale AI. See, in business, it's very common. You annex adjacent territory. So if you're labeling data, you ask yourself, what are the clients doing with the label data? They're training it for something. You ask this question that if they are fine-tuning it, they need data scientists. But if you could fine-tune it for them, they won't need those data scientists. And then you could even deploy it as a service and they could just be using service and their software engineers would be just fine in working the service right so it's just value add you go and annex neighboring territories so it is both both a pre-gone conclusion that companies like scale ai would venture into the territory and it's smart of them that's what business is so there's there's two kinds of labeling that i saw one in computer vision when tesla was labeling it they had somebody in guatemala uh you know draw this as a car so they can see they would take a their mouse and they would like click dots over it like you know kind of what we used to do for segmentation um you know in the old day old days so they would do that that's kind of how they would label but i also noticed that there's another labeling for large language models how they did it was completely different there was some set of skills but this is funny because they sent some of these to Guatemala and some of these to India. And there was scales in terms of is this happy? Is this sad? Is this? So they have different ways of labeling. So do I mean, what are your thoughts on the different the different techniques, labeling techniques that are out there because even snorkel AI's tried automated labeling, Pinecone, all these people had new tools. So what are your thoughts on labeling? This might be a digression, but I think since we're going into land chain, some of this might be important. Yeah, we'll come to that. So it's a very good question, Shani. And the best that I can answer is by quoting Leo Tolstoy and his book Anna Karenina, which starts with the famous sentence, All happy families are alike. Each unhappy family is unhappy in its own unique way. And what I would call these labeling strategies are each an unhappy family and unhappy in its own unique way but by that i mean all labeling strategies currently that we know of are deeply imperfect and yeah very susceptible to the introduction of biases and errors cultural biases and errors right and the perpetuation and the anduating stereotypes. And so let's just say that we are consciously, we need to be conscious that these are very, very imperfect tools. These are necessary evils. I would leave it at that at this moment. We need to make rapid improvement in the interpretable AI and in bias detection. So there's a lot of work going on. There is rapid progress, but we need to make far more progress. We haven't figured out how to create unbiased data. And so long as a biased data is a direct product of labeling, labeling remains a very imperfect process. Thank you. Thank you for that analogy, Asif. Asif, can we like, you know, like in clinical trials, for example, we have these double blended studies, right? Yes. And that's kind of to remove the in some ways the bias or even you know tendency to so is there something similar that you can do in labeling as well yeah kasi people have tried that so for example they would send the same thing to different agents and see how they labeled it and they would look for discrepancy. The problem is it fixes some errors, some discrepancy but not. I'll give you examples. Today, if I don't wear glasses and I look at you and I open my eyes wide, at you, and I open my eyes wide. Generally, when I'm talking to you, you're much more likely to trust me. There is more likely to be a connection formed. Yep. Right? You will consider me more open. But suppose I do like this, because in my case, because I do have astigmatism. And if you have astigmatism, you know that without glasses, if you squint a little bit, then things become clear. Yeah. If it can see it. So if you squint a little bit, it immediately creates a distress because it makes people wonder, why does this guy have squinty little eyes? Now, I wear the spec. There is a sense that, okay, I'm talking to somebody who, you know, I can't form direct connection to but the games change but maybe he will talk about things with great details. So what happens is that some of these biases are universal, quite universal and it is far more exaggerated when you talk of the female sex. of the female sex, right? Women, for example, it is an inherent thing that you wear glasses and you go for an interview and you'll do poorly. Is it? Yes. Oh, right. Your name is a bias. Call yourself Jackson. And you're less likely to be called for an interview. Because Jackson, and you're less likely to be called for an interview, because Jackson as a last name is fairly popular amongst the African, Afro American crowd. Right? So all yourself, Mr. Charles Thornhill. And you're likely to be called for an interview. It's a very Anglo-Saxon name. Right? So and this, by the way, is not true just in the United States. It is universally true. You can take people from anywhere in the world. So these biases are propagated and permeated much of human society. But have you noticed that taller people tend to be in leadership roles? Yeah. Even though there is no there is actually a negative correlation between height and intelligence beyond 5'6". So yet you tend to implicitly consider taller people as better leaders. So it's very, very hard to remove biases, and especially for people who are on the significant minority of society like i do know in india for example the the middle-sex people what are those technical word is hermaphrodite i don't know what the real world is people who are both men and women biologically right they're referred to as intersex but even that name is changing too. That name is changing too. Yeah. Okay. So always changing. And in India, they are a deeply marginalized community to the extent that they actually have a language of their own, a secret language of their own and communicate yet. They are always called paradoxically for weddings and childbirths and any auspicious occasions by at least one segment of society, but shunned otherwise. What does it do? You can actually read the tone of a paragraph. And the tone can give you clues on whether the person is a male or female, or intersex. Whether the person has or falls into the broad LGBTQ categorization, and you may not realize it, but subconsciously is acting on you. Yeah. Right? So biases are not that easy to remove. Very hard. See, to take this thought a bit further, there is a school of Indian thought, a very ancient thinking that goes back to the time that I come from Banaras, and a person who was particularly into all of this Kabir and Buddha, two people both from Banaras, I suppose both, or had a Bigfoot friend there. They both said that human beings are conditioned beings. We look at the world through the window of conditioning, when when I think of you, Kazi, I'm not having a direct perception of a person. There is an image that has formed of a friendly, warm friend with certain attributes. There is a realization that you come from the Indian subcontinent. So with it comes certain cultural presumptions of who you are. Indian subcontinent. So with it comes certain cultural presumptions of who you are. The belief that you work in biotech, that it still brings other historic inferences of what people in biotech are like, you know, people, people in biosciences are like, and so on and so forth. So that conditions are mine. So when I relate to you, I relate it in a way implicitly, different from the way, for example, that I'm relating to a soccer player even before I know what the soccer player is or I've had a deep conversation with the soccer player, right? So we are deeply conditioned beings. The biases, the things are extremely deep rooted. We know that beauty, for example, is a biological proxy for evolutionary biology. You probably know this more than other people, that if you go back and look at what is culturally considered beautiful universally, it is always correlated with a deep subconscious perception of health or some chances of survival in that ecosystem. Right? So why do so it's everywhere youth, right, has a greater chance of perpetuating your species. Is it any surprise that in cold countries where heads are covered, or you know, where there's not much sunlight, in warm countries, most people have black hair. But in cool countries, you can you can see a very interesting adaptation take place there. Younger people have blonde hair but as you age your hair turns brown right and then gray and eventually and so what is it it's a we find we always find the lighter colored hairs if you ask people given a job they'll always pick the job to the person with the blonder hair. Why? You may say, well, that is just wrong, and you can put a moral judgment on it. Or you could just acknowledge that this is built into our evolutionary biology, the need to survive, because that color is associated implicitly or correlated with youth is more likely to reproduce and continue the species right symmetric faces asymmetric faces are never never considered beautiful right or any face that is a deviation from the average so if you want to find in a culture what is considered the most beautiful face quite literally take a million faces from that culture and take the average of those faces and you will get the most what is considered the most beautiful in that culture it's a fact why because it the deviations generally show genetic defects or problems, medical issues, and evolutionary biology has programmed you in shunning those people. Have a very long face, bad luck. Have a very wide face, bad luck. Too long a nose, bad luck. Too short a nose, bad luck. It's a fact. So biases go to the fact that we are deeply conditioned human beings. And it is, unless we bring psychology into it and understanding of evolutionary biology into it, the way the technologies are dealing with biases, in my view, is too narrow and too short-sighted. You can't solve biases with more technology, right? You need far broader approach to root out biases from label data. Anyway, so that's a digression. Should we move past it, guys, or any other? Thank you. No, this is very helpful actually. So, all right. So we use the collator and then what do we do? We take label data, but not too much of label data. You know how many, how much label data did we take to fine tune? Look at this, only 3,668 rows, which is a drop in the ocean compared to the vast ocean that went to pre-training the transformer, isn't it? Bird-based encase was trained with billions of rows of data, hundreds of billions of rows of data. I don't know how many, hundreds of millions, hundreds of billions, but beyond a certain point, it all sounds like infinity, right? An ocean of data went into training these models. But it was mostly self-supervised learning means that that data was not labeled labeled data was sort of extracted through a trick by using the mass language model by something it was extracted through a trick and then trained whereas when you fine-tune you'd start with genuine honest to god label data doesn't it? You do that. And obviously, label data is expensive, so you don't get to have too many of those. 3,000, 4,000 is already pushing the limit. Sometimes you just have 100. So you fine-tune it. And when you fine-tune, you realize that just the fine-tuning with 3,000, 4,000 rows already takes time. It takes, well, on my machine, it took 36 seconds, right? On some of your machines, probably it took 10 seconds if you have more powerful GPUs, or in some other people's machines, it probably took somewhere between half an hour to an hour, right, if you are working on laptops or or one of those macs anybody has a number for mac m1 i think you would have that probably yeah 15 minutes yeah max at 15 minutes some of you were using windows laptops i noticed they were taking half an hour 40 minutes and so so on and so forth depending on the power of your laptop it took very so you can just the reason i'm bringing it up is, it should give you a sense that to train with just 3,000, 4,000 rows, it takes that long. So how much computing hardware does it take to pre-train this model with billions of rows of data? One second, guys, I will take this. So that brings up one of the topics I raised, the environmental cost of it. Think of reuse, as much as possible, reuse pre-trained models. Use the facility of hugging faces and other things within your own company also. Create your own private hub where whatever models you have fine-tuned, you can upload so that other teams in your company can further fine-tune it to their to some purpose right so try to share reuse models because these models come a tremendous carbon footprint and it brought me to the other topic use the code carbon or such libraries that keep you aware of the footprint carbon footprints that you're creating. That is quite a substantial footprint. The second thing we talked about, which we sort of today talked about also in great length, is these models contain all the biases inherent with the data. So when you use these models, remember that you are amplifying and perpetuating those stereotypes and biases. Recently, I heard a statement saying, and this is quite commonly heard, that writers are history. Now the chat GPT and these large language models will do all the writing. But can they? Can they do it? These are not things that are capable of true originality. These are rehashers. These are mechanical parrots. They look very intelligent. They can rephrase, paraphrase, synthesize. But synthesize from what? From the data, from the things that they have learned, from all the writing that has preceded 10,000 years of human history that has been encoded in writing. They're learning from that. So if you generate more and more content, if you let these models keep on generating content for you, what you will get are infinitely many variations of the same text applied to new circumstances and new prompts. What you will not get is the true originality of human thinking. Are we getting that? Right? So for example, let us take the hypothetical situation that theory of relativity is not yet discovered. We are in the world of Newtonian mechanics. All the knowledge with which you have trained the model is the Newtonian mechanics. Is it at all conceivable? Think about it for a moment. Is it at all conceivable that your large language models will ever stumble upon and discover the theory of relativity. The time and space-time are intertwined. The time dilates and space can length, strength, and so forth. They cannot. It could not possibly conceive of the idea that everything has an upper bound on the speed. Nothing can go faster than the speed of light. These are basic ideas we learn in high school physics right and yet a moment of reflection will convince you that there is no conceivable way that they can ever invent new knowledge like this right i would be happy to be proved wrong right they can adjust and they can fill adjacent holes of knowledge you know the interpolation engine so if there's a gap of knowledge, some little gap, which is waiting to be discovered and with general extension could be discovered. I believe that these models have become smart enough to do that, but they are not capable of truly revolutionary ideas. But simply because they have learned from a body of knowledge and they are drawing all inferences from it see if all i have met let us say if all i have met are four-legged animals every i would describe every activity and every ecosystem in terms of four-legged animals. I would not be able to conceive of an ecosystem of birds and fishes and dolphins, because that would be a huge statistical error. The probability of even conceiving of that would be such a small, vanishingly small probability that these large language models, to the extent that their purpose is to just emit the next most probable word, is by its very definition definition procured from originality true originality do you see that guys because at the end of the day they're just machines that use a softmax function to predict the next word yes most likely word and so they cannot possibly come up with something completely out of the blue likely would. And so they cannot possibly come up with something completely out of the blue. I interrupted somebody on the Zoom. Somebody wanted to say something. Oh, no, it's, sorry, it's me. I was, I had a thought and then, you know, it didn't matter. But you know, Scale AI has a whole another division where they employ, like, I think these are experts at writing. So there's somebody who gave a talk, they were in film sociology or something like that. And they employed that person to write articles for them so that that could then be fed back into the machine and then we could train the fine tune that and then rate the feedback that we were getting. So there's another system going on. So I do agree. The other, I guess, frontier is going, finding these super duper experts. So I'll have to agree with you 100%. That's all I want to say yes go ahead so I said uh so the man the mask language model when we can be encoding um that's able to find the hidden connections but it's the soft master limited you know the soft neck ultimately at the end of it so think of a large language model like a chat end of it so think of a large language model like a chat gpt4 and charge what is it doing you give it a sequence of text you say answer this question or whatever what is what it is really doing is once it has remember encoder decoder once it has a representation of your input it has to decode so it has to just produce the first word right once it produced the first word now it says that given the context given the hidden representation given the first word, right? Once it produced the first word, now it says that given the context, given the hidden representation, given the first word that it emitted, putting these two together, forming a bigger context, what is the next most likely word? It's just a softmax, which says, I have a vocabulary of 30,000 words. So at each step, it produces a probability table next to each of the 30,000 words and then asks which word is the highest probability and just emits that word. So it's not in essence, as I was saying, it's a there's no direct translation for this in English from this language. Yeah. We can invent something based on how English was formed. So and then it would create like this word that is not necessarily part of the corpus, but because of the tokens, like let's say it understood how English words are formed from the tokens, maybe it could create something that's not a system. Yeah, see, that part is true. What Patrick is saying is that can these large language models not essentially create new words and say this is its meaning in effect that it could you could say create a completely new word which should mean something like this and it can do that because then it can construct sentences and embed it so that the context gives the same meaning to it right so that is easy but then you ask it to cook up a completely new world. That's not true creativity. That's not discovering a theory of relativity when everybody was caught up in Newtonian mechanics. So that level of creativity, yes, these models can have. What's the limiting factor there as a result of these soft marks? Or is it just because the models haven't reached that point? Many, many, Patrick. See, first is that despite all the bravaha around these models and the euphoria, fact remains that we are caught into the next, predict the next word trap. So predict the next word trap is just softmax with temperature. What it means is that your softmax, you get the probability and then you sample from it. In other words, you sample in such a way that you're more likely to pick the higher probability word, but you may end up picking the less probable word, which is why you notice that when you, and we will see what that means. That's coming. We are anticipating what I'm going to talk about. Oh, people are genuinely genuinely asking there has to be another way. Now the thing is it is very fascinating that human language was so predictable and we didn't know that these machines, these mechanical parrots can produce something so coherent Simply using a statistical measure, right? But at some point it hits a wall. And I believe I recently heard an interview from the OpenAI CEO who said that this current generation of ideas, the large language models, has pretty much reached the wall. Now we need new ideas. You can't go too much further beyond chat GPT and GPT-4. You can scale it up to GPT-5, but you're beginning to hit a wall. So anyway, we'll come to that. Is that, Asim, because is it like the transformer architecture that is a constraint in some ways and we need something else then? See, let's just say that transformer architecture is something very good, but it's a necessary piece. People are realizing it's more fundamental than we thought, but they're more worried about what is it that we are missing. Just like before, like in 2015, you could have said, right, my RNNs and LSTMs, they work, but not as well as we would have wished, something is missing. In hindsight today, you would say, ah, for example, attention was missing. So we are still not done with the missing pieces. We are still discovering the missing pieces. And the journey goes on. See, since you brought that up, it's a little hard to answer that because in science, everything has missing pieces. In fact, that is Godel's incompleteness theorem. The great mathematician and philosopher Godel, he laid a fundamental statement in the theory of knowledge. He said, all knowledge is inherently incomplete. He was trying to solve the Russell paradox, Bertrand Russell through a paradox. He says that if a barber cuts everyone's hair, who is not a barber, who doesn't cut his own hair, then there is a contradiction in the village who cuts the barber's hair. here, then there is a contradiction in the village. Who cuts the barbers here? Right? So, and people have puzzled over it for a long time and then till Gödel came up with the system, he showed that any system of knowledge, axiomatic system of knowledge you create, will be inherently incomplete. And that was a profound statement to make because if you look back historically people have always dreamt of creating answering every questions every question if you look at mythologies right there is an answer to everything why why is there sun why is the why why do we have days and nights oh because the fairies are angels are carrying the sun around, right? And then the sun goes to rest. And so every question could have been answered and you couldn't contradict it. The two problems with those systems were, and you don't have to go far back. If you look at even more recent things, the 20th century philosophy is like, a psychology theory is like the Jungian philosophy and the Freudian philosophy, Freudian psychology and the Jungian psychology. Today, we consider them more or less nonsense. Freud did make fundamental contributions to psychology. He was a great thinker, but his theory ultimately turned out to be wrong. One of the problems with the Freudian psychology was every question could be answered. Like, why is a person having this behavior? Oh, there is an answer in Freudian psychology was every question could be answered. Like why is a person having this behavior? Oh, there is an answer in Freudian psychology. The problem with it is you could answer it. It was a complete system. Every question had an answer. And the problem was it was completely unfalsifiable, right? For example, if um why are you why is your neighbor not as happy as you today right and he has a glum face and he was rather angry at you when you went for a walk you can say well it has to do with his childhood issues right and how he was treated by his parents the trouble is how will you disprove that? How will you falsify that statement? So there's another statement, the Popper's falsifiability criteria, which says that scientific knowledge must be falsifiable. If you say that all things fall down, it's easy to verify. Just go and find a thing that doesn't fall down, but naturally just levitates. You have disproved gravity, isn't it? Asim, do you think this might be because humans are visual creatures? Because if you look at, I think DeepMind created what was called dream art, like even created things that even humans can't think about. I mean, most people dream and when people dream you, inside your head it's pixelated and it's vivid it images you know so I'm thinking I'm wondering if the reason languages are expressed differently and wrote differently is because um that's how people have been taught to communicate with each other but we're naturally naturally um visual creatures I it's a plausible hypothesis uh Charlie I don't uh I don't necessarily think this goes back to old questions that the philosopher Hume used to ask right that see we tend to as human beings have an idea we can for think of a fairy. All we need to do is put wings in the back of a person. Imagine, and you have imagined a new fantastic beast called a fairy, right? Or a Pegasus or something like that, things like that, a winged horse. But we also know that if you really look around the world, very unlikely that you'll meet a fairy today, right? So empirically, you don't find those things. The mind can construct out of components. And if you look at that thing, I'm aware of that work. What it is doing is it is creating alternate universe of things by composing things that you don't see composed together in the real world right so to that extent yes it is creative but anyway to the point that i suppose i'm making is that i think i lost the thread of thought but basically where i was going to is these large language models are missing some pieces, but it is the nature of scientific pursuit or scientific journey that everything is missing pieces. That is what Gödel's incompleteness theorem is. In fact, systems that are complete are nonsense. For example, religious systems, each religion is absolutely sure it is right. Have you ever found a believer who is not sure that all the answers are there in their sacred book right hindus are sure all things are answered in the vedas christians are sure it's all answered in the bible muslims are sure it's all there in the quran right and i'm sure that uh if we talk to i don't know, horses, they will tell you that all questions are answered in that grassland. You just have to look at it more carefully. So systems that answer all questions are complete systems. And generally, they impede progress. You know that. All countries, the moment they get heavy with these mythologies and religions, progress stops. The dark ages in Europe were the ages of belief. The enlightenment started with doubt, with the scientific journey. Scientific journey inherently says everything we know is wrong. But what we know today is less wrong than what we knew yesterday. What we believed in yesterday. And everything is incomplete. So to that extent, computer science, AI, is also an incomplete field. Today we discover transformers. It's one piece of the puzzle, the attention, but we don't have all pieces of the puzzle. In fact, I dread the day where people begin to claim that we have all pieces of the puzzle, because then it won't remain a scientific pursuit. It will become yet another religion or dogma. So we don't want to go there we want to forever feel we don't know and forever be on the scientific journey exploratory learning youth isn't it fun to always be discovering new things than to already have our head filled with answers to every question already have our head filled with answers to every question. So that's sort of a more big answer. Sorry for the philosophical tangent. So now coming to things of today. We will talk about the world of, in some sense, composition, composing. We know what large language models can do, right? They are, they also, and today in particular, we will end up focusing on generative models, right? That given some input, produce some text as output for now. And what I would like to show you are the possibilities. So you ask this question. Suppose I were to ask a large language model a question. What is the height of Mount Everest? It might have learned that fact. It may be somewhere there. And even though it hasn't exactly memorized it, because it is predicting the most likely word, it might, and this is the most fascinating thing, even though it hasn't memorized the height of Mount Everest, it has nonetheless, its weights and biases have taught it what sequence of words to produce, so that the sentence that comes out ends up answering this question, and somewhere contains the word that it is, what is it, 8,800, 8,848 feet or 48 meters or something like that. It ends up showing it. But then suppose I were to ask you a slightly different question. I say, what is the height of Mount Everest today? Now, this large language model is foxed, because the first thing it says that, oh, gosh, I know data only till 2022. But today, what's the difference spin on it? Isn't it? There's temporal information that you need today. By the way, in case you're wondering that the height of mount everest keeps changing every day what happens to it anyone knows it keeps growing no it keeps growing because the in the indian subcontinent plate is still banging against the asian plate and the whole thing is rising up right inch every year thank you there we go so if you want to know what is the height of Mount Everest today now what do you have to do you have to the this thing has to go find out the latest information somewhere and well you could go to Google and you can get the latest information. But when you do get, you might end up getting that information in feet. Because in US, we tend to whether much of the world has moved to the standard units. Here, we still swear by the imperial system, the Brits, right? Even the Brits have moved to standard units but we we use the imperial system of feats and pounds right and miles so it will tell you that it is so many miles high or so many feet high you may say well no no tell me the way the rest of the world understands it the scientific community understands it give it to me in standard units now there is a problem because just a direct statement, what is the height of Mount Everest today? Just height of Mount Everest, it would, the probability engine will end up producing the answer in feet, right? Or it could say, I can go search the web and it may search the web and Google may answer it in feet and say, well, I'm telling you, today the height is this much. Then you have a problem. You need to convert that feet into meters. So think about it. You need three pieces in the journey. You need to Google up the latest information, right? You need to then apply a math engine to convert it to meters from feet to meters. And then suppose I tell you if an airplane needs to fly one kilometer above the mountains, minimum one kilometers above the mountains, how high does the airplane need to be when flying over Mount Everest? So you know that the answer is there. It is there. But you need to build it out of pieces. You need to Google up the height of Mount Everest today. You need to convert it to meters. You need to add a kilometer to the answer and finally you'll know how high the airplane goes i said you've just told us what prompt engineering is and what prompt yeah this is it yeah this is it and we will learn that today so today is the beginning of a new thing that in natural language processing and in broadly in ai is the dominant activity. It is about language chains and prompt engineering. How do you put things together, many pieces together and do a task? The closest thing it reminds me of very naturally, I work on Linux systems. On Linux, what do we call these things? A similar thing when you stitch little pieces together to do something bigger. Pipeline. Yeah, pipeline. You create a pipeline by using the pipe command. And it is surprising how in one line you can accomplish what entire softwares on Windows do or Mac do. So the same, if you're used to pipeline and hugging faces pipeline language chains are just language pipelines right and we will learn about that and one of the things we will learn along the way is this um you know have you heard the story of Alibaba and 40 thieves Ali Baba and 40 thieves. Right? So the way the story goes, it's part of the Arabian Nights, or in the East you call it the Alif Laila. He discovers a cave in which some thieves go and stand in front. There's a big rock, and the thieves have to do a special intonation. They have to say, in English, you would say, open sesame. And in India, they say, or something like that. And then the big rock moves, and you can go inside the cave. So these language models are like caves with a door to them. You need to have exactly the right intonation for the door to open and for it to give you the jewels inside. The thieves in that story stored all the jewels inside the cave. So these large language models, because of their weight, strained weights. They're capable of much, but you need to knock on their doors with the right invocation, the right mantra. And so the art of creating the right mantra, the right of the right, the art of the right invocation has now developed a name. It is called prompt engineering. Right? has now developed a name. It is called prompt engineering. Now, a word of caution on prompt engineering. Prompt engineering is able to accomplish much. But today, at the end of the day, you will be fairly amazed at what prompt engineering can do. And so you might begin to again wonder that is programming needed anymore or is what happens to other things? Let us hold that thought. We'll go over it in a little bit. But first comes basic setup. We will use landshed, which is the dominant at this moment, the most popular, even though it is at 0.146 version or something like that. 0.146 version or something like that, right? A very modest number, speaking to how bleeding edge it is. It has still become the dominant tool used. And somebody just mentioned, I think Chris Nguyen just noticed that, and I think, Shalini, I think you pointed out, Patrick, I think Patrick, you pointed out Chris Nguyen's article which said that she looked at the top 10 most trending projects on GitHub, and they were all generative AI, and many of them were just direct users of land chain, creative users of land chain. So it should give you a sense of how vital is it for you to learn Langchain at this moment. By the way, none of these things existed the last time I was teaching this course. These are all very bleeding edge, very recent, very influential. So we will deal with that now. So what do we do? We'll set up our environment. now so what do we do we'll set up our environment so guys we'll take a few examples for this example today make sure that you have installed lanche pip install lanche if not uncomment this line and run it make sure you run it and also install google search results because we will take this example of the height of Mount Everest today and how high should aeroplanes fly over it today, right? So, by the way, it's a concocted example, so don't read too much into it. You can counter argue that no pilot in his right mind ever measures his heights down to the inches, right? But we will, for the sake of pedagogy, we'll assume that it is needed. Here we go. So we need some imports. These are your imports. And I tend not to use print. I mean, I use print, but sometimes it's a little neater to produce your output in a good HTML format. So I use display HTML. Fuck shit. neater to produce your output in a good HTML format. So I use display HTML. But should so once we have done that, you need two things. And this is part of it. This is where you start paying your pound of flesh to these companies now, commercial companies. OpenAI requires you to get an API key. You must go get it. So first task, you will go register there, in this break we'll take, you'll go register with OpenAI and get yourself a key. You'll also get yourself a key from the SERP API site for searching. And these are things that people use quite commonly. So do get those keys and then set your environment variables. One thing that I noticed a lot of Jupyter notebooks online or the tutorials, they hardwired the key in a cell, in aupiter cell never do that why because you will share your notebook with your colleagues who will share it with other colleagues who'll share it with someone else and before you know it the whole company is raking up bills on your behalf and especially if you're paying it from your pocket, the last thing you need to do is be surprised every month with a large bill. So my recommendation is on your workstation, which hopefully you need to log into, save it as an environment variable. How do you do it? Once again, there are ways of doing it. Here I've given the simplest and relatively less secure way. Put it in your .bashrc or zsh or whatever file, rc file, and do export. These are the commands that will put that. On Windows, you have to do a little bit more ceremony, as always. You have to go to something called the control panel and from there navigate through a whole set of menus and then do an environment variable setup. I assume if you're using Windows, you're competent enough or you'll have to Google it up. But one way or the other, let's go and do it. But before I go to the break, I'll show you an example. Once you get the key, we can instantiate an open AI object. When you instantiate an open AI object, remember guys, you are not instantiating it locally. You're not instantiating a large language model locally. That point is worth noting. That language model is remote. They don't give you the ability to download it. The presumption or the idea is it's too big to be downloaded. And now, of course, they don't want you to download it because they want to make tons of money on it. They'll give you all sorts of reasons, but the fact is you can't. They just give you an API connection to it. When you take the API connection, set a variable called temperature. Now, the way the temperature is, if you set the temperature low, you will get a little bit, when you invoke it once and you invoke it again, the results will look somewhat similar. When you set the temperature high, temperature goes from a scale of zero to one. When you set the temperature high, temperature goes from a scale of 0 to 1. When you set the temperature, actually it goes even higher, but okay, we'll take it 0 to 1. If you set it high, let's say you set it at 5 or something like that, you can set it high. But when you, the higher number you take, the more variation you'll see in the output when you try to rerun the same code. We do want variations. So we instantiate an OpenAI object. And then what we do, it is colloquially said that you have a large language model. Remember you don't actually have it, it's out there. You have a proxy for it. But we'll still use the word we have a large language model given the llm right so why by the way just as a side what what is this whole large language model thing transformers are language models remember this they encode as decoders large just means that it's bragging rights it's lots and lots of weights and biases there, lots of parameters. That's all it is. So you take a large language model and you give it an input, which is, what is input? Which is a sequence of, it's a sequence of words. It's token, it's a sentence. Give it an input and you expect it to produce an output. Remember, these are in the generative mode. You're not doing token is a sentence give it an input and you expect it to produce an output remember these are in the generative mode you're not doing classification you're not doing anything else you're just doing you are finding mass words you are actually in a generative mode right you are saying produce some output based on this question answer this question what is a good name for acute mobius strips? You guys remember what a mobius strip is? Yeah, the one that does not have a surface. The strip of paper you cut and twist and connect it. That's right. It has only one surface, no inside, outside. In other words, it's a non-orientable surface. So it comes up with the answer twisted 12 fairly reasonable isn't it then next time you invoke it again it comes up with a twisty loop do you see that it's coming up with different answers and you can run it on your own in a bit in the break and you will get some sense of where it goes right so actually it's been an hour and a half since me i was talking i'll take a 15 minute break guys and do this setup and reach this place run it to this level getting your keys setting up your environment variables will take time let's make progress together after that because then we'll venture into more interesting territory i tried it i'm getting a rate limit oh make sure that you have entered your credit card remember they need your pound of flesh get a paid account yeah so guys uh ask for this uh say that again please how much of the charge for this you'll find out at the end of the month so like i have an open ai account. Yes. So what do I do? Click there. Go to billing and go into a paid account. Set up a billing account, paid account. And when you do that, at this moment, you know, for our simple task, you'll probably rack up maybe by the end of the day, some 50 cents. I can't find. If you have that link to the billing account, can you please share? Can one of you please do that? Maybe Patrick, could you please do that? I need to take a quick break. But if you could please do I need to take a call. So Patrick, would you please share your screen? Oh, I said. Yeah, and show it to. Okay, so it's the upgrade, right? And I'll pause the recording all right actually one thing though i shouldn't i don't want to run it on this particular machine for a reason but okay so you say that who is the author of the book we create a prompt a prompt is a template with a placeholder, with a fill in the blank. And those fill in the blanks or those placeholders identifies as input variables that you substitute. It is very similar to Python's way of doing formatted strings, right? Except that the object here is called a prompt template. And people, even though the class is called prompt template in langchain library people more colloquially just call it a prompt so a prompt can refer to either a hardwired text or it can refer to a whole you know a template with fill in the blanks and input variables. Generally, it refers to the latter in most situations. So now, if you want to get the text, all you need to do is fill in the blank, isn't it? Convert a template into a text. So here we say text is prom.format book is equal to a snail of two cities, just as you would do for a string.format. You can print it. It will print a snail of two cities. I have not run it, but I trust you can run it. Why didn't I run it? I must be doing something. And then finally, this text is what we pass to the large language model, isn't it? Input is what you pass. Remember, at the end of the day, a large language model expects a text, a sequence of words, a sentence as input. Text as input. So when you do that, it will print you an output. It says the author of a snail of two cities is, and it will put out some absolutely random name. I find this amusing. know you know a characteristic of ingenious is if you ever ask an engineer can you do this his answer is yes and then he'll figure out okay what was i supposed to do right our answer is always yes so very much like that our creation too has the habit that if you ask it a question, it always has an answer. Unless you explicitly say, if you don't know, say, I don't know. So I give you this experiment. To the prompt, suffix the sentence. If you don't know, say, I don't know. And then run it again. Tell me what do you get. and then run it again. Tell me what do you get? I won't run it here because I'm not sure on this Windows machine it will run properly. These have all been tried out on Linux. So let me see. If I could connect to my Linux. For some reason, I'm not able to connect to my Linux. There's some networking issue, but we'll come to that. Okay, guys, did we all get outputs? Are you getting outputs on your machines? Anybody could tell me what happens if you add the... Yeah, if you don't know say don't know it just puts that word about him yeah no but what does it say about the author it gave some dummy name but then i rephrase the prompt to say if if you don't know, say don't know. So it just says, if you don't know, say don't know. Oh. It just verbatim gives back, if you don't know, say don't know. Yeah. So then you notice, guys, that this is a simple case in which what we did is we took a prompt template, gave it an input, name of a book to produce the text. Then we carried the text over to the large language model, fed it in to get the output, isn't it? So we are carrying things, feeding things to one component, taking its output, feeding it to the next component, taking its output, right? Now, suppose you didn't have to do that. Suppose you could just give the name of the book and out would come the author. You can do that by concatenating these two components together, the prompt and the large language model. And when you concatenate them together into a chain, you have created your first language chain. Are we together? And the way to do that is very easy. Nothing mysterious, or you instantiate a large language chain, large language model chain. When you do that, the two absolutely well, two ingredients that you want to give is what was the large language model? Well, the open AI model that you want to give is what was the large language model? Well, the open AI model that you just instantiated and prompt, the prompt template that you just created. And then what was the prompt expecting? What was the input to the prompt? The name of the book. What was the output of the large language model? The author. So this time, the author of the book turns out to be Pamela Duncan Edwards. So this time the author of the book turns out to be Pamela Duncan Edwards. For you now it says I don't know. Yeah, good. Very good. As if question. So like let's say for example, if I want to use the large language model for summarization. And then, but I also wanted to like then link it with uh you know like something like a semantic search where it goes and you know um finds like bunch of um you know we'll do all of that of course but another day okay all of that is there but you will already see how to do it by the time today we end today But you will already see how to do it by the time we end today. So guys, that is a chain. So do we understand what a prompt is and what a chain is? Is the idea very simple, guys? Maybe another question then. So like what I'm understanding is with the prompts, like for example, in pharma, we have, you know, for example, you want to do some sort of summarization, but for medical notes and medical, you know, journals and texts and et cetera, right? And then on the call center side, you know, they want to summarize more in terms of the, you know, the notes captured in the call, right? And so I could have like almost in some ways different prompts, right? For each of those kind of use cases. Yes, you do. In fact, Kajit, that brings up the whole thing. Prompt engineering has become vital because you can give enough clues and enough examples so that it speaks the language, the former language, or it speaks the call center language. Yeah, tribal dialects, right? It learns to speak those tribal dialects this is interesting yeah thank you yeah we'll do that it's not as uh we are not too far from it let's say that it's likely to be our next lab next day's So Abhijit, do you have a question? Yeah, but just the chain part and the blank part . Yeah. See, we know what a prompt is. Prompt is like, it's just a fancy way of saying, it's a fill in the blank string, just like Python. Then it will take some placeholder input variables. And when you do that, it produces a string, just like Python string.format that has some input variables and when you do that it produces a string just like python string dot format that has some input variables baked in and so that's what a prompt is now suppose you want to do this again and again you have a list of hundred books and titles and you want to know their authors so what do you want your statement is the same right Your statement is the same, right? Prompt template is the same. Who is the author of the book? Blah, book X. And then you give it the name. So it's a good candidate to create a prompt template. You do that. But what do you do? You have a prompt template, you have a large language model. You have to feed in the input, the book title, get the full string and feed that to the large language model. Because large language model will only take a string, right? And that string that will come out, the actual input that will go in will produce the author. Isn't it? So now you see that you are the one mediating between these components. So what a chain does is it says, you know, this is a no brainer. All I need to do is whatever this guy produces, feed it to this guy. Isn't it? That is exactly what a Unix pipe command does. Exactly that. So language chain is that. It will say, give me the components and I will take the input of one and feed it to the, I mean, the output of one and feed it to the input of the next guy. And you can go on concatenating many pieces together if you want. In fact, you can build language chain out of language chains. You can have a certain language chain and treat it as a component in a bigger language chain. And you can go on doing that. Right? And that's where the whole fun comes in. And you'll start seeing it in a moment. So what I do is, how do I create a language chain here? Just you create a language chain out of the prompt in the large language model. And when you do it, the input goes to the prompt. So what was the input? The name of the book. You could have, by the way, say the book is equal to a snail of two cities, right? But yeah. And then you say print the author and it will give you an answer. Abhijit, there's that answer equation. You see the value, right? It saved you the burden of being the guy mediating between the components. You don't need to be. You just chain them. And to you, the whole thing acts as one component. In goes title of the book. Out goes the author of the book. That's it. And that's a chain. That's a basic chain. And now we come to something where the power of these chains come from. It is the concept of an agent right and the word agent is very interesting the word agent permeates a lot of ai artificial intelligence and etc etc agents implicitly are things that are capable of doing something smartly. That's one way of looking at it. Anyway, what the agent does is, suppose you ask this question about the mountains. What is the height of the Himalayas today or something like that? It will say, I need to search. Suppose an agent has access to tools and a language model. The agent is the guy who takes your prompt, interprets the prompt, and figures out which of the tools, first which tool to use, then which tool to use, then finally which language model to feed it into. Are we together? And some of the tools themselves may be using a language model inside them. So I'll take an example. We will, for reasons that will become obvious when you look at the question. So here's the question. What is the height of Mount Everest in meters today? Right. If an aeroplane has to travel one kilometer higher than that number, how high should it be? Right. So do you really? So now there is something interesting. I interjected today. Right. Board interjected today. Right? Boarding in progress. Because I interjected today, it means that it will have to do some Google search. Right? So to answer this question, the agent must have access to something that can do Google search. Do you agree? Right? So we do that. Then also it has to do some bit of arithmetic so it turns out that yet another tool to do arithmetic is itself a large language model these large language models these days are able to do some basic arithmetic so now you need a searcher, you need a math expert. You put these together and you use them with the large language model. So you feed the searcher's input to the large, and you will see the sequence what happens. So let's see what happens. We created a few tool names. Now these tools are pre-existing. Let's see what happens. We created a few tool names. Now these tools are preexisting. It turns out that LanChain comes with a whole list of built-in tools, right? You use those tools, pick two of them, the search and the large, the math tools. Then you instantiate a tools object. This is a syntax for it. You give it the name of the tools and you also tell which large language model it needs. Why do you need to give it a large language model? Because one of the tools, LLM Mac, can't work without a language model in itself, right? It needs a large. Not all tools need a large language model. For example, the SERP API doesn't need a large language model. It will just go search on the internet right but this is it and then you create an agent so what will i agent me it needs tools to work with it needs a large language model to work with isn't it and then you instantiate agents of a certain kind and we'll come go deeper into it next time but at this moment just assume that there are different types of agent and zero short react description is one good one it's a good default one to pick for many situations for us agents already so you said there have been trends are they somehow like retrained or understand the specific yeah context yeah yeah it's like one is a nurse and one is a doctor one is a one is a carpenter kind of thing right so to the carpenter you need to still give the drill bits and the tools isn't it that's the thing they need to give give them the tool give them a large language model and then see what happens so guys is this code looking pretty self-evident so far? Let's see what happens. We give it a question. At this moment, I'm not even using a prompt template. I'm giving it a direct text. So I'm asking this agent to answer this question. See how it thinks. When you run it, look at the output. It says entering a new agent execution chain, right it says i need to find out the height of the mountain first how can it find the height of the mountain it needs to search so action input height of my and it knows what to search for height of mount everest so it has taken your text and from that extracted the relevant information. I don't know if you're impressed. This is pretty impressive, isn't it? It does that, and it observes that it's 29,000 and 32 feet. So it says that it has a thought. It says, Okay, now I need to convert it into meters. Right? So the action, how will it convert it into the meter? It needs an action. What is that action? Calculator. So calculator will, it needs another tool for every action. For search, it used the SERP API. For calculator, it needs to use the LLM math tool. But then it says action input 29 blah blah blah two meter asif have you heard of the fetch tool no i haven't tell me a bit about it so i know you're talking about we're discussing how about changing so you mentioned the search tool and we mentioned the calculator um but there's something called the fetch tool, which basically fetches, if it fetches different websites, it kind of squishes and summarizes everything that comes from the website. So that would be interesting for people to know. Oh, yeah, it would be. Actually, Shalini, why don't you do it? Would you volunteer creating a small Jupyter notebook, illustrating the fetch tool and just share it on the slack just create the smallest little jupiter notebook that will help everyone learn about it okay after lunch yes after lunch yes can i go to lunch right away no oh you're kidding oh two hour lunch it's a two hour lunch no do it charlie no pressure do it whenever you can some people here wanted to see it after lunch but they're excited about your uh suggestion so i just repeated it to you but do it whenever you can that's it in this case because the we said that the agent the tools it's it's understanding is based on the tools we need yeah figure out which of the tools it should use exactly so like if it's a carpenter that you gave a hammer so every problem you get to it looks like it will go nail yeah it will just go hammer at everything yeah so now we have a right? So now it is taking the input to get the answer in meters, and then it has a thought. Now I need to add another kilometer to it. And again, what does it need to add kilometers to it? A calculator. It goes, gets the calculator, and then it finds the answer. Observation answer is this. Now it thinks, I now know the final answer. So it says, the plane should be these many meters high. So do you see how it builds, how the magic is in the agent guys? Isn't it? The tools, go ahead. How does it like, so how did it say like action observation thought where did that come from that's what it does it breaks everything down to action observation and thoughts interesting hey so so in this thing uh how does it know which actions are supportable by which tools are supportable by which tools and also in this in the definition of the tools you had you use the LLM for the LLM map tool and again use the same LLM later in the description of the agent that's right can those LLMs be different or do you have to actually it can be very different you asked a very good question Dilip see at this moment i'm using the open ai frankly i would rather not but it's the only one that at this moment is giving good performance people are trying to replace it with other ones so for example the google has one and in the open source there is one the google one is the what is that x5 and bard and so forth and then the in the open community there's a celebras and so forth so one of the homeworks that you have for today is to replace the open ai one with the open source one or and the google one so there are a few llms contributed to the hugging face hub now please substitute it and see what kind of results you get. The big problem that we are facing is ever since OpenAI decided not to release any details about their models, everybody is in a panic. The whole world is trying to now come up with equivalently good models. Just in the last few weeks, there has been huge strikes. People have open sourced vast amounts of data to train these models. And there are a lot of models under training that I think in the next few weeks or a month or two will begin to catch up. And they will be in the open source. Open source community is taking it very, very hard that they don't have anything comparable. and there's a lot of activity here so uh one quick quick thing though the uh in the for the llm math if you used a little model that model can be different from the one that is used for the absolutely okay no doubt no doubt and then and then this now uh the question about the actions how to know which tools support which actions which tools support which action see what happens is that these tools uh are declarative and we'll come to that. We will create a custom tool and you'll see what it does. Right? How does it figure out? Okay. Yeah. Here is the thing. I'm deliberately not answering all the questions because I'm going to unpack. There is a lot happening in this paragraph, a lot. Right? At this moment, when you see it reasoned like this, it's almost uncanny it seems to be thinking like a human being isn't it yeah yeah there is a lot happening here and i'll gradually over that's why i extended this course by two more days we will be gradually unpacking all of this so making the mysterious the obvious gradually right because this is absolutely marvelous stuff that's been happening we'll do that so guys we'll end this lab with the realize with some observations note that we force the agent to use the search tool since we implied that the height of everest keeps changing and we need to know the height today the search tool returned the height in imperial units so there was a need to first convert it to the search tool returned the height in imperial units so there was a need to first convert it to si units which means you use the calculator right even before you use i mean even before you do anything finally the math tool has to add a thousand to it right so this was a very simple example. We are going to do funnier stuff, more interesting stuff in the next lab. So try this. Can you put constraints on this as well? Like for example, can you ask it to round it up because a pilot may get busy with the decimals, right? So yeah. Yeah. You literally anticipated the next lab that we are getting into. Literally the next lab, this is lab 16, lab 17. Lab 18 is exactly that. 17 and 18 are that. We're getting there in a moment. But very pertinent questions. So guys, I want you to do a little homework before we proceed, or do it in the lunchtime. What happens if I remove the word today get away with it. And then explain why it is. So that I'll give you three, four minutes for all of you to try. When you look at this, you know, this reasoning process, and you see the chain in action, it finally, you know, this is what got people, their imaginations absolutely fired up. The whole world has begun to talk about, you know, the emergent AI, artificial general intelligence and so forth. There's a lot of that happening here, but so we'll unpack it. See, it is certainly very impressive. We are in the next generation of AI, no doubt. I don't think it is AGI in my opinion. We are far, far, far from it. But it is certainly, we have far, far, far from it, but it's certainly we have crossed, we have crossed into the next generation. So, so Asif, if you don't give today, is the action gonna be look up my own database? No, no, no, try it. Sir? Dilip, don't hypothesize, try it. No, actually I haven't set up the API, sorry. Oh, no, you're being lazy, so set it up. I won't do this. Anybody who has run this can answer this question. What happens? Are you having fun, guys? So the important thing is, guys, to see carefully how it reasons. Because much of prompt engineering is becoming familiar with the way these things reason. And it helps you create very intelligent prompts then. Literally, the next thing we will do is prompt engineering but i i will wait for at least one person to answer this by running it absolutely very very very well it does in fact that's one of the labs one of the things we will do is i will take some fictitious of patient histories and we will interview the machine to answer certain things no no we'll use that open source thing right what is that thing called medical example but you said the meaning of this i have is even going forward. So some of the CSU friends are . I know last time there's a question in the chat. Oh, there's some question in the chat oh there's some question in the chat hang on um oh yes in fact uh jeanette you are absolutely right it is very it is an orchestrator of sorts that's what it is but in the ai language like see in server side, or in systems engineering, you talk about orchestrators, like, like, you would think about Jenkins and things like that. In the AI language, those things are typically called agents. In the literature of AI, they're typically called agents. But it's the same thing. There is any other. Oh, guys, don't put your open API keys in your code somebody has given um open temperature is blah open api keys blah most we don't ever because what will happen is it will stay in your notebook and your notebook will get shared and before you know it the whole world will be using your account what's the code to pull it from the environment you don't need to just do export and it is and the Jupyter Notebook will always be aware of all the environmental errors. I mean in the actual like launching public how do I pass the API from the environment? Why do you need to? I saved it in batch already. That's it then restart Jupyter that's it then restart jupiter once you export you have to restart yeah restart it you have to export and then source the bash rc file and then So remember that from the shell you must source that bash rc or just open a new shell open a new shell and restart Jupyter. All right guys anybody enlighten me what do you get? Ajay did you run this? Ajay, did you run this? Today, right? If you remember today, that's what happened. Yeah. It's the same for me. No. It should not go searching. Okay, I commented on a lot of things. I can run it. Let me just rerun it with whatever it is. It didn't rerun for me. Go ahead, Shalini. What happened? It didn't rerun for me but i guess everybody else is working but i think i know why you should ask put this question as homework um because for today we haven't specified a date or time so it doesn't know what today is so it doesn't know what today is. Yeah, so basically it's forced to realize this temporal information whereas if you don't put it, it will assume it will just try to predict the value from itself without actually searching. Because you can just ask this question from a large language model to chat GPT and say what's the height of Mount Everest, it will tell you. cat GPT and say what's the height of Mount Everest it will tell you. So the point of today was to force it to use the search engine. Alright guys, so I'll take you to the next one prompts and so welcome to the world of prompt engineering this is a very important one guys So guys, I must mention one thing. People are, are we done? Okay, I'll wait for some people to finish. Patrick, let me know when you're done. In the meanwhile, those of you who are remote can ask me some questions if you want. Okay, I'm starting guys Patrick, pay attention now. So, large language chains are prompts, we are going to talk about prompts. And this is the beginning of prompt engineering, learning to engineer good prompts, guys. Now, before I do that, there is an interesting debate that's happening in the community. People are saying that because we can ask things like chat GPT a question, basically, in our language, give it a prompt and it will produce the answer. Right. Why do we need programming anymore right it will do that we don't need to program things let it do that and the answer has to do with the fact that see basically programming within certain parameters is always deterministic you follow a hardwired set of steps and it produces an expected result. Whereas these things are stochastic, these are probability machines, large language models. What they do is every time they start at a given input, they start somewhere. And then they take you to an output. And then what happens is that they produce a softmax function, but they never go and pick the highest probability word. We think that softmax, if you're generating text, pick the highest probability word. There's actually a little twist. What it does is it samples the vocabulary, giving higher weightage to the higher probability words. But when you sample, you're not guaranteed to hit that word. You can hit other words. Is that because of the temperature? No, no. Just first take this as a fact that it samples. Now what happens is that if the probability of a particular word is far higher than others much much higher so the probability of its getting sampled is much higher so if you resample you'll again get the same word in the next iteration like it not always get it but far more likely to get the same word but if that, the highest priority is not so high compared to the rest, the other pillars, the words also have relatively comparable probabilities, a little bit less, then sampling is likely to stumble upon those words. So for example, the probability that your word is 0.2 and the next highest word is 0.19 and the next is 0.17. What does it do to which word will get picked? To the extent that you're picking words in generative probabilistically randomly, you're sampling. You're not directly just saying, I will always pick the highest probability word. You're sampling from that. So with if the if the probability distribution is more flattened, you're likely to pick other words the next time. Am I making sense, guys? And what controls the flattening of the probability distribution temperature? Last time we did distal but for a reason, one reason that i focused on this software temperature concept was soft max temperature concept was because that is precisely what is used the the soft max temperature is used to first flatten it out and then sample from it right and that therefore the temperature hypereter controls the degree to which the the there is variation in the outputs. Are we together, guys? Am I making sense? That is it. And that is why that concept of temperature is very important. And because it's in the softmax and softmax is the tail end of your large language model, therefore it is in the large language model. And this is perhaps the most commonly used hyperparameter. So it's something to know, because most people don't know what in the world is this temperature? Are we boiling the bottle? Something. So this is it. Are we together? And just for your recollection, I just put the expression for softmax. You all are familiar with it. It is just scaling the values by a temperature T. Now, if you remember, we handed this exercise last time. You can take a temperature of 1. We can take a temperature of 2, 3, 10. We took a temperature of 0.10. So here, actually, if you take temperature of 0, it doesn't make sense, but you can, because I think what they do is they offset it by 1. Like they add 1 to it. So when you say temperature 0, it means in the language of our last research paper of Jeffrey Hinton, it would be 0 plus 1, 1. Temperature 1 means there is no temperature effect, there is no flattening. If you make temperature 2, a temperature is equal to 0.95, then 1 plus 0.95 is 2, means there is substantial damping. You make it higher and higher, there's further right so that's what it is if you have prompt requests for an instruction let's say some to generate something random is it better to put it in the front or is that the temperature that's higher so you see random no random when you put the word generate something random it will automatically not affect the sampling random is just a token right it will just treat another word in the vocabulary but what you should do is put it in the temperature because then you can ensure that the next time you get something truly different now and we will see that in a moment right now so language the prompt template is our old familiar friend by now what does it have it has a string with a fill in the blank one or more fill in the blanks and then those blanks have to be filled with input variables isn't it so let's do some imports and now let's do a little bit of an interesting um prompt so i create a prompt like this. I say, I'm telling this thing that imagine that there is a story about cats. They like to drink milk and crawl in the neighborhood. How many of you have cats? Nobody cats okay write a short story where some cats show these behaviors right give the story a catchy humorous title write it in the format so guys do you notice that a lot is happening here isn't a lot is happening here? Isn't it? A lot is happening here. You're giving very precise instructions. So let's do it. We do that. And we give this prompt. At this moment, this prompt has no fill in the blanks. And it produces a story. And I'll let you ponder over the story and tell me whether it's good enough or not. Is the title humorous and catchy? Isn't it amazing how good these these things are no wonder writers are in panic right if if if this was a school story the probably it would have gotten an a grade no the other schools are running now they're zero catcher yeah Yeah, I heard that if you're really a stellar writer, it will mark your writing as GPT-based, Yeah. So what's going to happen to creative writers, Asad? Writers and programmers will become an extinct species gradually. So the saying is, but I don't think so. I think that the originality is needed. You know, you can't come up with the, see, you can't write Hemingway's The Old Man in the Sea using Chat gpt writing is not about flowery language that's part of it writing is about capturing and transmuting the core of the human condition the human experience into words and to the extent that you do that you need human experience you need the suffering you need human experience, you need the suffering, you need the angst, you need the joy, you need the sense of wonder, everything captured. And that can only come from new experiences. These machines are not getting new experiences. They're rehashing old experiences. Like the average writers. Yeah, the average writer is history there's no programming right like average programmers and yeah because the average writer is very uncreative it's just rehash yeah now go ahead perine your question was no this mostly generalizes yeah it's a generalized issue absolutely like Praveen mentioned, it only generalizes. And that's a very interesting use of words because machine learning, the word learning is synonymous with the ability to generalize. That's what learning is. So these things are learning to generalize from what it knows. And so it stays within the sphere of the known. It's interpolating in there. Okay. So if I have a question, this is not philosophical, but this word markdown, my understanding is it goes from text to, sorry, text to HTML, but people say you can do more sophisticated things with it in lang chain is is there i've been looking for information regarding that would you uh is is can you please tell me more about that i mean convert ordinary text into markdown so the mark, so a lot of these tools are import markdown. So the import markdown, so a lot of land chain agents are starting to import markdown. So I just don't get what they're going to do with this markdown. That's what I'm doing. See, what am I doing? I'm displaying this in a beautiful way, right? Look at this. I use markdown. I did import markdown. Why did I do import markdown why did i do import markdown somewhere i did yeah here it is from ipython display import markdown html it is just to prettify the output there's nothing deep out here okay right so the thing is now because i used a fairly high temperature if you repeat the same thing, you notice that the story changes. Boozy felines, a milk drinking adventure. Right. And now what happens? What if you set the temperature to zero? Then you expect the output, the two stories that it produces to be a little bit more similar. They will not be identical, but a little bit more similar. I will let you, they will not be identical, but a little bit similar. Because remember, sampling is still going on. So look at this. Once upon a time, one warm summer night, there was a group of cats. Do you see a certain degree of similarity here? So this is it guys. So now, going back to the question that I was coming to, programming is deterministic. Remember that large language models are essentially stochastic engines, right? You cannot use it to get deterministic answers. And one article that we liked very much was Chris Nguyen. She pointed out that you ask these things, open chart GPT, for example, to grade an essay. One time it says, very good essay, seven out of 10. And at the time it gives you four out of 10. It says this, this, and this is bad about it. Right? Right. So it completely depends where you start from. And imagine a teacher like that, based on whether the teacher has had coffee or not, or some unknown parameters, you get either a A grade or you get a failing grade. Right? You don't want that, right? So those are the limitations of these things. So fundamentally creative programming is not dead but it's it's going to be a big assist tool so now we come to prompting guys see that was about cats but you say why favoritism towards cats what about dogs and squirrels so you can create a prompt exactly like this to write stories about animals after animals isn't it so when my two daughters were young it's a bit of a personal thing um every day they would so i had created an imaginary uh i mean a character called uh lalbander red monkey so every night and to them it was very real So every night they would come and ask me to for a story about the Lalpandar, Red Monkey. And every day I had to be imaginative and come up with an entirely new story. Right. And so they must have heard over the years, thousands of such stories. Right. And there were all sorts of adventures and so forth. The thing is, I since childhood, there were well-read children. So they had read all the children's storybooks. So I couldn't have bought from them easily. I had to be a story factory to do that. But good thing is I'm sufficiently trained in the. Storymaking, you You know that Campbell's The Hero's Journey and the template of stories and so forth. So there is a way, there is a process. If you do build a story like that, you're likely to always come up with something that will catch people's imagination. The idea is that all stories in human civilization there is only one story and there are variations of that story in every culture and that story is often called the hero's journey and there are few archetypes and those archetypes keep recurring if you know the archetypes you can generate infinitely many entertaining stories actually that's why in the creative industry the it's called the story factory you know those divisions are literally called the story factory i'm told so i'm told so so let's create a story factory template it will create stories about cats about dogs about squirrels maybe about horses right so we do that how do we do that so you realize that if you were to do it about dogs we would say imagine that there's a story about dogs they like to chew on toys or play fetch and run around the park write a story where some of the dogs show this behavior the rest of it is the same right we'll make it into a template. Let's do this. I'm saying, imagine that there is a story about some animal. Are you guys able to read my screen? Some animal. They like to, and then you give some behaviors. Write a story where some animal shows these behaviors. Give the story a catchy, humorous title. Write it in the format, title, body of the story. Right? So what are the two prompt inputs here? Animal and behaviors. Right? So I hope that this by now looks self-evident, like as obvious as anything. And then let us do that. Let us do that we'll take our dogs dogs chew on toys play fetch and run around the park as a behavior and squirrels eat nuts and fruits and climb up and down trees and run around the grassy meadow by the way this is all me cooking up so it's if it is a bit cheesy i forgive me now what happens i print it out and it looks like pretty good text to feed into it let's feed it into the language chain we build a language chain with this template and all we need to do is to the language chain first if you want to a story about dogs you see how easy it is one liner you give it the animal and you give it the behaviors and out comes the story how many of you are impressed and if you read this story at least i found it quite entertaining really interesting right and the next story is about squirrel and you can see that that too is fun so as if we like let's say going back to my example before about you know having a focus on let's say the medical side of things and medical text and understanding of that versus a call center so how could you do that so you would basically feed these type of prompts up front to yes their mindset yeah that is it because that's why a lot of people are beginning to come and get carried away and saying that all of modern programming or AI is reduced simply to good prompt engineering. You want to do it for pharma, create pharma prompt. You want to do it for call center, go create call center prompts. There are companies doing nothing else but hiring engineers and their title is prompt engineers. So so now if you are a skilled prompt engineer please don't scoff at my very elementary prompts this is just the next class you should have is like prompt engineering then there is a class coming up called generative ai, purely Generative AI, that starts from where I am here now in this lecture. That'll be cool. To the rest of NLP. So if you are interested, let me know, because- Absolutely, I'll be interested. I think Shalini was going to teach us. Oh, Shalini was going to teach us prompt engineering. That's true. Shalini, prompt engineering. Yeah, sure. But prompt engineering that's true shalini prompt engineering yeah sure but asif you're better at explaining than i am so i can i can put together but in all seriousness i can put together a list of papers and uh put summaries of them and so you know some kind of refresher workshop but obviously you know the papers are a little deeper i was actually going to post a paper to uh the the chat here um that i that will be a big service if you can create a list of papers for me at least it'll be a big service because at this moment as you know i am full time on support vectors which means that more than half the day i I'm doing something I've never done before. I'm wearing a business hat, CEO hat and hustling. So because of that, my technical time is getting a little bit curtailed. Most of the paper reading I do at night and I am beginning to slip behind a little bit. So any help like this, if you guys see great papers, immediately post it to Slack because whatever you post, I always read or watch the video i always take time to do that so it's just a big help because i'm not getting time to catch up on the news i think as of last time i remember uh we talked that you'll have like two days or something and if you have to pay something extra that's fine i think you had that discussion yeah yeah but i added it to this course. I made them free days. So this course, I just added two days and we are doing prompt engineering now. In fact, next two sessions will be a lot about that. It's all prompted. That's nice. But the generative AI goes far deeper. It goes into the architectures of all of this in a much deeper way. So it will cover things like what you're looking for kasi like domain adaptations heavily in adaptations no because in our company actually i'm like um looking at that right now very actively um and we have like so many of these use cases we're trying to figure out how to you know almost like um do the prompt engineering to tune or whatever you call it like, so focus on medical versus manufacturing versus. Yeah, here's the thing, you can come by any of these days now that the Roza is over and you'll have more. Come by any of these days. I'll do that. Let's do that. Yeah. I'm sure. And I will guide you. If it gets more serious, like if it starts getting that takes more time and commitment, but then talk to your company, I'll be happy to be, you know, engage with them in a more formal way. That's if it is starts eating too much time. But you're always welcome to talk to me for as long as you wish. Thank you. but you're always welcome to talk to me for as long as you wish thank you so guys now about prompts you can you have a prompt without any user input sometimes you know it is for example you can say i'll create a prompt a witty quote on everyday life in contemporary times you can keep on asking and it should keep producing some or the other witty code isn't it so you don't need to give it an input now there's a subtlety here you you still have when you create a prompt template you cannot set input variables is equal to none you have to give it an empty list because the template always expects an object a list object even if it is empty so that's the only subtlety here. Other than that, there's nothing special. And then you can put it into your chains and do things and whatnot. What's that format? Oh, because the argument to the format usually is the input variables right but when input variables are missing you can just call it converts the prompt into a text it's still required to be called right right here not just to convert the text but when you put it into a chain you don't need to see in the chain i don't need to right i just say input is equal to empty you run it and it works just fine life is 10 of what happens to us and 90 how we react to it charles welcome so that brings us to the last topic before lunch, which is a few short learning. And it's very fascinating. Can you teach a language model something? A relationship between things, for example, I'll take an example. example you say animal so if the animal is dog you say likes to play fetch so it doesn't know what animal is because to it is just a token it doesn't know what likes to likes to is that too is just a token right and what the thing is but it is trying to create a relationship between input and output. Between these two words. Right? Animals, squirrel, likes to eat nuts. So we say this is it. Now we will give it a prompt. We will say that, let us say that the input prompt is animal. Some animal likes to do what? Likes to what? Right? animal some animal likes to do what likes to what right you can create a learner this few short learners right you're basically saying learn and then emit the answer don't just emit the answer learn learn from these examples and then emit an answer then produce an answer right so how will it learn you need to wrap that prompt template with a few short prompt template in which you give the examples, because now it needs to learn the examples also. Very straightforward, guys. You created these examples. You create a prompt. It will take animal and likes to, and then you give it the examples also. Right. then you you give it some prefix like when it runs what should it say describe the behavior of each animal input animal is dog likes to play fetch that was literally the example isn't it that's the example cat likes to crawl squirrel likes to eat to climb trees where did i get this because I hardwired these as examples to learn from do you see it guys now comes the question let's create a few short then we say if you give it the answer few short prompt just pass just format horse it won't know the answer because there is no language model to help it it's's just a prompt at this moment, isn't it? But then let's create it, create a chain with the prompt and a language model that will learn from the examples and see what happens. You say chain this and you give it a horse and let's say, what does a horse like to do? It says, horse likes to eat hay and run around in pastures. How many of you are impressed? From three examples, it learned the meaning of likes to. So guys, this is the state of the art in AI today. So guys, this is the state of the art in AI today. Then the last concept for today is partial prompts. Partial prompts are very much like partial functions in Python or partials in Scala and so forth. Or in Java also, we have supports. So imagine that you have a template it's a template which takes two inputs country and year name three historically significant events in country around the year yeah so it could be like name three significant events in england around the year 1800 right does this make sense, guys? The prompt, is it common sense prompt? By now we know prompts, but what you can do, and you can use this prompt, you can, in a chain, you put it for the language model, you create a chain. And when you do create a chain, you run it, and then out will come the answer. These are three significant events, the Battle of trafalgar right the naval victory the industrial revolution and the great reform act yeah i'll ask a quick question yes uh this is great i mean this is really helpful to understand the lang chain and its power the examples that you've given so far are more consumer oriented uh meaning for end users if i want to create let's say a storybook for children or something like that what would be some of the enterprises use case of this seems to be is this also relevant for enterprise oh Oh, yes, of course. In fact, a massive, massive, massive amount of work is being done in medical, in legal, in many spaces, education, in many spaces. I can go on and on and on. In fact, you pointed out to a bias in the notebook that I created. I created it with the intention of being simple to understand. So I took examples that are not of practical use or industrial use the next time i'll try to bring examples of industrial use right actual commercial and i have another question on this is on the previous section that you covered when you give tools to the agent how does know which like does it follow the same order of the tools that we give? No, no, no, no, no. Just hold on because that's our next level. Oh, okay. Okay. So let's finish this topic of partial prompts. So here you took country and year as the input. But let us say that if you are only interested in French history, has anyone lived in France? So I managed a team and one of the teams reporting to me used to be a French team. And whenever I would talk to them, the world's best culture was French. The world's best food was French. The world's most beautiful city was Paris. So it was hard to get anything good anywhere else in the world. And they were very, very nationalistic, very passionate, very poetic, and their best literature was French. Actually, I partly agree with most of it, not all of it. Their language is indeed very sweet so well but suppose you're one of them so then to you it is never a question of what was important in history in this country in this year the only thing that matters is what what was interesting in france in this year right so from the original prompt you can create a partial prompt in which you have already fed in an answer the country is france are we together so you have the ability to create partial prompts and once you have a partial prompt you can treat it like any other prompt in a chain and so you can say what are the three most significant events around the year 1789 as you know 1789 is when many things happened. The kings and queens and the noblemen vanished. They were gulletined rather. And Bastille was stormed. All the prisoners were released. And pretty much the French Revolution happened. You see the different acts, different components of the French Revolution being mentioned here. So that is that. So we'll stop with that. I left out a topic, guys, which I leave as a homework. Sometimes two things happen. Suppose you have a lot of examples. I gave three examples. And you want to do a few short learn now. But a you know the embarrassment of the riches i give you millions of examples and you say well let this prompt do few short learning but have a million examples now the first thing you learn is that is very uh counterproductive because soon the principle of diminishing returns will kick in right so for, I give you a homework to learn about prompt selector. What a prompt, few short learning, the prompt selector will decide, has some heuristics to decide which of the examples to take and feed in. Like if, if from million I have to pick 20 examples, which 20 examples to actually feed into the chain right given all these examples and there are many ways to do that one is to randomly pick some one is to pick based on how many there are many many rules but one of the more interesting ones is using vector embedding the search thing that you learned so you take a query string input string and you see this input is closest to which of these examples and you take the closest examples from the semantic embedding because you know that learning from those examples would be the most meaningful isn't it a few short learner from these examples would be the most meaningful and you can feed that so selector itself is a very powerful topic but I thought some homework is always a good idea right no frankly I was budgeting for time and I already noticed that we are at one o'clock and I still haven't started with the topic of tools but let's keep tools for the afternoon so guys selector selector is your homework for today and there's some embedded homeworks in these labs do all of these labs guys don't miss it and we will again take one hour for lunch one hour for you to do the homework and some surprise price for anyone who brings a nice selected jupiter notebook right and next in the afternoon we'll do agents and tools right and then of course we'll do time permitting we'll do a paper if time permits we'll meet again at three o'clock right so remember one hour for lunch one for homework but be here at three because we'll start sharply at three he asked if i i need help setting up the environment variables i'm doing google collab google collab you might as well forget about environment variables and just just hardwire it into your code that's what i did actually that's why i was passing it directly yeah so the reason for that is in the case of a google collab the two choices you can I did actually. That's why I was passing it directly. Yeah. So the reason for that is in the case of a Google collab, the two choices you can have, either save it in a text file that you put in your Google drive and in your code, read from that and set it. The alternative is that, see those instances disappear in 24 hours. So you lose it. No point in baking it in. The alternative is just put it in your collab, but don't share your collab notebooks with anyone. Got it. Thank you. Yeah, but I would strongly suggest that these collab VMs are very underpowered. If you can ask me, buy yourself a very good, good workstation, and especially here in the class we actually this batch. This cohort has a really good thing going for it. Typically machines that Lambda Lab charges 10,000 to build. Sukpal is giving it to people for 4K literally the price of the components not charging anything to assemble you get you you buy the component or he gets components from volume discounts and he builds it for you a few of people in the in the in this cohort in this class have gotten those things built the in the in this cohort in this class i've gotten those things built we'll definitely consider it the difference is for something taking one hour to train and something taking 30 seconds to train got it huge yeah i've been using google collab i have the collab pro so far on the homework and everything it's been. Okay, but as I get more serious, I guess, I'm told. 30 minutes. How long did it take on your machine? On your QoLab? Which one? The fine tuning. The fine tuning did not take too long. It was like 5 minutes or something. 5 minutes? Really? Okay. So Ab abhijit you have your answer five minutes how long did it take on your machine 35 seconds yeah see most of me it's like this if you want to be a photographer right you can you can take a iphone only that far yep what was this block code that you passed sorry i am running it right now uh must be one second i'll answer your question what's your question more data sets after what we did and i ran like those were taking like you know a few data sets which you could like to take from the audience that's all that's all by the time you get yourself your coffee yes yeah yeah so watch me this block code is html syntax hey um I'll show show you do you see this line i got it yes yes it is formatting it correctly yes for me these little bits of aesthetics matter now i don't know if you would consider my jupiter notebooks to be a little cleaner than the average ones you see on the web Okay guys, one announcement. Deep learning, many of you intend to take it. You haven't registered for it. Please do, otherwise I'll cancel the class. And preferably write a check. Don't send because this, I mean, or sell it to my number because you have my phone number, right? 510-298-9033. This year, sir, don't do Zelle because next year, you have to report it. You have to? Report it to IRS. Everything gets reported because it goes into the business account. Yeah. For me, all Zelle gets business reported. This year, Zelle. Before this year, they didn't care. Oh, no, no. See, you is that before this year they didn't care but no no no see you don't want to be on the wrong side of virus i've always been particular it's not worth it yeah okay so yeah guys uh you can either sell it or write a check or do something or you can use the website but then i lose 36 i mean not 100 50 100 something like that okay pretty huge amount that they cut plus they hold that money for uh three weeks before giving it to me all right guys oh yeah mostly you have any other questions the issue all right guys yeah mostly you have any other questions no i'm good i'm just running the i was driving so i just came back home i'm finishing up the 17 17 uh planchan prompts if i make it probably helping you but so far it looks reasonably straightforward. I am very curious to hear about your thoughts on enterprise use cases. Definitely, definitely. I'll bring some next time. All right, guys, I'm going to pause the recording. All right, folks, welcome back from your lunch and your homework. In the morning, we got started with Langchain. In particular, we learned about three core concepts, the concept of a prompt, the concept of a large language model that we can use, the concept of tools and agents. It's a tools, agents, prompts. They sort of orbit around the core of it, which is the efficient use of language models, large language models to achieve a particular task. Now, we realize that when you break a task up into smaller pieces and you chain them together so that multiple actors or multiple tools and agents can act on it, you get far better result than posing a question directly to a large language model. Now we also went through some ideas about engineering the prompts. You realize that by the time you look at these prompts, the first thing that strikes you is that these prompts are not one-liners, that they're reasonably well thought out things. Now in the second part, now in the afternoon, I would like to do, before we go through people's homeworks, I would like to do one last topic which we haven't covered, which is the tools. Agents and tools. So agents will talk a lot more going forward. Today I'll introduce agents and then talk about tools. So tools are, think of them as the glue pieces. You have large language models, you have agents that are sort of the orchestrator that make things happen, that make a decision which tool to use. So think of it as a carpenter. He has access to, she has access to many tools, drills, this, that, and large language models. And the agent is making decisions that to solve this big problem, how do you break it up into pieces? What should go into one tool, come out from that, go into the next and so forth? And into language models should come out. And what is the whole sequence of actions or subtasks that must be done in order for this to work? Now, that brings us to the question of, why would you need to do that? You can always hardwire a language chain on your own. The reason you do that is these agents are capable of doing what hardwired language chains can't. They're dynamic, they're adaptive. Based on the user input, they may or may not use a tool. And this was your homework. For example, there was a search engine that we used in the prompt. If you remove the word, what is the height of Mount Everest today? Then you realize the agent may decide that you don't need the search tool at all. And so it will bypass the search tool and go straight to the language model, get the answer, use the calculator, do the multiplications and be done with it. Isn't it? And so the agents have certain degree of flexibility they can decide how to dynamically build up a chain based on the nature of the input are we together right and that's what it was that homework was meant to illustrate that just because you gave it the tool doesn't mean that it has to use it. So with that there, there is no doubt therefore that wherever possible you should use agents to do your work. It can make judicious decisions at runtime based on what inputs it received. Now these are all very imperfect things remember the technology is evolving very very rapidly what is true today may not be true tomorrow even this land chain has come burst upon the scene it is at zero point what is it one four or something version one four six one four version so these are very fast moving projects, fast moving APIs. The problems of today are not necessarily going to be the problems of tomorrow. Right? But are they useful? They are tremendously useful tools. Now today we'll focus on the tools, the one pillar that we haven't talked about much. We use the search tool, SERP API, but there are many tools that are available in the land chain ecosystem. And you can wrap anything that you have, any program on your machine, into and put a wrapper around it and make it into a land chain tool, formal land chain tool, a custom tool of your own and so we will do two examples in the first example we will use a tool that is there built in that is sort of freely available in the langchen well not exactly freely available but okay that is available and the other tool that for which there is a functionality there's a library but we can make it into a tool with just a few lines of code so if you want to see what is the comprehensive list of tools available we can go to if you go to this website and let me increase the font a little bit, do you see that these tools. there's a modest list, not a very big list, but there is a list of tools that is available. yeah isn't it, this is the tool now if you go to API fine right integration is a cloud platform for web scraping and data extraction, blah, blah, blah, right? So it is somebody has a service on the web, and now it has been converted into two. Likewise archive, as you know, most papers in our field, they are submitted first as preprints to archive. That is how you get to know what has happened. I mean, almost all of you have now started reading Archive Sanity to find the best publications of the week or the month. And so that's archive. We will take an example today and see how we can search and archive. Bash, if you have ever used a unix system you know that bash is the shell so anything you can do on the bash you can you can basically make it into a tool and use it in your uh in your code in your chain chain, land chain, right? Other is Bing search, of course. And some of them need keys. Like for example, chat GPT. Chat GPT plugins these days are being talked about quite a bit. Chat GPT plugins and so forth. They are in a sense, nothing but tools. They're just other functions. So what is the definition of a tool? You can think of a tool as a function that has two qualities. One is the API signature. String in, string out. And preferably status. A string goes in, a string comes out. For example, you give it a few keywords search result comes out that's a tool right and you can think of many things bash you give a command and the output of that command is again things that scroll string that scrolls on your screen and the command prompt text in text out these are these are your tools the second quality that a tool must have obviously is that you must wrap it up in the land chain api just wrap it up there is a very simple syntax to convert any such thing any such function into a tool and we'll go through that we'll pretend that archive api does not exist as a pre-created tool and we will go through that we'll pretend that archive api does not exist as a pre-created tool and we will go through the exercise of creating it there are many things as a tool also say that again please there's something called human as a tool oh yeah right right so let's go check that out right and this is where you can input something so right text in text out well you are the one producing the output so this is it and um open weather see See, most of these are no brainers. Python, shell, again a no brainer. Then search tools. So there are various search tools that you can use. What is the weather in blah blah blah right uh google server api serp api there are many search tools we used one and you can look into many of these then a search ng search wiki wikipedia this is very useful you can you can we all want to search through wikipedia for information you can do that you can you can have Wolfram Alpha as a tool. And by the way, this is a pretty powerful tool. There is a lot that can be achieved by putting the computational power of Wolfram Alpha and hybridizing it with a large language model. Some of the articles or things written about it are quite impressive. And then if you are familiar with Zapier, then Zapier is sort of a, think of it as a ginormous integrator, a very fluent integrator into many things. It forms like the glue that integrates all the gazillions of apps essentially so you can have that you can have uh that agent as a tool right and and we can go on there is a pretty long list of things we can do as a tool so with that So with that, let's go back to our lesson. Explore these tools, guys. Take this as a homework, explore this tool. Now I will show you the example of how to use a tool. You can use a tool without using land chain, without using an agent. A tool can be used in isolation. Here's an example. We use a SERP API wrapper. Remember, you must have the SERP key, API key. Assuming you have that, you could ask it a question. Where is Support Vectors AI lab located? It seems to claim it's in Fremont, California. Like that seems correct. Now, let us use it with a chain. How would we use the tool with a chain? Very simple. What you do is you create a list called tool names. All of these tools have a name. And so you can declare a list of tools that you want to use, right, literally. And then what you do is you call load tools when you load tools you can give it by tool names and by the way you can also load to by literally instantiated a tool object and passing that also in but this is one way of doing it then then comes the main part you initialize an agent and this part pay attention to what is the agent taking it's taking a bunch of tools it's taking a language model an agent has a type at this moment we will talk more about types later but at this moment just take it as zero shot react description as a mantra i won't get into it a bit more now but later and verbose is equal to true verbose equal to true what it does is when this agent goes about it doing its job it will show its chain of reasoning it will show how it is going about thinking right so this is it now folks i hope when you look at this code it looks pretty self-evident what it's trying to do would you agree you instant now now just because you instantiated a bunch of tools doesn't mean that your agent may use it remember that it may choose to use it but it will never use something not in the chain but not in the tools list right within the tools listed now in this particular case the tools listed. Now in this particular case, the tools list has how many tools in it? Only one, SERP API, right, search. So suppose I were to ask a question and pay attention, how interesting, I worded it differently. What is the climate like in the city where Support Vectors AI lab is located? Why would you want to know the weather? Suppose you're thinking about spending three weeks here to do the ML 100 introduction to machine learning course. That course is four days a week. You may be better off just being here in Fremont and attending the workshops in person, right? If you were to do that, you would like to know what clothes to pack, how is the weather and so on and so forth. So it is certainly part of the decision-making process. You to do that you would like to know what clothes to pack how is the weather and so on and so forth so it is certainly part of the decision making process you could do that now you realize that i haven't i didn't ask it what is the weather in fremont that you could have answered using just a weather api tool open weather tool i'm not saying where is support vectors located so first it has to figure saying where is support vectors located. So first it has to figure out where is support vectors located. So let's see the chain, let's see its reasoning. It says entering chain, it realizes I should research the climate of the city. That's what it has to do, but it has to figure out which city, action search input climate of and city where support vectors ai lab is located so then it makes an observation through search that support vectors ai lab this is the address including the web address i don't know guys are you finding this impressive? Super impressive. How does it make that, it has to look that up? Come to deep learning. We will go into the, remember, this is a very applied course. The theoretical foundations and things we'll do when we go into, otherwise what happens is here, we'll get derailed. It's a long discussion. Moushmi, I'm not trying to brush you off, just saying that to understand how a lot of these things are happening and what is going on, there is much more to it and we need to know some core neural architectures. No worries. Is this agent is also in some ways like a model or something? Isn't this agent is also in some ways like a model or something? The agent is in some sense, yes, it's an orchestrator. It's a model in its own right. It's basically an AI object. The only thing that's not an AI object are the tools. Tools may or may not be. For example, the math tool is an AI object, it itself is a large language model. Weather tool, search tool, they're just tools. So, agent seems to have an additional pre-processing and post-processing capability. Yes, it is the orchestrator. That is its core strength. So it is also doing some string processing determining which part of the thing i need to search on and put through the pipeline exactly exactly and it takes do you notice that it is very uh inferential the way it infers the steps it has to do it is not something that can be hard coded so it is itself a machine learning model right amazing stuff it's just amazing isn't it so who wrote agent because agent itself has some intrinsic capabilities yes yes so obviously you see this land chain project there's a reason why they are funded with 10 million right off the bat. And then the people, see everything stands on somebody else's research, right? This thing has been going on for the last two, three years, prompts and agents and so forth. Since 2021, there has been a huge flurry of activity, right? In this space, because there was a growing realization that this is a direction we need to take in research very seriously. And now we are seeing the fruits of that very very strongly come forward so in fact when I was saying that the deep learning course is completely revamped that even the things that I taught last year even from last year this course is a completely brand new course. Anything I taught two, three years ago, before the pandemic, you wouldn't even recognize this entirely a different course. The reason is the field is very fast moving. And these things are to understand the beautiful, you know, the foundations of how these things work. to understand the beautiful you know the foundations of how these things work i don't know i find it just really worth worthwhile to understand how these things work when you see these things work see guys before you came to this class today would you have expected this magic how many of you knew that these things can be done how many okay let me put it this way you guys are all quite how many of you are completely taken you knew that these things can be done. How many? Okay, let me put it this way. You guys are all quite how many of you are completely taken by surprise? This is like, this is very different. It's very, very different right now you see why people are beginning to wonder if we are on the cusp of agi yeah this is like a harry potter wand actually if you look at like microsoft open ap open ai they kind of like show you know like in their schematic in the diagrams yeah but it's like it's good to see it working. Because they, like in the Microsoft, on their website also, they have, how do you use, like, for example, OpenAI with the semantic search, for example. Yes, yes. By the way, that's our next lab, next week we are doing that. So, by the way, would you like to demo in this break, would you like to demo in this break would you like to show the schematic schematics and uh things from microsoft i've stayed away from their portal for historic reasons but it would be nice for people to see it would you like to do a quick five minute demo let me find out okay do that right so look at this guys it figures out it needs to research the climate of fremont then it says search climate of fremont and then it comes up with the answer in fremont the summers are long warm arid true and mostly clear and the winters are short cold wet and partly cloudy over the course of the year and the dot dot here so it knows the answer yeah asif can you go up a little bit do you have an llm password oh yes of course here is the element where did you initialize this llm uh where did i initialize this llm uh that because it's giving me an error when i ran it oh my goodness you know what may have happened i deleted that line of code so just just instantiate it llm is equal to open opening my apologies i was cleaning up the code and i managed to delete the llm instantiation itself yeah nice catch i should i should put it here. Search now. Right here I should do it. Let me set this. Better? Okay. Now, this was, this is the answer, this is the output. And you would imagine that this is, when you ignore all this and you ask, what was the question, where is support? No, not this. What is the weather? What is the climate like in the city where support vectors? AI lab is located and it answered it very well. When you change the temperature of open AI here, will it also affect the way the agent to take its position. Now, what happens is this temperature argument is passed to OpenAI API. And the API knows, because all it does is model is fixed. It's just in the softmax layer that just affects the sampling. That's all. So the agent will not get detected. No. It will just still create the spots. Exactly. Based on exactly unless unless the response changes widely this agent is part of the opening yeah so the open uh yeah the not the open yes sorry the lanche lanche lanche so lanche's main things is large language model which not the openAI, sorry, the landchain. So landchain's main things is large language model, which it doesn't do, but you get it from somewhere else, typically Hugging Face or OpenAI, right? The things like that, Google, something. The second is it gives you tools, the tools API, and it gives you some pre-built tools. And as we'll see, we can create our own tools. The next thing it gives you is of course the chain, because that's the score thing. And the last thing it gives you is the ability to create dynamic chain by working on the input, like the agents, orchestrator, the agent, which says you don't have to do a hard wire chain. and that's the real power that is where all the research is going into that's exactly where all the research is going yeah that is it yeah the whole idea the whole thing that you could do and yeah see the interesting papers on the chain of thought and so on and so forth so guys here's the thing i am i was very busy these days getting support vectors off the ground to earn enough livelihood hopefully it will take off as soon as it takes off and it is on reasonably solid financial foundation i want to bring back the sunday seminar series sund seminar series, every week we discuss one paper. Just like I do it here. See, I'm doing it with you guys, right? Every week I cover one paper or one article in depth. So I do it actually for the open audience on Sunday afternoons. But recently I've been so busy throughout that even my Sundays are wiped out. So it's on pause, but I'll start it again. And when I do that, a lot of these things we covered last year, actually. But sporadically, what happens is that the talks would happen, then for months, it would be quiet, then again, I would get one talk, then again, for months, it would be quiet. But I want to bring it back on a more regular basis hey awesome virtualized implementations uh we do dynamic uh service chaining based on you know a dynamic event that happened you actually compose the service chain and then execute it uh and this thing is looking very beautifully similar to that list this domain yeah it is it is and in fact they'll consider that I didn't know about that part but see if you can take it to the next level by using these AI agents yeah yeah it's amazing yeah yeah I see a quick question on the observation and thought so it's action then observation and then a thought right yes so is it is the model built like that that it does one action then kind of compute some observation from it and then kind of goes to the next thought no it happens to be in this case the way that is it it will take some action then it will say okay if i have to do this action what is the action input then it will run that input through something, through a tool or through a model, and then look at the output. And then on the output, it will make an observation. Then with that observation, it will decide what next to do. And that will lead to the next action. So what happens is small bytes, right? Action causes it to figure out what the input to that action should be, because, right? For example, search. Search means I have to use the search tool. Then I need to figure out what should be the input to the search tool. But then when you feed it in, output will come out, right? It says observation, but then it has a thought. What do I get from that output? It says that, okay, now that i have that i should research the climate of fremont california right and this pattern repeats itself right the the terminal condition one of the nice things about transformers is that exactly that they they are very good at terminating all transformers generally generally as generative models they don't go on forever and ever because sooner or later the probability of the end sentence, end token, that token is just waiting for its chance and its chance comes and the moment it comes, it's over. The ability of this zero-shot VR agent, is this also dependent on the strength of the model that you're using? Oh, yes, yes, very much so. It's just a case of something that's not open AI. Yeah, so guys, this is your homework. Please do it seriously. Replace it with CELIBRAS, the open source models, one of the open source models from Hugging Face, and see all of these labs. If replace llm as open ai with celibress or something else please see how the results are affected it's very instructive to know that the gap i did as if in the break i did and i used that code in slack uh-huh you did oh how did it go uh tell me how actually i tried with google flan t5 excel ah wonderful actually what let's do one thing this is a good moment for us to take in five minutes take a break we'll start with your demo hold that thought in your mind yeah by the way which model were you thinking from hugging face celebrous celebrous okay. You can try that. But a T5 is a great another alternative. Now apparently Facebook just submitted a model yesterday. No, that was for vision, right? Facebook submitted or Google submitted? One of them, one of the two giants submitted a model. Oh, no. It's... No, Lama. No, Lama has been there for a while. By the way, what is Lama? How is Lama created? Hint, what did we learn last time? Destillation. Destillation. Simply a distillation of the large language model. So Stanford kids, they just sat down and for $1,500, they managed to train the Lama through a process of distillation, right? So never underestimate the power of distillation. What was that Shah Rukh Khan statement? Never underestimate the power of the common... So this is it. Okay, guys. So Mahushmila, hold that thought. We'd love to see your demo. I just want to wrap it up. It's my last topic. So you may say, can I create my own tools and add it to this tool chain? Absolutely. In fact, I highly encourage you to do that. Right? See guys, one of the things I asked is, Praveen has worked very hard to clean up the videos of the previous lectures. I hope you all are benefiting from that. Right. So, and he has also generated summary notes from that so i asked myself in every course that i give you notice that i give detailed quizzes that you guys take in the past and hopefully your founders quiz is useful i asked this question that in this course which is so much about generative ai and nlp can i create a model as a lab create a model to automatically create a quiz out of the videos? And guess what? There is a quiz that's coming at you. And I didn't create it is the is completely created by AI. And you can't say that, you know, I didn't learn that because it's taken straight from the videos. Nice. Nice. So this is the power of this thing, guys. Okay, so we will take an example. So there is something called archive. All of you are familiar with it. What if I say say let's go and assume by the way archive is already there in the list of tools but i took it as an example to illustrate that you can make anything into a tool so we'll pretend that it's not there right so there is a python library called archive when you install the python library right uh it will make the call to the archive website and search for you and bring you back the results so I will just show you how to use it nothing to do with the land chain so far you take a query you happen to take a paper and you create archive this API object and then you run the query on that object what will it return you it will return you a list of documents you print that document out it turns out to be an old old paper that I wrote in well 1997 well this work I did in 1995 but okay 1997 is when it finally got published right I was one of the co-authors so do you see the use of this library is very precise right you give it something it will give you an answer it will give you the text could you increase the font a little please thank you okay how about this yeah that. That's nice. Okay, so now we ask this question, how do I make it into a tool? Now I've gotten a bit more formal. You can do it with perhaps less steps, but okay, whatever it is. What you do is, this is the only necessary thing, but I went one step further by creating a argument name called preprint. So are you guys familiar with a Python PyDiantics base model? If you're not familiar, okay, think about it this way. It's a fancy way of creating an input variable. Input variable is the preprint. Those of you who are familiar with Java, it's nothing but a Java bean. Gojo, right? So just forget about it. So now what I do is we create a tool. I give it a name, archive. I give it a function, archive.run. Why? Because I know that this has a function call. Do you see the run function is there right so i give it that archive function and i say function name is archive.run and i say what are the arguments it expects well argument comes if arc schema at this moment look opaque to you you can ignore this line description is a polite thing to do you should always document your tools right search as the archive preprint repository so now what do i do see did you notice that i in one constructor just using the tool constructor in python i was able to create a tool of my own which is a wrapper around somebody else's library very Very useful library. Then what can I do? Let's say me using it. I can use it just like this. Tools, what are the tools I'm using? Well, I just give it a list of my tool. If I chose, I could have added Google tool also or whatnot, but here's the thing. LLM is obvious, the agent, blah. All of the rest of it looks obvious, guys. It's the same thing as before. The only difference is in the tools list, I give it only my tool that I just created, right? And then you query it and you ask the agent to run. You create the agent and you run it. And what am I saying in the query? Search for papers dealing with attention is all you need. Show the first five results right so now look at the way it thinks entering new this thing i need to search papers related to this topic so it figures out that because it needs to search and the only thing available to it is the tool available to it is archive it better use that so it says archive input attention is all you need now do you notice that the from the query it has figured out that attention is all you need is the is the actual input it should be that itself is an intelligent act isn't it to be able to figure that out it does that then it says observation publish title so it has come out with the results right and attention is already global local spatial attention for blah blah blah so there are many papers it comes out with and because i said print the first five papers show the first five results so publication then it has a thought i said print the first five papers, show the first five results, so publication, then it has a thought. I now know the first five results of the search. Right. And then it says final answer. The first five results of the search dealing with attention are these. And so this is your output. is your output. So do you see guys where we are entering? This course was natural language processing with transformers. And you are seeing how natural language processing which used to be the weakest pillar of AI and computer science has become a dominant pillar now because of transformers. It's literally become the new way of engaging with computers. It is fast becoming that. Isn't it? So, I mean, there is even a feeling that all the GUIs, application GUIs will now simplify itself. User interfaces will simplify itself. Just voice and text input. There was this interview with Wolfram and I was looking at it and he said something around like linguistic user interface or something like that. Exactly. That is the future, NLP interfaces. So how you, like at this moment, what happens? Everybody's webpage, web application, or desktop application, they all look wildly different. And you keep searching, where do I go? You read massive documentations to figure out where should I go? And there'll be some obscure button somewhere to click deep down in the sub, sub, sub, sub menu, right? But imagine what this is going to do to the world right so you you i mean rethink if you're a ui developer rethink your future yeah linguistic yeah the nlp driven uis nlpdriven user interfaces are the future. Like look at what Shashinda created, just one search bar. Just one search bar. And he's not even using at this moment, the agents in the large language, a land chain and large language models fully. He's using what was the first part that we learned about, which was the semantic embeddings and going from that so imagine where he will take this app once you go further right and can ask more detailed questions i remember the the demonstration that junaid gave that question answer that was impressive but now think of the next level that you can take it to with all of these things coming in right so guys this is your uh the next thing i'll give you is i'll give you another tool bash if you're on linux you can run bash. By the way, this is if you run it on my machine using Python, this is what you will get in that directory of NLP, right? So what I want you to do is go through the exercise of converting it into a tool that you can use in a land chain, use with the agent. Are we together? Nothing fancy. It is exactly a repeat of what you have learned. And then create a good query that utilizes this. Makes it work. And then you'll realize what it is doing to the user interfaces. These things can do. So likewise, become familiar with the Wikipedia library, create a tool around it, play with it, do things. So was it fun guys today? Yes. Yeah, we have it the course is getting more and more interesting every time isn't it? We are building upon what we learned and so it's getting more interesting. Next time, hopefully, you'll be even more pleasantly surprised. So now, guys, it's quarter to four. Let's do one thing. Let's take a 15-minute break, and then we'll get into the paper reading. The paper that we are going to read are posted on the Slack. Please glance through it, and I'll walk through it carefully after 15 minutes break by the way is anyone making fresh coffee i'm going to start a pot i'll start a book yes if i'm stuck can i ask you a quick question ah sure go ahead question. Ashu, go ahead. Thank you. So let's take these four numbers. We want to, what softmax does is it says, see, suppose you, I wanted to make them into fractions. I would have gotten easy. I would have said 3 plus 2, 5, 1, 6, plus 4, 10. Summation is equal to 10 yep and then because the sum is equal to 10 i could therefore have said that 1 over 10 2 over 10 3 over 10 4 over 10 they look like probabilities to me isn't it i could have treated if i wanted to convert a bunch of numbers so voltages let's say that the output of logits, that is the last layer, a hidden state, whatever you call it, hidden state, into this, we could have done fractions like this, right? So there are problems with this fraction. One is that, what if one of the numbers was negative, right? So suppose it was, just to recap the discussion, suppose the numbers were like this the summation would be zero all right and so it would be problematic it would be very hard to make it into fractions so the other problem with this is it doesn't sort of make the big number stand out. So you solve both of these problems with a very simple, very, very elementary mathematical device. What you do is you do e to the 1, e to the 2, e to the 3, e to the 4. Now let's see how much it is. This is 2.7, 2.72. This is approximately eight, I believe, right? And this is about 20, I would hazard a guess and say this is about 22, 21, right? And this is about 22 21 right and this is about 60 21 this is about 20 and this is about 55 you guys can verify i'm just saying approximately just just making mental mathematics so now look look what exponentiation did. What exponentiation did to this is it made this number really stand out, right? So if you make a thing, it is really much taller than the 20 is less than half of it. This is like this. This is this. Eight is even half like this. This is this. 8 is even half of this. And 2.7 is like this. So you realize that the ratios have gotten seriously exaggerated, isn't it? And now you can convert it, divide it by the sum. sum would be e to the j over all possible values of j just add up all of these numbers basically is equal to 2.72 plus 8 plus 20 plus 55. this is it so this is the softmax this is the definition one of the advantages it has is that it also takes care of this finicky problem of negative numbers because e to the minus 2, e to the minus 1, e to the 0, e to the 1, e to the 2. They are all positive numbers. So when you divide it by the summation, they all become fractions. So when you divide it by the summation, they all become fractions. Fractions between 0 and 1, they take values here. So they are all probability masses. You can reinterpret it as probability masses, which is what you want. So all the paratha finish? Only one left. Should I eat or what um that is on your own because because i don't want to say don't blame that if i eat what would you say okay so uh yeah so why did we have to take an exponent we could have done it using log right that day you started logs and you switched to exponents that's the part that was confusing to me i must have taken log for a different context this you did explain the trouble is if you take log it will actually have a counterproductive result it will see log of these numbers log of two log of one they are much closer to each other it will dampen it down remember exponential curve goes like this and a curve goes flattens out after a little while it never quite stops but it flattens out it goes like this right it rises up like this whereas exponential curve rises up exponentially keeps on increasing so you want to exaggerate the bigger numbers you want an exponentiation and then you just divide it by the summation now that is that log i must have talked in a different context you're mixing it up with something else what was i talking about anyone remembers where i talked about log well when you showed this example you started doing the log and then you switch to exponents that go back i'm sorry if i confused you guys like that but it wasn't this it is a simple question of exponentiation now the temperature part comes if you take these numbers like one two three four and you also divide it by temperature before you do the exponentiation right and then you exponentiate all of these separately two three four and you also divide it by temperature before you do the exponentiation right and then you exponentiate all of these separately right so then what happens is yeah i tried if you put an exponent number large it becomes garbage it starts spitting out garbage gpt that is right it is right so so what happens is that because it has an overflow problem the maximum that the computer can hold it will exceed that see remember computers cannot count from infinitesimal to infinity they can there is a minimum there is a minimum size that it can store in memory and there's a maximum size that it can store in memory, and there's a maximum size that it can store in memory. If a number is smaller than the smallest it can store, it will have an underflow problem in magnitude. It will have an underflow problem. If it stores, if you try to put it a number bigger than the max that it can have, it will have an overflow problem. It will produce garbage both ways got it okay so now the nation as if sorry for the digress no no this is all useful this is actually behind machine learning are a simple set of very simple math ideas if you get that right do you realize that distillation and all of these such beautiful techniques but behind it are such simple elegant math ideas right there's nothing complicated actually the beautiful thing with math is and with this machine learning is that ai is actually there's nothing complicated when i look at some of the business code that people have, application development code that I see people have, just so many corner cases and so many complications. And after a little while you get exhausted just keeping track of the side issues. Beautiful thing about machine learning and math is there are no complications. There are no exceptions to the rule ever. I like the fact. That's why I find this simple enough that I can understand. Okay. So we are going to now do this chip nuions paper, which I have pasted into this chip nuion. Building ML applications for production so llm applications for production so so in the morning we talked about these llm models and we said it is great you know we were amazed at all the things it can do the problem is when you take these things to production, it's a different kettle of fish. What is happening these days is the world is divided into a vast number of people who are playing with chat, GPT and LLMs and these transformers. And they are absolutely euphoric on the things it can do. And then there are lots of startups forming, companies forming that are trying to make it work in production. And that's where they're finding that it's much harder to make these LLMs work than a traditional software architecture work. Traditional software architecture is deterministic because the code behind it is deterministic. You face the same problems of scalability and performance and reliability and security and hardening and so on and so forth, right? Kubernetes, Dockerize, containerize and all of that. You do, you know how to deal with it. Trouble with LLM says that, and I let the paper speak it, but basic summary is that these are hard because language models, transformers are stochastic machines, right? They are probability machines. You sample from them, especially in the generative side, when they generate text, it comes through probabilistic sampling of some distribution. Remember, like the softmax distribution that we just talked about, softmax with temperature. And so whenever you sample, sample is a random process, inherently it's non-repeatable. So every time you ask a question, if you get a different result wouldn't quite trust the trust a system that answers the very same question differently each time. Anand Oswal, LL. Right so that's the that's the crux of the problem and we'll discover discuss it some parts of it are really worth talking about and i'm going to talk about. about and I'm going to talk about these things. So it says it's easy to make something cool with LLMs, very hard to make something production ready with them. Now LLM limitations are exacerbated by a lack of engineering rigor in prompt engineering, partially due to the ambiguity of natural language because what are you writing your prompts in? You're not writing it in code, you're writing it in natural language and partially due to the nascent nature of the field, we are just getting into it, all of this thing is just a couple of years old. Right, in the last couple of years we've been doing prompt engineering. So this post consists of three parts, we'll cover those three parts in a bit. But let's get to the meat of the matter. The first thing is the ambiguity of the language itself. Languages are notoriously ambiguous. And so with just a small, you know, I always keep joking, you know, this famous book, each shoots and leaves. The meaning is completely dependent upon where you put the karma. If you say eats karma, eats, shoots karma, right, shoots and leaves, if you just say eats, shoots and leaves, you're probably talking about pandas. On the other hand, if you put two kamas, eats kama, shoots kama, and leaves, or you just put one kama after eats, and then shoots and leaves, you're probably talking about a murderer. And it brings about the nuances of natural language. It's subtle. There are many subtleties. And this we are talking about in English. In tonal languages, it's even more. I'm told that in Mandarin, I try to learn Mandarin. And I never could get much further because it's a tonal language and I'm practically tone deaf. So the pronunciation makes all the difference. I never could get much further because it's a tonal language and I'm practically tone deaf. So the pronunciation makes all the difference. How do you pronounce it determines whether you conveyed the meaning correctly or not. So you can imagine that for the longest time I tried to say the right word for grapes, I believe it's pingua, but every person who's Chinese has told me that I'm wrong. That's apple. Oh, that's apple oh that's apple sorry pingua what is grapes yeah so yeah so i tried to say apples and i would say pingua and they would all laugh at me and say i'm pronouncing it wrong i'm saying it wrong all right you can say mom really mom becomes horse okay there you go you can make a horse out of your home by mispronouncing it so you can imagine how much ambiguity there is in the world languages natural languages right and nuances there are so when we define prompts the fundamental fundamental issues, first is that, think about code in a structured programming language. Just an extra comma or anything in the wrong place, and your compiler will tell you, your parser, lexa parser, compiler, they are all up in arms. Right? They will immediately point out the flaw in your stuff. If nothing else at runtime, you'll get the exception stack trace and you'll know exactly what failed. So that is not true. A natural language has failed. This prompts fail silently. You have a issue with the prompt. It's ambiguous, but when it goes into the machine, it doesn't tell you that you have an ambiguous prompt. The other is, there is a ambiguity in the responses LLMs generate. So for example, it is not structured data. It's not generating a table. You say produce a nice CSVv a data table like when you run a sql query what do you get you get so many rows so many columns isn't it it's deterministic it's clear for the output of a prompt from a large language model there is considerable ambiguity so for example here is a prompt in which you explicitly say, give me a score to this essay and nothing more. Expected output format, essay score, something out of 10 and nothing else. Let's look a little bit at this prompt. It says, you are a critical and unbiased writing professor. Given an essay, give it a score from 0 to 10. 0 means the essay is trash. 10 means the essay will likely get the student into Harvard. Output it in the following format. Easy score, I mean, sorry, essay score. There's a number. Score out of 10 and nothing else here is the essay and then in quotes you give it the essay so first thing guys do you notice that the prompt is pretty long isn't it it's pretty long and to the extent that open api is charging you by the tokens right you realize that you're beginning to rake up quite a bill. They charge you not only by input token, but also by output token. Both. So you're beginning to rake up bills. So here, it came out with an answer. SA scores 7 out of 10. This is what you would have expected. Isn't it, guys? And yet, you run it again and it changes its mind it says six out of ten and and despite you're saying and nothing else just give the score and nothing else it nonetheless gives you a commentary on that of why it is six out of ten right so do you see the the non-determinism or lack of structure in the output? It's inherently there. Then, so the user experience is inconsistent. You expect one thing. So imagine that you expect a large language model to give you, sorry, a quote, I'll have to make it a bit smaller, a quote, where's my mom's quote, whether to approve your loan application or not. But every time you ask it for a given applicant, it comes up with a different answer, and probably a different explanation for the answer. answer and probably a different explanation for the answer. You wouldn't like that. Now, people have tried to set temperature to zero, which is generally a good practice if you want predictable results. It mostly solves a consistency problem to quite an extent, but the problem is it doesn't inspire trust because you know that this is one of the many answers that could have been given now you've just pinned it down to one so chip say she says that imagine a teacher who gives you consistent scores only if the teacher sits in one particular room if the teacher sits in a different room the teacher's course, you will be wild, right? They'll be quite different. So here is an example. The same prompt now gives you, again, different answers and with different explanations. The first one says the essay has some merit. It presents a personal blah, blah, blah. The second one says, while the essay captures a good anecdote, personal anecdote about the author's experience with the blah, it lacks depth of analysis. So it is critical. In the second case, one is seven out of 10, you passed. The second one is four out of 10. In the USs what is four out of ten considered failing grade right right so how to solve the ambiguity problem so people are trying hard to solve it but it's a little hard right you can improve upon your prompts but remember you can do only so much so another technique did we just learn about the few short learners learn from example right so here is a short prompt see we use that prompt right uh cats like to do this dogs like to do this girls like to do this what do horses like to do do you remember guys in the morning we did that right? Anyone recalls? Yeah. Yes. So in the same way, this paper, this article has another example. It says measure some text for controversy. But here is some examples of judging controversy. Right? One plus one is equal to two. The controversy score is 0. Well, clearly the author has not read Bertrand Russell's philosophy in which he deeply questions whether 1 plus 1 is really 2. Anyway, leaving that aside, starting April 15, the only verified accounts on Twitter will be eligible for blah blah blah and score is halfway controversial next is everyone has the right to own and use guns fairly controversial then immigration should be completely banned to protect our country extremely controversial it's a polar it's a very polar perspective then then you say that the response should follow the format perspective then then you say that the response should follow the format but which is code is this reason is this here is the text when doing few shot learning two questions to keep in mind whether the lmm understands the example given in the prompt so generally you know when you create prompts one of the first things so i'll capture my my experience instead of going through this. My experience is that you have to give it baby steps. Right? Teach the LLM to reason in baby steps. If you ask it something too big and vague, it generally does very poorly. But when you give it baby steps. poorly but when you give it steps can you explain this in diagram because one of the reasons is that yes this uh author you know i mean this field is moving quickly some of this information like people are working through it and for the production side yes i mean in her in the book she says that usually uh ml teams they say oh they're just researchers but people have been successful some have been successful in getting things to production so it seems like you know if you say this i think we'll all forget but it would help if you have a diagram or something and i promise you i will it will be the screen a wallpaper on my uh on my laptop because this is something that I think a lot of people are thinking about. Well, yeah. So I see my experience has taught me this. Imagine that you're talking to your dog, right? Dog has the intelligence of a two-year-old. And imagine that the dog could talk. You're talking to a two-year-old child, or one-year-old child. So what happens is that the child doesn't have too much context of the world, but can reason. Actually, when you have children, you know that they become perfectly clever reasoning machines by the time they reach the age of two. So it's walk at one and talk at two, the moment they start talking you're done for as parents now they completely overwhelm you with reason but when you try to explain something generally when they're wrong or when they don't get it you need to break it down into very small steps and when you do that they keep asking you if you ever have a conversation with the two-year-old they'll keep asking you what is this right so if you say we'll go to the market what is market right you you realize that right they'll ask you what do you get in the market why do you go to the market all of those questions so i've often said that see if the meaning of all the nouns are clear and if you need to explain the nouns in what you have, this is one very basic trick. There are a bunch of small tricks I have like that, right? That set the context as much as possible and talk to a two-year-old and you will have a far greater success. Imagine that the two-year-old is talking back to you. You'll be able to write in my opinion fairly good prompts i think parenting is a big help in writing prompts seriously if you have a niece nephew who is very young right or if you have a dog trying to talk explain something to your dog you'll immediately i think in my view get a pretty good sense of how prompts should be created. Right? Do that. So you'll get it. I mean, what else can I say about it? So there are certain tricks, even she talks about it. They're very good tricks. But to me, intuitively, it boils down to talking to a two-year-old. The two-year-old who is a perfectly capable reasoning machine but doesn't have enough context of the world. Most of the time when I look at bad prompts I find either ambiguity, multiple interpretations, or I find insufficient context. And if you go back to what I said right in the beginning, transformers are nothing yeah probabilistic machines that pay attention attention are different weights to different things different tokens based on context you have to set the context the more crisp you are about the context the more likely you are to get a good result it just goes back the heart of the definition that these are attention machines. Asif, going back to Moushmi's problem though, should we then break down, maybe the question was too complicated for the hugging face model. Should we have broken it down into two questions and sort of led the system along the way. So maybe two sessions with the agent rather than one session. Now, very good thought, actually, Dilip. That's what I was planning to do quietly later on. Expand the prompt, break it up into many parts. Because the smaller the model, the more likely you are to have better results if you get if you make it a nice chain of thought right the word for it is cot chain of thought yeah break up the prompt into a sort of chain of thought so that is another paper we'll cover by the way the chain of thought paper we will cover in one of these sessions the chain of thought paper we will cover in one of these sessions when you save the embeddings like what we did in the first plan do you preserve the number of tokens that would pass through or just the ones that were important yeah so um patrick is asking a very interesting question he says when you convert text into embeddings, sentences into embeddings, do you preserve the number of tokens that went into the embedding or is the whole sentence one embedding? Did I get that question right? Oh, yes. Yeah. So the answer to that is, unfortunately, it's a lossy process. A whole sentence becomes a vector. From the vector, you can reverse backwards to infinitely many sentences which are semantically equivalent in that semantic space. Right so that's why it's sort of a hashing function in many ways semantically aware hashing function, so what you typically do is in the database you store the original text and you store the vector next to it. So when you find the nearest vector, you go back and return the text instead of the vector. You store it as a tuple. What if you only need parts of the text in that original lesson? Can I pick it up? Because I thought we'll sometimes just save this one in that original lesson. Can I pick it up? Because I thought we'll sometimes just see this one in Berlin. Yeah. So see, the question is, the definition of what a sentence is pretty ambiguous in the sense that in natural language processing. In English, it's pretty clear. You put a full stop or exclamation or a question mark, sentence is over. Period, right? But in NLP, it's more flexible. You can call a whole paragraph a sentence. So therefore, whatever sentence you fed into the sentence bird, let's say, or to the embedding maker, embedder, you need to store that, whether it's a paragraph or a sentence. Now what happens is the bigger the text you feed the more likely you to hit the limit for example you can't feed in a text more than 512 tokens to s bird yeah to some of the more bigger machines you you can go up to 4 000 big bird and so forth and nowadays 8 000 seems to be the limit right somebody was even talking about 16k, but I don't know what it was, or am I dreaming about it? But there's some thought of it. But these are giant, like you're saying, can I put an entire story as a single sentence for embedding? And it begs the question, at that level, is semantic embedding even meaningful? Right? So if you think about it, let's think about the story of the war and peace. So I once read a joke, some cartoon somewhere, there's a character who is very smart, and he's saying, I took a lesson in smart reading. I can read any book and summarize it in a minute. So somebody says, what about war and peace? Very simple. It's about Russian war with the French. Right. So much for the Russian. So much for the semantic embedding. So what is lost is just about the whole beauty of that story, all the human condition of war and the angst and the love stories and everything uh is lost right so that's that's somewhat similar to the semantic embedding you have to ask is it any more relevant so I would say that you should always break it into logical text boundaries like little paragraphs and embed those paragraphs so so for example let's say an alien came over and they gave us a letter their corpus of their alien language and i created an english to alien language dictionary um and then because because uh pocket ai is entering on this alien purpose. So now i'm gonna use launching to force to the dictionary. Should I embed each word? No, no, no word embeddings are bad because word embedding says it used to be so like 2,015, 2,016, and so forth. We now know that they don't work because word doesn't have enough context. The same word apple. Is it a computer? Is it a fruit? Or are you the sweetheart of apple of my eye? So things like that. So it has insufficient context. I mean, it's a dictionary. So it's not like a pair between the word and other? No, no. What you should do is you should take entire sentence translations. Yeah. And what you should say is this is alien language. It's English translation is this and it's embedding is this. I don't have embedding for the alien language, but I can indirectly find things by doing this bridge from alien to english to embedding and therefore i can find all the neighboring sentences in english right look at neighbors and then translate it back to alien and see what that means you got that right so it still has to carry the english embeddings for it to work. Yeah, well, because you have trained it on a vast amount of alien. So in other words, it becomes a last mile problem. All you need to do is have a way of translating from alien to English. But because you have trained a transformer with so much data in English, it would. Actually, that's the way. Forget about aliens coming. The embedding for most other languages is literally done like this because you don't have enough data for those languages i just said so just so it's not part of okay so yeah so prompt optimization how do you optimize prompts two ways one is the chain of thought right chain of thought. Chain of thought technique is exactly that. People found that if you use, if you break the problem down to small steps, explanatory steps, then these things do better. These models do better. In fact, they develop new abilities at which they were poor. Like for example, they were doing arithmetic wrong and they start doing arithmetic correctly. The original chain of thought paper that this one refers to, if you go to this paper, actually, let me see if I can click on it and go to it. Yeah, yeah. Chain of thought prompting elicits reasoning in large language models. This is a lovely paper. I actually covered it in one of my sessions. Yeah. So you notice that I want to give this as an example to illustrate the point of how you can write better prompts. Yeah, this should do this. This this example from the paper should bring home the point. Standard prompting is a Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has three tennis balls. How many tennis balls does he have now? Answer. You tell it the answer. The answer is 11. But what is missing in that is you haven't taught the language model how to reason. You see that, right? So it isn't able to learn from it. Then you ask it the next question. The cafeteria has 23 apples. If they use 20 to make lunch and bought six more, how many apples do they have? It will come out with a wrong answer. But the same thing, you give it with a bit more thought. Same question now, but in the answer, you give an explanation. You show how to reason in this form. You say Roger started with five balls, two cans of three tennis balls, each is six tennis balls. And you say five plus six is 11. So now the answer is 11. What have you done? You have shown how to go to the answer, right? You have shown the way you would do it to a two-year-old. And that's why I keep saying, talk to a two-year-old. Do you see that the difference between the thing on the left and the thing on the right is, the thing on the right is how you would have talked to your two-year-old explaining this, right? Do you see that, guys? Would you agree with that? Right. So you're giving it more context. You're giving it a way to reason through the process. And then you pose exactly the same question. You see this. This question is the same. And see, what is the model doing? It has learned your way of reasoning and it is following, imitating your way of reasoning. It says the cafeteria had 23 apples originally they used 20 to make lines so they have blah and they bought six more so they have now and they get the answer right right so this paper was quite illuminating when it came out like i covered it actually in one of my sessions in the past. So there is more to it, but you got the gist of it here. If you get this, one of the things that you see with this chain of reasoning is the accuracy of the answer goes up dramatically. And you can go through the whole paper, and I sort of metaphorically or in a more intuitive way, instead of using big words, chain of using, prompting and so forth, which are good words to use, but I used a slightly different word. I said, think of it as talking to a two year old. Does that make sense? The other thing people do is that when there is a, because of the stochastic nature of the output, what you can do is instead of generating one output, generate a few output, generate N outputs and take the majority output. If the answer is yes or no, or if you want to give grades, make it generate 100 grades and pick the average of the two. Realize that this can make OpenAI very, very, very rich soon. Or whosoever you're paying the bills to, and it can get you broke. OpenAI also charges for the length of the output? Both the input and the output. And output length is even more expensive than the input length. Right. So, so be careful. Remember, a few years back they were like okay, let's all move to cloud and then they realize cloud cost was very expensive because the Googles of the world were the ones who were making money and then they decided that maybe we should go back into the hybrid model where we keep something on-prem and something on the cloud. So I think something like this will happen here where after the hype settles and people realize the monthly bill is very expensive, then they kind of dial it back yeah no absolutely reality kicks in common sense kicks in eventually there is a reason here at support vectors we have a server room packed with machines it's far cheaper for us that we use our own machines rather than doing the cloud in fact I do we we only go to production in the cloud that's one recommendation i give as an architect keep your dev keep your testing keep your stress testing performance testing everything in-house don't go and do scalability testing in the cloud right otherwise your budget will get cloudy so do all that thing here but deploy to production in cloud because you will have you will need basically cloud is nothing but hardware plus um standardized it services right you don't have to maintain your own it team and worry about people leaving and payroll and so forth production keep it in the cloud pay the bill because you are not paying the bill the client is paying the bill your customers are paying the bill right suppose you make a software you sell a database as a service all the users of your database are paying the server the cost so that's that so another thing is break one big prompt into smaller simpler prompts would you agree with that guys this is again talking to a two-year-old yes you that is it so always have the mind frame that i'm talking to a two-year-old and a two-year-old is a reasoning machine but doesn't have context so you have to set the context and show the way by the way it's like think about it this way. If you have teaching experience, you're already a good prompt engineer. Yes, two year olds. Actually, you'll be surprised if you ever go to preschool or something, when you get some of these people who are absolutely wonderful with children, how you observe them patiently and you realize that they don't presume anything to the child they're explaining everything that to the adult is like of course right because they know that for the child it is not of course right so Actually, in my case, a little bit of a personal thing. My elder daughter, she grew up very close to us. We never sent our kids to daycare. We brought them up ourselves. And when my kid was one year old, she basically... I used to spend a lot of time at home. So she refused to go to preschool. So because I brought her up, I could see her mental development. It was very fascinating how children develop. with her. So it's a bit unusual, but I used to actually attend the entire preschool with her sitting next to her. She would be holding me for quite some time and she would be doing her painting on the aisle while holding me and making sure I'm nearby. And gradually she opened up and then she didn't need me anymore. It took her almost a year before she did that. But her teacher was very good, a teacher named Melanie. She had a knack to be friends with children, right? So if anybody could win her over, my daughter over, it was that teacher. And she did win her over so that I would quietly sneak out right after some time I would be there for about half an hour or so and then quietly sneak out and then go to work but in the process you see what it is to deal with minds that are absolutely fresh that are that are very fast, clever, right? Unburdened with conditioning at that moment. There's no context. There's no conditioning. There's no cultural baggage. They're just pure reasoning machines. Everything has to be reasoned. And when you talk to them, when you talk to a child who is two, three years old, you actually begin to realize how many assumptions you have in your mind. How much we become as adults, conditioned people. So, well, I shouldn't repeat that story anymore. My child is all grown up and now she's in the computer vision lab of a big company. So the cost is an important consideration. You know, the more detailed prompt you write, the more it costs you. And you can see the cost, they add up pretty quickly. For example, she gives the example that if DoorDash, which makes 10 billion predictions a day, were to use ChatGPT, they would be paying $40 million a day to ChatGPT to open AI. So what does it show guys? It shows that for real industrial application at scale, prompting is at best a work in progress. Isn't it? It may be impressive, but it's a work in progress. We need to do something. We need to build models that we can run locally? You'll be surprised how much money is being diverted to it. So that they are like they're sucking their like, open AI right, is getting any money it wants because all the startups are being funded with millions tens of millions and what are they doing they're just sitting and creating prompts and feeding money to open ai right so imagine at one end there's a there is the sand hill at sandpit lots of vcs sitting there and the other hand is open ai in between doesn't matter how many startups are there and what they're trying to make this money eventually comes here all industries don't you know the point is that you have to break the monopoly otherwise otherwise all we might as well send a paycheck directly to them otherwise otherwise all we might as well send a paycheck directly to them even taking it home so well chat gpt plus is 20 flat rate a month and we can have access to gpt4 so yeah can't you use that for experimentation yeah you should or even chat gpt 3.5 turbo and these things are pretty pretty good but in my view see this is all very temporary in my view within before summer is out before 4th of july let's say that by 4th of july there will be yet another independence day of a different sort we'll break through the monopoly i think so i don't know yeah because people are trying they're trying to who looks like uh who's lined up to be the best i think google has been having a lot of execution problems at this moment they screwed up a bit they will they will come through facebook will come through the guys will come through see at the end of it right who are the researchers it is the same community of researchers that keeps floating from company to company and going back and forth right it's a revolving door oh today you are here okay okay or tomorrow oh oh yeah we'll meet again in that company it's just, wherever the pay is more, they all run. Or they see more stocks, they run. So that's that. So I don't think there's anything. See, for example, if you look at Chad GPT, et cetera, I would say that it is just a marvelous execution. But fundamentally, I don't think that it is all that revolutionary beyond what Bard has done or what other models have done they will catch up so now comes another question remember last lab we did fine tuning of transformers now we did our predictions better by fine tuning and this lab we are all about prompting and creating better prompts to get good results so what are the alternatives you can either fine-tune a model you can either not use these models use something from scratch and train it or you could do prompting on these large models which is the superior way see it's a cost of upfront cost versus amortized cost like runtime runtime cost. Very much like cloud on-premise. On-premise, what do you do? You go and buy servers. The moment you want to buy servers, all the server makers, HPs and Dells, they start salivating. Super micro, they're all very happy. They see you spend 20, 30, $50 million. But after that, the only thing you have to pay is the OPEX, the operational expense. Some employee, the payroll cost, some electricity cost, some maintenance cost, but those are fairly low. But in cloud, there is no capital investment, there's no capex. They will even give you credits to attract you all of these crowd providers but once you're locked in right the opex keeps going up every year right and if you always the first thing is you always end up spending far more than you thought you would spend every single company says that the a bill is in spite of experience the bill is more than they expected now all the time it happens so you pay so this there's a somewhat of a vague slight analogy to that between prompting and fine tuning right in fine tuning you take your you take a model a checkpoint and you feed it label data right you put in effort and money to get label data. You put in computational resource to fine tune it, to train it. But once it is trained, it is a model that will do your task correctly. It won't do all the tasks of the world. LLM can be used for many things, but this will do your specific task very well, isn't it? That's what we learned till now. So it is more of a capex, right? You pay upfront cost. And after that, the runtime, after that, inferences are very cheap. Training is a bit expensive. Inferences are cheap, isn't it? But with prompting, it is the converse situation. There is generally no fine tuning. There's a little bit, but broadly there isn't much. It is all about prompt engineering. And the more better the answer you want, the bigger the prompts you create and the more expensive it gets, isn't it? So your OPEX goes up, right? Very much like cloud. You see that, right? So that's one way that I would say she I mean in her own way she makes this point so you take a pre-trained model you fine-tune it with inputs examples after that it's a very simple input output there is none of this creating a prompt with natural language and frankly guys if you can avoid natural language in your prompt, like writing long explanatory prompts, avoid it. When the dust has settled, I think people will realize that there is a place for fine tuning and there's a place for prompting. Prompting is good to start with, you know, to solve a problem for which you haven't invested enough effort to create a fine tune model. But the gold standard would be to solve a problem with a fine-tuned fine-tuned model that's my belief so it goes on and it shows so so one of the things that it says is that at one point so suppose you have a model large language model you can give a nice prompt and get an answer. Or you could take a pre-trained model, fine tune it with, let's say, n number of data points, and then make predictions. At what point will the predictions be comparable? A good prompt will beat just a pre-trained model because it's not trained on your task. Whereas you can write a prompt which is very specific to your task so at how much of fine tuning does a fine-tuned model meet and or meet or beat this and what she says is that it's a heuristic or empirical answer that approximately at about 100 data points, just 100 examples to learn from, a sample training set of just 100 for fine tuning, already fine tuning beats, a fine tuned model beats prompts. See, the cost of creating 100 samples cannot be too high, isn't it? Especially when you know that at the end of it you'll get better accuracy better model performance and lower cost than prompting something to bear in mind actually this was the one paragraph or the thing that i like the best, the benefits of fine tuning is twofold. You can get better model performance. In other words, with the example, the training example that you get to fine tune the model, that knowledge gets internalized in the model, right, forever. It changes the weights and biases. Second is you reduce the cost of prediction, obviously, right? So worth it. So there is a new though approach, which is midway between these two. It is called prompt tuning and that is something actually that's a hint to what we are going to cover in a subsequent day. I don't know whether we'll cover it in this workshop, I think so, or in a generative AI workshop because we may run out of time. We still have to do the visual. We haven't done the visuals at all. The computer vision stuff with natural language. You know, the multimodal learning. A lot of that waits to be done. But at some point we'll do it. So the idea is that, and by the way, this idea was introduced a long time ago in 2021. People were already researching these things. Yeah. So what you do is you don't change the prompt. You programmatically change the embedding of this prompt. And you feed the embeddings into your models, into your LLMs, into your transformers. You feed the embeddings. Think about it. That's what we are doing, isn't it? For sentence transformer, cross-encoded, etc. What are you doing? You're feeding embeddings and working off that. And you generate tokens from the embeddings. So LLMs don't get the prompts. They actually get embeddings. And so you bring in an in-between stage, you say, how do I convert prompts into the optimal embeddings? Because this begins to look like our first session, good semantic embedding. And if we get good semantic embedding, then what it does is it captures the meaning of what you're trying to say. Then two things happen if embeddings go into llm then obviously you're not paying cost per token and you could potentially have on-premise converted your prompts to embeddings pre-converted your prompts to embeddings and then use the embeddings at runtime to get uh even if you're using open ai model feed the embeddings into it and get gotten the results right so i said i thought you said you have to save the embeddings and the text yeah but but when you you do that for your search results but when you you feed it into an LLM, just give the embedding. Don't give it the text. So it's mathematical numbers? Yeah, vector, just a vector. Embedding is a vector, that's it. Embedding, remember, is a latent space into which a vector exists, a point exists. A point in a hidden space is an embedding. And the LLM returns also in the in the no it will return in tokens you can do that it doesn't you give it input as embedding a vector and your output you are expected to return tokens but this is not one zero shot this is you have to do some training and so on and so and so on and so forth. So prompt tuning is something people are looking at pretty carefully. In fact, llama and the alpaca, the Stanford's alpaca was exactly that. It was an exercise in prompt tuning. exercise in prompt tuning. For fine tuning, they used... Oh, no, no, no, sorry. Not that. I take it back. This was fine tuning with distillation. So you want to fine tune the model. What you do is you make a bigger model, make predictions. Train the bigger model, get predictions, and then fine tune a smaller open source language model on examples generated by large language model so let's go through the steps what you do is you have a large language model it works it works great remember this is your standard teacher student thing then you generate a lot of pass a lot of inputs to it. You look at its response. Now what can you do? They become, it's exactly the distillation process. You can take a much smaller student model and train it to learn from that. Now you fine tune a smaller model. So this is classic distillation. And when you distillation, alpaca is the distillation. Fine tuning with distillation and when you distillation alpaca is the distillation fine tuning with distillation okay so it's very interesting that the appeal of this approach is obvious after three weeks the github repository got 20k stars okay this is how fast it rose. So now we are getting into some new territory. One direction that I find very promising is to use LLMs to generate embeddings and build your on top of these embeddings. This is where we started our course. We said that if you generate embeddings, semantical embeddings, then with embeddings, you can do search, you can do recommendation systems, you can do many, many things. So the idea is we were using S-Bird now, which is a language model, big language model and so forth. You can just keep scaling the language models bigger and bigger and bigger. Ultimately, you get the embeddings and go for it. So to me, the fact that we got semantic embeddings and use that for search is sort of a bedrock. It's a quintessential example that you can use in many situations. And this is what it is saying. She goes on to say that 2021 was the year of graph databases. 2023 will be the year of vector databases right so then i won't go into the rest of it i'll let you go over it agents tools and control flow did we just cover all of that in lanchean no i won't go into it so take the example if you want to uh like using natural language query database right how would you do that somebody describes to you a task right get me data how many unique merchants are there in Phoenix and what are their names? So what do you have to do? You have to convert natural language to SQL, SQL to then execute the SQL, and then convert the results back into natural language. Isn't it? The first step and the last step are basically AI processes, LLMs. The in-between SQL executor is SQL executor. Nothing magical to it. It's just a task. So you have broken it up into multiple tasks and done it. So we just covered this in our day, right? Now, I will leave the rest of it aside. These are about the use cases that are coming up quite a bit these days. And I'll let you read these things on your own. But one thing that is there in this summary and I'll let you read these things on your own. But one thing that is there in this summary that she says I found interesting was, yeah, so one of the things people are using LLMs to is that they're good at writing prompts for themselves. So for example, for themselves and for other LLMs. So for example, you want to create a really good picture from what is that generative art thing called? Mid-journey. Mid-journey, right. You want a good prompt for mid-journey, you ask your GPT to create a good prompt for mid-journey and feed it into mid-journey and you'll get a picture. And it can sometimes, right, see, here's the thing, guys. In my view, right, I didn't see that much value because once you get into the mentality of talking to a two-year-old you know that mindset you write pretty good prompts and when you compare it to what chat gpt writes you may not find much marginal benefit by saving it through chat gpt if you don't know about the target then you want to conceive the idea about that yeah exactly so then you can ask chat gpt and it will do some things for you so by the way the stable diffusion people are now their own company stability ai that's right yeah oh they're yeah they're coming out with the Excel model now, which is pretty good. Patrick just mentioned to me that they already have an Excel model, which we hope shows up in Hugging Face soon. So anyway, you can read this article, Large Language Models Are Human Level Prompt Engineers, or what it is worth. It's certainly worth or interesting though my my opinion is that once you get used to intelligent prompt engineering it doesn't take long you know especially if you have a background in teaching children if you have little kids at home you have your own kids your nephews nieces etc i think you're inherently good prompting genius. Shalini, would you agree from your experience there in your company? I would say so. However, I think it's getting slightly more complicated than that. So it's, yeah, well, I think I was telling Kate, by the end of this year things would look completely different because things are moving so quickly a lot of people with a lot of ideas yeah yeah it is certainly true there are a lot of creative ideas so anyway guys this paper again is worth reading um play around with it see what it does a lot of data backing it up and saying it's good and this and that so get into the habit of reading papers guys there was a time i used to read one paper a week one or two papers a week now the things are moving so fast i find that almost every day i'm looking at one new paper it's just and still i'm in a catch up mode. I don't know how people catch up. I have a hard time catching up. I have a really running as fast as I can reading all this. Most of these papers are very easy to read after this course, especially after the NLP workshop. I hope you would agree that papers are beginning to look easier and easier to read. They're easy. So you just don't need too much time in an hour or two. You make your way through most papers, but you have to get into the habit of carving out that time. You need to get into that habit here. And of course, know your fundamentals, know your machine learning well. Hopefully in the Sunday session, we'll have somebody present a summary or something. Yeah. Yes. We'll do that. session we'll you know we'll have somebody present a summary or something um yeah yes we'll do that all right guys so it is time past time actually so I will stop the recording you guys can ask questions