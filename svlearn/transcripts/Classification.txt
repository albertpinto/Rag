 Thanks, guys. Give me a second. All right, welcome back everyone guys and I wish and I hope you had a good Dashera and there was one more Indian festival I believe Eid in South India so and the Navratri and many things in between so welcome back after all of that I will start today by reviewing what we did the last time. By last time I mean two weeks ago. Now in the previous week, which we didn't hold a session because of the Dashena slash Navratri festival, we were supposed to do a lab on multivariate regression. Means how do you do regression when there are many predictors involved. Today, actually I'm not going to do that lab today. I'm instead going to do classification, which is an important topic. And in the next two weeks we are going to do labs on both regression multivariate regression and on classification so we'll have two subsequent weeks that are just labs after that we will eventually have a little bit more on classification and then we'll deal with clustering and that will that will sort of sum up the scope of this particular workshop. So coming back to what we did last time, let's have a quick review. The previous time we talked about regression and understanding regression a little bit more in detail. So we said, if you're seeing my screen, that a regression model is sort of like a machine. You give it as input a set of features, X1, X2, all the way to XD. Let's say D different predictors. An example could be on a beach. It could be the temperature at the beach, the wind speed at the beach, the day of the week, and then the output is, you're trying to predict how much ice cream would be sold by your shop if you are an entrepreneur, how much ice cream will you sell on that given day to the children on the beach? So keep that one example sort of in your mind and it will help you concretize our discussion. Now, regression is considered part, it is a quintessence of predictive modeling because you are predicting something. The target variable that you're predicting, the output that you're predicting is a number, right? So it belongs to the field of, so this y, now a little bit of notation, I will just enter this, This capital R with an extra bar attached to it, with a sort of unnecessary bar attached to it. This is in the field of machine learning and in mathematics in general. This is a symbol of real numbers. Real numbers. Real numbers are any kinds of numbers, rational or irrational, negative or positive, fractional or whole, between minus infinity and plus infinity, any number. Basically, your concept of all that is a number. The word real stands for the fact that it is all along from minus infinity to plus infinity, but it excludes complex numbers or imaginary numbers. It's only real numbers. So when we take a regression model, in the simplest form, you can think of it as a machine that takes inputs and manufactures or somehow magically grinds the machinery inside it and then comes out with a number. Are we together? Like, for example, how much ice cream would an entrepreneur sell on the beach? That is a number and that's what it comes out with. Now with that conception in mind, we became familiar with many words that people use in different textbooks. The inputs are called, well I call them inputs, they're also called features, they're also called predictors. They're called independent variables, the regressors. And the response, the output, is often called the target variable, the output, the response. These are the three words that you will see me use for output. In statistical literature, you also see the word dependent variable. variable. Very rarely do you see the word regressant used, but you might. So these are all the words, more or less they are synonyms, and people in different traditions use different words for that. And the input side, same as to features, inputs, predictors, I would be using them quite often. Very rarely would I use words like independent variable and almost never use the word regressor and things like that. On the other hand, classification we realized was exactly the same kind of machine, except that what you're predicting is not a number. You're identifying the object, the attribute, whenever you're given features, you're saying these features belong to an object that is, for example, a cat, a dog, a duck, a cow, or something like that. In other words, you're identifying a type or predicting that these features belong to an object of a given type. So you're identifying a class. And that is classification. That's the word classification. Both of these classification and regression to the extent that they predict, they're called predictive models. They are also called supervised learning algorithms. The word supervised learning algorithm comes from the fact that you have to, this machinery will not work till it has learned, till you have trained it. And to train it, you need to give it training data. As we saw in the lab, you give it training data, it learns from the training data, it fits to it. And then it figures out the relationship between the input and output, it comes up with some internal conception of it, and then uses it to generalize to be now able to predict the value for any given input that it has not seen. So such algorithms are supervised learning algorithms. As we will learn later on, there are other classes of machine learning algorithms. For example, there is pattern recognition. Pattern recognition is not in the business of predicting anything, but it's looking at the data, and it is looking for some interesting pattern in the data. A quintessential example of that is clustering. It happens to see if there are clusters present in the data, so the data is not randomly distributed, but it's sort of forming clusters. It will detect that. Another form of pattern recognition is, I mean, sorry, yeah, pattern recognition would be dimensionality reduction, to notice that the data actually belongs to a lower dimensional space, and so on and so forth. And then there are generative models and many things, very interesting sorts of model. So these sort of algorithms are called unsupervised learning because there is no giving training the model. The model gets the data and recognizes patterns and tells you about it. It does not after that go and make any predictions as such. So that is unsupervised learning. There is a third form of learning which is called reinforcement learning it is the the sort of carrot and stick learning that we as children are used to receiving from our teachers you know uh in the early school years uh good good results positive behavior we get rewarded we get carrots and then if we don't behave ourselves or we do something that is Karatun So when you teach an algorithm, you give an algorithm and say, go play this game, but you don't give many rules. But what you say is if you do some things right, if the outcome is good, you get a reward. If the outcome is bad, there's a penalty. And then the algorithm learns from it. That is reinforcement learning. Now, all of these things are very obviously these are at the heart of this is machine learning these three the broad areas of machine learning uh they are just about everywhere we look and breakthroughs are happening all the time in the two weeks that we haven't met and we are meeting now some of you may have heard uh tesla one of the makers of uh they're trying to, the electric car company, actually here in my city, it's very close to my house, they make these lovely cars and they strive to make self-driven cars, fully automated cars. Now fully automated is a distant dream still, but recently they had quite a big breakthrough and these cars are able to drive in the city on their own, weave through the traffic, come to stoplights, avoid dangers, avoid parked cars and so forth. And they're able to do all of that on their own. And of course they're able to drive on the highway on their own. So it was a huge step forward in the field, in the space of, or in the journey towards automated driving. Now when you look at those breakthroughs and you see, and you ask which form of machine learning is being used, you'll realize that those are very complex Bs and there's a lot of learning that's happening, supervised learning, that is predictive modeling or pattern or object detection. It is looking at the road and it is detecting the presence of cars, the presence of people, and empty spaces that it can drive through. It is deciding that, yes, the road is free, the street is open and that it can go. It is detecting the traffic light and not only detecting the traffic light it's trying to it is able to figure out whether the traffic light is allowing you to go move forward or not yellow green red and so forth whether you should get whether you should turn left right and so forth it is making a lot of decisions you give it an end point and it is weaving through the traffic figuring out a path from here to there through all this traffic using the maps and going there when you do things on this grand scale obviously it is not just a little bit of machine learning but a whole lot of machine learning that goes into it if you stay with me I, I guess this one, I don't know whether you guys will continue or not, but the bash that is with me, for example, on Mondays and Wednesdays, they are, for example, going to be doing in just the next month, object detection and seeing how you can train a neural network to recognize, given a picture of the street, for example, it can recognize all the objects in it. It can recognize the animals, everything in it, in any picture. And you begin to then decompose or see how these very complicated AI machines work. In the few weeks that we are talking about, there's been, I mean, all sorts of breakthroughs keep happening. One person has trained an AI machine to detect whether a person has COVID, this coronavirus, by just recording the sound of the cuff, the sneeze, and passing it through an AI neural network, basically a regression model. And the regression model will score how likely it is that you do have COVID and things like that. So breakthroughs are coming all the time, and it's really good to stay in touch. Speaking of which, I'd like to remind you that every Sunday at noon California time, which is actually brutal, I suppose, for India time, it's 12.30 at night on Sunday, we do have a research paper reading. Every week we read one research paper. I announce ahead of time which research paper we'll read, and we then discuss the breakthrough. So this may not be of much interest to people who are in India because the hours are rather brutal, but for the California people, in case you're not aware, just as a for your information, we do have a research paper reading section every Sunday. Also at this stage, if this is your first engagement with machine learning it may be a little premature to start getting involved with that but whenever you guys feel confident you're welcome so we talked about regression and now we took the example of a linear regression in the linear regression we try to fit a straight line to data when we try to fit a straight line to data, so suppose you have data like this, there can be many straight lines that can fit. Now, the question is, how do you find the best fit straight line? And so that brought us to the concept of data and the parameters, the hypothesis, the hypothesis space and the parameters there. And how do you quantify the error? If you remember, we talked that the gap between prediction and reality is the residual error. And you need some way to accumulate or aggregate these residuals to quantify the error. And the two ways that we learned about was the mean absolute error. You just take the absolute values of the errors or you take the mean squared errors. You square the errors and you take it. When you square the errors, it is the most common way of doing it, mean squared error. For many reasons, it's a preferred way. In fact, I mentioned this mathematician, great mathematician, Carl Frederick Gauss. Gauss is of course considered the Prince of mathematics. He's a person obviously of the same caliber but in a different area as India's Ramanujan. Ramanujan was perhaps the greatest number theorist in existence who ever existed. In the same way, Gauss is considered to be the prince of mathematics. He made prolific contributions to analytical mathematics, the calculus kind of math. But anyway, so one of his results was he proved that if you have to choose a way to quantify error, then under certain circumstances, well-behaved circumstances, to quantify error, then under certain circumstances, well-behaved circumstances, it is best to pick the mean squared error as your measure of error and then try to minimize it. That is the least square method, the method of least squares. And that actually happens to be the Gauss-Markov theorem we talked about last time for anybody who is mathematically inclined, but we won't get there so given the error the question is how do you get to the bottom how do you minimize the error you can't just be drawing random lines and checking which one best fits your data so what you do is you actually go into the hypothesis space and you do something called gradient descent so to understand gradient descent, I talked about function. We talked about the concept of slope. The slope is nothing but how much height you gain or climb up if you move unit distance in the positive direction. So let's say that you move one step in the positive direction, and you gained height of three units of height. So you would say your slope is three. That's what slope is. And that's a geometrical way of looking at it. And in a more algebraic way, people call it is the derivative of the function or the curve that represents the, the, the, the path on which you are walking. Okay. It gives you the S the of that path, or it is the derivative, right? The steepness is the derivative of that and so people have created a lot of of courses you may have gone through in school. If you did do calculus, there's the whole bit of machinery that talks about differential calculus and at the end of it once you once you put the machinery the mechanics of differentiating all sorts of different kinds of function the bottom idea is just a simple idea that given any function if you were if you were a little ant walking along that function at any given point how much altitude would you gain if you move forward a unit distance so that is that and that is the derivative now we talked a little bit about that and we realized that if you want to move to the minima of a function like if you want to go home which which is the point at which the function achieves a minima the best way to do that is to go against the slope. And this is a recap of the last time. We came to this rule, this rule, which basically said that your next x value should be your previous x value, and then go against the slope and where alpha is the learning rate and when you do that you have the famous equation of the gradient descent because when you generalize this idea to higher dimension functions the generalization of slope is something called gradient and you have the gradient descent so machine learning at the end of the day is putting together two very simple ideas. It is first trying to create a function that maps the input to the output, right? And then it says basically that function will have all sorts of parameters. Then you go into the parameter space, you pick some function, some set of values for the parameter, let's say some straight line or something like that. Then you calculate how much error there is. For example, in the case of linear regression, you pick the, excuse me, the mean squared error, the sum squared error. Then the next question is how do you minimize the error? Learning is the minimization of the error as we talked about. And so that is the process of gradient descent. And much of machine learning can be reduced to the learn, the word learning is often a reduction of some objective function. In this particular case, the objective function is just your sum squared error or the mean squared error. So that is a recap of our last time. We also talked about this convention that we'll follow. Whenever we talk of data, we will use Roman letters, X, Y, Z, A, B, C. But when we talk about concepts or parts of a concept, like for example, a straight line that fits the data, a straight line is a hypothesis. It's your idea. It's your theory. And so the pieces that make up the theory, namely the intercept and the slope, those are the parameters of your model. We will use Greek letters. We'll use alpha, beta, gamma, and so forth. In particular, your textbook tends to favor the betas very much. You see the betas here, beta 1 and beta 2, as the intercept, beta naught is the intercept and beta 1 is the slope. That's how it uses. And so this is a question that when you look at the error surface, the error surface seems to have a parabolic structure, parabolite structure, and the shortest path home is the path of gradient descent. And you can do that to find the best value of that. The next concept we learned about last time was that of bias-variance trade-off. So what are the bias and variance represents? The two kinds of disease or two kinds of errors that you can have in your model. So we talked about, basically, errors that you can do something about and then errors that you cannot do anything about. So these errors are the reducible errors. So as you play around with your models, different models, you will have some bias errors and some variance errors, but your model will also have irreducible errors. The irreducible errors will come from noise in the data. They will come from just practical mis-measurements, and it will also come from the fact or acknowledging that there are factors not given to you in the data that are effective. For example, if temperature is the only thing that you are given in a data, in a spreadsheet, and the amount of ice cream sold, it is very hard because under the same circumstances, the same temperature, how windy it is, will determine how many children will play on the beach. Whether it is a workday or a holiday will determine how many children come to the beach because if the parents are busy at work and the children are busy in school, they are not coming to the beach. So for the same temperature, you'll see a variation of readings, how much ice cream you were able to sell right and so suppose you don't have that data you don't have other data like day of the week and the wind speed etc then in your model you will have irreducible error acknowledging the fact that the model that the data does not capture the whole reality right so in a way it is that which the model does not know at all, because the data doesn't have support for it. So removing the irreducible error, which is coming from those sources, the reducible error, the error that you can do something about by picking better hypotheses, making better models. Those your model errors fall under two types, the bias errors and the variance errors. I give you guys an intuition of bias and variance. So if you look at this picture, let me call A and B and C. A is what you would ideally like. Suppose you're shooting at a dart boat, you're hitting very close to the mark, to the target. B happens to be what you call the bias errors. In other words, you're hitting actually very well, except that you're shifted, you're hitting at the wrong bullseye. To you, you're hitting at the wrong place or aiming for the wrong place, but you're getting to that wrong place quite well. So those errors are bias errors. And then you have the variance errors when where you're hitting is all over the place. You have a huge variability in the location, the prediction of your data so it is all over the place and so that is high variance errors like the c represents the situation of why for variance errors so high bias errors tend to appear in models that are overtly simple high variance errors tend to be in models that are overly complex. And so we came to, it is worth reviewing, this famous diagram in machine learning that keeps happening all the time. By the way, it also happens to be one of the most asked interview questions in data science interviews. For some reason, it's a favorite of people. And perhaps for a good reason, because it just validates, you know, this core idea of machine learning. It basically says that as you make your model very complex, your bias errors will come down, but your variance errors will go up. It will start overfitting the model and you will, but because your total error, which is the green line here, is the sum total of bias and variance errors, you will reach a point of complexity at this point that we have drawn where you have the optimal model, a model in which you have the least total error, in which the forces are neither too simple nor too complicated. And if you inspect your data, if you go back and if you happen to know what your data looks like or what the force behind it is, you'll often find that this point of lowest total error, the balance between bias and variance error, builds a model that pretty much resonates with the ground truth. And so that is that now uh there's another last thing i talked about so bias and variance you tend to have sort of a trade-off variance is overfitting making overly complex models now let's come to a principle the last thing we talked about was the occam's razor about it was the occam's razor principle what was the occam's razor principle it said that suppose a two theories can explain a data and maybe make equally good generalizations from the data then science defines the has a definition for the correct one it says because in the absence of any there is no such thing as see mathematics is correct and not correct right 2 plus 2 is 4 2 plus 2 is not 11 and so forth in in ordinary arithmetic there are systems where that could be true in ordinary arithmetic, 2 plus 2. It happens to be 4, but not 11. You say 11 is incorrect, 4 is correct. Science is different in the sense that you can tell if an answer is wrong, but you can never tell if an answer is right. There's no right in science. So in science, we define right by construction. We say that if two models are better than all other models, but they are and they are equally good in terms of their predictive power and so forth, in terms of generalization or explaining data, then the correct one is defined as that one, which is the simpler of the two. So that is the Occam's Razor Principle, and that's very important. So whenever you build models, you have to ask yourself, that is there a model that is equally effective, equally good at making predictions and generalizations and so forth, but which is simpler than what I have built? Because if you have, then you hop to that and say, well, this is a better model. This is a more correct model in some sense. And one way to illustrate that is something like this. Suppose you have two models for helping people manage their diabetes. One model says if somebody has diabetes and this is the amount of sugar level they have, they need to do a whole complicated regimen, wake up exactly at six, between six and six thirty, and then do this much of food and this much of exercise and this much of walk. And then it's a very complicated regimented system that the model predicts or prescribes. So imagine going to a doctor who gives you this very complicated regime. The second model is, and let's say that another doctor is using a simpler model, and that model basically says if you have diabetes, you better exercise more and eat less, you better exercise more and eat less, cut down on food and sleep more. Exercise more, sleep more, eat less, less carbs. Now which of these would you consider a more likely advice to be followed? You would agree that if a doctor gives you simpler, understandable advice that follow just three things, sleep well, exercise more than whatever you're exercising, and eat less than what you're eating, and that will make a huge improvement on your type 2 diabetes, adult onset diabetes. You would say, well, that is a clear instruction I can go follow. But if the doctor gives you a very long and complicated regimen that must wake up at this time and eat this and so much of spinach and so much of rasam and no more than this, then you would agree that you would be able to follow it for a bit, but ultimately give up. You'll get completely lost in the details. get completely lost in the details. So that's the difference between simple and complicated models. So always simpler is the preferred one. All right guys, so that is a summary of what we did last time. I believe I also gave this example of the solar system. The Kepler solar system puts the sun at one of the focal points of ellipsoidal orbit, ellipse orbit, and the planets are in elliptical orbit right which is much simpler than the old ptolemies astronomy which put earth as the center and all these planets and stars they seem to be somehow wobbling around the heavens you know in sorts of these epicycles. So calculations were a lot more complicated, whereas the Kepler model made the calculations a whole lot simpler. So from a science perspective, which is correct? So the thing is, definition, so you can say that it's just a matter of perspective. In the universe, there is no stationary object. The sun itself is not sitting quiet. It is just a star in the galaxy. And so it is moving, spinning around in the galaxy. Then the galaxy itself is not stable. It is moving around in the universe and so on and so forth. So everything is in flux everything is in motion so what difference does it make whether you take earth as the center or the sun as the center and so the scientific answer there is that now it is preferable to put the sun at the center of a planetary system because it leads to much much simpler models much which makes calculations much easier, much simpler. It is a conception that helps you reason about our solar system and the universe in a much more elegant and simpler way than using the old model of Ptolemy, where the Earth used to be stationary and everything is moving around the earth. And that is how you look at science. So that is a summary that we did the last time. Now I've taken about half an hour, 40 minutes to explaining regression. It was worth doing that. Today what I would like to do is now move on to another class of predictive models called classification. Are we together? Classification. Before I move on to it, I would like to take any questions that you guys have on regression before we close the chapter on it. We are closing the chapter on the theory of it guys, but not on the practical. We will do one more practical lab on regression. Any questions guys you're all very quiet so one announcement to make uh anil pandey is here uh he's a very senior architect in silicon valley uh he has made a lot of contributions to a lot of very important projects in his career. And he has graciously agreed to be essentially a guide to all of you and help you with your labs. So if any one of you are having problems, please reach out in Slack to Anil Pandey. He'll help you with that. All right, so I'll help you with that. Alright, so I'll repeat what I said. Anil Pandey is here and he's graciously agreed to be to help you guys with the lab. He's the teaching assistant and he's really very senior here in Silicon Valley. He has achieved a lot, made major contributions to many, many important projects. He has actually contributed, I think being the leaderboard of Kaggle competitions and so forth. And so it is, reach out to him, he's an excellent resource. He's here in this session and you can reach out to him for help yeah thank you asif yeah yeah so you can reach out to me like if there are any questions thank you okay thank you thank you anil thank you Thank you, Anil. Thank you. All right, guys. So with that, I would like to give exactly five minutes break. It's 10, how much is it? 11.16. Could we just take a five minutes break before we start with the next topic? Make sure, just think about regression in your mind. Close your eyes. Think about regression. See if you basically have the concepts right. See, guys come and go today you are doing the lab with scikit-learn tomorrow it would be something else today you are using python quite likely in five in ten years you won't be using python you'll be using something else but it is the it is the conceptual understanding of a subject that stays with you for the rest of your life programming or this programming in data science is anyway easy but even to the extent that it is it keeps coming and going in my generation i used to do programming in what is today called machine learning or data science a guess in which language can anybody guess a guess in which language can anybody guess any guesses guys what one exactly i used to do i spent a lifetime doing my life in fortune and despite what people will tell you a fortran is actually not dead. As you can see on my desk is a book on Fortran, Modern Fortran. The word modern should give you an idea that Fortran is not dead, despite what people say. Most of the mission critical computations today are still being done in Fortran around the world, though it's not much used in commercial space because you don't expect people in the industry to be that deeply or scientifically trained. So when you do scikit and numpy, you may not realize it, you're doing it in Python, but actually you're doing it in Fortran because your Python code is just falling down to the Fortran and C libraries. just falling down to the Fortran and C libraries. So a lot of the code even today is always written in Fortran C, C++, right? And these libraries that we learn in data science, the Python, the R or the Julia, the three popular scientific computer, the data science libraries, in many ways they are layers on top of the mountains of work that generations of scientists, thousands, armies, like tens of thousands of very, very talented researchers and scientists and mathematicians and physicists have contributed over the last close to 50 years and 50 years of intellectual property has accumulated so those traditions are not going away but the one thing to know is that a while C C++ and Fortran are sort of there forever in many ways languages come and For example, SAS used to be very popular for statistical analysis or what we call data science today. It was a proprietary language. It's more or less fading out outside a small circle of commercial shops. In the same way, there is still a very popular language, two languages, MATLAB and Mathematica. They were very popular in academic circles in the industry for doing what today we call data science. But now you don't hear that much about it outside academia. And what were they? Was SAS or MATLAB or Mathematica, just completely new frameworks from the ground up, new languages, no, they were all sitting upon the same Fortran C, C++ libraries that people in my generation were creating and those libraries are still going strong. Today, we have dominant towards R and Python. Julia is the young kid on the block, gradually gaining mindshare. And in my view, it will take over after some time. But these libraries come and go. The foundational concepts remain the same. For example, the notion of expressing error as sum squared error or mean squared error. This concept was created by Gauss, the mathematician in 1793. Just think about that. Then Galton and Pearson, or when they use the word, they coined the word regression coming from regression towards the mean, a concept there, they observed in biology so we are looking at close to 200 years ago right so these ideas have stayed and mathematical ideas deep ideas come slowly and when they come they come sort of with a ring of eternity to it so it's important to understand the concepts libraries will come and go okay are we together so what i will do with that long explanation i would suggest that you take five minutes to just reason through in your mind what is it that you have learned i'll take a break of five minutes i'll drink some water come back and then we'll start on a new topic which is classification and so i will stop this particular video so because this was a review and we'll start a new topic after this Thank you. okay hey I can see. Okay. Gracias. Thank you. Thank you. Thank you. um Gracias. so please Give me two minutes guys. So any questions so far on regression? This is the last time to ask questions on regression. Anyone? Alright guys, so in that case, I'll get started. I'll get started. I'm starting the recording again. The topic now is classification. Classification is a predictive algorithm in the sense that we talked about inputs go in and a prediction comes out. It is as a predictive algorithm, it is also a supervised learning algorithm. In other words, there are phases. You have to first train the machine, the algorithm to build a model. Out of that model, then you use it to make predictions or to generalize it to new data and make predictions on new data. So what does classification do? As we discussed a little while ago, classification is used to identify the object or the class of objects that this data belongs to. So for example, if I tell you the height and the weight and the certain features of an animal, so those are the features, those are the inputs. And if you have to predict or identify what animals do these features belong to, that exercise is an exercise in classification. So I'll write it there in a little bit more formal terms now. Let's say that you, the basic premise is you write it in, first let me write it in a little bit more formal way so that it remains what happens is you have a x vector go in and what comes out is a y hat which belongs to where y hat belongs to some class g says that let's say cat dog etc right some animal class let us say and these features be, this is the feature vector. So that's putting it a bit more formally and more mathematically in a succinct way, but think of it in a more simple, like if that is not the way you want to think of it, think of D features, given an animal, given some animal, the first feature could be the height, the second could be the weight, the third could be the age, the fourth could be whether that animal has wings or not, and the next could be whether the animal has a tail or not, and the next could be how many legs does the animal have and what you have to do is you have to predict why hat remember predictions wear a hat why hat and you have to tell is it a cat is it a dog is it a horse right is it a cow what is it so you have to pick from a finite set pick from a pick from a finite set G. From a finite set G you have to pick. So that is an exercise in classification. Now there are many classifiers out there, lots of them if you ask how many classifiers there are. There are like huge number, almost countless number of classifiers that are out there. Then you ask this question, how do we learn about all of them? So the thing is, you never learn about all of them. You learn on a need-to-learn basis. But you need to know, once you understand how classifiers behave behave it is straightforward to pick up a few and then pick up the rest as the need arises right based on your problem set based on what you're doing you will use the appropriate classifier so when a classifier makes a prediction so let's play a little game and we say that there is a classifier which is given the data, and you have to tell whether it is a cow. Let's say it's a cow. It's a duck. Actually, let me just take two, just to keep things simple. So this is the reality. Reality is this ground truth is this ground truth is this but suppose you make a prediction model from model suppose you have that and let us say that you had, let's take some number. You had for the sake of simplicity, assume that there were 50 cows and 50 ducks, equal number of cows and ducks that you use as for training data or for test data. Let's say that you have trained a model M, class model m classifier and then you you you test it out on 50 cows and 50 ducks so what can happen is let's say that the prediction is cow cow hat or the prediction is duck duck hat right so let's say that your model predicts about, let's take an example, some random number, I'll take 40. 40 cows as cows, but it confuses 10 cows as ducks. And likewise of the ducks, let's say that it confuses about five of the ducks as cows and 45 of the ducks, it gets right as as a duck so does this matrix make sense guys to you right uh you you notice that i broke up the the 50 cows between where like this row is what that data instance was predicted by your model to be so you're saying that you showed it 50 cows and the model got 40 of the cows right, as cows, but it confused 10 of the cows to be ducks. Likewise, you gave it 50 ducks, but then it confused five of the ducks to be cows, but it got 45 of the ducks to be right. Now, is this a good thing like where are the mistakes let's think about it uh would you say or would you agree that this is a good thing to identify duck as duck if your model identifies lots of ducks as ducks that is a good thing isn't it guys likewise if it identifies cows as cows that is a good thing on the other hand this is bad these are your mistakes a duck being confused to be a cow or a cow being confused to be a duck that is a bad thing those are mistakes red ones thing those are mistakes red ones mistakes or the word that we use are errors these are errors and the green ones are correct answers or correct predictions. Would you agree guys? I need some feedback. I want someone to say that they are understanding this and it's looking familiar, simple. Yeah. Yes sir. It is simple, right? Okay, so in that case, now you ask yourself, what is the definition of a good model? How well is this model performing? In the case of regression, we had this mean squared error. In the case of classification, data, total data size. So what was the total data size? What's our denominator guys? How many animals do we have? Two. No, no, no. How many total number of animals did we try this model on hundred hundred hundreds there are two classes but there are hundred animals and how many of those did we get right Okay. Come again? 85. 85, right? 45, 40 plus 45 is equal to 85. And so you say, this is your basic sort of metric of the model. The name for this is accuracy. Accuracy. Accuracy. Are we together? Accuracy is defined. So here the accuracy would be 85%. Now, is 85 good or bad? It depends on your situation. One of the questions that keeps coming up is, this accuracy I got, is that accuracy good or bad? this accuracy I got, is that accuracy good or bad? And the answer surprisingly is that it depends. Is 85 good, 85 bad? Depends on the problem you're trying to solve. It also depends on the data asymmetry. So let us say that I changed the problem. I made the problem that of detecting cancer. Now, and by the way, this is based on real data, real things that happened in the late early 90s. So there is a disease and this disease is well studied. And one of the early success stories with machine learning or AI, there is a disease called, I mean, obviously cancer is a terrible disease. In women, the fear often is of something called breast cancer. It is one thing that all women fear, just as men fear, I suppose, prostate cancer. Sooner or later, a large number of people tend to go through it. Now, when you do have lumps or these things in women, you want to hear that it's a benign lump. Benign means it is not cancerous. So in the US, at least, women all go, usually once they're in their 20s, they go for something called mammograms. They get x-rays and if there is a lump felt or found in the mammogram, then generally it means that you need to go through further testing. Now, most women, if the lump is found, it is found that the lump is a lump, nothing to be worried about. But in some cases, those lumps, they need further study because they may be cancerous. You may actually have breast cancer. So when you do that breast cancer studies, the history of the field is that they used to do pretty invasive tests. In other words, what would happen is if they found a lump in the previous century for the longest time, they would literally cut open the breast, leaving a big scar, go into that lump area, see what it is, take core biopsies there, and then come out with the tissue, send it to histological exam. And then it turns out that you don't have cancer. Well, the good news is you don't have cancer. The bad news is now you have a scar on your body, like a pretty invasive process. So obviously, there was a need for less invasive processes. So the search for less invasive processes meant that can you just instead of opening up a person just go in with a fine needle, go to the lump and aspirate out some tissue. Aspirate means to suck out a little bit of the tissue and then you look at the tissue under the microscope and you decide whether the person has a tumor is it cancerous or benign okay which is a huge improvement because taking an injection is far less invasive than having a full-blown surgery that opens opens you up and leaves a scar. So that was the process. The process is called fine needle aspiration. When you do fine needle aspiration, then you look at the tissue under the microscope. The problem that came is that the reliability amongst the diagnostician, the pathologist looking at it was very poor. The same person, one pathologist would mark it as malignant, another would mark it as benign, and it's a lot of error rate there. One of the things that people asked is, can we do better? Can machine learning create a model that can quickly and with equal reliability or more reliability, I mean, reliability equal to some of the more trained pathologists and certainly better than the average pathologists tell whether the markers indicate a benign tumor or a malignant tumor. And so there was a breakthrough in the early 90s that made it possible. That is actually one of your homeworks. You will do that exercise in this particular workshop but the reason I brought it up is for a slightly different reason look at the situation when most of you go let us say that you go for your annual checkup and you're a woman and if it is a man I guess you go for a prostate checkup or something like that or something subject the probability that you have a tumor is or you have cancer is very very low right a very small proportion of people have that but if you do have that it is very very important to catch the tumor early to catch the cancer early the tumor early to catch the cancer early. Because if you catch the cancer early, I'll just focus on breast cancer for women, so I don't keep repeating myself. It's true for other forms of cancer, for example, for prostate cancer in men and so forth. The point is that if you catch it early, it is almost completely curable. You go through a few unpleasant months of chemotherapy and radiation, but that's about it. Today we can cure, we can basically bring a person back from breast cancer and put into remission forever. It never comes back in a large number of cases. So catching early is important. How do you catch them early? So let's do an experiment. Suppose you are given a lot of MRI, the X-rays, and you are asked to build a model that tells whether a person has or does not have a tumor, have it together. Whether they need to even, I mean, whether benign or malignant doesn't matter, whether you need to do fine needle aspiration at all, or any invasive procedure at all. Healthy people, young people, for example, they'll have a completely clear x-ray, right? So you go to that, and now, let's say that the prevalence of cancer is in one in thousand x-rays. That would certainly be true in the Western Hemisphere because everybody goes through regular checkups, so x-rays and these things are done on an annual basis in the US. It may not be so true in India. I don't know if in Bandar you guys are in the different cities that you are, how often you go through diagnostics. But in US is the norm. Everybody goes through it at least twice a year. Different kinds of tests doesn't mean that they go through x-rays twice a year. But you do go through an x-ray about once a year or something like that, if you're a woman. So most people will be absolutely healthy. So let's say that the incidence is one in thousand. I'm just throwing out a number. It may be high, it may be low. I don't know medically where it stands, but it sounds right. So suppose it is one in thousand. So you want a machine learning model that catches it. But suppose your accuracy is 99%, right? Would you trust that machine? Would you trust an AI algorithm? I'm posing you a question. Whose accuracy is about 99% yes you know it seems as though you would anybody else who differs actually no I'll tell you why so suppose you are a quack. And for example, in India it's common that a lot of quacks, they'll put up a board claiming that they have all the medical degrees, especially in small remote towns and villages. They'll claim to be doctors. Ultimately, who is there to check? And they will be dispensing quack medicines. They would not be trained in any of the traditions, either Ayurveda or the modern medicine or anything like that, either alternate medicine or that. They wouldn't have any proper training. But they're quite literally quacks in the sense that they know nothing but they claim to be doctors. So you go to such a person and all that he says is you are totally fine, right, because he doesn't really know how to read an x-ray so he declares everybody to be fine. What will be the accuracy of prediction of that doctor, of that quack? If the incidence of the tumor is one in ten thousand what will be the accuracy can you guess guys so let's work it out so one in the error rate so the the person is getting 9,999 out of 10,000 right. The only problem is the person who really has cancer or has a, let's say lump, that person is also being declared as not having a problem. So there is one mistake, one mistake in 10K patients. one mistake in 10k patients. Right? And so 9,999 patients, he's giving the correct diagnosis. So what is the accuracy? Accuracy, again, is correct over total. What is that coming out to be, guys? This is 99, oh no, 1 in 1,000. So 1 in 1,000. So 999. So yes, so it will be 99.9% accurate. Do you see this, guys? Right? But does it mean anything? It doesn't mean much because this model entirely missed the one person whose life is at risk. Do you see that? And by the way, this has been the history of quacks over the centuries in every country. If you look at modern medicine, the modern penicillin was discovered barely 100 years ago or less than 100 years ago and pretty much all the more potent drugs came after that. All the more potent drugs came after that. So when it, then there has been some alternate systems of medicine, for example, India has Ayurveda and then there used to be the Yunani system or the Hamdell system and so on and so forth, tradition. But they were far more effective for chronic diseases, you know, body pain and things like that. A lot of chronic diseases you know body pain and things like that a lot of chronic diseases they would address but when it came to infections or things like cancer etc we do know for a fact that as far as i understand there were no scientific cures or ways of managing them or treating them until about a hundred years ago. Almost everything came after that, after penicillin and the discovery of the modern medicines, the pharmaceuticals. But doctors have existed for thousands of years, isn't it? For you go to 500 BC, 1000 BC, you again hear of doctors in all over Greek literature, ancient Western literature, Eastern literature, everywhere. Every country has had doctors and people. You ask those people, how in the world could they be curing anything at all? When we go back and look at it, we realize that actually most illnesses are self-limiting. They cure themselves. You have an upset stomach, wait for a few days. The germ will flush itself out of your body and you'll be fine again. You develop a fever, you will recover. Like for example, today I seem to have a little bit of fever and cold, but I do know that in two days or three days or whatever it takes, this thing will run through the system. I don't need to take medicines for it, right? Except maybe if the temperature goes up, I can bring it down or something like that. So most diseases cure themselves, right? So when you go to a physician and the physician gives you, let's say, some colored water, all these so-called mixtures, nice looking mixtures that used to be there, gives you one of them and you get cured. What used to happen is, and this is now our understanding, that the patient would now develop an extraordinary trust in the doctor, because it's the doctor who, in a way, cured the patient. And the doctor would develop extraordinary faith in the textbook that taught it, that that colored mixture cures people of whatever disease it is, let's say, fever or something like that. For all these centuries, obviously it was very hard to remove the confirmation bias when the doctors read those textbooks that said use this for this illness and use that for that illness. And when they would do that, sure enough the patients would become okay. So it would confirm their bias that those medicines work. Today, of course, we know that those medicines are nonsense. They never worked. In the same way, you look at any doctor and you will find that there are lots and lots of patients who will swear that this doctor is a magician. He is really a great doctor. However bad the doctor would be, there will always be a group of patients who would absolutely adore the doctor. I say this as a person who actually has been very closely associated with the medical profession. It's a fact. Right. So, but what people don't know that sequence is not causation. In other words, just because you fell ill, then you went to the doctor, and then you became okay, does not mean that the doctor cured you. You would have been cured anyway in a few days, whether or not you had met the doctor. So our mind has the habit of treating sequence as causation, but it isn't. So a very silly example of that would be to say that, see, I came to Silicon Valley in 1999, let's say, just take a date, right? I ended up here after my graduate school, and right away, there was a dot-com boom. So would it not be silly for me to claim that because I came to Silicon Valley, Silicon Valley had a tremendous explosion of prosperity and innovation. That would be obviously completely stupid. In the same way, it would be, but it's hard to see that it is the same reasoning to say that because I went to the doctor and after that I became cured, therefore I can assume that I was cured because of the doctor. When we want to see the effectiveness of a medicine, what we do is we create a study in which we take, let's say, 100 patients. To 50 of them, we would give the real medicine. To 50 of them, we would give them placebos. Placebos are medicines that look exactly like the real one, but actually is filled with just inert material, maybe chalk internally, if it is a tablet. And the doctors wouldn't know, the doctors wouldn't be told who the real patients are. I mean, who are the guys getting the real medicine? And the patients of course wouldn't know whether they're getting the real medicine or placebo. They'll always be told that they're getting the real medicine. Then what you do is the person doing the clinical trial only knows, they keep observing how many, is there a difference in the rate of recovery between people who took a medicine and the people who did not take a medicine? So you need to demonstrate that the people who took the medicine, more of them recovered compared to the people who got the placebo. And then you can establish that the medicine is actually effective. These are very real things, by the way. Even today, we are doing that with the COVID. There's vaccines that we are creating. There is some of that going on. You always do double blind studies now as a gold standard. But anyway, the reason I brought it up in classification is that it sort of creates a backdrop of how you should use to determine what measure to use for the accuracy. You ask yourself, what is the asymmetry in data? So suppose in this particular case it is the accuracy of a blind a useless algorithm would be 99.9% so means your model you have a useful model you have trained a model to do something practical only if it your model model MC the accuracy of your model let me just call it accuracy accuracy of your model exceeds 99.9% in this situation would you agree guys yeah you would it has to catch those rare cases or another way to put it is accuracy may not even matter the only thing that matters is did you you may actually be willing to lose some accuracy so suppose I do this. Out of the thousand people, I declare five of them as positive cases. Positive, that is a cancer, potentially cancer patients, potentially cancer cases. So now, but so long as this five includes that one person who genuinely has a lump, would you take, now look at the accuracy of this algorithm. It will actually be lower than the fraud or the quack one. This accuracy is only 995, isn't it? Or why go at five? Let's take 10. let's say that you predict ten of them and you have nine ninety cases your nine ninety cases you're right actually 991 cases you'll be right and nine cases you'll be wrong because out of this ten one is correct nine are wrong right and so what happens is your accuracy is much lower accuracy is 99.1 much lower than 99.9 but would you consider this model let me call it m2 would you trust m2 or would you trust m1 the previous model you come again some of you are arguing for the previous model 99.9 model and some are arguing for the night the m2 which has lower accuracy so those of you who are arguing for the previous model give a reasoning. Why would you take the previous model? Do you have an explanation why would you take the previous model because the error is less in the previous model they compared to the second one sir Yes, that is one explanation anyone else has a different opinion or has a different reason anyone who prefers m2 M2? Yeah, so M2 would be better than M1, I think. And why would it be better? Why? Because it's actually, the percentage was low, but we have 10 cases actually positive. So it's better than that. That includes the actual cases. So think of it guys from the perspective of you being the person if you have tumor would you go to equipment that catches it or would you go to the equipment that doesn't catch it? You would want to be tested by that equipment that catches it. Now it may have false positives you know nine people are wrongly diagnosed to have cancer, but what will happen suppose the. Dr. G R Narsimha Rao, Ph.D.: Suppose nine people are scared now and they told you have cancer, the way in medical you would do it is you would put them through further tests. put them through further tests. Now suddenly there's a chance that those, all those 10 people have cancer, you would do blood tests and you would do secondary tests and now you would do fine needle aspiration or you would do a core biopsy, go and take some tissue out and check whether they have it. At the end of it what will happen? Nine of the people you would have scared unnecessarily in the beginning but then you would say, everything is fine, go home. So while that is a nuisance, it is not as bad as missing the person who genuinely has tumor. Because when you miss a person who has tumor, in a few months, the tumor spreads and then it is too late to save the person. Isn't it? And that is why the previous model is not as useful as the second model, even though the accuracy of the second model is actually lower. And so this is how you have to think about classifiers, guys. You have to ask yourself, what is the purpose I'm using it for? And so what matters more? What matters more is I don't miss the positive case and do not miss the positive case if you have to sacrifice a bit of accuracy no problem right and therefore the point that I'm making is it is not just accuracy that is a measure of how good a classifier is there are other measures and i will talk about two of these precision recall the other many of them so accuracy is just one of them precision recall sensitivity specificity safety routine specificity and we will do the definitions of this next time. But I'm just planting the idea in your mind that what matters is that there are many different measures of the goodness of a model and the content, the situation decides which measure that we do. So when you don't want to miss the positive test cases, you want to go for high recall, high recall for early, early cancer diagnosis, cancer diagnostic tests. And what happens is that while the first test is good, it will produce a lot of false positives. What you want to make sure is that before you give chemotherapy, you have done a lot of other tests. Now the test that you do before you are sure that a person has, before you start chemotherapy should be, it should be very sure that the person really has cancer so the thing that you are being uh that you focus on is that test should have very low false positives so the the the final test that should come after you know that by the time you reach the final test if you have even the slightest inkling or possibility of cancer you have been marked as positive so the job of the final test is to tell that you don't have cancer if you don't have it right so one second i have a doubt here so yes sir in case of m, you told M2 is better than M1. But here in both the cases, actually it is equipment which is showing it. So whereas in second cases, the 9% is detected positive, whereas the 1 is correct and the line is wrong. But in this case also, the 9% will be given the treatment of same cancer thing, which is not required. No, no, it is not. That's the point I'm making see so maybe i should dwell on it more see what happens in diagnostics is there are different tests the regular tests that you do the annual checkups etc they do they are on the side of or they are biased towards detecting or marking us positive if there is the slightest chance that something is wrong. So if there is a slightest chance in your x-ray that there may be a lump, it will mark you as positive. But then when you are marked as positive, you do not immediately go in for chemo or something like that. What happens is then the doctor starts doing more aggressive tests so for example they will do fine needle as in india you do fine needle aspiration that is pretty much uh the gold standard there most places will do that they will go in with a needle they'll aspirate out a tissue and by the way fine needle aspiration is true for all forms of cancer in india right whether it is breast cancer or any other form of cancer. They go there, then they aspirate out a bit of tissue and they look under the microscope now and they check whether you really have it, right? And then they will do other tests, automated tests also. In the West at least they do that. And then obviously if they are really, really sure before they put you through chemo they will go and actually open take a more invasive approach go and do a core biopsy in other words make a small cut go in endoscopically and take a core biopsy of the region right of the tissue come out put it under the microscope and so on and so forth, to be absolutely sure. They won't put you through chemo just like that. So what happens is, and that's the narrative I'm saying, the early test, you prefer M2. You don't want it to miss the positive case, even though it produces a lot of false positives. So these are what there's a word for it it is false positive right these are false positive but you but you allow that because you know that you have other safeguards other gates that will eliminate those later on whereas if you have a final test in the final test what do you want to do you don't want to this is the door before the chemo right in what do you want to do? You don't want to, this is the door before the chemo, right? In this test, you want to be absolutely sure that this person truly has cancer before you bombard this person with chemo. So then the more, you need to have very high specificity. In other words, it needs to say positive only for the people who are truly positive. Are we together? It may have false, at this moment, you don't fear false negative because the early test would have caught up. So here false negative is okay because you know that some other test has already taken care of that. But here, what you are making sure is that there are no false positive. You want to make sure no false. Low false positive, right? So let me summarize it a little bit for you see in the in the early test right early test the first test suppose there's a machine learning model there what you want to do is you want to have low false negative false negative what is the definition of false negative? It means you do have cancer, but this test missed it. That is very dangerous because you go home, live your life happily, and a few months later you get the shock that it has spread all over. So the early test must have low false negative. So generally these tests which are given on a routine basis, that's what you strive for. They should be relatively non-invasive, they're not lead to scars and so forth. They should be like, for example, in US mammogram is the standard. You go through a mammogram, you look at the x-rays and so forth. Then, but the, from a machine learning perspective, it's requirement is you need low false negative. The slightest hint Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam may have cancer you want a final test because beyond the final test is chemo chemo radiation etc and you know that these are very brutal treatments they they they do remove the cancer but in the process they practically kill the patient right and do a lot of damage to the patient so you don't want to send somebody for chemo who is actually healthy. So what you do is here, your requirements are different. Low, false, positive. Isn't it at this moment, what you focus on is low false positive, right? You don't worry about false negatives because this has uh the early test would have if this if this comes after early test you know that you don't have to worry about that are we understanding that now guys yes sir then it cannot be called as a model accuracy in that case sir because we are not getting a correct output yes and that is the lesson I want to bring out that see people in this field and in the common world have overstated the value of accuracy. If you look at this carefully in both the early test and the final test, accuracy is a meaningless metric. Right. The real metrics are low. The early test should have very low false negative rate, and the final test should have very low false positive rate. Right? But the reason I'm mentioning it to you is as you read your textbooks and other things, some textbooks are good, most are pretty mediocre actually. So in most fields, for example, in mathematics, when a textbook is written, at least in the West, they are written to a very high standard. You don't find mistakes in them. In this fast emerging field of data science, because it's moving so fast, actually the quality of the books that you buy, these programming books, this data science book, is pretty terrible, actually. So it seems that anybody who has understood even the basics of data science immediately feels the need to write a book because then, you know, good things will follow. And sometimes the books are good. Sometimes they are not. And when they are not good, they do a lot of damage. Because you know, now, a person who understood things badly has perpetuated that. Now, 100 people have read that book, a thousand people have read that book. And now they will also believe that to be true and continue with that kind of thinking. One of the common things that you find sometimes is, and I hear this, the reason is, you know, I work in the company, you know, you interview people, you get people. So you ask them these questions. And quite often you see that almost all the young kids who come to interview, they have that misconception that accuracy is what you go after. You ask them to write code and immediately they'll try to make it more accurate without thinking whether accuracy is the right thing to pursue. So keep that in mind, guys. Accuracy sometimes makes sense, but many times it doesn't make sense. It is an advertising too. Many, many things will say my accuracy is this percentage or that percentage. But see, the quack can go and say that he has a 99.9% accuracy rate in diagnosing cancer, because he's telling everybody you're healthy. But that quack is a quack. And 99.9% accuracy means nothing. Actually, it is far more useless than a test, an early test, which has a much lesser, which has a lesser accuracy, but has a low false negative rate. That's the lesson that I wanted to bring about. So all of these things can be derived from the confusion matrix. There's a, this particular matrix that you saw. It has a name to it. This is called, it has a famous name. It is the confusion matrix. Matrix, matrix, confusion matrix. So it is like how confused your model gets so if your model is perfect what would be the sign of a perfect model so for example cow duck cow hat duck hat you would say that what you would have is a perfect model would be 50 50 0-0-0. Do you see that guys? No mistakes. Principal axis would contain, the principal diagonal would contain all the data points. And the off diagonals will be empty. Means all the cows were correctly identified as cows, all the ducks were correctly identified as ducks. Now what happens in general is, you don't achieve 100% accuracy unless you have a very clean data and a very simple explanation for it. If the explanation is very, for example, if I were to ask you, can you tell the difference between a cow and a duck? We as human beings would have a near perfect accuracy or near perfect results, no errors, especially if the cows and ducks are just roaming around us, isn't it? It would be very easy and you would get, but if you give it to an algorithm, algorithms also can achieve that level of accuracy, but quite often data has noises, models imperfect. So you will get a certain error rate and so based on the confusion matrix the things that you want to do is look at all of these measures and we'll talk about these measures in the their mathematical expressions the next time. But today I wanted to give you guys a big picture of what it is like and how you judge a model. And I haven't talked about any classifier. Actually, next time also I'll make it a theory session. I noticed that we are beginning to run out of time. So the real, we'll talk about logistic regression. We'll talk about a few more regression algorithms, decision trees and random forest and so forth. So there are many algorithms that you use and these are the great classics of data science and machine learning. They're used on a day-to-day basis, very, very frequently. And you don't even know that these things are being used all the time, but they are used. But today I'm focusing just on the irrespective of which classifier model it is, how do you do the diagnostics of this model? How do you know that a model is good? So there is one more thing that is important. It is called the receiver operator characteristic Operator, Operator, Characteristic. Actually, let me write it in caps so that I don't make receiver operator characteristic characteristic and usually it is called the roc curve so characteristic curve well that sounds scary term very very technical, if there are scary and technical terms. So this receiver operator characteristic curve, the word is historic. It comes from signal processing. It has been borrowed from signal processing. That is why that strange name. Most people don't call it by that strange name. They just use the word ROC. Now, what is this ROC curve? This ROC curve is a property of a model, is a property of a model and how it behaves on test data, test data or real data, data. What happens is that there are two axes here. One is false positive. And a true positive. And both of these go from 0 to 1. So 0, 0, 0, 1, 1, 1, and 1, 0. Of course. So what it means is, at this point, you have neither false positive nor true positive, right? The model is such that it's basically everything is negative, let's say, right? So on the other hand, this point, so let me call it A, B, B point represents a situation in which you have no true positive everything is just false positive right that's a pretty bad model this is a c represents a model in which everything is true positive and your false positive rate of the model is zero that is a pretty nice model you would imagine you want a model that catches cancer when there is cancer, but doesn't produce any false positive. In other words, it doesn't alarm anybody who doesn't have cancer by telling them or scaring them that they may have cancer. And then this is the other thing in which the true positive and the false positive rates are the same, sort of very high true positive and very high positive rates are the same, sort of very high true positive and very high false positive also, right? So now what happens is this line, as you can imagine, this is actually a square, even though I made it out as a rectangle, this is a 45 degree axis. What happens is that useless models, people that make random predictions, there's that. Now what happens is as you change the threshold of accuracy, let us say that, let's take an example. Let us say that the model looks at the color of a solution. You do some test, chemical test, and then light passes through it and then some color comes out. And the color could be anywhere in the degree of redness right so how red it is zero red or fully red and so somewhere there is a cut off okay i'm deliberately using these words i'll use more precise words next time but i'm giving a hand-waving argument here. There's some point of redness at which you say, consider it positive, and below that, consider it negative. Let's say that if the solution is sufficiently red, you suspect cancer. If the solution color is not red enough, very likely red or some other color, you consider it not cancer. So based on this, you will realize that based on where you put the cutoff, your positive, true positive and true negative rates will be different. And it will in fact have a curve. A good one will go like this. In other words, with a very low false positive rate, you have achieved a very high true positive rate, right? A very high true positive rate. Are we together? Think about it this way. If your machine is good and very accurate, you know the right cutoff such that at that cutoff you may occasionally have a few false positives, but you get a lot of true positives. You know, people who are having cancer, you catch them all. This is the proportion of people. Let's say that 99% of the people who have cancer, you catch them, even though cancer is relatively rare in people. cancer you catch them even though cancer is relatively rare in people you would want a algorithm that is like that so this one right let me mark it as green this is good right and what is bad models that are not good will have something like this result in other words true positive and false positive are equally balanced. Now there are models that actually can be like this, even be worse than random. And it happens sometimes. So watch out for those. The point to remember is that in the ROC curve, these are the ROC curves. All of these are ROC curves. Now this is a little bit too much to learn in one day in diagnostics so we'll repeat this concept over and over again a few times. Now you say that the green is good and the red is not so good. How would you quantify it? One easy way to quantify it is how much area there is below the curve so you would agree that there is more area below the green curve than there is below the red curve right now the total area of this box is one because the height is one and the width is one so total area is one so you know that the the area under the curve will be somewhere between zero and one somewhere in this interval and the bigger it is the better so there is a word for this it is called the au area under roc curve and the symbol for that in classification theory classifier theory is a u r o c the way you write it is capital o a little u r o c all in capitals area under roc curve that is the term right under rac curve and usually that is a good measure of how well your algorithm is doing. Remember, it's only one of the metrics. It is yet another metric, but a very good one to determine how good your algorithm is. And we will use that with time. We will use that. So next time we have a choice, we can either do a lab, or I could explain to you the classifier theory, I'll see, or maybe maybe we'll do the lab. So all of these ideas become real. We will do a lab without me giving you an idea of how those algorithms work, but you will sort of do the lab and see them work. And then the week after that, we will do the theory in which we will learn about, I'll teach you about some, at least one classifier in depth, logistic regression. Now the confusing thing is, do you notice that the word regression is sitting in there? It is actually not regression at all despite the name but classification how confusing can it be but for historic reasons it is called logistic regression so logistic regression is actually a classification algorithm it's a very popular algorithm and very widely used and it's a great thing to start using in practical day-to-day life so we we seem to have run out of time next time when we meet let us do a few labs. We'll classify some data and we will actually use a breast cancer data also as an example. And we'll use that to illustrate how these things work. But without understanding logistic regression, just understand what classifiers do what their metrics are and how to use them we will do that and then subsequently in the week after that two weeks from now i will cover the theory of logistic regression is that a plan guys yeah let's do that all right guys and so I'm stopping the recording so you guys feel free to ask questions. Remember guys that these videos are all online. Now this YouTube live, it is available on YouTube, of course, since the session started, and it will remain on YouTube forever. The cleaned out version of these videos, my videographer will clean it out and post it to the course page. Typically, that happens in two, three days. In two days, a more cleaned out version would be there without the gaps, without the breaks and so forth that we took. And so you'll get two videos, one which was a review of regression and one which was the introduction to classification. Are we together? Can do that. By the way, let us review that. Let's see if you understood the concept. Classification is in some sense predicting a type or identifying a type. That is all it is. It is a mathematical function internally to a machine that somehow can do that. Now, we didn't go into how to build a classifier, the theory of a classifier. I thought it was more important for the lab perspective to know how it works and how do you judge a classifier? How good it is? In other words, what are the classifier? Model metrics, people use the word model metrics in the case of regression it was simple mean squared error or mean absolute error in the case of classification it's a little bit more involved the root of all these things is the The root of all these things is the confusion matrix, which looks like this. How many ducks you got as ducks? How many were you wrong? Cows, how many cows you got as cows and how many cows you got wrong? So forth. So then people talk of different measures. The most obvious one is accuracy. The total number of correct over the total number of data, proportion of correct. But if there's a lesson that I taught you, hopefully in the subsequent discussion is, accuracy is actually, in many situations, a meaningless metric, right? You have to look at a situation and decide, what are you looking for? So we took the example of breast cancer and we realized that if you want to do early tests, you know screening tests, the people in the US use the word screening test. You get screened for any signs of a disease. So the screening test should have low false negative rate, whereas a final test should have low false positive rate. Right. If you don't have it, it should say that you don't have it so that you are not exposed to a brutal regime of chemo and radiation. Then we learned about the receiver operator characteristic curve. Quite a mouthful. And the word comes from signal processing domains, but we usually call it machine learning. The term is ROC curve. ROC curve is a property of a model and how it behaves on test data, real data. And it is a curve that makes a trade off between false positives and true positives right if you want to in the limit you can get complete hundred percent true positive in most models only if you are willing to accept a lot of false like a false positive salsa which is unacceptable so what you do is you pick where you want to put the cutoff do is you pick where you want to put the cutoff, right, at which you have already achieved most of the, like, for example, here or here, here or here, where you have achieved most of the value of the model, it catches most of the positive cases, but has a relatively low false negative. But on the other hand, if it is a very scary disease, like cancer, then maybe you need to be here you need to catch more make sure that most or all of the true positives are caught even though you end up with a lot of false negatives right so we learn about all of these in lab next time and the algorithm that we will learn is called logistic regression log Logistic regression, despite the very confusing presence of the word regression in it, is not a regression algorithm, but a classification algorithm. It is the only classification algorithm that I know, which has a confusing name. But there are many, many such classifiers. Classification is a very rich area of machine learning. So there, like this, we will do decision trees, red-dub forest, support vector machines, gradient xg you know gradient boosting and so forth and those things we will do from an experimental perspective but from a theory perspective or understanding that of course usually people who come to the labs the who take the whole workshop sequence with me they do it slowly over time because it takes a lot of time to understand this theory people are with me typically for six seven months and in six to seven or eight months and in that they sort of go through the whole thing like for example i'm proud to say that the 2020 batch is finishing in december the people who joined in april uh may actually may batch is finishing this december and they are all getting ready to get their job in top tier companies the 2019 batch was extraordinarily successful. Support Vectors managed to place some of them in the top universities of US for graduate school and some of them in very good jobs Amazon and Facebook and Google and so forth. But then all that success comes for people who sort of have taken that whole six months of rigorous study. And when you do that, you learn, because the field is vast, you learn a lot. You learn about the deep neural networks, deep learning. You learn about many theories and lots of things in that. So, because this is a much more abbreviated workshop and also we are catering to an audience. A lot of you clearly told me you don't have computer science background. You're just switching into data science. So I've tailored it to be the entryway or a doorway to data science. So I hope those of you who are entering this field, are you finding the explanation simple and are you able to follow through? Is it helping you? Would somebody who comes not from a computer science background, please give a feedback? Anybody would like to give a feedback, guys? You're all quiet. I don't know if anybody's even here. Maybe people have joined and gone off to their work or such. Any feedback? Yes, this is really very informative and it's really a nice class oh thank you anyone else guys anybody else there in india has a feedback uh hi uh shrikant here so i am like uh academically i'm not from uh computers instagram but last year first 10 years i am into like this development java.net lecture but uh machine learning and all this mathematics of course we learned before but we have forgotten mathematics but machine learning is uh since in our organization there is option to switch there and i think what you are teaching us, it's really great to continue and now maybe I feel confident that I can take assignment, I can ask my managers to put me in machine learning project in near future. Yes, good to hear that. That is the goal that after this basic workshop, you'll be able to get your foot in the door you know get a job in data science and start doing that machine learning and start doing that so do the labs guys if you do the labs i feel or i hope that you will get started you'll get your job into machine learning or data science all right guys so if that is that is, I'd like to end the session now. Could folks from MLP please stay back? Thank you, sir. Thank you, Asif. You're welcome. Thank you, sir. Thank you, sir. Thank you, sir. Thank you. Thank you. Thank you. Thank you. Thank you.