 Okay, so the topic today is natural language processing. This, what does that mean? People have variously called it text mining or linguistic analysis and so forth and people have approached it from many different directions this field which is a natural language processing it is at the intersection of many disciplines there are people there's a subject called linguistics that some of you are aware of that tries to look at the structure of natural languages by natural, we mean human spoken languages. And if you look at the history of these, you will see that they look into the grammar of languages, how words come together to form sentences, what are valid sentences and what are invalid sentences. They look at the words and even into things called phonemes, the components or the pronunciations, the phonetic pieces of a word. And for many, many years, people have been studying it quite a bit in different disciplines. Then there has been a statistical study of linguistics and languages. People have, for example, one of the easy ways people could detect plagiarism or know which author wrote a work is just by looking at word frequencies, statistical properties of a text that you write. And it turns out that there's a reasonably unique signature that each one of us have. And you can therefore ascribe this text to, the authorship of a text to a particular person. So those things have been there. And if you, all these books on computational linguistics and so forth, anything that predates something like 2010, you would see quite often that they are based on mostly statistical approaches to a large extent. There has been some application of machine learning gradually over the years. You can take a couple of documents and you can take sentences, you can take words and you can try to morph it into a machine learning problem. When you try to morph it into a machine learning problem, the very first thing that comes up is in machine learning you deal with vector spaces. You deal with vector spaces. If you notice when we do classification, regression, clustering, you always think in terms of instance of data being a point in some vector space, some feature space. Are we together guys? Now the question is, how do we map that kind of thinking to linguistics, to language, natural languages and stuff like that? Words are not numbers, words are not vectors in the traditional sense, isn't it? Words are words. We grow up speaking the words and forming sentences, writing paragraphs and so on and so forth so the first order of businesses if we are going to apply machine learning to this domain we need to find a way to map language the natural language to a vector space representation language to a vector space representation. So to now, whatever I said, I'll just say that now in a little bit more succinct mathematical language. So this thing that we are dealing with is natural language processing and it has gained a lot of attention these days this is natural language processing and it's sort of borders or adjacent terminology used are text mining, computational linguistics, and things like that. There are other various words that are fairly adjacent to it. They may not mean, they may not be exact synonyms, but let us consider them as approximate synonyms. All right. With that in place, we now go back to machine learning. If you look at a machine learning algorithm, essentially, if you're doing predictive modeling, let's take the word predictive or supervised learning. What are the things you do? You have a black box, you have your algo. What goes in is an X vector belonging to D dimensional yes space and output if the output Y hat if Y hat belongs to a number it is a number but then what do you call it this sort of algorithm is called regression yes this is your regression and if y hat belongs to just a class c1 c2 together i'll call it with the category g which is made up of c1 c2 ck classification classification this is classification excellent and on the other hand if nothing comes out the whole point is to do or something you just take X belonging to our D and you do some pattern recognition in this cognition then such as what are some examples of pattern recognition clustering yes dimensionality reduction and and so on and so forth you have many for learning and there are a few other things. So now this has been a world of machine learning. We haven't covered reinforcement learning, but if you think of the machine learning that we have been learning in ML 100 and ML 200, you would all agree that this has been our staple diet. This has helped us deal with even images. We figured out in the last week that you could feed in an image because an image is nothing but a bunch of numbers. So suppose you have an image. You can take an image which is let's say three channels of RGB image RGB channels and let us say that you have a image of height cross width you can think of it as data data of three of three dimensions 3d isn't it every single pixel here every pixel is belongs to in this image belongs to some horizontal some row, some column, row, column, row, column in the image and channel. So you can think of data or an image as belonging to R3. And you can, if you so choose, flatten it down. You can even flatten it to a single thing of R to the power what would you say R to the power height cross width cross channel right those many you can flatten it out suppose you have a 32 cross 32 cross 3 image then you can flatten it down to a dimensional space which is 32 cross 32 cross 3. you can if you so choose you can flatten it down into a single vector like this now so image processing is something we became pretty good at if If you remember what we did is, just as a recap of last week, we would take an image, we would do some feature extraction from the image, actually using kernels, filters, isn't it? Using and congulationence the way the technology that you used is condolence and once you had done the feature extraction a be flatten it into some single feature vector space of some hidden number of dimensions and then see a input it to it to feed forward network Dense layers And so it's a three-step process first is you take the image and you extract some features from it once you have extracted all those features you flatten them into a one-dimensional vector of some hidden dimension and then you input that hidden dimension vector Into the feed forward network in a dense network, the way we are used to doing it for that. So the river data set or something like that. You straightforwardly feed it in. And so we have an algorithm. We have a process to deal with images. Images. This is the way we'll deal with, but it raises the question that, all right, images are fine. What about text? Language or data, when you look at data, a vast amount of data comes to us as text. Just unstructured text. People at the websites, the web pages, the blogs, the articles, the transcript of videos. This particular video, this particular session itself is a continuous stream of text, of words. So how do we convert this into a vector space representation? That is our first order of business, isn it but the the question is how do we and I'll pose this question how do we map language to active space representation this is our first order of business would you guys agree do you realize why I'm saying that you the to begin you need a vector space representation for your language yes so we do, but let's look at the pieces of the language. Our languages are made up of words, characters. They're made up of less granular words, then they could be sentence, and then they could be the paragraphs. And then there could be the paragraphs, or you could call it a document. Document means any essay or whatever it is that you want to say. Document example would be blog essay or so, and so forth. You get the idea of what you would call a document. It's just a generic term, but when we use the word document, we don't mean a technical document. We just mean any collection of paragraphs, if you or even sentences if you so wish. So these are the different levels of granularity that a language has. So you can create a vector space representation at the level of characters, at the level of words, sentences, paragraphs and documents. Just to, so while character level vector space representation is often quite fruitful, at this moment when we are getting started, I will start with words because words is a very simple intuition. Words is the most atomic unit, semantic unit that makes sense to us isn't it so a cat maps to something in the real world isn't it a dog maps to something in the real world and so let us start with words now if you know we'll start building the a little bit of terminology here that is fairly well prevalent in the natural language processing and linguistic world we use the word. So let's have a little glossary here of terms. Vocabulary What can you What is vocabulary, it is, it is essentially the set of all valid words are we together then language punctuations are the delimiters of of phrases and sentences comma semi comma colon right question mark full stop exclamation mark, etc. These are, and this is for example, for English. Isn't it? And then if you're using foreign languages, then it could be the inverted question mark and many other things. So these are your punctuations then we i think the word sentence hardly needs an explanation we know what sentence is a logical collection logical unit made of words words and delimited and terminated by a by some punctuation by a stop punctuation of some kind in every language you you find a stop punctuation. Now, we will use the word corpus, corpus to mean sufficiently, it is implicit, sufficiently large collection of documents usually pertaining to a field. For example, you can have corpus of the English language itself, the general English language. You can have corpus of the english language itself the general english language right you can have a specific corpus for legal uh for the legal profession and as you know if you have ever read a legal document especially in the united states it is a language of its own it's legalese it's hardly english right have you ever tried to read hardly English. Have you ever tried to read any legal document and compare it to normal English? You'll realize that it's quite different. Likewise, if you ever open a medical textbook, you will be in for a bit of a surprise. The textbook is filled with Latin words, isn't it so you can say that you can have a general English corpus you can have legal legal corpus you can have medical corpus you can have a music world corpus and you can go on like for example if you looked at the literature of legal it medical it has a lot of latin terms but if you look at the literature of physicists then it is filled with a lot of greek terms right so our physicists and mathematicians physicists corpus and you can just go on concocting a different sort of corpus and the plural for corpus of plural is corpora and so you will see this word corpus and corpora used quite often in this literature. But then the corpus need not be just for English. You can have, for example, the Spanish corpus, language corpus, right? And then you can go on adding your languages and so on and so forth. So I hope the meaning of the word corpus is pretty clear from all of this. So to set the grounds here, let us start with the notion of word. So you have a word. Let's take the word cat and let's say have another word dog, right? And then let's have another word called, what should we call it? Horse. Three words. Question is, how do we map it to a vector space representation. So one easy way you can do it is you can treat this vocabulary, the entire vocabulary. The entire vocabulary as As a categorical variable right it's a category category and each word is therefore a categorical value isn't it word is a categorical value when you deal with categoricals right how do you deal with categoricals how do you encode it suppose in classification problem or in the regression problem if one of the features is a categorical how do you deal with that do you remember guys encoding isn't it you do one-hot encoding so what you can do is suppose your vocabulary let's say that the size of your vocabulary be let n be the size of your vocabulary vocab that you read i.e so suppose n and I'll just use a notation so that we remember, whenever you put a bar in this space in natural language processing, you refer to the size, your meaning size. So size of this vocabulary, let it be N. So there are N words in your language. Now, you can choose what words are legally present in your vocabulary and what are not. So when people do the analysis, sometimes people say, I will take the 10,000 most common words. So for general purpose writing, so suppose you're writing a novel, you're writing a blog and so on and so forth, opinion pieces, in writing a blog and so on and so forth, opinion pieces. In the social sciences and so on and so forth, you're talking about issues. Most of useful conversation you want to do a 10K, 10,000 word, thousand word, words is enough. And you can reserve a special word let's say that you have a special word five for any word not in the vocab but then suppose you take 10,000 words then somebody else missing may say, no, English language. I would like to take maybe 100,000 words. Now, you realize that 90,000 of these words would be probably very infrequently occurring. And then somebody else may come and say, no, I need to have the vocabulary that is fairly exhaustive. And I think if you take the fairly exhaustive treatment of the English language, to my knowledge, it's somewhere between three to 5 million. That is the latest count. So what size of vocabulary do we take for one-hot encoding? You realize that these things have a lot of practical importance. If you create a word, if you one-hot encode a word, let's say cat and what it would become is and let's say you line up all the words alphabetically, somewhere there would be the placeholder for cat an index for cat and here it will say one everywhere it will say zero zero right but the size of this vector now is either 10k or 100k or million you know five million depending upon the size of the vocabulary you take. Now when you're trying to do computation with such large vector spaces, vectors, you realize that it's going to be computationally brutal the bigger the vector space you start out with. So different people start with different sizes of the vocabulary. In many problem sets, starting with the input vector space of 10,000 suffices 100,000 certainly suffices for most purposes I'm told that Google for some of its analysis it takes a vector space representation when hot encoding that is of million so each word is represented by a million sized or 5 million sized vector. But whatever it is. So we now came to one representation of it. And so somewhere here, Doug. I see one quick question. In terms of this 0, 1 encoding, are these typically encoded as strings or these become actual integers which are then used in the manipulations? Yeah. So you don't even, you even write it as 1.0, 1.0. become actual integers which are then used in the manipulations yeah so you don't even you even write it as 1.0 so basically this vector so you say that cat vector the vector representation of cat X of cat belongs to now the given that the vocabulary size whatever the n is. It's a vector that belongs to this where all values are 0.0 and then somewhere it's a 1.0, then again it's 0.0. You see that it's literally a vector in the true sense of a vector. And so for dog, there would be a different position, the dog position. Likewise for horse, there would be another one hot encoding. All the positions of cat will be zero, the position of dog will be zero, but somewhere in there, there'll be a position of a horse. You have it. And then let's say that you bring in a zebra, which should be Z. So I would imagine that everywhere all the way to zero till almost the end right let's say that you have a vocabulary made up of only animals i would imagine zebra is the last animal that you have so you'll have a vector pretty much near the one pretty much near the end of the vector right so you get a vector representation now let's look at this vector representation this is a valid vector representation and it is the beginning of things. We'll start from here. This is your one hot encoding. Given this one hot encoding, what can we do? We can say, well, we converted it to vector now let's start now let's start talking so out of this vector you can say well now i can go from words to sentences so for example if i give you a sentence or a document or a paragraph suppose i take a sentence the Suppose I take a sentence, the dog chased the cat. Are we together? So then each of these has a vector representation. The will map to its own vector. Somewhere it has a one. Dog will map to its own vector. Somewhere has a one dog will map to its own vector somewhere it says one a chase will map to its own vector isn't it guys and this will map to its own vector well there would be the same here we are repeating this and this would map to another vector somewhere there's a cat. The dog. So are you seeing that each of these words would have a one hot vector encoding? So how do we represent this sentence? I can represent this sentence potentially by adding up all these vectors. Now, what does it add mean? Add can mean one of two things. One is I can concatenate these vectors. So I can take the vector for the, comma, I can create a bigger vector, the vector for dog. I can take the vector for chased. I can take the vector again for the, I can take the vector now for cat. And so what happens is, suppose the size of this vector was 10K. And now I ended up with 10K, 10K, 10K, 10K, 10 10k and so you end up with a vector it is one two three four five 50k vector 50k dimensional vector well you could do that and there are certain use cases in natural language processing where addition when you say let me write this sentence like that, you could write it like this. Now, when you write it like that, you would agree that it would be the unique signature of this sentence. There is no other sentence which will give you exactly this 50K dimensional vector. Would you agree? Yes, sir. Right? Yes. Yeah, there would be no other such sentence at all. But when you represent it like this, there is a problem. You get a longer sentence, it would be a vector in a much larger dimensional space. So it is very hard to play with vectors if one vector belongs to two dimension space and another belongs to 15 dimension space. It's just problematic. You don't know how to even add them. So sometimes what people do is they truncate their sentences. They say that, you know what, if you have a document or a sentence, we will keep no more than 100 words. And you just imagine that every sentence is made up of 100 words. If your sentence is made up of less than 100, or maybe just say 10 words, 10 words, you take only 10 words. And if your sentence is made up of less than 10 words, you just pad it up with zero right empty empty space and so people have done that it does work it's a valid approach the other way that you could do it is you could create something called the count vector now what in the world is the count vector for the sentence? The count vector just does a summation of these vectors, summation of words, the word vectors, V for each of the word I, for I, all the words. So what happens here? Suppose you have a cat, let me just take the word cat plus dog cat plus dog it will become zero zero zero somewhere is the cat let's say this is the cat plus zero somewhere is the dog and so this will become quite literally vector addition so you'll end up with something a single vector like this do you see that now there are two ones present here right and then suppose your sentence has dog and then it also has a dog then this will become two because there are two dogs in your sentence are we getting a sense guys how do I create a count vector it's taken so whether it's a sentence or it's a paragraph or it's a document you can create a count vector just by concatenating them when you concatenate them then your vector space remains the vector space of the word itself the the vector space doesn't change but you are doing quite literally uh in the vector space suppose i have suppose cat is this vector and dog is this vector now you're adding these two vectors up. That's all you're doing. Then, so you say, well, that looks pretty nice. What could go wrong? This looks, we can make some progress with this. And people have tried to make progress with this. But now let's look at something. Suppose I ask you, what is the distance between two sentences it's a suppose I have a sentence s1 and I have a sentence s2 and you try to ask what is the distance between them it gets a little bit problematic because let's say that you even before you answer that ask yourself what is the difference between a cat and dog? What is the distance between these two words? You realize that it's very hard to tell because they are encoded in such a way that this is one this is zero and zero zero one somewhere else and so if you were to take Euclidean distance right so Euclidean distance is the L2 norm you remember the L2 norm distance Euclidean you will come to the conclusion if you can convince yourself that it will always be the square root of 2 do you see how like it will be 0 minus 0 minus 0 square everywhere except you'll have 1 minus 0 square plus and then 0 minus 1 square plus 0 minus 0 square everywhere right and so what do you notice that you will end up with a 1 square plus 1 square which is the square root of 2 so whether you're looking at a cat and let us say that you have a cat and a book the distance between a cat and a book and a cat and a dog they all are the same right that doesn't seem useful why is that not so useful? There's no uniqueness. There is no uniqueness. And that mischief propagates itself to sentences. The sentences, the distance between sentences has very little to do with the fact that the relatedness of words does not come through. So in other words, relatedness of words does not come through so in other words relatedness words is not captured you can try other games for example you can take a cosine of two words, w1, w2, which would be taking the dot product. So you take cat and the dot product it with dog, and you would realize that that would always be zero. If two words are not the same, the dot products would always be zero. That doesn't seem terribly useful. And even when you build sentences up, all it is telling you is how often the two sentences share words in common. That's a very poor form of finding similarity between two sentences or two documents. If all you're looking for is how many words are common between them. If all you're looking for is how many words are common between them. So that is when you begin to realize that you have probably run out of mileage. You need a better vector representation which captures the relatedness of words in some sense. So this journey is quite interesting and today's session is all about this journey to capture the relatedness of words, the semantic aspect of a language. Language is not just a random collection of words. Those words that we form express thoughts and thought has coherence you know it has a context so we will try to capture the context the the coherent structure of a thought with these words and how do we do that so the next milestone in this journey was an interesting concept it was called a TF IDF well that's quite a mouthful. It is made up of term frequency inverse document frequency. That's even a bigger mouthful. What in the world is term frequency inverse document frequency? Well, it is such a big mouthful. What in the world is term frequency in words document frequency? Well, it is such a big mouthful that in many, many interviews, I asked people very early on, what is TF-IDF? And in a surprisingly large number of cases, people who will come to you with resumes of years of experience, they still manage to stumble on it. They sort of vaguely know what it is, but they often get it wrong. So today we'll try to get it right. It turns out that this term, this big word TFIDF, there are many variations of this. There are subtle variations and people have played with different variations to remove the deficit the deficiency in the very basic the basic or the the the initial definition of tf idea but let's go into that what is term frequency this is associated with so this is a property of a word property of a word in a document. So TF-IDF is a number. So TF-IDF, let me put it this way. TF-IDF is something that belongs to the set of real numbers. So it could be 0.2 or something like that. Some number. How do you come upon this? And it is contextual. what is the TFIDF? This is an important thing to remember that it is a value that you associate to a word only in the context of a document. So now let's try to understand what is it that we are talking about. Let's say that you have the word the cat. And the word the and the word Siamese. So let's say that you have the Siamese cat was, or maybe let me just change it to a little bit more from Alice in Wonderland the Cheshire cat Cheshire cat cat appeared grinned and disappeared and vanished. Suppose you have, I will use the word, imagine that there's a document D1 that contains this text from Alice in Wonderland. So the Cheshire Cat appeared, grinned and vanished. Now, let's think about the sort of the interestingness of words. When you look at the word the, do you think when you read a document, you're absolutely struck with how interesting this particular word is probably not there is ubiquitous isn't it whichever document you pick up you'll probably find there so given a document all the documents di belonging to a corpus would you agree that the majority of them will contain the word the right so let's write it this way and let's make a little table here though and what we will do is what proportion of documents have it what proportion of Doc's so what would you say if I have to guess in a corpus how many words would have how many documents would have the just just throw a number guys and it goes close to 100% isn't it so hundred percent of them would have the associated with them right so let's leave that now we come to the word a Cheshire the Cheshire now let's look at the word the Cheshire what about this word it is likely to be unless you are reading Alice in Wonderland you're not likely to encounter the word the Cheshire cat it's a relatively obscure word or infrequently used word so let us say that this is a one in 10,000 words, 10,000 docs. Would that be reasonable? Yes. Just throwing in numbers. You can cook up your own values. The cat, the cat on the other hand would be, it's a little bit more common word and you would throw it in uh how many how often would you say let's say one in or maybe i'll make this one in hundred thousand the word cheshire one in thousand yeah one in thousand is something like that so now you observe that see the entering the interestingness of a word has something to do with how likely or how ubiquitous it is so this is the ubiquitousness ubiquity of word it's a measure of ubiquity in this column. Now you ask this question, in this particular document, given this document, how much is this document about this word? So you ask how many times has this appeared in this document? So let's say that you would say that the Cheshire Cat appeared, grinned and vanished. And then let's say that there were a few more sentences. The word the would be pretty common, one would say, right? It may occupy maybe a 5%, 5 out of 100 words are just the word the. Or maybe 10 of them. Let's just take some values. The Cheshire Cat, if this is about Alice in Wonderland, you would imagine that in this particular document, it may be relatively common. So it might have appeared seven out of 100 times. Maybe the word is even more common. I'll just, just for the sake of explanation, I'll make it 15. And the cat word has appeared 10 times out of 100. Right? So occasionally we use the word cat, but we didn't, or maybe we always use the word Cheshire cat together. So let us say that we had this also as 7. Right? So this is the term frequency. This is the word for that is a term of frequency or word frequency or word frequency. How often it appeared here. Are we together? Now, if I were to ask you, in this document, how much is this document about you think about the, you want to say, well, no, it's not really about the, we are pretty sure it's not about them. But how would you mathematically justify it? You would say, you know what? We should do this. We can take the term frequency and then multiply it by the inverse of how ubiquitous it is how not ubiquitous it is in the corpus so in this document it might have appeared but generally in the entire corpus how how not ubiquitous it is in other, you take these numbers and one way of getting not ubiquitous is to inverse it, right? If you take the inverse of it, what do you get? Right? The non-ubiquitousness. So let's say that we inverse this number. So for cat, the non ubiquitous value would be opposite of hundred percent is one of a hundred percent is one. So one over one is one, right? One in 10,000 means this, this becomes like one in this is hundred thousand and this will be thousand so you're saying that the is the sort of the interestingness or the da is very ubiquitous on the other hand Cheshire is not ubiquitous then in a way if I may coin a word the noniquitousness of the word Cheshire is 100,000 times stronger than the non-ubiquitousness of the word the. Would you agree with that, guys? Does it sort of resonate with common sense? Yes. And likewise, cat is a little bit more common. So maybe the non-ubiquitous strength of this is about 1,000. So now what happens is that if you look carefully, if you look at the non-ubiquitousness of words, they will begin to show a wide variation of many orders of magnitude. So you will see values like 1 in a million one in million so non ubiquitousness will be like ten to the power six ten to the power five ten to the power two then one you know you see a wide variation in uh of values whenever you see a wide variation of values, one easy way to deal with it is not to deal with the value itself, but to deal with the log of the value. So if you take the log of the value, it will become six, five, two, one. Like for example, our sound, how do we measure sound? A sound that is twice as loud is actually in terms of intensity, in terms of energy, it's actually one order of magnitude higher in intensity, decibels, right? But we... Asif, what's that right to the left, the bar right next to those exponent numbers. It's like one in something One in million. So one in million. Thank you. Yeah, one in million. Apologize for my handwriting. So, so what happens is you get these kind of wide variety of numbers. One easy way to deal with numbers which which vary across a large scale. numbers which vary across a large scale is to take the log of it very much like sound. The intensity of sound is or the volume of sound. Imagine buying a radio or buying a musical instrument and the knob said one, hundred, ten, hundred, thousand, ten thousand and so on and so forth. and you would be very much lost how would you tune between them right it would be very hard to tune between them so you look at the log of that log scale and turns out that the human ear actually is a logarithmic risk receptor right when ten times the energy comes we perceive it as twice as loud roughly speaking there's a there's a little nuance of decibels and so forth but let's go very roughly like that right which is why you know when you double the volume you have 10 times the chance of hearing loss roughly speaking right so the same thing here so suppose you take this one so you coin the term inverse document frequency inverse document frequency of word of what of the word in the corpus that is the implicit statement of the word in that corpus so for example now let's look at this the word was 100% present so 1 over 100% is and then you take the log of it is log of 1 right the word Cheshire cat Cheshire would be log of, well, 1 in 100,000. So what happens is you do one over one of 100,000. So it is log of 100,000, which is 10 to the five. Let's say that you're doing log base 10. So it is five, right? On the other hand, you're doing Cheshire, Cheshire, C-H-E-C s let me put the word correctly Cheshire Cheshire cat and the word cat is there log of you know 10 to the 3 is equal to actually 1 2 3 4 5 yeah this would be 3 right so this would inverse document frequency let us define like that ID F of cat of the is log 1 what is log 1 does anybody remember what is log of 1 0 0 exactly so IDF so now we can create a table IDF the is equal to zero idf cheshire is equal to five idf of cat is equal to three now what can we do this gives us sort of the rarity uh non-ubiquitousness of the word so what happens is when we are looking and asking in this document, what is this document about? Right? We can do this instead of just counting how often a word happens, the can mislead us. The happens 15 times in this document. So it would be misleading to say, okay, all right, this document is a thesis on the, right? see how how misleading that statement would be but suppose we calibrate it by bringing in this rarity of the word measure along with it and we say that let us multiply this let's multiply these columns and i'll just rewrite the data that we had previously. The word, so here is the word, here is the word frequency. There was 15 out of 100. then the then idf and we won't write the full big word it was zero the cheshire is seven out of hundred and let's say that the cat is also seven out of hundred and the value here for cheshire is five and the value of is three And you begin to see that something interesting is coming out. What happens if you multiply these two columns together? You get zero, you get 35 over 100, and you get 21 over 100. So what it is saying is that this document, in this document, Cheshire stands out and so what we have done is the last column is a ratio is a product so let me call it it's a product that column labels so word frequency times idea is the product of these two right and that is people don't use the word word. They use the word term usually for reasons, historic reasons, a term frequency. And so this becomes TFIDF. And that is the justification for TFIDF. Are we together guys? So TFIDF, now let us quantify. What did we just discover? See, we went through a process of reasoning. And the end of it we come to the conclusion that TF IDF of a word in a document D right is equal to of any word in the document is the frequency of the word frequency of word I right of the term i if you want to say times the log of log of what the ratio of how many documents there are divided by how many documents there are containing the word wi isn't it ratio these two. So how many documents there are total number of dogs containing the word now there is a subtlety whenever you take logs you have to beware what if your denominator here of a ratio whenever you take the log of a ratio, what if the denominator is zero? You will end up taking the log of infinity, isn't it, guys? So suppose you insist on finding the TFIDF of a word which does not show up in your corpus at all. Nowhere is it there in the corpus. But nonetheless, you want to find out what is its term frequency so your term frequency will be zero in a given document but what about the idf this will blow up isn't it there are let's say that there are one million documents but none of them contain the word some cooked up poet abracadabra or something like that so then this term will get into a bit of trouble so more strictly speaking when you program or when you deal with things what you tend to do is you tend to add one to it in the denominator you add one so that's a bit of a practical improvement that people make and so when you open your text book or you open look at it you won't see the one here but usually you a modified definition of TF IDF is you're divided by one plus the number of words and typically it does not affect your answer very much but it protects you against having bugs during programming and having blowups of your code so anyway so this is tf idf and now comes back let's bring it back to our vector space representation so we have a corpus let's say that you have a corp your vocabulary of 10k words now what happens is you can now talk of a tf idf vector for a given document di for a given document di you can talk of a tf idea vector how what you do is you take the vector now traditionally remember one hot encoding in the cat position you would have put one for cat for the word but when you do the count vector you were putting how many cats there were there number of cats right and the number of dogs that is what you were having in this list remember in the count vector do you remember guys that this is what you were having in this list. Remember in the count vector? Do you remember guys that this is what we were doing? Yes. A little bit. That's what we were doing. We are saying one way to add up all the words in a document or a sentence is just to add their one-hot encodings right and so we'll get the count the count vector but instead of count vectors suppose i do this i instead this is the count vector in count vector you put the number of cats but in tf idf what you do is you put for every word where the cat is supposed to be you put the tf idf of the cat of the cat is supposed to be you put the TF IDF of the cat of the cat cat in this particular document of course di di and then when it comes to the dog TF IDF of the dog in this document di and so on and so forth so for all the words that do actually appear in this document you will have so what will happen is you have a vocabulary words that are not there in the document if you are careful like if you are if you do it properly the term frequency will be zero so they will all be zeros the words that are actually present you will you will have pfidf values. Isn't it guys? Right? And so you have created another vector representation of a document, right? So you can say X, given a document di, you can now have a vector representation i right which belongs to the vocabulary the size of the vocabulary it's a vector representation and where the values are where the values are the component values are are the TFI of the relevant word in the doc. So this is a milestone guys and we will stop here. So what can we do now? Suppose you have a corpus of lots of documents. You can treat each document as a data instance, right? You can treat it as a XI data instance. Suppose you have 100 documents. You can have X1, X2, X100, isn't it? Now, suppose I give you, so now let's start talking here this is the beginning of fun now suppose i tell you that the first document is about calculus and the second document is about um something that is entirely not calculus what would that be politics something that is entirely not calculus what would that be politics politics right and then some document again is about calculus now what can you do do you see that you can map this easily to a classification problem you give it xi and you ask what is it calculus or politics are we together guys and you can now apply your traditional classifiers that you are familiar with in machine learning to this problem can we can we do that guys yes yes so this is an interesting milestone. And so TF-IDF therefore has been traditionally a workhorse in the natural language processing world. For many years it was. It is sort of the classic vector space representation of documents. And you can do regression. For example, given a document, if you want to score it by any factor, how much, for example, you can take this word and you can, for example, try to predict how readable the text is or how hard it is to read it and things like that you know you can do all sorts of things from this thing now you can do classification you can likewise create situations for regression score a document you can you can start clustering a document and then somewhat related to clustering is a thing called topic modeling and by the way these things will do in extensive detail when we get into the NLP part of things but today is just the fundamentals and introduction modeling so you can do all of this so that is TF IDF so we started by saying that in natural language processing the primitives are characters and words and sentences and paragraphs and documents somehow from these we need to manufacture a vector space representation because then the machinery of machine learning can be applied to it which deals with essentially vector spaces, right? R, D, D dimension real Cartesian spaces or Euclidean spaces. So we have to take a journey to get there. The simplest way that we can do is we can one hot encode the words and from them we can create the count vectors. Then we can be a little bit smarter. We can ask how interesting is a word in a document? How much of the document is about this word? Right? And there we can't just be naive and take the term frequency, word frequency, because common words like the will proliferate and always win the race. We need to also factor in or multiply them by their sort of rarity factor the rarity factor is given by the inverse document frequency how infrequently it shows up in the documents of the corpus and when you do that you get the the TF-IDF, term frequency, inverse document frequency, associated with a word. Now you go back for every word you have done that. So therefore, for a document, you can now go back to your one-hot encoding vector space. And now, instead of putting zeros and ones for each of the word, you can put TF-IDF for that word in this document. And so every document will therefore map to one feature vector. It just became a feature vector. And so it was a long journey, but what you did is you managed to create a pretty sensible vector space representation of this document. A document therefore became an instance of data. Right? And now you can apply the rest of machine learning, the machinery or infrastructure as is. And then comes the whole game of classification and regression and clustering and topic modeling and so on and so forth that we can do. So at this point, I'll take a break, perhaps for about 10 minutes at 8.30. I'd like to then continue on, but if you get a chance, just make sure that you understood it. Take out a piece of paper, write a sentence, and try to think what the TF-IDF means in the context of a word means in the context of the document and make sure that you got it. If you have any questions, if anybody has not understood the concept of TF-IDF and a vector space representation of a word, sentences, etc., now is a great time to ask before the break. So the document vector, which was before TF-IDF conversion, which we got through one hot encoding and later, which we got after TF-IDF conversion, it's the same document vector. It's just different representation, right? Exactly. So in other words, the dimensionality remains the same document vector it's just different representation right exactly so in other words the dimensionality remains the same the dimensionality is the size of the vector of the vocabulary but the the values instead of just being zeros and ones and now it is the TF idea so it removed the kind of stop words from the document, like from the document. It represents that when you take a document's TF idea vector, what we see is that the words which are stop words, typical stop words, the, is, etc. They all develop values of zero. Right. Because the idea is close to zero. Right. Because the IDF is close to zero. Exactly. And so, in a way, you can effect a dimensionality reduction now. Because you take no matter how many documents you take, all the stop words will have value zero. And if in effect, what happens when some columns have only zero values? They don't matter. You have just reduced the dimensionality of the problem of the feature space and the added advantage is that it helps finding out what kind of document it is yes it gives you some flavor of the document right and so now if in the tf id of space you can do clustering it makes sense to cluster two points, right? In this space, two documents which are near each other. And so if the cosine distance is less, what do you conclude? They're probably talking about the same thing, isn't it? Similar things. Okay. So all of these things begin to make sense. So now this is the first time you inserted some semantic value into the representation in the vector space representation but semantic values is still not uh i mean i'm trying to understand it's very dilute it's not very strong yeah maybe that now will go into more powerful thing uh way after the break so this is part one of our talk, we'll have part two, and then we'll have part three. In part two, we'll talk about something called word embeddings, right? Finding a much more semantically rich representation of the documents and words and sentences. And in the third part, we will do the topic called transformers. So word embeddings and transformers make the second and third part of today's talks. So Asif, quick question. Yeah. So the question is, so basically the larger the TF-IDF, that basically means the word is farther away. It is much more and more . is that the right way to interpret it see every word every document will have a tf idf let's say your vocabulary let's say imagine a simple vocabulary of only 10 words so every document will have a 10 dimensional vector right right so if two vectors are near each other what do they mean So if two vectors are near each other, what do they mean? What is that? They are more related. It can happen only if you look at the cosine distance. The only way these two vectors can be near each other. They are talking about the same Cheshire cats. No, but what I am trying to say is that the entire TFIDF number for rarer words is bigger, right? Yes. No, no, no, hang on, hang on, hang on. The IDF is bigger for rare words. But see, a word may be rare. For example, I may take a word cacophony. It is fairly rare to find the word cacophony in documents but if in your document the word cacophony is not used the tf idf value will still be zero you realize that right yes what i'm also also like the tf idf okay it could be zero if the word is not found Also, the TF-IDF, okay, it could be zero if the word is not found, but whenever the word is found, the number representing that word, the TF-IDF number would always be higher than the number representing a more frequent word. That is right. So what happens is that the idea boosts, essentially it applies a boosting to the term frequency. So what does the number mean? How do we interpret that number? See it means that how much the document is about that word. The higher the TF-IDF of a word, the more it is that the document is talking about that word even though the word might be lower in frequency even though the word is lower in frequency so how likely is it that you'll find the word a Cheshire cat in a document that is not about Cheshire cat but the Cheshire Cat document would be Alice in Wonderland. So the document is not talking about Cheshire Cat. It does. Cheshire Cat plays quite a role, an amusing role in that story. So it is significant. Cheshire Cat is a significant character in Alice in Wonderland. So I see. I had a similar question to what Pradeep asked. So in this method of encoding, what I'm seeing is happening is it's looking at the cardinality of words in terms of the number of occurrences of certain words is basically what the term frequency is contributing to and then we're taking a ratio with what document frequencies no frequency it is a ratio suppose the document has 100 words and this word occurs seven times the term frequency is seven out of ten. It's a ratio. Where I was going with the question was, it is inherently ignoring the proximity of some word with another word. It doesn't care of the proximity. It just is looking at the occurrences. So we're missing all semantics associated with proximity. Exactly. So in other words, the word that you would use is it is context unaware. Okay. It's context insensitive. Thank you.