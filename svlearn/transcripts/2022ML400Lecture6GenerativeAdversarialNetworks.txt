 The topic today is something quite interesting actually and one of the most fun and influential work that came out. It comes under the topic, or what it is called, the name itself is quite interesting. It is called, well, this is not a good color. Let me pick generative adversarial networks. GAN, they are called GaN. Now, I will go over the word quite interestingly. What you do is network. So here by network you mean deep neural networks. So what you have is two networks. One is called, let me just call it D. Think of it as a detective or a police. The more formal term is discriminator. Think of it as a cop. And then there is another network which is called the generator. Generator. Let's just see the counterfeit maker. Counterfeit maker. So what happens is this is the adversarial nature comes from the fact that they both try to fool like the generator tries to fool the counterfeit maker tries to fool the cop and the cop tries to catch the counterfeit and i'll explain what I mean. And then there is this word provocative word generative. Generative ultimately refers to the generator. What happens is that when the cops and counterfeiter, they become very good, right? They learn and they become pretty good as networks. Then you can throw away technically the cop and the generator can go about generating a lot of counterfeits that look real. So what do I mean by that? Let's take an example. Suppose you have lots of examples of, we'll take something, let's say a dollar bill. Dollar bills, because we're using the word counterfeits. So lots of dollar bills and coins or whatever. But these are examples of real examples, which we will write as an X vector. It is conventional to write it as an X as the real data, right? Obviously, then what happens is the job of the discriminator is to tell given an input given an input it has to tell is it real means did it come is it a real dollar bill or is it a fake dollar bill or fake coin whatever you want to say coin or dollar bill right it just has to give an answer. Let's say that the y is equal to 1 is real, and y is equal to 0 label is fake, right? You can choose. This is a conventional way that you put it like that. So you would say that, well, this is a pretty real-life situation. And as far as discriminant is concerned, what is it doing? Given an input, it's classifying as real or fake. You would call this algorithm, what would you call this algorithm? What is this network doing? Would you say that this is just a classification? Classification. Yes. It's just classifying between real and fake. Given an input, it is predicting that this is real, that this is fake. So this is very interesting. It will produce a y hat of its own. So if you give it to x, suppose the input is whatever the input is, it will just do D of input, and that will be a Y hat, right? For real data, for real notes, for real bills, dollar bills. Yes. Now, what about the generator? Generator is just sitting there. The generator has never seen what a US currency looks like. This is the most interesting thing. Typically, when you try to fake something, you have a pretty good idea what it is. Isn't it? You have seen it. The way people counterfeit bills, dollar bills, for example, is they would very carefully see all the hidden security markers there in a bill and try to counterfeit that. As you know, that itself is a... And so there's a real life counterpart to it. As you probably know, the old dollar bills, they didn't have US currencies. They didn't have that many security measures, right? The technology was not so advanced. If you take a dollar bill from 50 years ago, it would be very, very different from the dollar bill of today. And those dollar bills of the past are presumably much easier to counterfeit. So well, they get successfully counterfeited. Now, counterfeit is of degrees. The trouble is you don't have to be a perfect counterfeit. Most of the time, if it is a close enough replica, you can use it as currencies, especially abroad, right? Where, you know, it's sort of all sorts of all variations of dollar bill. I don't know if you know, most of the hundred dollar bills of the United States don't exist in US, they are mostly used outside. In fact, the world standard in currency, right? Especially in the criminal world is the hundred dollar bill. The US hundred dollar bill is the gold standard, right? So I've been told, I'm told that when, I don't know, let's think of some criminals, whatever, drug dealers or something, when they do business deals, bags of $100 bills change hands or something like that abroad, right? So, well, anyway, we'll leave that. Now, what happened to the dollar bills itself is over the years, the US government has been trying to foil the counterfeiters. Every time the counterfeiters begin to make something that looks uncannily like the real dollar bill, and only an expert can tell the difference, they add more security features to the dollar bill. And I believe every country must be doing some similar things. I once looked at the British pound, the British currency carefully, and it seemed to also have all sorts of security features embedded in it. So every country tries to make it hard to counterfeit. So what happens is the data gets more and more complicated, isn't it? You make it harder and harder to counterfeit it. And the counterfeiters use all the technology available to them to somehow create a reasonable enough facsimile of the original thing that it can pass for the real. It can sort of simulate the real and people can get duped by it. So this is the real world situation. The same situation we will bring in here. And the idea of, this idea was created in a paper, a landmark paper of 2014. Literally the title of the paper is Generative Adversarial Network, Luke and some big luminaries are on the paper. And it turns out that one of the co-authors of the paper is my cousin in the extended family. That was 2014 and it was hailed as one of the landmark breakthroughs in artificial intelligence and deep neural network. Many people went so far as to say that this is perhaps the first truly intelligent neural network. Well, I think it was a bit of a hyperbole. It is one neural architecture. We will look at it today, but it's an architecture that had profound results. It led to many good things and many bad things. The whole world of deep fakes comes from this generative adversarial network. So when the generator becomes very good at faking or making counterfeits, it can counterfeit anything, not just dollar bills, not just any text or something like that. It can counterfeit and generate human faces that don't exist. It can generate photographs of non-existent human beings that are so real, you would not know that this is not a real human being. It is able to create videos of people saying whatever you want them to say. For example, I think I began this workshop with this entire series with a video of an MIT course on artificial intelligence in which the lecture started with Obama introducing the course. Obviously Obama did not introduce the course. What was happening is one human being was speaking and that input was going into the generator. Generator when fully trained, what it was doing is it was producing a video of Obama in full Obama accent saying things that he obviously never said. Do you guys remember that video? Anyone in the audience, do you guys remember? If not, Kyle, could you please post that video back into the Slack channel right now from our course webpage? It is there on the course page, portal. Yeah, I think you got it. So I would say in the break, please do watch the video to be instructive. Now, the state of the art has moved forward to such an extent that you don't even need a lot of raw material. You don't need a lot of videos of a person, the voice, the thing, mannerisms. Shorter and shorter amount of information can be used to fake you. So we can generate a video doing, you doing all sorts of silly things, for example. I don't know um riding a donkey for example which you may never have done and you would swear you don't want to do or just about anything you want anything somebody else wants and it is getting harder and harder to tell it apart real, real things. So there used to be a saying that believe nothing that you hear and half of what you see. And that thing now has new meaning. Quite literally, even when you see things, you see photographs, you see videos, you don't know whether it's genuine or not. It is fake. So that is the ethical aspect of generative advocacy in my thoughts. Once you create a generator that can counterfeit things, that can fake things, it's a very dangerous tool. But it can also be a tool for great value. In fact, GANs have tremendous positive potentials. One of the things that people love to do, it is neural style transfer applies GANs and so forth. We use that. There's a little bit more to it than the classic GAN, but we will do that in the computer vision course in great detail. It's absolutely lovely. You can take any one thing, any one person like Van Gogh, with the Van Gogh style of art, you can take Monet, you can take Picasso, you can take whatever your artist is, favorite artist is, or style is. And what you could do is, you can keep it one side and then you can ask, you can take a photograph that you have taken, or Ansel Adams, whatever, you can take a, well Anselm is a very contrasty photographic style, but yes, then you can take any photograph that you have taken and you say what, I wish Van Gogh had painted this or Monet had painted this and lo and behold, you can just apply that style so that all of a sudden your photograph is rendered into near perfect Monet or Van Gogh or Picasso style or any style that you like of the artist. So that brings up very interesting questions. What is art now? I create a painting using GANs of a scenery, of a scene, which looks indistinguishably authentic in Monet style, should it be sold for $1 million or $10 million? It has become too easy to produce it. And so what is art itself? It's a very interesting question. In a time when deep neural network produces art that is very, very hard for human beings to produce with their paintbrushes and so forth, and it does it not in months, but in a couple of, well, minute, two minute, five minutes, and so forth. So this topic of generative adversarial networks is a very important topic. I'm going to go over this slowly. Now I must mention that GANs, I will explain the original GAN because that is the foundation of this thing. Once you get the foundation, remember that the classic GAN that I will explain today, and I'll try to explain one more the one of the more modern gans the wasserstein gans but gans has become a cottage industry there is a tremendous amount of research into gans right people are trying to like there's not only a initiative to generate for example deep. How do you use GANs to contract that and find whether something is a fake or not? And so forth, detect deepfakes, et cetera. And there are various GAN architectures and different aspects of GAN by using different loss functions and so forth. So it's a very rich literature. In fact, Manning, the same publishers who have published the textbook that we're using, they actually have an entire book called GANs in Action. GAN in Action. And I encourage you to read it. It's not a very fat bookures, variational GANs. There's many, many, many GANs, sorry, not variational, there are many, many GANs, convolutional, Wasserstein, and so on and so forth. The richness is quite, quite impressive. And GANs, we are doing it as an extension of like, or sort of beyond autoencoders for a reason. I started with autoencoders, and then I'm going to GANs. You will see there is a reason why it is so. See, autoencoders too, the variational kind, or the kind that I sort of alluded to, I said that in autoencoders, you come up with a condensed representation. Remember that, let me sort of relate it to it. In an autoencoder architecture, you have this, X goes in and X tilde comes out, right? Y, basically Y, you produce Y hat, which is approximately like X. Like you want it to be approximately like X, the output. Do we agree, guys? Right? Now, we have a latent representation. Now, we have a latent representation. This was the latent representation. Now, in the classic autoencoder, we allowed Z to be discovered automatically, whatever, it was no constraint. But there is a form of autoencoder which for time constraints we didn't go into but I will go into in one of the Sunday sessions these are called variational autoencoder in the variational autoencoder what you do is you put a constraint. You say that this Z, very roughly speaking, it must take a specialized form. Let's say that it must take the form of a bell curve. The Z must sample from a normal distribution of some kind. z must sample from a normal distribution of some kind. So let me introduce some conventions so that we know. So folks, have I introduced you? I mean, for those of you who have taken the original courses, you remember. When I write a scripted n, zero, one, what in the world is this? In our notation, what does it mean, this fancy n, calligraphic n, 0, 1? Do you remember? Normal distribution. Yes, this is the normal distribution. Now, what in the world is the normal distribution. Now what in the world is a normal distribution versus abnormal distribution? I once saw a joke. Somebody said this is a normal distribution and then this is an abnormal distribution or a paranormal distribution at first. So a normal distribution is just a bell curve. Bell curve generalized to higher dimensions. Or simply put, a bell hill of some form in let's say in two dimensions it's a bell hill of some form right somewhere centered some center here which is called mu the location of the center and how spread out the bell is is the sigma square is the sigma people write it as sigma or traditionally a sigma square variation so this this is written as N, N for normal, mu sigma squared, right? This says sigma being the standard deviation, this being the center of the bell hill. Are we together guys, right? And in higher dimension in what happens is that this mu and this sigma it is actually not um it's a matrix if this is a vector if mu in one dimension mu and sigma are scalars numbers in higher dimension mu is a vector why is it a vector think two dimensions in two dimensions this location will have a mu 1 and mu 2 component isn't it if this is the first access let's say X one access, and this is the X two X's would you agree that new way new the Center will have an X one extra location. R. Vijay Mohanaraman, M.D.: Are we together guys is the simple. R. Vijay Mohanaraman, M.D.: So imagine a hill sitting in this room. So imagine a hill sitting in this room. You would like to know what its location is. You need two in a xy on the floor. Where is the center? So wherever the center is on the floor, the floor location needs two coordinates, right? If you think in terms of the north-south point and the east-west point, you need to know its location. So it will be two dimension now sigma is the variance so it becomes something called the sigma matrix which will be the variance along first axis right sigma squared variance along first axis variance along second axis then sigma 1, 2, sigma 1, 2 squared, right? So that's what the sigma stands for, sigma matrix stands for, right? Am I getting it right? Actually, these twos are unnecessary. Sigma is this, so then you can always put the squares there. So this is called the covariance matrix. So generalization of standard of variances covariance. Raja Ayyanar? covariance matrix. Raja Ayyanar? covariance matrix. Raja Ayyanar? So anyway, it is a little harder if the math is elusive remember that in one dimensional a, a bell curve is this, its center is mu, its variance is sigma, right? How spread out it is a sigma. So go with that, so this is it. So when you say zero one, what do we mean? We mean a normal curve with center at zero origin and with unit one variance would you agree Would you agree? Guys, I need some feedback because the whole class is remote. So if all of you stay on mute and don't give me any feedback, I would never know whether you're understanding or not. Understand. It's simple, right? So this is it. So now let's generalize it to higher dimension. In higher dimension, in higher dimension in higher dimension in higher dimension suppose i take sigma to be just the unit matrix it is called the identity matrix which is basically one one let's say let's say that's two dimensions say two dimension this so this zero and identity matrix means the center is at zero zero and sigma the the covariance part is at is the identity matrix namely namely 1, 1. In other words, it is nice round bell curve centered in the, sitting at the center, bell hill rather. So imagine a nice, you have just poured sand at the origin. What is the shape it will take? Roughly speaking, well, it won't exactly take the shape of a bell curve, but to the first approximation, as for the sake of intuition it will take a shape that is symmetric around the origin right on the page and that's it that is your this is what this is okay that is the meaning of this i was just interpreting what this means right so just for brevity when i continue this discussion i won't be writing it in vector notation i'll just write it I won't be writing it in vector notation. I'll just write it as one, but assume that for generalization or for higher dimension, this is what I mean, because I tend to get sloppy when I'm thinking through and mix up my notations. So now one of the things that you have is that if you force the z to respect a bell curve just a normal distribution especially with respect to specific classes let's say that digits and so forth so you say that whichever way it comes here the value z z its distribution is let's say normal for a given class normal right it's normal distribution then what happens the auto encoder will learn it but then what you can do you can keep uh encoder will learn it but then what you can do you can keep uh you can keep only the decoder part of the autoencoder see the encoder part can be used for data compression but the decoder part of a variational autoencoder becomes a generative model because what i can do is i can take any z from normal zero one and when i feed it here and i sort of decode it, it will become something. It will become, let's say, if you're talking digits, it will become the digit three. And I sample from a nearby point, it will look like a different three. I sample from this point and it will look like eight right and so what has happened is we have created a generative model why because you can keep sampling you can keep shooting to the decoder any point sample from the normal distribution and it will produce a digit for you. Right, if you have trained it to, if you have trained the autoencoder on digits, the X vector belong to, you know, the digits for zero all the way to nine then, and that is what you try to recover here. You know that the decoder will produce a digit, image of a digit. And so irrespective of what input you give it will start producing lots of digits and so it has just become a producer of digits and very very interesting because uh it will produce those digits in all sorts of styles and handwriting styles and so forth right quite amazing that is the value of a generative model a generative model can generate once you train it it can keep to generate new data that Yeah, perfectly, perfectly mimics the original data and produces variants of it, produces variants of it. Now we can say this in a more mathematical term. We can say that a generative model learns the... Well, okay, I'll just say a statement, but if it doesn't mean, if you are not very trained in probability theory, I will make one distinction between generative models and discriminative models. See, when you create a discriminative, so in machine learning, you distinguish between generative and discriminative models. Everything that you learn, classifier, regressors, these are discriminative models, effectively. Think of a classifier. You're saying that what is the probability so suppose you're given an input vector x and this box classifier is producing a y hat and you are basically determining which of the classes it belongs to c1 c2 ck the k classes right So what you're doing is given, so can I, is this right? Is this what a classifier does, guys, given an input? It tries to determine whether it's a cat, a dog, a horse, a zebra, and so forth, right? That's what it does. So the more formal way, you can say that what is the probability of a class let's say c i given the given the input this and you will find you you want to find that class which maximizes this probability you know that was a soft maxing which class has the highest probability of being true, given that the input data is this? That is what a classifier is at heart, isn't it? Because it's never sure. It may say, I am, you know, 85% sure that it's a cow, right? I am, well, about 10% sure that it could be duck. And maybe I'm 5% sure that it is a horse. Let's take an example, something like that. Right? So 0.850.10.05. Those are your probabilities. Probability of a class CI given this input data, given the input data that we received. That's what it is. Do you see how this is the same thing, just written in a more succinct way, mathematical way. The trouble with succinct mathematical ways is, once you know it, it looks obvious, but if you don't know it, it looks rather scary. Now, what is this? But actually there's nothing scary. It's just a notation. This is what you call the. So in other words, a discriminative model, discriminative models, models discover or model condition probabilities. Condition probabilities in the sense that what is the probability, i.e., chance that it is a cow, for example, given the input x, right? Say a photo of x, a photo. Does that make sense? On the other hand, so do you see that? Well, yeah, now what's the big deal? But what is a generative model? What a generative model does is it says, you know what, suppose I could find the joint probability of X, the input, and a given output, C. Let me put it as Y because that's what people write it as. Y, for example, for classifier, Y would belong to, for classifier, write it as y for example for classifier y would belong to for classifier it is basically probability x and ci right so you find out all of the probabilities y y so y belongs to c1 c k any one of these values so suppose you could find the joint probabilities but now you say now what in the world does that mean let's make it somewhat real suppose you had a probability curve and you knew that literally mark it with two different colors that so suppose suppose the only let's take one dimension, and this dimension is the weight or the size of the animal, right? You would agree that for ducks, their weights, this would be the probability distribution of input x and duck. And let's take cows. This would be somewhere here, far, far away, much bigger ones. So I will deliberately sort of break the line here because they are very incomparably big. Cows are incomparably bigger than ducks. So this would be, would you agree that this would be the probability distribution of a cow's weight, right? This would be, where are we? this would be somewhere in the range of ducks go from what well newborn ducks let's say are one pound i don't know how big i have i have one question yes the the size of the cows are bigger but why the distribution we are assuming bigger because oh because they were cows can be see think about it this way uh ducks the range of weight is one to 15 pounds let's say i don't know maybe 25 how what are the biggest fattest ducks 15 pounds is that reasonable 30 pound ducks uh you know how they eat them so oh my goodness yeah those genetically modified, uh, hormonally fed ducks, I am told at 30 pounds. So 1 to 30 pounds. On the other hand, when you look at a cow, typical weight of a cow is what? Maybe a newborn cow is, let's say 200 pounds. Is that reasonable? Maybe closer to 20-30 pounds, only 20 30 pounds the side a newborn newborn a calf is never 20 30 pounds is it only no well okay let's let's make it at least as big as a as a big big dog right my dog is 80 pounds so should we start maybe 70 pounds oh whatever 50 pounds whatever you take it what is the other end of the cow's weight between 65 and 90 pounds per calf oh yeah 65 to 90. okay let's take a middle number 80 pounds so let's say a calf is about 80 pounds. And then what does it grow up to? A typical cow's weight is? Range? Thousand pounds? I don't know. Kate, could you please look it up? Oxes must be heavy. Still, the hundred should integrate to one, right? It might be flatter. No, no, according to that, could you Kyle, hold on. So what's the weight that you come up with? A female adult cow is average weight of 1600 pounds. The males are 2400 pounds. 24. So let's say that on the other end, let's say some really heavy weights are about 3000 pounds. So let's say that on the other end, let's say some really heavy weights are about 3000 pounds. So you asked this question, why is this? So do you notice that the spread, the basically variation is much higher of the cow's weight? Does that answer your question? Who asked? Manishish you asked that question so you get a sense right why it needs to be this much broader but the consequence of it's being much broader is that you uh actually hang on let me see there's another consequence of it see these distributions what they are done is so if these are frequency distributions, you normalize it. So you have to follow the constraint that the integral of Px duck Bx has to be one. Means the area under the curve, this area is one. That is the definition of a distribution, probability distribution. the definition of a distribution probability distribution right when you normalize it by the total area it is it becomes a probability distribution so the same thing what will happen here is this will be actually much more peaked it will be narrow and peaked and this will be broad and spread out right why because the same constraint has to apply. The area under the curve of Px curve has to be equal to 1. Abhilash, does that make sense to you? Yes. So now what happens is, suppose you discovered this distribution. suppose you you discovered this distribution what can you do with this do you notice that something beautiful suppose i had an engine that could take a point x i give you a point x uh no let me use a different color neither blue nor pink so that would be color maybe green suppose i give you a value x somewhere in this axis somewhere from this to this range and i had a machine that could generate given x given x it could do and it could tell that given this weight i know that this looks like, let's say that the X is here. What would this machine infer? If it knew this distribution, would it be fair to say that if it was you, you would say, aha, I'm looking at a rather big, mighty duck, isn't it? And so you could produce the picture of a big, mighty duck. Right, do you see that? In other words, if you knew the probability distribution of how the ducks are there the moment you put that point you can immediately tell duck big duck right even if you don't produce the image you know it's a big duck makes sense guys if i on the other hand took a pic a picture a point like this well let me use some other color, what is left behind, yellow, this, you would say, well, below average size curve. Would you agree? Would you agree? Folks? Yeah, that is it. So what you have is when you know the entire distribution, the joint distribution of cows and ducks, right? In just in one dimension, just in size dimension, you can basically now take any point, not only the points that you have seen. So what may have happened is you were given about, let's say 20 ducks, right? So they occupied only 20 unique spots on the size scale. And let's say 10 cows, or let's say 10 or 15, 10 cows. That is what you learn from. But if what you learned is this thing, P, X, Y, if you learn the joint distribution, namely this picture, you're in a lovely position because now you have generalized beyond those 20 ducks and 10 cows. You can say a lot about cows of all sizes and ducks of all sizes,'t it does that make sense so that is the point of a generative model generative models generalize beyond data to create a joint because they create a joint probability distribution literally joint probability distribution, a very fancy way of saying, so guys, remember in mathematics, things tend to have in the beginning, a lot of jargon associated with it. It looks like jargon, unnecessary jargon, but later on, it turns out to be very efficient shorthand that mathematicians used to talk to each other, this is it this is your mental picture a joint quality distribution is this am i making sense guys right why do we so i guess there's a when you say something is a joint generally you would think that it's a joint probability right but in this case all we are saying is that this is how we are describing the whole probability right no no let's take this example what is the probability using this picture can you answer this just just in a very intuitive way tell me probability of a x is equal to let's say 30 x is equal to, let's say, 30, x is equal to one pound, y is equal to cow. What would you say is the probability of this? Just looking at the picture. Zero. Yeah. So you were able to answer this question, isn't it? So knowing this distribution, now you have a joint joint probability you can take any arbitrary combination of x and y right if if i say x is equal to uh thousand y is equal to duck now what do you say okay right and now you can say let me take this point a p x is equal to, I don't know, 100 pounds, 100, y is equal to cow. You would agree that this would be far more likely than this thing, and you would come up with a number, whatever the probability distribution is at this particular moment, right? So that is why it's called a joint probability distribution. Do you see how it answers the question? Anywhere, any arbitrary value of X and Y. So in other words, it looks at the joint space of X vector. Suppose X was in this case, R1. And Y in this case, of course, is not a real number, but it is some category, C1, generalization, C1, C2. What you can do is for any combination of, you have an answer, isn't it? When you have an answer for any arbitrary combination of X and Y, you say that you have a joint distribution. Making sense now? Yeah. That is it. So what generative models learn is what this model, for example, learned, the decoder part of a variational algorithm is that it learned somehow that some distribution and it says this point, any one point I can tell what it is. It's a, if you say the probability that this point and it is a three, right? Or something like that. And when you put it to the decoder process, it will actually re-render it as a digit, right? It will decode it as a digit. Go from probability to a digit, right? So if you were to apply a decoder, for example, to this cows and ducks things, one would imagine that you would start producing fairly interesting images of cows and ducks. So those are generative models. In the spirit of generative models, the GANs are the next more interesting step, right? So what I would like to do is, I have been motivating the example of cows i mean generated models let's take this as a checkpoint do we understand let's go back and see what we have understood generated models a generative adversarial networks now we explain both the words did we explain the adversarial nature of the networks? What are the two adversaries? Anyone would like to volunteer? What are the two adversaries in our adversarial network? Two enemies or two people who are trying to outdo each other. What are those? Generator and... Generator and the discriminator, or in colloquialial words the counterfeiter and the cop. That's how it is. The cop is trying to tell whether the bill is real but the generator is trying to produce as close to as indistinguishably a counterfeit that is almost indistinguishable from the real one. So that explains the word adversarial. And I explained the word generative, what generative models are, isn't it? And because we are doing it using deep neural networks, these are of course, two enemy or adversarial neural networks that are fighting it out. One trying to outdo the other, do better than the other. Just as in real life, the currency fakers, counterfeiters, they try to outdo the government and the government tries to outdo the counterfeiters. Someone told me, I don't know how true it is, but it is almost impossible to counterfeit modern US dollar bills. It has so many security things, it's nearly impossible. So people who counterfeit, I'm told, they go and counterfeit much older currencies from 20, 30, 40 years ago, because they are still valid tender, so I'm told. And someone told me this fact, I don't know if this is true, I'll use that as a break point for a 10, 15 minute break. Somebody told me that in US by law, if in a printer, color printer, you try to photocopy, or not a color printer, in a picture, yeah, I don't know, maybe in the printer also, but in a fax machine, not fax machine, photocopier, you try to photocopy a dollar bill, the automatically the law or some printers, or maybe all printers, I don't know what the situation is, and I don't know how true it is, it will change the size of the image. It will know that you're trying to copy currency and it will change the size. I don't know how true it is, but so I was told. So instead of going to the photocopier in case you have a mischievous mind, try the generative adversarial networks. No, don't do it. There's way too much. But anyway, we'll use this as an example to continue after this. So folks, look at this picture. What can you say about this picture? Yeah, it doesn't look like the photograph of a person? Yeah. It does. And what if I told you that this person does not exist? That this is not a photograph. Now just ponder over the subtle lighting and everything that you see around this person. Isn't it very, very realistic? And yet, this is a picture that generative model that has literally been produced by Gantz. Isn't it amazing? So I will produce another picture. There it is. We have another picture now of a person. This person, too, doesn't exist. And this doesn't exist. And this doesn't exist. And once you see enough of this, you have to ask, well, how do I tell which of these are real? It turns out that common man is not trained to detect fakes. And that is the big risk of artificial intelligence. But even photographers, 50% of trained photographers cannot tell a photograph of a real genuine photograph from a fake for a generated fake. It needs some degree of training actually to be able to tell the subtle differences. In this picture, it turns out that the mistake is a bit obvious. Look carefully at the way this is attached to the ear, earring I believe it's called, and look at the way on this side it's attached. And do you see this artifact, this white artifact here? A real person would probably not have this you see where my mouse is yeah yeah that's right so it is when you look at images you you have to look at the subtle clues to figure out you have to find regions where the ai has made a mistake, basically. Another region perhaps could be this. Look at the background. Here, it has interpolated some artifact. Yeah, where my mouse is. So it is that. And you have to learn, basically, to. One thing that I find is, if you find that the image is too symmetric, human beings, human faces are never absolutely symmetric. So too much symmetry or too much asymmetry should get you a little, or very noticeable asymmetries or something should get you a little bit worried. This one I can't easily tell. Yeah, what could be? So if you notice this guy has a short haircut and nonetheless has hair hanging from below the ears. So either he had a bad barber, or it's a fake, things like that. So you have to go very subtle to find this. I invite you therefore to visit this website, which is called thispersondoesnotexist.com. Go there and play with it. And it's an informative site. It tells you like how to generate, like how to tell whether images are real or not quite often like for example if this is a child the earrings are a giveaway that something is off with this sometimes the background is off or things that are touching each other are off right the way they touch each other that or it's a little hard actually to tell fakes and it is getting harder and harder as this ai algorithms are becoming smarter and smarter and sometimes it's a little easier so let's let's hope that the next image that generates you can do it better is this easy to tell if this is fake can anybody see obvious markers i'm not a very good observer i might miss it but if anybody else is seeing something so i think all the dots on the this this side where your cursor is it appears something on yeah on the ear there is a white marking right even on the this this side where your cursor is it appears something on yeah on the ear there is a white marking right even on the background there's something on this idea wall seems to have little chips which may be real which may be fake but it does seem to be odd that here there's so much texture on the wall in here it's all just plain and as you mentioned right the hair on the on the right inside down below the neck oh yes yeah but she has it on the other side also but not to that extent yes but sometimes it is a little bit, so it's tough. You see that it gets rather tough to tell the fakes. Here, what would you say is the telltale sign? The ear on the left. The headband and the hair seems to be merging. A little bit of the merger is there between these two. So yeah, this is it. And it gets subtle. I'm not a very observant person. I don't think I've ever looked at faces more carefully in my entire life, but here it is very obvious. Can you see the way the fingers merge into the... Yeah, this one's bad yeah this one is a really bad fake okay so you mean photoshop accident now this one this is a good fake the picture looks very realistic to me maybe the background is a little harder to tell what do you mean by fake you have to detect whether it's a photograph the background is a little harder to tell. What do you mean by fake? You have to detect whether it's a photograph or it is a computer generated image. No, but the background or the front? The person or the background? The background. Okay. Yeah, something does not look too realistic. So there was a Kaggle contest where they looked at the facial things and you had to look at the wireframes. I don't know if this is what I was asking you. Looking at the eye position and all because you can generate those also. I don't know if the data set is still available. So you have to figure out if it was real or not okay interesting so that would be a good good project for people to play if you can write a detector to classify whether something so yeah in other words in our language of gan you're writing an efficient right discriminator yeah so uh everything is fake none of these are. So I'm kind of trying to understand the background fake or the person everything is fake? Everything is fake. The eyes are fake. Everything is fake. Everything is fake. Everything is fake. Okay. And that is the amazing thing. Go to this website, this person does not exist. And can you imagine that machines, just a little bit of mathematics later you end up with you end up with a machine that can produce generate data like this yeah right this is scary yeah so this is the power of GAN so now that I have given enough to you about it I hope I piqued your interest to understand the mathematics which we are going to do now. So how does this machine work? How do we do this? Like see remember I said that it is a game between two adversaries, the discriminator and the generator. So let's work out the technical details of how in the world this is even possible. Are we all ready for it now, guys? All game? Isn't it fascinating? You'll be surprised how easy the mathematics is. And this was the landmark paper of 2014 that started all of this. And after that, then there is style Gans. Oh, by the the way i didn't give an example maybe i should do that because the theory will continue for quite some time let me give an example of the style gans neural style transfer and guys we will just download these models because it's very hard to train these models on your own machine so let me go and do style train these models on your own machine so let me go and do i think kate has a nice project on the restyle stanford yes yes it was part of our project for 2020. 2020 when we do did this course it was actually a style again was part of the project transfer make so pick art so i'll give you an example of how these things work try on your own or jpeg or maybe. Neural style transfer is social media, free code, creative art. Is this the website to do it with? Create. Let's go create. No, this is bad. Neural style transfer. Neural style. Okay. Turn your photos into high definition art. Get started. Do any one of you have a picture online? But I'll just take examples. This can take quite some time to do it, but guys, you'll write this code when you do the computer and the vision part, we will be doing all of this as your homework. And even now you'll do a simplified version as your lab work so look at this original picture is the picture of a person right uh let's go here this is the picture of a person you're taking a van gogh style and you're asking when this style gang to render the original picture as though Van Gogh would have painted it. And I'll let you decide whether it looks realistic Van Gogh or not. To me it does. But... Asif, what's the name of the site again? There are many such things. Neuralstyle.art Again, as I said, there are many, many such sites. Play with that guys. People are even trying to create art and sell it. They will take your picture, make it into Van Gogh's art or Monet's art and then they will say, we'll print and send it to you and we charge you this much. Which is that website where you write text and it generates art? Oh, that is the DALI. DALI. So that's beyond Gantt's. It's a different neural architecture. We'll come to that. OK. DALI, by the way, has a, when I cover dally on the sunday research uh now there's a dally too okay so it's already in your second generation i don't know if they're approved or not okay but you do so yeah yeah you should play with that it's a bit just a couple of shots from my neural style app okay so uh anyway that is a different neural architecture but since sachin brought it up i'll show it to you guys huh dal become very popular yeah d that is a different neural architecture, but since Sachin brought it up, I'll show it to you guys. Become very popular. Yeah. DALI is. Let's say what you can do is you can create use this to create impossible things, for example. I think there is a way to render it also. Right. There's a website where you can enter text yes um what is the url for that api no yes absolutely no no no not this one uh so what they've done is they've given credits to people so like you burn credits pretty fast Yeah. How to use it. So anyway, I can't quickly. This is, by the way, some of the things we will do in computer vision. In the computer vision class, these are real things. So I don't want to jump the gun here. OK, you can play around with it on your own. I won't jump the gun. But the bottom line is you can ask the system to do impossible things. You can say, create a chair in the shape of an avocado. And it will create all sorts of beautiful chair designs that people wouldn't have thought of into the shape of avocado. And that is the amazing thing, like the website of this, but that is beyond just GANs we we need a little bit more GAN is a Delhi is an architecture in itself so do you notice this here an armchair in the shape of avocado and it produces these chair designs and most of you would agree this looks pretty realistic isn't it would you all agree that this looks realistic. You can say an illustration of a baby daikon radish in a tutu walking a dog. You can ask for the most impossible situations, and it will start doing it. So the world has moved. And you can do, like for example, would you be able to tell that these are not real pictures? A storefront that has the word Open AI written on it. Look at this. Asif I posted the link where you can type text and get images. Yeah, on Slack. Yeah, very good. Nice. Now you can make sketches and you can say well, render this sketch into a picture into a real photograph. And so you know, you drew these cats and it became picture into a real photograph and so you know you drew these cats and it became it became exact text prompt as a sketch ai generated images etc the exact same yeah so let's go with the link kyle has now posted uh this is a mini one. Okay. Let's do that. It is word guys for you guys to get interested. Let's say, I will say. Let's see. This is a mini-dally. It may not be as accurate or as good as the... Sorry, not mini-gain. It's a mini-dally. It may not be as effective, but we'll see. It does take time. It does take time. It will come after some time. Yeah, that's why I was saying that. Yeah, right. By the way, guys, why don't we do this? DALI was not part of this course, but if you want, I can make it one of your own books. I can give you the... You want to hear a tidbit about Gantt? Go ahead. Ian Goodfellow. If you're saying, could you come closer to this so everybody can hear it? So basically... Come by. Ian Goodfellow, right? No, no, hang on, hang on. Come by closer. Actually, come up to the stage so everybody can hear you. So can you hear now? Yeah, I think people, remote guys, can you hear him, Sachin? Yeah. So there was a week where the guy who did yarn networks, right? So he was saying the two, three days before he was having a very intense discussion with another two guys and they were somewhere in Stanford or somewhere and he actually got a brain stroke. He was admitted in the hospital. He was there for a week and he says he's lucky to have come back like things. And all the work that he did is collection of whatever he remembered prior to that, before the stroke. So essentially, the amount of math involved and the thinking involved is so that's his dissertation, right? Yeah. So that's, that's why I wanted to say that how complex it is. Yes. So he was almost so he said that I was feeling that's that's why i wanted to say that how complex it is yes so that's the so he was almost so he said that i was feeling that my life's work is going to like i may not remember anything so whatever he wrote is what he remembered the prior the before before where they had this intense discussion in the garage or somewhere so he says it was like the smallest of place and it kind of went over two, three nights non-stop. Wow. The lesson is must sleep. But he said that his brain was so... he says it's from that intense thinking that he most likely got it because it must have been very emotional. That's how he got the Gans. Yeah, yeah. So he was sick. Literally lost my life for Gans is what he said. Very interesting. So guys, we got a monkey with a banana with banana in its hands. How many of you would consider that this looks pretty realistic isn't it but yeah that's great but what if we made it something completely odd something what doesn't eat bananas fish a fish eating what's that ask for a horse with noodle legs and these are not convincing okay a fish eating bananas eating and apple let's try this fish a fish doesn't eat apple uh it might come back after some time and this is the last example after that we'll go but by the way dali is a different architecture based on the idea you can see the evolution of generative models guys based on the idea. You can see the evolution of generative models, guys. We just talked about generative models. The mathematics are so simple in some sense. It is just joint probability distributions. And you're causing a machine to learn this joint probability distribution. Well, here it is. Not realistic, so it made it cartoon. oh yeah it looks pretty good the fish has a smile to open trying to eat and it's a there's an apple nearby. Okay. Yes, with a worm in it. Animation jobs will be lost. Guys, programmers are programming itself. Now these generative models, you probably know the GPT-3 and many of these transformers, they're writing code and they're writing much better code than most developers write. So world is changing very rapidly, guys. AI is eating the world for lunch. If you're not doing AI, in some form or the other, get ready to change your profession. Okay, so now for the mathematics of it. Let's work through it. You'll be surprised at how amazingly simple the mathematics for this is. It is simple only if you understand it. Of course, it is not really simple, but we'll try to make it simple. So once again, let's get back to theory and brass tacks. We have two core entities. We have the generator. we have the generator what the generator does is given an input z right you give it some arbitrary input or input goes this this is called a latent vector it's something hidden hidden means it's hidden from the discriminator, hidden from the output. You don't see it, right? So you do that. It will produce output g of z. So you say, all right, some arbitrary function g, it will do a transformation of the input. Now what you want to do is, let's say that you're looking at a picture like we took a counter, by the way, don't take counterfeit bill as example, I don't want to be the guy who created a whole bunch of counterfeits, currency counterfeits. So, we will use and as all textbooks do, they use handwriting digits as their typical example so no counterfeiting currency bills right um obviously i don't think it's even legal like photocopying or even taking pictures of currency bills to my understanding is illegal i think the printer doesn't print it that's what people say i don't know printer doesn't print it and if you if you photocopy it yeah it changes the geometry right something like that so um so we'll take digits we have all these handwriting digits are you guys familiar with the handwriting digits and by the way they know when it is printed from which printer it came out oh yes that the code gets embedded somewhere yes so that people know yeah right oh thank goodness imagine what a economic disaster it would be if people could actually i i i'm told that government has a whole branch which tries to protect the integrity of currencies because it would be an economic collapse without it thank goodness i mean every time i look at a new currency bill i'm thoroughly impressed like it's just amazing how much technology goes into it anyway so here we go this is your cop discriminator What the discriminator will get, there's a pile of real data. It's a real data. So think of real digits sitting here. This typically in the literature is considered x. x represents genuine data. Yeah, turn off the lights. Oh yeah, turn off the lights. Oh. Genuine data. No, the lower one, the lower one. Yes. So genuine data is x. What it produces now... No, no, no, I don't want this one. Well, you don't want this one. Yes. Yes. So the input to this system could be either this or this. Never at the same time, of course. Now, the cop's job is y hat is real, which will mark as 1, fake, which will mark as 0. It will mark as zero it will predict something close to it so in other words let's say that it predicts y is equal to 0.98 right y hat what is it trying to say y hat is closer to one or zero at this moment is very close to one so it is the discriminator is saying that most likely this is a real real digit all right it's a real data it's not a fake data all right for the sake of our storytelling we'll continue with the currency as a example so now what happens is in the beginning, go ahead. How does the discriminator know it is real or fake? It's whether it is zero or one, unless it knows the universe of all the possible combinations. Yes, we're coming to that. We're coming to that. So all we know is that this discriminator, so let's go to the discriminator since you asked that question we are building up the mathematical terminology carefully here discriminator and i'll write in smaller handwriting if it is not visible on the screen of those of you who are remote please let me know discriminator what the discriminator will do is it will take a real for real data x it produces discriminator will produce let's say y hat y hat is equal to d of x let's say some function of x, there's some transformation, because a neural network is a function, right? So it will produce a d of x, right? And what you want to do is, when you want to train it, you want to say that the loss of the discriminator for real data is a loss between the real the prediction dx you want dx to be as real as possible right why real why real maybe one but i'm just writing it in a notation because it could be one two three four five multi-class right so you want to say the probability of it being real versus real that would be the gap would be the loss function in other words the loss function of the discriminator for real data is low loss if dx is essentially y real so far so good guys this is standard classification are we together yes and the the classification problem this is typically this could be a logic basic larger the uh binary if it is just two class classification real binary cross entropy Okay, binary cross entropy loss. Remember what was your binary class? It was basically log of dx, right? Multiplied by Y real, this, for real data. Now for fake data, data, let me do this, for real data. For fake data, what do this for real data for fake data what do you want fake data what is the input input is input is coming from the generator right input is g of z so output So output is basically D of input. And so it is equal to D of G of Z. Isn't it? So what do you want? If you want to do the last part here, So what do you want? If you want to do the last part here, loss for the, so let me just call it real, a D discriminator, fake, will be, basically, what do you want to what? It should be very close to what? Why? Fake. It should be able to tell that this is fake. Isn't it? If you give it a fake data as input, you want the discriminator function to predict that it is fake. Isn't it? In other words, you want, ideally, you want ideally D of the fake data, discriminator function of the fake data, to produce the result saying that this is basically close to fake. It is fake. So while training, we do know we do have these labels. Yeah, we know that. In training, we know know, we do have these labels. Yeah, know that. In training, we know whether we are feeding a real data or we are feeding a fake data. OK. A discriminator will try to maximize its ability to tell the fake and the real apart. Isn't it? It's trying to maximize this. What is the counterfeiter hoping on the other hand? The counterfeiter is trying to counterfeit a bill in such a way that the discriminator has a hard time telling the two apart. Make sense? The counterfeiter is trying to minimize the gap between the real and the fake. Sorry, between the counterfeit and the real thing. It's trying to make the counterfeit look as much real as possible. Would you agree? Yeah. Yeah. So now let's look at the loss function. So the total loss function for the, so for discriminator, so for discriminator, the loss for one item, one item of data, whether it's real or not, the loss of the discriminator would be the basically trying to minimize the gap between the real data and why real, plus the gap between a fake data, fake D prediction on a fake date, fake D prediction on a fake data and Y fake, right? These are the two terms. Now you agree with this part guys? So far for the discriminator, this is for the real part and this is for the fake, right? If you minimize both of these things, it can tell the real as real and tell the fake as fake. It's a winning discriminator. It's a winning cop. It's a good Sherlock Holmes. Would you agree? So you say, well, now the point is we glossed over something. What did we gloss over? In reality, when you train a neural network, you give it a mini batch of data, right? Endpoints or batch of data or something like that, a mini batch or whatever it is. In each step of learning, when you do gradient descent, you need to give it endpoints. So now let's bring in those summations and other things over so more properly when considering a batch size actually should be mini batch size n loss over d loss function of the discriminator is the usual thing, average loss over I is equal to 1 to N of the same thing. Guys, can I just leave it as that? Right. So. This previous one, this entire thing, I don't need to write it. So you assume that it is there. That's it. So you have to remember that you have to take care of that. No, that's all right. But what about the generator? Generator has an opposite goal. Generator says, you know what? Every time I produce, you take some input z, latent vector. It becomes gz. What would it like to do? And this gz is given to the discriminator it becomes d of g of z right this is the y hat that's produced y hat the last function you want to minimize you want y hat here of the fake. So y hat of fake. What do you want? You want it to look as close to real, y real, as possible. Isn't it, guys? You want it to look like the real thing. Does this make sense? The generator, the counterfeiter, would like to produce such a data point, GZ, such that when you pass it to the discriminator, the whole thing begins to smell like the real deal. Would you agree? So it should resemble some real value, some real, right? Discriminator should, basically, you need to fool the discriminator into the belief that this is real. So you want a loss function, the loss for the generator. Generator, of course, never sees the real data. Real data, it basically wants to minimize the gap between D of G of Z and Y real. In other words, it wants to fool the discriminator into believing it is real. So you have a winner. If, for example, this turns out to be 0.99, discriminator says, I have 99% confidence that this is a real data point. This is a real currency, let's say. In that case, the counterfeiter has won isn't it would you agree he just successfully produced a currency that is being considered real by the cop by the discriminator and that is the goal of the counterfeiter would you agree guys yeah that is it so the loss of that and then obviously to be more precise again with n sample in batch, what do you need to do? Loss is just being more precise is equal to 1 over n of the loss of d g z y real summation over i is equal to 1 to n the usual stuff i won't i won't uh go over the point so guys it looks without interpretation this board that i just produced here this might like confusing, but I want you to absorb this. With the explanation that I gave, tell me if the entire mathematic looks self-evident to you at this particular moment, the way it's written. What's that first character on the generator? Is that Z? Z, Z is the input, whatever input. So I have been silent about what the input is. Why should it be latent? Yeah, the reason for that is, means something goes into the generator that makes it produce GC. Now I have been silent about it. Anything that you can't see is considered latent, means it's internally generated and given to the generator. So typically what happens is, internally generated and given to the generator so typically what happens is typically oh so okay so what you mean is there's a box outside of it with generating you cannot see it's not something that is coming from real world it is self-generating and then giving it to you yeah for gans okay what happens is that for gans most gans uh gans Z is nothing but sampled from the normal distribution, 0, 1, right? It's basically noise. That's why it's called Z is basically a Gaussian noise sample. Right? You just sample instance. You just take a bell curve and randomly pick a point and you pass it to the generator. So why I'm asking this is, yeah, I wanted to generate cartoon images for the medical applications. Right. Which actually looked like the real tissue sample. And my take was this, rather than generating this randomly, you actually take it from the biology properties and generate it. So that I talked to this guy who runs the Google mind, right? With him saying that, how do you care like from where I got it, as long as I'm able to take the real one and make it look like a tissue sample. Because I know what that structure and all is known because you know in nature, like the tissue is not going to go. So rather than taking this long route to generate it, why can't we from a smaller subset of known data generate it, but still have the discriminator be able to fix it because then you can generate within 5000 or 10000 images, you can create a larger set to train it. See what really happens, Sachin, is these GANs are very effective. So the training that takes a while, but it doesn't take that long. What you are referring to is beginning to look like something a variation of GAN called conditional GANs. In conditional GANs, you have more potential, you say you don't just give it a random noise, but you actually give it an instance, you feed it the label. You say that okay, this is what Y should look like. Try to imitate this. Those are conditional GANs and they work. So as I said, the GAN world is very rich. It's a whole world in itself. You enter that world and you can just stay lost in it for a very long time reading all the papers. There are entire websites devoted to all the different research papers and varieties of GAN. And now what I would suggest, and this will be your lab, the GAN, so the lab that I'll do will write a GAN from first principle, that's okay. But in practice, what you do is, for example, PyTorch hub, P hub, it has already put well trained GAN models. So for example, Facebook has contributed a GAN that produces the same pictures that you were saying this person does not exist, something like that, it will produce it for you. So one of the lab exercises that we will do in this is, we will try to use it, play with it, and see how well it works in this course, because here we are more focused on the foundations, on the mathematics, on the theory, but later on when you come to the computer vision class, we will go really deep into the image-related GANs, very seriously, image and video related GANs. We'll go into it more seriously and we will study it much more. So again, first crawl, walk, run kind of a thinking. So this course, as I again said, it's foundational. So now you say, well, this is lovely. Now this mathematics that i wrote guys once again please tell me that this page read it i'll give you a few minutes to absorb it don't be shy tell me if i if i need to re-explain something i'll be happy to do that anything that you guys want me to explain? Anyone who feels they understood it, let's ask the question in the reverse way. So Z is in N, which is natural. No, no, N is the Gaussian, Belkin. Gaussian noise. Normal noise, yeah. Yeah, you're just picking something, a sample from a normal distribution. That's all. And this is the magical thing. So guys, observe something very interesting. The generator, the counterfeiter actually never gets to see the real data in the classical form. It's a very difficult task. So what will happen is, in the beginning, the discriminator will win, right? Because what will the generator produce? Generator will produce let's say you're talking about digits, generator will produce some random image with dots and white spaces there. And it will try to pass it off as a digit that the discriminator what will it do, it will very, very easily be able to tell the real from the fake because in the beginning, whatever the generator is producing is essentially very close to noise. Are we seeing this? So the discriminator has to be faithful in this thing not to trick the generator, right? That is right. All that the discriminator does is, discriminator is just a classifier, that's all right. It's a pure classifier. Now what happens is, so it is happily saying, yikes, this is not, for example, the book example says nine with a tail. Discriminator says, nines don't have a tail, horizontal tail at the bottom, right? So you throw it out. and then after a little while the gradient of the loss when you back propagate it is now drifting towards one of those states or one of those images by shifting the the random patterns in such a way that now it is becoming more structured it's beginning to produce a shape that discriminator does not produce so much loss on because you're doing gradient of the loss, all that the generator needs to do is change the image slightly, right? And then see how much loss comes out. If the loss is less, good. Now, I need to change it a little bit more. And I can keep changing the loss, use the gradient descent, and so on little bit more. And I can keep changing the loss, use the gradient descent and so on and so forth. The generator can keep shifting the shapes till the loss gets really minimized. And what happens is that generally the discriminator wins in a classical GAN. Discriminator, the cop is always better than the counterfeiter, which is true in real life also. The whole reason that you don't have counter currencies happily floating around, thank God, is because the people who can detect it are far, far smarter, right? They have better technology and they know it. So discriminator has a much easier job. All it has to do is classify. Generator has a much easier job. All it has to do is classify. Generator has a much harder job. Generator has no clue what real data looks like, but it is trying to learn it through back propagation. So discriminator generally outperforms the generator, but all that the generator will do is it will make sure that generator doesn't outperform it by too significant a margin it will decrease the margin so that it ultimately takes you to a state at which the discriminator does make mistakes so some counterfeits begin to look real but even at that stage the goal of this while you produced a discriminator and a generator the point is not to create not to defeat the discriminator. Discriminator will always do a little bit better. And there's a mathematical reason because discriminator does quite literally a discrete, see the word discriminator comes from the discussion that I just gave, namely that it is a classifier is a discriminant model, right? Whereas the generator does a joint probability distribution. Joint probability distributions are always harder to compute. Generative models are always harder to build. And the generator has a harder task. So the discriminator begins to win. Now, unfortunately, it leads to a problem in GANs, which is called mode collapse. It's been a pretty serious problem. What the mode collapse means is that, see, once the generator, let's say that you have 10 digits, 0 to 9. Once the generator, let's say that produces something that looks like a 0, and the loss function begins to minimize guess what is in the best interest of the generator to keep the loss function low it can just go on producing more and more zero like things because it has learned to produce good zeros right so it will basically say it is safe for me to just produce zeros because anything else i produce looks wrong let me produce zeros so it won't produce other things very much so you'll see this in the lab the classical gan it will produce some of the easy easy digits well zero three maybe eight some of the ones it will tend to produce much more and it will not produce harder ones like four etc etc some things that are harder it won't produce so much of that right so what has happened is the generator has gone through a mode collapse the word mode collapse is very interesting i'll explain the word mode collapse. So why is that? Yeah, I'm explaining that. Because it's lazy. Yeah, so imagine that. So again, let's say that you're trying to do, let's bring back the example of cows and ducks, right? So let's go back to our example of cows and ducks. So you have the cows here and you have the ducks here. Let's go back to our example of cows and ducks. So you have the cows here and you have the ducks here. Let us say that the generator has learned to produce this, the ducks, right? In the beginning, the better it does, it produces ducks, it learns, it does gradient descent. So soon it will learn how to produce ducks. But then when it comes to cows, it learns, it does gradient descent. So soon it will learn how to produce ducks. But then when it comes to cows, it tries to predict something here, something here, something here and it is getting much greater loss. So what may happen? Well, cows and ducks are just two class, so it will probably succeed. But when you take the nine digits, zero to nine, what will happen is, think of it like that. There may be something else, let's say three, zero, and then it sort of samples and produces this and it gets low loss. So it sort of encourages the GAN to produce more of trees or try to produce more of trees and lies. Because if you look at the loss function that we related that we wrote, look at the loss functions, all it is doing is trying to minimize the gap between what it produces in a real one. So, so long as it keeps producing what it has learned, produces low loss, it is doing very well. So it tends to overlearn in limited areas. The word mode stands for most frequent area. So because these are bell curve distributions, mode is peaked. So what it means is that it will pick up a few of the classes that it learns early on to get it right and it will just keep producing more and more samples of that does that does the reasoning seem reasonable if you were faking it right you have to fake currency is one dollar bill what is the next bill is there a five dollar bill five dollars and five ten there are twos but not many not many and maybe hundred dollar bills sorry is there a two yeah there's a twenty dollar bill fifty dollar hundred dollar right is there a thousand dollar bill until there's one for bank transfer interbank interbank or something like that okay oh my goodness okay so let's say that you have a lot of bills. You have been trying to fake all of them. And it is hard, you know, a $100 bill is really hard to counterfeit. And now you take bills for many generations, you know, 2000s and 2020s and 1980s and so forth. You will soon figure out it's a lot easier to fake something in the 1980s let's say one dollar bills of 1980s rather than 100 bills of 2022 right so what will be your tendency you will go where you know you'll exploit so between the exploitation exploration spectrum the exploitation exploration spectrum not learning one digit at a time like zero no no no no it is yeah all zero to nine all of them because the image that is fed into the jet into the discriminant could be any one of those digits real digits when you train it right no but what prevents you from learning how to fake out zero first then make a one first okay no no no you don't that is gets into conditional gains then you're saying let me fake let me train it to do each digit separately right you don't do that what you typically in a traditional gang what you do is you can take any one of the real digits and now you say generator generates something that looks like this and you look at the gap between right so what happens typically is, because you're averaging over N samples, you just need to get- The easiest one. The easiest ones, right? And you're doing pretty well, right? And so that's what happens. That's a more collapsed problem. So what you need to do is, and there's more to it. Like what happens is that there is, because we use a sigmoid in the generator there's a bit of technicality you have the problem of vanishing gradients and so on and so forth we won't let's not get into too much of that so now we understand what mode collapses just keeps producing producing samples for a subset of the real data. Right. So in other words, real data classes. Suppose digits are there. 0, 3 will start predominating. And I don't know what else is there. 8 will still start predominating. So these things will start predominating and some other things won't get done at all, right? Or will be done very poorly. So that is mode collapse. So how do you prevent the mode collapse? What we need to do is we need to handicap the discriminator it has undue advantage right it has a much simpler problem to solve you need to add some penalty terms to it right handicap it in some way so there are many many techniques to handicap it one of the techniques that is a fairly popular and is not very old actually, I believe it has just come out in the a gradient penalty. So let me say Wasserstein was obviously the creator of this. He has a concept called the Wasserstein distance and so forth. He has a concept called the Wasserstein distance and so forth. We created this concept. GAN is gone. This is gradient. Now, that is quite a. Well, you look at it, it's pretty scary. And by the way, I have explained the math in a way that is intuitive. At some point, I'll put it in the very scary formal way that you see it in papers and you'll see that they are exactly the same. So this particular thing, what it does is, it says that discriminant, the loss function of the discriminant, let us change it a little bit. We make it the loss function of the discriminant. And just forgive me for not putting all those one over and averaging and all of that. I'm skipping that, guys. I'm just being sloppy, right? And what it is is that you're saying that discriminant this b is the g of z so what do you want you want this thing discriminant g of z when this number is high like let's say that you produce a fake and a discriminant says a high number 0.99 probability that it is uh it is real and then it has gotten it wrong isn't it would you agree then it has gotten it wrong so loss it there is more loss then there is one more term there is the d of x now here high number is good or bad dx if you say the probability that it is real for a real data point the number goes from 0 to 1 in the range of 0 to 1 is a high dx good or bad for real data good for the generator or for what the discriminator we are only looking at the discriminator so discriminator would be happy right when dx is large and so if i put a negative sign basically you're saying because it's, let's have a negative penalty for it. High positive penalty when it is wrong for the fake, negative penalty when it is right for a genuine one. So these two terms are pretty much understandable. Right. But then what it does is it adds a term which is scary. So that's one way to put it. What it does is it takes the gradient of the discriminant value. But what it does is it is a, remember this is fake data fake this is real now what it says is it takes a data that is a mixture of fake and real so what is real data x what is fake data g of z would you agree g of z is the fake that the generator is producing and it is saying take some arbitrary mixture of the real and the fake so how do you do that here is a little bit of mathematics that you should know suppose i have a value a vector a value uh let's say 10 and i have a value here 20 and I say a pick an arbitrary point that is a mix that is somewhere between 10 and 20. Are we together? So for that what you can do is you can say let me go and take this thing you would say that, let me go some arbitrary distance. So this distance is 20 minus, actually, let me make it 25. 25 minus 10 is the distance, isn't it? So you say that I need to do some arbitrary amount of distance. I need to go towards 25 from 10. Would you agree with that? To get to an arbitrary point x, x would be epsilon times d plus 10. 10 because you are starting out at 10. Given the fact that origin is here zero would you agree that this distance x is epsilon times d plus 10 10 is the starting one do you agree with that guys seeing this picture right so epsilon is a number between zero and one yeah yeah so it's some fraction between zero and one what you're basically saying is that go some part of the distance, the complete distance from 10 to 20, the total distances. So let me draw it out again. 0, 10 and to some value. I'll just take 25. Right. Now you're saying if I want some point X. are saying if I want some point x in this interval, right? So let's say x is this, x is this. Now the question is what is the value of x? How would you do that? So you would say x is equal to, would you agree that it is this distance plus some arbitrary fraction of the distance from this. So the distance between 10 and 20, 25 is 25 minus 10, right? So let me just call this A, B. Now let's generalize it. Distance is equal to B minus A. So you multiply the d by some number in the between 0 and 1. So if you multiply d by 0, you're not adding anything to it. Now, you say x is equal to d. So first of all, you have to add 10 because you're starting from 10, 10 plus. Would you agree that this will take you to some point in this range? X will be in the right place between A and B. Does this make sense, guys? It's very simple algebra. Guys? Yeah, it makes sense. guys yeah it makes sense it makes sense it does it simply makes sense right so any number between zero and one and let's call this epsilon well upside i don't want to use the word epsilon is suppose i use the word epsilon with this symbol, which belongs to, because it's so close to each other. Let me use another symbol. Let me use alpha. Alpha belongs to the interval zero to one. It can be anywhere between zero to one. So X is equal to 10 plus alpha times D. But what was D? D was, sorry, and now i can replace this 10 with a so this what was 10 can i replace it with the real value which was a generalize it a plus alpha d so now a plus alpha alpha, what is d? b minus a. And so this equation becomes, it becomes alpha b plus 1 minus alpha a, right? So any arbitrary point can be written as like this. Am I making sense? I'm just rearranging the terms. And so now in your notation, suppose I, so this was X, actually let me not use X because X in our notation is, I want to bring it back to the mathematics. Let me use Q, some Q point. Q, this is Q. Let me not use, because I point, Q, this is Q. Let me not use, because I'll tell you why I'm not using X. I don't want to mix up the notation, Q. So Q is this. Now let's map it to what we have. A and B are two arbitrary vectors, right? So if I give you two vectors, x and g of z, a is equal to x, b is equal to g of z. And I ask you to create a mixture, something that is in between the real and the fake. Real, x, and the fake right what would you do you would say well that is easy i need to take and replace epsilon now I used alpha. So in this, in GANs, WGANs, it is common to use epsilon rather than alpha. Just a change of notation. So then you would say, well, that is easy, all I need to do is epsilon. R. Vijay Mohanaraman, Ph.D.: Of one of the things be. R. Vijay Mohanaraman, Ph.D.: Plus one minus epsilon a. R. Vijay Mohanaraman, Ph.D.: Right or you could interchange the two it doesn't matter right one way or the other, and so. way or the other and so that is what it actually the way the gans do it they do it the other way around they call this uh so here's the thing okay let me leave it as a actually i gave the notation as suppose i make b here suppose I make b here here and a to be this in our argument doesn't matter so from fake to real right so what would happen is this would become epsilon x plus 1 minus epsilon g of gz would you agree guys right so this is now this vector let me call this vector the m vector mixture is a mix of real and fake are we together so suppose what instead of giving either the real as input or the fake as input, now you're giving the discriminator, the cop, a much more advanced problem to solve. So you're hand- Airbrushing, right? What's that? You're airbrushing the real image, say with some noise in it. Yes, in a way. Of course it's a big- Yes yes that is one way to do it you take a real person and maybe you add a mustache to that person slightly fake it right suppose you do that and now you ask the discriminator well is this real or fake right so the discriminator will say Well, is this real or fake? So the discriminator will say, Naita, the cop, poor fellow, what is the best it will do? It will say, GM. Well, this is good. It makes some prediction. Not GM, sorry, what nonsense. DM, discriminant. Discriminant does DM, right? D of the mixed vector. Now you say, but how sensitive are you to this entire faking business? are you to this entire faking business? Right? Like how much does your decision change by adding very small amount of fake? You know, when I'm sliding between the real and the fake spectrum, how sensitive are you really to this? So remember sensitivity, one of the measures of sensitivity is the gradient. So let me do that. So let me ask this question. So let me leave it this way. A discriminant or fellow will say, now we say, how sensitive is the discriminator discriminator for m like for small changes in m how much does it change this decision right so that would be like for example remember that if you have a function, just to make your recollection, if you have a function, and you have a steep gradient, steep slope, what does it mean? Small changes of value will lead to large changes of the y. Would you agree? If this is x, small changes of where the slope is high, small changes of x will lead to vast changes of y. In other words, the derivative and its generalization the gradient they are measures of sensitivity of the function to changes in the input that's the best way to think that's the best intuition you can carry about the gradient of this thing so well how sensitive this will be given by D of M, right? And pretty much we are getting very close to the main intuition, which says that the function is we don't want this sensitivity to be too high, right? So what we do is we say, hey, let's add a penalty term remember in the l2 regularization that we did yesterday night what was the penalty term weight squared the circle right so the weight squared wi squared was the up to a lambda parameter so that weight of the model is somewhat analogous to that for regularization. So it is basically regularization. We say a dm squared. And then you say you further subtract it from this value you wanted to be closer to one for reasons. The rest of it is technical detail. So you want to do that and then you square it and then this is or L2 norm. And this goes to the regularization theory that you have learned from me. This is it. So what you do is this is your penalty term. Now, what happens is, remember, ridge regularization always has a lambda parameter. So therefore, let's bring in that lambda also. So you have this gradient of B of M, right? Ridge regularized minus one squared, right? You treat this as your penalty factor. So your penalty has a gradient. It is a gradient penalty. So your penalty has a gradient, it is a gradient penalty. The more sensitive the discriminator is to small changes between fake and real, the more you penalize it. You don't want it to go. So the end result is you make you give more of a chance for the counterfeiter to catch up, to basically learn to outdo it, to come closer to it. So that solves partly the mode collapse problem. It's not a fully solved problem, but it does make it better. Now, that is a lot of things. Now, let's rewrite this term in the conventional language that is written. What is a D of the gradient of the discriminator of the, what function? What was M? M was epsilon X vector plus one minus epsilon G of GZ. Remember, this is what M was, if you remember, a vector. And now the equation begins to look, when you put all the pieces together, it's not hard, but it does begin to look rather, let me make this equation, Let me make this equation squared. This is the penalty factor, penalty. And now let's write the total loss function for the discriminant. Discriminator. You have the loss of the discriminator is literally what is it remember the original terms you want to encourage it to where is it you want to encourage it to uh recognize fake as fake so you the more the the higher value it gives to fake and says it's real, the bigger the penalty, minus you wanted to give, you do want it to give good values, high values to the real data. And therefore, every time it gives high value, your loss is less. You want to go the other direction, plus the penalty factor, which is this. And by now, I trust you guys are feeling that it's all getting very matzy. But see, guys, here's the thing. Most of this mathematics is something that, I don't know, I always feel it needs a bit of explaining. Otherwise, you never get it, and which is what I'm trying to do, that this plus 1 minus epsilon g gz, I hope, along the way I did not screw up significantly, 2 norm minus 1, right, and if you are familiar with ridge regularization, if you treat this as the w, you realize that this equation, this literally is your w, this is the w vector, then this is your, literally, your w, w2 norm, right? W norm, w dot w. And this is your standard L2 regularization term. It's very interesting that that's what pops out. Except that this W is this interesting quantity. It's the sensitivity. You're penalizing the sensitivity of the discriminator to changes from, yeah. So that's that that is that that is the last function of the Wasserstein that so now you realize let's go review everything we learned because we did learn quite a bit and is it what's the time oh we have only 15 minutes. So definitely time to review in the next five minutes. Guys, is this thing clear? GANs are one of the most influential and dominant neural architectures. It is important to know it once properly. Those of you who are remote, please give me some feedback. Is it making sense? Jen, is it making sense the gin is it making sense jen is quiet amrit is it has it so far made sense yeah um yeah as you as you mentioned like if you just look at the math individually then it's kind of complicated but if you explain it in terms of like the number line you do and everything it makes more sense yes so go over this guys go over this and this is one of the the textbook that i've given is one of the books that tries to explain it well and i felt that even that was a little bit fast-paced so i further broke it down into smaller pieces especially for the Wasserstein GAN because Wasserstein GANs are very very uh respected and considered state of the art these days Wasserstein GAN because Wasserstein GANs are very very respected and considered state of the art these days Wasserstein with the gradient penalty is quite there and new things developments continue all the time and maybe next time when we are giving we are doing the same course next year there will be something else that is the state of the art it just speaks to how rich the field is right so guys this is it uh i'll review everything from the beginning for the generator what does the generator need to do generator produces from input the input for gans are trivial you just pick one from the random noise that's why it's called a noise goes in but it's not just typically gaussian noise goes in generator will produce any specific dimension to it right oh it's that has to be that is implicit so guys when the generator produces gz one of the basic things is its shape or vector space size as a vector must match that of real data so for example if it produces the two-dimensional data and the real data is seven-dimensional discriminator will have a field day yes so the dimensionality must match generator has to produce things that in every way looks like the real right the vector space the vector space must match so those things yeah but those are basic one of like uh 101 things otherwise your trouble is z is of smaller dimension than gz or how do they relate see it is not necessary okay one way to think of z is that theoretically at least one way to look at it is, you know, in the olden days before there were photographs, or even today, when you go to a police as a witness, you sit down with a sketch artist and the sketch artist will ask you, so, okay, you saw that criminal, right? That somebody who burglarized a car or whatever, what did he look like? And you said, the guy had long hair. So the person who's at his nose now, what else do you remember? He said the guy had a beard. Okay, right done. Thin or fat? Oh, it was rather thin. Okay, what kind of eyes? Say, oh, round eyes with, I don't know, blue eyes or something like that, something or the other. What colors can, something like that. You give a few descriptors. What does the sketch artist do? The sketch artist takes very little bits of information. Blue eyes, brown eyes. Hair, bald. Or, you know, lots of hair, some very little bits of information, but from that, because it is a human train generator, it is able to produce a picture of a human being, isn't it, with those characteristics. So that is the nature of Z, right? With very little, you can think of it as a stick figure how do you convert a stick figure into an actual photo right that's one way of right so that is and for example you do that now generally what happens is for gans actually you take very little bits of information you just take it off and ideally so for with a generator, because you have lots of layers in between, doesn't have to be, dimensionality of Z need not match the dimensionality of GZ. GZ will be an image, let's say 64 pixels by 64 pixels, right? Whereas Z has to have a dimensionality that is informative enough. Like it should have enough capacity to explain what is being produced because ultimately Z is being transformed into GZ, right? So you can't pick just a number and say, now from this number somehow produce a 64 by 64 image. That's too little info. So even when you give a random number, normalized number, give it a sufficiently high dimensionality, right? Rich enough dimensionality. That is it, it's a hyper-private. You can just pick, and usually that's not a problem. You take a sufficiently large dimensionality of Z, give it the very fact that even just taking normal noise, Gaussian noise works good enough for GANs, it speaks to the power of GANs. That is incredible. That's absolutely. It will do. Right. Yeah. Now, there is one more thing that I want to say. It is considered a min-max problem, and there's a bit of mathematics to it. I don't know if I want to go, actually, we're out of time. Let's keep it for later, maybe in the lab session. It is called in in game theory and in certain words you have a problem of min max in which one guy is trying to maximize something and one guy is trying to minimize something so look at it from the discriminator what is the discriminator trying to do you want the discriminator a good loss function in the ideal situation you want to maximize the ability of the discriminator to tell the real as real, isn't it? And fake as fake. And what do you want the generator to do? You want to generate data such that there is discriminator is hardly able to tell the difference between fake g g g and y g z and the fake right the very little gap right it's trying to minimize this loss that is why this sort of algorithms are min max algorithms and the name is there the adversarial you have opposite goals this is the zero sum game kind of thing that's why it's like that right but at the end of it what happens is that frankly both get both it's not zero well it, it's played as a zero sum sort of opposite game is trying to be. But at the end of the cop becomes a super cop, and the faker becomes a good faker. They call it a good counterfeiter. And because it becomes a good counterfeiter, you can throw that, for example, you can get rid of the cop. And now the counterfeiter will happily produce lots of counterfeits. like you just saw that we saw that lots of faces are being produced that don't exist right because the beauty is once you have trained a generator you have a generator model it can it can go on generating infinitely many samples that look real right very close to real samples that look real, right? Very close to real. And that is the value of GANs. You train the GANs to produce these, right? It can produce a whole lot of it. So think of it guys, GANs are something. Till now we have done autoencoders and feed forward networks, et cetera. Now we are getting close to things that have profound real world implications. I would strongly encourage you to what chapter of the book is it? Chapter, I'm going to have to put on my reading glass for book. Here's my book. It is chapter nine of your book, guys. Chapter nine of your book. Please do read chapter nine of the book, at least read the parts that I've taught you. There's a lot. There's Congelational Gants etc. and we'll cover those at some future date. If you are into then cover the whole chapter but at least cover the parts that I covered. And already it is enough for now. But then look around the world and look at the implications of GANs, what it can do, and ponder over the ethical implications. What it is, like we say that technology is a tool, it can be used for good or evil. When it comes to GANs, it sort of comes into focus. When you see with impunity how it can be used for mischief, it's a little frightening. And at this moment, we are in the opposite situation. The generators are doing very well and it's very, very hard to detect fakes. Because obviously the people who train the GANsans they will not give you their discriminator right but they will use a generator to produce fakes and now it's your job to figure out is it fake or real and you don't know the point is you don't know it's not label data so how do you detect fakes how do you detect deep fakes and that's itself an ongoing area of research all right guys so i'll end with that if you have any questions i hope you found this interesting very yeah so yeah see deep learning right now that we have built foundation, every subsequent part, every subsequent session in my view, you'll find it to be more and more interesting and we're just laying the foundations. Once we get to application, it just is fun, utter fun. All right guys, so I'll end with that if you don't have any questions.