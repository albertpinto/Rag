 So one way to think of specificity is specificity like in a disease is the last test or your last hope. Like if you have been marked as positive for a disease at the end of all your diagnostic and screening test let us say that you go to the one test which will clearly tell you whether you have the disease or not so that test should be high specificity test so if you don't have, it should be able to tell you you don't have it. So that is what specificity is. So in the early stages, let's say that you are looking for cancer, breast cancer in women or prostate cancer in men or some cancer. What do you want to do? The consequences of having it are pretty devastating. So the early test should have very high sensitivity, high recall, means the slightest hint that you may have cancer, you mark the person for further screening or further testing. And you only screen out people that you're absolutely sure don't have it. So what will happen is you'll end up with a lot of false positives, doubtful cases. Then subsequent tests should be more and more specific. They should weed out the ones who are negative. So specificity is useful for subsequent tests. Sensitivity is useful for the early tests when you can when you don't mind a reasonable number of false positives that's one way to look at it the next is what does logistic regression do well it's a it's it is obviously a process of classification of data with a linear decision boundary. I think most of you got this right. Next was a little bit hard for those of you who don't know calculus and those of you who have done engineering math with me and so forth, who have practiced basic calculus, but remember that this is fairly straightforward. If you have a logistic regression equation, this, the derivative of this equation of p in this is actually, you can write it in terms of p or you can write it in terms of x. So you wrote it in terms of p, this is correct, but this one is also correct. So this one. So this is in terms of the x, the input features. This is an implicit function of the response itself. Now, when we go to neural networks, it is this closed form, actually, that will be much more useful in back propagation. So this is again, kernel methods. Again, a kernel method question. We'll skip that. This question is logistic regression is a nonlinear classifier because the logistic function is nonlinear in shape. It is true that the logistic function is nonlinear in shape. However, the decision boundary that logistic regression makes is a linear hyperplane. Hyperplanes are linear. So the correct answer here is false. We'll skip the support vector machine that was for review. So this problem is something I sort of told you the story of a quack, right, in the class. Suppose you have a disease that's expressed in 1% of the people. Consider a data set that has sufficiently large random sample of the population. For each person there are relevant biomedical markers quantifying quantified as predictors. So if you want a classifier then if you want to have a classifier that acts as a basic screening during the regular medical checkup in order to rule out the possibility of the disease in the beginning what do you want guys do you want high set high false positive of what can you tolerate false positive or false negative just for early screen knowing that later on you'll do further screening for early screening the what what is more acceptable false positive right and so you want something that has a very low false negative very low false negative a low false negative is is that it is it is also called a sensitive test or the high recall test. So that was correct. And so they both mean the same. They mean the same. So it was a bit of a trick question and deliberately, you know, adding a little bit of a twist to this questions are preparing you for you. You guys, if you go through job interviews you can face this you should consider a classifier of some value if it has the following properties respect to the baseline obviously it must beat the baseline now we missed one question did we know okay baselineifier, what does it do? It ignores all the predictors and just looks for the most frequent, the most common class in the target. So if most people are healthy, it will just declare everybody to be healthy. It won't even look at the predictors. So what is the value of the baseline classifier? It sets a baseline, zeroR sets a baseline. You need to write a classifier that at least beats the baseline. If it doesn't beat the baseline, your classifier is no use. It's quite useless actually. So these are kernel questions. I'll skip those accuracy is always a trusted metric norm now actually see people often quote accuracy that my algorithm or my this thing is so accurate accuracy does not mean much for example if you're looking at a rare disease any lab test even the shoddiest of them will have high accuracy because most people won't have the disease and it will mark most of them to not have the disease. The question is whether it can identify the true positives. You know, the positive cases as true positive. So that was it. This was just for fun. I thought I'll add this cat climbed over the wall. Actually on my wall cats keep sitting and the cow gentle with the moon. It occurred to me yesterday actually, yesterday evening. Was it yesterday or the day before? It was a moonlit night. I had a big moon in my backyard when I sent in the evening I get to see the moon rise every day just when the Sun is setting on one side on the other side of my back here I'm seeing the moon rise so you see the moon rise and then you see a cat sitting on the wall so then this sort of thing came to my mind. Anyway, we won't go over 19. Precision is, what is precision? Precision is exactly that, the proportion of true positive cases and. That is the precision. So that is that. This matching is correct. Obviously true positive specificity is the true positive, true negative over negative. In other words, what proportion of negative cases it could mark as negative think of specificity as a robust rejection test so when you're doing the ultimate test for cancer before you put somebody through chemotherapy what is the one test you want to do you want to do a test with high specificity accuracy is that like how often it is correct precision is that the true positive how often did it get to the, you know, the true positive over false positives. Recall is true positive over positive. In other words, what proportion of the positive cases are actually positive. are actually positive. Specificity is the opposite of required. That's why a lot of people they tend to use, medical people, they use sensitivity and specificity because their definitions are so simple. Sensitivity is true positive over positive. Specificity is true negative over negative. Error rate is of course the off diagonal in the confusion matrix. False positive and false negative, the proportion of those. What is the F1 score? F1 score actually is just the harmonic mean of precision and recall. So when data is reasonably balanced, so you don't really care that much about one overwhelmingly about either precision or recall, one good way of taking an average is to take the harmonic mean rather than the so what happens to a harmonic mean you will if you think a little bit about it you'll realize that if either precision is too bad or recall is too bad f1 score will decay. You can convince yourself that it is so by thinking about the harmonic theme. So 1 over F1 is 1 over precision plus 1 over recall. So what happens when, let's say that the recall is very very low. One over recall blows up and it washes away one over precision. And because it's very large, then when you compute F1, it just becomes one over the battery, a bad blown up inverse report. So, and the same thing is true for the precision. So how many mean is the F1 score people value precisely because both precision and recall have to be good for F1 score to be good. You don't always want that. Sometimes in the literature people will say that we prefer the F1 score because it's a composite score. But you know situations sometimes demands differently. So use it judiciously no one metric is the best specificity and precision are not synonyms logistic regression is a method commonly used to perform non-linear regressions now the the word regression often trips people it's a classifier of course we know that and then moving onwards to it, which of the following could be considered an equation for logistic regression. So here I Think many of you got most of you got this right? I taught you these two forms and if you just multiply the numerator and the denominator into the This exponential you'll realize that this is the same as this. So it was nice, Shankar, you got this right. Then to try to capture a nonlinear decision boundary with logistic regression, people often say that you can't do that. It's a common thing I hear in interviews, for example. Actually, you can. You can do that by mapping it, using, expanding it to polynomial basis or using a kernel mapping function. Both of these techniques we did. For example, we could do rich data set. If you go and try classifier three, which is the egg-shaped data, you'll realize that if you use polynomial logistic regression, or logistic regression in polynomial terms, it will solve the problem. Likewise, of course, a kernel method will solve the problem. Likewise, of course, a kernel method will solve the problem, right? If you map it to a different space, it will solve the problem. We went over this in the lecture very recently, so I'll skip that. This question is a little tricky, and I don't know how many of you got right. So if you have a disease rate, a disease that expresses itself into only 1% of the population, then a classifier with an accuracy of 99% is good or bad? It is actually useless because it's the baseline classifier. If you mark everybody as healthy, what would be your accuracy you realize that it could study nine it could still be 99% so a 99% accuracy has only maybe a marketing value it has no scientific value in this situation so your classifier would be useless recalling sensitivity or synonyms. Yes, that is true. And finally, the last question, sensitivity is the probability of detection of a positive test case. That is what sensitivity is. Like if you are positive, what is the probability that you will be marked as positive? It is also the true positive rate of the of the proportion of positive cases how many did you catch these are essentially the moment in the frequentialist framework this probability is literally defined by the statings the equivalent statings and that was the quiz guys guys so this was too much of this had to deal with this confusing terms like precision recall sensitivity specificity much of it dealt with that and the fact that people often confuse the logistic regression to they have all sorts of notions some people think it's for regression. Some people say that it can only be used for linear classification and so forth. So I thought a quiz to clarify the meaning would be useful. Then F1 score is what we did and some other little things. And that was your quiz for this week, guys. Субтитры подогнал «Симон»