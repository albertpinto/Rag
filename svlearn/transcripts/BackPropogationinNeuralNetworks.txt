 On Monday we covered the topic of regularization. I will quickly review that topic and today what we will do is I'd like to finish one more topic, which is back propagation. Back propagation is a straightforward topic. I don't want to leave it for next week because we have other important things coming up, convalescent neural nets and other things coming up next week. So I'd like to finish it off right today itself. I'll also briefly touch upon optimizers and we'll handle the rest of the in-depth discussion on optimizers in one of the extra sessions. So, and this is all so that we can get started with congulation neural networks next week. So let us recap what we learned. networks next week. So let us recap what we learned on Monday. So if you're looking at my screen you will notice that we basically said that regularization is a method to decrease overfitting to data. If you take an overtly complex model, you run the risk, if your model has too many parameters, you run the risk of overfitting. And the way you avoid overfitting is by regularization. It dampens the oscillations basically. And this is the basic idea that you want to get a curve like this pink curve, which is complex, but you don't want to get the yellow curve because the yellow curve is complex but it has it has over fit the training data and so is likely to do very poorly on the test data the reason it has it does poorly is because it has fit not just to the information content of the training data it has also learned from the noise, it has fit to the noise and that is a bad thing. And the trade-offs are that you can take a very simple model, like for example this blue model which will have high bias errors, then you don't have much that you can do about it. Or what you can do is take a bit more complex model, but you can regularize it. When we talk about regularization, we talk of many techniques of regularization. In particular, we talk about seven ways to regularize. One is simply taking the batch size in deep neural networks. When we set the batch size, don't make your batch size very big. Make it small so that your loss surface does have perturbations. It keeps wobbling around and you that make sure that you don't get caught up in local minima. The suboptimal minimas. The other way that you can deal with it is by using simple data augmentation techniques. We talked about a picture. You have a picture of the cat, invert it upside down, and left, right, and lateral invert. Take sections of the cat, right? So that it is still a cat and do all sorts of things like that. And you therefore have data augmentation. Now the rule is when you have complex algorithms, one easy way to tame them, to tame the oscillations, the overcutting is just to give it an overwhelming amount of data so that those oscillations get dampened and you have a good model. Now, that is data augmentation. The other is to ensemble it. We are aware that, for example, in ML 200 we learned that when you have a random forest, random forest is an ensemble, it's a bagging of lots of trees, weak learners. When you ensemble a lot of individual weak learners, you get a pretty good learner. So that's where we have a theorem that says that if each of the weak learners is able to make better than 50% prediction, just a little bit better than 50% prediction in binary classification, the overall ensemble will make impressively good predictions. So that is using ensembles. Now remember that sometimes it is said that a neural network is itself an ensemble of nodes. Now that's not enough. You can take an ensemble of neural networks and do that so you can take you know you can train your neural networks with different properties you can train it with different input features things like that and and then you can stack all those neural networks together and get a better prediction and that will regularize it. Another thing you could do is simply batch, batch normalization. The rule with neural networks is, and we talked about it, that in the activation function, if your values, if your response from a neural net is any one node is too high, then it becomes the input to the next layer, which is big, and that input causes a saturation of the activation. Activation will become close to, let's say, if you are doing sigmoid, it will just asymptote to one or minor or zero. So the learning happens very slowly. And so one easy way that you can control all this is by a batch normalization. It's a good thing to do batch normalization, do that. Then the early stopping, like you know that in the beginning, the neural network will be fitting to the data then towards the end it will start fitting to the noise so you should know when to stop the way to do that is after each epoch you see the divergence between training data and test data the performance and training and test data if the if you see that test data is beginning to become significantly worse than training data evaluation, performance on training data, you know it's time to stop, that is why. And then comes the next method, which was dropout. What you do at each layer, it's sort of like a Cheshire cat. You make these nodes disappear for a batch so as one batch of data so one step one iteration goes through the day through this network certain nodes are just passivated they just go dark they preserve their old weight values you don't remove them but they don't learn they don't do forward passes or anything. They just completely go dark. And then what it leads to once again is, it is a form of introducing, you know, blurring the optics on the loss landscape, it's sort of introducing a bit of noise into it. And what it does is it makes sure that it's unstable the lost landscape remains unstable and you don't get trapped in very shallow local minima that's a virtue of this dropout and lastly we talk about the more complicated l1 l2 regularization these regularizations of course by the way i've mentioned that L1 is lasso and L2 is ridge. You will also see a new term often used, let me just write it here, L1 is lasso. People refer to it as lasso. They also refer to it as a Laplacian regularization or Laplace regularization. Why is that? It has to do with the Bayesians and we haven't done Bayesian thinking, but one of the beautiful results, if you make an initial hypothesis, a prior hypothesis of the weights, and the shape of the prior is a Laplace function. Laplace functions are usually peak functions like this. And we won't go into it because it's a bit more complicated topic at this point. If you start with a Laplacian prior, it turns out that L1 regularization automatically comes out. And in the same Bayesian technique, if you start with the Gaussian prior bell curve, the Gaussian is also called a Gaussian regularization, because if you make your initial guesses of weights as distributions as Gaussian distributions, then L2 regularization automatically follows from that. Now I've used some esoteric words. If it doesn't mean anything to you, don't worry about it at all. It will make sense only to people who have done some amount of bias. But just to know that these are other words that people use for L2. Gaussian is another and ridge. L2, Gaussian and ridge are the same thing. L1, Lasso and Laplace are the same thing. So those are the methods of regularization. Now we also talked of the fact that L1, L2 are actually the same method. The fundamental idea is that you constrain the weights. You say that you're not willing to let the weights blow up. So you set a budget and you create a unit circle around the origin in the parameter space for weights and biases. And you say that beyond a certain measure they can't grow if you constrain it to a certain size then you can't then go and look at the absolute minimum because the absolute minimum may happen far far away from the origin but the best you can do is a point like this point that is a point like this, a point that is a point like this, an optimal point. And this point is your constrained optima. Let me write it. Constrained optima. This is the best you can do at this point. And there's a beautiful geometry to this. Just as a recap, at this point, it turns out that the perpendiculars, the gradients, they are obviously pointing in opposite directions, so they have to be multiples of each other. And when you take that and you write it out, it turns out that in a couple of steps, you can come up with a new loss function, which is made up of the sum squared error term and this other term, which is the regularization term. Now, in this regularization term, you notice that this guy, lambda is the hyperparameter. Usually people take values of lambda starting with 10 to the minus 4, 10 to the minus 3, 10 to the minus 2, 10 to the minus 1. So you take it in powers of 10. You can even go 10 to the minus 5 sometimes. Right? And you can use that and see which is the best value of lambda. So here is one intuition. I said that in the parameter space, in the weight in the parameter space, you make a unit circle that could be a diamond in the case of Manhattan norm or Euclidean, it will really look like the natural definition of a circle. So you set a budget and this is your budget. This is as far as you can go, radius. One way to think about radius is your radius is inversely proportional to one over lambda. So what does it mean? If you want to give a large budget, lambda should be small. If you want to give a small budget, lambda should be large because they are reciprocals of each other. So what it means is you make lambda very big, you are over dampening the system, right? You're pushing, you're not giving much choice to the model. It has to find the optimal value within a very tight circle around the origin for the parameter space. You relax it a little bit and then you get more flexibility. And if you go and look at this picture, what it means is that, see, if you take a budget that is only this small, you realize that the optima that you will find will be not as optimal. The residual error, the error that remains will be significantly more than if you have a slightly bigger budget. So one question that comes, therefore, is what is the optimal value? How do we determine the optimal value of lambda? Would one of you like to take a guess? How do you find the optimal value of Lambda? Anyone would like to guess? By trial and error. And how would you try and how would you do it by trial and error? So one value then find the test loss and compare it with another value test loss. Excellent. Yes. So as you slide your lambda back and forth, what happens is that your training error will be something, but your test error, you have to look which of these gives you the least test error and you have to go with that. So therefore it's a hyper parameter of the model that you have to somehow find the best value of. The traditional methods used to be the grid search or the randomized search, but as we are going to learn in this workshop series, today we have more smarter ways of figuring these out. We'll talk about automated machine learning and one core topic there would be hyperparameter optimization. One of the hyperparameters that we will optimize in a smart way will be this the lambda parameter. The summary, the basic thing to remember is a bigger lambda means a smaller budget, smaller circle that you're allowing. So you're only, the bigger the lambda, the smaller the value of weights that you're allowing. Right? And in a way you can see that from this equation. See, if you look at this equation, the loss comes from two parts. One is the error part and one is the weight part. So what happens is if you want to minimize this, you would like to keep your weights low. Right. If you keep the weights low, I mean, if lambda is big and weights are very small so that the product is reasonable, what will happen is if you make it too small, the weights too small, then you realize that the error surface will meet at a point that is too high sum squared loss. So as you move out from these contour surfaces for the error, the sum squared error, or the first term begins to go up. Vaidhyanathan Ramamurthy, But the penalty term begins to go down because the penalty term is this square of the is the is basically the size of this particular circle. So the more you shrink the circle one term in the last decreases, but the other term X increases. the loss decreases but the other term increases and the whole point is where is the where is the perfect balance between these two right at which you get a good test error minimum test error so that's what it is so any questions guys guys, before I continue? This is regularization, and I have added into the quiz that I'm about to release tonight or tomorrow, a lot of questions about regularization. Now, regularization is mentioned in your textbook, main textbook. Please do read that. textbook, main textbook, please do read that. It is also discussed, the L1 and L2 are discussed in the ISLR, the textbook for ML 100 and 200, Introduction to Statistical Learning with R. So please go and review there. The review in ISLR is more detailed. It's more, it goes slowly and explains the concepts. The review of it in the textbook, the deep learning textbook is a little bit quicker. It just gives you the summary of it. So, if we are clear with that, I will then move on to the next topic. Should I or are there any questions guys before I do that? So as if the prediction here is also a function of the weights correct the error term yes it is so here are the steps you look at see when you do the model you have a model right model is made up of these weights and like I'll just write weights their bias is equal to w naught right these weight terms so what happens is that for any given value of the weight input vector goes out y hat comes out right from y hat right what do you do from y hat and a feed in the the actual y you get the loss you know y minus y hat you get the loss function and in the case of binary classification or sorry classifier it is the cross entropy loss and there are many loss functions but the common ones we will stay with are the sum squared error and the cross entropy loss. So whatever the loss is. Now you ask this question, how do I minimize the loss? So the loss is made up of these two terms, right? And so what you have to do is if you try to minimize the weight, how? And that is the process we will learn today you sort of how do you like you know as i told you you can imagine that this box is made up of all of these resistors which are going through some distortion function what you're basically saying is that these are real stats you know you you now you can start sliding your weights back and forth. If you think of them as real stats, you're just calibrating your real stats, fudging around with it and see what happens if I change this weight this way and a little that weight that way. And how does it affect therefore the prediction and therefore affect the loss? And the thing is, we want the loss to decrease. Now we learned a way to decrease the loss in a systematic way. We don't want to fudge. We don't want to just guess an optimal value of w, the parameter values, the w star. In other words, that cannot just be hard to just guess. And one can illustrate it with a simple example suppose you know you have data and I ask you to find a best fit line now how many lines can you draw on this plane can somebody guess you can many, many lines and each of them has associated with it a weight vector, you know, which is made up of the intercept and the slope. B and W1, let us say, the bias or the W naught and the W1. Slope and intercept, each line has a separate slope. So one way you could do it is you could just randomly keep guessing the slope and the intercept and then pick the one and keep track of the losses. And finally you pick the one that takes this value. Now, why is that not a valid approach? Could somebody argue against it please? What's wrong with that? It's not feasible. I mean, you can, most of the time you end up with the wrong estimate. Yes. See, the thing is that in two dimensions, like in two dimensions itself, there are infinitely many lines that you can start out with. Even if you limit yourself close to the origin those weights and you try to draw the lines still there will be a lots and lots of lines that you'll have to play with and it becomes a grid search. In higher dimension it's a computationally very expensive grid search. That is one reason practically it's not feasible. The second reason is suppose you found amongst your grid search you found a value that you think is the lowest amongst the ones you looked at. There is no proof that it is truly the best solution. On the other hand we learned about a technique which was gradient descent if you remember. if you do gradient descent then what happens you take the shortest path to the minima to the center right i won't rehash gradient descent but the idea is that every single shift of the weight in decreases the loss right so from step one step two generally roughly loss keeps decreasing it might fluctuate a little bit occasionally increase and so forth and we saw that in our labs occasionally the loss again it shoots up and then again comes down and so on and so forth but broadly the trend is that it keeps coming down isn't it and it comes down rapidly and in a systematic way so that is why we value gradient descent as the optimization method in this i think one person wants to ask this question. Why do we not use a second order methods? If you are into mathematics, one of the things that may come to you, those of you who have background in numerical computing, let's say we are using gradient, which is the first derivative. using gradient which is the first derivative and yet there are methods like Newton-Raphson and things like that which utilize the second derivative you know the they use and then the so those methods can be used for smaller data sets when you when your data sets are huge can be used for smaller data sets. When your data sets are huge, it becomes a little infeasible. And those methods, again, we realize that in a world where we don't have a convex surface, globally convex loss surface, but then the proven way, the one way that we know converges well is the gradient descent way. It has been very well researched. It does extremely well. And generally what happens is if you come from a background in optimization, linear optimization and convex optimization, et cetera, you might be tempted to say, well, you know, there are other techniques that we can use. Why don't we go use that? All those techniques have a place, but in this particular case, in the case of neural network, generally experience has shown that nothing beats some form of gradient descent. We all just use gradient descent in a practical way. It just works. And people have studied it deeply. You know what is going to happen. It's a known entity. You know its behavior and so forth. So that's great insight. So any other questions, guys? So we will then move on to the next topic, which is a back propagation. Let's go to that topic. It's a very interesting topic. It seems to have been back propagation. It's at the heart of how a neural network learns. This is how a neural network learns. So it is really at the heart of it. Now if you look at the history of backpropagation, people thought it was discovered something around 1986. And then first people thought it was discovered around 1991. Then they realized that somebody had done work in 88, then 89 actually, then they realized that work was done in 86. And now more and more historical analysis shows that it is an idea that is powerful enough that many, many people discovered it independently. And the roots go back to 1960s and late 1950s. So practically when the computers came about, and remember those were the days of giant buildings, computers used to occupy giant buildings and have big vacuum tubes and solenoids and so on and so forth, magnetic cores. Already this concept, this very interesting idea of back propagation had been formulated in one way or the other. And I suppose it's a testament to how central it is to neural networks that people have bothered to investigate the history of this idea and how far back it goes. So I'm going to explain backpropagation to you guys. So the way I'll do that is actually it's today actually I'll give quite a bit of the time to back propagation. I do want to finish it before we go to the labs the even though today is a lab day because as I said we won't get time later on. So So back propagation at its root needs one calculus concept before we understand what it is. And that calculus concept is called the chain rule. Now, most of you would remember what the chain rule is. It's a very abstract thing so suppose you have Y is some function of some function of Z and Z is some function G of actually let me write it the other way down in a systematic way Z is some function of Y or W and W is some function of y or w and w is some function of y let me just go writing it some function of y and y is some function of q let's not use q f and g are gone. Let's say T of T of X. So you realize that if I change X, it will change Y that will change W and that will change Z. Do we making sense guys? So in other words, z is, if you write it, you can write it as a function of, of a function of, a function of x. This is just algebra. You know, you take the value, what will dx produce? It will produce y. Now what will g of y produce? It will produce w. And what will f of w produce? It will produce z. And so that is z. So are we clear guys? You see this relationship? And you can represent it as a diagram if you so wish. You could say that x goes to the function, t takes it to y, right? And then let us say that g takes it to g of y t of x and so this takes you to z and so there is some function let me call that function composite function that let us let that function be s right which is a direct short circuit right so you can go this route or if you know s you could directly take the derivative of x i mean small changes of x will directly you can plug it into this equation and you'll know how much change will happen or what the value would be for z. So far am I making sense guys? I'm saying something quite simple. Right? So just keep this picture in mind that you can write things in a very indirect way. Okay? So for example, an illustrative thing that people often say is that, you know, suppose some major thing happens, cascading effects happen. Let's say that some company, well, I guess we are talking about weather and so forth. So what happens? Suppose we have a very hot summer in California. Well, high temperatures leads to the ocean getting warm. That leads to thunderstorms, dry thunderstorms. That leads to fires. And that leads to, I suppose, a lot of economic problems. And we are seeing that. Just taking a very practical example. So you can say that the amount of money that California has to do with the weather conditions now or in particular number of let's say lighting bolts and whatnot and the number of lighting goals depends upon or the number of storms depends upon thunderstorms depend upon water temperature or land temperature maybe it's the land temperature and the land so therefore a very hot temperature caused by weather can lead to lead to indirectly to a lot of consequences that's the concept of a function of a function of a function right so a whole set of implications or consequences this would change if something happened to X. That is the idea of this complicated rules. Now comes a very interesting thing. Suppose I ask you this. Suppose S is not, you don't know S. as only as f of g of t of x. I'm just writing this circular symbol because I don't want to keep making all those brackets. So suppose this is it, z is this, but this is the best you can do with this. You want to write it like that. So comes an interesting question suppose I do X changes by a small amount it becomes Delta X right how much does Z change so you want to know what is the change of Z with respect to small changes of X so what we want to know is how much does Z change, a small change of Z with respect to small changes in X, right? So this is called, and there's a bit of calculus, but let me just mention the calculus. The small change of z would be d z i mean derivative of z with respect to and you can ignore this part if you want to ignore dx times the small change in x by delta this is small change so the small change in z would be this so our simpler way to say it is that Z final or Z next would be equal to the current Z. And suppose you made a small change in times, a small change in X. a small change in x. So you realize that this is this. Now, and then the higher order terms, so you'll forget it in Taylor expansion, but for very small values, very, very small, that's why I use the word small, so that the higher order terms disappear in a Taylor expansion people say we won't go into that Taylor expansion let's ignore that because they'll vanish now we look at this term what is how do we compute the derivative of Z so here comes a bit of a rule you say that let's see how changes propagate. How change in this. So you realize that this is pretty complicated, right? If I write a function, let me just write something. T, let us say that T is equal to sine squared of X. G is equal to E to the minus minus t square gt and f is equal to f of g f of this like I think I should use the word t, t of x was y isn it? y is equal to this. And then, so this f of g is basically equal to, let's write something even more complex here. Let us say that it is equal to, let's say it is equal to g square, right? g square. So now you could write the whole thing in a complicated way. You can say that z is equal to, what is it? It is equal to, first of all, sine x squared, e to the minus sine, well, let me just write it as minus t because there are too many things here. e to the minus sine squared well, let me just write it as minus t because there are too many things here, e to the minus sine squared x, right, and the entire thing squared, right, or squared, or maybe I'll just make it simple. Let us say that it is equal to 10 times this. Just let's write it 10 times this. We'll make the last one a little bit simple because I don't want to write a complicated expression here. So now you realize, or maybe why not make it complicated? Let us say that this is equal to, for lack of something better, I will say cosine, cosine of g. So now you have a very complicated expression. It is equal to the cosine of the e to the z. Now, how many of you feel that you can just look at it and tell the derivative, dz dx? This is exactly the sort of problem you hope you're not asked because it's been a few years since we did differentiation and integration so what i'll do is i'll take a little bit of a time to brush it up and give you an inkling of how very easy actually it is to solve it see most of us remember basic things so let me just make a little a box memory box and typically you can look it up the derivative of so i will write functions derivative of x is e to the x itself derivative of x to the power n is n times x n minus 1, derivative of cosine x is minus sine x. Suppose these are very basic rules that you may remember. And let us say that we just remember these few rules. The question is, can we use that to solve this problem, this much more complicated problem? So one way that you can solve it is to just ask this question why how can I make this part how can I make this part this complicated part nothing but this part look it look up into this table right easy question so I have a joke here it's a little bit tangential joke but I'll make it you in for my children when they were in high school and middle school, they used to watch a TV show called Psych. Some of you may know about it or your children may be watching it, Psych. Does it ring a bell with any one of you? Yes, it does. It does, right? So I'll just give you a little bit of a background. So there are two characters, young characters, Sean and Gus. Gus is the sensible guy and Sean is the one who has very good observational powers to solve various crimes, but he is in other ways not very intellectually endowed. So now, once it so happens that they have to stand for a mayor's, they have to campaign to be a mayor of a city, Santa Barbara. Why? Because they're trying to solve a crime and they start polling against the leading candidate who they suspect is implicated somehow. So people now start coaching Sean on how you deal with the press and how you deal with people. So one advice I remember from that which is very very good, it says that in politics there's a very simple way to be confident. Never ask the question that you, never answer the question that you were asked. Instead answer the question that you wish you were asked. So in other words, always answer the question, not the question that you find harder, you don't want to answer. Find some related question that you would actually prefer answering and answer that instead. And it sort of works. And if you notice, that's the constant exchange between journalists and politicians. When asked a question, politicians will always give, not answer that question, quite often, not always, but sometimes answer something else. And the job of a journalist is to bring the politician back to that question and pin him down or her down to answering that question. So there is some flavor of that. So I'm going to use that reasoning that how can we answer this question by not answering this big complicated question, but answering something that we know we can. So let's look at this problem. So suppose I could write z is equal to just cosine of, what was it? W cosine of w, right? So here, can we take the derivative of w? If instead of finding the derivative with respect to x our job was to find the derivative with respect to w could we have done that we could have done that we could have said minus sine w would you agree because all we have to do is look up this table this table that we have and we realize that here is that here is the answer sitting there. So here is the function and the derivative, f prime. So this is easy. Now let's ask the other question. What is w? W happens to be e to the minus sine squared x well that is complicated but again let us simplify it and say what if it was just e to the minus t some variable t and then could we have the solve for T you would say W is equal to e to the minus T this is easy DW D P would be minus e to the minus T right once again you look up the table and you say so let us hold on to this partial results this is true and then we can ask ourselves so what is T here it turns out that t is a t is sine square x it looks complicated so let us say that this is actually sine write it as sine x square so let me create some other variable y is that is that appropriate or maybe i'll just i don't know what I called y is u and you say that u is equal to u square u being sine x now is this again solvable this is very solvable you can say dt du is equal to 2u and so do you notice that at each point we are not solving the problem that we were initially given. We are chipping away at the problem one little bite at a time. We are only solving the problem that we wish we had, right, in a simpler way, the parts of the problem. And we make progress with that and finally you say that, all right, now what is u? u is sine x. So this is another result and so u is sine x and it turns out that yes now we can solve it in one stroke and this is a cosine x and so you are now you say let's see i have a whole set of partial results i have dz a dw i have a dw dt i have a dt d u and i have d u d x and it turns out that uh here a calculus sort of comes to your rescue it says that d z d x the initial problem that you wanted to solve is nothing but the product of all this partial you know steps chipping away so you have innocence chipped away at the problem piece by piece and you have solved it right so you would say that this would be cosine x here and then you would say dt du is 2u right so that would be 2 now what is u u is sine x 2 sine x right then comes dw dt what is dw dt minus e to the minus t so minus t is sine square x sine square x here and then so this is one more factor. And then comes the final w, or w, or no, this dw, I think we took care of this. Did we take care of this? dw dt is this, and dz is minus sine w. So minus sine of w. But what is w? W is this whole thing. E to the minus sine square x. So you can just multiply all of these things and you would know what your answer is. So I hope I haven't made a mistake. I might have made a mistake somewhere along. Yeah, this is a sine of this entire thing. Sorry, sine of this entire thing, minus sign of the entire thing. So this minus minus cancels out. And then you have this complicated product here, sign of e to the minus x squared and so on and so forth. And so what you did is you found a very complicated derivative. Do you notice? By chipping away at the problem at a time? Now you would say, why would I do that? Well, the answer to that is now you go back to your initial update rule and you say, well, how much will you remember that we said that the next value of Z is, if you make a small change in X, it is the previous value of Z plus dz dx, isn't it? And we just computed that and we of course knew the previous value of z. And so what happens is we have figured out times delta x, of course, the small step. So you notice that we can choose how much of a step we take. We know the previous value of Z and so we know that if we take a small step how much will Z change. Do you see this guys? Or another way to say that is delta Z is equal to the derivative DZ or DX which was a complicated thing times delta x right so guys any questions here this is just a sort of a review of the kind of calculus we have done a long time ago am i making sense guys anybody who has a question if not this was the only preliminary we needed to make progress. So now comes the question that how do we relate this to a neural network? In a neural network, we have let's take a situation. We know that we have layers upon layers of nodes here. You have input, you have lots of layers and layers of nodes and all of them have complicated lots and lots of nodes are here. I just put dot dot dot and finally you have some output y hat. Output leads to the loss function. Right? Now this last function is dependent on the weights at each of the layers. Right? There are a lot of weights at this layer. Lots of, so this is the layer one, layer two, layer L, lots and lots of weights will be there. Right? So depending upon how many nodes there are and each node will have multiple inputs coming to it and so on and so forth. So there'll be quite a few weights here. And you ask yourself that if we want to do gradient descent, what do we do? We take the value, any particular weight. Let us say that some weight, now how do we think think of a weight at this moment just think of a some simple weight let this be let this be and are being pretty parameter or arbitrary neural net parameter some weight I'm considering bias as just anything I'll just use the notation so you would you remember that gradient descent said what did it say? That W next, the update we make when we want to improve the, or we want to make the machine learn and make less mistakes is this. W, the current value of W minus the learning rate, right, times the derivative of the loss function partial derivative the derivative of the loss function with respect to w now you notice that i'm using a notation i'm not using b i'm not using d l d w i'm instead using d this funny symbol partial this is called a partial this funny symbol, partial, this is called a partial. And this acknowledges the fact that the loss function doesn't depend on just this weight, it depends on all the other hundreds of weights. If any one of them changes, the loss would change. So this contributes to the loss. Whereas in one dimension, if y is, like when you say y is equal to a sign of x, then y depends only on X in our situation or Y depends on this loss depends on many weights I just took an arbitrary weight and this this is our gradient descent truth so so far we are together guys we did gradient descent and this is what we need to do the the only now comes the question that how do we compute this guy this guy is the big problem actually because it's not so easy to know and it begins to look like a problem because see if if If some weight here changes in the first layer changes, W1, right? And it may belong to some node. Let us say that it belongs to node I and in node I, so let's say ith node in the first layer, right? So let me say layer one it is a node in this node suppose their parameters are x1 all the way to x n so what happens is each of these will be i1 i n so in other words any one weight will be i j in the first so you notice this peculiar you need to tell given a weight which layer it belongs to which node it belongs to and which of its inputs coming to it you're going to apply that that sort of a weight to right so if you think about it the the Z of I, the sort of accumulated result would be the W of I in the first layer. And I'll just use this bracketed term. It doesn't mean that the power, it's just to signify that you don't raise Z to the power of one or two or three. It just signifies the layer. two or three it just signifies the layer in the first layer suppose you take the ith node ith node for ith node in layer one right so this ci would be this times what? J X J. Isn't it? So in other words, let me just write it down. It will be equal to WI0 plus, which is essentially a BI of the first layer, the bias, plus X1. the bias plus x uh x1 let us say if we can get this right x actually let me space it out a little bit so it would be wi one x1 plus wi 2 x2 plus and we can go here so suppose an arbitrary X I J is there X J right plus all the way to X n dot dot dot you will get to W I n X n right assuming n dimensional input to this first layer so you would agree that this is the linear, that this is the affine transformation we talked about and the output would be the response, the output of the first layer would be, which is what? It is nothing but the y hat output is nothing but of the first layer of the first node in the layer would be i in the first layer the ith node will produce an output which is nothing but the activation function and i'm using sigma for activation function activation function of z i so does this make sense guys so in other words this ith node node i i in layer one is made up of all of these weights coming in. So Z I layer one and this is the activation of Z I in layer one times and that becomes the output. Right? I could write Y hat. Maybe I could, we are used to writing Y hat. We could write, but Y hat is the ultimate one at the very, end we will just write a here a of the first layer the output of the first layer from the i-th node is this so does this make sense guys does this look very familiar so far so from this let's come up with a generalized notation we will generalize and write a notation which we will say that in our notation weight I J of layer L Right would stands would stand for interpret as follows Interpret as Follows It is the weight associated with node in layer L and input input component X J, whatever the Jth component of that is. Does this make sense, guys? So we will take this as a notation. Do you see how simple this notation is? But if you think about it, you'll realize something very interesting. This big W, what is its dimensionality of it? Like how many dimensions does it have? If you look at a neural network, how many, it will have many, many layers, many layers isn't it right so let's think about it let's think of a neural net which has I will just take two three here and two here and let us say that the inputs are coming here from something or the other. So you realize that here I have, let's say that there are two inputs. X1, X2. So now I can write this. This is the first, second, third, first, second, third. Let me just name these guys. Node 1, Node 2, Node 3. So you would agree that this is W1, 1 of layer in the first one and this one is w uh i don't know one two and this one is w two one this is w two two this is w three one this is w three two do we see all these weights here guys and besides the weights of course we have the biases also w naught is equal to you know w naught of the first one w naught of the biases are fixed w naught the second well not fixed they also are parameters they change w three zero right so do you see how many things we got do you see how many things we got if we write it you're already looking at just looking at one node you're looking at a vector isn't it so suppose I fix the first parameter W layer 1 and the first one so you know that it itself is made up of w1 1 w1 2 w1 3 and not to mention the bias term w 1 0 it is like this likewise you have w a 2 of the first layer that will look like a vector w 2 0 w 2 1 w 2 2 w 2 3 3 and you could do that so what happens is you get this and now you have W 3 of the first layer so putting W 1 1 W 2 node of this weights of the second node and weights of the third node together what do you get suppose you put them together you get this giant what is called a matrix one zero w one one w one two w one three a w two zero w two one and we can fill it in w three zero all the way to w three three do you see this guys and this is still in the first layer you could say that the matrix of weights for the first layer looks like this are we together guys and so there will be a matrix of weights for the second layer also w2 would also look something similar to that right and w3 will also look something similar to that are we together guys so far any any any questions guys, it looks good. Okay. So these are your weights. Now let's see. We will look at something that I would call the forward pass in this deep neural network literature. You call it the forward pass. Forward pass. powers because that suppose you have a neural network layer one with layer two layer l going here and you take the x vector and you multiply it through each of these it will take you a while to realize that how do i do that? Suppose I have X1, X2. So we have X1, X2 here. So how do we do this multiplication? What is the output of the first node? If you look at it, X1 went in, X1, X2 went in. Let's look at the first node of layer one. Now, what was it made up of if you remember this is made up of zero one one one two are we together it is made up of these two things actually did i uh no why know why did I bring in this wasn't there I apologize and w3 is equal to why because there are two of this so now suppose I take this input you have this matrix this is called the weight weight vector and you take the weight vector. And you take the input vector. Input is x1, x2. Suppose I put a 1 here. Now see what happens if you take the product of these two. The product is you take the pairwise product of these two. The product is you take the pairwise product of this two. So this vector and in notation, you can say this is W transpose X if you're thinking in terms of matrices, vector notation and so forth. But let's just look at it in simple terms. So if I multiply these two, you would agree that this becomes w11 x1 plus w12 x2 which which agrees with the notion of what it should be this is the z right z is equal to that and the output a of the first one of activation function so So I just remove the subscripts for a while. Sajeevan G. Of the first one, the one with C is Sajeevan G. Sigma. Sajeevan G. So all we did is we multiplied this now generalize it to Sajeevan G. This big matrix. What happens with this matrix, By the way, there is no third element here. Now see what happens if I do this. I took one, X, one, X two, and I multiplied by W one zero, W one one, W one two, the second node node 1 of the first layer this is the node 2 of the second layer 2 0 W 2 1 W 2 2 because there are two inputs and W curd is a 3 0 W three one w three two you realize that i can multiply i could i could find the output of each of these things by doing it together and when i do it what i expect is what i expect to get as a result is z1 z2 z3 isn't it guys and z1 z2 z, z3. Isn't it, guys? And z1, z2, z3 is the output. And then you apply the sigmoid to that and you'll get sigmoid of z1, sigmoid of z2, sigmoid of z3. So this is your a1, a2, a3 in the first layer. So far, guys, am I making sense? I'm not doing anything complicated. I'm just doing basic multiplication to see what is the output from the first layer. And I know that the net output from the first layer is basically these activations, A1, A2, A3. People call these matrices and vectors and so forth writing data together like this but let's stick to a simple terminology so what we realize is that if we take the weight matrix of the first layer and you multiply the the input to the first layer let's say that whatever the input is to layer one transpose and i'll just put a transpose here just symbolify that in matrix notation you would call it transpose you will get and you apply the sigmoid of this you will get that a vector right a vector a vector being a one one a one two of the first layer and this is of course of the first layer a sorry the first there a to a three of the first layer so so far I hope guys I'm just creating some fancy notation to help us along but nothing nothing great you know that we have to multiply all of these weights together isn't it guys but now how do we find the output of the second layer well it turns out that the x went in and this a vector from the first layer came out and as far as the second layer is concerned a vector is the input to the second layer these activations are the input to the second layer. These activations are the input to the second layer. The second layer doesn't know that it's a hidden layer. As far as the second layer is concerned, it is getting inputs, which is this A, A1. And so, and this is the input vector. And so it will do exactly the same thing what would be the output here a two vector would be Basically if you use this notation, it would be sigmoid of What will it be a one vector? Transpose times the weight vector the weight matrix of the the weight matrix of the second layer the weight matrix of the second layer. Basically, you're taking the same kind of dot product. In other words, what you're doing is, suppose, and I'll just write for the second layer to illustrate it. Let's say that the second layer has whatever number of nodes. Let's say that for the.: There, there is a first Jay Shah, Dr. Anand Oswal, Ph.D.: So a second layer. Jay Shah, Dr. Anand Oswal, Ph.D.: This is it. And I'm just writing it in the location here. Now I'll go back and put this 123 or sorry 111 Jay Shah, Dr. Anand Oswal, Ph.D.: To symbolify this is the first layer. Now what happens is that we are complicating the notation. Unfortunately, you see in neural networks, you see this complicated notation. But I wanted to emphasize that this is actually very simple. It is just a way to remember in this maze of weights, which layer and which node does this weight belong to? So now we can say of the first node, 0, 1, 1, 1, 2, whatever number that is there, let's say k nodes are there, 1, k, and this is all for the second layer, etc. And now you could do for the next node and for the next node, you'll get the matrix W2, second layer matrix, second layer weight matrix and the only thing we know is that the input is a 1 a 2 a 3 this is what is going in if you look at this picture where is a picture right here three activations are coming out from here this is a one one this is a activation from the first layer a one two and the second node in the first layer and the activation from the first layer node 3. Do we agree that these are the three inputs to this layer, the second layer? Asif, this assumes that your matrix comes from a fully connected layer, right? Yes, at this moment we are looking, let's look at a fully connected layer. So we have a dense network just imagine that the outputs are all going into the next layer and then electron will argue that it doesn't make a difference whether it's fully connected or not dropouts etc they don't affect the basic mathematics so now looking at this therefore what can we write this is just a fancy way of saying that the activation and what will come out of all of this, multiplying this will give us the activation vector from the second layer. Activation vector of the second layer is therefore equal to the activation vector of the first layer times the weight matrix of the first layer, isn't it? You're right. So now we are getting the hang of it. This entire thing is beginning to look like a sort of a simple thing. We can say the activation from the first layer was the input vector. And obviously, I put a dot here, but actually, you can think of a transpose. Maybe I'll put transpose. Think of transpose as just multiplying the way that we are multiplying. Of the first layer, A2 is equal to A1 of the first layer times the weight two, and again the transpose, A3 is A, well, two two transpose and weight of the third layer and now do you see suppose i have n layers l layers what do you conclude a of the l layer is equal to the activation of the l minus one layer times the weight matrix of the lth layer right and so you start from the input so what happens is x vector goes in a1 comes out right the a1 vector comes out of the first layer that produces the a2 vector activation of the second layer and keeps on going in to the activation vector of l minus one layer and that produces the activation vector of the last layer which is our result do you see this chain guys yes and do you now see that this is just nothing but a lot of multiplications? Because I could, for example, I could squeeze these two and say, therefore, A2 is nothing but, what is it? A2 is nothing but X transpose times weight matrix one, right? And the whole thing transpose times W2, right? And now I can keep on substituting all of these values. So you're looking at the input times lots of multiplications. So you say that this is your forward pass. The forward pass is just, just crazy matrix multiplications multiplications but easy like so long as you don't get lost which what this notations mean what it means to say L I J you know what this means then all you have to remember is that is this one expression namely the SF one one second this is Chandra here you made a slight error in that I mean you shouldn't be doing a whole transpose. Actually, what you have on the left-hand side, they are all the transpose itself. They are A1 transpose, A2 transpose. Oh, that's right. Because that is how the, I mean, just because later on when somebody is looking at the notes, they shouldn't get confused. I mean, it is A1 transpose, A2 transpose. A1 times this. You're right notes they shouldn't get confused i mean i yes it is it is a1 transpose a2 transpose and all this one times this you're right so the transpose is in the first one only absolutely thank you thank you so the matrix multiplication also when you are doing this chained multiplication you don't have to do the terms you need to do transforms excellent yes you just need the transform of the input vector and after that it is just straightforward yes and one more thing actually i missed uh there was one more mistake which is that i forgot the activation functions here where am i so let me put it in a red color sigma the activation functions right so what happens is that um so the general rule we can write it as and the activation function of a bond is equal to the activation of the product, right? Weight products. So let me just write it in big bold things. The output of the L-th layer is equal to the sigmoid of the output of the previous layer times the weights of this layer. And weight is, I'll just write it as a big bold to symbolify that this is a matrix. Are we together, guys? All we are doing is continuously multiplying, right? So forward pass is just a bit of multiplication. So long as we know the weights, we are able to multiply. So what happens when your neural network is fully trained? When your neural net is fully trained. Thanks Chandan for pointing that one out. So when you're, what did I just write? Neural net is fully trained. All you have to do to get y hat is basically the activation of the last layer is equal to and then all you do is x goes in and you keep on multiplying and then you get this answer it is just multiplication but it does raise the question how do we know the value of the weights in a fully trained network the weights have already learned the best weights the best parameters have been learned so how do we learn it the answer to that is of course gradient descent but gradient descent is a little bit complicated in our situation see the activation of the last layer depends on the activation of the previous layers times the weight, right? Which itself depends, which is a function of one layer before it isn't. So does this remind you of this peculiar thing that we were talking about? Let's go back, where are we? So guys, does this sort of remind you of this Z, the last layer is a function of the previous activations, which is a function of the previous in the previous and ultimately in the beginning is the input. And what you're basically trying to say is how do we, how does the entire network depend of Barry with respect to either the inputs or the weights of the system so but if I want to find out what would we want to find out let's go back to our gradient descent rule our gradient descent rule was just to recap gradient descent now we'll write it in this fancy with this all of these notations remember this was that the law that we need the gradient how much does the gradient change if i know or sort of let me just write a partial derivative to emphasize that partial derivative of this let me write green symbols partial derivative of this with respect to some some layer l weight i j of some layer now you know that if i know that then in that layer and for Jay Shah, Dr. learning rate it is the learning rate right so we will uh this is the learning rate so the only thing we need to know is let us figure out a way to compute these changes layer by layer we just need to compute what we alpha is our decision isn't it we decide let's say that we say 0.01 by choice isn't it we may choose some value some value so and you have fixed it the previous value of weight of course you know all you need to know is in the learning process you need to know this so you now there is interestingly there is a very descriptive name to it it is called the backward pass the backward pass of learning this is where the learning takes place So you do it only while learning, not while just making predictions, not at inference time. It is only when you want to improve your weights because the loss is still unacceptable. The errors are still unacceptable. So how do we do that? So what I will do is, sorry, I somehow went back to a previous give me a moment guys i didn't know how i managed to do that oh there it is i figured out how is it w i j to the power of l minus one previous term not the power it is not the power it is just layer it just talks about the layer l once again it is not the power guys. See, if W cube would have been the power, third power, I deliberately put it here. This is third layer. The previous layer, like L minus one layer or is it L only? What do you mean by L minus? Oh, this it is l remember for any parameter so think of it like that so forget these weights let's go back to our previous machine learning remember we used to say that the weight of ij you know l layer equal to weight of ij the previous layer no layer equal to weight of IJ the previous layer. Yes, that is true. Not the weight, the activations. Let's go back again and see what happens. Input X goes in. You have some weights. It will multiply with these weights to produce the activation of the first layer. That will hit the weights of the second layer to produce activation of the second layer. And it will keep going onwards till it produces the final result. Oh, I should remove the transpose from here. So it will produce, this is your final, ultimately, at the end of your network, whatever is the last activation, that is your result, isn't it? You're just multiplying in the forward pass. Am I making sense? I think Harini's question, if you go back to that last expression that you showed, the way I was interpreting Harini's question is what you've shown here is essentially the computational pass where the weights are adjusted so the weights of the lth layer is adjusted based on the weights minus the loss function basically do l by do weights of that right that's me it's the I just say this out and out you say that the weight the new weight this any one weight some arbitrary weight in some layer. In whatever layer it is, in the ellipse layer, how would you identify what is the coordinates of it? Like will say it belongs to ellipse layer ith node node and jth weight of the node weight of the node weight of the or weight yet weight associated with get input to that node do we do we agree Kaiser this is just the notation harini is this clear right and all the same so it is just some parameter think of it as beta now if you just imagine that it's beta you remember what was our update rule for beta how do you find beta next it used to be the previous value of beta minus alpha times the gradient of the error, right? Gradient of the loss with respect to beta. Do you remember? This is our gradient descent rule. We did in the previous workshops also. Do we remember this guys? So in other words, in a parameter space where a beta is the axis and this is the loss. Remember how do you make, how do you make a step, a better step from here? How do you step back? How do you make this step? You say that this is the rule, isn't it? Harini, is this clear? Does it remind you? Yes, sir. Okay. This is it. It just so happens that the beta, we are talking about the many betas in the neural network is just complicated. It is I J of the L, L layer. And so you say that the W I J of the L layer. The next value of it. the pre the the value that was there before we learned before we applied our a bit of learning minus alpha before we did the gradient descent step times the loss derivative of the loss with respect to exactly this the existing value of the weight right and, this becomes the more detailed, sorry, this becomes the more sort of a complicated way of writing. The same thing, realizing that there's so many beaters and it needs three indices to notate it, i, j, and l, to give you the coordinates of any one of the parameters of weights so far so good guys yes next one missing the previous equation so i got confused okay but now are we clear yes good you say that you say that the next would be your value in the next step is that correct so see what what are you doing go back to our example of uh resistors with a distortion function this is your beta you made up you you you put some voltages in x1 x2 you got some current this is not what you wanted right so you have an error now how can you fix the error you need to look at the loss here the loss the error here and you have to say hey i need to decrease this need to decrease the loss right or the error because machine learning is the learning word in machine learning simply means decreasing the loss the mistakes you want the output to be what it really is so this is your i hat or y hat let us say y hat is is different from y and this gap is the mistake and what you want to do is the total mistake that you make you you square it and do all sorts of things average it the average squared mistakes is your loss total average loss you want to minimize that so how do you minimize that you can't change the input but what you can do is you can improve this beta goes to beta next improved the improved value of it so what you do is you want to move the real start a little bit and you want to say okay i want a better beta and this loss is informing me how to do it i just need to take this step the next value of beta should be my current value minus the some learning rate times the gradient of loss with respect to this parameter rate times the gradient of loss with respect to this parameter. Are we together guys? No? Yes. Yes. Sanjay, we got that. Yeah. As simple as that. So see, that is all it is when we use fancy words like AI. I was just trying to relate it to when we go through the epoch and the steps. Yes. So we are in the learning step mode. It is the backward pass. See the forward pass, what did we do? We just computed the output. Output produces a loss. Given the output, there are mistakes. It gives us the loss. Now, if we can find the gradient of the loss, Jay Shah, Dr. produce Y hat and therefore it will produce the loss and then the backward pass needs to this backward pass would be which we are getting to is basically updates update of gradient descent so so far what you are saying is X goes in then activate the first layer activation comes out that leads to the second layer activation all the way to the final layer activation you you think of it as time but it it is not time. This entire pass happens practically immediately because it's all matrix multiplication. In fact, the way the system treats it is as one giant matrix multiplication, right? W. It is multiple yourself. In Markov chain over there also we do probability matrix multiplication, right? Actually a notion of time is stepped. So the other way to put it is that in markov chin monte carlo there is a sequential like yeah there is a path in other words you say you take randomness you start from somewhere and you take random steps yes the neural network in the traditional neural network when you do gradient descent there is no randomness. Once you have initialized the weight in other words you have taken a place in the parameter space there is a very direct path with a little bit of zigzagging to the optimal point right. This is your gradient descent path. to the optimal point right this is the gradient descent path so this is the difference between neural network and then exactly there is no gradient descent in markov chain monte carlo focus it takes a completely different approach in fact it uses randomness that is the interesting thing with the bioshoots they somehow use the power of randomness to solve problems okay so that was a digression let me come back to what we are talking about thank you so so guys where are we we realize that the forward pass input goes in and the activations keep moving forward right so X leads to the first layer activation leads to the second layer activation leads to the L minus one, which is your y hat. The problem is this in the beginning, all your weights in the beginning. All the weights are random. Oh, by the way, I must mention one more thing. Sometimes people ask, why can I not start a neural network and keep all the weights the same? So let me pose this same at initialization there are many ways to answer this, but this is really interesting. Think of a simple neural net. There is a, and let me simplify it down to just one input, X1. And it goes, and let's say it produces an output, Y hat. So this is your first layer. This one produces a one and this produces the activation of the first layer two and this produces the activation of the two because there's only one node here. So what happens if all the weights are the same? You basically have the same layer repeating or the same node repeating. Right. So think about it this way. All the gradients. So if you have the same weights, right? When you compute the loss with respect to this weight and this weight, it will be the same, right? If this weights were the same, the loss with respect to any one of the second layer weights, but because the weights are the same, the gradient would be the same. So they would make the same step backwards. They will, in the second layer, let me just call it W, what should I call this? W21 and W22. So W21 and W22 will make the same step, right? 2 and minus alpha one and this will be w two two minus alpha you realize that if these are the same and the gradients remain the same because there is a symmetrical situation then even the modifications will be the same so what you are looking at actually is a very interesting problem you don't need so many notes in the layer anymore it is as though which is as though all of it is just one big node each layer is just one big because the other node is not learning anything new or anything different from the first layer do you see it guys right and even the forward journey is just adding is just multiplying what one path would produce Do you see it guys? And even the forward journey is just adding, it's just multiplying what one path would produce. You just multiply it by two because the other path is producing the same results and you're just adding it up. So people often use, they borrow a concept from theoretical particle physics. There is a concept called symmetry and symmetry breaking in particle physics or in physics let's just say physics because now it is a condensed matter in all sorts of physics in theoretical physics there is a called symmetry breaking which means that it turns out in nature for some reason so imagine that you are sitting on top of a hill right around him just as an example but you're sitting here this is a state of um this is not a i mean this is a really nice state uh you are equally far from every point or whatever it is but then if you slightly perturb it shake it up it will take a definite path down in some sense right so now of all the directions you have a preferred direction we have done something or come down something this is not a very good example actually we have a theoretical physicist amongst us and then we'll probably give us a better analogy but okay the idea is they talk about some symmetry breaking so something that was perfectly symmetrical is not symmetrical anymore So something that was perfectly symmetrical is not symmetrical anymore. You have to choose. As if you could do one thing in the same mountain peak, you can put a small, like a shallow, I mean, that is like a local minima there. Right. At the mountain peak, you put a, then it's like a, locally, it is like a very stable place. That's right. But you put some randomness, then it's like a locally it is like a very stable place that's what you put some randomness then it's not stable anymore then it comes out and you have a problem that's a great way of putting it yeah so it is locally stable you're right and then as chanda pointed out you take a preferred direction now you have a you have made a choice right so that's that so now in a network also a similar thing happens so people are borrowed and they and they keep saying that in neural networks you need symmetry breaking in other words you cannot start with all the weights the same if you do that you're basically screwed because the entire layer will act as a single node other so you will end up with a lot of redundant nodes. Those nodes are not helping you at all. They just add into the computational cost. Guys, are we understanding this concept? Yes, sir. It's a very simple concept. So in fact, there is a problem. It has some bearing on regularization. And now is the time to mention that. that see why is it that we do drop out when l1 l2 regularization was there and they work for almost every area of machine learning they work in neural networks also they work but there is one problem after a little while, some of these weights during learning, they begin to sort of resonate with each other. They begin to align with each other. They'll become multiples of each other or so forth, right? In the same there. And so it is almost as though they have become the same. And there's a certain degree of symmetry that has come in. At least that partial subset of nodes then begins to act as one nodes and so you know you have lost some flexibility in the model it's become a bit more brittle you don't really have as many learning parameters as you thought you had do you see that guys okay if that happens it's a little bit technical why it happens, but with L1, L2, one of the problems is this begins to happen. And so what happens is, imagine that there are two nodes. I don't know if you guys can see me. Let me see my own picture to see if you guys can see my hand. Are you guys able to see me? Yes. Okay. So imagine that you have two nodes and they both have essentially the same value. One strong argument in favor of dropout is suppose this node, this node that I'm saying in one step one iteration this this goes away this becomes dark so now this node is there it's alive and this has gone dark so I'll remove this now this is still there so when the learning takes place when the back propagation takes place this would have learned and it would have moved away to a different value it will not have the value that this node has right so in the beginning if both of these have the same same value and i take this away now after a little while this has a different value after this gradient descent step this has a different value and so they are not symmetric. Is this argument simple guys? That is the main, that's a very strong argument actually in favor of, so let me just put it in the diagram also. So it's in the notes and you can carry this. Suppose you have two nodes, node, node, and for some reason the weights, one, w2, they begin to align themselves. So what happens is you notice that they're co-learning, right, in the back prop and so forth. Then there is a problem. So what you do is in one step one, right, step one will be a forward step and backward step let us say that in step one you go and nuke this this is not there anymore so this will have w2 will go to w2 minus whatever the loss w2 is w1 and let's say that they are both equal to some value. Let me take some value theta and W1 is still theta. It doesn't change. What have you done, you have broken their symmetry. So that's an argument, a strong argument in favor of using dropout. A big argument in favor of using dropout. So you know these are the little things guys. I don't know if your book, the textbook does mention it. I'm sure it does. It's written by the most, one of the biggest guys in this field, the great pioneers. At some point it might mention, but you know it is these little things that makes this whole field theoretically very, very fascinating. I don't know if you're impressed, but I always find this really neat to think through what is going on as the neural network is learning. And it's very interesting actually. We think when you read a book on neural networks, or at least I hopefully I'll fool you into the understanding that you understand neural networks, this deep neural networks. But actually I must say that that would just be fooling me because nobody really understands why these neural networks are so effective and why do they work so well in some situations, not in others. A lot of assumptions we used to make about them are proving not to be true. We know a little bit about why these neural networks work, but the underlying dynamics, we are just beginning to study. And as study it you know little gems keep popping up there's an army of researchers and they're all doing an incredible job of finding all sorts of things like all good research it is a collective human effort to find things but anyway we will we will stop there Now let's come back to the main topic at hand, which was back propagation. What does this have to do for propagation? So I showed you this example that the derivatives, you know, you can do a chain rule to compute the derivatives way back. If something depends on something else, depends on something else, depends on something else depends on something else depends on something else you can use change a chain rule to get your work done so now we'll apply it here so let us say that this is a last layer so I'll take a very simple neural net every layer has only one node are we together so if it has only one node then it will have w naught is equal to some a bias term and w1 right so if it has only one node then the previous layer is putting out and there's only one layer input input is one dimension let's say that this is layer L this is layer L minus 1 this is layer this is layer 1 are we together so what is the activation what would be the activation of the layer L this will be your y hat isn't it this is your output guys do you agree i'm creating a neural net which is just a chain simple chain of a one-dimensional chain each layer has just one node now you know that given y hat and y which is a reality ground truth you can compute the last function. So the last function could be MSC. It could be mean absolute error. It could be so regression Cuba regression and so on and so forth. Regression, whatever your last function is I won't go into or it could be a cross entropy etc etc for classification right so I'll just use it l but just to illustrate why don't we pin it down to regression right the argument remains the same whether we are doing regression or classification but I will take the argument that we are looking at regression so the loss function is given as y hat minus y loss so sum over all the data points right some over all the data points, right? And maybe you want to divide by N also, but plus regularization terms. Let's put it this way. So this is your last function. Now that we learned regularization, you can assume that regularization terms are always there. So guys tell me that this looks very simple. This is your sum squared error. By now, this must be as familiar as the You know the back of your hand. Are we together, guys. Please give me some feedback. Yes. please give me some feedback yes so now we'll ask this interesting question we this is a weight W at the layer W W and looking you say W L W L and W L 1 right So let me just use, okay, let me use a simpler notation. So now tell me, how do I update this weight? For me to update this weight, I need to know, so suppose you make a small change in the weight, W, right? How much change? Let's look at what happens. If you make a small change in weight the in the last layer W and by W I mean the weight the slope here let's just say right you can put one zero whatever you want or trigger a change in z because z is b plus w of the last layer times the activation of l minus 1. so a small change in weight will trigger a change in w guys is that simple trigger a change in W guys is that simple okay is it making sense yes it's just this is just a your direct multiplication now a change of W Z will cause a change of the activation whatever activation function there is, right? Because what is it? The AL is nothing but the signal, some activation function on Z. So this will change the activation, namely the output, right? And that change of output, because you now look at this, what is Y hat? Jay Shah, Dr. S. Mthombeni – Sr. L Activation of the last layer Produced by the i-th data minus y i Square, would you agree with that? So therefore what what will this do it will cause a change of the loss function so the thing is you you put up this you make a minor perturbation of the weight a Small change of the weight of the bias and it will have a cascading effect It will change W which will change activation which will change the loss so far so good guys so this is a chain reaction for you right in a line one causes another now if you get this now look at this now you ask yourself how much is the effect so you say that a small change of activation like suppose you have a small change of activation right how do i relate all of this let's start with this let me just take w the w of the lth there let us compute how it works so the way to think about it is that look at this chain reaction you say a small change how much l will change if you change w a little bit so the way to pose this question is how much sensitivity way to pose this question is how much sensitivity does loss L have to a small change in WL? So you make a small change and how much did it change? If it changes a lot, then the ratio would be huge. If it changes only a little bit, then maybe the ratio is small and it probably means that this little w does not need to be changed. It is not having much of a material effect in decreasing your loss or changing your loss. But let's work it out. Jay Shah, Dr. is therefore finally the loss with respect to a l. So little change, you can't compute it directly, but you can ask it step by step. Suppose I make a small change, it will lead to a change in z. So z will be sensitive to w and changes in z will, a will respond to the changes of that. And then changes in A, L will respond to it. Right? So the staggered effect is if you could compute these sensitivities, you pretty much know how much the loss is sensitive with respect to this weight. Am I making sense, guys? So this is essentially your chain rule. Isn't it? Now, let's see. Can we actually compute it? It's very simple. So let's take a particular case. In particular, we'll take a particular if. And this is the big if. I'm just taking a particular case, L is equal to 1 over n a L minus y squared. And, you know, there is a i here and the summation of i and so forth, plus rectum. So, so I'm going to, you know know skip some of the notational aspect right so now let's look at this can i find like by the way these things in the limit of very small in the limit of limit of delta w going to zero these these things become derivatives and so you say so this is equal to z w so when people write this partial derivative all they are saying is that uh it has to be genuinely very very small this is it right So now ask this question. Can you compute this term? This term. Of course, you can compute. Look at your loss function. You can say dL dAl is equal to nothing but two times two over n times summation over a l i minus y right this is just calculus the derivative of square of x is 2x isn't it d dx of x squared 2x what is x here x here is is this term ai minus y. So do we like this result? It is a known. Now we ask this question. So can somebody speak up? I need some feedback. So far so good, guys. Is it easy to understand? Anyone, please? Repeat that. The delta l by delta al and then you're drawing this arrow. How did you come to that conclusion? Which conclusion this one here? It follows from the fact that n a l minus y square so now ask yourself what is the derivative so i'll just forget the i subscript what is the derivative of l with respect to the activation you can take any one particular activation this is this is averaged over all of them so what do you think this is? It is a function of a right? It's a square term. This would be one over n and summation over i and all that. Now what is this? What is the derivative of this term? Two times a minus y. I'm forgetting, I'm skipping the underhand subscript do you see this guys does it make sense yeah so that is where i say that this is computed this is easy now we ask the other question what about the for the second term let me use a different color here what about this guy right so for this guy also it turns out that it's not that hard actually it depends upon which activation function you use so suppose you use an activation function is logistic logistic. So logistic A of Z is equal to 1, 1 plus e to the minus z e to the z plus e to the minus z. And I might have interchanged this term. So I keep always messing it up. So beware. Take it with caution. So these are examples of the activation functions. Examples. If you take ReLU, which looks like this, ReLU, and we'll cover activation function in detail. It is even easier. So suppose ReLU, you take a slight bend here. So either the value is az the this is az is defined as either one or zero based on whether z is a z is greater than zero z is less than zero so again you can take the derivative at any given value of z except this one singularity you can take the derivative at any given value of z except this one singularity so now it's a very interesting thought can i find what we are looking for is a z with respect to z right or of a given layer so suppose i put a layer here layers doesn't matter so can we do that so you can confirm that a very interesting fact, those of you who did engineering mathematics would remember, for sigmoid, this is a same as D A D Z is equal to, believe it or not, A, it is equal to one minus A. Surprising, isn't it? It comes out to be very elegant. For tan H, in fact, this was one of the assignments if you do engine math a prime happens to be 1 minus a 1 plus a in other words 1 minus a square so if I have the activation do I have the value of a of course I have the value of a in the forward pass do you remember that in the forward pass what happened at each step I know the value of a I keep computing the value of a so I know a and therefore I know its derivative because the derivative can be written in terms itself are we together guys right so the blue part is also taken care of then comes the last part which which i will take another color to do what about this part well it turns out that this is the easiest part because z of l of a given area, let me use a different color here. Z of L is, if you remember, what is it? It is the bias of L, the bias term or W naught term or the bias term of the L plus plus what the activation of the previous layer times the weight in this layer do you remember this expression guys guys are we together just this right this is a simple multiplication that we did a linear come just the linear product plus a translation. That's how we define Z. This is practically the definition of Z. So then I can say DZ D. What do I want to compute? D W, right? D W of L is equal to what? It is just the activation of the previous layer. And we have a result here. Now, when you put all of these results together, what do you find? You find that you have computed D L D W L is equal to well D Z L L D W L times the activation of L times the Z L times the loss function DAL. Remember this was the original expression and you just computed each of these terms. So you, for example, found that this is equal to AL minus one times, or AL AZ. Let's, if you take sigmoid it is a l a l one minus a l i'm just taking the sigmoid for for for logistic one minus a l times and now what was that? This was 2 times whatever it was 2 over N summation over I AL minus YI. You see? And so what has happened guys? I have managed to compute exactly what this is so far so good guys guys i hope what i'm saying is just multiplying the derivatives the chain rule right can somebody please give me a feedback if you understood this? Premjit, is this clear? Premjit Sidhu, I'm following, Asif, but I would need to review it once more to kind of put all things together in my head. Asif Badghaman, Okay. Yeah. So it is, what we are seeing is that, you know, you compute a lot of things in the forward pass, but if you know your mathematics, then the backward pass, you can use the chain rule and those pieces that you computed in the forward pass to plug it in. Say everything comes from the forward pass, right? You can plug that in to compute the derivative and now observe something very interesting. This function, this guy depends on ultimately after these parts are easy, but it depends on knowing the derivative of the loss function with a L minus one. Isn't it? And so you can argue that I can can do you can generalize from that and you can say of l minus two l minus one would be something in the previous layer would be um dz L minus one, which is again, DWL minus one, DAL minus one, DZL minus one. The same thing applies. There's nothing special with L except that you need to know DL, DAL minus one. So the question is in the last year, it was easy. We could use this formula for the loss. But what about any layer before that? Here, we need to find this out. So it turns out that that also, we have a chain rule that you can do. And you can argue as follows. You can say that, suppose I know this for the last layer. You can say that suppose I know this for the last layer, DL, DA, L minus 1 is equal to, so small change in AL minus 1, AL minus 1 will cause what? It will change a small change in Z of the L-th layer and the rest of it remains the same change in the z of the lth layer will cause a change in the activation of the lth layer and dl the activation of the lth layer and so we have a relationship we can find these activations the sensitivity of loss to activations of any given layer by knowing the sensitivity of loss to activation of the layer ahead of it right and that is why you say that the direction of computation is backwards you first compute the lth layer sensitivities to find the sensitivities in the l ellipse layer this again is a l one minus a l if you if you are using logistic and this is whatever you computed so far right and that thing this thing the fact that you can layer by layer compute it is called back propagation are we together this is the back propagation so guys I will post these notes and obviously the video will get posted I want you to understand at this moment see if you have followed it through then you can watch the video a couple of times and grasp it now what happens here is that once you get it you realize that it's trivial but till you get it it all looks confusing because they're all of these layer coefficients and ijk and all of that are flying around right so let me just review it and then i would like to take some time for the lab. What we are seeing from the beginning is that first of all, let me go back. We go back and learn about the chain rule. There is a chain rule in calculus. Basically it helps us decompose a complex problem and solve it piece by piece. When something depends on something, depends on something, depends on something else, depends on something else, there's a huge chain of relationship. You can deal with that sort of a complex chain of relationships like this. x causes you know x causes y to change y causes w to change w causes z to change so when you have this causal change then a relationship change but then you can apply the chain rule to deal with that problem now we come to the review that the node is nothing but weights producing weights inputs weights producing Z and then the activation of C remember the analogy the magic box that I told you Z this is the these are the weights these are the inputs and this is the distortion function. AFC and this produces Z and this is it. So once we have this idea in mind, then we look at our neural network and we realize, oh goodness, any one Z, any one weight weight it has certain coordinates it belongs to a certain layer it belongs to a certain node in the layer and it and it is the weight associated with some input component right coming from the previous layer so you need three indices to uniquely identify a weight in a neural network so what it means is that this is our parameter and we need to find some way to do gradient descent with respect to this weight we need to be able to use a bit of magic here a chain rule magic to be able to compute that so we do it in two passes in the forward pass to see ultimately what we need to do is we need to do this gradient descent step with respect to any arbitrary weight but to do that we need two passes in the forward pass we just compute all the the inputs and the activations and all the way to the output then we compute the loss then from the loss we start computing computing the gradient layer by layer from the end to the front right so we start moving back layer by layer and computing the gradients or computing the computing this right and the the main idea is that these are at the end of it, when all the dust settles, it just comes down to one simple thing. The change, any change sensitivity of loss to a given weight, which is what you need, this is your gradient that will help you do gradient descent, depends upon, in that layer, depends upon the sensitivity of loss to that layer's activation. But that layer's activation, you can relate it to the sensitivity here, gradient here, related to a gradient of the layer ahead of it. So you can keep computing backwards. So likewise, you can imagine that, see what the way it would work is, suppose you have the last layer loss a n will come out and you can therefore compute so let me go step by step you can compute this this will help you compute the LW the last layer weights and then this will help you compute the L the activation L minus one and actually let me write it in as vector as a matrix as a sort of a notation like this just to simplify otherwise will occupy a lot of space but this so if we know this then all it is is you need to know what your activation function is in a basic amount what the derivatives are this will tell you and now you can d l d w l minus 1 and then that will help you tell d l d a l minus 2 d l d w l minus 2 and it keeps going up till you find the derivative ultimately of dL dA1 and dL d weight 1 of the first layer. And ultimately you reach here. You keep back propagating and this is your back propagation. And once you have back propagated, you can therefore take the step because after this back propagation in the all the gradients being computed. All you have to do is change in the view next of anyone there and I'll just skip the layer and notation and so on and so forth. Right. Is the W value minus the dl dw right and here i've erased all the implicitly there are all these indices layer and i and j and so this is the step learning step so the way it works and if you remember in the code we did that uh this was was there in your quiz. What would you do? You take a network, if you remember your code, you take a network, you give it the input, it produces the prediction, the output, right? So this is your, then you say loss is equal to a label loss function if you remember loss function in which you gave the labels output right and once the next step is so then with me go and then what do you do? You do backwards network that backwards backward. What is this doing? Back propagation, back prop of gradients. It computes all the gradients. I cleverly computes all the gradients. And once you compute all the gradients, then all you have to do is you have to take a step right you take a step what does the step do step does the gradient descent because now you know the gradient so you can do the gradient descent step step so the word step refers know the gradient, so you can do the gradient descent step. So the word step refers to the gradient descent step. This is what it is referring to. Are we together? And so if you go back and read the code, you literally see the logical structure. So what are the forward passes? Which of these are the forward passes? This is the forward pass. Which of these are the backward passes? Backward passes. So the learning takes place in the backward pass. And once you have trained a neural network, of course, you don't do the backward pass. You only do the forward pass and you freeze the gradients Hey, you freeze the parameters and you freeze the gradients. You don't get the gradients be updated anymore Or the weights to be updated Am I making sense guys, so that is how neural networks learns very simple Layer by layer they learn now very simple layer by layer they learn now if we are done i would like to take another 20 minutes to get back to some amount of lab just for fun because i'll release it um we we are a little bit behind at this moment we are half a session to to one session behind so in the end i'll add a session if needed, or we'll do it in the extra sessions on Saturdays or something. But I would like to show you a little bit of a lab that I'm going to upload. So now let me show something to you in this notebook. What I did is I want to take a simple example as a base and now we want to do understand classification and regression from a metric perspective. So now we have done code, we know that neural networks can be trained to do regression and classification. What are prediction mechanisms? The question that arises is how good are the predictions? And how do you know those predictions are good? So the next lab we will give to model metrics. What are the metrics? How do you know that something is good or bad? So for that, those of you who have done workshops with me in the past, you will be familiar perhaps with this particular notebook and this river dataset. It's a very simple dataset. I'll just walk you through it through this a little bit. This dataset looks like this. So there's yellow points and there's this purple points and you have to write a classifier that can distinguish between the yellow and the purple point. In this particular one, just to illustrate, we have used the logistic regression. This is literally how we did it in ML 100 and 200, actually ML 200. And we just show how you should do careful feature engineering and so on and so forth. And you can solve the problem. You can look at it but what i want you guys to focus on is the model metrics just read it if see if you can understand it next time i'll explain to you both in classification and in regression what are model metrics we will take the river here and maybe the breast cancer data and we'll take the California housing data. The purpose is just to illustrate the concept of goodness of fit and the goodness of a model. How do you quantify it? So read it. Now what I have done is at the end of this river notebook, I have also put a toy example for you. I have taken and again the code is there. There's a binary data classifier. And by the way, I've deliberately kept the code a little bit imperfect. So see if you can improve upon that. What it does is, I'll walk through it. This is just importing all the classes. There is a binary data network. You can choose how many hidden layers you have. Then you create a classifier, then you load the data. Now you notice that we are training the data. This is something not, something we didn't do with CIFAR. CIFAR data set, you remember? It was already pre-trained. We downloaded and we used it to make prediction. But here we first train it and then we make predictions. Very much like you did with the universal approximator. So here you go, this is the loss taking place. And then here is the classification report. Well, this report is too good to be true. It is, the neural network does an amazing job of classifying it. It comes up with a perfect score. No mistakes. The data had no noise. I didn't get time to introduce noise, but this is it. So this is it. Now, if you plot the prediction versus the reality, this is what the reality was, and this is the prediction. If you look at the prediction do you notice that there's no sharp edge here it's a curve it's a round curve do you see that guys at the bottom i don't know if you can make out it's a it's a good nice round curve it's not very sharp and now there's a bit of homework. Do this. At this moment you can do this without even getting too much into the model metrics. Go read the code and see what happens if you change the activation function from ReLU to tanh. Then play around with the number of epochs. What happens if you run it for less epochs? Then change the number of hidden layers. Oh, no, this should be hidden layer, sorry. Number of hidden nodes in the, number of nodes in the hidden layer to see what is the minimum number of nodes you need. And obviously, it is something I talked about. Remember when I argued for neural networks being universal approximators? I made that argument. So you just see how many lines there are and how many straight things you have to do. If you attended OLA's, you know, the Saturday session, Sunday session, it should be even easier. This thing is two dimension how do you unfold this manifold in such a way that it becomes a straight separator so go you can do it if you reason like that you'll be able to answer these questions lastly i've left you with a problem a genuine coding problem. You have the river dataset, can you classify the river dataset by using this code? So the hint is, if you just directly feed in this code, you may not succeed. You'll have to do some small changes to the code and this should give you time to play around, fool around with the code and make modifications to it and so on and so forth. So see if you can do that. I'll leave you guys with that much. Any questions guys? Asif, when you apply a max drops, does that apply during the feed forward step or only during back propagation sorry when we apply what max drops what is max drops oh you mean drop out a dropout sorry dropouts yeah good question you only do it in training because you know what are you doing so suppose you have two nodes you you you take this away for one batch then you take this away for another batch at the end of it when it is fully trained you keep it both but suppose your dropout ratio was a 30 percent so now what happens is if you keep all the nodes initially let's say that the layer had 10 nodes so only seven of them were being activated three of them randomly were being pacificated then your total z came from seven nodes and now it is coming from all 10 nodes so what you have to do is you have to scale the z down you have to multiply the z by 0.7 otherwise it will get amplified so you bring back all the nodes but during evaluation you just amplify it a little bit and the good thing is networks take care of that for you good thing with pytorch is a lot of these things essentially are taken care of are we together if you go into eval mode it will smarten up and it will take care of it any other questions things Go ahead, Kati. This is Harini here. Harini, go ahead. Asif, in Colab 3.8 Python is not, I'm not able to install and work on it. So how do we go about doing it? I needed help. Did you try pip install Python? Yeah, we tried. It says the version is not there or something. There's an error. Asif, I read about it online and they say that Google doesn't support the version for Python. Okay, guys. So what I'll do is I'll backport the code. I'll remove that part which uses a Python 3.8 functionality. functionality and put patch it so that it works with 37 then you can run it and program works and after that none of it is working I didn't hear it your voice is the first week only image net. After that none of the notebooks work. There's some error or the other because of this. Guys, are all of you facing this? Anil, are you also facing this? Hari, Priya is also having issues with this. We need to collab once, Asef. Otherwise everyone has to upload 3.8. So I'm doing it on my local environment and I mean I can we sort of verified in an else environment that everything is working it's all working as if I'm sorry say that louder please locally everything is working as if collab by nature check it No, because the reason is with Anil one day, he was the first who wanted help. So I helped him and then I made everything work in collab. Maybe I need to go see what steps we did to make it work for him in his collab. I'll post those steps. Otherwise I'll backport the code. There's only line the unlink one cent one line that needs 3.8 so i'll replace it with something that is older version compatible all right uh as if can you keep both the versions if it's possible for the course of course i'll comment it out and i'll put a comment there saying the 3.8 use the version see the difference is this guys people used to deal with files directories using the os and system the packages but just as in every language like java you have the lovely and io packages In Python also they have modernized the IO, the file system access and so forth. So there is a pathlet package. And the preferred way or the recommended way now is to use the pathlet. They're still building it out. So every version they add more features to it. I just happened to use the features that I had on my machine which is updated to the latest. So I need to backport it. Remove that and just use the older API. It weights from all the layers right yeah the back the backward step updates all the weights in all the layers layer after layer after layer backwards that's when you you call it the backwards. It's like a back, you know, it's like a wave that's propagating back. Do you have to like go back to change like the later layers after it changed to, you know, earlier layers? No, no, it doesn't work like this. So, okay, let me once again show what we mean. And I'll have to share my writing screen for a moment. Are you seeing my writing screen? Yeah. So suppose you have layer one, layer two. Let's just take layer three. And this is the activation of the layer 3. So X goes in, activation of layer 1 comes out, that produces activation of layer 2, that produces activation of layer 3 and that produces the loss. So from the loss, what do you do first you compute a D. So this gets computed first last with respect to Here, I'll just put two things a three I'm just writing it in a symbolic manner means the gradient with respect to what the activation in the way this will get computed. So this First gradient with respect to both the activation and the weight this will get computed so this is uh first number one now once this is done then you go back to number two you know you move back to number two and here now you can compute using what you computed here using this you can compute now a2 w2 you need this this is number two you can't do number two without doing number one right and then it back then you move back again one more layer and then you can compute WL with respect to a 1 W 1 and then you're done that's it you the moment you have all of these three things what does it mean gradients are done and so the network dot step will do what the network dot step will just invoke the gradient update because all the gradients are now found and so gradient update you can do globally at one go like you don't have to do one at a time it is just a standard matrix multiplication I see all you needed was the gradients how do you exactly update them like I just want to know like all math works how the math math works is literally matrix multiplication. Like we said, let's go back up and look. We computed like like in very practical terms. If you remember Let's look at this example like the sum squared error and let us take the example of a logistic sigmoid function. So we happen to know what is the derivative of this with respect to a we already know it isn't it like i who wrote the uh the sum squared loss you know the guy who wrote the class called msc loss guess what he did he wrote this he baked this formula into the code right so it is baked in it's not that the computer magically finds the derivatives but the creator of that loss function had to bake in this formula then what happens is you then what is the derivative of this this is the this you remember is the activation function so for every activation function when you create you are required to also tell what is its derivative make it in so all you're doing here is plugging it in you know at the end of the day you if you if you look at this this is computed this is just this activation loss and of course course, you know the activation. Yeah. So therefore it is just simple multiplication. Once the gradients are found, it's just simple multiplication. That's all it is. So guys, we learned about back propagation. I'll put up this code. I'll also put up the california data set but do you guys remember the california data set some of you from the ml 100 200 so that will make two good starter points so let me just say river a data set the california housing price prediction now that of course is a favorite pastime of people here in Silicon Valley. I'm told that people almost every few months they go and look at Zillow or whatever it is earlier to see if the house value has gone up or down. So well we are going to use that data set and we'll try to predict the house price one reason to do that is this a very good data says to learn from also we have done extensive notebooks on it if you have taken the previous class so then it puts the neural networks in perspective how do you use the neural networks and your homework this time is for the toy data which is v-shaped data you can see what does the change of epochs size of the hidden layer and the activation function those are all your hyper parameters right in a sense how do they affect the efficacy of your model and read the reward data set to see how in machine learning we quantify the the metrics the goodness of fit of a model the quality of performance of a module so read it and you'll get some insight into that Thank you.