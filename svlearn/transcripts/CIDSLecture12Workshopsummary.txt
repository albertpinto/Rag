 Last session, we talked about an interesting topic, Monte Carlo. This I introduced as a way to solve rather intractable problems of integration as a motivating example. We took the case of a square that encloses a unit circle, let's say. The circle has radius one. And by the way, what is true for unit circle is true for circles of arbitrary length you create a square dartboard that encloses that circle then you hit the dartboard with dots you throw lots and lots of dots at it when you do throw lots and lots of dots at it they will fall anywhere in this dartboard and the assumption is of course darts don it. They will fall anywhere in this dartboard. And the assumption is, of course, darts don't fly off the dartboard, which tends to happen in real life. But our assumption is all darts hit somewhere in the dartboard. If you do this, and if I were to ask you, what proportion of darts made it into the circle? Let's say that inside the circle is a positive target. Outside, you miss. Inside is a hit. So what is your hit rate? After a bit of thought, you would say plausibly it is the ratio of the circle's area to the square's area. Now the circle's area is obviously for unit circle pi, and area of the square because it is two two the diameter is two it is four two square is four and so if you ask this question let y be the proportion of dots inside the circle yeah so y will eventually become the area of the circle divided by area of the board, which is pi by 4. Now, and which is what we write here, y ultimately becomes n over n. In other words, pi by 4 becomes the proportion that actually hit inside the circle, little n being the number that were inside the circle, n is the total number of darts that he threw. And from that, we come, which so far looks very simple, but then we come to a pretty striking observation. We notice that if you take this equation and write it like this, you just said pi is four times the proportion of darts that made it into the circle, which is quite remarkable. We are used to finding the value of pi using some infinite series, some power series, or some Taylor expansion. So there are many, many, many tricks, analytical tricks, and number theoretic ways that you can compute the value of pi. However, what we are looking for is something more that an empirical scientist would do rather than a mathematician would do. We are saying, let us do experiment in which we throw lots of dots. And by observing the proportion, we can compute the value of pi. In other words, you're deriving the value of pi through empirical evidence, a very startling observation. Now, if you were to write it as a couple, a python code, you can literally write a two-line code, a two or three-line code. Here it is, little n, and the proportion, and so pi is this. In three lines of code, you could compute the value of pi. What you just did, what we just did, is use a set of techniques that collectively goes by the name of Monte Carlo. People often call it Monte Carlo simulation, or Monte Carlo experiments, or Monte Carlo trials, and so on and so forth. So it's Monte Carlo. People often call it Monte Carlo simulation or Monte Carlo experiments or Monte Carlo trials and so on and so forth. So it's Monte Carlo. In a more generalized way, but still sticking to one dimension, we can say that consider an arbitrary curve. Let f be some function that it's a function of just one feature, one input feature. Let's say we are looking at, for example, how much ice cream is sold on a given day, right? We are sort of treating it as a continuous variable. So on different days, the entrepreneur, our young entrepreneur that we have been using throughout this workshop consumes different amounts of ice, I mean, sells different amounts of ice cream. If you were to ask how much ice cream did the person, did a young entrepreneur sell over the course of, let's say a whole year, and we can we can discretize or we can consider this as a to the first approximation like a continuous variable rather than a discrete variable and so the total amount of ice cream sold would be the sum total of ice cream souls the area under the curve would be a good measure of the ice cream. So more precisely, given a function f as a function of x, if this is the curve, you want to know what is the area under the curve. It would be the integral of this. Now, what is integral? Another intuitive example that we took is, and I think that's more precise, if you look at time, this x is time, and fx is the speed at any given time. You have been driving on the highway, and this is a record of your speed on the highway as you slow down and speed up and so forth so a natural question to ask is what is the distance that you have covered so far right so the distance that you have covered would be given by an integral of the speed with respect to x or more simply put if you were to chop this interval into this a from time a to time b into tiny duration intervals time intervals then it would be in a sense the speed at that time multiplied by the duration because speed times distance speed times time is distance right so that would be the distance you would do in that very small amount of or duration of time now we say that in the asymptotic limit when the durations become very small but then this summation tends to become the integral. So now that's a pretty fancy language. If you remember, I mentioned that I use a simpler language. I say that this integral or the total distance covered eventually becomes this thing, fx times this, which becomes the integral. So as an integral, we need to solve it. The question is, how do we solve it? Not all functions have analytical integrals. It's one of the results we talked about. When you have a function, it's very easy to differentiate. All functions, more or less, you can differentiate. But when you talk about integrals, integrals are a completely different cup of tea. We can, well, in the world of integral in formal mathematics, there's the Leibniz integral, the Riemann integral, and so on and so forth. But even if you stick to the standard Riemann integral, which is area under the curve kind of a definition it turns out that most functions actually most functions cannot be integrated the few functions that can be integrated in a closed form in an analytical form i can't hear you okay um. Are you asking a question, Harini? No, sir, no. Okay. So most of the integrals that can actually be done, in fact, all of them are usually there in your calculus textbooks. And that may give you the impression that integration is just like you can differentiate every function, you can probably integrate every function. But as a clue, if you remember, integration is a lot harder than differentiation. You do all sorts of techniques when you are in high school. You probably do integration by parts, by substitution of variables and so on and so forth. All sorts of basic techniques you learn. When you get to more advanced level, perhaps graduate school, you learn about all sorts of techniques like the Feynman's trick to do integrals and so on and so forth. And in machine learning, when we do the next workshop, which is the math behind data science, the foundations, I will be, we will go through some much more advanced integration techniques. And yet, the set of integrals that can actually be done is finite. And generally, any expert in the physical sciences and the mathematical sciences, physics, chemistry, and engineering, usually knows all the integrals at some point of time without realizing they know all the integrals that can actually be done in closed form or functions that can be integrated in closed form. So then there are two recourses. One is you can do numerical techniques and we talked about it. You can discretize what we just did, discretized an adjective. And there is the Runge-Kutta method, and so on and so forth, a numerical analysis method. The problem is in higher dimensions, what is a discrete bits of along the X axis becomes discrete patches of region in the feature space when you're trying to do integral. Now in the feature space, if x represents a point in a D dimensional or a P dimensional feature space, you realize sometimes that if you make 1000 discrete intervals as your tiny little slices, then the total number of those hypercolumns over which you're evaluating F is 1000 to the power p. Let's say that p is 10. You can convince yourself that 1000 to the power 10 is a ginormous number. You can't numerically solve that. It becomes an intractable problem for all practical purposes. And therefore, sometimes even numerical methods begin to not give us good results. Therefore comes a technique that we, exploiting what we just did with dartboards. It asks the question, can we apply a similar technique here? And the answer is yes. What you could do is you could take this function and sort of wrap it, package it in a box. You want to compute the integral from a to b. So what you do is take a sufficient height, h, such that fx is always below h in this interval. And so you basically wrapped it up in a box, put this function in a box. When you put it in a box, this box is your dartboard. Once again, this boat. And once again, you start shooting the darts at it when you which which is a metaphorical way of saying just pick random points in this rectangle when you take random points in this rectangle the points that are below the region how many points are here? You would agree that the proportion of points that are below here is the area of this function. So let's say that the area of the total board is b minus a times h. Why b minus a? This interval is b minus a, and h is the height of the base, b minus a, height of the rectangle is h. So that is it. And so you can say and h is the height of the base is b minus a height of the rectangle is h so that is it and so you can say what is a proportion that stays inside proportion would be the number that hit the number of dots that hit inside times and you can work through the little bit of algebra times the area of the rectangle and so you can come to the conclusion that the um to compute the integral which is the proportion of dots that will hit below the curve you just need to do some very elementary algebra and when you do that then you realize that the integral eventually becomes this an expression in terms of an observed value. In other words, what you did is you used an empirical method or what you call Monte Carlo simulation to solve an integral. And I just now reformulated that circle problem in terms of this. We won't get into that. Now, I mentioned why is this important? This was just a taste of things to come. There is a whole field of machine learning and many people believe it's the right approach, including me believe that it is the more appropriate framework to look at machine learning, which is the bias in machine learning. When you look at machine learning as a journey from a prior belief to a posterior belief, then you are thinking as a Bayesian. So one example that I keep giving is, if you ask a typical person, what is probability? Probability is, they'll say, the proportion of an outcome given a sufficiently large number of trials. For example, if I ask you, what is the probability of a head in a coin that may or may not be fair? So let's say that the probability of head is p. How would you find that? One way that you could do is you can go on tossing coins, let's say a thousand times or ten thousand times or a sufficiently large number of times. See how many times did you get a head, right? And so the ratio or the proportion of times that you got as head is the probability of head, that is a frequency is definition of probability right go ahead such a so. Can you. one's probabilistic model is from saying these are two parallel kind of thinking or tracks but at the end of it the reality is that one should be able to think in both ways to convince oneself right so i did that kevin murphy's that book right yes after like fourth or fifth chapter is like he gets so much into probability you start losing the bigger picture but i believe that you have to always go back to that and say this is the right way to do it how where you said that now you believe that like a lot of people believe this is the way to do it is this two parallel track that has been going on for quite a few decades and people won't converge or is this people are saying now the future is that i think we should always think in this term as opposed to the prior thing okay the normal way of doing yes so for those of you who are remote explain the context i didn't catch the context that's right so let me rephrase it and for those of you are remote and who didn't catch the context and for those of you who are remote and who didn't catch the context, Sachin here is asking a question. He of course knows that there's a frequentist approach and there's a different approach, the Bayesian approach. The basic thing is where do they meet or do they meet at all, right? And what is this about? When you look at machine learning, when you look at data science, quite often you see two independent approaches. One is people who more or less avoid getting too much into probability theory, who look for, like for example, they say, what is the best fit line, which is more likely to predict the data. So broadly speaking, they are within the frequentialist camp. Likely comes from the likelihood estimation, something we talked about, but I won't go into such detail now. However, at the heart of machine learning is uncertainty. See, I'm making predictions or decisions in the presence of insufficient information. This is life. And I mean the confidence. That is right. So you always are because if you had perfect information, there is no need for artificial intelligence or machine learning. You could just write code, a deterministic code, a program, and that would solve your problem. So machine learning is probability at its core, has probability at its core. Now the question is to what extent do you acknowledge or be rooted in that probability theory? And that probability theory, the right way, is the Bayesian. So now I'll explain what's the relationship of Bayesian and Frequentialist in a moment. Is a Bayesian. Most machine learning experts are deeply Bayesian people. Physicists are deeply Bayesian people. We believe that the Frequentialist approach is inherently wrong. And I'll give a simple example to illustrate that. So we talked about the long runs of coin. You do lots of trials with a coin. And look at the proportion of heads. That's a probability of head. But suppose I were to ask you this question. What is the probability that human activities is causing global warming right and will destroy the earth the human activity caused global warming will destroy the earth now if you were to ask from a frequentist perspective, he will gleefully say, absolutely wonderful, go ahead and keep screwing the earth a few 1000 times. Each time we just do whatever human beings are doing today. And then wait and see that the earth gets destroyed. And if it got destroyed, oops, but hey hey this is just one trial let's do it all over again right so the trouble with this to know and then let's say that you do it for 10 000 times then see how many times the earth got destroyed and how many times it didn't and so you have the probability that the earth, that human activity will destroy the earth. That's a frequentious perspective. Now, if you're in the scientific discipline, you'll call this absolute nonsense because you don't get to do that experiment twice. And frankly, you don't want to do it even once. Isn't it? You don't really want to take the chance of playing it out and seeing whether the earth is getting destroyed. The purists in the frequentist camp would say there's only one definition of probability and that is that you have to look at the proportion of something happening in evidence. And at some level, that is also if you really think of the debate between global warming and people who deny it if you look at the so they will deny it for all sorts of reasons but some of the more legitimate reasons or severely legitimate reasons they give they are a bunch of very seized thinking people who genuinely believe that there is no global warming caused human destruction of the earth. And if you look at the reasoning they're giving, they're almost always saying that, see, look, we don't have data. We don't have evidence that all of this has ever destroyed the earth. The counter argument that the people who are so worried about global warming's potential to destroy the earth is that you will never get the chance to develop that data. We as a species would disappear. So what is the other way of thinking? We need to rethink the definition of probability itself, which is what I gave in the example that we did last time. Suppose I gave this example. Suppose you land up in the dark, you and your friend land up in the dark in some city. You have absolutely no idea which city you have landed in. You make a bet. You say it's Seattle. Your friend says, no, it's Phoenix. Both of you agree that in Seattle, there's a 90% chance of rain every day. And in Phoenix, let's say there's a 10% chance of rain every day. I'm just cooking up numbers. These are most likely not exact numbers. But to the point that we all consider Seattle rainy and Phoenix, a desert. So you have and then what happens is, you say, all right, you you believe I believe Seattle, you believe Phoenix, let's see what the data says. So what you would say is that your and my prior beliefs are different. My prior is rainy, your prior is it's a desert. Right? Then let's say the evidence is rain. One day it rains, second day it's dry, third day it rains, fourth day it rains, fifth day it rains. If you think about it, it doesn't rule out a desert. Even desert can have that pattern in that, especially in in there's always unusual weather patterns right it could happen but which is it favoring so you say that if you believe you are in seattle what is the likelihood of seeing this evidence right so you realize that and we work out the number that it is far far more of seeing this weather pattern this data in seattle than in phoenix and so what happens is when you take your likelihood multiplied by the prior you get the posterior that is the bias theorem right and that is the foundation of biational analysis system. The data educates you. The likelihood of the prior belief producing the data helps you change your prior belief to a posterior belief after the fact belief, which says that now the probability that it is Seattle has gone from 9 10 to 99 point i mean uh 99.8 percent practically a certainty and the probability that you're in phoenix has gone from let us say 10 percent or whatever that you had down to not being seattle has gone down to 0.2 percent right so the biotins think of probability as a degree of belief. They say that you'd never have perfect information and a probability in many ways quantifies either you can say belief or the degree of uncertainty. How much you don't know and data just reduces how much you don't know and takes you to the maximum a posteriori hypothesis. What is most likely to be the truth? Go ahead Premjit. And I think we frame the conversation looking at a planet getting destroyed. Then we move to the Seattle versus Arizona to explain. Now I see a uh oversimplification aspect that's coming up right when i try to compare seattle and arizona using rain as a determinant it is fairly intuitive to say it rains a lot in seattle it rains it hardly rains and i'm just using arizona as an example for a desperate hardly arranged there so just that one attribute on which i could gather data for five days allows me to improve or decrease my confidence in a prior but when we go back to the earth analogy like will human action cause damage to the planet? There are a zillion variables at play. Gathering any more data over several days, how does it influence the prior? Okay, so first of all, it's a slight change of terminology. You don't speak about confidence in the prior, you speak of the a posteriori belief, posterior belief or posterior probabilities the the question is basically the seattle example there which city did you land in your belief example is a one-dimensional example the only thing you care for is whether it rains or not right but in global warming there are many many variables yes you could you could take any small set of those variables, right? And ask yourself, all the data of that feature space, which evidence, what is the evidence pointing to? What is the likelihood function for each of the hypothesis? There are two hypotheses. Hypothesis one, the probability, let's say that all this activity will destroy Earth. Let's say that you start with a prior probability of 90%. You really are worried. Another guy believes half, 1%. Both guys are welcome to start with their prior belief. The whole point of science or scientific journey is a biocent journey. It basically says everybody from the experience can bring a hypothesis, initial hypothesis or prior hypothesis. All such hypotheses are valid as prior hypothesis. But then you look at the empirical observation, that's a crucial word, the data, when the data accumulates, you ask which of the hypothesis is the likelihood function favoring, right, and then you come to a posterior that, and so while there are infinitely, I mean, many variables, let's say, for global warming or for weather conditions, it doesn't mean that you necessarily need to dip into all of them. Quite often, just a subset of them are enough to take you to a very posterior belief that makes you more or less certain that if we continue to do what we do, we won't exist as a species. Earth will not become habitable anymore. It will destroy. It will destroy this earth as a home. And we have reached that point, by the way. Scientists argued all through the 2010s, till the end of 2010, there was a huge debate whether the evidence is sufficient or not. Somewhere around 2015 or 2014, there has been more and more global consensus developing. And now only crackpots say that it's not going to happen. We are in imminent danger. The belief now, for example, is that it is not just that we are destroying the Earth. And I mean, Earth will not get destroyed. What will happen is earth's weather patterns will change so drastically climate will change so drastically that our life human beings will not find it habitable we as a species will disappear the probabilities of that are overwhelming and it is not the first time that it has happened. For example, and it's one of the things people don't realize, Earth's atmosphere actually used to not be oxygen. It used to be carbon dioxide. There were species, and there were some other gases. There were species. Earth had enough biodiversity thriving with that atmosphere. But Earth quite literally went through a process of poisoning of the atmosphere, a toxicity of the atmosphere, in which this poisonous thing called oxygen came about, wiped out most known forms of life. And then a new form, new forms of life adapted and emerged to that could survive with oxygen in the atmosphere and we belong to that set of species right but all of this data is pointing to the fact that the way we are doing things we might be coming to the end of our days right so that is the bayesian journey anyway that's a long digression and a summary of last time make the connection to what i was trying to get to us we need yet another determinant which is essentially identified strong variables no no you don't need to you don't need to the point is that and this is again we are strained from the point so i'll close with this thought see in all of this machine learning you can feed in a lot of data and the the algorithms are able to surface and utilize the stronger predictors or the conspiracy of the stronger predictors and they're using that connection yes that's what i think it does give you another example which might probably be a little bit easier for you to understand is that you go to a doctor, right? We can go into more detail later on. They're not sure what is wrong with you. It's very common, very, very common. They will prescribe different tests to you. Depending on the outcome of the test, they will do certain other tests for you. And different specialties will have their own set of tests that they will do certain other more other tests and different specialities will have their own set of tests that they will choose but no matter what order those tests are done you usually tend to go towards the right cause which is called ultimately this may be but they're not pinpoint nobody can pinpoint it's the probability of everybody sitting together when all the force patients sit together they'll say you know what i think this is probably what is wrong with them which is causing us to see this the order of the test doesn't really matter it will always take you to the right that's that's all right likelihood of this disease yes yes increases or decreases based on the observation of the. CT scan. So guys, this is a now, this is all a very interesting discussion but i'll now go back to. to our workshop review. So let's put this on the sidelines for the time being. What I would like to do is, let me see how far along are we. Yeah, this seems to be the set of notes. All right. So now I noticed that we have given a better part of about half an hour or more to the last session's review. It's an interesting topic. We'll put it on hold and we'll go back to the basics. We started this workshop by saying there is broadly a field called artificial intelligence. Artificial intelligence is a very interesting topic. It turns out that human beings have tried to create artificial intelligence way, way before even the Industrial Revolution. You see mentions of artificial intelligence in almost all of the world's mythologies. For example, in the East, you have the story of Aladdin and his magic lamp. You rub a lamp and this magical beast comes up with superhuman intelligence and superhuman ability to do things for you. You conjure it up. The Jewish tradition talks of the golem. You make dolls out of clay, figures out of clay, and you could potentially have the ability to put life into that statue. And when you do put life into the statue, it's a dangerous power. It can do a lot of harm as well as do a lot of good and it has to be used with caution. So with right since the beginning, and likewise in every tradition, you see this in every mythology, you see statements of human beings creating essentially their own images, but much better versions of their own images. So this is built into human psyche to play God, to build things of our own that we can endow intelligence with. While this has been a long dream, and if you look at the history of computing and it's worth reminiscing on that, if you look at Charles Babbage who was trying to make the first computing engine, and he was very particular that he was not creating a machine just to calculate. He wasn't trying to make a calculator, a fancy sort of a way to do quick computations. What he was interested in is to create an intelligent machine. And this was Charles Babbage was trying to do it using, of all things, gears. He never quite succeeded because mechanical engineering had not developed to the level of precision and perfection that you could actually realize that machine. Today we do know that those machines have been realized. I believe in San Jose, if you go, you'll see a Charles Babbage computing machine which has been implemented and the whole thing works. It does compute. It is a bona fide computer. Then along the way, you see other milestones. You see during the Second World War, the Enigma machine coming up, and Alan Turing and his group trying to decode the Enigma. And in the process of doing it, while everybody else was doing code breaking, Turing, many of you may have seen that movie, what was it called? The Some Games, the imitation game. If you haven't, please do see it. Being in this profession, you should see the history of your field. Alan Turing was an unusually odd character, extremely bright, he has made profound contributions but he had a pretty tragic life. So his vision was not to just break the Enigma code but was to create the universal computing machine and what today we call the Turing machine. And he did succeed in creating something like a Turing machine using, well, again, gears and electrical circuits, a big improvement in Babbage, who was just all gears. He did manage to create that. And today, Turing machines are everywhere. We just call them computers. We don't call them Turing machines are everywhere. We just call them computers. We don't call them Turing machines. What was Alan Turing's dream in creating the Turing machine? It was actually to create artificial intelligence. Ever since that happened and thereafter, computers were made using vacuum tubes. All the time we have been debating about artificial intelligence. For example, Isaac Asimov's three laws of robotics. How do you make sure that robots, artificial intelligence doesn't take over humanity and destroy it? People have worried about the ethics and the power of artificial intelligence. So artificial intelligence is not something that just dawned upon people in the last 10, 15 years. People often feel like that as though it's a brand new subject and we all need to learn that. But it has been there ever since. It even predates computing itself in many, many ways by thousands of years as an aspirational dream, as a millennial wish of humanity. Well, the subject is broad. It contains many subfields. The theoretical core of artificial intelligence is machine learning. And one of its most common applications is robotics. So when you talk of a Tesla car on autopilot, it is robotics. When you look at your Roomba, it's robotics. When you look at any of today, manufacturing has almost all become robotics driven. If you go to a modern manufacturing plant, you find that there are probably 10 robots for every human being there, right? Robots are doing everything. The reason that you can order anything more close to home, anything on amazon.com and practically find it delivered the same day or the next morning, next day, is because of robotics, right? The fact that you send a letter in u.s and it reaches any corner of the united states very quickly in three four days right is because of robotics and ai that is sorting and handwriting recognition is happening know, whatever you write on the envelope as the address is never read by a human being, actually never. It is sent into a sorting box where optical systems and artificial intelligence, deep neural networks, study the handwriting, they decide what address it is. And at the bottom of every mail, you may find this little barcode kind of a thing, little squiggly marks. And those squiggly marks is the computer encoded address. It goes through automated sorting machines in places after places and reaches its destination. And the only time a human actually looks at it is when it is put into the basket that a mailman has to carry for your neighborhood and the fact that most letters don't disappear is a testament to the fact of how invisibly and how perfectly artificial intelligence has been running pretty much the modern world for the last 20 years by the way this sorting of letters has been going on for more than 20 years. We just assume it, but just think of it as a fact. In India, if you sent a letter, it takes weeks before it reaches the other end. The probability that it will never reach there is quite high, right? Even though they're also, they're trying to bring in all of these systems now, used to be quite high. It was anybody's guess whether a letter would reach the destination at all or not right. that's how it used to be, but in the United States, which is a land at least five times the size. of India and with far fewer people who can be postman and who can actually work in the postal office, the entire thing works seamlessly here. So artificial intelligence, if you really look around you, it is today permeating every aspect of life. You run the Roomba, there's artificial intelligence. You run your car and their computer. There are many microprocessors that are using some form of machine learning to basically compensate for the mistakes you make. There was a statement that if a modern man in the 21st century in the US is given a car of the 1970s, the person would have an accident within an hour. We have all forgotten how to really drive a car. We don't realize that modern cars, they give you so much of help and assistance that you don't realize that they are practically driving the car, just taking hints from you, right? And that is the power of machine learning. Today, when you write code, even if you compile it, let's say you use the time on a GCC compiler to compile your C++ code, you may not realize, but today that code is optimized using machine learning and artificial intelligence to create the best possible binary that will run at blazing speed. So code optimization is driven by machine learning right more than that today we are entering an age in which ai is writing most of the code those of you who have tried out the code pilot uh in visual studio code uh you may realize that and uh we will if you haven't i strongly invite you to go do that if If you try out these things, what they do is you just so much as write the comment of the function that you're going to create an artificial intelligence will take over and write perfect good code, whether it is Python code, whether it is JavaScript code, Java code, whatever it is, it will write perfect code for you. It figures out what you're trying to say, and it will write that code for you, right? Just ponder over it. We are entering an age in which most of the coding will not be done by programmers. Most of the coding will be done by artificial intelligence, right? So the age of the programmer, right, the rule of the programmer, right? The rule of the programmers now will begin to fade very soon, as in not in decades, but in like three, four years, those changes will come by. And it will become far, far easier to write code, just as it is far, far easier to drive a car today. Because of all the AI, it will become far, far easier to write code. Kids will be able to write code. You wouldn't need a lifetime of education and a degree in computer science to write code anymore. And this is coming. This is coming as in the next four, five years. So there's a power of that. Now within machine learning, the important word is learning. Where did the word learning come from? The initial dream of artificial intelligence was to create what is now called a strong intelligence. We know we can't create strong intelligence. In other words, roughly speaking, thinking machines. We can't create machines that can think, but what we can create instead is a weaker form of intelligence. We can create machines that learn. And therefore, we have machine learning. The core of artificial intelligence is machine learning, not machine thinking. We do not know how human beings think. Therefore, we have no way to encode thinking as a mathematical function and therefore make progress in the field. Nobody knows how to even start in that direction. Those of you who are familiar with the history of the subject would remember that there was a legendary company here in Silicon Valley many years ago, 30-40 years ago, literally called the thinking machine. The great noble laureates came and worked in that company including richard b feynman and so forth it did not succeed ultimately it got broken up some parts went to sun some went to oracle it became a state machine learning group but uh as machine learning we can do it. So machine learning is what? The way we do that and the example that I keep giving of learning is we learn through first by quantifying the errors and then by reducing the errors. So one example that we gave is of a teacher or a mother taking some children or maybe just a child to a special zoo where there is a meadow and you have on a meadow where there are lots of cows and ducks so the mother is trying to teach this child it's a two-year-old child that look at that feathery little creature it has a beak and it has webbed feet it's a duck and then and look at that big animal with a big swishy tail and horns and a huge size and that's a cow right you show the child many examples the child is too small it's a two-year-old child it's it's having a trouble grasping all the complicated big words of adults and all the things so you ask ask this child, what is this animal now? And the child will just make random guesses in the beginning. And so the child will often be wrong. But as the child gradually begins to get a sense and you go ahead. Yeah, I mean, it's not that the child doesn't recognize you know, because within years of intelligence, it goes into making human being, which I've already recognized. Right. Putting it into the words that we want, that is the problem for the child. That is right. So that's a very good profound point, actually, you made, Sanjeev, that see, for cows and ducks and so forth, there is a million years of species learning that's encoded somewhere in our DNA when we are born. For example, it is known that children or even the child of a calf or something will be scared of a snake, but will not be scared of a hen, right? So there's some pre-encoding, but let's simplify the picture for now. Let me say that you're talking to a child and you're using these words that if it has feathers, then it's a duck, if it has a tail then it's a cow. So what happens is that the child doesn't get it and makes mistakes. What is the evidence that the child has begun to learn to recognize cows and ducks? The evidence would be that it is making less mistakes. The error rate goes down. As you show it more and more examples of cows and ducks, the child says, well, it recognizes most of them, still makes mistakes, but recognizes makes them. And what you expect is happening in the child is, in the mind is forming a concept of what a cow is or what a duck is and so forth. And as these concepts begin to form, the child makes less mistakes with these things. Right, okay, so I noticed that we are almost at time for a break. We will take a break. Give me a few more minutes, guys. more minutes guys all right so in other words you have quantifying the error and a way to reduce the error the way we measure the error for example for regression is through some squared error residual error all right and we try to minimize the residual error. I won't go into all the technical details. For classification, you look at the confusion matrix, how many mistakes that you made. These are ways. And then there is a mathematical technique, which is very powerful, which is to reduce or find the most efficient way to reduce errors, to learn, the fastest way to learn. That was gradient descent. And it's a family of algorithms. We started out with the very basics. And we talked a lot about it. I won't talk about it at all. But basically, we need to find the shortest path home to the point of least error. So what is happening here? Yes. So I won't go into that. Now, one of the concepts we learned is the bias-variance trade-off. What is the bias-variance trade-off? In the bias-variance trade-off, you have the fact that there is a ground truth that you don't know you can build models of different complexity if you build a model that is overtly simple you will have bias errors if you build a model that's overtly complex you will have overfitting or variance errors and so the way to guard against it is to train to test the model against the data the model has not seen, data that you have sort of put or hidden under the pillow, sort of, to put it like that, and not shown to the model or to the algorithm while it was learning. When you do that, then you can ascertain whether you have a good model or not. And what will happen is at certain degree of complexity, for example, for dataset two, the right degree of complexity was it was a sine wave kind of function. You could go to a polynomial of degree three. And if you went to a polynomial of degree three, it seemed to have worked quite well, which we are illustrating here. So the more technical answer is the total error is the sum of three parts the bias part actually you use bias square for reasons because of the way bias is mathematically defined but think of it as bias variance part and then there's a epsilon the The epsilon is the irreducible error of the system. Right? So that's that. Let's see if this seems to be coming from the cloud. And for whatever reason, I don't seem to be seeing it come here. Yes, it's finally here. So we learned about regression and classification. I won't go into those things over and over again. We learned of the powerful technique of regularization. You can regularize data or remove variance errors by either throwing a lot of data at it or by using more powerful regularization techniques, some of which we learned, rich regression, lasso regression, and so on and so forth. We also learned about power transforms along the way. We went into a geometrical interpretation of regression, sorry, regularization of how it is a problem of constrained optimization. So, by the way, one of the things I will do is I will find some way to share this entire notebook with you. And that is a plan to do next Saturday when I'm free. I will give time to make that possible. So constraint optimization is a wonderful thing. Get familiar with it. Read up on it a little bit more. We'll need it a lot. We'll review this quite a bit in our math of data science all over again. Now, we talked about Minkowski norm. We generalized the definition of Euclidean distances to a broader notion of distance, which is that of the Minkowski distance. And it leads to different shapes of distances of unit circle. What does a unit circle look like in other spaces? It turns out that unit circle looks round only in Euclidean space. In other spaces, it doesn't. And using a norm less than 2, less than Euclidean, helps you occasionally do dimensionality reduction, because it will throw away many of the variables. The puncture point will be along some of the axes right and so it will do that this is again i think so let's take a break here then i'll start reviewing classifiers so guys i hope this is by fire classifier the main goal is to to identify given certain given a datum which contains certain attributes, certain features, observing the value of the features, you have to identify or tell what is it. Is it a cow or duck? Does this person have a malignant or benign tumour? Does this person have a malignant or benign tumour? Does this person have diabetes or not? And which tree is this? Is it a weeping willow or is it a pepper tree? And so on and so forth. And one of the many things, for example, how would you, which of the colours of the spectrum is it? So things like that. so when you do that you have the concept of a decision boundary to classify or to build a classifier is to is equivalent to saying i have found a decision boundary which is a hyper surface or an embedded manifold in the in your feature space right now all of this must by now be looking very basic and you will have, obviously all of these things you have in the video recording, but I'll also take out screenshots. We'll find some way to do that. The distance function was important. The logistic function was important. We went through the derivation of that. And cherries and blueberries, they served us pretty well in this particular part. We also learned about the thing about loss function is made up of the cross entropy terms, right? Why is it cross entropy terms? It comes from the joint probability, from the maximum likelihood estimation argument, MLE. From there, we get this. So that is something to remember. That's the most often used loss function for classifiers. We learned about the receiver operator characteristic curve. We, again, we learned of some special techniques, in particular, linear and quadratic discriminant analysis, what they are. Basically, if you imagine that these two things are Gaussians or normally distributed, the two classes are separate, you can connect the tippy tops of maximum uh probabilities or where the gaussian speak and then tippy top you connect a line and you find the perpendicular bisector that's your decision boundary right and this is taking a long time to render because it is taking a long time to render because, just taking a long time to render, one second. So Microsoft thing guys, I think it's pulling it from the cloud or doing something. I have no idea. Yes. No, it's not yet doing it. Anyway, so while it is rendering, let me start talking about it. So you do the discriminant analysis, you do, you can make linear discriminant or you can make quadratic discriminant analysis. Then we moved on to clustering, pattern recognition. We talked about data always has clusters. One of the articles that I posted in Slack has this beautiful comment, one of the papers, by Jay Gould, Jay Gould of the great theory of evolution fame, the person who wrote the landmark book on the theory of evolution, summarizing all the data and so forth. And he says that one of the most remarkable things in the world is that it is most natural to see patterns in the data, patterns in nature, to see clusters, for example. But when you don't see it, it means something. It's very, very hard to create truly static noise. In fact, we do know that even the static noise of radio and TV are not really noise. Embedded in them are the subtle echoes of the great big bang of the universe itself, right? The microwave background of the universe. You can still hear the echoes of the big bank. Quite a fascinating thing actually. So now that we know that clusters are everywhere, how do you find clusters? We use k-means clustering to to do that we do hierarchical clustering k means clustering hierarchical clustering i won't go into that we used a measure which is often called inertia or within cluster some squared distances. By the way, this is often also called dispersion. D squared, the distance. These distances are often called the dispersion, square of distance. So how do you find the optimal K? We use the scree plot and the ELBO method to find the optimal K. Stop me guys, I'm going quickly because you all have done this but if you feel that something needs further explanations i'll be happy to do that then we talked about the a more formal definition of distance what is distance it turns out the distance is a function that obeys some basic rules distances are always positive distance between if the distance between two points is zero it is because both of them are the same point and not otherwise right that is why i use the double if this is the mathematicians if standing for if and only if means is both ways if two points are the same the distance between them will be zero. If distance between two points is zero, then they are the same point. And then the important Schwartz inequality, which says that the distance, the shortest distance between two points, right? The straight distance between two points is always shorter than any detours. Any detours that you take. Any detours that you take. So I often say that going to work always involves when you go to work and you sit down at your desk before you sit down at your desk, there is always a detour through the break room, the picking up coffee, having a cooler, you know, cooler side, water cooler chat. And then you finally land at your desk. And that path is longer, but going home from work is always a straight path. The shortest path, you get into your car and you head straight home at the end of the day. So just making it very real. We learned about the dendrogram and how to build dendrogram. We learned about linkage functions,rogram and how to build dendrogram. We learned about linkage functions. There are many linkage functions. And we learned about density-based clustering. It is the most, the DB scan uses this reachability argument that basically it says that in the points that are within a cluster or near the center of the cluster in the interior, they will have lots of neighbors in short distance. points that are within a cluster or near the center of the cluster in the interior, they will have lots of neighbors in short distance and outliers will not have neighbors. D-U scan is good but it has these two hyper parameters, epsilon and number of points in the neighborhood, which is a fatal flaw because of that you don't you get different clusters based on what it is we went through the labs one of the things we could do maybe in an extra session this coming tuesday if i'll see is i'll just as i'm reviewing the lectures i can review all the labs in one hour or something like that let's see if time permits. We can do that. So we will continue to do little activities throughout December. We won't end the workshop right here. DENCLU. What is DENCLU? It is a more, in my way, the scientific way of looking at it. It looks at the so-called influence fields. If each point sheds a sphere of influence around it, those influences add up and any arbitrary point in space, which is space will be the under and will experience a sum total of influence from all the other points, right, all the points here. And so you, by looking at the gradient, a field is a word that theoretical physicists absolutely love right and i being theoretical physicist to me it's natural the natural way to solve any problem is using fields and using manifolds so differential geometry and algebraic topology right so you can see me getting excited over here bringing in the things and then looking for the attractors. Attractors are points where those fields achieve maxima, local maxima. We also learned about the curse of dimensionality. I wouldn't say about that. It basically makes in higher dimensions, space becomes extremely sparse. You have to go pretty, pretty far to meet your in-courts neighbors. When you have to go that pretty far to meet your in-court neighbors when you have to go that fast are they really your neighbors right so for example in bay area if you want to talk to your neighbor you just open your window and maybe call out your neighbor and the two of you can talk through the window through through our respective windows right or if you want to share a piece of electronics, a USB cord, you literally stretch your hands out of the window and your neighbor will stretch their hands out and get it. So distances are short. But if you're in the middle of Idaho or Wyoming, to meet a neighbor is a ritual. You dress up, you get into a car, you drive for one hour, and then you hit your neighbor. So that begs the question, is it really neighbors? Californians would hardly call them neighbors. So that's the problem of higher dimension spaces. We also, one of you provoked me to just talk about spectral clustering. So I did that. I don't know whether I should apologize for that or not it's a it's a chain of reasoning especially these sort of complex algorithms you should do only after you have done the math of data science right because i went it needs graph theory it needs uh matrix you know the eigenvalue decomposition of matrices and so on and so forth but for whatever it is worth it's a powerful method and we talked about it i asked you about what was the feature sizes beyond which we say that this thing is like too big and the literature was saying that 20 features are 20 features after which you kind of get into this realm of... See, the point is that it's hard to tell. There is a whole body of debate in research. There are people who argue that even in very high dimensional spaces, sometimes because the data is highly non-uniformly distributed, you can apply certain tricks and do clustering. There's a whole body of new techniques people keep coming up with. But the golden rule that I keep saying is, if you're doing clustering, don't cluster in the original space, cluster in the kernel PCS space, reduce the dimensionality. That's where the whole thing of and also the whole technique of manifold learning etc i didn't talk about like big algorithms actually we introduced it in the boot camp dsne and umap if you remember they come from manifold learning and they sort of they help you learn the underlying lower dimensional manifold that the data adheres to and do your clustering there rather than in the original space. So that is a basic rule to say that can I do clustering in a hundred dimensional space? You know, it's anyone's bet, most likely not, but sometimes you succeed. If the data happens to be so. Remember the no freelance theorem. There are no certainties about data in machine learning. It's all about knowing the answer after you have explored the data. I often say that doing machine learning, doing data science is very much like dating. You don't get to know your sweetheart or a person in the first date. You know nothing. First date is a ritual. It's a well-practiced ritual in almost all cultures. In India, the parents will introduce the boy and the girl, and they will have this very stilted conversation of politeness. In the West also, the boy and the girl will put on two different perfumes and two different nice sets of dresses. They will meet at a restaurant and they'll have a very, very impressive conversation, bring their best foot forward. But knowing takes many interactions, isn't it? There is a study actually, the psychologist did that said it takes at least 70 hours of interaction between two human beings before trust is established. It's a very interesting number. So it's like that. See, there are no certainties, right? You hear people make blanket statements like I just happened to have a friend when I was young in that. I remember when I was doing graduate school and there was this very good friend of mine Kamla, I won't tell the last name. She made a blanket, she had this blanket statement, all Indian men are male chauvinist jerks. Well, I knew that there was a great deal of truth in it. But I still felt belittled. Right? Well, all I could say is such generalizations are never true. It's like the no freelance theorem. Each human being is different. You can't apply really hypothesize that this algorithm will work. Because to hypothesize that this algorithm will always work is to hypothesizing a particular structure in the data, which may or may not be present. Right. So, if you break the set of features in something where you have like a gut feeling that if I try to bring say, let's for the sake of argument say 100 features, let me look at how these 20 features in that space, then another 20 set of things where you kind of validate your gut feeling as to okay these things are coming over here in this thing and then try to apply it the larger yes yes is that okay that is okay everything is okay see sachin here's the thing you're exploring data just as there's there are no rules of human relationship or dating there are no rules with dating with data either. You have to make friends with data. You have to keep exploring it and approaching it in different ways, till it begins to reveal its secrets. So pretty and life can be very strange i um uh maybe we can pause the recording for a moment please okay respect the spectric clustering was a long journey i would normally not have taught it unless after doing the math of data science we will revisit it unless after doing the math of data science. We will revisit it when we do the math of data science, because you'll be mathematically equipped to do such a long journey, you know, a journey of 10 steps. You first take the points, you make neighborhood graphs, then from them you do adjacency matrix, the degree matrix, the Laplacian matrix. You do eigenvalue decompositions you find principal directions you project down to a lower dimensional space in that lower dimensional space then you go about doing let us say k means clustering right it's a pretty pretty long journey or density-based clustering or whatever it is it's a pretty long journey right so now just as a basic terminology, I keep saying decision boundary for linear methods is a hyperplane. What is a hyperplane? It is just a fine hyper surface. What is a fine? A fine just means it doesn't have to go through the origin. It can translate in any place. So if you have a feature space of p dimensions, the decision boundary necessarily has to be a hyperplane whose dimensionality is one minus one p minus one right for example in two dimensions a hyperplane is a line it's one dimension things like that then we went into this discussion of support vector machines we started out with the picture of my hometown, Benares, which has the river Ganga or Ganges running through it. One side is the city, another side is the agricultural area. We used that as a motivating example. And I basically said that support vectors are lighthouses. Lighthouses that light up, right? And guide a boatsman. And the part that an intelligent boatsman will take not to hit the banks is, and stay away from both the banks of the river, not to get stuck or capsize with this boat, is the decision boundary. The river banks are the marginal margins. The formal word is maximal margin hyperplane classifier. Now you can bring in some mistakes because ultimately they can be lighthouses. Some trees might have floated into the river during the rainy season and so on and so forth. So you can have soft margin classifiers. Then, now guys, this example, I hope you got it. I hope I want to make sure that since you're taking your workshop at support vectors, you at least understand support vectors, what they are. Those are those key lighthouses that help you navigate. The Boatsman, who is the decision boundary navigate. The path of the boatsman is the decision boundary. It also should give you an idea of why I call this company support vectors. This place support vectors. I hope it serves that purpose in your life for your machine learning purposes. And then we talked about kernels. What are kernels? Kernels are relationship between two points, most often based on distance, some functions of the distance, right? But actually dot products, more formally the dot products. There's a bit of formal mathematics to it, which we will learn about later but the bottom line is if you take a functions of dot a point and it's dot product with the lighthouses it is enough to tell you for example help you classify the point does it belong to one side of the river or the other side of the river you just need to know the lighthouses so So the basic common sense intuition is there. A good boatsman will see that those lighthouses, let's say that the two lighthouses are different colors, right? There are blue lighthouses on the city side and there are red lighthouses on the tree side, agricultural side. You can use your common sense and tell that using that you can tell where you are, isn't it? For example, if you see the blues on the right hand side and the reds on the left, you know that you are literally in the river, right? If you notice that you are seeing both the blues and the reds away from you on one side, but blues are shining brighter than the red, what do you conclude? You're on the city side. If on the other hand, you notice that your reds and blues are all on one side, but the red is shining really bright, but there are dim blues. What is it telling you? You're probably on the agricultural side. It is as simple and common sense as that. The mathematics of this is sort of complicated, and people often get frightened with their mathematics but i hope i've made it very intuitive and simple for you of why lighthouses guide you in deciding where you are in the feature space right so far so good guys right so that is that and what did I do after this. Well, this, this thing is again, taking its time to catch up. Now in support vector classifier you take a budget. In the budget you determine how much time, like, how many mistakes you allow for, that is the c and when you use lighthouses especially the rbf curtain the gaussian light houses one of the first most important thing is how far the beam of light goes or how tight it is the bigger the gamma the more tight the lighthouses and the light it sheds the smaller the gamma the more wide it casts its light around the lighthouse right so what happens is when your decision boundary is very um non-linear you need a tight lighthouse and lots of lighthouses right so suppose you it goes like this right you don't want to listen to this see this lighthouse which will guide you in the wrong direction and not make you go in through the river. So you need lots of lighthouses when the river is really bent, sharply bent. On the other hand, you need fewer lighthouses when the river is more or less straight. Or you want the lighthouses to shine light far. And therein is your bias way and straight off also in the gamma and the budgets that you keep is your bias wave so that is that then we talked about i believe the i think at this point this one node thing was beginning to give up and we went to what did we do from the beginning oh yeah support vector machines we restarted we talked about it we talked about this distance kernels so what support vector machines do or support vectors do is it takes your data projects it into a higher dimensional space where there is a linear decision boundary because there's a linear decision boundary you can find your maximal margin hyperplane so you can flow your river through it and once you can flow your river through it you can come back all right to lower dimension space now you can also use it to do dimensionality reduction one of the interesting things you could do is that you could take the, go to a higher dimension where data linearizes, solve the problem and then project it down to very low dimensions. Now we understood one of the mysteries is what is this whole thing about covariance and what is its relationship to principal component analysis. We found that basically you can wrap the data in an ellipsoid. When you wrap the data in an ellipsoidal wrap, then if you look at the covariance matrix, you will find that the covariance matrix on eigenvalue decomposition is telling you that this is the major and minor axis of the ellipse. We did that in the quiz. I noticed that you all seem to have done, I believe, reasonably well. Do I remember the principal component analysis quiz? Kyle, have we already given the principal component analysis quiz or we just issued it? Just released it. Oh, just released it. So I haven't seen the results. I think I'm saying it from the last batches, people or something. Okay. So here it is. So the basic thing is that it helps you find. Now with principal component analysis, you can use the ELBO method. Proportion of variance explained. And so you can tell where the ELBO is, at what point you have achieved the amount of explanation that you need. So that is the dimensionality reduction technique, principal component analysis. Principal component analysis is an old, old method. It was discovered. It has many fathers, like great methods, many inventors or discoverers who claim to have discovered it. The oldest one that I found surprisingly was actually an Indian and he discovered when India was still a colony of the Britishers. Obviously he wasn't recognized very well, even though his paper was published in international journals. Later on his work was forgotten. His name is Kosambi, the great mathematician Kosambi, the great and forgotten mathematician Kosambi, rather, I would say. He did prolific work, but most people don't know who Kosambi is. We talked about decision trees or classification and regression trees. Their big virtue is they're highly explainable. You can use it to explain things. By the way, Kyle just released the quiz for decision trees also. Guys, please do take the quizzes. I noticed that the attendance to the quizzes has gone down. Do the quizzes during this December break, guys. You will learn a lot. How many of you feel that the quizzes are good? All of you are, right? They are useful. So do please take the quizzes. Kayal, everybody here is saying quizzes are good. So do the quizzes. No. Yeah. So this is, we went through the examples of how you can go on partitioning the feature space into regions of higher and higher purity. we looked at a few purity indexes genie this is the proportion of mistakes then genie then entropy genie and entropy are closely related the older techniques mostly use genie i mostly use entropy coming more from an information theoretic and physicist background. Physicists love entropy. The whole of thermodynamics starts with a statistical mechanic, starts with entropy. One reason much of that language has been imported into machine learning. So then we do that. You do some pruning of the trees because it's a greedy algorithm, you need to reverse it. Then we went into techniques of ensembles, right? We talked about the wisdom of crowds. We had a few stories about the wisdom of crowds, of Navy losing a submarine and so on and so forth. We learned about bagging, which is bootstrap aggregation. So we do that. Then boosting, we learned about boosting, which is like, how do you learn from the mistakes of the prior model and build a model just to pick up the signal left behind, the signal in the residual? Do you remember making plots of the residuals and seeing patterns in them? Boosting literally tries to learn from the patterns in the residuals and seeing patterns in them. Boosting literally tries to learn from the patterns in the residuals. That's the beautiful thing about boosting. Then there is stacking. I talked about stacking, having layers of it. Right. And how we do that. Then implementations of it, random forest. I explain how it does. We all talked about the subspace. One important rule that I said that never use, never let a tree make a decision, a split decision by inspecting more than the square root of p features, right? Randomly pick square root of p features or less. And sometimes m is equal to one is pretty good. One of the dirty little secrets is random forest works often as well as the most sophisticated random forest when you set the number of features at a time to one when making a split in the region randomly pick a feature and find a split in that in that right so that is that and well then course, after bagging, boosting random forest and all of that, we did a extra session, which was on Monte Carlo. And that is the sum total of all that we learned in this workshop. I hope I hope it was a good and worthwhile intellectual journey. I would encourage you to start working on your projects at this moment. Today is all about finishing the projects. Let us have our first presentation at three o'clock. So you have from now till three to work on your projects as a team. And if you need help, you can reach out to me. Of course, I'll be coming to the individual rooms and talking to you guys if you're here, otherwise on Zoom. And from there, let's take it at three o'clock. Have your lunch in between at three o'clock. Let's start our presentations, guys, and let's have some good presentations. By now, you all must have had time to do some very good. Now one request I will have after the presentations, all of your teams submit your Jupyter notebooks. What I will do now that I have time tomorrow or day after, I will sit and give you detailed feedback on your notebooks. That feedback you may find useful. Yes, if you were to ask for books that you should do before you come for the next session, I would suggest forget about the programming books. Programming you pick up as you go along. So there are excellent books on the practical side, but leave that. I would suggest that once you have done ISLR, you should do Bishop's book, Pattern Recognition and Machine Learning. It's an excellent book. Yes, exactly. It's an excellent book. Kevin is there, but I always find that the sheer clarity and elegance of thought that you have in Bishop is unparalleled. It's an excellent book. The other thing I would suggest is go to the foundations, make sure that you calculate your linear algebra, your probability theory, these are fixed. So pick up a book on the mathematics. I always suggest, see, in IIT, we did Irwin Krasick. And obviously, I'm still very fond of it. If you remember, I've taught batches, your batch of math of data science I taught with Irwin Krasick. It's an excellent book, not all chapters are needed, we just need five, six chapters of it. But those chapters are excellent. It's a fat book. Nowadays, a good thing is it's available for only 10 bucks on Amazon. Can you imagine a $200 book being available for just $10? It's an excellent book, again, absolute beauty and clarity of language. Big data. Irwin Krasick, Advanced Engineering Mathematics. But ultimately, you know, it's your cup of tea. One thing I would advise you, here in Fremont, there is a half price book, HPB shop in the Fremont Hub. In the Fremont Hub, there's a half price books store. Go to the math section and you'll be surprised at how very good textbooks are available for $5. You know, $100 textbooks are there for five, all that it is, is the university has gone from the 16th to the 17th edition. So that beautiful, absolutely wonderful 16th edition is now available for like 10 months. This is US. But the point is, you have to know that math knowledge is eternal. Between 16th and 17th version, the only difference would be some pictures would be more colorful, and some new example based on the US news or something like that would have crept in. The foundations never change. The foundations of much of that engineering math was laid at least 200 years ago. So I would say that do that, that would be the best advice I'll give you. All right guys, if there are no other questions, let's break for lunch. I'll see you guys at three sharp three pacific standard time sharp we'll do the recommend this is tangential guys and this is not machine learning but it will give you the foundations that makes machine learning easy one of them is this book topology illustrated right is by peterialiev. It's a beautiful book. A lot of colored pictures. Makes understanding of topology very easy. Now, must tell you that this is pure mathematics. So you may feel that, why am I wasting my time here? Also, this sort of books are projects, you do them on the course of three months, you don't just sit down one weekend and say, okay, why am I not done with it yet? The other book, remember I keep talking about manifolds, the calculus of manifolds. This is it. A visual introduction to differential forms and the calculus of manifolds it's a little bit again on the traditional formal side it's a lovely book lots of good pictures illustrating and if you look at these pictures they look like the sort of pictures you find in machine learning textbooks quite often so i love these books if you can i do start this is a long-term project if you want to become gurus of this field pick this up this is what will help you in the long run this is a no-brainer it's a no-brainer it has to be bought these are very good books all right guys and so with that word I will finally go off to lunch myself and see you guys later. See you at 3.