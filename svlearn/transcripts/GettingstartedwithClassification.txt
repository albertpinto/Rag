 The topic now is classification. Classification is a predictive algorithm in the sense that we talked about inputs go in and a prediction comes out. It is as a predictive algorithm, it is also a supervised learning algorithm. In other words, there are phases. You have to first train the machine, the algorithm, to build a model. Out of that model, then you use it to make predictions or to generalize it to new data and make predictions on new data. So what does classification do? As we discussed a little while ago, classification is used to identify the object or the class of objects that this data belongs to. So for example, if I tell you the height and the weight and the certain features of an animal, so those are the features, those are the inputs. And if you have to predict or identify what animals do these features belong to, that exercise is an exercise in classification. So I'll write it there in a little bit more formal terms now. Let's say that you, the basic premise is you write it in, first let me write it in a little bit more formal way so that it remains. What happens is you have an x vector go in, and what comes out is a y hat, which belongs to to where y hat belongs to some class g such that let's say cat dog etc right some animal class let us say and these features could be this is the feature vector vector so that's putting it a bit more formally and more mathematically in a succinct way. But think of it in a more simple way. If that is not the way you want to think of it, think of the features. Given an animal, given some animal, the first feature could be the height, the second could be the weight, the third could be the age, the fourth could be whether that animal has wings or not, and the fourth could and the next could be whether the animal has a tail or not, and the next could be how many legs does the animal have. And what you have to do is you have to predict why hat, remember predictions wear a hat, why hat, and you have to tell is it a cat, is it a dog, tell is it a cat is it a dog is it a horse right is it a cow what is it so you have to pick from a finite set pick from a finite set g right from a finite set g you have to pick. So that is an exercise in classification. Now, there are many classifiers out there. Lots of them, if you ask how many classifiers there are, there are like huge number, almost countless number of classifiers that are out there. Now that's, then you ask this question, how do we learn about all of them? So the thing is you never learn about all of them, you learn on a need to learn basis, but you need to know once you understand how classifiers behave, it is straightforward to pick up a few and then pick up the rest as the need arises, right? Based on your problem set, based on what you're doing, you will use the appropriate classifier. So when a classifier makes a prediction, so let's play a little game and we say that there is a classifier which is given the data and you have to tell whether it is a cow, let's say it's a cow, it's a duck. Actually, let me just take two, just to keep things simple. So this is the reality. The reality is this. Ground truth is this. But suppose you make a prediction model from model. Suppose you have that. And let us say that you had, let's take some number. You had, for the sake of simplicity, assume that there were 50 cows and 50 ducks, equal number of cows and ducks that you use as for training data or for test data. Let's say that you have trained a model M, classifier model M, classifier, and then you test it out on 50 cows and 50 ducks. So what can happen is, let's say that the prediction is cow, cow hat or the prediction is duck Duck hat, right? So let's say that your model predicts about Let's take an example some random number I'll take 40. 40 cows as cows, but it confuses 10 cows as ducks and as cows, but it confuses 10 cows as ducks. And likewise of the ducks, let's say that it confuses about five of the ducks as cows, and 45 of the ducks it gets right as a duck. So does this matrix make sense, guys, to you? You notice that I broke up the 50 cows between, like this row is what that data instance was predicted by your model to be. So you're saying that you showed it 50 cows and the model got 40 of the cows right, as cows, but it confused 10 of the cows to be ducks. cows but it confused 10 of the cows to be ducks. Likewise you gave it 50 ducks but then it confused five of the cows, five of the ducks to be cows but it got 45 of the ducks to be right. Now is this a good thing? Like where are the mistakes? Let's think about it. Would you say or would you agree that this is a good thing to identify duck as, if your model identifies lots of ducks as ducks, that is a good thing, isn't it, guys? Likewise, if it identifies cows as cows, that is a good thing. On the other hand, this is bad. These are your mistakes. These are your mistakes. A duck being confused to be a cow or a cow being confused to be a duck. That is a bad thing. Those are mistakes. Red ones. Mistakes are the word that we use are errors. These are errors and the green ones are correct answers or correct predictions. Would you agree guys? I need some feedback. I want someone to say that they are understanding this and it's looking familiar, simple. Yeah. Yes sir. It is simple, right? Okay, so in that case, now you ask yourself, what is the definition of a good model? How well is this model performing? In the case of regression, we had this mean squared error. In the case of classification, actually it's mean squared error. In the case of classification, actually, it's simpler. You just say, all right, let's look at how many I got correct. Number of correct over total data. Total data size. So what was the total data size? What's our denominator guys? How many animals do we have? Two. Fifty. No, no, no. How many total number of animals did we try this model on? Hundred. Hundred. There are two classes, but there are 100 animals. And how many of those did we get right here? Okay. Come again? 85. 85. 85, right? 45, 40 plus 45 is equal to 85. And so you say, this is your basic sort of metric of the model. The name for this is accuracy. Accuracy. Are we together? Accuracy is defined. So here the accuracy would be 85%. Now is 85 good or bad? It depends on your situation. One of the questions that keeps coming up is, this accuracy I got, is that accuracy good or bad? And the answer, surprisingly, is that it depends. Is 85 good? 85 bad? Depends on the problem you're trying to solve. It also depends on the data asymmetry. So let us say that I change the problem. I make the problem that of detecting cancer. Now, and by the way, this is based on real data, real things that happened in the late early 90s. So there is a disease and this disease is well studied and one of the early success stories with machine learning or AI. There's a disease called, I mean obviously cancer is a terrible disease. In women, the fear often is of something called breast cancer. It is one thing that all women fear, just as men fear, I suppose, prostate cancer. Sooner or later, a large number of people tend to go through it. Now when you do have lumps or these things in women, you want to hear that it's a benign lump. Benign means it is not cancerous. So in US at least, women all go, usually once they're in their 20s, they go for something called mammograms. They get x-rays and if there is a lump felt or found in the mammogram then generally it means that you need to go through further testing now most women if the lump is found it is found that they the is lump is a lump nothing to be worried about but in some cases those lumps they need further study because they may be cancerous you may actually have breast cancer so when you do that breast cancer studies the history of the field is that they used to do pretty invasive tests in other words what would happen is if they found a lump in the previous century for the longest time they would literally cut open the breast, leaving a big scar, go into that lump area, see what it is, take core biopsies there, and then come out with the tissue, send it to histological exam. And then it turns out that you don't have cancer. Well, the good news is you don't have cancer. The bad news is now you have a scar on your body, like a pretty invasive process. So obviously there is a need, there was a need for less invasive processes. So the search for less invasive processes meant that can you just, instead of opening up a person, just go in with a fine needle, go to the lump and aspirate out some tissue. Aspirate means to suck out a little bit of the tissue. And then you look at the tissue under the microscope and you decide whether the person has a tumor, is it cancerous or benign. cancerous or benign, which is a huge improvement because taking an injection is far less invasive than having a full-blown surgery that opens you up and leaves a scar. So that was the process. The process is called fine needle aspiration. When you do fine needle aspiration, then you look at the tissue under the microscope. The problem that came is that the reliability amongst the diagnostician, the pathologist looking at it, was very poor. The same person, one pathologist would mark it as malignant, another would mark it as benign, and it's a lot of error rate there. One of the things that people asked is, can we do better? Can we, can machine learning create a model that can quickly and with equal reliability or more reliability, I mean reliability equal to some of the more trained pathologists and certainly better than the average pathologists, tell whether the markers indicate a benign tumor or a malignant tumor. And so there was a breakthrough in the early 90s that made it possible. That is actually one of your homeworks. You will do that exercise in this particular workshop. But the reason I brought it up is for a slightly different reason. Look at the situation. When most of you go, let us say that you go for your annual checkup and you're a woman, and if it is a man, I guess you go for a prostate checkup or something like that, or something, some checkup. The probability that you have a tumor or you have cancer is very, very low. A very small proportion of people have that. But if you do have that, it is very, very important to catch the tumor early, to catch the cancer early. Because if you catch the cancer early, I'll just focus on breast cancer for women, so I don't keep repeating myself. It's true for other forms of cancer, for example, for prostate cancer in men and so forth. The point is that if you catch it early, it is almost completely curable. You go through a few unpleasant months of chemotherapy and radiation, but that's about it. Today, we can cure, we can basically bring a person back from breast cancer and put into remission forever. It never comes back in a large number of cases. So catching early is important. How do you catch them early? So let's do an experiment. Suppose you are given a lot of the x-, and you are asked to build a model that tells whether a person has or does not have a tumor. Have it together. Whether they need to even, I mean, whether benign or malignant doesn't matter, whether you need to do fine needle aspiration at all or any invasive procedure at all. Healthy people, young people, for example, they'll have a completely clear x-ray. So you go to that and now let's say that the prevalence of cancer is in one in thousand x-rays. That would certainly be true in the Western Hemisphere because everybody goes through regular checkups, so x-rays and these things are done on an annual basis in the US. It may not be so true in India. I don't know if in Bandra you guys are, in the different cities that you are, how often you go through diagnostics. But in US it's the norm, everybody goes through it at least twice a year. Different kinds of tests doesn't mean that they go through diagnostics but in us is the norm everybody goes through it at least twice a year different kinds of tests doesn't mean that they go through x-rays twice again but that you do go through an x-ray about a once or once a year or something like that if you're a woman so most people will be absolutely healthy so let's say that the incidence is one in thousand i'm just throwing out a number it may be be high, it may be low. I don't know medically where it stands, but it sounds right. So suppose it is one in thousand. So you want a machine learning model that catches it. But suppose your accuracy is 99%. Would you trust that machine? Would you trust an AI algorithm? I'm posing you a question. Whose accuracy is about 99%. Yes. Yeah, it seems as though you would. Anybody else who differs? Actually, no. I'll tell you why. So suppose you are a quack. And for example, in India, it's common that a lot of quacks, they'll put up a board claiming that they have all the medical degrees, especially in small remote towns and villages. They'll claim to be doctors. Ultimately, who is there to check? And they will be dispensing quack medicines. They would not be trained in any of the traditions, either Ayurveda or the modern medicine or anything like that, either alternate medicine or anything like that, either alternate medicine or that, they wouldn't have any proper training. But they're quite literally quacks in the sense that they know nothing, but they claim to be doctors. So you go to such a person and all that he says is you're totally fine, right? Because he doesn't really know how to read an x-ray, so he declares everybody to be fine. What will be the accuracy of prediction of that doctor, of that quack? If the incidence of the tumor is one in 10,000, what will be the accuracy? Can you guess guys? So let's work it out. So one in the error rate, so the person is getting 9,999 out of 10,000 right. The only problem is the person who really has cancer or has a let's say lump that person is also being declared as not having a problem so there is one mistake one mistake in 10k patients right and so 9999 patients he's giving the correct diagnosis. So what is the accuracy? Accuracy, again, is correct over total. What is that coming out to be, guys? This is 99, oh no, 1 in 1,000. So 1 in 1,000. So 999. So yes, so it will be 99.9% accurate. Do you see this guys? But does it mean anything? It doesn't mean much because this model entirely missed the one person whose life is at risk. Do you see that? And by the way, this has been the history of quacks over the centuries in every country. If you look at modern medicine, the modern penicillin was discovered barely 100 years ago, or less than 100 years ago. And pretty much all the more potent drugs came after that. So then there has been some alternate systems of medicine. For example, India has Ayurveda and then there used to be the Yunani system or the Hamdol system and so on and so forth, tradition. But they were far more effective for chronic diseases, you know, body pain and things like that. A lot of chronic diseases they would address. But when it came to infections or things like cancer, etc., We do know for a fact that, as far as I understand, there were no scientific cures or ways of managing them or treating them till about a hundred years ago. Almost everything came after that, after penicillin and the discovery of the modern medicines, the pharmaceuticals. But doctors have existed for thousands of years, isn't it? For you go to 500 BC, 1000 BC, you again hear of doctors in all over Greek literature, ancient Western literature, Eastern literature, everywhere. Every country has had doctors and people. You ask those people, how in the world could they be curing anything at all? When we go back and look at it, we realize that actually most illnesses are self-limiting. They cure themselves. You have an upset stomach, stomach, wait for a few days. The germ will flush itself out of your body, and you'll be fine again. You develop a fever, you will recover. Like, for example, today, I seem to have a little bit of fever and cold, but I do know that in two days or three days or whatever it takes, this thing will run through the system. I don't need to take medicines for it, right? Except maybe if the temperature goes up, I can bring it down or something like that. So most diseases cure themselves, right? So when you go to a physician and the physician gives you, let's say, some colored water, right? All these so-called mixtures, nice looking mixtures that used to be there, gives you one of them and you get cured. What used to happen is, and this is now our understanding, that the patient would now develop an extraordinary trust in the doctor, because it's the doctor who, in a way, cured the patient. And the doctor would develop extraordinary faith in the textbook that taught it that that colored mixture cures people of whatever disease it is, let's say a fever or something like that. For all these centuries, obviously it was very hard to remove the confirmation bias when the doctors read those textbooks that said, use this for this illness and use that for that illness. And when they would do that, sure enough, the patients would become okay. So it would confirm their bias that those medicines work. Today, of course, we know that those medicines are nonsense. They never worked. never worked in the same way you look at any doctor and you will find that there are lots and lots of patients who will swear that this doctor is a magician he is really a great doctor however bad the doctor would be there will always be a group of patients who would absolutely adore the doctor i say this as a person who actually has been very closely associated with the medical profession. It's a fact. Right. So, but what people don't know that sequence is not causation. In other words, just because you fell ill, then you went to the doctor and then you became OK, does not mean that the doctor cured you. You would have been cured anyway in a few days whether or not you had met the doctor. So our mind has the habit of treating sequence as causation, but it isn't. So a very silly example of that would be to say that, see, I came to Silicon Valley in 1999. Let's say, let's take a date, right? I ended up here after my graduate school and right away there was a dot-com boom. So would it not be silly for me to claim that because I came to Silicon Valley, Silicon Valley had a tremendous explosion of prosperity and innovation. That would be obviously completely stupid. In the same way, it would be, but it's hard to see that it is the same reasoning to say that because I went to the doctor and after that I became cured, therefore I can assume that I was cured because of the doctor. So today in medicine, for example, we have the double blind study. When we want to see the effectiveness of a medicine, what we do is we create a study in which we take, let's say, 100 patients. To 50 of them, we would give the real medicine. To 50 of them, we would give them a placebo. Placebos are medicines that look exactly like the real one, but actually is filled with just inert material, maybe chalk internally, if it is a tablet. And the doctors wouldn't know, the doctors wouldn't be told who the real patients are. I mean, who are the guys getting the real medicine? And the patients of course course wouldn't know whether they're getting the real medicine or placebo they'll always be told that they are getting the real medicine right then what you do is the person doing the clinical trial only knows they keep observing how many is there a difference in the rate of recovery between people who took a medicine and the people who did not take a medicine. So you need to demonstrate that the people who took the medicine, more of them recovered compared to the people who got the placebo. And then you can establish that the medicine is actually effective. These are very real things by the way. Even today, we are doing that with the COVID, there's vaccines that we are creating. There is some of that going on. You always do double blind studies now as a gold standard. But anyway, the reason I brought it up in classification is that it sort of creates a backdrop of how you should use to determine what measure to use for the accuracy. You ask yourself, what is the asymmetry in data? Right? So suppose in this particular case, it is the accuracy of a blind, a useless algorithm would be 99.9%. so means your model you have a useful model you have trained a model to do something practical only if it your model model mc the accuracy of your model let me just call it accuracy accuracy of your model exceeds 99.9 percent in this situation would you agree guys yeah you would it has to catch those rare cases or another way to put it is accuracy may not even matter the only thing that matters is did you you may actually be willing to lose some accuracy so suppose i do this out of the thousand people i declare five of them as positive cases positive that is a cancer, potentially cancer patients, potentially cancer cases. So now, but so long as this five includes that one person who genuinely has a lump, would you take, now look at the accuracy of this algorithm. It will actually be lower than the fraud or the quack one. This accuracy is only 995, isn't it? Or why go at five? Let's take 10. 90 cases you're 990 cases you're right actually 991 cases you'll be right and nine cases you'll be wrong because out of this 10 one is correct nine are wrong right and so what happens is your accuracy is much lower accuracy is 99.1 much lower than 99.9 but would you consider this model let me call it m2 would you trust m2 or would you trust m1 the previous model which one you come You come again? M2. 99.9. Some of you are arguing for the previous model, 99.9 model, and some are arguing for the M2, which has lower accuracy. So those of you who are arguing for the previous model, give a reasoning. Why would you take the previous model? Do you have an explanation? Why would you take the previous model do you have an explanation why would you take the previous model because the error is less in the previous model they compared to the second one sir yes that is one explanation anyone else has a different opinion or has a different reason anyone who prefers m2 yeah so m2 would be better than m1 i think and why would it be better uh why because it's actually uh the percentage was low but we have 10 cases actually positive so it's better than that. That includes the actual cases. So think of it guys from the perspective of you being the person. If you have a tumor, would you go to equipment that catches it or would you go to the equipment that doesn't catch it? You would want to be tested by that equipment that catches it. Now, it may have false positives. Nine people are wrongly diagnosed to have cancer, but what will happen? Suppose nine people are scared now and they're told you have cancer. The way in medical you would do it is you would put them through further tests. Now suddenly there's a chance that all those 10 people have cancer. You would do blood tests and you would do secondary tests. And now you would do fine needle aspiration or you would do a core biopsy, go and take some tissue out and check whether they have it. At the end of it, what will happen? Nine of the people you would have scared unnecessarily in the beginning, but then you would say, no, everything is fine, go home. It is not as bad as missing the person who genuinely has tumor. Because when you miss a person who has tumor, in a few months, the tumor spreads and then it is too late to save the person. Isn't it? And that is why the previous model is not as useful as the second model, even though the accuracy of the second model is actually lower. And so this is how you have to think about classifiers, guys. You have to ask yourself, what is the purpose I'm using it for? And so what matters more? What matters more is I don't miss the positive case. And to not miss the positive case, if you have to sacrifice a bit of accuracy, no problem. And therefore the point that I'm making is, it is not just accuracy that is a measure of how good a classifier is. There are other measures and I will talk about two of these precision, recall, there are many of them. So accuracy is just one of them. Precision recall, sensitivity, specificity, and we will do the definitions of this next time. But I'm just planting the idea in your mind that what matters is that there are many different measures of the goodness of a model and the content the situation decides which measure that we do so when you don't want to miss the positive test cases you want to go for high recall high recall for early early cancer diagnosis cancer diagnostic tests and what happens is that while the first test is good it will produce a lot of false positives what you want to make sure is that before you give chemotherapy you have done a lot of other tests now the test that you do before you are sure that a person has uh before you start chemotherapy should be it should be very sure that the person really has cancer so the thing that you are being uh that you focus on is that test should have very low false positives. So the final test that should come after, you know that by the time you reach the final test, if you have even the slightest inkling or possibility of cancer, you have been marked as positive. So the job of the final test is to tell that you don't have cancer if you don't have it right so one second i have a doubt here so yes sir in case of m1 you told m2 is better than m1 but here in both the cases actually it is equipment which is showing it so whereas in second cases the ninth person is detected positive whereas the one is correct and the line is wrong but in this case also the ninth person is detected positive whereas the one is correct and the line is wrong but in this case also the ninth person will be given the treatment of same cancer thing which is not required no no it is not that's the point i'm making see uh so maybe i should dwell on it more see what happens in diagnostics is there are different tests the regular tests that you do the annual checkups etc they do they are on the side of or they are biased towards detecting or marking you as positive if there is the slightest chance that something is wrong right so if there is a slightest chance in your x-ray that there may be a lump it will will mark you as positive. But then when you are marked as positive, you don't immediately go in for chemo or something like that. What happens is then the doctor starts doing more aggressive tests. So for example, they will do fine needle. In India, you do fine needle aspiration. That is pretty much the gold standard there. Most places will do that they will go in with a needle they'll aspirate out a tissue and by the way fine needle aspiration is true for all forms of cancer in India right whether it is breast cancer or any other form of cancer they go there then they aspirate out a bit of tissue and they look under the microscope now and they check whether you really have it right and then they then they will do other tests, automated tests also. In the West, at least they do that. And then obviously, if they are really, really sure, before they put you through chemo, they will go and actually take a more invasive approach, go and do a core biopsy. In other words, make a small cut, go and do a core biopsy. In other words, make a small cut, go in endoscopically and take a core biopsy of the region, right, of the tissue, come out, put it under the microscope and so on and so forth to be absolutely sure. They won't put you through chemo just like that. So what happens is, and that's what we are, that's the narrative I'm saying, the early test, you prefer M2. You want it to have, you don't want it to miss the positive case, even though it produces a lot of false positives, right? So these are what, there's a word for it, it is false positive. These are false positive, but you allow that because you know that you have other safeguards other gates that will eliminate those later on whereas if you have a final test in the final test what do you want to do you don't want to this is the door before the chemo right in this test you want to be absolutely sure that this person truly has cancer before you um before you that this person truly has cancer before you bombard this person with chemo. So then you need to have very high specificity. In other words, it needs to say positive only for the people who are truly positive. Are we together? It may have false. At this moment moment you don't fear false negative because the early test would have caught up. So here false negative is okay because you know that some other test has already taken care of that. But here what you are making sure is that there are no false positives. You want to make sure no false, low false positive. So let me summarize it a little bit for you. See in the early test, the first test, suppose there's a machine learning model there. What you want to do is you want to have low false negative. False negative, what is the definition of false negative? Means you do have cancer, but this test missed it. That is very dangerous because you go home, live your life happily, and a few months later, you get the shock that it has spread all over. So the early test must have low false negatives. So generally, these tests which are given on a routine basis, that's what you strive for. They should be relatively non-invasive, not lead to scars and so forth. For example, in US, mammogram is the standard. You go through a mammogram, you look at the x-rays and so forth. But from a machine learning perspective, its requirement is you need low false negative. The slightest hint that something is off, it should just mark you positive. Then you will go through many, many tests. And then let's say that there's a final test and let us say that all of these other tests are still saying you may have cancer you want a final test because beyond the final test is chemo chemo radiation etc and you know that these are very brutal treatments they they they do remove the cancer, but in the process they practically kill the patient. Dr. G R Narsimha Rao, Ph.D.: Right and do a lot of damage to the patient, so you don't want to send somebody for chemo who is actually healthy, so what you do is here your requirements are different low. Dr. G R Narsimha Rao, Ph.D.: Falls. Dr. G R false positive isn't it at this moment what you focus on is low false positive right you don't worry about false negatives because this has the early test would have if this if this comes after early test you know that you don't have to worry about that are we we understanding that now, guys? Yes, sir. Then it cannot be called as a model accuracy in that case, sir, because we are not getting a correct output. Yes, and that is the lesson I want to bring out, that see, people in this field and in the common world have overstated the value of accuracy. If you look at this carefully, in both the early test and the final test, accuracy is a meaningless metric. The real metrics are low. The early test should have very low false negative rate and the final test should have very low false positive rate. The reason I'm mentioning it to you is as you read your textbooks and other things, some textbooks are good, most are pretty mediocre actually. So in most fields, for example in mathematics when a textbook is written, at least in the West, they are written to a very high standard. You don't find mistakes in them. In this fast emerging field of data science, because it's moving so fast, actually, the quality of the books that you buy, these programming books, this data science book, is pretty terrible, actually. So it seems that anybody who has understood even the basics of data science immediately feels the need to write a book because then you know good things will follow and sometimes the books are good sometimes they are not and when they are not good they do a lot of damage because you know now a person who understood things badly has perpetuated that now hundred people have read that book a thousand people have read that book and now they will also believe that to be true and continue with that kind of thinking one of the common things that you find sometimes is um when i hear this the reason is you hi you know i work uh in the company you know you interview people you get people so you ask them these questions and quite often you see that almost all the young kids who come to interview they have that misconception that accuracy is what you go after right you ask them to write code and immediately they'll try to make it more accurate without thinking whether accuracy is the right thing to pursue so keep that in mind guys accuracy sometimes makes sense but many times it doesn't make sense it is in advertising too many many things will say my accuracy is this percentage or that percentage but see the quack can go and say that he has a 99.9.D.: diagnosing cancer because he's telling everybody you're healthy, but that quack is a quack and 99.9% accuracy means nothing. Actually, it is far more useless than a test and early test, which has a much lesser which has a lesser accuracy, but has a low false negative rate. but has a low false negative rate. That's the lesson that I wanted to bring about. So all of these things can be derived from the confusion matrix. This particular matrix that you saw, it has a name to it. This is called, it has a famous name, it is the confusion matrix. It has a famous name. It is the confusion matrix. Matrix, matrix, confusion matrix. So it is like how confused your model gets. So if your model is perfect, what would be the sign of a perfect model. So for example, cow, duck, cow hat, duck hat. You would say that what you would have is a perfect model would be 50, 50, 0, 0. Do you see that guys? No mistakes. Principal axis would contain, the principal diagonal would contain all the data points and the off diagonals will be empty. Means all the cows were correctly identified as cows, all the ducks were correctly identified as ducks. Now what happens in general is you don't achieve 100% accuracy unless you have a very clean data and a very simple explanation for it. If the explanation is very, for example, if I were to ask you, can you tell the difference between a cow and a duck? We as human beings would have a near perfect accuracy or near perfect results, no errors, especially if the cows and ducks are just roaming around us. Isn't it? It would be very easy and you would get, but if you give it to an algorithm, algorithms also can achieve that level of accuracy, but quite often data has noises, models are imperfect. So you will get a certain error rate. And so based on the confusion matrix, the things that you want to do is look at all of these measures and we'll talk about these measures in the mathematical expressions the next time. But today I wanted to give you guys a big picture of what it is like and how you judge a model. And I haven't talked about any classifier. Actually, next time also i'll make it a theory session i noticed that we are beginning to run out of time so the real we'll talk about logistic regression we'll talk about a few more regression algorithms decision trees and random forests and so forth so there are many algorithms that you use and these are the great classics of data science and machine learning. They're used on a day-to-day basis very, very frequently and you don't even know that these things are being used all the time but they are used. But today I'm focusing just on the irrespective of which classifier model it is, how do you do the diagnostics of this model? How do you know that a model is good so there is one more thing that is important it is called the receiver operator operator characteristic actually let me write it in caps so that i don't make typos Actually, let me write it in caps so that I don't make typos. It is called ROC receiver operator characteristic. Characteristic. And usually it is called the ROC curve so characteristic curve well that sounds scary term very very technical term if there are scary and technical terms so this receiver operator characteristic curve the the word is historic. It comes from signal processing. It has been borrowed from signal processing. That is why that strange name. Most people don't call it by that strange name. They just use the word ROC. Now, what is this ROC curve? This ROC curve is a property of a model. Is a property. of a model and how it behaves on test data or real data. What happens is that there are two axes here. One is false positive and a true positive. And both of these go from zero to one. So, zero, zero, zero, one, one, 1, and 1, 0, of course. So what it means is, this point, you have no, neither false positive, nor true positive, right? The model is such that it's basically everything is negative, let's say. So on the other hand, this point, so let me call it A, B, B point represents a situation in which you have no true positive. Everything is just false positive. That's a pretty bad model. This is C represents a model in which everything is true positive and your false positive rate of the model is zero. That is a pretty nice model you would imagine. You want a model that catches cancer when there is cancer but doesn't produce any false positive. In other words, it doesn't alarm anybody who doesn't have cancer by telling them or scaring them that they may have cancer and then this is the other thing in which the true positive and the false positive rates are the same sort of very high true positive and very high false positive also right so now what happens is this line as you can imagine this is actually a square even though I made it out as a rectangle. This is a 45 degree axis. What happens is that useless models, people that make random predictions, there's that. Now what happens is as you change the threshold of accuracy, let us say that, let's take an example. Let us say that the model looks at the color of a solution. You do some test, chemical test, and then light passes through it and then some color comes out. And the color could be anywhere in the degree of redness, right? So how red it is, zero red or fully red. And so somewhere there is a cutoff. I'm deliberately using these words. I'll use more precise words next time, but I'm giving a hand-waving argument here. There's some point of redness at which you say, consider it positive, and below that, consider it negative. let's say that if the solution is sufficiently red you suspect cancer if the solution color is not red enough very likely red or some other color you consider it not cancer so based on this you will realize that based on where you put the cut off your your positive true positive and true negative rates will be different. And it will, in fact, have a curve. A good one will go like this. In other words, with a very low false positive rate, you have achieved a very high true positive rate. A very high true positive rate. Are we together? Think about it this way. If your machine is good and very accurate, you know the right cutoff such that at that cutoff you may occasionally have a few false positives, but you get a lot of true positives. You know, people who are having cancer you catch them all this is the proportion of people let's say that 99 percent of the people who have cancer you catch them even though cancer is relatively rare in people you would want algorithm that is like that so this one right let me mark it as green this is good This is good. And what is bad? Models that are not good will have something like this result. In other words, true positive and false positive are equally balanced. Now, there are models that actually can be like this, even be worse than random. And it happens sometimes. So watch out for those. The point to remember is that in the ROC curve, this curve, these are the ROC curves. All of these are ROC curves. Now, this is a little bit too much to learn in one day in diagnostics. So we'll repeat this concept over and over again a few times. Now, you say that the green is good and the red is not so good. How would you quantify it? One easy way to quantify it is how much area there is below the curve. So you would agree that there is more area below the green curve than there is below the red curve. Now, the total area of this box is one because the height is one and the width is one so total area is one so you know that the the area under the curve will be somewhere between zero and one somewhere in this interval and the bigger it So there is a word for this. It is called the AU area under ROC curve. And the symbol for that in classification theory, classifier theory, is AUROC. The way you write it is capital OA, little u, RO ROC all in capitals. Area under ROC curve. That is the term under ROC curve and usually that is a good measure of how well your algorithm is doing. Remember it's only one of the metrics. It is yet another metric but a very good one one of the metrics. It is yet another metric but a very good one to determine how good your algorithm is and we will use that with time. We will use that. So next time we have a choice, we can either do a lab or I could explain to you the classifier theory. I'll see. Or maybe we'll do the lab so all of these ideas become real. We will do a lab without me giving you an idea of how those algorithms work, but you will sort of do the lab and see them work. And then the week after that, we will do the theory in which we will learn about, I'll teach you about some, at least one classifier in depth, logistic regression. Now, the confusing thing is, do you notice that the word regression is sitting in there? It is actually not regression at all, despite the name, but classification. How confusing can it be? But for historic reasons it is called logistic regression. So logistic regression is actually a classification algorithm. It's a very popular algorithm and very widely used. And it's a great thing to start using in practical day-to-day life. So we seem to have run out of time. Next time when we meet, let us do a few labs. We'll classify some data. And we will actually use breast cancer data also as an example. And we'll use that to illustrate how these things work. But without understanding logistic regression, just understand what classifiers do, what their metrics are and how to use them. We will do that. And then subsequently in the week after that two weeks from now i will cover the theory of logistic regression is that a plan guys yeah let's do that all right guys and so i'm stopping the recording so you guys feel free to ask questions Thank you.