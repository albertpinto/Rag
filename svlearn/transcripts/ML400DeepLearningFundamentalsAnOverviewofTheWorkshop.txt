 All right guys, see we are launching into a journey. This journey is about artificial intelligence. Deep learning has become synonymous with artificial intelligence. So I'll start by asking a question. What is artificial intelligence? Or moving back from that, what is intelligence? How do you say that something is intelligent or not intelligent? If I were to ask you, is a stone intelligent is your table intelligent you would probably shake your head and say no you don't associate intelligence with a stone you don't associate intelligence with the table now if i ask you about a child is a child intelligent you would say yes the child is intelligent even though the child may be too young to answer any of your questions. Yet you ascribe intelligence to the child. And so this begs the question, what is the quantitative definition of intelligence? Intelligence or creating intelligent machines has been a dream of mankind. We see intelligence all around us in living creatures, all creatures, all sorts of animals and plants and plants and even now but two trees we now ascribe intelligence so what is intelligence would anybody like to take a stab at it copying and modifying. You mean control C, control V and then change the variables? No, for a child, the child copies the action and then modify it accordingly. No, I was just making a joke about programmers, but you're absolutely right. A child learns a behavior, observes a behavior, repeats it, but after that adapts and improves upon it. behavior, observes a behavior, repeats it, but after that adapts and improves upon it. And so that adaptation and improvement is a mark of intelligence. Would anybody like to give a different example? Intelligence is being able to adapt your behavior based on changing circumstances. For instance, game strategy, like playing chess. Adaptability. Yeah, that's a great point. So guys, I'll mention it this way. When you have, when we talk of intelligence intelligence we look at people and people who have very good memories we often say what a bright person because they can answer all sorts of questions so from that perspective a giant database of knowledge or just a huge let's say database that you have these days let's say an database that you can query using sequel you can ask this question is it intelligent and typically you would say well no it's terribly knowledgeable it's a storehouse of memory of knowledge of facts of data but you don't ascribe intelligence to that so then you ask your question so what is intelligence one way you could say is that intelligence is a property of thinking beings but then that raises the question what is thinking people have tried to in the initial versions they thought intelligence is thinking and they tried to create artificial intelligence as thinking machines if we go back and look at the works of Alan Turing the founder of computer science arguably and I would also argue the founder of artificial intelligence when he created that great machine his dream was to create artificial intelligence and he said that in a few years he looks forward to in 50 years he felt that by 2000 mankind would have created thinking machines underlying that was the notion that artificial intelligence or intelligence is about thinking and it is indeed about thinking except that there is a problem with that the problem is none of us know what thinking is thinking is something that's very obvious we all think we can see the other person think and yet we can't put a finger to the thinking process we can't see the thought rise we can't see the mechanics of thought and to the extent that we are scientists and engineers we deal with observables we deal with measurable quantities and as observables and as measurable quantities thinking so far has eluded us it is not something we are able to quantify so it wouldn't do to call it thinking machines and by the way some of you may remember in Silicon Valley long ago there was a huge company called quite literally thinking machines it got it great Nobel laureates worked in that it got broken up Richard P Feynman worked in that it got broken up into parts later on it didn't succeed and then because it was trying to do thinking machines then parts of it went into oracles the data the data mining group of oracle actually was derived from the part the ai group from the thinking machines it's all ancient history now and the hardware part went to sun and well now sun itself is part of oracle So there's a lot of history about it. But anyway, thinking is not something we can quantify We can't write equations about thinking today as measurables as observables So, how do we define intelligence? The way we do that is i'll give you an analogy. I'll give you a story that should illustrate the point. Let us say that you take your child to a meadow. And we'll think of a very simplified meadow in which there are cows and ducks. So you show your child a cow and being a responsible parent, enthusiastically, you explain that look at the webbed feet and look at the feathers and look at the beak of the little duck and the ducks are small. And then after a little while you encounter a cow and you say, well look at this big cow with its swishy tail and horns on its head. And's a cow and the child let's imagine that the child is two year old child after a little while the child will say all right i'm beginning to get it i know what a duck is and what a cow is how would you know that the child has learned one thing you could do is you could go back and a pointer to the animal the duck that you had shown go back in a pointer to the animal the duck that you had shown as duck and ask it to tell whether it's a duck or a cow well the child may say duck but actually and then it looks at the cow that it has seen before and it may say well this is a cow does that mean that your child has has learnt about cows and ducks you would say perhaps but there's a flaw in the argument some people some human beings they have perfect recall if they see something this sort of like a photographic memory they will remember everything you tell them and so you don't know whether your child has just memorized that duck or that cow and is doing a recall from that memory and saying this is a duck or a cow so the question therefore comes how would you know that your child has learned the way you would know is this you would would encounter a new duck, which every duck is different and every cow is different from other cow. And so this duck or this cow would be somewhat different from the previous ones that you introduced the child to. And then you show this cow and you ask, what is this? Which animal is this? And if the child says cow it confirms perhaps and you show the child more instances of cows and the child says cow and more instances of duck and the child says duck alternatively and you of course randomize and show them show the child but then you begin to develop confidence that your child has learned so inherent to the concept of intelligence is the ability to generalize from instances of facts instances of data you can generalize you can abstract if may use sort of a cooked up word, the child pretty much grasps the essence of a cow, the cowness of a cow. And then the essence of a cow has these properties to it, big, swishy animal with four legs and swishy tail and horn and so forth. And the essence of a duck, the duckiness of it, captured in the feathers, the webbed feet, and the beak and so forth and with that sense can look at a new animal and identify that yes it has the essence of a duck and so it must be a duck that is intelligence that is learning but there but that learning comes but there but that learning comes progressively that ability to generalize which is a mark of learning comes not at once but it comes slowly in the beginning your child will be making mistakes let us say that you have been enthusiastically as all parents do explaining a lot of properties of cows and ducks and your two-year-old child is over vaned so what will happen is your child will initially just be guessing you show it an animal and it will guess maybe come maybe duck and when you guess there is a certain error rate you show a hundred examples it will get some proportion of them just by guessing right but as the child begins to grasp and you correct the child you say no no that, no, that wasn't a cow, it was a duck. See, it has feathers and so forth. So as you explain to your child the qualities of that animal, gradually the conceptualization forms in the mind. And the ability to generalize emerge. But how do you know that the child can generalize? Because now you show animals and the mistakes to generalize emerge but how do you know that the child can generalize because now you show animals and the mistakes become fewer the child will still make mistakes if you look at an animal a cow from really really far off it might look rather like a duck to the child and so and so forth so there may still be mistakes but they would be far few and far between so the quality of learning is that you start with the initial error rate but you can decrease the error as you learn from the data you can decrease the error and that is a quantitative definition of learning today when we say intelligence. Today when we say intelligence, we mean, when we say artificial intelligence, we mean not thinking machines, which we have absolutely not the foggiest idea how we will build. We don't know how we think, far from being able to quantify and build thinking machines. So our goal in artificial intelligence is actually far more modest. We want to create machines that can learn, that can generalize. Now learning quantitatively defined as equations, as something very concrete, learning is a reduction in error rate. Looking at the data and as you learn from the data, the mistakes that you make in your prediction in your identification between cows and ducks, it goes down. Are we together? That is learning and that is intelligence. So this course is about obviously artificial intelligence. Artificial intelligence started out as a vast field in the 50s. Minsky and these people they had vast ambitions about artificial intelligence. It has had its ups and downs. There was a lot of hype and the hype was not delivered upon so there was an AI winter for the longest time. People abandoned or stopped taking AI seriously it was left to the academic research and the business will thought it it's not really material then it came back again and then it again went into sort of a winter so it seems to come up emerge into in generations one generation will AI come it will deliver on some things there would be a massive hype there would be Hollywood movies made about how AI is going to take over the world every every few years we have you have a cycle of that hype and then it will fail to deliver on that hype I believe in 2020 you already see a lot of articles emerging these days that says that AI is not delivering on its promise. Quite a few such stories emerge. On the other hand, there are also stories that is absolutely amazed or that is busy hyping up the power of artificial intelligence. There are people making movies about AI taking over the world. So be aware of that. When we talk of AI, it is something specific. It is machine learning. It's the learning in machines, the core of artificial intelligence is the theoretical core is machine learning. It has other aspects, robotics, the application of machine learning becomes robotics. So for example, today, we are all waiting with bated breath for the self driven car. Tesla seems to be making a lot of strides. Google is trying to have a self driven car. At this moment, it's still a dream. We are far off from complete autumn automation but there are enough exciting progress that we are all captivated by it we certainly have a computer assisted driving most of us would not be able to drive a car of the 1970s it is something that may surprise you most modern cars are embedded with a lot of microprocessors, attraction control, this and that, radar systems that warn you of your blind spots, and adaptive cruise control that keeps watching the road and so on and so forth. And augmented vision through when you're driving at night, some of you have cars that will project and show you if there's a deer or a person on the road that the human eye can't see so all of these have become realities and we have all gotten used to that and so we are in machines which are partly driving themselves but we still feel as though we are driving so it is all around us but we still haven't reached perfect self-driven cars that's still a dream that is robotics many of you have robotic vacuum cleaners in your homes the room bars and so forth and robotics is everywhere if you go to a tesla car manufacturing plant here in fre, you would realize that pretty much the entire manufacturing is done by robotics. And that's that world we are in that is applied artificial intelligence. The core of artificial intelligence is the theory is machine learning. Now, machine learning has a whole lot of algorithms. And there are, for example, starting with the simple linear regression, logistic regression, etc. And those of you who have taken previous workshops with me have gone through all of that. I won't repeat that. Support vector machines and random forest and so forth. And then there are neural networks. Neural networks are having a resurgence. They have been sort of coming and going, coming upon the landscape, being successful, doing something practical, then fading out into sort of a winter, dark winter, and coming back again. But we have these days a massive resurgence of neural networks. And that is the topic of this workshop, deep neural networks. When we talk of deep learning, deep learning is a lovely term it seems i don't know who coined the term but certainly great marketing there is nothing deep about deep learning the deep part refers to the neural network how many layers you have in the neural network now these things are something we'll talk about when we get into the technical aspect from the next lecture next week onwards this week is fun and games we will just take a tour of the field of artificial intelligence but eventually you'll see what the deep refers to it refers to something technical so deep learning is not some esoteric learning but it's just an architecture that has been given this name deep neural networks so in this has been given this name, deep neural networks. So in this workshop, we'll deal completely with deep neural networks. We have done other algorithms in previous workshops. Now some of you have come to this workshop directly. You may not have background with previous algorithms. You don't have to worry, we won't need those algorithms, but we would need some absolute basics of machine learning. So if you take the quiz week zero quiz and you score below five, then you should come to Basic remediation class in which I introduce the core ideas of machine learning. I will be having it in a couple of over the weekend or at some point I'll have an hour session on that so do please attend it so today we are in an interesting situation when people mean AI they don't mean the academicians mean the whole vast field of AI of which machine learning is the theory but in the business world when people say AI or artificial intelligence more and more they are referring only to deep neural networks and so that will be the context of this particular workshop now AI has made tremendous progress I started out by saying artificial intelligence is not about thinking machines and that stands on its face not only do we people keep talking that oh ai true ai may be here at some point they talk about the the singularity the moment of singularity at which the computers will become self-aware and they'll become sentient beings and they'll take over and so on and so forth there's a lot of hype out there it is all fun and games it is it makes for great Hollywood stuff and it makes for great stories to tell children but when you look at the field actually we have not even started we don't not only are we is not it not only is thinking machines far away we have not even the foggiest idea how to go about tackling in equations and in quantitative form the thinking process itself until we can do that i suppose we can all sleep well in the confidence that these machines are not going to become sentient any day soon. Having said that, these machines are learning machines. And at learning, they do learning specific tasks, not everything. Not like human beings that can learn all sorts of things. But for learning very specialized things. They're extraordinarily good. They're far better than us. You give it one particular task, computers calculate better than us. They make predictions better than us. They will beat Kasparov at chess. They will beat the champions of any game at the game, the goal and whatnot. champions of any game at the game the go and whatnot so it has become a fact that things that are learned computers the artificial intelligence will beat us at it it's a given because they are extremely good at learning so that's the distinguishing feature now when they learn and they and they also have a capacity of remembering they can remember vast quantities of data all of you have seen the magic of googling today you can ask Google to answer any question you want you can go Wikipedia is a giant repository of knowledge and so when you google for answers it looks almost as though there is a wizard behind Google that's answering your questions these machines have become very good at responding to your questions so for example the chat Barts the Alex's and so forth I have become so good that there are some amusing statistics at one point I believe it was about Alex's or was it about the Microsoft or was it about the Google thing there was a statistic that every day these chatbots these machines they receive close to 100 genuine marriage proposals now you would say what how could that be everybody knows that these are machines but it isn't there are a lot of people who are stuck to these machines they pour out their heart to these machines and when these machines respond when these artificial intelligences respond people have felt intelligences respond people have felt that they respond so well that they come to the conclusion that it's truly really wise and deeply compassionate and a large-hearted and they convince themselves that a machine cannot do it behind these machines there must be a real human being that the company is not telling you about and they are having a conversation with a real human being and so they end up very sincerely proposing to these machines in the hope that they're proposing actually to the human being this behind the machine that I suppose is a testament to how realistic these machines can be when they talk to us. Today, and these are getting better and better. Just last month we had the one particular neural architecture, a ginormous neural neural architecture called GPT-3 released by OpenAI. That is so humongous that you can't even download that ginormous model. So it has been exposed as an API, a rather pricey one at that, so people aren't happy about that. But if you talk to it, if you try to use the API and casual usage is free you would be surprised at what amazing things it can do for example this may completely surprise you you can give it drawing so you can give it your intention right in words I want to use a interface which will which has a form to take the users this or that where user can interact this in that way and to your surprise it will write user interface code html and javascript and it will write perfect code for you and generate the user interface and just think about it for a moment what a tremendous achievement it is we always thought that coding is a deeply intellectual activity we all preen ourselves to be rather bright because we can code and now here is a machine that can code and it can even code without bugs whereas we seem to produce a bug every 10 lines of code you write till till we debug it through inspection in QA. So this is the world. It is able to do computations, arithmetic. And so it is showing resemblance to intelligence. And yet, it is at heart a machine that has learned a lot of data. And it has learned to generalize from the data. So it has learned. generalize from the data. So it has learned that machine cannot think. It doesn't understand what it reads, but it certainly can learn to generalize from that. And when it can learn to generalize, people have been asking the philosophical question, at what point do we say that it does not have intelligence in the true sense because it is a part of intelligence memory is a part of intelligence we don't we consider up child dull or a person dull if the person cannot remember things as a bit of autobiographical statement I am terrible at remembering things. I manage to lose my key every day and things every day and my family whether it was my parents when I was growing up or my friends who were with me in graduate school, one of them Chenda is here actually in this audience and or my family, my own wife and children, they are forever frustrated that i can't remember where things are if it is not the keys i'll forget the cell phone and i'll be searching for that well thankfully my daughter the first gift she gave me with her earnings internship earnings was title bless her heart and so it's just something that I need to remember if I if I miss the cell phone I can use the keys to find it and vice versa and it has been quite a helpful thing so this is it knowledge not to have memory shows a certain deficiency of intelligence. Likewise, not to be able to learn from data. You know, when we teach a child and repeatedly, no matter how many ways we explain it to a child, and the child doesn't get it. You may take the child to the meadow every single day, but the child still cannot remember, cannot understand what's the difference between a cow and a duck. You would say, well, the child is dull. It is, you would say well the child is dull it is you would say it's lacking in intelligence because learnability the ability to generalize from facts right from instances of facts is a hallmark of intelligence and yet this intelligence is not thinking right in the case of sentient beings yes it is thinking but in the case of sentient beings, yes, it is thinking. But in the case of machines, it is not thinking. But then, well, of course, we don't know what thinking is. And so that's where we stand. But they are extremely clever, these machines, at convincing you that they are thinking. So something to bear in mind. Somebody said that with this artificial intelligence, these machines will conquer the world. And was rather worried until you observe actually that these machines need not be intelligent they have already conquered the world these things drive your car with this microprocessors and these adaptive systems they help you in every aspect of life. They do surgery for you. Robotic surgery is guided through these machines. And a lot of things are done. Bariatric surgery, the weight reduction surgery is done through robotics. And many, many surgeries are done through robotics. In the audience, we have at least three medicals, two doctors. They will confirm this. So that's the amazing thing about these machines. The history of AI is pretty long and deep. You would say that the creator, computer science began with Alan Turing when he was trying to solve the problem of the Enigma during the Second World War, break those code machines, those cryptographic machines the Nazis were using to send messages. And it was the breaking of the code, the Enigma code, that made the biggest difference to winning the war for the allies, allied forces. That's the history of it. So with that came computers with a bang, but Alan Turing wasn't just trying to make a fancy calculator he was trying to create artificial intelligence but the history goes even further back you look at any culture you look at the Greeks and they are dreaming of creating things that can think you know modeling or making statues or things that can think you know modeling or making statues or things that can think you look at Indian the Indian culture the cultures of the East you just look around anywhere at children they'll be making little dolls of clay and they will pretend every single girl you can think of has a doll and makes the doll talk and speak and so forth. In the Jewish mythology, there is the concept that you can make out of clay a person, and I believe the word is golem, and you can breathe life into it and the golem can do whatever you want to. In the Middle Eastern mythology, you have Aladdin and his magic lamp, and out of the magic lamp comes the jinn, and so forth. So this dream has been a millennial dream. Human beings have been trying to be gods. They have been trying to create intelligent beings of their own. And so it's a very long history to the field of artificial intelligence I'll leave the statement to that on a class webpage you'll find two articles one is by Forbes which is the history of AI I would encourage you to read that as a reading for this week the second is video a YouTube video by Lex Friedman of MIT, who starts out his artificial intelligence course with, again, what amazing things AI can do today. AI can today write, it can read, it can write poetry, it can do arithmetic, of course, which is a no-brainer. poetry it can do arithmetic of course which is a no-brainer it can smell it can walk it can talk it can translate and you begin to wonder that at what point like like to the common man it feels like magic it feels like the story of Aladdin and his magic lamp you rub the magic lamp and out comes the jinn. It can do anything and it can do those things far far better than we can do and we seem to have produced that. We have the story of Frankenstein and we seem to have produced the new Frankenstein except that Frankenstein was rather scary but here we hope there's artificial intelligence that benign. So that is about the history and the amazing things that AI can do. But today, what I would like to talk about is the new developments in AI and how we are going to do this course. As I said, today's lecture is very different from all other sessions that will follow. It gives you the big picture. It takes you to the arc of this field so that we know a general sense of where we are going. So traditionally, when artificial intelligence or a course in deep learning is taught, and there are many, many courses you can find on Coursera and the excellent Andrew Inks deep learning courses there. Then there is Jamie Harvard, who gives another one very practical. He's trying to democratize deep learning by saying, let's get started from day one. And he altogether sidetracks the theory to a large extent. I mean, not completely. He does a good job, but he gets you started first and then brings in just enough theory. And Andrew Ng's approach is first the theory and then the labs. And everybody has a different approach. I will let you judge my approach but if you look at all of them traditionally the way they start out is they'll get you started with some practical things then they will talk about a few architectures they will talk about so-called dense neural networks or feed forward networks these are dense layers packed together they will solve a few problems they will talk about you know the aspects of it like back propagation what is back propagation in that and what is activation function what are optimizers and so forth and then batch normalization and a lot of the nuts and bolts of it then they'll move on to convolation neural network which is for computer vision that is what you need for recognizing cats and dogs in simple terms or cows and ducks in our case you'll actually have a homework a lab work on cows and ducks so you can do that convolation neural networks and then traditionally people move on to recurrent neural networks lstms at this moment these must all be words to you, but we'll come and understand it in due course of time. So let these remain as buzzwords for now. But these are neural architectures. And then the last thing, as time permits, people go into something called the transformers. And transformers were a big breakthrough that happened in 2017 end of 2017 December and became really the big news of 2018 and the world of natural language processing has now was changed forever with the coming of transformers so that's a traditional arc of learning but i won't be i will be doing that but i'll actually do it slightly differently see in machine learning in ai there has always been one nagging problem the machine learns only part of the solution it doesn't learn everything and let me tell you that with an illustration. This illustration, I'll deliberately keep it simple, but those of you who have taken the prior courses, it will resonate with you. So suppose you have a curve, you draw, just draw a curve on the page. X, Y axis and Y as some function of x. Need not be linear, just make some curve. Then as a function, you want the machine to learn that function so that it can generalize and tell you the value y for some arbitrary x. So there are many ways of doing it. One of the ways that you can do it is something called a polynomial regression. You think of it as a polynomial equation in X. Y is equal to beta naught plus beta 1X plus beta 2X squared. So up to X to the N. Now the question is, how many degrees of polynomial do you take? If you take a two lower degree, you under fit the data. The higher the degree, the more flexible it is. So imagine that you are holding onto, just imagine that you have a straight line, a ruler, and the ruler has certain degree of plasticity to it, flexibility to it, right? And the higher the order of polynomial the more flexible your ruler is so the so if you have a curve and when you have the data the trouble is you have to decide before you see the data how many degrees of polynomial you'll take in your model so suppose you take a straight line assumption and in the ground truth, the reality is it has three or four ups and downs in the curve. Then your straight line will suffer from a problem called the problem of bias. High bias means machine learning will just learn the best line approximation to your curve. But the underlying reality remains that line is not the best description or the best fit to that data. And machine learning will not will be silent on that. It says, well, you do want to try a quadratic. Now you can try a quadratic, you can try third order and you have to manually keep trying different orders of the polynomial and it will keep fitting in better but then you might be tempted to say why can't I take a 50 degree polynomial always and fit it to the data or 10 degree polynomial and fit it to the data it turns out that that leads to its own bag of problems and those problems are called problems of overfitting if your straight line is way too flexible then as you bend it begins to develop what are called oscillations and it's a technical term it just becomes begins to look like you know the way the sound waves are you see oscillations it begins to look a little bit like that there are big highs and lows in that curve that your model builds and when it does that it overfits the data it tries to go through every point in your data and it becomes a fairly useless model it has a class of errors a type of errors you say high variance errors those are errors of overfitting so therefore you say that the degree of the polynomial is the word is hyper parameter of the model it's a hyper parameter as a hyper parameter it is in your hands to decide what it should be before you ask the machine to learn you can tell the machine that learn a sixth degree polynomial model of the data and it will learn the best possible six degree polynomial but it will stay with the sixth degree if you say no why can't you just figure out on your own what the degree of the polynomial is, that process is called hyperparameter optimization. There's a word for it. That is a task of itself. So you have to make the machine learn the first degree model, the second degree best model, the third degree best model, and you have to keep going till you come upon a model which has relatively low bias errors and low variance errors. And these two sort of go opposite, not always. Generally, if you increase the complexity, bias errors will go down, variance errors will go up. So people talk about the bias variance trade off. And so you reach a point a valley of the of should you say the happy valley point at which you you have decreased bias as much as you can without letting the variance inflate and the variance is also as low as it can be reasonable so that the total error is low that is the bias we instead of whether we violate is a trade off. Remember that there are situations where you can actually have both. If you have a bad model or you have a peculiar model, you can have high bias and high variance errors, but together It doesn't mean that if you reduce one the other always gets reduced. But as you dial up the complexity one tends to go down and the other goes up. So that is a hyper parameter of the model. But then there are other hyper parameters in the model. So for example, to dampen the oscillation, you do something called regularization. Regularization is a fairly technical thing in machine learning. Just think of it as putting a dampening or as if you are listening to music or putting some earplugs so that you don't get the oscillation so strongly they sort of get muffled in a way that's a very rough analogy but when you build a model you prevent it from oscillating too much uh those polynomials and when you do that the model improves but there too there is a factor how much of the muffling or damp And when you do that, the model improves. But there too, there is a factor. How much of the muffling or dampening will you do? There's a dampening factor. And machine learning cannot learn the dampening factor in its own. So it has traditionally been the case that people would do all sorts of means to find the best values of the hyper parameter here I took two hyper parameters many models have more hyper parameters to deal with we have encountered that some of you who did the previous workshops on different topics not on neural networks not on deep learning you have encountered lots of hyper parameters the trouble with hyper parameters is they exponentially grow so for example how much should the dampening factor be it could be zero zero it could be very high ten thousand and any number in between infinitely many possibilities in between the other axis could is the degree of the polynomial which also is unending now you have huge space to search this is the search space of hyper parameters and then by the time you get to 10 hyper parameters which is very common you have just try to think how many literally hyper parameters you can guess even if you make a grid even if you take some discrete values along each of them if you just take 10 values along each of the hyper parameters to test you have 10 to the power 10 that is 10 what is it 10 trillion times you'll have to train your model to find the best model and this has been a problem in the field people have had some basic techniques like you know if you take up your standard o'reilly or Manning books, quite often you'll hear the word mentioned, a grid search. You just go in a grid, take 10 values and write a loop in which, maybe let's take two hyperparameters. For every 10 values of the first hyperparameter and every 10 values of the second hyperparameter, you build a model. So you end up building 10 square models, 100 models, and you then pick the best of them. And then perhaps you zoom in and you build some more models. The other improvement upon that has been randomized or pure randomized search rather than grid search. Today, we know that grid search was actually a bad idea. We do randomized grid search. And that's the beginning of a far deeper topic today when we talk of hyper parameter optimization this leads to something called automated machine learning we are telling a machine that see not only learn the best model if we tell you what model to start with but good discover the best model Go discover the best hyper parameter combinations that is hyper parameter optimization. So go through the search space, but don't go about it Stupidly when we do this grid kind of search it is not scalable. It doesn't work you need far better techniques to do it and the good news is the recent years have seen a phenomenal break set of breakthroughs in this topic and the broad topic is called automated machine learning a machine learning whose job is to find the best model most people i suppose have not yet as of today I gotten involved or understood what automated machine learning is quite often when you say AutoML to them the short form they usually mean the Google's AutoML service and Google has done a great job capturing the word I suppose but no it is not that automated machine learning is a field of machine learning which focuses on making the machine learn the best model itself hyper parameter optimization is one part of it the second part and part of that is beyond that which classifier should we use there is a theorem in the machine learning. It is called the no free lunch theorem. It says that no one algorithm will always outperform other algorithms ever So don't let anyone tell you that whatever the favorite algorithm is random forest or support victim machines or deep neural networks that it will always do better than other algorithms. That's not the case for every, every data set tells decides or there is one particular algorithm for each data set and it will be different for another data set. That's the no freelance theorem. We talked about in previous workshops or you won't talk about that. Yeah, but it's a profound result actually to remember. So if we you know, we may tune one model one Algorithm to the best possible limit and yet we may be barking of barking up the wrong tree You know, there may be other algorithms that are much better So now you need to expand your search space not just in the hyper parameters but you need to expand your search space to the list of models list of algorithms and so the field becomes vast and beautiful and that is the field of automated machine learning in the space of neural networks by the way we will do a lab in this and when we do the lab next time on wednesday um i hope you will be absolutely amazed that it did a far better job than what you could have done yourself now those of you who have been taking ml 100 and 200 you have your lab notes with you and you must have your hyper parameter optimizations and you will realize that this machine now has automatically come up with an architecture or modeling that embarrassingly beats whatever you did or at least matches it if you have been really putting a lot of effort into it you'll be surprised that these algorithms now match it so that is one in the world of and then associated with it is the concept of meta learning meta learning is not just a machine learning from data but it is learning how to learn from data and that's another big topic and in this workshop we'll cover that because that is where all of these things are heading and their third aspect of automated machine learning is something called a neural architecture search. So today, and what happens is the neural architectures that we come up with, all these classic architectures, it takes an army of researchers and years to painstakingly experiment and build it. Neural architecture is in a strange place. In practice, we know how to make it work by tweaking and tweaking and tweaking and trying out things. We have just enough theory to understand them, but we don't understand them fully. It's very interesting. We don't have a roadmap. We don't have a lamp that guides us, a theoretical framework. We don't have a roadmap. We don't have a lamp that guides us a theoretical framework. We don't really understand why why these deep neural networks. They work so amazingly well. If you ask somebody, he'll say, yeah, the theory deep back propagation and this and that and all that. But actually, we're just scratching the surface. No one but actually we are just scratching the surface no one actually has a theoretical explanation of why these deep neural networks work so absolutely amazingly well and so the way we humble and we sort of stumble in the dark and we try this and hack that and we come up with a new architecture and then some architectures are winners you know they do very well in the competitions they achieve state-of-the-art performance and then we all celebrate but it leaves open the question how did you come up with that architecture is there a systematic way to get to that fortunately a tremendous amount of a progress has happened recently and And that is the field of neural architecture search. Very, very amazing. So it is a two step process. You ask a neural network not to solve a problem, not to come up with the best model to solve a problem at all, which is machine learning, which would be the step, which would be the traditional way you would do it. You would throw a convolation neural network and say, all right, here it is. I'm using VGG or I'm using inception or Alex net or whatever. And I'm going to train it to solve this image classification, the problem instead you do something quite remarkable and absolutely actually when the first time you encounter that it's a moment of epiphany I still remember being thrilled and walking around in a day is the first time I learned that you could actually do this so what is this that I'm talking about it is neural architecture search you ask the neural network a simple question you say design me the best neural network that will solve that will model this data now pay attention you're not asking the deep neural network you're not taking an architecture and saying all right learn and do the best possible learning so that you can do good classification you can tell the duck from the cow instead you are asking this question here is the tremendous amount of data and here is a problem we need to distinguish between cows and ducks come up with a neural architecture that can do it and the amazing thing is we can the machine comes up with or gives birth to a neural architecture which then you use to solve the problem so it is a child neural architecture you create a neural architecture to give birth to a child neural architecture and that child neural architecture then goes and solves your problem in the most optimal manner it learns from the data in the most optimal manner in other words it has the lowest classification error between cows and ducks and this guy's is the reality today today if you solve a problem the right approach is try out the architectures that exist the classical architectures is good if it can solve your problem so be it you don't have to try any further but if you have a hard problem and you're not getting the accuracy the performance that you want then the right thing to do is to use automated machine learning to generate the perfect or the best neural architecture you can to learn from the data. And that today is the state of the art. That's the cutting edge. So we all live in the world of this pandemic, COVID. COVID has made everybody's life difficult. Children are not able to have a college experience, is a tremendous loss a year of college life and college is the fun part of your life one of the most fun parts of life children are not able to go to school we are all working from home and when we work from home you would all testify to the fact that you're working longer hours right picnics are gone and flights are gone and travel is gone so it's quite a nuisance and yet pandemic this virus seems to have conquered so one question that was asked is can we have a good neural architecture that can look at a lung x-ray and tell whether a person has COVID or has some other lung problem, asthma or something, or is completely healthy from a lung perspective. Now, why is that relevant? Today, it is relevant because the state of the art method, which is the RT PCR reverse transcript transcript days. What is that transcript reverse transcript polymerase chain reaction. Did I get that right so well. RT PCR. Okay. It's a technique. It's a it's a technique that you use to test for COVID that's the state of the art. That's what we are doing in US. It is the best we can do, but actually it has a pretty bad record. It will miss 20% of the people. And it will come up with 5% false positives. So 5% of the people who are healthy, it will scare them by saying, well, you have COVID. And it unfortunately, unfortunately this is the fact people miss 20% of the people it misses now the situation may have improved since I last gathered this data but that is pretty bad if you think about it the collective intelligence of humanity has not produced a better test so people ask can we use a neural architecture to do it this was asked actually very early on in December itself and I believe it was some Taiwanese a Korea it was Koreans who first researchers who first did it and now there is an open source global effort with this AI so people used automated machine learning and neural architecture search to come up with a neural architecture we will cover that architecture in this course we'll actually do a lab on that and when you look at that architecture it is so complicated one look at it and you know that no human being could have produced that so it becomes an interplay you use you use automated machine learning to come up with the neural architecture and then you tweak that architecture you learn some more there is a human intervention to make it better but there is also a lot of neural architecture search in that process so that is the cutting edge so that is the first topic that we are going to learn about in this workshop, we will start your first lab with automated machine learning because it's practical. You can take that lab, you can just go to your workplaces, and you can immediately apply it to datasets there. And when you apply it to the datasets there, it is very likely that off the bat, you will start getting state-of-the-art results. I have seen people who learnt it and who immediately went and applied it to their fields in their workplaces and they completely, they literally in a few instances, fortunate instances, they wiped the floor with whatever was considered the state-of-the-art in their field. So the results can be quite remarkable. wipe the floor with whatever was considered the state of the art in the field. So the results can be quite remarkable. So therefore, it is a very important topic in deep learning. And we should start with that. In other words, the return on investment is tremendous. It is a unfortunate thing that most people are learning the traditional way and they either never reach automated machine learning or they reach it rather late in their life or after quite some time but we should get there i think in a few years it will become the norm but it hasn't yet but we will be starting out with that the second thing we will do and we'll put rather upfront in the early weeks is something called transfer learning. Let me talk about transfer learning for a moment. See what happens is when we train these neural architectures, for example, the likes of inception or these transformers, the BERT and the Excel net and so on and so forth, and this GP gpt2 and now the gpt3 the sheer amount of computational power they need is so stupendous that only giant companies like google like facebook can or maybe the governments can afford to train the neural networks at all you can't dream of training the neural networks however much, however powerful a workstation you have under your desk. You'll fail to do that. Just to give you a perspective on how long it takes to train this. Before we get to that, those of you who have taken machine learning with me before, you must have realized that even if you're training something like support vector machines or random forest or XGBoost or gradient boosting or cat boost or whatever, the training takes, what, a few hours and maybe overnight, a day or two, and then it is done on reasonable data sets. And you don't need like a planet scale cluster to do the computation, to do the machine learning, the training of that model. But with these latest deep neural network model, the sheer amount of computing power you need is just unbelievable. You need clues to, for example, I believe which of the excel net was it or something or gpt2 or gpt3 was trained on i'll have to get the facts which you'll have next time they needed 200 000 tensor processing units right think of them as giant those state-of-the-art video cards, very, very expensive video cards. It took 200,000 of them to collectively compute and get trained, to train a model. By the time you train a model, somebody looked at the environmental cost of it. And this is something we don't talk about much but we should the ecological cost of training these models more energy is spent in training one model than somebody estimated and gave a very interesting comparison if you run a hundred and eight cars I think is the word he used a hundred and something cars this is 110 it's rounded up 110 cars the total amount of energy that they will burn in the entire lifetime all their years of running it takes that much energy to train one neural network once so for a moment sit back and think how much energy are we talking about and it should give you a scope of how massive these neural architectures are they are nothing compared to human brain human brain is absolutely phenomenal it's not even one percent or 0.1 percent or 0.0001 percent of the human brain the human brain you feed it what is it a little bit of rasam travel and it runs fine a cheese a cheese and bread and cheese and it runs fine but these machines these so-called these artificial intelligence intelligence neural networks, they take a ginormous amount of energy and huge computing power, and then they get trained. So the question therefore arises, when a resource is this expensive, should we not all share it? Should we not all benefit from it? And the good news is, yes should we not all share it should we not all benefit from it and the good news is yes we can when you train these neural networks you may train it for one task but it turns out that that you can often apply it to a different task in a related task and it does just as well you don't need to retrain it. So you can transfer the whole architecture, the whole trained architecture, a fully trained model. You can take it and work, which is tremendous good news. Now, deep learning has been a very open community. Those models have always been shared, all of these BERT, Albert, and GPT, GPT-1, GPT-2, ExcelNet, and all of these are available in open source. The only one not available in open source is the breakthrough of July, which is GPT-3. It is so humongous, apparently cannot download it it hasn't been shared but what is most unfortunate is open AI has come up with this ridiculously expensive pricing I hope they change it I don't know it's being talked about otherwise you and I won't be able to use it in our practical work but up to GPT-2 we certainly can use it so these trained architectures are available all you need to do is download it they they are big I believe they take about under a gig of space so think about a model that has so many hundreds of millions of parameters it takes about a gig of memory a gig of storage just to store and it gets loaded into memory so these are ginormous of course but we can use it the next thing we can do is suppose it doesn't exactly fit a problem then we can tune it what we can do these models when they come they come in the so-called headless manner means the last bit of it has been chopped off you take it off and so what you can do is you can attach the head or sort of the the the specialization for your problem you whatever you are trying to do you attach to the very end of this very deep neural networks and then what you do is you do fine-tuning you do a little bit of learning of just a few layers just the last layer or maybe the layer before that also and that proves to be sufficient when you do that more often than not your problem is solved and it is solved to a much higher degree of accuracy and higher performance than if you hand cooked a neural network architecture and yourself and you trained it with all the computing power that you have which is again tremendous good news what does that mean it means that you don't have to be rich like Google right you don't have to have the wealth of croatia to be able to train these neural networks for your practical job. It is a massive democratization of artificial intelligence. These humongous models, you can just take, write barely five or ten lines of code i really mean it guys five or ten lines of code boilerplate code freely available on the internet you can use it and there are enough examples there and you can apply to your domain you can apply to medic medicine you can apply to fraud detection you can apply to engineering you can apply to all sorts of oil exploration and many many problems that you can think of and you can get state-of-the-art results in those areas if I have to say is there a free lunch lunch in the world I would say as of 2020 transfer learning is the one great free lunch we all have irrespective of whether you're doing it on a laptop a puny little laptop or your small google jupiter notebook to the extent that you're taking a model that has been pre-trained and you're using it or at worst you are just doing fine-tuning the last layer learning in that model and you're using it or at worst you are just doing fine-tuning the last layer learning in that model and you're getting state-of-the-art performance can you beat that and yet it is surprising that the way most people learn machine learning they don't start out with transfer learning and in their workplaces also you often see them it is almost like you see people sitting and building bicycles when you know there's a formula one race car sitting nearby why in the world would you do that so use transfer learning in this workshop will greatly emphasize transfer learning guys these are two big enablers they can get you into practical useful high-powered situations in deep neural networks within a week or two why in the world would you not start with this so we will start with these are we together guys huh we'll do that then we we will today, I won't, I will guys, I'll take obviously, we'll finish at 10. I would like to now change and talk about what architectures will we will learn in this workshop. Now, when we talk about deep neural networks, people often think, oh, deep neural network, I will write a neural network you know how bad can it be uh there you put a few dense layers and you're done but actually deep neural networks or deep learning is a whole field in itself and i will give you an analogy guys that may help you see at one time engineering used to be the field it It was hard. You were an engineer. That included being a mechanical engineer and a civil engineer. You were the engineer. You did all of it, manufacturing engineering, civil engineering, mechanical engineering. They were all one in the same. Aeronautical engineering, they were all in the same. Gradually, the engineering differentiated itself. Aeronautical engineering specialized itself. Mechanical engineering differentiated itself from civil engineering. Differentiation took place and each of these became big fields. Mechanical separated from civil. Then naval engineering became a different engineering and so on and so forth. Many specializations came out of mechanical. Then aeronautical engineering became its own engineering, naval engineering separated out. And so you see this differentiation take place and each become a rich and deep field in its own standing. Likewise, then came electricity. So when electricity came, they were electrical engineers and everybody would be trying to become electrical engineers but then out of that gradually electronics came about semiconductors vacuum tubes came and after vacuum tube came semiconductors and with semiconductors came microprocessors when you could do a lot more you weren't just talking about diodes and so forth but you could talk about lot more you weren't just talking about diodes and so forth but you could talk about these transistors and lots of transistors there came integrated circuits and then there's today uh integrated circuits are not amazing they are a fact of life you make giant chips people talk of entire system as a component as a chip and we live in those world so then electronics or computer hardware we call it computer engineering or hardware engineering separated itself electronics engineering and you had communications engineering so new fields keep emerging and what happens is it starts out small as a part of some other field and then it differentiates itself and explodes and so we had once we had the hardware computer engineering software used to be just the afterthought, you know, you just train the machines, you write some programs to do something for you. But then software part took off computer science came into existence and became a giant field. Then within computer science, artificial intelligence or machine learning was a field right and remained an academic field for a very long time why it was so and why today it has exploded we'll talk about in a moment but the fact is that artificial intelligence has today become such a vast part that it is it's machine learning in itself is a whole world in itself you can give years of your life i have been doing it in one form or the other um in different ways now for 25 30 years i can honestly say that i most things I don't know. There are far more things that I don't know than I know. Every day I wake up and I feel there is so much of catching up, so much of learning to do because this field is absolutely exploding. Wonderful, great minds are in this field. And every single day you wake up and you look at these portals which have the news and you see breakthroughs you will see in the course page i mentioned some of them how to continue learning about artificial intelligence is a section you will see a lot of youtube videos that talk about the the cutting edge research every week a new important paper shows up on archive or two shows up on archive it gets talked about some breakthrough keeps happening it's hard to keep up but one habit you can develop and i hope i help you develop is every week read a research paper and reading a research paper seems intimidating what i would like to do is guide you into it. It's far easier than you think. But once you get into it, you will learn about things before, way before others, and you'll be able to use it. When the community is small, you can talk directly to the researcher. He'll guide you, he'll help you with his code, and you can move ahead pretty quickly with solving your problems. So that is about the research paper and artificial intelligence. Machine learning has exploded within machine learning, deep learning in artificial intelligence, deep learning or this deep neural networks has become so vast that when we talk of architectures, it's a zoo out there. So people typically think of dense nets and CNN and so forth and RNN, but that's not it. You're scratching the surface. There are so many architectures and now we have automated machine learning that will discover architectures for you. So in this particular workshop. Obviously we can't cover them all, but we'll do some core architectures. core architectures, the kindergarten of all architectures and the one that is actually in practice very useful. I think of it as the logistic regression of the deep learning world just as logistic regression is simple to learn in the when you're starting out with data science, but it's tremendously useful everywhere. In the same way in neural networks, it is the so called dense network. Just a neural network made out of dense layers. These are also called feed forward networks or multi-layer perceptrons. These are words that people use I deliberately called it MLP it was an allusion to multi-layer perceptron but obviously in the workplace the team is called the machine learning platform team MLP team so anyway a bit of a bun there so multi-layer perceptron there's a basic thing we'll start out with then there is the so-called convolation neural network that arose by studying how a cat's brain when a cat sees something the optic process of a cat the optic area of a cat's brain how does it see different regions of an image or of the scene in front of it? So out of that neurobiology research was discovered an architecture in neural networks called the Convulation Neural Networks. It has had amazing success. When we talk of self-driven cars, Convulation Neural Networks is a giant part of it. I think in recognizing the object, there's a lot more going on there, of course, but it is a part of it. Then we'll do something called recurrent neural networks. These are things that learn things that are sequential or periodic. So for example, time series data has a certain sequentiality to it. Weather tomorrow can be predicted based on the weather today and yesterday and the day before and so forth. So in these sort of situations, you use recurrent neural networks. Language is another thing for recurrent neural networks. The next word that I will speak is context thing for the next word that I will speak is contextually depended on the sentences that I have been saying so far and so these are called language models how do you tell what the next word should be in the sentence or what the next sentence should be in this paragraph or what the next paragraph should be in this document so there's a language modeling and in there recurrent neural networks for what the next paragraph should be in this document. So those are language modeling and in there recurrent neural networks were the king for many years though they are being superseded by a new architecture called transformers. And these transformers are the ruling king. They were discovered in 2017 and December and 2018. We are in the world of transformers now pretty much much of natural language processing is now done using transformers so we will learn about transformers a lot in this workshop then we will also learn about various things like auto encoders and variational encoders and so forth we will also learn about something which is quite interesting these are called generative adversarial networks these were discovered in 2014 and they are considered perhaps one of the biggest if not the biggest breakthrough in deep learning of this decade i suppose gans generative adversarial networks and transformers are two big highlights of this 10 years what generative adversarial networks are is a very interesting game of deception and catching the deception. So imagine that there is one rough analogy I'll give you is, suppose there is a con artist, an art, what is it called, those people who reproduce other people's art and try to pawn it off as genuine art forgers, art forges so they imagine that there is an art forger he keeps producing forgeries and there is a detector there is a art person expert who is supposed to tell whether something is a genuine piece of art by some great artist or it is a forgery of that artist and so that particular examiner is being served randomly occasionally a genuine art and occasionally a forgery the forger obviously tries its best to forge the art he'll try to make the best possible forgery and the inspector will try to smarten up and be able to tell that the forgery apart from the genuine thing and so you train these two together to become insanely good at what they do each of them and when you use these things together you get amazing power some some really interesting things come about and actually in the next half an hour I'm going to show you what some of these things is in fact that will be your reading assignment and we will do this lab we'll the first lab that we do on wednesday it is are all these marvelous things that neural networks can do so um we'll see how these networks these uhANs can do painting. They can paint like Picasso or any great artists that you wish. And they paint with such impeccable authenticity that you would believe that it is a Picasso or it is a Monet or it is a whatever you want it to be. It's quite amazing actually. So we will use these, these are GANs, and the last architecture we will learn about are called graph neural networks. We live in an interconnected world. Most of the data is interconnected, life is interconnected. We live in relationships, family is a relationship relationships family is a relationship society is a relationship professional relationships are there in workplaces things are interdependent one thing doesn't happen without other happening first so everywhere you see networks in play the internet itself is a giant network of computers and devices right so one of the things that we have today is a steady of large-scale networks and large-scale networks like for example the power grid of the United States you know we have a national grid is a large-scale network the highway system is a large-scale network so when you have this large-scale network you see a pattern you see a method in the madness all these networks they have unique characteristics which the human eye can't detect but you can quantitatively extract it those characteristics so for example there is a degree of centrality to nodes, to networks, how interconnected the network is. And there are remarkable results. For example, you must have heard about the sixth degree of separation, that between any two human beings, you can have at most six degrees of, or on average, six degrees of separation or less. Actually the number is smaller than that. It's more like four to five degrees of separation typically between two people. Amazing, isn't it that you take the the whole of social graph of humanity and you can go from any two human beings with just four or five intermediaries or six intermediaries occasionally more but mostly just this number now the study of these graphs these networks leads to some profound results when you apply deep neural networks to it you get some really profound results and deeply the graph deep neural networks is actually the cutting edge these days a lot of the state-of-the-art results for example the best recommender systems and so forth which were done with other architectures when people are trying it with graph neural networks, all of a sudden they're able to beat the state of the art. So we will do graph neural networks. So just to enumerate, we'll do the MLP, the multilayer perceptrons, we'll do the convulationals, we'll do the recurrent neural networks with the lstms and grus and the at this moment these are all words don't don't worry about it they are simpler than you think and then we will do something called the transformers then gans generative adversarial networks along the way we learn about auto encoders variational auto encoders, variational auto encoders and so forth. And we'll learn about all sorts of things, embeddings and so forth, word to work and glove when we do natural language processing. So there are some very fascinating animals in this AI Zoo that we will encounter. So that is the scope of what we will do. before we do that i'd like to mention one thing these are all things i've been extolling the virtues of ai and as engineers we are all gadget level nerds a typical engineer has more gadgets and than the average person the typical engineer is much more likely to splurge and update to um state-of-the-art gadgets the latest iphones or the best camera or whatever is much more likely to do that we all are in love with gadgets and all these lovely toys when you look at artificial intelligence we tend to jubilate we tend to celebrate the power and the amazing efficacy of these tools but these tools are not all for the good with power comes responsibility and that responsibility is one of the things that i would like to teach in this workshop it is under the topic and from a technical perspective this is not a class on ethics, but I will talk about ethics once in a while. These things can do a lot of damage. This in 2020, in 2019, we began to realize that blind belief in AI can destroy the world in a quite literal sense and there are many many examples of it i will take two examples there is uh recently amazon uh i believe it was amazon they tried they opened up a facial recognition system you could show it a picture and it would tell you would tell you who it is. Then they were trying to give it to law enforcement until somebody observed something pretty obnoxious about them. One researcher took the pictures, the portraits of all the black congressmen and senators, the black caucus of the United States, and put the pictures through that image recognition system. Very embarrassingly, in many cases, I think in almost all cases, it identified those people, not as luminaries, not as senators and congressmen but as rapists and murderers and whatnot and the facial recognition system obviously did not do that for caucasian senators and congressmen it just points out to the danger that you show a picture of an African American person to this AI, and it will immediately tell the law enforcement person that this is a murderer or this is a rapist. We do already have a tremendous problem in this country. As you probably know, in the United States, we are having, I'm saying this because we have people from different countries in the audience, in the United States, there is a movement in progress called Black Lives Matter. And this movement has risen from a deep angst, from an unbelievable amount of injustice that has remained in this country, even after this country has made tremendous progress since the days of slavery so it's a paradoxical situation we have made tremendous progress in this country this is probably the best of times when it comes to how much freedom and equality the minorities have and yet the sum total of freedoms and privileges and equality that the minorities have the african americans have is pathetically low it is way way way below where it should be so we are in a paradoxical situation so that leaves a tremendous scope for improvement the u.s news is filled with videos of black men being beaten up they're being shot they're being mistreated they're being thrown in prison much with a much higher frequency than people of other races it's a fact and within that fact you throw in an ai system that confirms the bias that confirms the racial uh discrimination and you can imagine what tremendous evil it can do it is fortunate that people have been studying it they have been the people in the humanities and social sciences have been on the guard and they caught it and it's been uh now a serious uh area of study in ai now when we do that you ask this question how did it happen a large part has to do with the fact that most people they have they have to use an old term irrational exuberance towards ai they feel it can make no mistakes let's use it it's not like that it is a tool every tool it's an imperfect tool and we have to use it with caution from a more technical perspective we cannot have black box learning this deep learning models are black boxes there are if you remember the story of wizard of Oz it is like that wizard that is responding to a Dorothy's question and everybody's question from behind the curtain it answers the questions but it doesn't give you an explanation why all of these are black boxes and as in the story of the Wizard of Oz Dorothy ultimately went and looked behind the curtain and as the story goes when she looked behind the curtain she was disillusioned she found that the wizard was not really a wizard he was just a guy who was saying things today we are in a situation where we need to have a similar degree of skepticism and disillusionment towards deep neural networks towards artificial intelligence towards machine learning itself black-box models are dangerous things they can do they can perpetuate a lot of harm they have they can be tremendously powerful they can have brilliant performance but there are situations where it is unacceptable to have black box models so what is the way out of it we do want to use this powerful highly accurate models but we don't want them to be black box. So that brings up this last part of this course, which is explainable AI. Today we want to go behind the curtain and see how this machine is thinking. How is it reasoning? We want to explain it and not just interpret its results. So explainable AI is a deep thing. And in technical terms, we are not talking ethics. We are talking algorithmic means. It's a whole area of rapid progress. Those of you who attended the boot camp last year know that we made, we devoted an entire day to explainable ai or interpretability we did all sorts of shape the shapely method lime and so on and so forth and we are going to do a lot of that in this particular course too so that is the scope of what we are going to do guys we are going to do all the neural architectures that a traditional course covers definitely we are going to do all the neural architectures that a traditional course covers. Definitely, we are going to do that. We'll cover all of them. We will do automated machine learning. We'll start with that and with transfer learning. The reason is we want to become productive as soon as possible. This week, of course, we are going to play with toys just to familiarize ourselves with the world of AI. But I hope that by end of next week, you will finish your next week already productive, hopefully already going to your workplaces and applying these techniques. That is the goal. And after we have done that, we will go back and learn the architectures and as we learn the architectures we will also keep our eye on algorithms and means and techniques in every of the models that we build we will make sure that it is explainable explainability of AI and is a casualty of modern uh coursework or trainings in ai it's very unfortunate they think it doesn't matter they think that it's a people think it really doesn't matter it's a side thing or it's an ethics question it is not if we as a humanity have to retain our freedom interpretable and explainable ai is of the greatest importance the United States the elections were hijacked by algorithms as you know there was a Cambridge analytics and whatnot we all know the history of it they completely hijacked the social graph that is Facebook and the whole country felt manipulated and generally were manipulated that is the risk with these AI systems quite often the biases are not visible so well of course you can do deliberate malice but even when you don't intend to the risks are there and in very practical terms you can get it wrong in this field a story is told which I will tell you it must be true though i haven't verified that but it is told so many times in artificial intelligence community that it's a lesson in itself so here is how the story goes once the u.s military looked at all this silicon valley and all this bubbly developments in artificial intelligence and they asked this question, can an artificial intelligence distinguish between a civilian vehicle, like a car with a family going in it, and a military vehicle, a tank, for example, that is coming to attack you. And the military wanted a way so that it could just do optical recognition of that. So they came to a group and they said, yeah, of course we can do it. How would we do that? Well, you send us a lot of pictures of civilian vehicles and a lot of pictures of military vehicles and a lot of pictures of military vehicles and we'll train a neural network to do that sure enough military you can imagine well i can quite imagine a general giving the orders to soldiers and each soldier saying yes sir and promptly taking out their cell phones or the cameras and taking pictures furiously and suddenly there was an enormous amount of pictures of military and civilian vehicles this company got it this research group got it and they trained a neural network with astounding accuracy and they said this is very easy too easy we did it military said really oh really then they got another set of pictures to test it out they tested it in the lab and sure enough the algorithm was working marvelously military got convinced they said all right let's go deploy it and here is the problem the moment they deployed it to everybody's surprise it was a uniform failure it just completely failed and the question arose why you give it some images in the lab it works you give another set of images to it in almost of the same kind in the field you show it real pictures and now it's not able to do work at all in the field you show it real pictures and now it's not able to do work at all in the field what happened to this AI and you scratch your head until somebody noticed the obvious see if you think of a military when do they take picture when when do this when does a soldier see other military other military vehicles well in the morning before they go out for patrolling or in the evening when they return from their patrolling when do they see civilian vehicles well in the morning before they go out for patrolling or in the evening when they return from their patrolling when do they see civilian vehicles well when they are patrolling in the daytime in broad daylight and they are seeing they are moving through cities streets and so forth and seeing civilian vehicles all around so what happens is when you see something in very early morning or late evening the pictures are dark the military vehicle pictures are dark is something or daylight the civilian vehicles the pictures are pretty bright so what the neural network picked up on was the ambient light and you couldn't detect the bias you couldn't detect that the input data had a bias. It was not obvious. That became obvious when the algorithm failed, when the deep neural network failed. So there is a lesson here. In a black box model, it is very hard to see what is it that it has learned, what the deep neural network had learned as ambient light matters, which is not the sort of learning you wanted it to do, isn't it? And so what should we do? We need to have a means to look under the cover. We need a means to look behind the curtain to see how it works. And we will learn practical coding techniques to do exactly that. We will always look behind the curtain and see why is our model making the predictions it is making, explain it, interpret it, and that will be important. And I hope you'll internalize that as we move forward. Now, we have another 15 minutes left, guys. i would like to give you guys a little assignment to have fun with and i'll show you some of those so to change track now i'm going to show my browser here So let's go to the deep learning workshop. When you come here, guys, so first thing guys, you become intimately familiar with this workshop, I'll give you an orientation. Here's the table of contents on the right. It's sort of counterintuitive intuitive but it is as it is it's a learning it's a lms something called a learning management system model model is the leading learning management system we are using it's open source in the world of corporate learning management system cornerstone on demand the day the place i work actually is the leader but this is using the open source tool it's more conducive to our course there is another learning management system I'm going to migrate to it is called open edX based on the edX platform but for now it is this now in this this is your table of contents I am seeing more topics than you because I have hidden some of the topics for now and I will be trickle-feeding these topics on a weekly basis every week when you but by the way announcements look out for announcements guys they'll show up on your mobile devices this is your social forum where you can go and post questions at this moment I don't preparing for the workshop yes for example uh premjit has one it is a response uh then there is a chat i would and as you do the project i would encourage you to a chat here guys because then others can also see the responses you are seeing and i mean it guys you know I have produced many batch of students over 30 years and if I have to point to one factor that determined how successful a batch collective collectively would be I can point to this is the community spirit batches that developed a strong community spirit that engaged with each other a lot they tended to do very very well going forward there are groups you know people form deep friendships when they attend these workshops they form groups and these are professional networks they form that help them for years they stay in touch for years and they often you know go on and work together and things like that so that social aspect of learning please don't underestimate it there is a reason we you know have fond memories of school and college we think of colleges not just as places with classrooms but as places where we hang out with friends. Let us have a virtual version of that. If you if you post a question that everybody should get an answer to, I'll certainly respond to that, so it should have the nature of a fake you. If you guys have questions posted, what I will do is if I see questions show up in the in this chat, in this and many people and the the response is important enough i will actually copy it over to the frequently asked questions section this is the survey which you must take guys i hope let me see how many people have taken it 26 survey responses we are still waiting for another nine people in this audience please do go and take that survey learning tips are fine so youtube channel please do subscribe to it so guys everything you notice a check box here next to it so once you have visited this chat room and become familiar with it acknowledge that you know it is there by clicking on it you see you can click on it so it will indicate to me that now you know that there is a chat room and you you're aware of the fact in the same way frequently asked questions once you know when you visit it once and you've become familiar with it go put a check check mark next to it you finish the survey put a check mark next to it and likewise well of course there are certain things hidden from you when you when you have subscribed to the youtube channel put a check mark next to it likewise this is a learning how to learn is a course this is just a course you can visit it you don't have to enroll in it or something for those of you because I often get people who says you know I'm taking a course after many years something this academic and deep this intense so I often recommend that maybe it's time to sharpen your instruments before you work on the material and your mind is the instrument maybe it's time to regain your learning techniques that you had in before you work on the material and your mind is the instrument. Maybe it's time to regain your learning techniques that you had in college and you have forgotten. So you can go take it is based on the latest cognitive research. I highly recommend this course. You take this quiz, you put a tick mark next to it. So guys always acknowledge that you have done something by putting a checkmark next to it. Now, what will happen is as you keep doing things you will get badges badges of recognition that you have achieved certain milestones and watch out for those badges they are they give you a sense of where you stand with respect to learning deep learning by the time we give you up the expert level badge you will not go out to interview for companies and come back empty-handed you'll really be impressing people but it takes effort and there are many badges in between before you get there so these are your milestones ways to recognize see you are professionals I'm not going to sit and give you grades and marks and so forth what you do in college with kids we are all professionals and grown-ups so but you will get badges and you have to earn your own badges and you will know what badges you have earned that's it and it gives you a sense of how far along you are there is a hands-on lab well at this moment just go and read this instruction which is i already showed you about hardware about the development environment now i will put some suggested projects let me talk about the projects labs are same for everybody but the projects have at least three different trails or specializations you can do a project in medical informatics and I'll suggest a couple of projects kovat is one of them actually kovat data you can do a project on the analysis the deep neural network analysis deep analysis of giant networks like the social graphs and so forth you can do that is one trail you can do a project on a fraud detection and anomalies and things like that that is a specialization there's a project on that you can do a project on natural language processing NLP a lot of people use NLP in there I mean are looking forward to using NLP in their workplace and guys I mean it by the end of the next week you will be productive with NLP in a genuine way in a practical way that you can use it. So do those labs seriously and then you may want to, if you want to take it seriously, do the project that is NLP, NLP specialized project. And the last is a project on computer vision. Now the medical informatics also is very closely to the computer vision. So there's a huge degree of overlap. Maybe we'll leave those two together the medical informatics will be more on computer vision the this fraud detection natural language processing these are your broad areas and networks deep neural networks I mean network sorry well not the opening networks using deep neural networks to study giant graphs or giant networks at scale. Those are your four trails or specialization. And if you do any one trail, you can ask me to give you a letter of endorsement for that particular trail, for that particular area of work that you have developed expertise in. Next is the weekly research papers. Too early but from next week I will be posting a research paper. Maybe I'll post one right now. The idea is you can glance through it, try to read it and then we'll get together on Sunday afternoon and I will walk through that paper very carefully. So, and these are landmark papers. It's really worth knowing, purely optional. Those of you who are just entering this field, you can entirely skip that. Now here is the fun learning resources I'm coming to. So look at this, what deep learning can do. And I'll take 15 more minutes guys, if you'll bear with me. Let's go here so you all are familiar with the great artist who is this person can anybody tell who is this artist with this star in mind one golf one golf okay and then what you can do is you can take a simple photograph that you have taken. You see, this is a picture of a row of houses next to a canal. Somehow it looks like Amsterdam, but I don't know. You can take this picture. And ask the Deep Neural Network to. Paint it like Van Gogh and see what comes out. Do you see this guys do you see this photograph become this image so play with that guys and you can apply it you can say let's try it let's take a picture I drop your photo or click one from your computer so let's go and get a picture here i'll go to one very good place to go to picture without violating copyright is unsplash right let us take a picture of what let's take a picture of what would you like to take a picture of? These days dogs. I like dogs. Well, dog and girls. I like dog and I have two girls. So I'll take this. How about this? Which of these should we take? Well, maybe this one. Let's go download this we have a picture here and's go back and where is here it is dog and girl let's drop this picture here so here is a dog in here let's try to use some style some great artistic style let's take something pretty I don't know which one would you like to take perhaps let's take this and you'll have to by the way this is not the only site it's called deep art it's not the only site you can take whatever there are many sites that do this and you will be writing code to be able to do it. You may say, oh my goodness, this is so complicated. How can one do it? Thank you for it will launch you have images. It may take a while for me to be ready. Feel free to submit other pictures. So if you wait a little bit your image here will come up and you will notice that you'll get something uh very nice you'll get the painting very much like you see in the website now this is the power of artificial intelligence when this paper came out then it is called the neural style transfer. This process, this algorithm is called neural style transfer. We will do this lab and we will write the whole code in this particular course. When we write the code, you'll be amazed that you can do it. It's not that hard. It's actually amazing that you can and you can tweak it and you can do all sorts of amazing things with it but just pause for a moment and think it can perfectly understand an artist's style and can forge it or apply it to new things but it isn't just with images not just with paintings you give it Shakespeare and you give it a paragraph of text and it will apply Shakespearean style to it or apply Charles Dickens style to it or somebody and you give it a paragraph of text and it will apply shakespearean style to it or apply charles dickens style to it or somebody else style to it so it raises a question therefore that what does it mean to say you have created art when you can take very ordinary paintings or very ordinary pictures and you can transform them into world class art with a deep neural network genuine world class art and by the way i think this website even sells it they will make it into a big painting and send it to you and people do that you know they'll take their favorite pictures of their children and so forth convert it into deep art they'll have Monet apply the Monet style or somebody, one of the impressionists and then have a large printout, a canvas painting of it, and they hang it in their walls. It's happening. You can take writing and you can pick one of the great writers and apply the style of that great writer. These deep neural networks can recognize that we will do that in this workshop so this is one thing that you will learn in this workshop play around with it guys I don't know well in my case my images let's go there oh yes it's done look at this do you notice that guys this picture with this person's uh this sort of a thing became this beautiful painting most of you would be willing to pay for this painting if this was your daughter isn't it any any any takers and so it is with uh poetry you can ask the computer the artificial intelligence to write poetry for you well it writes rather substandard poetry today but in future hopefully it will do better it goes and completes your paragraph for you so let's try a bit of that today and we'll learn about that I talked about neural style transfer today I talked about transformers what can transformers do? Let's go and see what a transformer can do. We will go to this transformer and let me remove this text and say the most important. Person in artificial. Intelligence, I guess, can you guess who it is? Is now let's hit tab it well it seems to believe is the most important person. Are you guys reading the sentence? It completed it by saying it is not the computer or the software developer. It is the AI itself. How many of you are impressed? Not not. It seems to be bragging and then after that it gets a little bit sidetracked we are not um it begins to not make sense after a little while and you can give it any any sort of thinking um what do you think is the future of artificial intelligence I think let's see what it if intelligence okay things that a human could not do do you see guys this is a machine that doesn't think and yet it shows uncanny resemblance to what a thinking a being would say and these are transformers and we transfer learning you and I will be able to reproduce this ability in our code in a few weeks. If somebody had told me this fact, even five years ago. I would say it's a pipe dream. And yet today you you folks will be doing this impressive things in just a few lines. So guys, if you learn this, take this seriously this workshop. I'm going to end with these words. This workshop may be the most sort of, it may be an inflection point. If you do it well, you may go on to found companies. You may go on to do breakthrough works in your own organization. The application of AI is enormous. Data knowledge is exploding that more knowledge has been created in the last two years than in the entire history of mankind. You would say that is just junk data. That's not true. Yes, data is exploding. But I'm talking about meaningful knowledge. If you go to archive.org, which is the place where people post their research predominantly at least in computer science and the technical in the physical sciences you will see genuinely that more papers have been submitted in the last two years than in all the years preceding it and this trend has continued research is exploding exploding, knowledge is exploding. Right. And the way that we are going to be able to manage all this knowledge and benefit from it, it is no more that we are going to put it all in a database and write SQL queries. And I don't say it as a disrespect to SQL and the old traditional knowledge management. I worked for Oracle and I have fond memories of Oracle. It has a wonderful database. I worked with big data for many years that has an even bigger repository of facts. But we are deluged with facts and knowledge and data. And the only way we can do that is with manage it with is with artificial intelligence and data always has a story to tell whenever you encounter data that data didn't just come about it is weather data behind that weather data let us say they are forces of nature they are generative forces you look financial data their market forces They are generative forces. You look at financial data, they are market forces. You look at biological data, they are biological processes driving it and forces driving it and generating the data. So behind every data is a narrative. of artificial intelligence is to understand discover uncover the narrative that's hidden in data and to harness its power that's what we do in practical terms and that is a mission for our workshop this five-month journey all right guys so today i'll recap in the next few minutes what we did we talked about the administrative year in the first hour, all the rules and so forth. I hope that will be a separate video. It's already live on YouTube. You can go and see the recordings in the live section. Then after the break, we talked about, in very broad strokes, the whole arc of AI. What is it about? So it's an unusual lecture, it's the first session, but it is important to get our perspectives right before we dive into code, before we start doing things. So today we did that. And what we said is we are going to do this workshop in a sort of unorthodox way. We'll start with the latest research and then back our way into the foundations so that we can become practical and useful and do real things from day one. Are we together? There we talked about the power of artificial intelligence and its various manifestations and architectures. We talked about automated machine learning which is tremendously relevant in the future. We talked about benefiting from other people's work, the transfer learning. And lastly we talked about the fact that with power, with this tremendous power that you see, that with power with this tremendous power that you see comes responsibility and that responsibility is real if we don't pay attention to the human dimension of this we will destroy the world in very genuine terms we'll destroy a civilization we'll destroy the world so the ethics of what we do is tremendously relevant otherwise we are building frankensteins is tremendously relevant otherwise we are building Frankenstein's and fortunately the steady or the interpretability or the the act of looking behind the curtain at the wizard is a technical topic it itself is driven by code and processes and methods we can do it and we are getting better and better at it tremendous amount of research is going into the xai explainability of ai it's the hottest topic these days so we will learn to always as not ignore the interpretability of it whatever model we build we will as part of the exercise do its interpretability as we go along and so that is it and so guys please go back to the website and this fun section a play around with the fun section here you will see a lot of Google experiments for example I'll do one of them yeah this is the writing you see how well it writes how it learns things drawings maybe i'll do this launch this experiment and what i will do is actually you can do let's draw Let's draw. I see rainbow. Oh, I know, it's I. Yes. So, you know, you just, you can't hear the sound, but if you do that, as you draw, the computer is trying to interpret what it is doing. So these are fun things. Their purpose is to become familiar with the scope of things we do with artificial intelligence so this fun learning resources guys I think this week engage with it now the other work item is do get your textbook it's important lastly in the home before we do the labs there is one bit of thing to do which is read this the development environment for Read it become familiar with it. If you're not able to do anything about it. It's okay, but be aware of the material I've talked about it Then when we start on Wednesday, we'll start we'll dive deep into our first code that is able to do something useful alright guys so with those words I will stop the recording and I'll open the floor to question and answer Hãy subscribe cho kênh La La School Để không bỏ lỡ những video hấp dẫn you