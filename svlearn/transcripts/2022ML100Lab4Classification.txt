 Hi folks, today is Saturday morning here. We are going to do the fourth lab session. The topic today is classifiers. So we covered classifiers in the theory section on Thursday. So we will now bring it down to actual practice. Now, I would like to review what we learned in classifiers. And then I would like to also finish one more topic, one more type of classifier before we go into the labs so the we talked about the fact that in classification you identify a class from a set of classes given certain attributes of the data, you say, well, this must be a cow or this must be a duck. If it is small, feathery and so forth, webbed feet, these are all features. Given the features, a classifier, a successful classifier should be able to say, well, this must be a duck. So that is classification. Now, when we do classification, machine learning is all probabilistic. There are no certainties here. So one of the intermediate steps is you produce a probability that it is a duck or a cow. When you produce such probabilities, it leaves a final task at hand. At what threshold would you put the cutoff and say beyond this threshold of probability, it is a duck. So if the probability of a duck, whatever it is, the probability of a cow will be one minus the probability of the duck. You can arbitrarily pick one of them as the positive case. Let's say ducks. If that is a positive case, then you say that if the probability of duck is a 50% or greater, it must be a duck. That is one way of looking at it. And where you put the threshold, we talked about it, it depends on what the real life situation is for example if you are doing diagnostics for cancer and it's an early screening test you want to make sure that even 10 probability or very low probability that it is there is a malignancy you would flag the person as a positive case needing further investigation. That's exactly how you flag it. So you will end up with a lot of false positives, but that is a worthwhile price to pay because if you get a false positive, the worst that would happen is the person would go through further tests and then it would turn out the person is quite healthy. On the other hand, if you miss a positive case, then time is the enemy. The untreated cancer can get to the higher stages where it is far more expensive, both in terms of medical care and in terms of human suffering, to bring the person back. So it all depends on the context, where you put the threshold. Now, we also talked about the confusion matrix. The confusion matrix tells you, it basically says how many positives were identified as positives, how many negatives were identified as negatives. The diagonal are the correct answers. The off diagonals are mistakes so for example if a negative sample is marked as positive this is your false positive this is saying that a perfectly healthy person actually is cancerous in a diagnostic too likewise a false negative is when somebody does have cancer answer but you say no your classifier says no who seems here now we talked about accuracy two things that i left you to figure out were precision and keep going this was a homework i don't know how many of you uh did that homework but now i will um in a short while I'll explain to you what those things are. I talked about another model diagnostic it is called the receiver operator characteristic. Its history predates the notions of machine learning in classical. It has to do with the false positive rate and the true positive rate. Initially it has a signal processing so called communication. And what we search for is the area under ROC, a bad classifier, a random dummy classifier, which show a linear behavior. You increase false positive rate, you increase true true positive rate, your false positive rate will fall because it's making random cases. But a good classifier will be able to achieve a pretty high true positive rate at the expense of a very tiny false positive rate. Since machine learning algorithms are all probabilistic, we have to be cognizant of the fact that there will always be an error rate. Remember, this is the same thing as the irreducible error. We talked about in the case of regression of irreducible errors, the errors that you can't get rid of comes from the fact that there are many things you don't know your model hasn't accounted for that probably if it did the error rate would go down and it also comes from instrumentation errors so you can only make the area under ROC curve that good now I talked about two interpretations of frequency of probability the frequentist interpretation, which is based on number of trials. Probability in their definition is proportion. And then there is another definition, which is probability is the degree of belief. How much do you believe that something would happen? If I toss a coin, how strongly do you believe that it will come out heads and tails rather than tails okay how strongly do you believe that tomorrow it's not going to rain and it turns out that both of these probability definitions they follow exactly the same algorithmic properties now which one you take in practical terms is a matter of, it is simply a matter of convenience. In many places, it is perfectly okay to go with a frequentialist definition in case the work done, though one would argue that the Bayesian definition is more, you know, it's just more theoretically correct. We talked about given probability, out of probability, we created a notion of surprise. It is what I call surprise, the technical term, the information content. How informative something is if it happens, but the probability is low. So if you know that the probability, the desert probability of no rain, of dry day, is close to 99%, tomorrow it doesn't rain, or today it doesn't rain, you're not surprised. There is not much information if somebody comes running to you and says, Hey, it didn't rain today. You'd say, what else would you expect? On the other hand, in the desert, the probability of rain would be, let's say, one percent. And somebody came running to you and said, Oh, good, good, great. Did you know today it is raining raining and now you would be very surprised isn't it so there's far more information content when an unlikely event happens so surprise is the reciprocal effective reciprocal of the probability with a log you you put a log there and when you do that, it ensures that events of no surprise are zero, their values are zero, the surprise values are zero. Where there is certainty, there is no surprise, and no surprise means zero. Surprise should evaluate to zero. So log one over p, therefore therefore is a good measure of surprise with that notion uh actually uh unless almost oh i just noticed something one minus p over p no this thing i just noticed uh it is sort of it is p one minus b is the odds. This is a slight error. So if you guys are looking at fixed odds, I just noticed that I had put it upside down. The odds of something happening is probability of it's happening over the probability of it's not happening. So that was, it slipped me. OK, good that I noticed it. So that's that so total surprise now when you're doing when you're doing flip coins etc etc then the average surprise that you have for trial is called the intro now and it comes out when you work it out into the country negative b log which is the traditional form in which you see. Now when you're talking about the loss function of a classifier, a model, a good model would produce the evidence, the data, if you know that the ground code is, it's a duck, you should say probability of duck is as close to one as possible. So the element of surprise would be, again, the reciprocal of that. If it is not, for example, if the probability of a duck, it says 0.1, you should be hugely surprised. So that is that. And so if you add up all the surprises from each of the data points, how surprising the results are, or you can call it unpleasantly surprised you classified. The sum total of that is the loss function. And we can do that loss function, the probability, because probability you model as a continuous function, you model as some function. So the basic idea is for classifiers, so this needed to be said, so let me say, the probability is a function of the input. You expect it to be some function of the input, right? It's some probability function of the input. These are the same thing. But because it's a function, you can take its derivatives, you can do gradient descent. So a loss function written in terms of probability is amiable to the whole machinery of optimization, of gradient descent, and any of the other optimization patterns. Whereas if you take a number like the error into something, can't do it's a little bit harder to do gradient descent straight from that now so cross entropy we said is basically the total how surprised you are with the ducks results and how surprised you are with the cows that is it so we summarize all of that surprise entropy entropy, cross entropy. Now cross entropy has another derivation, which is in terms of the likelihood function. So likelihood is often misconstrued and people think of it almost unanimously. They confuse it with probability. It's slightly different. Likelihood is that, is the probability that a given hypothesis will produce the evidence that's actually there. So suppose you have evidence data says rain, rain, dry, rain, rain. You may have two probabilities. You may be in, let's say, Phoenix, Arizona, or you may be in Seattle, Washington. First hypothesis, if you are in Arizona, then you would realize that the total cumulative probability, the joint probability of happening based on your initial hypothesis, that chances of rain is just in person, will be much lower, the likelihood will be much lower which is a joint problem the likelihood that you are in seattle would be much higher based on this model and so the evidence has supported your the hypothesis that you're likely in seattle more likely that you are in seattle the maximum likelihood estimation is essentially the intuitive process of saying, given all the hypotheses, pick the hypothesis that produces the greatest likelihood of producing the evidence or the data set. Data set is often called evidence of this error. So it is most likely to have produced this evidence. So that is the maximum likelihood hypothesis or maximum likelihood estimate. When you use the maximum likelihood estimator, how would you maximize that in terms of your probabilities of cows and ducks? Let's say p's are the probability of ducks and q's are the probability of cows. You just multiply that with joint probability whatever evidence that you see and then you maximize find that value of p that maximizes well maximizing product of probabilities is tricky because computers tend to deal with floating point numbers in a in a way that is suboptimal we don't know because we have finite storage or we don't have infinite precision so a product of small numbers fraction numbers is fraught with danger fortunately it turns out that if you have a product of numbers you can take their log because log is a monotonically increasing function taking finding the max of a function is pretty similar to finding the max of the log of the function and so if you take the log of the function then you you can maximize the log it becomes addition because logs the log operator converts multiplication to addition, which is much better. Now you're less likely to have a deflow problem during the numerical interpretation. But then, furthermore, it turns out that most optimization libraries or functions, they like to minimize things rather than maximize. So that is easy. Just take the negative of the maximum so that is easy just take the negative of the law and so uh that's the end result that the last last function is the negative of the log likelihood negative log likelihood i should have written that sentence let me write it down there's a famous phrase that people use negative log log this is an important milestone praised for us in our journey so you say loss for classifiers is given by negative log likelihood now comes the question is this the only way you can capture the loss function it is by far the most popular comes the question is this the only way you can capture the loss function of the classical class it is by far the most popular but there are many adaptations and variants and especially when you do deep neural networks there are all sorts of losses called contrastive losses many other forms of losses that come along but this is by far the most popular, the most used, and the most intuitive to understand. So then we now with all this architecture, let's go back and do some actual classifiers. We learned about two classifiers. I said that there is one classifier that people tend to ignore or not include in their analysis and that is a mistake as of people who are learning from here from support vectors i would strongly encourage you start your analysis with a dummy classifier why the dummy classifier because it sets the baseline it is the same thing as the null hypothesis. Dummy classifier says, what is the performance if you ignore all of the input and you just look at the majority class of the output? Whatever, and that accounts for class imbalance in the data set. So for example, if you're screening a thousand people for let's just say any common form of cancer, let's say you screen them for breast cancer, which screening in the United States is very prevalent. So when you do that, maybe in reality, the ground truth may be that out of a thousand or three or two or maybe one or one person has or let's say three person had three people have genuinely cancer on average right if you were to randomly take a sample of 1000, on average you'll find 3 people to have cancer, the breast cancer. So, a dummy classifier would look at it and say, oh, 997 people don't have cancer. So one easy way, just say that whenever any data is given, just ignore the data and say you don't have cancer. That is a quack doctor. Now the doctor would be right 99.7% of the time. Astonishingly accurate. And so the doctor perhaps will get famous for the validity of his diagnosis, but of course the doctor is a quack because the three crucial cases that he should have caught, he missed. And the difference between a quack and a real doctor is that a real doctor would catch those three cases, would do a diagnosis or use x-rays or whatever means necessary, and would come to the conclusion the radiologist would catch and say yes i see answer so a good classifier therefore it's like a good diagnostician it should be able to pick that rare event in other words your classifier must do better than the time classifier. That is a measure of learning. If it doesn't do better than the diamond classifier, it is not good. It's of course embarrassing if it does worse than the diamond classifier. So you should watch out for that. You should always compare yourself to the diamond classifier. It's often called in literature the zero-R classifier. The importance of this was obviously there's a lot of literature that have emphasized this, but we'll come to that to a few very good articles. But then the more meaningful classifier, we talked about logistic classifier. Logistic classifier is often called the logistic regression. And you have to be aware that it's a misnomer. It is not regression. It's actually a classifier and it often trips people. So under no circumstances think of it as a regression algorithm. It is a classifier algorithm. So in the interest of caution, always use the word logistic classifier. Just omit the word regression and you'll be better off. What does it do? We took the example of blueberries and grapes and their sizes. Take this example as a mental picture and we will use it for many many examples you have to determine whether a piece of fruit so let's say this well my grapes are green but imagine that they should have got their grapes so a green grape you don't know what this is is this a blueberry or is this a grape know what this is. Is this a blueberry or is this a grape? And for a moment imagine you can't see the color. So you would say that small light things are blueberries, slightly heavier, more voluminous things are grapes. And so on the weight and size axis, this is the distribution of blueberries. Now there may be some grapes that are very tiny and there may be a few blueberries that are ginormous and so in this feature space, this space you call the feature space, or the input space, in this case, why am I writing this blackboard R2? Anyone? Would somebody like to warn it here? A real number? It is two dimensional. This means that it's a two-dimensional so remember this is two dimensional and real space right a real number space right you you need not even say euclidean you may impose whatever metric you want but euclidean is a real you want but you keep it as a real valued space space though often implicitly so remember guys when you write this as a thing and it stands for n dimensional space and when you do that the underlying assumption is that it's real value continuous space when you do that the underlying assumption is that it's real value continuous space right so your data would you agree that if rate is one axis this is x1 and this is x2 what is the dimensionality of your input space it is two so it's a two dimension space and that's the notation so guys see remember that math is just an elegant and So guys, remember that math is just an elegant and abbreviated way of writing something. There is no mystery to math. If math is not self-evident, then we haven't understood it. But most of math is very simple. In fact, I would argue that of all the subjects I have studied in my life, I've always found math the easiest simply because it is a set of very obvious things. One thing follows from the other with a little bit of faith. So you can have a journey of reasoning throughout the whole thing, unlike some other fields in which it is far harder to get a report as well. Economics, for example, is terribly difficult. It's hard to get two economists to ever agree sometimes. So those are hard subjects. Anyway, so we looked at this, and we observed something crucial. First thing is we learned the concept of a decision model. So here we drew a straight decision model which is what a logistic regression takes to it it takes a linear that's why it's that's why it's called a linear method so logistic regression logistic regression is a linear is a linear classifier. What does that mean? It draws a hyperplane decision boundary. Well, in two dimensions, a hyperplane is nothing but a line, but you can now generalize to three-dimension feature space, four-dimension feature space. It will be a hyperplane is nothing but a line but you can now generalize to three dimension feature space four dimension feature space it will be a hyperplane what is the what is the defining quality of a line or a plane and its generalization hyperplane which is that the perpendicular vector points in the same direction everywhere isn't it That's the nature of it. So now, if you draw this, this big green thing that you just drew is a thing. Sorry. Let me just take a flat. Yeah, so this thing is a very crucial thing. And geometrically classification is nothing but the search for a decision boundary right or may i write that sentence that if you look from a pure geometric perspective which i find to be a more intuitive way of understanding machine learning i will say this statement classifying a. To learn a classifier, a classifier is to have to have discovered a decision boundary. This is an important statement. So it's a geometric way equivalent of this lot of algebra and so forth. It's as simple and as intuitive as that. All classification is a search for a decision. In the case of logistic regression, it's a hyperplane. But you will encounter algorithms in which it won't be a hyperplane. It will be just any manifold, any surface, hypersurface that separates it up. And not just a hypersurface, a smooth continuous hypersurface even that we distinguish it out. Now look at this, we talked about the intuition that the distance from the decision bound is a pretty good predictor of whether it's a grape or not. If you're in negative distance quite likely is blueberry, very huge negative distances. For high positive distances it's a grape or not if you're in negative distance quite likely is blueberry very huge negative distances for high positive distances it's definitely grapes isn't it so for example a huge negative distance is being here uh this point this point a you would agree that this point where where my cursor is is certainly blueberry, you're pretty certain about it. On the other hand, a point like this, B, is certainly grape, with high degree of certainty you can say. But as you go from A to D, you would realize that your certainty that it is a blueberry keeps decreasing, and your belief that it is a grape keeps on increasing isn't it you believe that it's a blueberry decreases and you believe that it's a great increases so if you take the grape as the positive case you hear the probability of a grape at a is very low as you as you keep moving forward the probability that it is a grape keeps on increasing monotonically. And the important word is monotonically. There's no reason to believe it. Your belief decreases as you grow. In fact, it continuously increases. creates a probability relationship between your belief is probability that probability that it's a a grade as a function of the distance so how do we do that so there we did a little bit of intellectual gymnastics very simple thing right and there are many many ways you could do but logistic regression is the most obvious way you say that okay probability goes from zero to one but distance goes from minus infinity to present value so how do i deal with that well let's do one thing first convert probability into odds because then it will go from zero to infinity you happen to know this and as you know common people talk in terms of odds not in terms of probability so p over the probability of it being a grade divided by probability of it's being a blueberry is the odds p over 1 minus 2 then odds are better odds go from 0 to infinity see probability goes from 0 to 1 odds goes from 0 to infinity so we are getting somewhere and now if we take the log of odds it goes from minus infinity to plus infinity which it so happens is exactly the domain of is the range of values for distance so the most intuitive theory that you can build out of it is is to theorize that maybe log odds is the distance, the probability and the distance are the same thing. And that's the logistic regression, logistic Janssen power. And this is it. And this is how I remember the distance function. And I also said that the distance function is given by an expression, which I didn't prove. It is this. But when you plug these two things together, you'll get the more traditional form of way in which this is a more traditional way the same thing is with books. The books don't tend to write the way I wish they did because to my mind, it is simple geometry. It makes it look common sense, whereas this equation, the more traditional equation like this, is, I don't know about you, but to me, if I were to encounter this for the first time, I would be scratching my head and saying, where in the world did this come from? So anyway, that's that. So this, how does it look? The probability looks like this. It is a sigmoid function. this how does it look the probability looks like this uh it is a sigmoid function when when you are deep into the blueberry territory the probability of this being a great is zero then gradually as you come closer to the decision boundary it begins to be less and less zero it begins to rise up after crossing the decision boundary it begins to cross 0.5 and move towards one and when you are sufficiently in the grape territory, you have driven into the grape territory, then the probability that it's a grape saturates around one. So the word saturates, the mathematical word for that, and you should always use simple language. The mathematical word is the asymptotes. The zero and one are the asymptotes of the log artistic vibration function. Now, that's where we start. Now this thing, the distance function is given by that. I'll keep it as an extra time, perhaps today afternoon or today or in an extra session with an announce now in this course uh i wanted to make an announcement um we will i will be giving um so today i mean we'll finish class requires today itself i will take perhaps an extra half an hour also um because there is a bit of theory that I didn't cover. I'll finish that before going into that. But then next week is, instead of doing clustering, which is the next topic, I want to do a topic called regularization, which we have not covered so far. So we will do regularization and the week after that, we'll do clustering. So in other words, I'm extending the course by one week after that we will do clustering so in other words i'm extending the course by one week simply because we have missed i mean at the pace at which we are going we have missed a crucial time so please note it down if you have been traveling plans and so forth that this course is extended by we will do regularization next time and clustering the We will do regularization next time and clustering the week after. So next week is regularization. So before I go into the lab, let me talk about another classifier which is worth learning about. And so there are many, many classifiers, but we are sticking to the simple ones, the linear ones. So there is another classifier which actually predates the logistic regression by many years. Logistic regression, despite the simplicity of the idea, is surprisingly recent. It was discovered surprisingly recent, I believe in the 70s or 80s it was discovered. There's a classifier that was discovered even before that, I think in the 20s, and goes to that data scientist or statistician, Krisha, who we talked about, let's say, in polite terms. So this is called linear discriminant analysis, LDA, and something called quadratic discriminant analysis, QDA. These are the two. And broadly, they are called discriminant analysis. If you don't want to discriminate between the terms, just call it analysis. It's based on a very simple idea. And let me illustrate the idea of the single dimension. So suppose you're looking at, let's bring back our ducks and cows, cows and ducks. Let's look at one attribute, pick an attribute, let's say weight. So if you look at the weight of a animal, would you agree that most of these ducks they would be having weights if you make make a histogram the ducks would be sitting here close to zero very small values whereas the cows what color should we get the curves let's give cow a nice okay i'll just give it this time cows have a much greater variance of ways so let's say that cows, if you plot out the weights, well, obviously this hardly looks like a histogram, but imagine that it is a histogram. So would you agree that if this is the x-axis, the one feature, which is the weight, most of the weights of the ducks would be filed here? Ducks. And this would be the region for the weight of the cows, because cows weight what? I don't know, maybe 600, 500 pounds. Anybody knows what's the average weight of a cow? Could you somebody please look it up? Can you please help me? What's the average weight of a cow? Ducks, let's say 10 pounds. So what will happen is the duck will have a mu of the duck is equal to approximately, I don't know, what's the average weight of a duck. I'll just hazard a guess, 10 pounds. And if you look at the number, 1,600 pounds of bulls. That would be 1,400. Oh, it's a cow. It's 1, 1600 pounds. Are we together? Are we together guys? So what happens is these two, they have vastly different weights. The mean of their weights are here. Also, the duck's weight varies in a pretty small band. Like, I don't know, a newborn duck may be three pounds, four pounds, and maybe two pounds, tiny little duck, and may grow up to, let's say, 20 pounds. I don't know if a duck is over 20 pounds, but let's make a guess. So it has a pretty small band variances it sort of ranges this to this let's say five pounds to 15 pounds okay five to 15. whereas in the case of a cow it may be easily this may be uh let's say variance of the weight is somewhere from i will just hazard a guess, 1000 to 2000. Just the core weight here. Does this look reasonable guys? It would help actually if some of you gave me some feedback so I knew that I'm making sense to it. So I knew that I'm making sense to it. Are we together, guys? Anyone? Yeah. So if you now want to make a classifier, you can see that and you can say, well, it is just dead simple. All you need to do is draw a decision boundary. Take these lines. take this line here let me just call this the new one this line and find its perpendicular bisector this line find the perpendicular bisector which would be probably here right this point because it's a one-dimensional data only the weight this point which is the midpoint between the two mus is a good separator between the two isn't it so mu the decision boundary right or you can just say that the decision bound is at mu of the duck the center of the duck duck hill right plus the center of the cow heap divided by two would you agree divided by 2 would you agree i hope this is simple intuition just go and pick the midpoint between the two hills oh i should shouldn't it be like the mu of cow minus mu of ducks by two but it's the same it will be the same okay so it will end up hitting so no as if i think raj was correct mu q minus mu duck by two i think perpendicular by sector that is true so think about it this way is, so this part is mu duck, right? And now this, if you do this point, like this distance, if you go half the distance here, you are saying that it is mu cow minus mu duck divided by two. But then to get to this location from the origin, you still have to add the mu duck. So add mu duck to this and it will become, what will it become? Mu cow minus mu duck plus two mu duck over two, which is the same as this. Are we getting it? Because you have to... Yes, we got it. Sorry, we thought it's from mu duct we calculated. We never thought it's from zero. So, this it turns out, now let's generalize it to higher dimensions. We remember that we didn't just take the weight of the cow and the duck, we also looked at its size. So let's go to two dimensions now. Our little feathery creatures, their new size is the centered here. As you know, their weight and their volume will show a normal distribution, a bell curve distribution. So you will end up with a, should I use an intuitive term, a bell hill, a generalization of a bell curve this is a bell curve and this also both are bell curves if you do a histogram of the weights of the weights they will the histogram will roughly look like a bell curve so what do you call a two-dimensional bell curve a bell hill let's call it the technical term is not well technical term it's still a normal multi-dimensional normal it is still a normal distribution that's a technique of Gaussian distribution. Except that it is 2D. 2D Gaussian distribution. And well, this is the weight axis. This is the size axis. Well, as you can see, ducks are small and light, but cows are big and they will be centered around here and so they will be clustered does this make sense guys so that if you were to project this make sense guys so that if you were to project the data on the weight axis it would be roughly a bell curve here it would be here it would be also a bell curve i'm just inverting the bell curve so that i don't go about overlapping things this would look like that a far wider that far wider bell curve here. This is great. Would you agree? And the same here. Here, a bell curve, blue. If you project it on the, if you take the shadow of this data on the alarm, it's called projection. Project it on this, you would expect the bell curve with the belt curve with the curve to be there isn't it so common sense says that hey for one dimension we took a we took this point as that as the decision boundary the same thing we can apply to the second point what do we do we take this so when you take this now we know one point in the decision boundary but that is not enough we need to in two dimension we need a plane we need a line so where should the line go let us use the common should i make the line tilt let's say like this is this a good tilt for the line as a decision boundary or should i pick a line that is like this so should i pick the second one better and why is the second one better the second one better because the second one exactly bisects to the plane to be true the first one is like it's nearer to both of them so what you say is you know if you take a different sample of data you are much more likely to make mistakes with a right it's too close to the data. You want a plane, a decision boundary that separates the data away from itself as much as possible. And because that will be a more stable solution. You know that if you take different samples of the data, you are not going to, your decision boundary won't change if it happens to be B rather than A. Isn't it? And so you say that your decision boundary is the perpendicular bisector of the line connecting the two, isn't it? Well, this is imaginary decision. This becomes your decision boundary. Very simple intuition, as you can see, not any complicated mathematics here. It's the decision boundary here. Just take now what are we saying in this way of looking at it what exactly are we saying let's look through this we are saying that it is this is if this is a decision boundary and once again our this point mu of the decision boundaries point this point let me just call this point point um p p is that uh now you know that mu now comes an interesting complication this mu this let's look at the bell this point has a x1 axis and a x2 value right it has a value along the weight this the average weight of a duck and the average size of it so you can say that for the duck mu of the duck is now a vector it is mu 1 mu 2 would you agree for the duck mu 1 mu2 duck, let me just say duck. Likewise, you would agree that this is a point. It has xy coordinates, right, or x1, x2 coordinates. So mu of a cow is also a vector. That is mu1 of a cow, mu2 of a curve vector, isn't it? And so you're saying that the point P, again is exactly the same reasoning applied. It is mu1 dot plus mu1 curve divided by a half of and this is mu two curve plus mu oh sorry mu two duck doesn't matter I'll write it just for consistency would you agree that is the average of the two vectors right the point and the direction of that is obviously the perpendicular intersection so when you use this approach you are doing linear discriminant analysis and that is the so there are some underlying assumptions to this. One of the assumptions that you have is A, so assumptions. The assumptions are A, that in the feature space, the points, the data points for each class data for data for each class what are the two classes here cow or duck show normal distribution uh normal distribution this is an important assumption would you agree guys this is one underlying assumption for example this assumption doesn't doesn't quite hold for our grapes and blueberries. Do you realize that? If you look at this example of grapes and blueberries, do you feel that all the blueberries are nicely forming a building in this particular case? They don't seem to be in this particular case or this particular sample of data. So that is why it's an artificial example because in reality they would. The grapes also, they're separated out separated but i wanted to illustrate a pond they seem to have lost this very much i think i have to wait for this thing to catch up so this is one assumption. The second assumption is E. The hills have each the same variance. What does it mean? This is the, let's say that this is for the duck and this is for the cow. You just make you you know that the variance of the cow is much more of weights of the cow is much more than that of the duck but as a simplified assumption you say that the sigma square of duck is approximately sigma square of the cow means both the classes have the same variance. So remember that assumption may not be true, in fact is not true in our case of curve syntax, but linear discriminant analysis, it just simplifies the problem. It says that assume that they are the same and then just find the perpendicular bisector of their centers, the central variance and that is your decision. when you make these two assumptions you are using the linear discriminatory system so it tends to draw it draws a linear hyperplane through the data So the data. So this is important to address. The second thing and we are almost done. If you violate the quadratic disk analysis basically says here each hill can have its own separate on separate that is sigma duck is not equal to the variance of the cow so if you make this assumption now you're in a bit of a trouble because see in linear discriminant analysis you need to find just mu 1 mu of the duck mu of the cow because see in linear discriminant analysis, you need to find just mu one, mu of the duck, mu of the cow vector, and a common sigma squared. But here, so three, there are three variables that you need to evaluate. In quadratic discriminant analysis, you have four variables. There's the mu cow mu duck there is a sigma cow sigma squared so there are four values and what does it do in the feature space what it does is this makes and you can take this as a fact but we won't have time to prove this this makes the decision boundary a quadratic curve so what are some quadratic curves a quadratic curve or hypersens so in two dimensions can you can you mention what are some of the quadratic curves? Like a parabola? Yes, parabola. Ellipse, circle, hyperbola. That's it. So it will be any one of these. These are two-dimensional surfaces. so your decision boundary won't be straight so this is something to know and as you can see the derivation of reasoning through a linear discriminant analysis is a pretty straightforward and we quickly went through so in this course we will stop because we are only doing mostly linear methods we do that now classifier is a rich literature it turns out that classifiers are by far the most commonly used machine learning variables the real world all sorts of situations spec for a classifier a predict as a predictive problem so how many classifiers are there, algorithms are there? Practically countless. If you, lots and lots of new classifiers keep showing up in the community all the time. If you're in a hurry to publish a paper, go create your own classifier. You want a quick PhD, go write, do some, come up with a very interesting classifier that meets the state of the art lots of classifiers the same is true for regressors but there are many many regression methods though i would say that in the classification literature is a little bit richer are we together and the range of classifiers are and regressives are predictive. So these are all called supervised learning. See, whenever you make a box in which you put some input and you're predicting a value, it's called supervised learning because you need to train, you need to make the machine learn from training data. Whenever you're training the machine on data, and then once it is trained using the prediction it's an example of supervised learning i don't like this term supervised and supervisor so i tend to just say that prediction boxes are regressors and classic powers right prediction machines are regressors and classic powers now that is that for today. Now let's take a five-minute break. We have been talking now for an hour. If there are any questions, please ask me. Otherwise, we'll take a five-minute break, and after that, we'll launch straight into the labs. and pausing the recording so we did linear discriminant analysis quadratic quadratic discriminant analysis, and logistic regression. We will use this in our labs. Now I must say one thing that the LDA here, the LDA in machine learning has actually unfortunately two different expansions. One is linear displacement analysis. And the other is a topic that we will learn one day when we do natural language processing in your ML500, which is called latent, often often used for eg topic modeling you use it for topic modeling so be careful which of the two this acronym in different domains are different or different expansions so So just be on the guard for that. All right, so with that being there, now onwards to the labs. So first, okay, precision and record. Let me just explain this. The best way to do that is, look at this picture. Let us say that the green side, the left-hand side, are all the dots that are truly positive. So green zone is positive zone. The gray zone is the negative zone. All of this data on the left-hand side of this square are all genuinely positive. And all points on the right-hand side are genuinely negative. And let us say that your algorithm, what it did is it managed to call this subset of points inside the circle, the dark green points, as positives. And it managed to call these points here right also as positives so now what has happened if you think about your algorithm which parts are mistakes which of these two parts if the if it calls everything inside if your So these parts are mistakes. Which of these two parts, if it calls everything inside, if your algorithm calls everything inside the circle as positive. So it made two kinds of mistakes. First, it identified a lot of points out here where my mouse is in the pink area. They were actually negative, but they are marked as positive they are predicted as positive that's a mistake isn't it so those are false positives but at the same time these points were negative these points were truly positive you know because the whole left hand side is made up of positive points but your classifier missed all of these points because it missed all of those points it marked them as negative and so it ended up with a lot of false negative these points are actually positive it marked it as false negative so in a very geometric way see there are formulas for precision in recall but i always like to one advice that i would give is this is perhaps the most intuitive picture i have seen of precision and recall definition precision is how many true positives you found out of everything that you called positive so in in other words, if your precision is high, you will have a low false positive rate. So it's precise, it manages to pick up the right, when it calls something positive, it quite likely is positive. Recall on the other hand is picking the proportion of the positives that it found compared to all the positives. How many positives did it find? So suppose they were in a population of 1000, let us say that 10 people had cancer and your classifier found three predicted three as having cancer so it missed the other seven so what would what would be the recall be three by ten point three three by ten right because three out of ten it got on the other hand it marked let's say that it got three of those positives right but it also marked four of the negatives as positives isn't it so what would the precision be three by seven three by seven exactly So it marked a lot of, it also had a lot of false names. So now let's make it all real. See what happens, say, is in medical, no test. Medical is a science which is quite soft in the sense that human body is profoundly complex. There's a reason. I believe at one point it used to be called the terra incognita, the unknown territory. Much about the human body is still not known. Despite all the progress of science and medicine and miraculous, amazing, amazing progress, still most scientists believe that we are barely scratching the surface. There is a lot about the human body, about all biological processes that we don't understand at all. Basic things we don't understand about it. So here, even questions like do you have a particular disease or not, one test doesn't work so the way you do that is you you take a you generally have screening tests in the US at least you have the first course of action is that they will do a screening test let's say it's cancer you would do a turn off positive, then you just become a suspect. There's a suspicion of cancer. Then you're sent to another test which confirms whether you have cancer. So tell me in the screening test, which would you aim for high precision or high recall if you had to pick one and get that right which is more important precision or recall or accuracy articulate some guesses guys. Precision. Precision because you want to, the ones that you mark as having cancer should have cancer. But precision doesn't account for the fact that you missed a lot of people who did not have cancer i mean who had cancer that you missed them the false negatives remember precision will not be affected by a lot of false negatives it is only affected by false positives but what will happen if you have false negative is that that would be catastrophic isn't't it? Because you send a lot of people happily home saying you don't have cancer, and by the time they come for the next checkup in six months, you have progressed to a pretty advanced stage of cancer, doesn't it? So for a screening test, precision does not seem to be as good a measure to shoot for. What would you shoot for? not seem to be as good a measure to shoot for. What would you shoot for? Accuracy also is not terribly useful when the data is so asymmetric because a very rare number of people have cancer. It turns out recall is because what recall looks at is the proportion of true positive amongst the positives like of all the positives how many did you find as positive even at the expense of false positives a good good screening test will capture all the positives as much as possible but it it may make mistakes. It may also mark a lot of healthy people as positive cases for cancer, isn't it? So in a screening test, you would rather have high recall. Guys, are we understanding it? In a simple screening test, you want a test that has high recall. It shouldn't miss the positive cases. So recall is what? Proportion of what you found over the total number of cases. So when the recall is close to one, what does it mean? It means that you caught most of the cases. Are we together, guys? In screening tests, you want that. Are we together guys? In screening tests you want that. But after that, when you're going for a confirmation, you know that by now a lot of the negatives have already been weeded out. But in the final test, which is right before surgery or aggressive treatments like chemotherapy and radiation, what do you want to do? You want to have, you want to weed out what? The false positives. So you want to make sure that the ones that you mark as positive are truly positive. You don't have false positives. You don't want to put a healthy person through massive doses of radiation and chemotherapy okay so as a final diagnostic test before aggressive treatments are imposed you want high precision guys am i making sense Yes, Asif. Yes. That is it. So that is the intuitive difference. One is not superior to the other. It is based on the use cases. And do you notice that between both of these use cases, the one measure that wasn't terribly useful was accuracy or error rate or its converse versus the error rate so it is it is rather unfortunate that many many textbooks and this and articles and blogs and all these kegel notebooks when they do classifiers they tend to be a little too fixated on accuracy what is the accuracy that they're doing accuracy? What is the accuracy that we're getting? That generally, and it becomes a habit, it becomes like the p-value that statisticians develop for quite a few years. You shouldn't think like that. You should think about the problem and ask for this situation, what is a good measure? What should I shoot for as a diagnostic measure? You have many. For example, you have precision, recall, you have accuracy, you have error rate, it's called velocity error rate, and you have area under RLC curve. Right? So you have to consider all of these things together. And pick the best one. For screening test, pick pick high recall for the final test before doing aggressive treatment do a highly precise test that's that and ask yourself in the beginning of analysis which one you should do so all right with that any questions i'll end the theory now i'm going to move into labs With that, any questions? I'll end the theory now. I'm going to move into labs. Asif, how about doing the test twice or thrice to confirm it? Do you think that it will lead any result or it's just a stupid thing? It's like idiosyncrasy, expecting different result each time. See, there is instrumentation error. Unfortunately, in medical, because these are biochemical reactions, and they work only under fairly strict laboratory conditions. The fact of life is every laboratory has a certain error margin. And that error margin is usually pretty bad actually. For example, in India, the COVID tests are very unreliable. They get it wrong quite often, 30% of the time. So they mark a lot of people who don't have COVID as having COVID, they mark a lot of and they miss a lot of COVID cases. Even in the US, the numbers were better. But even with a very sensitive RT-PCR, there was still a sizable error rate. And the recall was not good enough see a good RT-PCR test would have caught all the positive cases at the expense of some false positives but I from what I understand there is it isn't perfect it's good but not perfect fortunately it is pretty good. The record is quite high. So if it says you have COVID, you probably have COVID, right? Or you don't have COVID. Most likely it got it. But it may also be true that you don't have COVID. It's a false positive. So false positive rates are there. These are chemical tests. They're based on many, many factors. So remember that you do the same test in two different laboratories, you often get two different values based on how skilled the technician is. And so in the US at least, every laboratory by government is required to give their NMR. There's a quality metric and they have to declare what is the NMR. So for many blood tests, it is standard to send the patient for tests multiple times, at least three or four times to confirm. See if you have only one test that you can do for that disease, then common sense says do the test at different periods of time, at least three, four times before you come to an answer. But fortunately for many things like cancer, there are multiple modalities of testing. There are many, many different things you can do, some more precise than the other. So for example, in cancer what happens is you do, the most aggressive thing you can do is core biopsy. You can literally go and take a piece of that organ or tissue, take it out, put it under the microscope and see where you have it. But it's pretty invasive. Getting a core biopsy done of the liver or any other organ should not be your first course of action. Or doing a core biopsy of breast would be pretty, pretty traumatic. So you do it as a final test before you go into a radiation or something like that. You instead do a screening test, but you hope that the screening test has a high, high report, even at the risk of producing quite a few false positives. So that's how it goes. And sometimes things happen. Like for example, if you have a chest congestion, or you just happen to have a little bit of something, you go for an x-ray and it may look as though you have cancer or some problem in the lung. Two weeks later, you get the same screening done. It might all be clear. So human body is complex. There are no definitive answers to that. Any other questions, guys? So I'll start the lab with something that I wanted to show you. We talked a lot about correlation and what correlation means and so forth. that I have created. Where is that gone? Let me start it again. Localhost 8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8 Oh, by the way, I must mention that I've released a lot of notebooks now. The total number of notebooks there on the website now is 25, which means I highly encourage you if you want to get the most out of this course to go and study all of this code. Understand all of them I have released. So lots and lots of them, some of them I have released yeah so lots and lots of them some of them i have not covered for example a florence nightingale is just for you to have fun to see how beautiful visualizations can be created landmark visualizations can be created but i've released a lot of things that it is worth studying here there are 25 notebooks and by the time the course is done, the number is likely to get to 30. Actually, not 30, but I've released about 30 notebooks. So it will go to 40 notebooks. So please keep studying the notebooks. I can cover only some of them in the class. I can't cover all of them in the class. So we'll take this data. Once again, what do we do with the data we load the data so guys this notebook is already on the course portal and you can i would ideally like you to go and follow this, do it on your own, because my walking you through, today I'm walking you through in the interest of time, because we have a lot of territory to cover. i trust that you are doing the homeworks if you are not doing the onwards then it's counterproductive practice makes a man perfect so unless you practice and you say do things on your own it is no use there's no point in watching uh world champions play tennis you won't become good at tennis you You have to go to the field and play tennis on your own. You see the point? And most mathematical disciplines are about practice. Once you understand, first you learn, you listen, you understand, you learn, and then you practice, make it a part of your life. And then it's going, so making that. So I take this data, by the way, I added a little bit of style just to show that we can add styles to things. Just to make it look prettier. So here are values. It contains three fields, x1, x2 and t. T is the target variable. T you have to predict right now t in this particular case is either 0 or 1 but it's not obvious from here if you looked at a few lines of data you will see so what i am doing after you look at the data right this data by the way we can open in we can open in let's open it in a tab and when you open it and see a few rows you can see that it is do you see what are the three columns of the data x1 x2 and t like x1 here is 6.227 x2 is 5.372. But if you look at t, its value is either 0 or 1. Always 0 or 1. So it is the target variable. It's supposed to be, you can think of it as cow or duck. Are we together? That's your target variable. So let us visualize this data. And one of the things you should do is of course convert t into a categorical variable such as but I've skipped this I'll let you do it. When you visualize this data you again make a scatter plot just as we did except that now you can specify the color of this. You see the color of each of the dots it should be based on the value t t value of that record the third column t value so this is it the rest of it i hope you know s is for size so what happens is if you just make a vanilla scatterplot without prettifying it it doesn't look so beautiful. One of the things that I've been emphasizing is if you want to create publication quality work, you should always spend some time on the aesthetics of your graphs, of your visualizations. It matters a lot. So here we go. S is for size, alpha is for what in the world is alpha for? Does somebody remember? Opacity. So this is not size actually. Let me move it here. This was size. This is opacity. Opacity, transparency, whatever you want to call it. And C is the new thing we introduced. It is data, which column of the data should you base the color on? So you're saying pick the color based on the value of C. And now I'll introduce you in visualization to the concept of color maps. So when you visualize colors, one of the things in data visualization that happens is you have to be sensitive to the fact that 30% of men are colorblind to different degrees. And I don't know if it is prevalent amongst women. I'm told it's relatively rare, But 30% of men are color blind. So you need to pick colors that they can separate out. They can't see certain colors. It doesn't mean that they don't see any color at all. They just can't see certain colors properly. So you have to pick colors when you are separating things out, sensitive to them. So Matplotlib and all libraries, they recommend that don't randomly pick your own colors. Instead, pick colors from very safe, known color palettes. Those are called color maps. And matplotlib has a pretty rich set of color maps. You can pick any one of them. Red, blue. RD is for red. BU is for blue. Obviously, you can see red and blue here. So let's go and find all the color maps. So here we go. Color maps. Tutorial or classes. There are lots of color maps here. So you can have a sequential color map. By the way, veritas is pretty popular. Quite often you see it being used. Now you can have a monotonic color map like yellow gradually going to red or red blue where is my red blue red purple so no it's not here see it must be where is my red the one that i'm just using and you can have three colors yellow orange brown and so forth red purple blue green purple let's see maybe sequential map 2 will have that it doesn't have that okay divergent colors will happen so you can see that there are different color maps you can choose right red blue yeah it's very divergent if you use a divergent color map then the things will really stand out away from each other. You can have cyclic color maps and then you can have discrete color maps. If you know that you're going to have only a few classes, you can pick things from discrete. By the way, this tab1, tab2, tab3, tab4, these are the ones that the software Tableloid, tabloid, tabloid tends to use. So, and then there are miscellaneous color maps. You can use whichever one you like. So there are many, many color maps and each of them has been carefully thought through, validated, and when you create your visualizations, use a known color map. It's a, you can play around. I'll let you discover the beauty of all these color maps and find which one strikes you as beautiful and what you would like to use. These are examples, use that. So we learned something about today. We learned color maps. The rest of it is simple. The x-axis, the y-axis. Yes, as I said, I use LaTeX. I don't know, did you all install LaTeX? How many of you installed LaTeX? Anyone who installed LaTeX on their machines? the machines. I install logic. Very good. Very good. It does make your documents look better, isn't it? So the next thing we will do is we will again use the red-blue palette, but we will draw just the scatter plot. And these are called is like these are smoothed out histograms these are called kde plots these smoothed out histograms are called kde plots now kde is a topic that merits its own little discussion i will do kde these are called density kernel density estimates it's a lovely lovely thing. Its intuition is very clear. If your histogram was smoothed out, what would it look like? If you think that histograms are trying to tell you about a shape, what shape is it? That is called the density, KDE. So, by the way, this is a way to create a multiple plots code to do. Very simple. You say in the diagonal here, draw the KDE plot. So you see the diagonal has KDE plot. Along the upper, above the diagonal put the scatter plot, below the diagonal put the two-dimensional kd plot right so what happens is it will make contour lines the contour lines of where each of the types is more dense so once again guys i won't get time to do visualization you have to pick up visualization if you find the code hard please sit with us in the set with the teaching assistants they'll be very happy to help you or explain you and if you look at the documentation of it it becomes self-evident so that's the visualization code now we break the code into the train and test. First we separate the predictors from the target region. We always do that remember and then we separate the data into two tests. We need to hide the test under the so that the algorithm doesn't know about it while learning. then we take a logistic regression classic by the way newton uh conjugate gradient is a solving method you don't need to take it even if you just say logistic regression it will by default take something what do we do with our algorithm from regression you're familiar with it all we have to do is take the algorithm and call dot fit on it or fit with the training data. When you do that, it will and then you can use it to do predictions and it will give you the predictions. Once it gives you the predictions, you need to, you can compare the actual value with the predicted value, isn't it? How do we get the confusion matrix? By looking at the actual value and the predicted value and see where they agree and where they don't so when i when we look at this if i do did i do it in a more visual way no okay you come to that right when you look at this what does 295 suggest this this diagonal most most of them are true positives and true negatives. But there are how many mistakes are there? 25 and 26 added together, 51 mistakes. Isn't it? So what we have here is precision and recall for each of the classes. The precision and recall in this case are pretty high. Now I'll introduce you to a new concept called F1 scoring. F1 score is a balanced, like when you have a situation in which you don't particularly want to root for or emphasize precision or recall separately, you may say this is a case in which I care about recall or this is a case where I care about precision. But in general, if you don't care, like in this case, blue dots versus red dots, right, you don't really want to emphasize one versus the other. So then a very good metric to shoot for or improve is the f1 score right f1 score is the harmonic mean between precision and record do you remember what harmonic mean is from school algebra i'll just mention it for a moment harmonic mean is algebra I'll just mention it for a moment harmonic mean is so let's let's talk about the various means mean right arithmetic mean arithmetic mean of two numbers is x plus y over 2 this is the definition would you agree geometric mean is defined as x times y square root right which is the same as xy to the power half. Harmonic mean tends to be 1 over harmonic mean is equal to 1 over x plus 1 over y. And I believe there's a factor of half am i missing a factor of half here one of you if you could quickly i don't think there is okay maybe there is a factor of half so it is the hormone i don't think there is somebody i think it's into two i think no uh there is a half so this will become what does this become this becomes like h is equal to so if you solve this equation this becomes h is equal to 2 times x y x plus y but I tend not to remember it this way. I tend to remember as half of the reciprocals, average of the reciprocals. This is harmonic mean. So when you take the harmonic mean, so f1 square is the harmonic mean of precision and recall this is it how would you feel this is its definition so when you don't particularly want to emphasize one or the other, take the shoot to improve, train your algorithm to optimize on the harmony. So here the F1 score is for both zeros and one is pretty good. Ninety two, ninety four close to weighted average is doing pretty well. Precision recalls are all it means that this classifier is doing pretty pretty good now what is this classifier do it's a logistic regression So what will it find the data. R. Vijay Mohanaraman, Ph.D.: In this data, what will it find guys. R. Vijay Mohanaraman, Ph.D.: It will find. Divide. R. Vijay Mohanaraman, Ph.D.: It or. R it will find a divider right zero or one right it will try to find a line like this isn't it would you agree yes so okay now the trouble is i annotated it and I don't know how to undo that. This comes from Zoom, so please give me, oh, Claire. Well, Claire didn't help. All right, so let's go and find the decision boundary in the data. Your logistic classifier has found a line. That line will have a slope and coefficient. Would you agree that it will have a slope and an intercept? Yeah. Right. And and yeah so its values are given as so the coefficients of this are this and if you work out the slope classifier coefficient classifier intercept y what is the equation that it has written right the equation that it has written is the property, ultimately log p1 minus p is equal to beta naught plus beta 1 x1 plus beta 2 x2. This is what it has written. Do you see this, guys? So I have this value. I have this. These are the coefficients, and this is the intercept. Right? So I have this value, I have this, these are the coefficients, and this is the intercept. Now, you see that. Then one of the things I said is that actually classifiers come up with a predictive probability. That any point in the feature space, what is the probability that it is? Is the intuition clear guys if you create an algorithm here what it is doing is this may be your decision boundary okay this may be your decision boundary but it is also true that if you take any arbitrary point x you you will it you will have a probability a probability of x of being great so any data value here with that size and that weight you have a certain degree of belief that it is a grape or not but here you would probably have less than half the 0.3 is your belief isn't it so for each point for each point uh i like this statement for each point in the feature space R2, for each point let's say x in the feature space there is a probability there is a probability x of it guys is this by now after all the discussion am i saying something pretty obvious so every point there's a certain probability, if you want to find the probability and the good thing is that the scikit-learn, the artistic regression actually comes out with those probabilities. So, you know, you don't need both the probabilities. If you ask the classifier to give you the probability at a point, it will tell you the probability that it is a grape and the probability that it is a blueberry you don't need both why because the two add up to one exactly so you just need one you can pick the zeroth column or the first column doesn't matter so you take that you get the predictions now what can we do you can use this to visualize the predictions what we'll do is we'll we'll do some visualizations, all of these values being there. First of all, there is a library, yellow brick, very convenient. It can do all of your, remember we learned about confusion matrix, classifier report, and area under ROC curve. Now there is one more thing that we'll talk about, feature importance. classifier report and area under ROC curve. Now there is one more thing that we'll talk about, feature importance. How much did in making the prediction, how important is the X1 feature and how important is the X2 feature? And the reason that matters is sometimes, for example, if you have diabetes and predict somebody has diabetes, how much does the, how much being over eight matters versus how much does some other factor, let's say eating, you know, certain how much salad you eat every day. So the two are both will affect whether or not you have diabetes, but one will have more impact than the other. So that is it. So that is feature importance. So when you look at this, this is a plot I like, actually. I like to do this visualization because in one single shot it draws everything. This is the classification report. Then by the way, it also puts t there but ignore that here. here then oh no it takes the average of the t not t there where am i did i yeah so it gives you the 1 and 0 and it gives you the precision recall right and their averages now it also gives you the confusion matrix here is the confusion matrix how many cases did we get wrong? 25 and 26. Do you remember? But it puts it as beautiful colors. It says that in making the prediction, the second variable is a little bit more important than the first one. Maybe, let's go look at the data. There is more variation along the second variable than the first one. This one rather than this one. Because along the x1 axis they have much more overlap than on the x2 axis. They are more separated out, so easier to make, to use x2. If you look at the area under ROC curve, where is the area under ROC curve? i have i don't know is it visible guys yes the roc curve would you call this a good curve or a bad it's a good curve because it has a pretty high area under roc in, the area under ROC is 99%. It is hard to beat that. So you have a pretty good class. Remember ROC curves, the axes are false positive, true positive. It achieves true positive rate pretty quickly. So now, getting back to zooming out again. Let's visualize the model predictions. So one of the things we do, that I do actually, and I would highly encourage you to do, is take this, and at each point, color it by the probability that it is a blueberry or a grape and the way you do that is you say that if it is a blueberry then mark it like this blue if it is a grape then mark it yellow right and so this is what we have done in this particular case it is the probability that it is a blueberry is high, so blue, otherwise it will be a grape is high than market. So you look at the feature space, do you notice that as you come closer to the decision boundary, which is the white line, the blue color becomes lighter. What does it signify? Anyone? That's most probably could be a gray. Yeah, you're getting your belief, your confidence that it is a blueberry is decreasing. And then somewhere in there, there are white dots, it means you're literally sitting on the fence, you don't know whether it's a blueberry, or it's a grape. And then as you cross over the decision boundary gradually you begin to feel more and more that you are in great territory so how do you figure out right see because i dotted to you with blue and red i made sure that the visualization deliberately agreed with that. So what I did is, if you notice I picked the column for blueberry, this is one of them. This column, if I had made it zero, the picture would reverse itself. So it is like, you know, this thing red blue. So then I would have had to invert it by saying underscore r and it would exchange the color of the unit. So these are just visualization trips. So guys this code, I hope it looks simple to you, but here the important thing is color comes from a continuous variable. It comes from the value of the prediction. Where did we predict? We use this as prediction. So guys, I wish, usually before the pandemic era, everybody used to be here in class in front of me. We used to have a very interactive discussion and I would look into people's eyes and see whether you're understanding it or not. But we are in a new world. So it is a little hard for me to get a sense whether you guys are finding it easy or hard do i need to repeat myself are we finding it easy yeah i'll see if i am i think i missed something important so we started this thing with uh three columns right x y and t t is uh either t is the binary one right target is a boundary so at what point or how did you figure out that the prediction is right or wrong where is that data coming in from see here so what happens is i took the data i visualized it so clearly i see that logistic regression will work because you can imagine a linear decision boundary through this. So what happens is you do all the histograms, this is just data visualization. Then first thing you split the data into test, training, this is all. Then what you do is you build the classifier and you make predictions. right? Then comes the model diagnostic. Here is when you know that you have a good . What are the model diagnostic things that we learned in theory? The confusion metric, the accuracy, the precision, the recall, and the . Here is my problem. How did you know that a particular datum is right or wrong? Whether you predicted it as a grape and it turned out uh as a blueberry well you can you can do that you can emit out how many the the exact data points where it made mistakes you can you can produce a list like where why it's just another line of code see you know the actual value y test the ground truth and you know the y predictors so where did you get the ground truth from oh because you split the data look here you split the data into two parts yeah but it's the same data right you you just split it into two random sets yeah you split it into x train y train and x test y test right you build a model on the training data are we together then you take the model and say make a prediction on x test so it will come up with y hat or the predicted value y predicted but you also happen to have y test next to you and you can compare the prediction to the y test which is the ground truth no i think i'm very confused right now okay i mean the it's it's not the predictions so let's say you predicted whether it's a grape and it turned out to be a blueberry where did you get that information from that you're wrong i'll just let me draw it i don't want to take too much time but this is important i think i'm missing it so see here is the data set data the first thing you do step one you separate out the x from the y the target really here y is t and here x is x1 x2 this is step one wait wait why is the target why is the target? Why is the target variable? T. In our case, it's T. T for target. It was like, your job is to predict Y. Job is to make a model that can predict T. In this case y it is convention to always represent when you are writing code the target variable as well it is the t column because your data comes as x1 x2 t right oh maybe i thought x is blueberry and and y is uh no no why doesn't exist in the data so uh you have to infer i told you that remember that in this the y is called p right so you split it out okay and now what is your job so now you do this data now you take split x into x train, y into y train. This is your training data. You also have x test, y test. In other words, y being the t. In this particular case, it's t. Here also it's t. Now it's the t column. You separated it out. You now build a model. You build a model which works for training data learns from training data yeah i mean i get this part i it's not the explanation that is a problem it's like so so t indicates whether you are right or wrong what does t indicate wait a minute right see this is the ground truth when you look at the test data you know that for each of the x this should be this is the real ground right let us call y if you look into the file the values are zero and one so in our mind we can call this blueberry to help us and this is the berry we can we can imagine that this is the grade this is our imagination imposed upon the data now this is this is making some prediction i passed it x test what will come out y prediction it will make some prediction on the test data but what do i compare y predict to for each value of x i happen to already know the y so i can just put y predict here actually let me mark it as y hat right predictions as i said y hat so you would agree that for the test data i can make a three column data set now x is x i already know this is x y already know y test i'm not writing test test test as a substitute and then this is the prediction how where did you get the prediction from from here from your machine you got that right um you use the machine to make prediction on x test yeah yeah i i got that part so now all you need to do is compare the y pred with this so suppose it says zero right it was supposed to be a bluebell and you predicted zero here it was supposed to be a bluebell and you predicted zero here it was supposed to be a grape and you predicted it as bluebell so this one was right this was a mistake so the the third column that t is the column which says uh okay now i understand so t is the ground okay so so one and is the, is the same as blueberry and grape or right or wrong? Blueberry, blueberry. Yeah. So you predict it and then you compare it with the actual T that was in the column and then see how close you are. And okay. Understood. That is, and that is why you notice that model diagnostic if you notice this line element number eight it starts with the entire confusion matrix is built out of comparing the y test which is the ground truth with the y prediction isn't it you're comparing that. Yeah, understood now. The whole picture is there. Yeah, thanks. Everything follows from because this is the ground. Everything follows from that. So guys, anything else? Anybody needs explanation? So you can divide the feature space. Up color the feature space by the probability that it is a great. And now. Suppose you want to build you want to color the whole space so you know for each point each actual data point you said how confident you were that it is a grave right these were the actual data points but you know this entire space has a lot of empty space. Any one place you should be able to make a prediction. So when you fill the entire feature space with predictions, it becomes this beautiful graph where you can literally see how your classifier is acting and you can also see where it is making mistakes. Do you notice that the further away you are from the decision boundary, the more deep colored it becomes? The bottom right, bottom left is deep blue, top right is dark red. Isn't it? So for classifiers, I would very much encourage you, especially when you're looking at low dimension spaces, very much encourage you to make this plot. Many of these plots actually, once again, I would say, people don't take the trouble to do it. I mean, many textbooks don't even mention it very well, but make it a habit to draw these plots to see what is your model trying to tell you. So if you got this one right, and then the ROC curve, if you just want to draw the ROC curve, you can say it is very good. And by the way, if you want to draw the decision boundary as a straight line, this is somebody asked me to do that. We don't often do that. Well, here it is. Here is the prediction decision boundary. Do you think the decision boundary is doing a good job? Yes, this is it. And by the way, the reason I mentioned it is I wanted to show you this little bit of visualization code. I won't go into it because it's self-explanatory. Do you notice that I put an annotation here, decision boundary? So Matcraft gives you the ability to annotate. It gives you the ability to do all sorts of cute things with your visualization. But this was just a question. Now, because it's a linear decision boundary, the other algorithm I could have tried is linear discriminating basis. Isn't it, guys? LDA. We learned about it. it now this part is easy if you try lda the code remains exactly the same the only change is you change the name of the algorithm do you see this instead of calling it logistic regression you're calling it linear discriminant analysis that's one of the beautiful things about scikit-learn it's a very consistent library when you use this oh did you notice the error in fact it tends to make a it tends to do a little bit better than the old one but it might be fluke or it might be generally better it's just a little bit better enough to see one person better right which may or may not be a fluke once again you can do the same, compute the y hat for the predictions. You can do the ROC curve. Once again, would you call the ROC curve a good curve? One would, right? And you can do again for each of the points that are actually there, you can ask what would you predict it to be you can see what it is predicting it to be and then once again you can project it up to there you can draw the lda decision boundary probability map and so forth and you can once again draw the decision boundary area decision boundary and this is the decision boundary it is decision boundary, LDR decision boundary, and this is the decision boundary it is in. So nothing too different. The feature space etc. So this is for the classifier data one. Let's take a five, let's take a 10 minute break and then all of this code is there on your website class portal. In fact I've released a lot of notebooks by now dozens of notebooks 20 25 notebooks please please take time to study it practice it become very familiar with it i will Now let's move on to the second dataset. You'll notice that our pattern of analysis, the steps we'll follow will remain exactly the same. And this is one of the things you should do. Practice more and more and have the same set of steps. And then at each step, ask, what am I seeing? What is it that we are seeing here? So with that, I load the data, do exactly the same thing as before. And then when I come here, how many classes do I see? There are four different classes isn't it t the values of t there are four different values so can i use logistic regression directly well it turns out you can but the way it does it is that it does one versus the rest and so on and so forth so it does internally multiple logistic regression models. But we will. The lda is far more efficient, because you can still find the Center of these things and drop up particular bisectors I hope i'm making sense, even if there are many, many areas of data like like here this is one area of data this is another what can i do i can connect the centers and find the perpendicular bisector isn't it and i can have this for this entire region for a this entire region for b this entire region for c and this entire region for c this entire region for d isn't it i can partition the space accordingly right and of course outsiders so forth i hope i'm making sense guys that you can always use the lda method for even if you have n classes it's very natural and they do you notice that how data does show a normal distribution along each of the axes? That should be encouraging. It means we can use LDA. And let's try logistic regression, which, by the the way, remember you said that logistic regression just draws one line between two classes. Well, the trick is you can say, suppose you have cows, ducks and dogs. You can ask this question, is it a duck or not? Do one classification. So it's called one versus one. Is it a dog or not? Or is it a cow? And then you can build those decision boundaries and basically do the same. Come up with situations where it's more likely to be a dog rather than not a dog, that region belongs to or has the highest probability of being not then it is dark territory otherwise that territory so you can apply logistic expressions but the way it does it is internally it does multiple logistic progression models when you do that it comes with the accuracy of 97 and it comes up with the f1 score of basically a pretty good f1 score 97 percent score and it's doing pretty well it's doing pretty well now which you would imagine if you notice in your mind's eye can you see that these things are reasonably well separated up would you agree guys, guys, looking at the data? If you look at this data, they are pretty well separated up. So by now, the process is exactly the same. And you must be saying that, yes, I am pretty familiar with it. This looks good. Let me zoom into this picture a little bit more. You can see both of these variables seem equally important. This is your confusion matrix, two predicted class and not. Obviously, there are four classes. So where is one class being mixed up with the other? Are all the errors, the correct predictions are the diamonds? Look at the ROC curve. If you have to squint to find it, where is that ROC curve? I'm not going to go there. Do you see it's right at the very top? Isn't it the area under the ROC curve is practically one it is you can't even see it rise and reach the slide guys are we seeing that so area under ROC curve is practically one means this data was very easy to classify right and when you make the predictions it seems to make the right predictions let's oh why intercept something is allowed to rerun this code and help themselves I'll fix it and post it or maybe on the class websites So when you do, and by the way, you can use logistic regression, you can use LDA. When you do that, ROC curve is practically here. This was easy. You could use LDA, you could use logistic regression, either of the two. Let's go to classifier three, which is a little bit more complicated. You look at this. Can you imagine this is a two class classifier. Can you imagine a straight decision boundary, a logistic regression decision boundary that will solve this? It's an elliptical. Yeah. So neither a logistic regression directly will do it nor an lda will do it what will do it a qda quadratic discriminant right so if you just naively try logistic regression you will get a pretty bad you look at this this is what you call a bad rc you look at this this is what you call a bad ROC isn't it guys can you see it the ROC value is actually worse than random making because diagonal is here and this deep red line it is actually underperforming the diagonal by just a little bit. It's pretty terrible. And when you visualize the predictions on the model, it has nothing to do with it. It just drew a straight line through the data. But what is it really? What should it have been? It should have been here. It's not doing a good job. So what you do is you could, what could you do with this? If you draw a decision boundary, actually in the future space, I need to run this notebook guys. This is a screw up. If you try an LDA, you wouldn't go any further. Unless you do one thing, confusion matrix. You have to do a quadratic discriminant. By the way, the topic. So this notebook, guys, I need to fix. There's a mixer. I am doing quadratic discriminant analysis, but I'm giving the title linear discriminant analysis. Quadratic discriminant analysis, would you expect it to work,? You would and you see it's doing famously. Look at the ROC curve here compared to the Hopeless curve you saw in logistic regression. So for this data set because of the non-linearity and look at this how beautifully it makes the prediction map correctly and this is it it just gets it right so and it has a beautiful area and the other signal so now comes the question i'll ask you this puzzle can you make logistic regression work can you make logistic regression work? Hint, I taught you a trick for regression. How do you make the logistic classifier? Because you know that the decision boundary is elliptical, isn't it? How do you do that? X square by A square plus Y square by B square equal to 1. No, no, no what what have i like what do i do yeah see i taught you polynomial regression remember you can expand the feature space do you remember guys so what happens a quad ellipse ellipse is a quadratic curve so what degree of what is the minimum degree of polynomial i need to go to create an elliptical curve to right so let's see what happens what happens if you take this logistic and in this particular case, I did it for polynomial features of week four, you can play with it, what works. I'll try with two, see how well it works. You'll see it begins to work and then try better. And when you do that, lo and behold logistic regression, which was a disaster, all of a sudden, is it working now? all of a sudden is it working now yes it's looking very good in fact it's working better than anything else and this is the thing and look at the prediction map it's doing a very very good job what is the F1 score I think it beats the other ones 90 It's doing a very, very good job. What is the F1 score? I think it beats the other ones. 90, yeah, class probabilities. Make my plan, yeah. No, it's more or less the same, 93%. How much was it for the other ones? I forgot, 93%. Quadratic was giving... What was quadratic? 93. It's the same. So they pretty much they compete and they have the same model, very similar models for the decision model. So there's a lesson here. The lesson here is linear methods especially with polynomial expansion can do a great job sometimes right now people and this is one of the typical interview questions you ask people rarely do i get a candidate there will be people who came claim 10 years of experience in data science i give them a classification problem like this, and I ask that they will try it with logistic, then they won't do it, and then immediately jump to some highly nonlinear, like random forest or . Those solve it, but you lose interpretability. A logistic regression model is very interpretative. You have the beta coefficients isn't it so try to make simpler methods work don't immediately jump to a black box because black boxes have an interpretability problem right they are opaque and if you put in effort if you put in parts you can make your linear methods stretch out and solve the problem quite often, more often than you would think. So that is the lesson to learn. Now, you may say this is all very good. We are building our muscles. Now let's do some real data set. So for real data set, we'll take the breast cancer data. Let me see if I have opened it. Okay, so this is the famous Wisconsin breast cancer data center. This is old now. Obviously, this belongs to 30 years ago, I believe. At which time it was a remarkable research paper that mentioned this, and the scientists who produced this, they got considerable notice of this. So what they did is they painstakingly looked at cells and they extracted features from it. They looked at the area of the cell, the smoothness of the cell, whether it is bulging in or out, the fractal dimensions. So they really went as a scientist and created all sorts of features out of it. looked at the mean radius texture the then they looked at the standard error of actually how much variation there is basically think of it as variance they looked at the worst radius the smallest sos and so on and so forth and with all of these features you can do in case, we can do a description. So the whole problem in those days is, and even today is that, see, this is a scare. If you had a woman, people dread it. I am told that some famous actress, actually, had a high uh hereditary risk of developing the cancer and so she preemptively had a breast is that scary and it turned out to be a good decision so um like it's like prostate cancer people are scared of prostate cancer so women people are scared of positive things so women are scared of this now how would you find that x-ray may show something you do an x-ray and there is some area the fuzzy area and now you're worried your screening test showed there may be something there there's a suspicion of cancer how would you know whether there's a malignancy or it is just a fat now you know some dense area of the of the tissue of the gland so the way you have to do it is you can of course open it up cut open and go in there take the tissue put it under the microscope and then say aha you are totally healthy but that is pretty invasive and it leaves a scar, isn't it? And it's traumatic. So one of the things that has evolved is something called fine needle aspiration. Fine needle aspiration is you take a thin needle, you poke it in, deep enough to the tissue, and then you aspirate or suck out a little bit of the tissue. When you do fine needle aspiration, you have to do tissue. Now you can look at it under the microscope. When you looked at it under the microscope, it turned out that the human error rate was pretty high. The lab people, they could not clearly tell whether they are seeing the answer or not. So then people tried this machine learning exercise that can you do it? And right off the bat, it came out with a much higher accuracy than people were getting by, just putting it in the microscope and the computer from what I understand. And then of course the biological techniques itself, this is just a screening test. If you have a suspicion, then they are much more sensitive tests, which are much better than the machine learning test they buy them to test that can be done but as a first line they tried to build a model using this so there are lots of predictors one of the first things you notice is when you have lots of predictors you can do what correlation between the predictors you can do what correlation between the predictors you can look for missing values first of all are there any missing values there are no missing values you can check for data imbalance when you do this you realize that there are more people who have benign like whose tissue turn out benign or they're not, you'll just turn out to be at benign. And about 200 odd people who turn out to have cancer, malignancy, right? That is the ground crew. So what would your baseline classifier, what would your dummy classifier do? If you ask your dummy classifier to predict, what should it predict? 50% that it's benign it's all benign right and so how often will it be wrong so it's three 357 over the total will be 63.4 percent time it will be right and the converse of it will be 36 or so percent or so percentage will be wrong. The characteristics will change. When you have data which has so many features, data visualization, getting a sense of each of the features distribution helps. So let's look at the violin distribution here. When you look at the violin distribution, can you look at any one of these radius, tell me which one can I use if you want to guess which would be a good predictor that you could use perhaps single-handedly to know the reasonable distribution. I'll decrease the size of this or increase it. So let's look at this. Can I use this one, radius mean? What if I did a decision boundary at value something like 14 at this point? If I make a vertical line, I I mean a decision boundary that says if your mean radius is less than less than 14 or 13 right it is benign if it is more than that it is done would that work it would probably work most of the time yeah most of the time it will work but texture texture is hard because there's overlap. You see the overlap region there. So radius mean is a pretty good candidate. What about perimeter mean? That also seems to be a fairly good candidate. Somewhere around 90, if you may pick a value as a decision boundary, that one single predictor might be pretty good. And you can find a lot of single predictors that could be used. And by the way, the fact that today, when you look at it as a box in this catalog, and you can literally see, you can find good guesses of what would work as single predictors. It's a remarkable thing. For example, you look at Kcavity mean, these two are fairly well separated out on Concav points mean. So you could actually make a single variable regression model and get quite a bit of accuracy from that, though of course including all the factors we have. This was a point actually made by a person who wrote a very controversial paper a professor holt from new zealand from the bottom of the earth right he pointed out that for breast cancer including the original researchers people were making very complicated models using all of these features right but is that all necessary what if you took only one predictor you call it the one rule right a one predictor rule and if you use only one predictor then writing a classifier is just finding along that predictor the best point at which to that point is the decision boundary isn't it anything to the left is one thing anything to the right isn't it okay am i making sense like for example for perimeter words if i take a predict if i just split it at 110 right below that it's all benign above that it's all I wouldn't be far off you know it will make some mistakes but it'll be a pretty good model and so he cautioned actually that people are in a hurry to build complicated models you shouldn't just as it is a good practice to first build a dummy classifier and know that a good classifier must beat the dummy classifier in other words 63.4 percent accuracy it must do better than that before you go and do something really fancy pull out the latest deep neural network now with some something like that why not try the one hour rule? Because it will tell you how much, how obvious it is. The answer is obvious in many ways. So he created the one rule thing. One rule thing we do in psychiatry, in terms of psychiatry doesn't have a support for it. But the best you can do is take a decision tree of depth one so actually i wrote a little library that you can use which i literally call the one predictor one predictor classifier okay and it is there in our github i'll release it and i'll put it in the course website you can use it so look at this if you will notice that this is the point i'm making the observant reader reader will notice that the diagnosis is oftentimes well separated by a single feature such as radius mean look at the or area or area mean, or something like that. Let us take this. You took area mean. When you draw out the box in Wiskplot in red, DJ, you see how obvious it seems one predictor will do pretty well. Many predictors may be better, but one predictor pretty much gets you quite far. Let's build a model with this. Why is this happening? First of all, you notice that if you do a correlation map, find the correlation between things, you get, you notice that there is a significant degree of correlation between the features. Deeper the color, the more the correlation as if it is because the area and perimeter is mostly dependent on radius itself right it's like two pair and pi r square no actually that could be the see the human cell is very complicated. Those are... Okay, so here's the reality. If you look at the cells, healthy cells are usually not wrong. They are like peculiar shaped, right? They could be banana shaped, they could be any shape, right? So your pi r square won't work. But it turns out that the cancer cells, because the cells divide rapidly, what is is by definition a cancer cell is a cell that has runaway replication. It is born before, it never gets a chance to grow to full size. It just replicates while it is still young and then replicates again. So what happens is it stays tiny. So you see a lot of tiny, tiny, lots of cells in cancer. Those cells tend to be generally more round than a full grown healthy cell. Right. So the area argument applies much more for the cancers by our correlation. So if you were to filter this data and only take the malignant data and there you look at the correlation between the radius and the area it will be highly correlated but it is not true for all the all the data and this is where you know real life is subtle there are many nuances when you deal with real life but it is true so you you see that very light color and very light color is positive correlation and there are lots of positive correlations here and there are a lot of very dark colors are also very strong negative correlation there. So you do expect a lot of that and you can build a correlation chart and when you build it you'll see a lot of high numbers here and let you meditate on it. You can do all sorts of pair plot for me. So suppose you take only the mean features, not the, it's a smaller data set. Let's look at the pair plot here. What do you notice? What do you notice? This is just the diagnostic. You take only the mean. You see the distribution. If you just take two variables and you plot it out as a scatterplot, you can pretty much see a decision boundary. Let me zoom in and show you. Some features, decision boundary is more obvious than others. Let me, where is, where am I going? Look at this, the density plots. Look at this, look at this. Can you guys see where my mouse is? Yes. You can see that one type of data is at the bottom left-hand corner. The rest of it is at other places. And in this, the two curves, these density curves, are fairly well separated out. So you can get a reasonably good classifier. Here it is even more true. See, the intersection area is a measure of errors so if you were to take this predictor it's hopeless because both of the bell curves are overlapped isn't it do you see that both of the bell curves are overlapped i mean both not both of the density plots are overlapped so you can't take this pair of features i mean this particular feature you can't take but if feature, you can't take. But if you take, let's go through one by one. This could be used, this is bad because there's a lot of intersection, this is reasonably good, this is better, this is hopeless, this is okay, okay, this looks pretty good, or this looks even better, right? And this happens to be concave point mean right concave point so of all of this concave point mean seems pretty good isn't it they have very little overlap so the same thing for standard error you can do that and i won't go into that we have got for worst we can do at the end of it they all begin to look similar but you can do a similar analysis now so let's start building a mode first things first you need to what do we need to do real world data we need to scale the data. And you can also drop some highly correlated columns. What are the highly correlated columns? You can figure out and pick a few columns and say, these are highly correlated. I'm going to drop it. Label, apply, somewhere. Correlated columns somewhere up there. I've've defined i've handpicked a few inside there are two correlates it's not used so we make a dummy classifier out of these with a dummy classifier what is the average accuracy you are getting 63 percent that makes sense that is the ratio of good to bad 63 you know that the benign the benign cases are 63 percent of the rate so you don't do you expect any better from the dummy classifier no you expect exactly the same so anything must beat if it beats this it's a good classifier isn't it let's take only one predictor we took ariam and please play around with different ones that you learn and see what happens when you take area mean out of those many cases you get very small number of errors nine plus five and what does it say f1 score is 92% for 0 and for this, 86%. So pretty good numbers, right? And average accuracy, 90%. You think it is better than dummy classifier, guys? Is it a good classifier to use this? Fairly good. Pretty good. And this was the point that professor holt was making that sometimes you can get pretty good results with this very simple model don't get carried away by sophisticated model and the you know euphoria of using the latest and greatest tools if you visualize once again how well you look at the AUROC curve it looks pretty good isn't it. guys. The area is 96% so now let's do a multivariate logistic regression. We take all the variables or I mean throwing away some highly correlated variables. Otherwise you'll have multi-collinearity problems. When you do that you jump to 97%. 97% is definitely better, isn't it? It's definitely better than 90 and so it is worth doing multi-factor analysis.'ll tell you a trick I deliberately didn't pick the best predictor. I'll let you discover which is the best. When you do that, the hint is the gap between the best one predictor model and this all predictor model will narrow down quite a bit. But I leave it to you to discover which is that predicting. So when you do a model with all the predictors, again yellow brick comes in, it is telling you that the features that matter most are heavily the big bars are radius and this fractal dimension. are radius and this fractal dimension. Fractal dimension again matters why? Well, it has to do with the biology. Human cells are irregular. So they have high fractal dimension. So in other words, if you ask this question, what is the dimensionality? Given a circle, what is the dimensionality of the periphery of the circle given a disc the periphery is a circle and what is the dimensionality of a circle two circle is two dimension is one dimension right it's a curve disc two dimensions circle is a curve it's just a line bent around but it turns out that when you look at real life situations like for example if you were to walk along the edges of a snowflake that you get into this beautiful beautiful world of practice and then it turns out that it is no more typically you don't call it one dimension it develops a dimensionality between one and two so it develops a fractal dimension so slightly the more snowflake like the boundaries are of example the example is given the what is the actual shoreline length of england you can argue that it's very very long right much longer than a rough measure so that is where it is it's the ratio of the actual length that you would see hypothetically and a very rough sense of it that's one sort of an intuition of getting up to each so the fractal dimension of healthy cells can be pretty high but that of cancer cell is a nice one because they just bond so it could be used and this model is saying that is a pretty good predictor not bad so this is coming out with the feature importance now the feature importance once again because it's a linear model and you have scaled the data quite literally you can read off the coefficients and tell which are the biggest coefficients in magnitude so the biggest coefficients in magnitude that you can see are this guy 96 isn't it number one and this guy 94.94 and 0.96 which are important so according to it if you plot it out it says oh goodness i don't know why the dollar sign came up i screwed up somewhere okay um where did i introduce the dollar sign i'll remove that this one also affects so these are not dollars these are important where is the feature importance okay i'll figure out where it's so which is the most important feature radius second most important feature is this and so on and so forth it goes you can look up radius again happens to be the most important feature now in this model we did not get fractal dimension is pretty close it's an important feature yeah right concavity is an important feature cancer cells are circular they're bulging out so any banana shape or concavity is more indicative of a healthy grown-up cell same results you can get so shapely remember i told you about the novel or at shapely who got he was in the institute of advanced study and by the way have you guys seen the movie a beautiful mind anyone who remembers seeing the movie beautiful mind yes mind anyone who remembers seeing the movie beautiful mind yes yeah yes John and the the about the life of John Nash no it's about equilibrium right play game theory yeah that's one game theoretic so John Nash created the the fundamental of theory by which today, for example, wireless spectrums are auctioned and so forth. It said that in games, whenever people are negotiating, you can find points of Nash equilibrium. And ultimately, if both sides are lashed, they'll gravitate towards the equilibrium. And good negotiators instinctively know that how much they will get and not get right so so when skilled negotiators from both sides come together they invariably end up at a point that you would call a Nash Equilibrium right and so anyway so he wrote that theory John Nash actually And so anyway, so he wrote that theory, John Nash actually is a person that I, my generation used to actually have the pleasure of listening to the lectures. So this Shapley was actually a classmate, a classmate of core researcher with John Nash and Shapley got the Nobel Prize for only one big item, the Shapley values. So this is the Shapley value distribution of the corpus. What is the Shapley value? The bigger the Shapley value, the more it matters. So you can see radius has a huge impact. What is the other big Shapley values? Concavity means flying out. What other things are flying out? Compact se is flying out in the negative directions fractal dimension is flying out in the direction so these are the important ones then there is such a thing as partial uh partial dependency plot it just shows how much um as you change the value of radius the answer answer changes, right? So the steeper it is, the more impactful it is. And clearly this one, concavity mean and concave points mean, pretty like the answers are very sensitive to the values. Now, I have not explained about the partial dependency plot. I'll do it in an extra session sometime, so ignore this part. I will stop here today. We are almost out of time. But Shapely, learn to study the Shapely library. I give you a homework. Go study the library in Shapely. I have used it here. I've also used it in regression. Make yourself familiar with it. This is the tip of of the ice i'll just show you one curve from shape but there is far more in the shapely library than i put here learn to use the shapely libraries only for logistic regression you can just read off the coefficients because it's a medium you can't do it for non-linearhicle models. So guys, this is an example of a real-world dataset. You realize that all the techniques that you learned with classifier 1, 2, 3, simple datasets, they just transfer here. You build your muscle with simple dataset and then you grapple with complex datasets because you know how to grapple with it. How to deal with it, you have a particular intuition. And this is it. This is is your this is a famous data set the wisconsin diagnostic breast cancer is it has been quoted in innumerable papers anytime somebody comes up with a new classifier algorithm it is almost a given that they will go classify the iris flowers data set in the um this this this particular wisconsin diagnostic data set and there are a few other such classics they are often quoted in papers let's see you can do that so guys please study these notebooks i have released all of these piece. If you don't study, don't practice, then you'll feel lost. I am going to stop the recording.