 . When we did the last week, not last week, two days back, we covered the theory of Arima. of arima and the moving all of this we covered and also we covered the single exponential smoothing and double exponential smoothing triple exponential smoothing now what what turned out in all these models is that there are lots of hyper parameters like p q d and capital P capital Q capital D and L. So there are a lot of parameters to play around with. And somebody who is like a novice will not really know which one to use. And so they end up using something called auto arima which automatically tries to detect what is the best values. But auto arima suffers from, I mean, it becomes too generic you it becomes like a black box you you just know that this is the prediction but you do not know the why so this is one of the kind of i would say drawbacks of arima that is not the only drawback one more is that if you see inherently all the models whether it was auto regression, moving average, or their combination, they are all in some sense, linear models. Okay. And, and linear models with the difference operation, which kind of makes it probably when you integrate a linear model, it becomes quadratic and so on. So that, that is, that is what these models are whereas when profit so the advantage of profit is it's so let me try to do it with my yeah so in profit what you have is You first of all say that you can have multiple, you can, you take care of trends in two ways. You can take care of trends saying you can either have a linear trend or you can have a saturating trend. Okay. This analog is not there in the ARIMA models, in the other uh whatever else we have discussed so far so the linear trend is what we know something grows steady whereas a saturating trend is something we know that it has a limit and it goes like this so it saturates and you can also have a floor so let us include the floor which means that it's it like a sigmoid. It has a floor and a saturating. So this is one feature that Profit has. So you can specify this. So by default, it is having a linear trend and you can specify it to have a saturating and floor. And the second one aspect is that you can have change points. Now, what is this change points. So we all know that invariably When we are having growth. It is not like it is a steady growth or it's a completely you can you can always have when you have a time series. x0 x1 and let us say these are all different dates now there may be a particular event like for example in 2020 around march we had a significant covid situation so there would be a you can expect that the time series is actually going to show a trend which is quite different maybe till then it was showing one kind of trend and another trend so this concept of change points you can actually bring in change points and again by default it uses the change points based on the past history it tries to figure out where there is likely that those changes in trend in terms of automatically detects the automatic detection happens of change points if you don't specify. But if you do know based on the actual domain, you know that something significant happened on a given date, you can actually introduce that as a change point. So this is another feature that profit has. And then seasonality by default it uses yearly seasonality weekly these are the two by default it uses but you can again introduce more seasonality as per your problem requirement you can introduce monthly and of course you'll have to specify and you can it could be other periods any customized period also you can introduce you can introduce these customized periods okay and that is the one more feature that okay so you can have this multiple seasonalities and then there is one more feature which So you can have this multiple seasonality. And then there is one more feature which profit brings in, which is not there with holiday events, which is not there with the other linear models, which is that there could be some significant events which are like one-off events and they repeat. Like for example, one example that is given in the Profit, if you go to the link on Profit, they have talked at length about one football player whose profile is being accessed on the Wikipedia. Super Bowl is an event which I think there are two Super Bowl events and of course there are many other playoff events, Super Bowl events and there are multiple playoff events. So each one of them is a kind of a significant event and you can expect that on those days, very close to those days, the access of the football, the access of the web page describing the football player will actually go up. So there will be a spike. So you may have, if you see, if you're trying to draw a graph, normally it may go like this, it may go cyclical and all that. But then on those specific days, it may go up. those specific days it may go up. So it would look like an outlier if you are just looking at the normal trend. But if you take into account that there are these special days where it shows a spike, you can actually predict for those special days in the future. So if you have these holiday events which are being captured, and you know that on those holiday events, something special happened. Then on those future of those holiday events, also you can actually predict more accurately rather than just saying that these were outliers. So these are some of the powerful features that profit has. And it, it does that using, I mean, in the Facebook page, it says it uses the Bayesian approach, which means it uses a prior which, so all these are based on prior information. Then using the prior information, you come up with a posterior. This is the concept that is being used. All of these features, if you notice in the Python package, probably today's notebook will cover a little bit, but not all possible features, like holiday event seasonality and multiple seasonalities. But the advantage is that the person who is using it has an intuition i mean it is not like like if you in arima for example when we had in arima we had a p q d and a capital p capital q capital d and l now if you ask a layman what is q it's not very uh i mean the arima may have figured out that there is a particular value of Q capital Q equal to two, which is giving the best result. But what does it mean? It's not very, there is no interpretability aspect is kind of missing here. Whereas in this, we can interpret, we can say that, okay, there is this holiday event where this happened and therefore we are expecting. Likewise, there is a seasonality so if we go up there is a monthly seasonality and therefore it is happening or we can say that there is a change occurring at so and so instant of time and therefore the trend is changing i mean each one of these features is interpretable by the user so So that is the main advantage. And whatever, I don't see any reason why, I mean, if you use profit with all its features, I mean, of course, the most, if you just naively use profit, you may not get a good result because you would not have used each one of these features because it assumes some default values. The default values may not be the best, but you can play around. You can play around and those playing around is more something which is interpretable for the user. Unlike these ARIMA models where interpretability becomes a question mark. This is, yeah. Any questions? arema models where interpretability becomes a question mark okay so this is yeah any questions it's under one quick question the change points so the way i'm interpreting change points is basically an indication to the model that there's a structural shift to the data right yes so i i it is essentially to the trends so normally when you say there is a trend, okay, let me normally when you say that. That was my question. Let me, I'll complete the question and I'll let you explain that. Since there was one thing which is basically, we had this thing called saturating trend and then there's another thing called change points. What is the intuition for what change point is doing? The way I'm wondering is if it's a structural shift, is it kind of bifurcating the is a change point which happens on let us take this example march 2020 first of march 2020 so till then let us say there was a saturation so what it means is the growth there was a growth let us say there was a saturating growth with a cap of 100. Okay, so that means there was a saturating growth trend, which is having some cap of 100 in some units. Now, we are saying that after March 2020, this cap has actually become 200. For example, it could be the number of video conferences in a given day. So let us say the saturating cap was 100, which was going to, now you're saying that actually no, since this date, the saturating cap has actually become 200, which means there are twice as many number of video conferences. So the trend will now shift. Similarly, this is for a saturating trend whereas if you are so it goes towards the 200 if it was a linear trend it means some in some sense you are saying the slope has changed the slope till here was like this it has become like this so there is a change in the slope so So between X2, every successive point, let us say there was a delta, which happened to be like, I don't know, some difference between two time intervals had a delta equal to 10, was the trend till a particular point. From that point onwards, you expect a delta equal to 15, for example, between every two successive points. i make it clear yeah i mean this is basically the explanation of there is a structural shift right yeah correct by indicating by using that option called change points you're communicating to the algorithm that i am aware of structural shifts in my data because I have domain knowledge The example that you gave it's like on that particular day in March 2020 my system expanded maximum video conferences from 100 to 200 yes, so the observation of a data point in April 2020 of 150 What is that? in April 2020 of 150 video conferences is actually under utilization rather than over utilization because I know the capacity. Correct. What I was looking for is what is happening inside it. I mean, are they splitting these things into multiple pieces and doing it or is there some other way that it's actually… It is really splitting it into multiple pieces. That is how it being done but it is not multiple arima models they are they are having their own uh like it's there is a regression there is a linear regression till here which is with one coefficient the there is a linear regression with another coefficient here okay so in some sense the coefficients they are allowing it to change at those change points they're building multiple models yes yes based on the change points indicated correct correct so again you don't have to i mean to to start with you don't specify those change points you add them in when you know that there is something special because otherwise you are going to make it too flexible and again it becomes confusing so they have given default options where the default is it will look at the history and see that okay there is a significant shift in trend happening at some point in the past and let us say they are also able to predict that this happens cyclically this change points are happening at a particular cyclic fashion where the every couple of months there is a change in the in the slope they will actually use that to do the prediction for the future okay so the change points get introduced for the future also but in fact if you go through the profit paper they are saying so the change points get introduced for the future also. But in fact, if you go through the profit paper, they are saying that the change points are one of the significant reasons for large variance. So they don't recommend adding too many change points. So in fact, they have a rule saying that they have a sparse number of change points. So even though there may be potentially 100 change points based on the data, they have a default saying only 5% of them. So 0.05, that means 5% of the change points means five out of those 100 are actually considered as real change points. Now, which five? It's a choice which is made by the profit package you can tweak it so again this change point percentage so if there are potentially 100 change points and only 5 of them are treated as real change points you can change this parameter to make 0.2 which means 20 of the potential change points can be treated as change points so there are these options given in the package there is both an r implementation as well as python implementation to do this modification to the default parameters okay all right thank you yeah default parameters okay thank you yeah so this is about profit so let me now go back to the last of the theory topics that so all all the deep learning methods for time series so they they end up using either this rnn lstm gru so let let's go to let me go back to the writing pad so let's look at what i mean probably before I go into the deep learning models, let us just look at the most basic example of what an RNN does. We all know that RNN is the most basic recurrent neural network. Then we have LSTMs and GRUs, which are in some sense improvements on top of the rnns so what does an rnn do we have an input at a given point in time we call it x of t and there is an output which actually feeds back into the so we have this uh bring back and then we have a y of t as well okay and the way the typical equations are written is you have y of t. So we have a hidden state, which is h of t. I think the notation is small h. Let me change it. h of t. So y of t is equal to, so it could have been a vector. So it could have been a, so let me write it as some some weight vector which is connecting the output y to the hidden state h and acting on h of t let me ignore the bias terms okay then we have what is h of t itself h of t is going to be having two components it will have first of all it will have a w h x x of t plus a w h h h of t minus one and again i have ignored the bias terms so this is so the hidden state depends on the current value as well so the hidden state gets the hidden state of the RNN gets calculated from both the current value and the previous value of the hidden state and the current value of the hidden state determines the current output. So this is this two together determine what an RNN is. and the way usually people write it so that it is more it's conducive to doing the computations they expand it out they unroll it they call it the unrolling so you have the h of zero and then you have the x of one and then you have the y of 1 and then you have the h of 1 x of 2 and so on right so this is how it is unrolled and when you do this you also back propagate when you are doing your let us say you have your inputs and your outputs you can do your loss function calculation you can do back propagation by going multiple levels back you treat it as a feed forward network where this is this whole thing is a feed forward network except that the weights are common so the weight whx that is coming here and in the next block and the next block they are all really the same weights there is no different weight so this is how the back propagation is done. The loss is calculated. But whatever it is, whether it is RNN or LSTM. Now LSTM, the reason why LSTMs and GRUs were introduced, we know that they were to take care of the vanishing gradient and vanishing slash exploding gradients. So these were the reasons why LSTMs and GRUs were introduced. So they make sure that we have, I mean, ultimately there are some more states that is not only a hidden state, there is also a cell state, but end of the day, from a perspective of utilizing the, either the RNN or the LSTM or the GRU the way we use it in our either whether if we are using either in PyTorch or in Keras, TensorFlow whatever the from a perspective of usage it is just replacing the RNN module with LSTM module there is nothing much in terms of that if you don't really need to know what is going inside it so if you look at all these examples you have an X of T and you have an output which is Y of T and so this is the quintessential problem that your RNN LSTM and GRUs are trying to solve so in in in specific our problem is very clear we are saying that we need to figure out what is x t plus p so we are trying to find the future values of the same sequence so this is a special case so this becomes a special case of the more general problem where you're trying to figure out one series from another series so this is rnns can handle this if they can handle this they can surely handle this because this is a special case right so this is the i would say the basic premise of using RNNs, LSTMs, GRUs. And I went through the paper which was shared by Anil. So let me go to that. Where was it? Yeah, this one. And actually, it is giving a very good coverage of the deep learning models. But essentially, this is the picture which i think he has also shared it on the slack and if you remember what was covered in the transformer sessions on natural language processing what you would have you have an input sequence say in english or something and you're trying to generate the output sequence in another language french so this was the problem which asif had explained now here it is different slightly different in the sense we have this numerical input data and instead of trying to predict another language we are actually using the same series to predict what the output is so we end up passing as the first value the last value that of the sequence and we use that to predict what the output is so it's one at a time so what happens if you feed in the last value of the input to the decoder and the attention model the this is basically the transformer model which has the encoder and dec attention model, this is basically the transformer model which has the encoder and decoder, it will generate the corresponding prediction for that step and that itself goes back as the input, and then you get the next and then you can keep playing the same game. So you pass in T6 and you'll get T7. So the same architecture which we have used in NLP for translation with practically no change can be used for the prediction of the next values of the series where you have all these X at different times and you are able to predict the x at future times so this is what the whether it is rnn lstm gru or even the attention models the idea is you use x of t to predict x at t plus p at future values for the same sequence so this is is, I would say, the summary of the theory behind using deep learning. And of course, why is it more powerful? The reason why it is more powerful, one reason is that automatically all the non, you can take into account all possible nonlinearities. So because the very fact that neural networks are nonlinear means that nonlinear relations can be encoded. Okay, that is one key advantage. You don't have to assume any linearity in the relations. And that is one key advantage. Another is another is if you go back to this picture on whichever we saw here this picture the fact that there are these attention layers means that your next value of your output it can actually have attention coming from many other values in the past it doesn't have to be only based on the most recent value of t4 the fact that you have this attention model here means it can pay attention to something which happened four time instance ago so t5 can depend on t1 uh something like that so that is what this attention model helps it it helps in That is what this attention model helps. It helps in generalizing the notion of whatever RNNs and LSTMs do by making the output pay attention to what happened many instance ago. Attention models help in relation between different past instance of series on current value okay so so this is probably one of the other advantages that deep learning models with attention have so and of course i mean we have we have seen that deep learning models invariably outperform other models because they have many more parameters to play around with. And you have this whole architecture of having you can put multiple layers of RNNs. I mean, it becomes very simple. The moment you have an RNN, you can actually stack multiple RNNs. You can stack many linear layers on top. So you can you can do a lot of bring in much more complexity into the whole problem and solve what probably could not be done through non-deep learning models so this is i would say i would i am going to probably start with the lab right now unless you have any other questions on this so chander one quick question on the the attention model and the uh the use of transformers so when we were looking at in general transformers and how we will work with transformers for let's say doing things like translation and so on we could literally pick up one of those pre-built models that uh that's existing and retrain a couple of layers and then be able to reuse that with a specific content right i could make it learn something that relates to my set of documents and so on correct correct i'm presuming in the context of using a transformer for doing uh forecasting of time series you have to build the entire thing from scratch none of the existing models are useful is that the correct i think you're right i have to go back and see the examples because i have not seen the examples where attention models are used this paper is explaining it but what you're saying makes sense because we cannot use those embeddings i mean it's not like we have this um unlike let us say bert or bert was trained on lot of existing corpus of documents i mean you don't have an existing corpus of time series data that you can train something on so you you have to use something from scratch but the fact is we have these nn dot transformers so we can use in pytorch we have nn dot transformers we can use it to build the attention the encoding and the decoding and but we'll have to train it ourselves with the current time series data. It is not like we can use a kind of a bird layer because but depends on the fact that language already has a lot of prebuilt rules. I mean, when you have sentences, you know that this is the subject. This is a verb. And I mean, you know, many things already with the subject this is a verb and i mean you know many things already with words whereas in a time series if you're come coming with a new problem you have to train it on the existing time series you don't have other past time series to train it on yep okay that's what i mean that's what i felt intuitively just wanted to confirm yes yes yes i i you're right but i want to see some examples where it is used because this paper is saying that it can be used for influenza-like illnesses. So that is, so they are making an example, but I have not yet gone through the notebooks. So I want to see some, what I will do is I will try to find notebooks which are using the attention model and I will post it on the slack okay all right thank you yeah yeah okay so um let's now move over to the notebooks which i have already shared on slack is that fine yeah people just need to remember to install the profit library Yeah, people just need to remember to install the profit library. Yes, yes. So yeah, let's start. So what I'm going to start with first is the spectral decomposition. Yeah, so everywhere you need to, I mean, wherever you do not have a particular something doesn't work, you have to do a pip install or conda install as required, have to do a pip install or conda install as required depending on your environment and so i have also all these notebooks which i have posted in the zip file today they are all coming from references which are available so here is this reference analyzing the frequency components of a signal so this is the problem which in the last class when i covered i said that we have a time series we can do a non-parameterized analysis of it and non-parameterized because there are no parameters to tweak here it is just based on the fact that you want to figure out what is the kind of frequencies that you are observing in this signal and that that can give a good indication of what is going to come because the moment you have frequency it means you have a periodicity so if you know that these are the significant frequencies of the signal you can use that to do the prediction of what is going to happen that many time periods later for example and I mean it's also like a principal component analysis. If you are able to reduce your F T into, even though it may have many frequencies, but most of them are with vanishing amplitudes, you can take the most leading terms and it becomes like a principal component analysis, a good approximation of the original signal in with finitely many periodic sinusites so you use though that as an approximation and it gives a good explanation of the future values without having to go into the entire frequency range so here what we are going to do is there is this weather.csv which is already there in the I have already downloaded it. So what does it contain? Let us open that weather.csv just to see what it contains. So it turns out that these are different locations in France and in each of these locations, of course course you have the date and you have some precipitation i don't think we are using the precipitation we are using the maximum temperature and minimum temperature as far as this notebook is concerned these are the two which are being used okay so this is the notebook that is being loaded so what are we doing in this example um so the usual uh imports are being done so the main thing here is um there is a scipy has this fft pack this is the fast fourier transform package so i i think scipy automatically includes it but if it doesn't you may have to do some pip installs to get in make sure it is there so maybe you can try it along with me when and when you're running and see if it works so long as you have scipy i think it should work otherwise you may have to do something specifically to load it okay so you we do the usual thing we load the data frame and then we are only filtering on whatever is after 1994 and this is the so what are we doing here here we are saying that for a given date so because this has multiple locations in france uh we want to look get a get an averaged value for a given date so this is even though there may be this temperatures reported at multiple locations in france we want to get the averaged value of the t max and t min across france so that is what this group by date mean does okay and then what are we doing here we are doing to date time of so this is let me see what to date time does index so this this we are going to use later okay don't worry about this immediately this is just converting this whatever is on the first value it is converting it into a date variable and then here we are taking the average of t max and t min and it's we are not normally when you take average you divide by two but it turns out that the details of this in the csv the is my screen visible or should i expand it i can see it fine you can see it fine okay so this 20 is because these are in units of one tenths of a degree for whatever reason this is how the data has come therefore we are dividing by 20 to get the actual temperature in celsius okay and then here finally we have we have plotted this so when we plot this we are seeing that this is how the temperatures are cyclically varying from 1994 onwards of course it is cyclic because these are temperatures temperatures have an annual cycle so we have deliberately chosen a problem where it is easy to do the fourier transforms i mean it's in some sense we are taking an example where we know that doing a Fourier transform will give a unique value for, I mean, at least a few unique values for the frequencies. Okay, so next We are computing the fast Fourier transform. So this is the way it is done. So we are taking the temperature, which was here. So we are taking the temperature, which was here, the data set which had the averaged value of Tmax and Tmin. So this is just a time series now. Temp is a time series. And we are converting it into an umpire array. We are applying the fast Fourier transform. numpy array we are applying the fast Fourier transform it becomes and whenever you have a fast Fourier transform you can actually find the spectral density which is just the absolute value squared so I mean this is called the power spectral density it is the because this fast Fourier transform is a each frequency will have an amplitude which can be a complex number. If you remember when we covered this last time, we had this A times e to the power of i, j, and then two pi k by n. So that A is actually a complex number, whatever multiplies the particular exponential. So to get the, how much important an amplitude is you need to do the modulus squared so that becomes the power spectral density okay so that is the step here and then um okay so here uh we are determining this one. Okay, FFT pack. So yeah, here what we are doing is we are writing a unit. I mean, ultimately this FFT is another series it's a series with which is having a lot of values now we need to know which frequency a particular series corresponds to so that is where we are trying to determine the in some sense this is the y value the the the y value of the in a xy graph you have the x and the y right so this is the y value we need to find the corresponding x the that is the corresponding frequency at which this this becomes the fourier transform so this is we are using because we need to do it in terms of days we want to find the frequency in terms of 1 by 365 okay so here the one would the unit one would be one if you see one means in this it means the frequency is once per year so so if when you do this plot so you have this fft freq as as the X axis and the Y axis is going to be the temp FFT value. So if you now do a plot. If you do a plot of the you typically tend to do for the modulus the modulus squared so this is the plot of if you see the temp ssd against the fft freq and when you do this plot you see that there is a peak occurring at x equal to one which means when exactly when the frequency is one which means once per year you have the maximum spectral so what you notice here is that we are having the highest component of the power spectral in fact there is a logarithm being done which means it will kind of dampen out the if you don't if you hadn't done this logarithm you would have had a very sharp peak here because it is logarithm it looks like this is just slightly more than the others okay any questions on this graph so what this graph shows us is that uh it it it it proves what we see here when we looked at this it we saw that there is a cyclical nature which means that there is a periodicity with one year as the frequency and of course there are other higher frequency components now many of them may just be noise so what we are seeing is that the most significant component of the periodicity is coming from the once per year all others are very low and this is what we infer from this diagram okay in fact whatever if you are here is a filtering done this is a kind of a low pass filter what it means is anything which is greater than 1.1 which means you are saying that if if something is happening more frequently than 1.1 times a year which means you are saying that it is happening within a year there are fluctuations you are treating it as noise so within a year fluctuations if you treat it as noise, you end up basically all these frequencies are chopped off. And if you chop them off, and then if you try to do the inverse Fourier transform of the same, so you see here, in this case, there were a lot of these ups and downs which were happening within a given year. So in some sense sense by doing this chopping off all of them saying that whatever is greater than 1.1 you're making it zero if you do that and so this becomes only the slow changes which means only the changes which happen either once a year or slower than once a year maybe once in two years once in three years okay so you are you're getting rid of all changes with which are happening within a year and only keeping those changes which are happening either once a year or slower than once a year so if you do that and then if you do the inverse transform of the same so here is an example where the inverse transform is being done IFFT. So this does the inverse transform. So you get the slow moving changes of the temperature and and if you plot that slow moving changes with the date. So you remember we had a date function earlier here that date variable is being used in this plot you're trying to plot the slow moving changes with the date variable and you see that it's a nice sinusoid so which means that if you get rid of all those noisy terms it is almost perfectly a sinusoid okay so this is uh an example uh where we are i mean we have taken a contrived example where we know that it is cyclical and it is once a year things change but this could have been let us say you didn't know all this you're coming with some totally new data you you can do this without any knowledge of what the frequency is expected to be. You do this whole calculation of fast Fourier transform followed by the spectral density and then plotting it. And let us say you're finding some peaks at some one or two places. It means that those are the significant ones which really comprise the signal. So this is the value of doing the frequency based spectral decomposition. Anyone had problem? Anyone tried running this notebook and had a problem? I tried running it, but I'm getting an error while installing the profit. Profit? No, this has nothing to do with profit this is i have not yet come to the profit and i must have been running the air quality one yeah okay don't don't run there i am running the one which is um the name of this notebook is sorry spectral decomposition for profit uh we can i mean the there is very detailed instructions given in the web page also how you should install it so we'll come to that a bit later i'm i'm trying to do because i'm trying to do it in the order in which i covered the theory last class so we first did frequency decomposition which looked very mathematical when we are doing it but then the package makes it very simple if you use scipy's fft pack calculating the fourier transform is a one step and then calculating the power spectral density is required because this fft is a complex number you need to do you need to know which value is important and which is not for that you need to have real values so you do the modulus squared and then you plot it and you see that wherever there is a peak and wherever there's a trough you you know that those are the significant ones and in this case we are just filtering out whatever is happening more frequently than once a year why are we doing it because we see from the diagram that we are just filtering out whatever is happening more frequently than once a year why are we doing it because we see from the diagram that we are we are having a lot of noisy terms I mean it looks like a periodicity but it's really nice so we want to get rid of all that noisy behavior by getting rid of frequencies which are greater than 1.1 in in this diagram it means all these are being chopped off all of these are saying we are saying all of this is actually zero if we do that and we do the inverse fourier transform which is another one liner and we plot it with the date we get a nice how temperature moves now what is the value of this we can use this to predict how temperature is going to be in march of i don't know 2024 we can we can say by continuing it so it would be a good guess apart from some of the noisiness that can come in in the future okay so this is working fine this notebook is working good okay nice okay yeah so I'm getting an error but I think I'll try to fix it later it's it doesn't like the date parcel it doesn't like the date okay a past dates it is saying it's the missing uh column called date did you do this pd to date to trade time did you do this oh no i didn't do that well that code was not there in your the one you shared no it is there this is like i i said when we when we came here i said that this line it's in the is required mainly for that last did you try just restarting it is there and running all yeah that line is there yes yeah try and restart the kernel and run all that usually solves most of my problems no i need to add that code that line can you show me that line it is there in the zip file okay yeah let me share it like uh quickly because i don't see that when i look at maybe you went to the internet and downloaded actually for me it was not working when i went to the internet and did it i had to make this change explicitly here in the notebook no no this is like here this is your notebook like your zip file okay maybe accidentally you hit a dd on that um and it just took that one cell out yeah actually uh the i had a problem at this exact line number because of the way the notebook was defined in the internet but then i when i modified it with this modification it started working after that okay fine okay you you try it and let me know if it still doesn't work we can probably check later you can message me and we can try to figure it out yeah okay so i this is one notebook which is on the spectral decomposition um then uh this one stock prediction is the next one i wanted wanted to cover. Okay, again, I have given the reference here. Okay, so this is a time series methods using all the smoothing techniques and univariate models, which means it covers the single exponential smoothing, double exponential smoothing. I think triple exponential smoothing is not covered in this but it is just a straightforward generalization and it also covers the arima models so all of them are covered in this example here so let us walk through this example again the reference is given for you to go through and read in detail it uses common techniques to manipulate time series and make predictions so what is done here of course the prediction are not i mean you cannot trust the predictions here much because if you could then people would have made a lot of money by now because it means they can predict stock prices so this is done more for a example on how you can handle things in time series okay so let's just use this example to understand how to implement the different techniques that we have learned okay so standard imports. Okay. And, um, Stan, uh, stats models. Let, let, let, when we come to it, uh, we will understand why we are doing some imports. Okay. Uh, then let me proceed. Okay. So here is a stock prices sampled or CSV. Let us quickly go through that CSV to see what it contains. Okay, so that is a this is the stock index. I think we are only focusing on GF in this particular notebook. So there is a GF index, a GF stock, and we are seeing its high, low and close. And we are only interested in the end of day price. We are not looking at the intraday. So we are looking only at GF. We are looking only at end of day and we are going to look at the I think the closing let's see I think we are using the closing price and we are using that to determine what it can be in future okay so let us see what it is doing so first we are reading it as usual we are reading it we are reading the index column is going to be the date and parseDates equals date. parseDates is equal to date. I don't know what it really does. Anyone knows what this parseDates equals to date does? Anyway, let's proceed and try to understand if required. Then what we have done is we have this consists of all tickers. Now, if you go below, we are filtering it out to only contain whatever is GF. Because if you look at the beginning of this notebook, we are only looking at the German front. So that is the GF. So we are saying if it is GF, we are getting rid of it. We are also getting rid of intraday. So we have this data which is filtered and we are dropping a lot of columns. We are not interested in most of the columns. What are we interested in? We are interested in the high low and close okay so all other columns are being so let's see once we get rid of all that we see there is a ticker there is an opening price there is a high price low price and close price so this is what we are looking at and for purposes of this notebook we are trying to predict the closing price for future so we are looking at the first we are plotting the closing price okay and so this is the graph of how it has been happening in the past. And here are some techniques which are used. So these are all functions which are defined based on the theory that we learned. We have this plot moving average. So we can pass in a window. So we can say that window is 3, 4, 5, whatever we want to say we can give. This becomes an integer and if you see what it does uh what is it it it calculates a rolling mean it calculates a of of that window And why are we doing it? We are doing it because this by itself is having lots of ups and downs. So the hope is that by doing a moving average with five days as the window, the hope is that we will get a slightly smoother curve. In fact, if you plot by 30 days you get an even smoother curve so this is this is what this rolling of moving average does it is doing it by it is taking a mean over those successive window intervals of time and plotting against the same X value the averaged value so this is what the moving average value does now why are we doing it we are doing it so that there is a smoother curve that we can hope to predict because trying to predict something so noisy is probably more difficult i mean it may not be feasible whereas when something is a smoother curve we may be able to see what is going to happen in future okay so this is the reason why this smoothing is done let us say if there was a cyclical nature coming in the smoothed value we may be able to predict the when the next cycle is coming i mean in the case of stocks maybe it is not a great idea to do it anyway because stocks are so unpredictable anyway but let us say it was temperature this would have actually made the original curve much smoother instead of if you seen the last example which we covered on spectral decomposition that noisiness that we saw with the data which is happening let us say we had done a five-day moving average we would have got a much smoother curve which would have made it easier to do the predictions if you only focus on the time time based approaches okay okay so this is now let us say we have this 30 days smoothed and what we are doing in the subsequent sections of this notebook is that we are going to try out this exponential smoothing which is this is now the single exponential smoothing if you see it has only one parameter alpha and if you remember the formula what we did is the smooth value of the of the series is defined as alpha times the latest smoothed value plus sorry the other uh i not the smooth this value. This is the original series. The series is the original series. We are trying to define its smoothed value. The result is the smoothed value. So what we are saying is the result is going to be the latest value of the series will contribute with an alpha and the previous value of the smooth version of the series will contribute with one minus alpha if you remember this is what we mentioned during the single exponential double exponential is very similar except that it will include a trend which will have the similar kind of gamma and one minus gamma and triple exponential will have a seasonality which is having a beta and one minus beta. So that is what we covered in the theory. Now we are seeing it actually being implemented in the package. Actually this function is so simple, we do not have a library for it. We can actually construct it. We, this is a construction of the function which does the same thing okay now if you plot the exponentially smoothed value of the series with alpha so you can actually do exponential smoothing series alpha so this this is going to take a lot of this is alphas is basically a range of alphas so you're you're passing in a series of alphas and for each value you are going to look at how the exponential smoothing is going to look so this is what this is if you see in this example you are plotting it for two values of alpha 0.05 and 0.3 so if you see in this example you are plotting it for two values of alpha 0.05 and 0.3 so if you see the 0.05 is the curve which is which is 0.05 0.05 is the smoothed value of the curve so the if you see um this if you go back to this formula right if alpha is very small what does it mean if alpha is very small we are saying that the the effect of the previous values is more because one minus alpha is high if alpha is very high in in the limit of alpha being 1 if alpha is 1 it means the smooth value is exactly equal to the series if alpha is very high so the maximum value being 1 the smooth value which is result is exactly going to be equal to the series value which means you are not doing any smoothing so if you see as you keep going closer and closer to one the curve becomes similar to the original curve so it's not achieving much whereas if you pass in the small values of alpha you get a smooth version right and the what you notice here is the smooth version is having the features of this exponential, sorry, this moving average based smoothing, but then it is more closely following the curve because we are doing an exponential smoothing where the nearby values end up having more effect than the past values. So that is why it is closer to the curve whereas here in this moving average model uh what happened was all the 30 days if you look at the here it was 90 days all the 90 days were given equal weightage in this moving average right so if all the 90 days are given equal weightage then there is going to be a huge error between the average and the actual value on a given date whereas in experiential smoothing you achieve quite a bit of smoothness and yet you are close enough to the curve because you have given most weightage to the recent instances compared to the past instances is that point clear yeah yeah okay so just like you can use moving average to do a prediction now you can actually use the instead of using the moving average which is probably having more errors you can use the smooth value to do the future predictions so this is this is the how do you do predictions you because you let us say you have come up with an alpha now plotting the next value in the series becomes straightforward because if you have the current value of the series you can predict the future value because the smooth value is anyway the future value so you can use it to predict the result and if you didn't have the actual value for future you can use that itself to predict further into the future so this is the exponential smoothing actually gives a way of predicting the subsequent values once you have chosen the alpha of course choosing the alpha requires doing some minimization of errors what is the best value of alpha and all that you need to do a little bit of minimization of loss and there are packages to do it so we don't have to worry about that anyway and there is a okay in in the in this approach of single exponential smoothing i already mentioned in the theory class we do not we are assuming that the there is no significant trend happening it is just that the most recent values have contributing but otherwise there is no significant trend happening it is just that the most recent values have contributing but otherwise there is no trend to the whole behavior whereas if there is a trend single exponential smoothing will not work well so we need to introduce a another parameter here he's calling it beta but in the class i could call it as gamma. So, but it is essentially the two parameters where you have not only the if you see the value here, there is a trend also apart from the series, the smooth value of result. So if you see the result that first of all, the trend is having the difference between the current and the previous and the past value of trend. Both are contributing to the trend. And then if you see the result is going to be appending, this is like the smooth value of the series and the trend. And the level itself, how is it calculated? It is calculated based on this similar calculation. If you see this, this is the Python way of writing it. But if you see the level, it has the alpha times. This is basically the single exponential smoothing component is coming here the past values getting multiplied by 1 minus alpha the most recent value getting multiplied by alpha so this is the double exponential smoothing implemented in this function here okay and again we can plot it and what we are seeing here is we can plot it with several because we have alpha and beta you have basically a grid of parameters and you can choose the best what is the cartesian product and then which one is giving the best fit in terms of the minimum loss with the original series. Okay, so this is how it is implemented here. And once you have the best value of alpha and beta, you can use it to predict the future values now i mean these are all more meant for i would say historical purposes invariably we do not use these methods today because we have far better methods in terms of either using arima or profit or the deep learning methods so these are more for i would say understanding the time series concepts uh how it evolved over time but i don't think you would be actually implementing these in your real life problems so this is more for a completeness that it is given here okay yeah okay so okay so having done the i have not covered the triple exponential smoothing, but it is a generalization. Okay. The triple exponential smoothing includes also the seasonality part. So we'll have to introduce a third seasonality and then do it. It is not done here. here but now this is the next part of the notebook is focusing on the arima method now in arima method what is the right value for p q and d there are some heuristics to determine what is the right value for p q and d so that is what this plot really does what it is doing is it is plotting some a metric called the dickey fuller metric and this dickey fuller metric when you implement it you end up getting um ps 0.547 so let me try to understand what is being done here. Yeah. So this Dickey-Fuller metric in this example, let us say we had this, I think we are using a different, so what we are trying to do here is we are trying to make this, so if you remember one of the points we mentioned in the previous theory classes, So if you remember one of the points we mentioned in the previous theory classes, our goal is to figure out what is the time series, which is stationary. Now if you go back to the original stock prices, it is far from stationary. You see that it is having lots of ups and downs. Correct? So stationary means that a value at a given point should have should be very similar to a value at another point right that is what stationary means right whereas this stock prices is far from stationary it is it is having quite a bit of swings of i mean many many days it is so low and then it has gone so up. So this is true with stock prices. Whereas we know one thing based on stock prices, we know that the change between one day and the next day, let us say we are looking at the stock price today and the stock price tomorrow. Let us just say that we are doing a difference. Now that difference will hardly that that will actually probably have a more stationary behavior what do i mean by stationary behavior it will if you look at the difference between stock price today and yesterday and let us say the stock price between one month from now and I mean 30 days from now and 29 days from now and the stock price one year ago and 365 days ago and 366 days ago so if you look at that difference they are probably going to be very similarly behaving so if you see the their behavior will have a stationary behavior because you they will all be going up and down over the same uh zero value because one day it would have gone up another day it would have gone down so it is it is very unlikely to see uh this kind of huge swings in the pri in this behavior of uh delta between one day and the next so this this is what this dickey fuller metric tries to do so this dickey fuller metric is a test for stationarity so what we are doing is in this dickey fuller metric it comes up with a value for so the way it is done here is this you have this p value there is a so this is where this package which we had in the beginning i said when we come to it i'll explain so we had these um stats model smt and all that so this is being used to compute the dickey fuller metric now a dickey fuller metric which is less than 0.05 is usually considered a good indication that you have come to a stationary distribution so the reason why we need this is to do these arima models we we typically need to have a stationary uh uh time series to begin with so if you start with a time series which has a dickie fuller metric of 0.5 that straight away tells us that we are having a very poorly it is not really stationary in fact if you do the this is one more way of seeing it there is a lot of correlation between the value today and the value the next day because we know that that's how stock prices are. Stock prices are not I mean, whatever is the stock price today, it is very closely correlated with the stock price yesterday and whatever is yesterday is correlated with the stock price two days back and so on. OK, and if you see there are two ways of doing this. There is a auto correlation does the full correlation between one and all the days partial auto correlation does only with the previous day and neglects with the other day so in either way you see that there is a very strong auto correlation okay whereas if you do this difference where you are taking the closing price today and the closing price one day ago, now this turns out to be almost uncorrelated. So this is and here is where we have a Dickey-Fuller metric of zero also, which means we have actually come to a stationary distribution. distribution so here if you see the auto correlation uh if you see there is no correlation whatever happened like the fact that you had let us say today and yesterday there was a delta of i don't know five dollars yesterday and day before yesterday there is no reason to believe that it should have been five dollars it could have been minus five for example so the fact that there is no correlation is coming here and this means that we are in a once we have come to a difference we have actually come to a stationary distribution and this is good for us because this tells us that what should be the so this this kind of indicates to us that d equal to one is a good choice d equal to one because that that is the if you see if you remember in arima we have this pq and d d is the number of differences that we need to make before we can have a stationary distribution so because arima model comes with a lot of these parameters i mean there is quite a bit of tweaking to do otherwise you will be coming with a model which doesn't make any sense so we start with d equal to one and we have a range of i mean why we have this range of zero to five and zero to five because we do not know what should be the right pa and right q so we have to kind of fit it is it clear i i don't know if i one question i have here is i understood the differencing what does d more than one mean what what how do we intuit that okay that means that uh d more than one means that you need to do two times uh just doing the difference once is not good enough you have to do like this can happen if you have a quadratic growth um if you have a quadratic growth you know that the first different derivative is still growing and therefore you need to take one more derivative to make it stationary okay so this is uh this is where it is used so using this dickie fuller metric will give us a i mean what we can do is we can iteratively apply the dickie fuller metric we first do the dickie fuller metric on the original distribution we see that it is high therefore we take a finite difference once and see what is the dickie fuller let us say it is still high then we do it once more and let us say it becomes low. It means we are good. We have d equal to 2 is the right value. Sorry, is there a case where differencing doesn't work? I don't know about that. See, that is the problem with these Arima models. We are assuming a lot of linearity and we are, I mean, there are so many assumptions made that there is a regression model which is having linear coefficients with the past values, regression with noise variables with linear coefficients, and that some taking finite amount of differences. I think, I mean, surely it has to work in a large class of problems, which is why it is so popular and famous. But if somebody is new to the time series analysis, they may not be able to figure out, okay, what should be the right value of D, what should be the PQ and all that, which is why I think profit is i would say because you can easily interpret each and every implement i mean each and every parameter you are passing in you can interpret it you know that if you are giving a seasonality with the weekly seasonality you know what it means and likewise if you are saying that this is having a saturating trend we know that there is somewhere there is a capacity and we are trying to reach that capacity so there are these interpretable parameters unlike arema where lot of parameters are more mathematical and we have to probably with lot of practice we get the intuition and the d uh so the the p and q uh is that implemented after the differencing or before the auto regression and moving average so uh what is harima model comes with parameters pqd capital p capital q capital d and the period which is here mentioned as s now all of them are unknown to begin with now here because we are trying to reduce the problem we if you see we are still putting a range of 0 to 5 0 to 5 0 to 5 and all that here the only thing is because we see that doing a first difference is making it if we didn't know let us say we didn't know this we could have put d range 0 to 5. and it would have made the problem even slower because we have to iterate through all those possibilities to arrive at the arima rate so what i'm trying to understand is uh does the difference so differencing happens before the auto regressive and moving average component starts right do we start with stationarity and then do the modeling or see arima takes all those parameters so we are just trying to freeze on the d here okay so if you see the arima it is taking in if you look at the maybe i'll jump ahead and look at the actual call to the Arima. If you see this, it is having all the parameters, having the P, D, Q, this is the Q, and then it is having the capital P, capital D, capital Q and the period. So it is taking all those parameters. The Arima function is it is just it takes all those functions but we are trying to be a bit clever in this in this particular notebook because we know that taking a first difference is making it stationary we are not really taking a range of d's we could have taken a range of d's if we didn't know and said which is the best day we could have tried to come in fact auto arima does that there is something called auto arima which where we are not at all doing all this dickie fuller metric and all that we have no idea about the P no idea about Q no idea about D and no idea about seasonality so we take we just take a random grid of values and we choose which is the best by using the auto arima so that is the auto arima but here in this problem we know that by taking first difference we are getting a distribution a time series which is having stationary behavior which means by stationary it means if you look at the point here and the point here there is not much difference they are they are behaving identically and therefore we know that d has to be one so that is what's confusing me so if it's stationary how would auto regression work because then one is not dependent on the other right and now oh no no no no this is stationary in the in the sense of a random variable what i mean by stationary means auto regression can still be the case i mean you your current value can still depend on past value but let us say the current value is, I mean, if you're taking, how do I explain it? You see, let us say you had a steady trend. If you had a steady trend, then the current value and the future value will surely be different apart from the noisy terms. Right. surely be different apart from the noisy terms right if you had a steady growing trend the current value let us say the you are looking at the population growth uh with which is probably doubling every i don't know three years or something like that but then wouldn't that be taken out once you difference it once or twice yes yes yes exactly exactly but then how would the auto regression work if it's already different it won't work right like the auto regression component p in auto regression you are saying that the current value depends on the previous value okay but you are not saying that it is the previous value plus a delta. If you are saying it is the previous value plus a delta, that is where the difference tries to take care of it. You got that? So let us say the current, let us say that is, if you are having the current value is equal to previous value plus point i don't know 1.3 times the two previous values now that is not saying that there is a trend whereas if you say that the current value is equal to the previous value plus five then yes it is a trend so but in auto regression you don't you you you are allowing for the regression which doesn't involve a steady delta you're using your you're allowing other possible values of self-regression not not that constant intercept term where the current value is the previous plus delta that that delta is being taken care of by the d nice and what is the difference between autocorrelation and partial autocorrelation okay so that is a little mathematical what it is autocorrelation is it is taking correlation of a given value with all possible values. But in partial autocorrelation, you're saying that the dependence on the past values which comes independent of the dependence on the because because when something depends on the previous value, the previous value, of course, depends on its previous value, right? And therefore you have a autocorrelation. If A depends on B and B depends on C, there is an A depending on C. That is the full autocorrelation. But if you want to say A depends on B and independently A also depends on c then that is a partial auto correlation okay uh i mean if you go back to the auto regression problem you could have said that current value is equal to uh having some noise term plus uh i don't know some alpha times the previous term then it is only correlated with the previous one but because it is correlated with the previous one it is also having correlation with the one the one which is even previous to it because the previous one is correlated with the one before it so that is the uh auto correlation which is coming because of association. But let us say you had an autoregression where you had Alpha one times the previous value plus Alpha two times two previous value. Then it is independently being regressed on both the previous and the previous to previous. Then there would be a partial autoc correlation with both of them so uh one last question so if there is after differencing uh is there a in that dickie fuller chart which you are showing above right the p equal to zero chart no the zero one the one below for differencing is if there is a trend like if there is a trend will we see a different kind of a chart or will it still look like this after difference we will we will see here we will see it won't look like this it will have a p value which is not zero and it will look so again i mean it will it will also have a probably if there is a trend in the difference it will again show a steady growth or something but that does that still means that the differencing is good it's just that there is some trend right no it means you need to do one more differencing that's what it means right if the trend itself is showing a trend but um so we are differencing to make it stationary but if there is trend that's what the auto regression part will take care of right no no no auto regression is not to take care of trend auto regression is to take care of other regression apart from a trend. The differencing is to take care of trend. Oh, okay. Yeah. So yeah, it's a bit confusing. The differencing is to take care of trends and this is to take care of other effects arising, which is not the trending part other than the D is always to take care of trends. So there is trend and seasonality. Differencing takes care of trend. Yes. Auto regression takes care of what? Like noise? Auto regression and this moving average regression, both of them take care of events in the past affecting what is happening now. But ones which we don't know. Yeah, events in the past. So here Q takes care of events in the past which we have not much idea about affecting what is happening today. Whereas P takes care of actual series events in the past which are affecting what is happening today whereas p's take care of actual the series events in the past which are affecting what is happening today right so let us say there is no trend but yet we know that the current value of x is having some whatever is the past value it has a i don't know 50 effect on whatever is the current value and apart from that there is a noise term so let us say there is a 0.5 of the x t minus 1 plus some noise which means our current value of x has a dependence on whatever the last value was but that doesn't therefore mean that there is a trend because it is the trend would mean if you are saying it is the past value plus a delta that delta is not included in the auto regression that auto regression in this piece you don't include a delta that delta gets taken care in the d's i don't know if i explained it right did you get what i'm saying in a way i am still confused between differencing and auto regression exactly so so differencing i understood it takes care of trend yes yes auto regression auto regression you can say it takes care of the uh all the association apart from trends because you can have other kinds of association right with the past values uh but and those being taken care of by cues, the noise terms. So that is the most generic one. See here, on the one hand, you are saying that it can depend on the past value of the series. Another is you're saying that it can depend on some finitely many past other events. Like I gave an example last time that the marks you scored today it can depend on the effort you put in last week or the effort you put in the week before so that is the way cues you should interpret whereas if you are looking at the revenue today and the revenue one quarter ago there may be uh yes whatever was the revenue one quarter ago may have a contribution to the revenue today because invariably at least 50 percent of the deals continue for example let us say whatever was happened one quarter ago you expect at least 50% of them to contribute to the current quarter. So if you are having 0.5 times, the Xt minus two will anyway be there. And then there will be some other noisy term. So that is what this takes care of. Okay, got it. Got it. Okay, got it. Got it. Okay, so yeah, so here if you see what we have done, we have figured that D has to be one, and then we are trying to optimize it with lots of parameters which are in zero to five range and all that and we are coming up with the so in this if you run this you will you are saving the best model the one which is having the best parameters are being saved and you're the best model summary so this is being run here and once you have the best model summary, we are trying to predict the best. Now you have a value for P, Q, D, S, everything you have. So once you have the best P, so it turns out that the best P, Q, D happens to be 0, 1, 0. That means it is uncorrelated. P is 0, Q is 0. I mean, this is a stock price right so this is not surprising that we are getting some weird results and for that other one we have got a which is the is it like four quarters? Two, three, four, five. I'm trying to understand seasonal order. I think PQD is. But Chander, what you are seeing on your results are not the same that I see on my results. So I guess there is. Yeah, it is because I mean, there will be a noisiness that comes in. So I think we should not worry too much about it, but I want to, at least I want to interpret which is, so maybe let me do one thing. Let me run this and let us, let me. Oh, that thing takes about 20 plus minutes to finish. Oh, it takes 20 plus minutes. So let me not run it right now um i am just trying to understand which is the pqd here um we could have done a you have already run it have you run it i have run it um okay can you do one extra line and print the pq uh just do a separately at this point print the print these values pq pq so that we know what is okay let me do that yeah what error metric are we using here to come up with the best model okay the here it is uh it is using this aic best aic model dot aic so this is training data fit yeah yeah there is only training data here now did you print it out yeah chander it's uh in the order that you said right pq pq it's 0033 so yeah i mean it has discovered in this because of the data it has discovered that there is no auto correlation and there is a capital p and capital q what about the s did you print s no I didn't it's a smallest where is this smallest smallest let me just add that quick because those were all the parameters which we are trying to figure out here yes is five five which means it is assuming that there is a seasonality of five uh and the capital p and capital q being three means that it is having a three orders of regression in both the auto regression part as well as the moving average part for the seasonal terms and it's saying that there is no auto regression and no moving average in the unseasonal part of the arima so this is this is what it has come up with um and we can once we have the best one we can actually do it make prediction because we we have this arima model we have frozen theRIMA model. We have frozen the PQD, all those parameters, and we can use it to predict. So this is what it, so this is the summary of the ARIMA model. I mean, I don't think there is more to it at this point. So let me move on to the third notebook, which is using profit. I think air quality is the one which uses profit. So here one of you said that you were not able to get the. Profit to load rate. Anyone have you tried this this again I have used one notebook which has come from internet I have referenced it but for profit the documentation is so extensive you can even go to the profit webpage and it is I think having very good examples you can actually walk through those examples also i mean i i i wanted to show an end-to-end notebook but if you actually go to this you can go to get started in python it is giving lots of examples as individual snippets it is giving it as individual snippets. It is giving it as individual snippets. It is giving it as individual snippets which you can run both in R and in Python. And it is also saying how you can tweak the parameters. So this is good enough, but I wanted an end-to-end notebook. Therefore, I used this reference. And one of you said you are not able to install. So it's working now. Okay, it's working, working fine so there are instructions given what you need to do to make it work okay um yeah so i i was able to get it work on my local with just basic installation so what we have done here in this having installed profit um having installed profit this air quality is csv which contains all this data so it has so we have we are loading it and i think we are using this to predict how the nitrogen content nitrous oxide content is going to be that is what the this particular notebook is trying to do so we are if you see there is a date data being cleaned here so again we are using the date whatever we did earlier to date time And I think finally we come to only columns which contain nitrous oxide, I think. Okay. Let me, so we are having a lot of columns, but if you see the modeling is focusing only on is focusing only on NOX. So this is the, we are dropping all the columns apart from the one which has NOX. And so finally we have a data which contains a date and the value for NOX. Okay. And we are using this to see how the future values are going to be. So this is what this particular notebook is trying to do. So let us start from here because the rest of the stuff is straightforward. What we have done most of the time we plot it and yeah. So here, whatever we are doing in the beginning is all straightforward stuff, which we normally do. We load it and we are dropping columns. We are getting rid of NANs. But finally, this is the data. So if you see, we are importing profit. And so in profit, we expect the columns to be of the form. We need to have this DS. I mean, the other one can it doesn't have to be way. But this is one of the columns which on which we are doing the time series. So this this needs to be restated to. I mean, the column has to be called DS. It is explained also in the profit package. If you go to the Facebook Web page. OK, and. package if you go to the facebook web page okay and so what are we trying to do we are the prediction size 30 means we are trying to predict uh the last 30 values so we are we are we are we are getting rid of the last 30 values here so if you see train df is basically whatever is the original data frame without the last 30. And then we are going to use it to predict the last 30. So this is what is happening in this line number here. And we are using the so it's very simple. We are using the profit without any parameters. So like I told, this is the most naive way of using where we are allowing profit to take all the default assumptions. We are not doing any yearly seasonality, nothing. I mean, we are. So everything, if you see, there is no seasonality, there's no weekly seasonality, no daily seasonality, because we have not put, if we needed to put yearly seasonality as true, we should have put it here. Likewise, weekly seasonality is true. And many other, we could have added our own seasonality if we want. So those are all the features available. So right now we are saying none of them is there we are just having some maybe the trends which and again the trends will only be linear here and piecewise linear why is it piecewise linear because profit automatically assumes that there are some change points in the past and it doesn't assume saturating because we have not specified saturating so it is assuming only linear and piecewise linear piecewise linear means between some change points it becomes linear so with this profit which is in the most naive way it fits the data and then you can make once you fit it this becomes the model on which you can make predictions. So usage is very simple and and the predictions, of course, we are going to predict because you we got rid of the last 30. If you remember right here, we got rid of the last 30 and now we are trying to use the model to predict the future of 30 right so this is what is happening here now when you do predict the forecast will include also the past because the that's what the forecast does it will not only for all the past values that we have already got the data it will have values it will also do it for the future 30. that is what this predict does okay uh so if you see originally forecast dot shape is 7313 if you see train dot shape uh where is there a train dot shape maybe not yeah but df had a shape let's see what it was yeah the original data had 73 right and we took got rid of the last 30 means it became 43 now we have added back the 30 that is where it has become 73 again okay so it it came back with all the right and now the forecast has apart from the y hat y hat is the predicted term we also have a y hat lower and y hat upper so the y hat is the predicted term. And then we have the Y hat lower and Y hat upper, which is the upper and lower ranges. So if you see the plot, it is actually plotting the overall forecast with those error bands. And we can we can do components, we can do components and components at this point is only having a linear trend because, as I said, we are not having seasonality. We made seasonality a zero. I mean, we didn't put seasonality for yearly or weekly. We didn't put it that the most basic version of profit we when we use Naively it was all false by default okay so what else can we do on this so we can also calculate the errors so this is just done to see what the errors are between the forecasted value and the actual value because we do know the original data set had for the last 30 also so we can actually calculate the error and um so this is that's this is plotting the y hat y lower y upper and the actual y so this actual y is the one which is in so this this is your y this is your y lower and this is your Y upper. And the actual Y is here. So you see it is, I mean, of course, that very noisy ups and downs have gotten rid of. So it has already taken care of outliers. So creates a smoother prediction so this is about the profit package i mean i have of course i have not included all possible examples that if you go to the profit you can actually do so many things you can you can you can do saturating forecasts you can change points you can modify you can add seasonalities you can include holiday effects and each one of them there is a simple interface in the package to do it you can also do seasonality also can be multiplicative i mean normally we do additive seasonality but that is the default behavior but you can do multiplicative which means you can have the seasonality actually gradually increasing over time so that is what the multiplicative seasonality does okay so this is the third notebook i um and you can play around with i both the reference here along with this facebook's python implementation you can play around with because it is very well described here also okay that's about air quality and the last one i'm going to cover today is a deep learning implementation of it doesn't include the attention model so like i said i will try to get the attention model implementation and i will post it on slack but this one doesn't include the attention based deep learning so this is what we are doing here in this is time series analysis using deep learning methods. So we have in this we can use either RNN, LSTM, GNN, GRU. All of them can be used. In fact, even CNN can be used and we can use multi layers. We can use multiple layers so we can choose to use more than one layers of RNNs, LSTMs and so on. So I have done this example. Let's say walk through it. It will become clear. It is using LSTM, but replacing LSTM with GRU and RNN is just a one liner. I mean, there is no big deal about it. We can, we can easily replace one for the other. Only the attention model will be a little different. And I think we have to use an end or transformer. I will try to get an example and post it. Okay. Any questions so far before I move to this last notebook? Can you go through some of the theory for LSTM and GRU here? Just as a refresh. Okay. LSTM and GRU. So RNN I went through some time back, right? RNN you are clear, right? The RNN is what we see here. This is clear, right? To everyone? Can we quickly just go through that? Yeah. Okay. So RNN, what we are doing in RNN is basically these two equations. In RNN, we have every RNN has a hidden. So this was what came first. We have a hidden state for the RNN and the hidden state itself is going to depend on the input as well as the previous value of hidden state so and these are matrices so this x can be a vector h can be a vector that means these are matrices and of course this h will be a vector this is this is clear or should i explain this more this equation um yeah i think it's good to me it's great okay uh so the the all the feed forward networks we always have vectors and we have matrices uh so the same thing is being done here we have a vector we have a matrix and this is what creates the HFT from this. Right. And this HFT, actually, there is also a tan hyperbolic, which I missed. Let me add that here. We invariably have some activation. Yeah. Activation function. Let me write that so that it's so so typically we have some activation function here acting on this okay this is your y of t and h of t also there is no here there is no activation function h of t is dependent on wh x x of t wh h of t minus one um is there an activation here here not sure i don't remember now so x of t here will will that be the time component yeah yeah x of t is a time series this is the input time series okay x this is the x value this is the input so x here is the input which is at a so this is the input at time oh yeah so there might not be a activation because it's already a number output um yeah i think i have to go and confirm but i think there is no activation here whfx this you have this previous hidden state gets multiplied by a weight matrix for the previous hidden state current x of t gets multiplied by a weight matrix for the input and this creates a hidden state here now this hidden state will get multiplied by a weight matrix for the output and the there will be an activation this could have been any it could have been a tan hyperbolic or it could have been a relu whatever okay but if it's a why is the output right isn't it the prediction yes yes it could be between zero and one right or minus one and one it has to be some actual number correct correct correct so yes uh it could have been also it could have been a without any uh activation i mean it could have been a unit activation also right so it could have been any one of these it could have been a tan hyperbolic or relu or nothing if you didn't have so here i think you here also you should have an activation then because yeah only then you can bring in a non-linearity so this one could have been tan hyperbolic or relo or unit and it can't be right that's what my question is could it be a zero to one or minus one to one activation if it's uh a zero to one or minus one to one activation if it's an actual output if it is an actual output i don't know i'm just yeah yeah you're see what normally happens is this is just one of the layers right so what what now if you are stacking multiple rnns one on top of the other then you can have activations in between the final layer is what doesn't need to help okay so let us say this is instead of doing it as just one rnn you are having one on rnn then another rnn and then a linear layer okay you can have a linear layer where you are having the not a RNN but it just a normal feed-forward dense layer right but then how would the other RNNs get their hidden units I didn't get you the ht-1s where would the okay maybe I should let me try to write it afresh because i'm otherwise cluttering the same page so here we have this rnn block okay now this rnn block by itself has a y now you can feed one more rnn here or you could feed a dense layer this could have been any other any of the blocks. It could have been an RNN or a dense layer. Either of them could have happened. Right here there is an RNN for sure because we want to take into effect the fact that past values can affect the future. So that is why we have this here. Now this one it could have again been an RNN or it could have been just a dense where you're just saying that whatever inputs come here, they have a big contribute to the outputs. Right. Now, what you are saying is your final Y of T, which is your output, could have should not have an activation because you expect it to be a real signal. That is true. But all the intermediate levels, because in neural networks, we need to have these. I mean, intermediate signals will have relu's and tan hyperbolic's and all those. Okay. But for the cascading RNN, the XT would be the output YT of the previous RNN? No, no, no. XT is always the input. YT is the output of the RNN. This is the RNN. What about the next RNN? What would be the XT for that RNN? It will be... No, no. When you split it out, it is the same rnn you're just writing it twice you when you're because instead of putting a loop like this you're breaking this and you're putting it here to the same rnn standing here and this becomes x at t plus one this is the that is the okay oh this is the next value of the s so this is the next value i'm talking about the stack rnn like stacking okay in in stacked rnn you are saying that you are having multiple rnn's really so whatever is the output of this the y here is actually creating it is behaving like the x of t uh for this new rnn let us say you have rnn2 right so so my question here it will be if that is the case and this yt into that rnn is already uh tan h or re-do then wouldn't the values anyways lie between zero and one irrespective of if we put some uh 0 and 1 irrespective of if we put some tanh or relu or we do not put it at the end the values will drive between 0 1 or minus 1 1 right if we put activations at all yes yes yes yes i think so the final output cannot be a 0 or 1 kind of a thing right this is a focus no final output is not 0 or 1 no but what i'm saying is if we already have an activation of any of our outputs our final output will have to be between zero and one right we cannot have any activation no i don't understand see we always know in in neural networks we always try to normalize the values so that they are lying between zero and one we do this max min scalar and all that i don't know if you remember um one of so we we try to because otherwise we don't have good performance on the neural network we are even if we have values of x of t which are uh in a range maybe from i don't know minus 500 500, let us say, we try to scale it down so that it is in the range of minus one to one, plus one. We do this maximum scalar so that the inputs are always between the having values, which are between minus one and plus one, and then only feed it into the neural network so this we anyway do for all neural networks so with that as the background your y of t even it is a good idea to scale it again i mean it is a good idea to scale it again so one of the ways of scaling it would be using tan hyperbolics because that will make sure that your output is always between minus one and plus one right i mean relu is going to make it strictly positive maybe not a good idea when you are having a time series which can take positive or negative values but tan hyperbolic is a good choice of an activation function here and then that will automatically make sure that the input to the next rnn or any other if it was not rnn it was maybe just a dense layer either of the cases you are making sure that the inputs are in the range of minus one to plus one if you are having a tan hyperbolic here so okay but i guess output has to be uh one which is not activated right unless our our training data itself is between minus one and one or if we have already converted it to that's what we do anyway right normally in all time series we anyway if you even in this example of notebook that i'm going to show we have done the max min scaling if you see let's go to the section if you see there is a min max scalar range minus one to plus one so this is always done okay i see okay So because if we don't do it, neural networks don't perform well. Okay. So even if your inputs, like for example, the number of passengers and all that, it is obviously going to be far greater than one. But the fact is to make a prediction prediction we convert it into a range of minus one to plus one and then the output also becomes minus one to plus one we use the same whatever scalar transform we used we invert the scalar transform to come back to the original prediction right okay uh so the theory wise i mean rnn is clear right where we are rnn is basically using two main equations um the equation like i said uh one of them says that h at time t so let me put activations on both because in general you can have activation on some activation one acting on W H X on X of T. So here hidden layer is kind of autoregressive component. Wait, let me write it H H on h of t minus 1 so this is the first equation of the rnn which says that there is a weight matrix which converts the input vector into the hidden vector and then there is a weight matrix which converts the previous hidden vector into the hidden vector both of them contribute together and then an activation acts on it to create the current hidden vector so this is the first equation the next equation says that there is some activation to which again can be any one of the activations which is having a weight matrix which converts the hidden vector at time T into the output at time t so this is what rnn does right okay and all the other lstm and gru i mean they are gated units which are doing generalization of the same thing. All they are doing is, in LSTM, for example, it says that there is not just a hidden state. There is a cell state also. And the cell state is, instead of just applying, I mean, it turned out that using this ended up creating problems when we are trying to have long-term dependencies because in long-term dependencies you ended up having either vanishing gradients or exploding gradients i mean the the reason being you end up having the weight matrices actually keep multiplying each other if you go back in time and if depending on the i either it would explode if the corresponding eigen value is greater than one or it would become zero if the corresponding eigen value is less than one so either of the cases were not helpful so it meant it meant that you really couldn't persist memory for a little longer period so they came up with explain that in like slightly more detail i i mean okay i didn't understand the i can really so y of t if i try to write it as a to w y i mean i'm not sure if i'll be able to exactly replicate the mathematics but intuitively what we are doing is we have this h of t and we let us say we substitute so we have a to w y h and then we have a yeah got it on w h x X of T plus WH H H of T minus 1 right and now let us say I go back and rewrite because this one I go back and rewrite H of t minus one in terms of so i have h of t minus one in terms of x of t minus one and h of t minus two right so i if i go back and write it i see that i am going to multiply matrices w h h with w h x and w h h and i can keep doing this i can keep whx and whh and i can keep doing this i can keep uh replacing h of t minus one with h of t minus two h of t minus three and so on and i will end up having lots of matrix multiplications which are repeated so i'll have a whh times whx and it will get keep getting squared and as i keep going back in time so it means that those matrices if i write it as let me try to say only for the h component i'll have a w h h whole squared when i try to go back and then there will be a ht minus two then if i go back in time three times h of t minus three i mean i'm only looking at the last component wh times wh okay so a similar argument can be made elsewhere so as i keep going back in time i will end up having w h h to the power of n h of t minus n now this matrix depending on the eigenvalues i mean this is ultimately it's a matrix right a matrix think of it as a generalization of alpha per n now alpha power n is either going to become infinity if alpha is greater than one or becomes zero when alpha is less than one. So there are two possibilities, right? As n becomes large, alpha will become infinity or alpha will become zero depending on alpha being greater than one or less than one. So in a similar way, this weight matrix will either blow up or it will go down to zero as n becomes large you got that point right yeah so uh i mean the exact details for each and every component i'm not writing but the intuition is essentially that just using rnn's ends up creating a problem of vanishing or exploding gradients. It won't be able to back propagate. Yes, yes. We will not be able to back propagate because it will end up saturating. And it means that the dependence on the past values actually becomes negligible. Okay. negligible okay so this LSTM and GRU were alternate ways where we said that okay I'll first talk about LSTM and GRU becomes a kind of simplification on LSTM so in LSTM we say that there is not only a hidden state which we had in RNN we also have a cell state and each one of of them, so what is being done here is the cell state. So there are gates introduced called forget gate, then input gate, output gate. Okay, so what are these? What do they mean? So the forget gate tells you how much of the cell state remembers from one to the next. So it means, let us say there is a cell state at Ct and there is a cell state at Ct plus one. Now, the forget gate tells us whether the previous value of the cell state propagates to the next state that is what the forget gate tells how much of it let so forget gate will actually have an output of between zero and one if it is zero it means the previous cell state is forgotten if it is one it means the previous cell state is remembered it could have been anything between zero and one likewise the input gate tells how much of the input affects the current cell state i mean uh how much is the uh how much is the cell state takes how much the CET plus 1, how much it depends on the I at time T plus 1. Okay, because the cell state at any given point not only depends on the cell state in the past, it also has to depend on the current value of the input. So how much is the input going to affect the current cell state or not that is defined that is what the input gate tells so the input gate is again the output of the input gate will be something from zero to one zero means the current input doesn't contribute one means the current input contributes to the cell OK, so these together forget gate and input gate end up telling how much is the current cell state dependent on the previous cell state and the current input. OK, and then the output gate is telling how much is the cell state going to affect the hidden state like it could be that the current cell state has no impact on the output so i i think this this usually is considered as the minus one to plus one i think so here this output gate whether, whether it has a contribution, zero contribution or a minus one contribution or a plus one contribution to the hidden state. So this is what the output gate. So these three gates are there in the LSTM and that helps in propagating because now the advantage is unlike in the past in the RNN for sure the cell state of 10 cycles ago had no impact whereas if here you could have configured the forget gate in such a way that it was having lots of ones if it had lots of ones then it means the cell state 10 states ago could have preserved into the cell state today. So what do you mean lots of one? See the forget gate is basically a value from zero to one, right? If it is one, it implies that the current cell state is strongly correlated with the CT minus one. So let us say we configure the Fargate gate in such a way. I mean, Fargate gate is determined by some weight matrices. Let us say it is 1, which means your CT can be equal to CT-100 also because CT is equal to CT-1, CT-1 is equal to CT-2 and so on. It is not vanishing or exploding so the cell state can get preserved are these hyper parameters all these three no no no these are gates again there there are weight matrices which contribute so as part of the whole training process you also train the um weights which govern these gates okay okay so I mean in in here in in RNN we had very few weight matrices in RNN we had only WH H WH X and W Y H these were the only matrices only parameters to train whereas in lstm you have many more you have each one of these gates is having some weight matrices so you you have one for a forget gate you have one for the input gate you have one for the output gate okay and you have relations all of them contribute to the so when you're training you end up training all of these but the point is here in principle it was never possible to remember the state beyond a certain number of cycles whereas here in principle it is possible to having trained it you there is a possibility that the cell state is remembered across cycles because it is not vanishing i mean you can have a situation where the forget gate output is one and if the forget gate output is one there is theoretically a possibility that the cell state is remembered across hundred cycles but how are these matrices different than rnn? Like what code causes it to be a forget gate and not a WHH gate? No, no. The architecture internally is different. So, okay. Maybe there are good LSTM. Let me try to just... LSTM, let me try to just LSTM, maybe a good, let me see if this helps. not a great article I think this is not too great see the internal architecture is very different if you see this picture here the weight in a in a rnn there are there is a the beauty of rnn is it is very simple to explain because it is it is just having the X Y and H so your H depends on X and the previous value of H and Y depends on H so there are these three matrices which you end up training during your RN and training in LSTM there are many more matrices to train sorry many more matrices because you have all these there's a w matrix here w matrix here w matrix here w matrix here but there are all these matrices and again they repeat because this l the lstm is the same lstm when you when you're going here it is the same lstm at a different instant of time so the weight matrices are the same but instead of having only three matrices to train you have one two three four five matrices and each one of those matrices is having some dimensionality right but the advantage here is that there is no vanishing and exploding because of the fact that these weight matrices do not all end up multiplying one one another so that they exponentially vanish or exponentially go up that is the only thing so in principle you can have a situation where the cell state gets remembered over long periods of time i mean there is you cannot directly say how is whh defined in terms of these that it is not there because here first of all you have two different values H and C you have a cell state and you have a H so so so just tell you know thinking about attention, the training automatically decides how far back you want to pay attention to a time series. Yes, one way of saying that. concept where we are explicitly introducing dependencies between the current value and the some of the past values so attention is a little different LSTM is still going step by step it is still going only one I mean you are still saying that the LSTM model also is very similar to the RNN architecture where you have this LSTM block. LSTM block, which has not only a H, but also a cell state. Both are there. Right. So, again, the cell state and the H state go here. So you have an X and you have a y and the y is actually the same as h here okay so you can you can say that this is this is also h so what you are doing in this lstm is um it is similar to the rnn structure where you have x at time t goes in it takes in the h at time t minus 1 c at time t minus 1 and it creates a c at time t it creates a hatchet time t and that contributes to the at t plus one right is it is this picture clear yeah yeah what i'm saying is after training the weight matrices kind of give a feel of attention right in terms of what it is not exactly attention because the way attention is explained is little different. In attention, what you're saying is that... The only difference will be this one will be sequential while attention will be more... Correct. Correct. In attention, you are actually paying attention to, let us say the Y of T pays attention to... So let me write that here instead of so let me in attention model what you're saying is you have x 1 x 2 x 3 x 4 for example and let us say you have a y 1 y 2 say you have a y1 y2 y3 y4 you're you're saying that your y1 can pay attention to this as well as this i mean x2 right for example so you're directly saying that this can depend on this and this rather than through x4 whereas in this lstm models and rnn models you are always saying that y1 i mean let us say this is x of 5 instead of saying x of 6 x of 7 x of 8 so the instead of saying that 5 depends on 4 and therefore depends on 3 and therefore depends on 2 and therefore depends on three and therefore depends on two and therefore dependence on one you are directly saying that x of y has a dependence on two so that it is paying attention to some of the past values as well so that is what attention model does so here you are it is the encoder decoder architecture where you are encoding it into some abstract vectors here and they are being taken in here and then there is a decoder here which takes in not only the last value but this abstraction and this abstraction contains all the information about different values and they are all going in and contributing to x of 5. So this is the attention model but LSTM, GRU and RNNs are following a sequential approach. Nice. But even with the sequential approach, if it's going to pay attention on x2 and x4 x3 and x1 in the matrix will be zero right and ones will be on x1 and x4 for the matrix for the lstm matrix the forget me lstm is uh it is saying that your h at t plus one depends on X at t plus 1 and whatever H and t and c and t are. So it has all these dependencies. Now, these may in turn depend on this. And therefore, it will depend on the X at previous time and all that. But so my question is, if, say, there is a long-term dependency on the previous X, Xt minus 10 or whatever. Yes. But there is no dependency on the previous x x t minus 10 or whatever but there is no dependency on any x after x t minus 10 then uh does that mean that's not possible in rstm or does that mean the matrix will be such that there will be a one for x of t minus 10 and zeros afterwards uh i'm not sure of the you understand what i'm saying i understood what you're saying um or there needs to be a continuous line of one of the cell before you see here we are assuming that it is a time series which which is having some, I mean, if you are saying that X of T only depends on X of T minus 10, it means you are saying that there is a seasonality with 10, right? Something like that you are saying. So if there is a seasonality with 10, it is possible that X1, X2, X3 up to x9 are all uncorrelated with each other but then x10 is actually equal to x1 x11 is equal to x2 and so on for example so you can have a time series like that now whether an lsdm can accurately predict that kind of a time series so let if you let me try to draw the time series let us say i have a time series so you this is your question let us say i have a time series where i have some value at x equal to 0 some other value at x equal to 1 some value at 3 4 let's say only uh 5. Maybe if the value is just occurring at intervals of 10 and there is nothing in between. Something like that. Let me try to complete the thought process of what you are saying. So you have some randomness. If you just look at these 5, it is random. But then when you go to the 5, it kind of repeats. You see that whatever is happening at x1 has happened at x5 right whatever has happened here has happened here whatever has happened here has happened here okay so let us say it follows this pattern and you are now your your question is whether an lstm based model will be able to whether an LSTM based model will be able to predict this series. I think, yes, I'm not able to tell you why, but I mean, LSTM should be able to, it may not be as efficient as attention models, but I think it will be able to, because it is just a periodic waveform ultimately. So I think it will be able to. So the underlying concept i was trying to get at is and again i'm i don't remember much of lstm but you talked about matrices right forget matrix or input output so yes and ones and zeros so will those ones and zeros be encoded in the matrix such that it could remember or will it be some different mechanism altogether i i don't know i think we may have to we may have to try a synthetic example where we are putting some random values and then trying to repeat it and trying to see if lstm is able to lstm based time series whether it can predict it or not i i don't know the answer we can try it i mean it shouldn't be we can use the same notebook that we have in this example whatever i showed in this example we can go through this example okay that was it yeah so in this example we can actually you can try it as an exercise homework exercise no you create a synthetic model which is having a time period of five or ten or whatever where within those first ten things are totally uncorrelated there is no connection and then it repeats after every 10 and try to use the same i mean use the data set and see if it's able to predict with just this lstm because here we have used lstm in this notebook we have used lstm so you can tweak it just the input data will be the different data let us see if it works if it works we know that i mean i am not able to think why it will or it will not work at this point i'm not sure but intuitively chander just the fact that the whole lstm right long short term memory yes the information that let's say at time at time t0 that needs to influence let's say t13 it needs to pass through one two three all the way till 12 to be able to go influence 13 correct yes yes yes correct so the if the interval keeps getting longer and longer lstm should will fail right it will perform inferior way yes correct correct that is that is why attention models are preferred so to the question that nisar was asking i mean if it's a shorter time interval if the influence is happening from let's say five periods ago or four periods ago the chance of lstm working is higher than if the influence was happening over a significantly long back period. Isn't that the correct intuition? I think so. But honestly, I have not tried enough of these notebooks myself to say one way or the other. Theoretically, whatever you're saying does seem correct yeah the intuition i was trying to get more at is does does there need to be like does it need to have like a sequential correlation and as soon as the correlation ends you cannot think back like the lstm cannot no no no no it is that is what i'm saying is not true that is why i'm saying you can try out an example because no see lstm also is not only it is also having a linear layer on top i mean you can you can it is not just lstm based time series you can you can put a linear layer on top to create more non-linearities so you can you can arbitrary functions can be simulated with LSTMs with with any neural network we know that right arbitrary functions can be simulated with neural networks so ultimately if you see this is just a is this if all we were trying to model is this function we know that even we don't even need an rnn we we know that with a regular neural network we can simulate this function right if if you have access so i believe that with lstm we should be able to but best is to try it out with the synthetic data and confirm it. Okay. Okay, so this is the last notebook. I think we have some time, right? So it's not, the notebook is very straightforward. So you can play around with it at your pace. So we have loaded, I mean, there are the standard torch imports and this one is basically just to get all the data set names but really we are only focused on the flights data set and this is the passengers in different months okay so here i think what are we doing we are plotting month versus passenger so we have up to 140 months we have all the passengers and we are seeing some seasonality is there there is a seasonality we see in the graph itself and we are trying to now predict this future using this notebook okay so we have this all data and here we are getting rid of the last 12 basically the last 12 months data is being gotten rid of and the train data is all the data except the last 12 months and the test data is only going to be the last 12 months okay and here is we are doing this min max scalar to convert the trade data into minus one to plus one okay and so here is the training data which is normalized this is the first five values last week just being printed out and what are we doing next float tensor converting train sequence okay so here is something which is interesting what is done in all these rnns we need to have a all these time series based network models we need to have a sequence length so we need to create a x which is a vector of inputs which is x1 up to so here you are taking the this is based on a this is tw is the width of the sequence so what we are doing is the x1 up to let us say it was 10 10 and this was 10. it means we are taking from uh 10 10 and this was 10. it means we are taking from um for x 0 to x 10 will be the training input and the corresponding label will be the 11th so if x 0 to x 10 is the input x 11 is the output got it right likewise so we are creating a the original sequence, we are creating lots of training data. So let me write it in so that it becomes clear what we are trying to do here. Let us say we have a data which is x0, x1, x2, and dot, dot, dot up to xn. If we have chosen the training window, the window length as 10, then we are saying that x0, x1, up to x9 is the x vector and x10 is the y vector, is the output. So this is what this line number is doing. And then we are doing the same thing. We are creating more multiple of this we are saying x1 x2 x10 is the input and x11 is the output okay so this line sorry this in out sequences is essentially doing that it is creating lots of training examples from the input data and the corresponding outputs. And it is creating input and output. So the training sequence and training label. Clear? Yeah. Okay. yeah okay and this train window is taken as 12 which means it is taking or at any given point it is taking 12 consecutive months to determine what the next month's prediction should be okay so here we have this in out sequence so we have this 12 months data and the next months then next 12 months and then the next output and so on. Okay. So if you see the 0.9385 becomes the first element of the next one. So this is what is done in this model. Typically in all RNNs you do this where you have a sequence length because you need to define a sequence length to have a otherwise your input will become infinitely long. So you need to have an input which is having a finite sequence length. Of course you can break your original arbitrary length sequence into many sequences of that many length so that is what is being done in this in out sequences okay and now you are feeding it to the lstm module so first we are defining the class the class is very simple you are you are having a lstm this is coming from pytorch which has the input size and hidden layer size so the input size in this case is i think it is going to be let us see how it is called okay the input size is an input parameter to this class okay and the forward the for the all these whenever it inherits from module you need to implement the forward which essentially says what the network does so first of all you apply the lstm which takes in the input and the hidden right and the hidden right and it creates an output and a hidden cell so both it this is what your lstm does it takes the input it takes the hidden cell value and it creates a output and a new hidden cell value okay so hidden cell consists of both the hidden state and the cell state. I explained some time back that you have a hidden state as well as a cell state. So the LSTM consists of both. So this hidden underscore cell internally contains both the hidden state and the cell state in two separate variables. So this is what is happening in this line and then you are you are adding here if you see you're adding a linear layer what i said no sometimes you add you just the lstm is not enough you add a linear layer on top of it so the linear layer is from the which is having the hidden layer size to the output size and here the output size by default is one let us see what is happening in this gender what is the hidden cell hidden underscore cell underscore we had two things right we had a hidden state and a cell state right right so both of them are combined in this into one variable called hidden underscore cell oh okay so we'll see when you come to it you'll see that how it is being used okay so here the model is being called with the default parameters okay if it is being called with the default parameters it means that input size is one hidden layer size is 100 and output size is one and the last function is being called the mean squared error loss and you have the atom optimizer with learning rate 0.001 it is having an lstm which is basically having this one input 100 hidden layers and there is a linear layer the linear layer is having input features as 100 because that is what it is hidden layer size is 100 and the output features is 1 and the bias is true is that is the default value. Okay. So. So question, why is input size one? Why is it not the size of the array? No, no. Each value, each passenger is a number, right? So it is not a vector. The input size is one. Because each passenger is just one value. it is not like a vector of passengers right here what we are trying to do is we are trying to predict the output and from the input the time series is what time series is basically uh passengers at different instants of time correct okay yeah the passengers are different instants of time and each passenger the number of passengers is basically a value it is not a vector if it turned out that there were multiple features in the input vector so then it would have been a vector it would have been many uh but aren't we inputting like uh an entire sequence of number of passengers we are inputting 12 at a time that comes later that is a separate uh that is a separate uh parameter which we will see so yes the the way your point is very valid. In fact, I also have got confused on this in the past, but the fact is there is an input size one, there's a sequence length, which is a separate parameter. We'll see that. Okay. So there is an input size, which is one. So if you see here for sequence and labels and train in out sequence. So here the way LSTM works is it takes in as arbitrary sequence lengths, but it still has each of, so what it does is in an LSTM, the input size refers to the, if you go back to this diagram here let me go show this diagram here there was this for example let me go to the rnn case in the rnn case sorry i always end up zooming here okay in this rnn case if you go here i said there are two fundamental equations one equation says that you have the weight matrix multiplying x at time t and whs times h of t minus one correct correct now in this case what is x of t x of t is only one value right it is not a vector okay yes it is true that we are passing in a sequence of 12 but the fact is there is only uh this is this is not taking into account t t minus one t minus two those are yes there are up to t minus 12 that is the sequence the exterior represents the entire sequence x t is the value of the sequence at time t at a given time t oh so in this example x of t means the value of the input at time t so in our case it is the number of passengers at a given month so this is a this is not a vector it is a number right oh oh sorry i got confused so this is not a sequence it's not a vector it is a c i mean in the sense it is a value it is a value for a given value of time it is a value if you are including all the values of t yes it is a sequence but for a given value of t it is a number correct but in this case it represents the sequence right and actually actually represents a sequence that is what we are feeling in like we are filling in the 12 numbers up till time t that's what i'm trying to say so that is that is the way the architect the package takes it okay but the theory wise if you see theory wise this x of t is always a number only unless we have a vector of data let us say we have something like i don't know the speed of air molecules the speed of air molecules and you're trying to figure out the speed of air molecules the speed of air molecules and you're trying to figure out the speed of air molecules at time t at time t plus one time t plus two yes it is a vector because there are three directions to the speed i mean the velocity the velocity will have three directions there it is a vector but here we are saying it is a number of passengers so it's a single value and the how the 12 is being the number of uh the sequence which has 12 no that is being taken in a different parameter that is not being taken as part of this uh lsdn size okay i will see that we'll see that in a moment so one is this input size which is one because there is a scalar yeah yeah one number there is a number of passengers so now what we are doing is for sequence labels and training out sequence so here is where this thing comes so this train in out sequence has each each element of this train in out sequence has the sequence which is having 12 values and a label which is having a single value right okay so here if you see the model acts on sequence now the model is whatever we have already created the model is basically the lsdm model and here the model dot hidden cell so to begin with you are making the your zero you are giving the initial input as zero to both the hidden state and the cell state both the both of these so there are two pairs here hidden layer size hidden layer size so there is a hidden state and a cell state both of these so there are two pairs here hidden layer size hidden layer size so there is a hidden state and a cell state both of them are given values of zeros that is the initial state and then the subsequent values of course will end up based on the weights it will multiply it but this is you have to give an initial value because if you go back here where was it yeah it has to have a h0 yes yeah yes where did i have it very first value i don't know if i think you had it in rnn you had h0 coming in yeah okay so uh whatever it is the very first value has to be initialized and that is initialized with zero typically that is the initialization done the very first value is always done with uh yeah here h of zero so the very first value is uh initialized with zero typically which is what is being done here also uh that is this line here and then you have y print which is the model acting on sequence so this one uh the the sequence yes has 12 values okay but the fact is each one of those 12 is a scalar value so that is the reason why you have the uh input size as one one question i have here is uh does it matter like uh what sequence we select does it affect the you know quality of the model you mean whether it should be 12 or 24 and all that yeah exactly yeah um or should it be the entire time series i don't think it should matter much uh we can try i mean honestly i don't know what is the right one here he has chosen 12 okay but i don't think we can try changing it and seeing how much it affects i don't think it should matter because why i am saying it shouldn't matter is we are anyway if you see the whole rnn approach or lstm approach it just it takes a sequence and it predicts the output and anyway the previous value of the gets included only through the hidden state that we have already seen right in this whole right the way the y of t depends on h of t and h of t depends on x of t if you see it is only dependent on h on t minus one i mean the y of t depends apart from depending on x of t it depends on h of t minus one right that is how y of t depends on the previous it doesn't directly depend on x or x of t minus 1 right so it shouldn't matter i mean maybe you should at least give some 12 or something but whether you give 12 or 24 or 48 or or some other large number it should not affect much because it it is indirectly all the past values are getting encoded into the hidden and cell states no but in that case like from a commercial perspective if you know if they are taking a commercial time series of production of certain items for the last 10 to 20 years, obviously the production of a certain item 20 years ago might not be affecting the item which you're producing right now. Hopefully that assumption. But if you take the entire time series, there'll be a hidden state which will hopefully have some amount of correlation to that 20-year-old item. No, no, we are not saying that. No, what we are saying is you have a long sequence which is being broken down into smaller sequences. That is how the training data is created here. You have a very long sequence. Here, this sequence is consisting of some 140 months but you are not saying that all the 140 months are used to predict the 141st you are breaking it down into 12 month intervals and saying that these 12 depend on predict the 13 then the next 12 predict the 14 next 12 predict the 15 and so on so So, and in the process of doing that, because you're using LSTM, the question is whether you use 12 or 13 or 14, it shouldn't matter much. I mean, you can try it. I mean, honestly, the best way to, these are all with experimentation. You put in the sequence length, which is different from 12 and see how much it is sensitive to it in my opinion it is not going to be very sensitive but the best way to confirm is by running it theoretically it should not matter that's what i'm trying to say theoretically whether you have given a sequence length here the training window 12 let us say instead of training window 12 i gave training window 20. in my opinion it should not affect okay but you try it out because we can always play around i mean it can it can become one of the trials you can do with this notebook change this training window and seeing how it affects your overall prediction okay okay okay so but uh if you see this is uh very simple here once you have done this you you do this y-pred in terms of this and then you do the usual backward propagation too because this is the training phase you're training it and you come up with the best parameters so that the i mean you here it's being trained over 150 epochs okay and once you have trained it over 150 epochs now we are doing the future prediction which is you're trying to use that whatever model you have created to do the evaluation so here is the evaluation done you are trying to again for doing the evaluation you are first zeroing now this one whether it is a good thing to do i'm not sure because i feel the hidden state at the 12 instance ago whether it is important or not is a question mark, right? I mean, it may not matter because that is what we are trying to, let us say, 12 instance ago is a good enough assumption to say that the hidden state doesn't matter. Okay. If you do, if you say that, then yes, it can be zero and you can predict what happens. But doesn't that mean we are just forgetting after every 12 instances? initial hidden state of zero okay so i these are things you can play around with maybe this is one reason why it is a good idea to keep a longer sequence lens because then you are when you are evaluating it you are going to evaluate with a sequence where you are forgetting longer intervals ago rather than can you expand on what this evaluation means what what is that evaluation is no no here we were training it so far we were training it and once we trained it we came up with the best weight matrices to minimize the loss but having done it uh having having come up with the best uh model we need to evaluate it on the test data right so to evaluate it on the test data we have to now apply the model to the so here is where the the model is being applied to the sequence correct so when the model is applied to the sequence we need to start with the initial value for the hidden and cell vectors right but but but for time series shouldn't we actually carry in the hidden states because we are dependent on the past right that is true what i'm saying is if if you let us say all you had is the model okay you don't know the history you have been you have trained the model and you have a model now now you are trying to apply it to a future prediction now when you are trying to apply to the future prediction you are going to come up with the current sequence and trying to generate what the next value of the sequence is now when you are doing that in some sense you are value of the sequence is. Now, when you are doing that, in some sense, you are saying that the sequence is all you have. You don't know what is before the sequence, right? So when you are doing that, the assumption indirectly being made is this sequence is good enough to predict the output. It means this hidden state and cell state don't really matter at the beginning. So let us say in this case the sequence of 12 because we have taken a sequence of 12 it means that with 12 values i'm able to predict the 13th value i really don't need to know what happened before the 12 that is what is say the the fact that i'm zeroing out the hidden state and cell state is essentially saying that. Now if this is not a good assumption, maybe the sequence length should be not 12 but it should be 20 or so that we have a longer sequence to predict the Yeah, but what I am trying to understand is like if we are putting this into production based on whatever history of data we have for a given yes we would want to carry forward those hidden states right i understand for you yes we can we can we can carry it forward so we can keep the hidden state and we can use it in fact that is it can be done it can be if if we did have the entire history we can carry that hidden state and we can use it to predict we can do it that would probably be better approach to doing it than zeroing it out okay okay so this is all there is to this deep learning based approach and this lstM could have been replaced with RNN or GRU without. It is a good prediction is not great here, but not that great. So, I mean, it is not very great. Yes, that is a fact. Wow. So probably this is not the best example. I mean, I got this example more for its instructive ability than for the fact that it is. It is having all the library elements and all the things you need to do like min max scalar and the fact that you have taken a sequence of a given length so those are and the fact that you can easily replace the lstm with an rnn here this could have been replaced with an rnn it could have been replaced with gru so those are the that is why i chose this particular example out of the ones I found. I mean, you can try to see why, whether it can be made better. Maybe you make the sequence length higher. Does it make it better? I'm not sure. Right. And yeah, your point, that is a valid point. Maybe you need to keep the hidden state. yeah your point that is a valid point maybe you need to keep the hidden state so if we keep the hidden state which we already had maybe we will have a better prediction yeah yeah okay so that's about it Thank you.