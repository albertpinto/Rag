 So today we're going to talk about something called generative adversarial networks. These are, this breakthrough came in 2014. It turns out that one of my relatives, actually a nephew, was on the paper. This Ian Goodfellow and all these big guys, but it also has somebody I happen to know. So many people have hailed it. For example, Lee Kun has hailed it as probably the single biggest breakthrough in AI in many many years so and it is being really talked about quite a bit it led to many good things and many bad things amongst the good things we talked about amongst the bad things that it led to was a deep fakes which have become quite a nuisance these days so they are extremely powerful neural architecture and this neural architecture. And this neural architecture is different from what you have been studying so far. The standard classifiers and regressors that you have been studying, it's quite different from that. And the best way to understand it is with a sort of a frame, to frame it as a game between two adversaries all of us at least have played a cop and thief in our childhood right so that is the best way to learn about these networks so imagine that there is a generator of counterfeit money right and sort of there is a of counterfeit money right and sort of there is a generator it so we'll call it a generator this is your thief generator and it generates some counterfeit currency you know dollar one dollar whatever ten is there a ten dollar currency probably not like whatever the denominations are a dollar thousand it's a credit sector of different years and different things like that right and this is this there is a police whose job is to find out whether a bill that it receives is fake or correct discriminator this great later discriminators job did I spell discriminator correctly maybe there's no I here I forget this crim later it's job it's the police its job now you know that this is the team or counterfeiter so this thief will always try if you are in the business of creating counterfeit money what will be a goal to create as accurate a money as possible you know a dollar bill as possible so that the police cannot catch it and what is the goal of the police they'll try to get pretty smart at it they look for all sorts of subtle clues to be able to tell when a currency is real so let's say that you have a pile of real money. Real money. So you have dollar one, dollar 10, dollar 100, dollar 1000, and of all different kinds of years. So as you know, the bills have many, many variations. Each of these bills go through lots of changes over there so think of it like that so what you do is so the thing is they have their adversaries right generator or the thief will try try to pawn off of as much counterfeit currency as possible right because that's how the thief is going to make money the the police will try to catch as many count counterfeits as possible isn't it this frames the game as many come to Possible. So this is interesting. Now comes the very interesting part. So let us say that in the beginning, you have a blank slate. The thief doesn't know how to create good counterfeit money and the police doesn't know how to recognize counterfeits. They are both novices. So now let's try to teach them. What you can do is you can ask the generator, the counterfeit, the thief to produce some bills. So this will be fake data right and then you take a few real data real currency and images of them and then what you do is you create a batch in which you sort of give half and half of this and then what you do is the real currency you label as real so you know you create a create data to train the discriminator to teach the policeman you tell him that see look this is real this too is real this too is real and the other one is fake this other one is fake right and so forth are we getting the sense of it you can label the data data. In other words, you are the expert Let's say that you are omniscient you can see what really is going on and you can give this police Some lessons to learn Right a table you can show these are reals and these are fake. So what will the police do? Let's say that it will Intern it in the beginning if you think of the pullers as just some neural net, a puller's neural net, it will have random weights, randomly initialized weights. so what it will do and the last layer you can make it your logistic unit logistic regression why logistic regression guys yes all it is doing is real or fake it is answering either one or zero but more specifically it is coming out with a probability of, let's say real, I mean, we can take probability of fake, it doesn't matter, one way or the other, it's a probability of the real, right? So for the classifier, so discriminator is in a sense a classifier that tries to classify as well as possible are we together so do we remember the the loss function for the classifier guys yes not the loss function the loss function classifier loss entropy yes very good classifier loss function I think the common one is cross entropy cross entropy loss so for example the loss would be you know the the summations and so forth so let me use the notation that is used in these papers when I say the X when I write this e with this peculiar a double bar think of it as in some sense roughly the same as doing this one over number of data points of something something averaging it and finding the expectation is more to it but let's just stick to data points of something something averaging it and finding the expectation there's more to it but let's just stick to this definition so i'll use this big e so this would be what it would be the expectation of log of real so when will it be one when you give it real data right real data p for data they can call it data right probability for the data and then they would be if you add one more to it you will get a log of by the way if this e is getting a little bit fake probability of fake if you remember this used to be the cross entropy thing let me write it out as minus this if you remember the it used to be this right hat isn't it minus for a positive case and y j y right for the negative case again y ln of course there's a i i here y ln sorry 1 minus y ln 1 minus y hat right so basically the converse of it right roughly speaking you don't write it in so many words you tend to just symbolically write it as y ln y i just to be brief y hat and sometimes when you know that this is a positive case y i is the positive one sometimes people brief it as this and the other term is ln 1 minus y hat when they know that it's the negative case. In this case it's the fake. This is the real. So this is a little bit of mathematics that you may or may not remember from our earlier sessions, but take it as a fact that there is a cross entropy loss that you you can use a very vanilla garden variety cross entropy loss you can pick up from for a classifier and that will serve the purpose for your police right so let's go back to that image and I'll redraw this image so there is a thief traditionally the thief is represented like this thief it is the generator or the thief and this gets some input but it produces it tries to produce a fake right a good good fake and then this fake you feed into and then you take a pile of real data data and then you you sample from that so now you get the training data for the for the discriminator which is basically a classifier of sort is basically a classifier its job is to tell fake versus real is basically a classifier. Its job is to tell fake versus real. So now comes an interesting thing. Do you see that these two, the thief and the police, they have adversarial relationship as in real life. The thief would like to pawn off as many counterfeits as possible. The police would like to make sure that most of the counterfeits are catched. Do we agree guys? Yes. Something simple that I'm saying? Yes, sir. It is. And so how do you train this? What you do is let's follow a few steps and I'm giving you the more non-mathematical sort of way of looking at it. To train the discriminators, step one, step one, you just make the generator in the beginning. The generator is not trained. The thief is a nervous, the cop is a nervous, right? You ask the thief to produce some counterfeits. He'll produce the most useless counterfeits, isn't it? It might not even have the denomination or something. It will just have some random image on it. So you produce a small batch of counterfeits. Let's say that you produce 10 counterfeits. And you take 10 real. It becomes your mini batch. Your mini batch becomes 10 real. Jay Shah, Jay Shah, The cop is to again completely untrained, is a novice. So what will it do? It will make a lot, the cop is going to make a lot of mistakes. It may randomly call a good dollar bill a bad one, a fake one, and it may randomly call a fake dollar bill as a real one. So in other words, the loss of the discriminator will be, will be high. Are we together? Asif, can I ask one question? Yes. How is the generator, like generating this fake images, are we feeding it into the dimensions and the images? Yes, yes. So what you do is, and this is a very interesting thing. People use a mathematical language, but just imagine that you give it a random data, just random data, random numbers. The word that you use is you can give it random Gaussian bell curve or random normal, random, random uniform distribution. What is uniform just between zero and one pick a random number and just feed it in. So the only purpose of that input is because neural networks expect some input. So just throw in some random input. It doesn't really matter. And this is the beauty of this actually architecture. You'll realize that this input is a non, like an irrelevant thing. Are we together? Right. It's, it Great. It has a mathematical relevance actually. It belongs to a class of algorithms, which make beautiful use of randomness. Things like Metropolis algorithm, Hastings Metropolis, and so on and so forth. There are lots of algorithms that significantly benefit and they use randomness to their advantage. So we'll soon realize that it belongs to that class of algorithms when we go a little bit into the math of it. But today, let's get the intuition right, guys. So Prachi, are we clear? This is just something, it doesn't matter. You just feed in something random. Just think of that as just a trigger point. The generator now gets data and says, aha, I need to do something. And what it does is it somehow magically produces a fake. So even the dimensions of the image don't matter, right? That you can match. That should matter. So all of these images should be, let's take a number just to make it very real. The 64 cross 64 cross 3 so make sure that your fake image is also 64 cross 64 cross 3 together so you try to I mean use basic comments and you produce that now what will happen is this cop will make all sorts of silly mistakes right randomly give some values so when you compute the measure of error is the loss in the discriminator that will be high so what do you do at that stage what you do is you what can you do with the loss guys how does a machine learn from the loss you go back and adjust the weights yeah so you go back and adjust the weights yeah so you go back and adjust the weights in the discriminator not in the generator in the discriminator you go and smarten up the police it then it is shown the real answer and it says oh my god I got it so wrong so then it begins to see the differences between the fake in the real, the cop, and then begins to say, all right, I'll do a better job now. Right? So the weights of the, I will get adjusted by this WD some weight of the think of that. And I'm just writing it as a giant weight vector, the giant weight matrix. So the weight matrix, all the weights of the discriminator, they will all improve. So what happens now? The next time it will do just slightly better. Isn't it? Right? So if you don't train the generator anymore and the generator keeps creating really lousy fakes and you just keep on training the discriminator what will happen sooner or later the discriminator will figure out how to very accurately distinguish between real and fake right for example the real will have will actually look like a dollar bill but this generator which has not been trained it is just printing garbage images images on the paper, on the 64 by 64 by three images, it is random noise, isn't it? So would you agree that if all you do is keep training the discriminator, the discriminator will keep getting better and better at recognizing real from fake, right? So if the next step, I don't do anything to the generator, but I again make another 10 of fake images from the generator, another take another mini batch from the real data, and I put it together and give it to the police. Once again to make prediction, then let the police see the mistakes, which is the loss, and then learn from those mistakes. What will happen? Guys, I want some feedback. Everybody is on mute and it's very hard because I can't see you. I'm not getting feedback. So tell me when it is hard guys, because if you recently all said that the sessions are getting hard, I want you to not feel that it is hard. It's very simple. Would you agree that if you were in the loop in which you just kept on taking random images from the generator and you kept on training the police, the police will become extraordinarily clever. Yep. Over time, the discriminator will get smarter. Will get smarter. It's just like real life. Then what happens? But what you do is you don't do it like that. you don't do it like that what you do because now the all that the police has learned is to catch a very like hopeless and nervous counterfeit counterfeit makers isn't it just hopelessly incompetent thieves he'll be able to catch that's not really good so let's try to smarten up the thief now. So what you do is after this guy, the police has learned something, you do this. Next time around, you play the interesting game. So this was step one, right? Where should I write it? Let me write it here. So you realize that that was step one. The cop got a little bit smarter now you go to step two in the cycle in the step two of the cycle what you do is you freeze the police you say police you can't learn anything right so how do you do that in the in the language of neural networks how do you make sure neural network doesn't learn updating the weights exactly you turn off autograd you make sure that it does not learn back propagation doesn't take place through the through the police right or its weights never get updated so now what you do is you again generate this time through the generator you generate let's say a batch of images I just said 10 images or 20 or whatever it is but then you don't take the data no data you only feed fakes only feed the cop fakes but when you feed it deliberately you choose to misguide it so you want to check how well they're at the like from a thief's perspective, how will the thief know Vaidhyanathan Ramamurthy, That it is succeeding. So what what thieves do in real life. Also, they do they will float some effects and then pretend that it is real. Vaidhyanathan Ramamurthy, Isn't it, so let's do exactly that we create level data for the police. isn't it so let's do exactly that we create label data for the police discriminator and the generator it will agree but every instance that you feed this time around into this because it's label data you say you you claim that it is actually real right do you see that I'm putting one here by one, one was supposed to be for real Real. The two was supposed to be for I'm sorry, not to Apologize zero was Zero, zero, perfect. zero for fake but so what you do is you create data which deliberately is designed to fool the cop because that's what you would do in real life so you would give this 10 let's say 10 images of data but you would claim that all of them are real right and so then what will happen the the police is is not able to learn now in this stage the police will make predictions is not able to learn now in this stage the police will make predictions what may happen is it may make it may detect that alternate fakes like it what will happen is you are forcing it to you're saying that this is actually uh real so let us say that the police has gotten a clue right some little bit it has learned. So what will happen? What will happen to your loss function? Because you're misguiding it, the police will this time end up with another loss, mistakes, isn't it? In fact, the better the police is, the more the mistake it will make with these fakes. Are we together? It's better, Asif. Why would it make a mistake see suppose the police is perfect he can tell the real from fake so what will his y hats be so this is your x this is your y what will his y hats be all of them will be close to zero right 0.1 0.01 they won't be close to one there'll be very very small values isn't it so if the police is good his predictions will be very small are we getting that guys so let us say that you have a pretty smart police he will pretty much predict that all of these are fake the probability that they are real is close to zero. Preet Singh, So this is a crucial point, guys. Make sure you understand that. Preet Singh, Do you get this. If the police is smart. Preet Singh, He will say, well, he looks at a fake and says it's a fake, but you have given it label data claiming that it's a that is not fake. So what will happen, there will be a big discrepancy between the between the label and the prediction older terminology you're using that model in an apply mode here it's not in a learn mode right you're using the discriminator in an apply mode exactly the distributor the police is just applying the learning or it is just in inference mode or predict mode. Inference or predict mode. It is just predicting. It's not learning anything. Whatever it has learned in the past in step one is fine. But now it is making prediction. And, you know, the cop may have picked up a lot. It might have picked up a little. know the cop may have picked up a lot it may it might have picked up a little right but in any case because you're deliberately trying to fool the police with your data what will happen is predictions will be uh hopefully different from your labels the y-hats will be different from why is it making sense to anyone, guys? Yes, sir. Yes. It's simple, I think. So if it is there, so what will happen is you'll end up with a loss now. Let me call this the loss with respect to the generator. Now, what you can do is you want to adjust the weights in such a way that this loss decreases. Why? Because you want your counterfeit, the cop to predict what would you want it to have do. This is the police's prediction. Whereas what the thief wishes, what does the thief wish the thief would wish that the numbers are closer to one you know 0.951 or something like that this is the thief's wish and this is the police's prediction if you notice between this the loss is big in the beginning whereas in the when the thief reaches the thief nirvana or the thief heaven all his counterfeit will go through and the loss in that time will be low does does that make sense yes yes it's as simple as that. So what do you want to do the thief now goes and says, hey, let me do a gradient descent gradient descent of the weights. Sajeevan G. With respect to the thief. So instead of saying wait with respect to the thieves. I'll just say gradient descent with respect to the thief. Sajeevan G. Of the loss right and so you say that the this network has updated itself so see that technically i should say some weight in the thief has now gone to wait the weight matrix has gone to weight minus alpha this is the strict thing the the g but i I'll just sort of cheat because it's a lot of notation. I'll just say that the generated model somehow has updated itself. The thief has become a little bit smarter, right, in the second step. So what happened in the first step, the cop got smarter. In the second step, the thief got smarter in the second step the thief got smarter as if in this case the only nuance I'm observing there is you only get one kind of errors in the training mode yes it's a binary classification a currency is either fake or it is genuine. It's counterfeit or real. Now, what I meant by the one kind of errors is the only errors that will be fed back to G is cases where a fake was detected as a fake. Right? But it doesn't give... But it is not producing fake. See what happens, it always predicts produce, G will never not produce a fake. G will always produce a fake. That's it. Okay. Yeah. Okay. You notice that the thieves don't have the US mint, right? Money making printing press. So obviously counterfeiters can only produce counterfeits okay in that case it's the universe set for it yes exactly so so what happens is that so guys follow me i'll just repeat what i said in the first step what you do is you give to the police a bunch of real and a bunch of fakes and then you say okay go make the classification so it will make mistakes if the the cop is a nervous he'll make mistakes so then you back propagate uh his or her mistakes and then you update the discriminator you you update the cop's intelligence you know it's become a little bit smarter it's learned something but then now the copy the the thief is in trouble the thief is losing so what you to do, you need to make the thief a little bit smarter, which is what they do. So the next time around, you don't let the police learn. In the second step, you let the thief get a little bit smarter. And so what you do is the thief, of course, publishes all its data. But this time when you are training it, you deliberately misinform the police. You tell that what is coming from the thief, the fake currencies, are actually legitimate. Because you need that. You need the loss function from that to back propagate and do that. See, because when will the thief be able to learn? When it gives to the police saying this is real. And the police says that no, the probability that it is real is 0.1 or 0.01, then the thief says, oh God, we are screwed. We better go find what is the obvious thing we missed on the currency. Isn't it? Maybe they forgot to put one of the founding fathers, whatever, Washington on top of the currency. Right? A massive blunder so the way they would learn is they need to they need to get trained themselves right so see how much you can fool the cop if so you can't fool the cop much it means that you need to back propagate this tree this thing and all of those gradients back propagate and come back to the thief and here is the beautiful thing how is the gradient back propagating and this is sort of the diabolically fun stuff so you realize that the predictions are always being made at this level by the police? So the loss function is always with respect to the police's predictions, right? So actually what happens is that the gradient flows like this except that here you won't, the police will not update him, update the weights, her weights, and the counterfeiter will get to update the weights, will learn, learn something from it. It is, you know, there's a very intuitive idea. What the counterfeiter does, imagine that they are listening to the gossip of the policeman, right? And they are sort of keeping their ears to the ground and asking, okay, what is the cop able to figure out? So let's say that the cop has figured out that the digits are too small, right? If that's the feature the cop has learned, because the gradients are flowing back it is almost like you know the thief is listening to it do you see that and then it says okay let me make the let me make it a little bit smaller right yes um since we discussed the the back propagation as the part of the chain rule, does this mean you take the whole model of the discriminator as just one step? So we don't anymore because we're not keeping the discriminator as a black box. Just think of it as a simple classifier. If it helps your imagination, just think of it as a log, just a simple logistic classifier, one node. Okay. And since we, since we froze that node we don't so when you do the chain rule there's no loss at that part at that point right the loss is after no no no loss has nothing to do with the upgrades no loss is the difference between prediction and reality why and Remember? So when you freeze the cop, the cop goes into just the inference mode. It doesn't learn anymore. She doesn't learn anymore. So whatever data you give, the learning that she has acquired till then, she will use that to make a prediction. And in the prediction, there would be a certain amount of errors, which will be quantified by your loss. Oh, I understand that part, Asif. I was just wondering how the backpropagation starts from the loss generated by the... Oh, this is your chain rule. This is your standard chain rule, backprop. See, okay, let me just mention the four stages. We did that in code, right? Let me remind you of the Python code. First, what you did is you took the network. Sajeevan G. Which had the forward method. Sajeevan G. Given the input, it would do the forward computation. Right. So what was the pytorch code, you would do network. Sajeevan G. X. this is this is invoking the forward what is it producing for you y hat isn't it so you would take the actual Y the actual X and now you feed this in so now out comes y hat so now you know X Y Y hat three things so this is step one What is the very next thing you do? You compute the loss. Loss is equal to whatever your loss function is, right? Let's say cross entropy and you give it y, y hat. Do you remember this was the second line of your code in the inner loop of learning? And the third line of your code was now that you have the laws what do you do you do backwards right you do network backward what does this do it computes let me just write it in another color computes and back propagates all the gradients but see just because every every layer now has the gradients does not mean that it will actually do the step 4 step 4 being gradient descent which is W is equal to in a simple form W minus w see any given weight needs to do this right to learn this is the learnings this is the real learning part learning step do we remember guys patrick do you remember that this is our a core loop literally we see it as four lines in the PyTorch code yes so I'm imagining in that part so now suppose I don't do this I don't allow this learning then what happens for the police I prevent this learning from happen so it will compute the back of the gradients it's all right the gradients. It's all right. The gradients will get computed. Let me put it, let me redraw this image. Here is a thief generator. Here is real data source. So at this particular moment, here is the police, the discriminator. So you end up with a loss. Loss will cause the gradients. The loss, once you compute, then the gradients will go all the way back to the source, right? Because here is a random, actually, this source for historic reasons. People always write it as Z, random data of some form. But it doesn't matter. What is interesting is that this doesn't matter. What matters is that if the back propagation flows through this, you don't allow the thief to learn, but you can allow the, I mean, you don't allow the police to learn, but you can allow the thief to learn in the second step. Isn't it? what will happen in the in step one the cop learns in step two step one cop learns step two thief learns are we together where g this is the thief generator is the G, this is the thief. Generator is the thief. And this is the cop. Patrick, am I making, is this making sense now? Yes, Asif. So basically here, it was step four. Step four had like a gated process where the first part was you didn't allow the other one, the generator to update, but you just let the and then the second step, you allowed the discriminator not to update and then just the generator. Yeah, because I use the word steps here, let me just call it cycle one, cycle two. So one cycle of data, one iteration number one, so you alternate between iteration the iteration one would be take a mix take a 50 50 percent mix 50 percent 50 percent of real and fake currencies images, label them correctly. How can I label them correctly? Because I am taking data that's coming from here. I can label one data that's coming from label zero label them correctly let the cop predict compute loss back prop only to cop let the cop get smarter let the cop get smarter. Let the cop learn. Are we together guys? Then in the second, and this is the alternating step you do. You alternate between this and this. The second step is, can somebody remind me what the second step is? What do we do this time? You freeze the learning of the police and let the thief learn. Exactly. Freeze the cop's learning. So the cop cannot learn. Then feed it deliberately misguided data. So this is the time for the thief to get smarted so the feet the the thief will now pawn off a counterfeit currency as of right ones and deliberately feed counterfeit fake data to cop but claim label it it as real because this time it is the thief who's trying to figure out how much will the cap cop actually catch if Take the loss and back prop and update or gradient descent the thief. Right? Are we together guys? So what will happen is in in one step the cop will learn and the next step the thief will get a little bit better then the cop will get a little bit better and then you repeat the iterations you keep on cycling between this and the amazing thing is that see you have a large corpus of data actual data so you're sampling into it and picking up a mini batch but as far as the thief is concerned as with counterfeit money you have a random generator you just need a trigger and the and the thief will just go and just freely generate as much image as many images as you want right there is no effort there is no data constraint it's a generative that's why you use the word it's a generative model it will go and generate it right as many fakes as you want and so little bit by little bit do you see that they keep making progress what will happen in a cat in this peculiar cat and mouse game between the thief and the cop what do you think will happen after some time? Can you guess? Both get smart. Both get smart. They both become very smart. And in fact, this is a classic game theoretic problem. Those of you who took AI and course may remember, and if you don't, most of you have not taken a course because in our generation I suppose it wasn't very popular so this is a name for that in a good in AI we have an end game theory we have a name for this sort of a thing it is called mini max so without will introduce maybe the math later on in the second half so this is called the minimax i'll go so what does it mean the the thief the cop is trying to minimize the mistakes and the thief is trying to maximize the mistakes that the cop makes do you see that the goals are opposite it's a two-player game and it's an adversarial game between the cop and the police right now whenever you play an adversarial game or things like that uh in game theory there is a concept of a nash equilibria or nash equilibrium great so what it means is that imagine and at this moment i'll make hand waving arguments later on I'll make it more precise yeah so so imagine that in some space right it is it's somewhat like you know descending to an optimal solution so there are certain points of equilibrium and what will happen is this situation will roam around and finally hit upon an equilibrium think of it as some sort of a minima in the game right these points are called Nash equilibria Nash equine points and it's a sort of in the state space so the interesting thing by the way Nash Nash was a great mathematician who single-handedly created a whole revolution in game theory and in economics so there was a movie made on his life which if you get a chance see it it was called in the beautiful mind exactly beautiful mind beautiful mind movie and in fact much better than the movie i always found actually i found the movie good as it was to be almost uh well nowhere close to the actual book most nowhere close to the actual book the the book is far superior they call the beautiful mind it's on the life of Nash so anyway if you get a chance you might want to watch the movie at least so that is a minimax and he this is applicable here it says that you'll ultimately reach one of the equilibrium points. So this sounds very good. We'll be able to solve this. This was the initial idea proposed in the paper, generative adversarial networks by all of these authors this is 2014 it was a game changer guys it was to computer vision in those days a massive massive game changer why is that a game changer because think about it if the thief becomes really good at counterfeit right you the thief becomes very useful you can forever go on printing new new money right or in a more practical sense and now the thief can generate the the kind of data that you want it becomes a generator of fake images and fake videos do you see that guys it can perfectly well create a scene that doesn't exist and the discriminator the cop won't be able to know the difference I said yes give me a couple of examples where this can be applied yeah I'll give the In fact, I'll bring up the paper itself, the generative adversarial network. Why do I not do it right away? But see, there's a little bit of a theory, if you just allow me, I will take another few minutes to explain the theory completely. And then we'll see examples. Examples are absolutely stunning. It was a game changer in artificial intelligence. Many people say that really artificial intelligence actually begin to feel intelligent or something about intelligence with this GANs coming about. They're true in somehow to the spirit of AI. So hold the thought in your mind. Go ahead. Next question. Asif, there's one question about if you scroll back to where you had the G and the D diagram. G and the D diagram. Okay. There it is. Yeah. Where were those two boxes? Yeah, here are the two boxes. Yeah. So now, okay, let's leave there. So in the explanation, when we were looking at optimizing those ones ones cycle one, you were essentially making D smarter, right? So that was one that was understood. There's a backdrop algorithm and essentially you did the gradient distance to get D smarter. The cycle two, why do, why do we need to do back prop through D? Why wouldn't just the error be passed to G? See, the trouble is, the trouble in this particular loss function is, the loss function is defined in terms of the weights here. It's just a mathematical reality. If you think of, for example, if you think of a logistic just think of a basic logistic unit this is the output suppose this is your police be police right the simplest police you can think of is just a logistic unit this is producing by hat you're comparing why had to buy to produce the loss. But this loss will respond to the changes of this guy. Isn't it? You can compute the loss knowing the fact that, let's say you use sigmoid. You know how to compute mathematically the gradient with respect to the weights here. You have no way of computing the gradient with respect to the weights in G itself directly. Okay. So you have to go through this to get to there that's right and you know there's a there's an interesting bit to it because what happens is that in some sense if you think about it it is almost it's a very diver it's quite a diabolical scheme these counter features right they they pass they pass forward some fake bills then they look at the cops if the cops pick it identify it so imagine cops are doing imagine them sitting there with their doughnuts and talking about it how smartly they caught the caught the counterfeit bills right so these thieves are listening to it and they know oh maybe the mistake we are making is the two is too big I'm just sort of making it be sort of a real it's never like that the mathematics is more abstract but if I were to caricature it a little bit it would be like ah maybe the digits that I put were too big or the color was off okay so what do I need to do now I need to just go fix that right so that is what is happening because when it flows back through the car listening to the chatter okay so it's a pretty very clever very clever way of doing things the idea itself is very elegant and clever and that's why it was reality as one of the biggest breakthroughs of this practically of this century in ai in many ways many people said this really really remarkable and it's a game changer and so forth certainly it was considered for many for a long time to be the single biggest breakthrough of the 20 since 2010 right and now uh transformers have come in you see in the in the world of energy so these are the two big ideas. The transformer I've already taught you. So now the biggest problem that happens is, how do you know that this sort of games converge, that they actually come to a good solution? Because if the cop is stupid and the thief is stupid and they're having a hard time learning, this game can go on forever and it may never converge. You see the problem there, right? But thanks to the game theory and the minimax algorithm, we know that it converges, right? And it converges to one of the Nash equilibrium points. So then generative networks began, people began to design and do things until they hit a bit of a bad news. The bad news they hit upon is this. So imagine that you have, I'll just take an example. Let's say that there is a data set called MNIST. MNIST is a digit recognition. So suppose you have zero, one, two, all the way to nine and human beings have written. So somebody will write zero like this. Somebody will write one like this. Somebody will write two like this. And so, you know, there'll be many, many ways to write this. So MNIST dataset contains 60K images of digits. They are monochrome. They are 28 by 28. And then the fashion MNIST was created because MNIST became too easy for the modern neural networks to solve. For years, MNIST was a benchmark and very hard to make progress on. And then suddenly this came CNNs and deep CNNs and there was a lot of progress. In fact, the year in which all of these things started getting trivialized and other ImageNet data sets and these things began to trivialize was 2014. 2014 was also the year independently the generative adversarial networks coming so here you have a shoe let us say you have a shirt or skirt let's say you have a shirt let's say you have a pant let's say so what it contains is labeled images of maybe you know garments or garments and fashion items deliberately to make it a little bit tougher because M is became too small so now there is a problem with this game with the adversarial network see what happens is that if you want the generator to be really smart, it should be able to produce each of these items. So if you think MNIST, it should be able to produce very good zeros, one, two, three, all the way to nine, all the 10 digits and fool the cock. Isn't it? But the instability that comes in the system is that what it will do is it will produce a one, something like a one, slightly like a one. And then it will check, did the cop catch it? Suppose the cop catches it, it will back prop and now this time it will produce a better one and better one and better one and so what happens is that of all the nine of the ten digits it will get fixated on one and so both the police and the thief and the counterfeit counterfeit maker and the cop they will both become very good in playing the game with one right the trouble is and account the counterfeiter is not generating any other digits at all it's just generating one so it hasn't learned to generate all ten digits you see the problem there right and so it gets stuck so after a little while what happens is that it becomes very good with one but then you get it unstuck out of one, right? Because the cop has become too good. Then this will go and it will go to two. It will start producing two, right? Because now the cop has caught up and then it will produce three. So in other words, maybe it will produce eight. It will produce five. It will keep cycling and they will forever keep cycling back and forth never stopping because either the cop and the thief will become very good with only one digit rather than all the digits right so that is a problem that these adversarial networks face and so that the original one had this problem and so there was a tremendous amount of research. And today GAN is a whole world in itself, just as transformers and NLP are a complete world in themselves. In the same way, the GANs, the Generative Adversarial Networks, they're a complete world of research. And there is a tremendous amount of research in creating all sorts of GANs. There is one particular GAN I'll talk about. It is called DC GAN. Now I can't talk about all of the GANs, they're beautiful things. There is cyclic GAN, there is this GAN, that GAN and whatnot. We will actually deal with many kinds of GANs when we come to the image processing aspect of the second and the third part, right, of this workshop. But at this moment, suffice it to say that we get the big picture idea. And towards the very end, if those of you who are, actually, let me not do it today. See, I want to keep it at the level that it is easy to understand. But the minimax it is easy to understand. But this, the minimax theorem, the way it maximizes is very simple. You use something called a symmetric KL divergence and so forth. The arguments, if you're familiar with mathematics, it's like four or five lines of derivation, but I won't go through that. I'll keep it for the weekend. Let's, why don't we do this? This weekend, we will cover the GANs original paper. Does that interest you guys yes yes so then we'll cover the mathematics there because at this moment it may derail our discussion I want to take you guys pretty further along this so that is that that's one problem so then people have come up with various architectures to prevent it from getting fixated. One of the architectures that people have created, which is extremely successful, I mean actually not, but for some time was very successful, was DCGAN, the next sort of evolution for that. It stands for Deep Convolutional GAN. deep convo nutritional again and this word is explain what it does so what it does is to the generic the classifier is roughly speaking with minor variations it is basically your standard CNN right so how does a CNN go wide then lesser then you know it becomes bigger smaller smaller but more number of items right and then finally you have the feed forward layers right the densely and then finally you have a softmax do we remember guys the CNN the standard CNN architecture yes so this classifier is your discriminator this is the police right and it will produce the Y the and therefore, you already know why to produce the Y hat and therefore you can compute the loss and Y hat and you know how to therefore train the police that was easy. What you do for the thief is something that is most remarkable you create another cnn but you turn it upside down you turn it backwards so you remember that suppose you start with big image 128 by 128 128 typically across three channels rgb channel right and by the time you go to the last is corn Blair it has probably become like four times four times and the number of channels has exploded maybe the number of channels has become 512 isn't it so the image size reduces the feature size reduces the number of features increases the channels increase. You know, this is a typical CNN. Do you remember that, guys? Yes. Yeah. But actually, so what you do is you do something counterintuitive. You do just the opposite of that. For the T, for the generator, what you do is, and again, there is a random, and by the way, this doesn't matter. It could be uniform random number generator, Gaussian random number generator, and so forth. Ignore that part. What you do is just the opposite. You create, for example, you may create a vector of random numbers of just size 100 so it is your z is basically of size 100 right and what you do is right here you explode it into a deeper rather deep let's say 512 this is 100 let's say or something like that you create it into what would be like this this begins to look like this right and then as you come closer to closer it produces an image so suppose this image was 128 by 128 by 3 the last con glare actually is producing this image which is 128 cross 128 cross 3 so now you see why it makes sense right you took some signal and you sort of deformed it into this image and this becomes your but typically you call it X is the generated function of Z this is the input from the T this is what the counterfeit input from the thief this is what the counterfeit counter why am i forgetting the spelling of counterfeit is it f-e-i-t i hope counterfeit anybody would like to correct counterfeit right so this is the counterfeit image that you produce and it sort of makes sense because what do you want you want to ultimately produce the image and so your entire continent is run backwards the only important thing is do you notice that there are no dense layers like after this there is no dense layer because you don't what you have gotten back is the original image and at this end you don't you don't layer anything in the dense layer a typical con has convolutions followed by dense layers flattening and dense and a soft max you don't keep all of that you just keep this now they're minor details so for example you turn off pooling there is no pooling here pooling you turn off the stride when you hop across you have instead of pulling you do strides of two if you remember the you just pulling is stride of two typically if you use a two by two filter right but uh you leave the straight part but you don't do the max or the average or any of these right and so you run this entire cnn some people also call it decon because it seems to be the opposite of condolation in some sense in their mind so what you do is if you think about it what you have done is suppose you have a unit line zero to one you're generating numbers from this and let's say that the picture is made up of pixels which are all localized like this so what it is doing is in the zero to one interval it is localized like this so what it is doing is in the zero to one interval it is localized like this what it is doing is it is showing everything to this quenching everything to this in many sense anyway this is I don't want to go into the math because now you'll say how is this working we'll do that over the weekend but basically it's a very interesting idea when the generator network trains it figures out what should it look like what should the image look like right so i will give you a small intuition before the break so imagine that the only thing you want to do there is an image there is an image. That image is very simple. In the weight scale from zero to one weight, ducks are here, right? And your elephants are here. Are we together? So, or your cows are here. Cow, duck. So a one dimensional duck looks like this, let's say. Duck. And a one dimensional cow, let us say looks like this let's say duck and a one-dimensional cow let us say looks like this or something like this maybe I'll make it wider because the weight of the cow is a little bit distributed and the weight of the duck is also distributed let's say that this is your weight scale on a scale of 0 to 1 would you agree that this is a typical duck will look like this and a typical cow the data of the cow will look like this right so for example of one particular cow one particular duck may be sitting here one particular cow may be sitting here this point may represent a cow but a cow won't be sitting here that wouldn't make sense one instance of cow will not be with a value here so so what happens is that imagine that the generator in the beginning produces a generator let us say that it's only trying to get a sense of the duck so it produces generator produces a duck like this somewhere in the middle right so now the duck should be where is it uh the duck should be here so between the the police has learned to look for duck rates like this, but the counterfeiter has produced this. Do you see the gap? This is in effect a measure of the loss, isn't it? Loss is related to this gap. It's a function of this gap. So in the learning step, when the generator learns, the next time what will it do? It will try to produce the duck a little bit here and it will keep on moving like this. Does it make sense? It will gradually make smaller and smaller, lighter and lighter ducks. It won't make a duck of 200 pounds. Are we getting that? Yes. Now, let's generalize this idea. And for cows, it will either produce this. It will learn to produce cows here. because if you really think about it so now let's look at the cow and duck in one dimension suppose your uh so this is it this is your duck this is your cow in the beginning you have a uniform spread of values a uniform random number generator uniform in the interval 0 to 1 it will produce values equally everywhere probably with equal probability isn't it it has a uniform probability but what you really want is most of the time you should successfully generate data here so what you should do is you should basically have somehow change the shape of this uniform if I may say so something like this all of this will converge here or something like this half all of this will converge here and all of this will converge here you're changing the shape of this uniform distribution. You're bending the line basically. So that's a distribution line. And that is what the producer produce this function. Now if you were to look at it in two dimension, so suppose what happens is that you look at the weight and size of the car. So let's say that these are your ducks. So let's say that this is the weight axis, this is the size axis. Here are your ducks, light and small, and here are your cows. And let me use green to signify predictions. You know, the generator in the beginning produces things like this. You see that, right? So if it produces things like that, even a rather dim-witted cop will probably succeed or quickly learn to detect counterfeit cows and ducks. Are we making sense? But gradually, what will happen is this uniform distribution will change to, after some time, you will notice that what has happened is that after some time... Guys, please mute yourself so the the points that this is generating the generator will start points only in these two regions because it knows that if it produces point most real images and everywhere else is a counterfeit it will get gone so this is the journey this is the learning journey for them for G right for the chief does it make sense guys so instead of producing green points all over the place it will start producing green points in those two areas where it has figured out it means something that's where the real data is sitting isn't and so once it is doing this what has it done you you take this generator this and you you ask it to produce something. It will either produce a fairly good cow or fairly good duck. And you know the amazing thing is that now you can produce images, countless images of cows and ducks. Things that may not even exist. Guys, please mute yourself okay guys so are we getting the are we getting the point guys and what happens is that think of it from the point of view of a thief and this is how thieves become smarter and smarter right and thieves become smarter and smarter right and cops become smarter and smarter so today also i mean really cops and thieves have a pretty interesting game going adversarial gameplay and that is generative adversarial networks and the amazing thing is for the dc jan again uh you have the you remember it gives you one thing that is really amazing so DC again what was DC again the convulation deep convulation right remember the summary of that was generator he looked like a neural net run backwards, a convulation net run backwards, like from a generator. And this was the thief and the discriminator was a bona fide convulation network, right? To finally a softmax. This was the police, yeah. The police. This was the DC gap. Now what happens is that the internal representation they create, like for example, if you look at these vectors that they produce, the representation of the cow and the duck in the images they have a property which is very similar to word embedding so let me explain remember that word embedding and we learned about uh word to the word to back do we remember these words guys we did word to weck i talked about glove but didn't cover it anybody remembers what to wake guys yes sir yes we do yeah what was the point you could do things like king minus man plus woman was queen isn't it now the interesting thing is the same thing you can do with sort of a image latent the people call it the latent space embedding the hidden state of that embedding of images this is one of the lovely byproducts of dc gas so what you can do is you have a picture of a man man with glasses i suppose this is a man with glasses right and then you take man with glasses and then you do this you subtract minus man you take a picture of a man minus just a man you take any picture for a representation of a man i raise a presentation of a man right so the man is like let's say something like this and then what you do is you add some picture of a woman woman so how does a woman would look right So, how does a woman would look? Right? So, as you can see, I'm very good with art, right? I'm kidding. Substract. So, you know what it will produce? Can you guess what it will produce? A woman wearing glasses. Exactly, and it will produce a very very very realistic picture that would fool most of us of women wearing now glasses so as if the question here is in, given that this is a machine doing it, right? Right. So it would essentially be the guy, the man with glasses, his facial features would now show up with feminine features, but still wearing glasses, correct? Yeah, what will happen is, typically what happens is, this person looks very feminine. Yeah, woman dominates. And the glasses come through because you've subtracted the average faces of men. That's it. It's quite realistic. So let us do one thing. It's going to be 20. It's almost a break time. I want to make sure that you guys understood this. Let me pick some names. Pradeep, did you understand what I was explaining? Pradeep Kaur, Yes, I did understand. The only parts are in the DC GAN, the first generator part which is inverted CNN that part I having little difficulty following that what is the advantage of having the inverted CNN see what happens is that you need to take some noise that's pretending to be features and deform it into an image i see that is all sticking there hannah how about you you'll be very quiet are you getting it yeah partially i probably need to listen again okay so uh what i would like to do guys is in the second half i'll review what we learned once again but before that i want to tell you some other magical things that it can do so one of the things you use a neural style transfer and you often use one of the ways that you can do this is with GANs so how do you do that what it does is you take it's called domain change it's very very effective so let's say that you have a picture this is a picture of you of your baby your child sitting there in my case I have lots of pictures of my young daughters when they were young I mean when they were little children now they are young but they're college age so let's say that there's a picture of playing the piano so you're playing this piano or something like this. Now this picture is a photograph. What you can do is you can take an artist, let's say you can take Rembrandt. And Rembrandt used to paint people, human faces and human with great feeling and invocation and so very evocative pictures. human faces and human with great feeling and invocation and survey okay two pictures and he was a great master painter well it so turns out that since he is dead we can't have him come and paint our daughter playing the piano so what you can do is you can feed it into this sort of a GAN system and what you will get is a very realistic Rembrandt painting of your daughter of the girl playing the piano like the deep art website what's that? You have in the portal, like the playing with the Deep Art website, right? Right, right, yes. Yeah. So, this is it guys. It's very amazing. You can take a landscape and you can have Van Gogh repainted for you in a beautiful painting. And it also leads to some very interesting style transfer is not just for images, you can do style transfer now for Preet Singh Rao, MA, RDN, authors and language. You write a paragraph of text and then your teacher, let us say, is somewhat unimpressed with the elocution in your text. with the with the elocution in your in your text you can immediately invoke any writer of your choice and try to do a neural style transfer of that writing style into your into you what you have written and you'll be quite surprised how effective sometimes this can be so so this is a power of generative adversarial networks. And in the second half, we'll do some examples and then we'll move on to the graph neural networks. This week, actually, I would like to do some labs on GANs so that it all becomes very real. You know, when you play with it and actually see it happen, it becomes very real. Now, the reality is guys that GANs, they are generative models they feel like magic how can you feed random nonsense into the generator but nonetheless the generator just deforms it into you know it will always draw a vector that is either here or it is here and it will not do this and the point is that those random input is just seed, is just randomly making it pick from here or from here, draw a vector in one of these two regions of the feature space. That is where, for example, a cow and a duck, a cow and a duck are sitting, images of cows and ducks are sitting. And so it is also making images that don't exist, for example and not only that it leads to additivity of images you can do arithmetics for example you can take you can now do amazing things like you can say can I have a table with the Apple right and of all things let us say a rocket on it or a launch pad or something like that on it and it will it will create it for you amazingly it will do that the other application which is very interesting is and this is where natural language processing meets this images you feed in text. Vaidhyanathan Ramamurthy, You feed it a narrative into this GAN where I'm putting the generator and everything together. You say Vaidhyanathan Ramamurthy, A room. Vaidhyanathan Ramamurthy, Room with a window window and a girl playing piano and guess what the what the GAN will do it will quite literally produce a person playing the piano this is as good as I can make it and there is a window there in the room and this will look like a pretty realistic room to you well this one doesn't look realistic but GANs are a little bit smarter than me at making pictures and so just think of the just think about it the potential for it or for example just think of the potential nowadays for saying things like you know people used to do a lot of 3d modeling people would say here is my landscape can you see how it will look if I put a swimming pool here if I put a few trees there and this and that right now the potential is and I'm sure that somebody will start commercializing these ideas and do this there must be a lot of the commercialization solidarity things move so fast you just describe the scene you just describe how you want it to look and soon enough it will produce a photo realistic picture of how your backyard can look right and it will not generate one it will generate as many as you want each of them will be different right and it will serve the purpose for you so that is the power of GANs. And as you can imagine, the GANs have been very, very influential. There is a whole zoo of GANs. There is cyclic GAN. There is this GAN and this GAN, and so on and so forth. And every day, there's more of them coming. So they remain a very, very hot area of work. Asif, quick question. remain a very very hot area of work as if good question yeah so when do the generator and discriminator will stop iterating that is the nash equilibrium yes so you know you run it after many epochs you'll know you look at the loss function right so there is a bit of mathematics see the minimum that the loss function can achieve and there's a bit of mathematics see the minimum that the loss function can achieve and there's a bit of mathematics is minus log of 4 this is this is you can't beat this right so soon what will happen is what happens in reality is you see your loss decrease decrease decrease and then after a little while you notice that it's not decreasing anymore you stop are we together it's a standard neural network training how do you know when to stop you see the loss like in the beginning it will start like this it will start like this and then start trailing off when you reach a point of no returns damnation returns a plateau stuck. No point in more training. As if when you draw a loss curve like this, you're basically referring to either D or G. Yeah. So what you do is very clever actually. In code, you do both of them together but when you compute the loss you compute the total loss between the cop and the thief okay and then see both of these losses ultimately they'll plateau off so you will have a very very clever thief and you'll have a very very clever cop right and that is that is your generative adversarial network it's a beautifully simple idea it's quite interesting that you can make these two play against each other and improve and they do actually improve right there's some instability in the original versions but now people have been to a large extent been able to completely solve it. I wouldn't say that they're completely solved it. They have been able to solve it quite a bit. But even today, you take some of these GANs and you run it once, it will learn very well. You learn it the next, you train it the next time, now things are not so good. So people often try to run it a few times and pick the best model they freeze it and there's a lot of work that actually creates very stable gang training so ask if one question so i think the originally when you started explaining i thought we are we are trying to perfect the police by means of a thief but in this case looks like we are trying to perfect the thief or the generator and police is just guinea pig. Absolutely you smarten the police but actually you don't care much about the police. You just want to you just want a very very clever thief. Yes so all the applications are basically just using the smart generator basically exactly because the generators can do it's like free money right imagine here you cooked up a printing press in your backyard in your house in your basement and it could print currency just as good as the real one would you think it's of value yes Would you think it's of value? Yes. So pretty much that's it. That is GAN. Police will stop I guess when no more it can detect fake. Yes, exactly. So it works is that ultimately the cop becomes very very good right I mean sorry the the counterfeiter becomes very very good and what you're feeding it is very hard to tell from the image so you get a good good cop but see the thing is even though the in a way you can say the thief wins in some sense but actually the cop also wins because the cop is a super cop. Unless you are dealing with an extraordinarily smart thief, that cop will catch most of the thieves. Isn't it? Anybody not such a super thief, the cop will catch. So the cop has also benefited a lot. He's become a super cop. cop will catch so the copy is also benefited a lot it's become a super cop as if the examples that you gave in terms of the illustrations where this kind of a concept is useful if you were to compare those examples with where let's say a studio like Pixar has already taken that is basically they are doing multiple layers and they're doing simple matrix algebra right right so that technology has also reached a certain stage where they can superimpose pictures on other things. They can create virtual images, like a thing like avatar can be created, right? There are people moving inside a jungle and all that. Absolutely. I mean, have you seen deep fakes? I remember in the class website, I put a deep fake of Obama introducing a class. Let me show that to you guys in case you missed. Just to show you what beautiful things, maybe that's a good way for us to start with this business of what GANs can do. So if you haven't seen this video, which is there in our class website, is high time it turns out that these days uh ai is so powerful that no less than a person no as the president of the united states comes and introduces the class so let's watch this hang on before i do this let me uh share the sound share the sound, share computer sound. And I'll start from the beginning. 2020 and welcome to the deep learning lecture series. Let's start it off today to take a quick whirlwind tour of all the exciting things that happened in 17, 18 and 19 especially, and the amazing things we're going to see in this year, in 2020. Also as part of this series, it's gonna be a few talks from some of the top people in learning in artificial intelligence after today, of course. Start at the broad, the celebrations from the Turing Award to the limitations and the debates and the exciting growth Okay, I'm not able to maybe it's not this one But there is a lovely video. I think I posted it to us in the chat Send it back to me on slack. Yeah, okay. I Want you guys to see something oh here it is yes this is it thank you so let me play this it got me thinking about my full-time employees and their ability to got me thinking about my okay let me I don't know this must be something else Full-time employees and their ability to survive on $8 an hour in New York City, and foremost in all of our minds, has been the loss and the grief felt by the people of Orlando. Most of us don't get our health care through the marketplace. We get it through our job or through Medicare or Medicare. And what you should know is that, thanks to the Affordable Care Act, your coverage is better today than it was before. Women can get free checkups, and you can't get charged more just for being a woman. To give his employees hope together to pass a common... There's a bill that would boost America's very, very hard times. Some progress, at least within the small confines of the legal community, I think it's real important. Here we go. President Barack Obama, when you're giving a speech, make sure you use a lot of pauses. America's businesses have created 14.5 million new jobs over 75 straight months. We're developing technology. Every technology can be used in some negative way. And so we all should work towards making sure that it's not going to happen. Anyway, guys, so do you see how realistic that Obama's talk was? Yeah. I tried that video. That is it. So that's the worry, deep learning plus the business of creating fake. Now you have a generator. You can go creating as many fakes as you wish. And that is both the power, the fascinating power and the tremendous risk of the worry of AI. know with this sort of it's heralded as a major breakthrough in the eye it certainly is but you also have to ask this question is it pretty much going to tear apart the fabric of society the way it is anyway I'll leave you guys with that thought and then I'll show you some examples after the break let's regroup at 9 o'clock yes the link to the video you had shared about the obama teachers that i might do yes yes why not why don't we see that too please please do share it did you post it to uh our would you just posted the 2020 deep learning our deep learning slack please oh you did here we go excellent let me uham I sharing the right screen? Yes, I'm sharing the right screen. Okay. Just for fun, this is a one-minute video. Enjoy it. This year I figured we could do something a little bit different and instead of me telling you how great this class is, I figured we could invite someone else from outside the class to do that instead. So let's check this out first. Hi everybody and welcome to MIT 6S191, the official introductory course on deep learning taught here at MIT. Deep learning is revolutionizing so many fields, from robotics to medicine and everything in between. You'll learn the fundamentals of this field and how you can build some of these incredible algorithms. In fact, this entire speech and video are not real and were created using deep learning and artificial intelligence. And in this class, you'll learn how it has been an honor to speak with you today so I didn't know guys at least I find this super super exciting And it is both the power and the scary part that is so fascinating, so powerful, and so scary. Isn't it? And yet on Sunday, you'll realize that the mathematics is almost 10 lines at most. It's very simple. If you got the idea of police and thief, you got the whole of it. We can rewrite it in a bit of algebraic or mathematical jargon, but that's about it. And now, on Wednesday, we will do labs, and you'll find that the labs are so easy. And very, very real things you will be doing in the lab on Wednesday. It is our last session, and I thought it would be a good way to end the fundamentals part, the part one of this workshop. And you'll walk away, I mean I hope, suitably impressed at your own ability on what you can do with these generative adversarial networks. In the second half, I would like to show you some examples from the original papers and we will then continue on to another kind of topic which at least i would like to touch upon it is the topic of graphs network large scale networks so just to describe to there's a difference between the discriminative models and generative models in the discriminative models which you're familiar with, you, for example, ask this question. Given this input, which is the likeliest output? So what is the probability of each of the labels? Or what is the probability of a given value? How much ice cream? Which is the most probable amount of ice cream that will be sold? Things like that, given this input. So those are discriminative models what what is the probability of a label given the data on the other hand generative models they don't ask that question they basically say if you look at the input and output all put together what exactly was the probability distribution that is the underlying ground truth so it is like this so let me maybe I'll show the other screen the question the yes see the question that a generative model is doing is it's trying to gradually learn these things this probability distributions isn't it once it has figured out that data is distributed in the two corners here this is in the language of mathematics you say that you have figured out the probability distribution of X of the data of the data you figured it out and once you have done that now it becomes very easy you can generate as many instances of data as you want and that is lovely so I'll give you a very simple example to make it real suppose suppose a data is coming from a bell curve, right? So what happens is given X, you get a Y. This is your Y. And it's a bell curve centered around some mu and it has some sigma square variance. So you get data. So you can do training in two ways. You can either try to figure out some you know discriminative model that tries to capture the shape of this curve the other thing you could do is you could basically say why if i take it to be let's say that two pi square root of two pi up to some amplitude and then e to the minus x minus mu square over two sigma square this is the equation of a bell curve by the way guys so now what you're saying is what is mu and sigma square and a the amplitude if i knew these three things then what happens i can generate arbitrarily many points that will look exactly like the input data. Do you get the point guys? If you figure out the bell curve and now how many points can you generate that looks like the data, the original data, you can generate infinitely many points, right? And therefore what you have done is this is your, in effect, in a very oversimplified sense this is your generative model right you have you have created a model which is about a joint probability of x and y right what is the relationship on the other hand when you make a other model like y probability of y given x like for example y is something like that a x plus b you try to fit something like that then the game is different here you're direct you're not trying to predict i mean you're in a way indirectly trying to predict y but you're not what you're trying to do is discover the underlying distribution, which produce the data to begin with. And when you can discover the underlying distribution, now you can generate arbitrary many models. So how does it relate to images? See what happens is that think of a one. So you have a text, a digit, it is, let's say one it is if you think about it it is some pixels right so if you stack the columns together it will become I don't know one two three one three then four five six six something something somewhere somewhere they will be shading this is your this suppose this is 28 by 28 this is or in this particular case 4 by 4 the team just write it realistically 4 by 4 when you stack the columns 1 2 3 4 vertically when you stack it you will have 16 is equal to four by four vector, feature vector. And now what happens is you take all the ones, and you would all agree that most of the ones, they will look like minor perturbations of this, maybe somewhere here, something like that. So what happens is all of these ones in the feature space, in an abstract feature space, they will occupy a region. This region is in some sense one, this region is in some sense two. Are we together? Right? And the game that the generators plays, they figure out which part of the feature space is valid data, legitimate data, and which part of the feature space not to go there and not to generate points there. Are you getting that, Pradeep? So, what we're basically saying, if you go back to this example is that this green point don't generate this green point. A point here is good. In a way it is basically finding the real Y, like which has generated the data itself. Yeah, what you're looking for is a joint probability distribution, input and output, the complete distribution function right it has figured it out so in the regular problems we can we use gan to just find out what is the distribution itself because many times that's the problem not just gan see the trouble is gans don't tell you the distribution that That is another problem. See, so let me answer your question. What happens is real life is very complicated, right? So this probability distribution function, right? Uh, is very hard to figureidimensional, and not terribly practically useful. So can you figure this out? And if you really tried to do something, there are ways that you could sort of see what the GAN is seeing. Therefore, you can get an idea of what this function is. And we'll talk about interpretability of the model. I said explainability and interpretability of the model in the way, in the part two, that will be one of our starting points. So yes, you can, but when you can, or these generative models, like for example, there are many of these, this is one, negative sampling, inverse transform, then Markov chain Monte Carlo. They belong to a family of algorithms which are generated and the beautiful thing is that they can generate a lot of new samples representative of the original data without necessarily needing to write the probability function in explicit form ever. I see. That's the point. And so, sir, what is the limit of this in the original example, like one of the example we said that, you know, that there is a girl in this room and playing piano and then picture was drawn. So like that way if somebody says you know I need a program like say ReactJS or any framework I need a program to be written doing this and this right. So will it do this also or in the current the training data see what happens is that there's so many competing technologies these days remember I talked about transformer last week right right and I showed you the transformers some of these big transformer models like GPT-3 they're literally writing code for you yeah so that is one avenue to train here what happens is can you train again again and frameworks like this to do some things like that yes you can but mileage varies right at this moment in writing code because it's more energy see the output is text whenever the output is text generally trans transformers do better then whenever the output is image you'll notice that the gans do better but the output is image you'll notice that the gans do better okay that's what i wanted to know like in which area it is uh stronger but but it can do text also it's just that uh transformer happened to be winter so far yeah yeah so these things are very powerful uh and there see the concept of gan is there now how do you implement it you implement it using condolations you implement it using what and there's a very interesting thing I'm just having a conversation with one of our participants here a gender in this a few months ago his son did a very interesting paper which was to train machines, you know the textile machines, to work with the kind of cotton seeds that are there in India, the kind of not so long length, short length cotton that is produced by Indian farmers. As you know, long cotton is very, very good. Apparently there are a lot of machines in the Western world. Everybody is perfected the technology to deal with it. But as those seeds come to India and they are all under copyrights and whatnot, then patent and whatnot. So what happens is that the Indian farmers were losing out. So Chander's brother here is an idealistic person. He does grassroot innovations. Amongst the innovations that he has done is that he wanted to create a machine that can deal with that and he has created the machine the textile machine trouble is to calibrate it is you need another machine which is extremely expensive right to calibrate it properly and fine-tune it so the question that Chetan here did is, can you instead create a neural network that you can use instead of going and renting this machine, which will cost you tens of thousands of dollars a day, which is hard to afford. And sure enough, he succeeded at doing that and he is writing a paper on that. Sangeeta Mohandasumni- And I must say that support vectors was involved in helping him out. Sangeeta Mohandasumni- Yeah, I can share the reference at some point if probably I'll put it in slack. He has put it as an article paper. Sangeeta Mohandasumni- Thanks. Sangeeta Mohandasumni- So this is a great work. And by the way, he's just in college. He has not even finished college. So this is the power of this. And if you look at the architecture of what he has done, it is in effect a GAN. There are two sides which are... Actually, neither he nor I knew it was GAN. But now that Asif is giving the lecture on GAN, we realize that it is very similar. I mean, we had a very similar cost function where there was original one RNN was trying to simulate the data, which the second RNN will think that it is actually the real data. So it was in some sense, it is very parallel to this GAN framework where you have a generator and an adversary. So nice guys, so I'd like to just show you a couple of things in the interest of finishing off this discussion before I go on to networks. By the way, guys, I hope you realize that GAN is a topic on the course website. If you are seeing, maybe I should go there and remind you that, I don't know how many of you visit the websites, do you guys visit the course portal or check out what things there are? It is filled with a lot of things guys. For example, the interview and job offer tips. There are all sorts of tricks with pandas and glossary of terms and interview tricks, what to do, so on and so forth. Many, many things are there. So amongst the things that are there is the Gantt. So this is the original paper. By the way, there is one explanation which I found to be just a little bit of mathematical knowledge you'll completely enjoy. It is by this person, Ali Godsey. Ali Godsey is a little bit slow and in the old traditional style, he'll write on the blackboard, whiteboard. And some of you who have actually visited Support Vectors will realize that I also fall in the old school, but he's really very good. You can listen to his YouTube video on GAN and it's just wonderful. I have put it here and maybe it's not visible on your screen, so I'll make it bigger. So this is the paper. This is the video. I'm saying guys that if you want to usually what happens is you want to learn things from the textbook but I feel that the This paper the original paper is a masterpiece of clarity If you just read the paper and you can skip the mathematics in it it is the best introduction to Gann itself and Then watch the video of Ali Godsey. That is all you need to do. Forget your books, forget your practical books and labs and all of that. Just go and watch these two things and after that in the practical of course we'll do the lab, we'll do the PyTorch code and so forth. So these are the two good resources. I'll go to the paper for a moment. So this is the paper he goes on talking and so forth so the basic idea is that once again I've used the same notation as this paper D is for the police the G is for the generator discriminator in the generator now are those a few with computer with that sort of a minimax background this is a standard two-player minimax game is the loss function one tries to the discriminator tries to maximize the ability to tell the apart things apart and the the generator that he tries to make sure that the difference between the fake and the real is as small as possible that's why it's called the minimax the adversary right and the the basic idea is that see what happens is that in the beginning, if you look at these black dots, let's say that that is your real data. And in the beginning, the generator, see generator always has some hypothesis about the real world. Let's say that the initial hypothesis of the thief is, it is producing data from its own internal distribution that it has in mind like this, like the green line. Right? So what happens is gradually, step by step, do you see that the green is getting closer and closer to the data point? And finally, the green is now sitting upon the data point. It sort of have figured out the distribution that will produce this data, isn't it? And notice that I'm not saying that find the best fit curve or something like that that I'm not saying that find the best fit curve or something like that. I'm instead using the word distribution or the probability distribution, because that's what you use. What is the distribution that will align this generator to produce data? That is real data. So that it could essentially be a source of fake data that nobody can tell apart. So that is the gist of it. And as I said, there are two steps to it. One step, you update the discriminator, means the police. The second step, you update the thief. Do you see this? There's a little bit of mathematics which you can ignore unless you're interested in it. And on Monday, we'll talk about it. So the mathematics guys just purely ignore it this uses a the uh uh sort of a jensen shannon divergence for those of you coming from information theoretic background which is a symmetric kl uh kl divergence ignore all of that and you don't need to read any of the mathematics the the explanation in the beginning the way he talks about it in the first page just up to here just one paragraph two paragraphs puts the gist of gan in your mind the rest of it is just uh you know in fact you can you can look at this picture and most of the paper unless you're interested in the mathematics you can just of the paper unless you're interested in the mathematics you can just skip then let's go to the results so here these are the results and I just increase the screen size even more are you guys seeing my screen so you see you see the ones in yellow right that is real data so that is the real data. And the one just next to it, you know, to the left of it, do you see this 9028 where my mouse is? This is the data randomly generated by the generator here except the ones marked in yellow boxes they are real data and do you see how the last column the these two columns are very closely matched how how legitimate it looks do you agree guys oh yeah yeah this is another example these are some of the instances that were fed. And now look at this generated instances. Would you agree and look at these faces and say, my God, they look like photographs. They look like pictures. Scary ones, but yeah. Yes. And the thing is, this was the original paper in 2014. Today, the state of the art has moved much, much further. It is not so blurry anymore. It's very much more 3D, much more color, much more realistic these days. So it generates very realistic pictures. And here, so things like that, you can see it gets pretty close. And this was the original, it started a whole body of work. Now if you go back to the course again, where am I, or DC GAN, I think I should mention the results from DC GAN also. I'll add it to the course website. This is the original paper of DC GAN. The GAN Wikipedia has a picture which looks very real, the genitor of the GAN. Oh, okay, nice. Let's go look at it together. So this is the DC GAN paper. And this also, by the way so the state of the art has moved very very forward so this is your do you notice that the generator is a congolation network run backwards typically you have a big image on the left and gradually as you move forward you get a smaller and smaller image in a lot of channels but you start the opposite way here you take a hundred dimensional random data or people take either random or they take gaussian normal distribution and they just feed in and right away what did it do it created a four by four and a thousand twenty four of those features right so it projected it into a large dimensional space and created four by four images of all sorts. And then it is just doing the reverse of the forward pass. It's just going the other way around, expanding it out. Right? The only thing is max pooling has been taken out. There are minor things you use, leaky relu, this, that, but let's not get caught up in the details so so now it has created these images when the generator now you ask it to produce images of bedrooms and it comes up with all these images i don't know about you guys but this looks to be a pretty realistic uh images of uh on the first pass it produced this after a little while it begins to produce this and these are all things photographs that just didn't exist the gang is just conjuring it up out of thin air isn't it impressive yes yeah and they look such photorealistic, like they literally look like photographs. And the state of the art has moved even further. And the amazing thing that you'll realize on Wednesdays is just a few lines of code. That's the power of deep neural networks. And other things and what features it is saying. Anyway, this goes into this this but it is trying to understand what the features are how is it understanding bedrooms and so forth we'll leave that I won't get into that but here is the here's the thing that I was talking about you take a smiling woman and you subtract a neutral woman and you add a neutral man and all of a sudden you have a smiling man. Do you see? You have lots of generated pictures of a smiling man. Give it a few, yes, go ahead Patrick. What is the corpus here to be able to detect this is it the features that the like from your filters that When this Gans are trained at the end of it those filters get trained those filters get trained to either generate or detect specific features Right. Each of the filter is a detector of a specific feature One will detect remember remember edges, circles, there's that. So at the end of the day, they become pretty smart at either detecting or generating the features. So when you give it images like this, it knows which part of the feature space you are. Then you give it vectors like this. So in other words, just vector thing, smiling women will occupy one place in the vector space, embedding latent factors. Then you subtract a neutral woman. So what do you get? You'll get the ghost of a smile, right? What you're left with is just some other vector, which is sort of abstract. In the feature space, of course, it's a real vector. Not the feature space, in the latent space, in the embedding space, it's a real vector, except that it doesn't have any real meaning. It's like a smile without a person. And then you add a neutral person there and then this vector and this person added, you go to another region of the embedding space and that, if you go and see, if you draw from that region of the embedding space and you reshape that you know the point into 28 by 28 image that you say or whatever Square it into an image wrap it around reshape it into the image that column vector Make it two-dimensional and suddenly it will begin to look like a smiling person smiling man You get the idea? Yes, yes, I see. That's it, isn't it? I mean, I don't know if you're excited the first time you encounter you just say amazing. Even after all this time, I still feel amazed every time I open these things I feel amazed. Likewise, here is a guy with glass. You remove, you subtract the man without, the man out of it and you have just the wearing of glass hanging, a vector that represents just the wearing of a glass without necessarily a person, a ghost with a glass. Then you add a woman and the ghost is gone. You have the woman wearing a glass and quite realistic. It almost looks like even in this early versions, and by the way, statethe-art has moved much further ahead this looks to me a fairly reasonable job at creating the picture for women wearing glasses many women generating lots of pictures of women wearing glasses would Would you agree? So that is what it is, that's what that's what GANs are guys and you can do, how do you convert this to this? There's a whole process of going from here to here and so on and so forth. I'll let you have fun and I'll let you have fun and that finishes the topic of GANs. Now what I would like to do is again I must say that I did not explain the mathematics deliberately because I decided that we get into more technical depth on Sunday. So I will spare people today to that but if you want obviously the video recordings will be there and let me now go and review what we have learned after we review i would like to take some time to introduce you if possible to the new world of graph neural networks and so forth but for now so what did we learn we learned that we need to get a smart thief, because the thief will create a lot of fake money for us. Doesn't it? That fake money being whatever, counterfeit money being counterfeit images, counterfeit this, that, whatever. Pick your thing. But we'll start with money, dollar bills, $100 bills, $1,000 bills, and so forth. So the generator is the thief. The discriminator is the cop now the thief will pretty will try to his goal is to fool the cop and the goal of the cop is to not be fooled they have adversarial relationships right and so the way you train them and the cop is essentially a classifier binary classifier says fake not fake right the way you train it is first you give the cop the chance to learn whenever you train the cop one step you give it half and half of fake and real data and you honestly label it the fakes you label as fake the real ones you label as real let that let that is a cop predict then if the cop will make mistakes you get the last function you back propagate and you have a smarter cop slightly smarter cop next stage you make the j and the and obviously the thief is now falling behind so now what you do is you make the thief generate some images but then you misinform the cop as the thief will do it will try to pawn it off as genuine so even though it's fake data you label it as though it's real data you label it as real and you give it to the cop now the cop will get confused because you know here it is it's saying one real data but actually it will cut its model if it is any good will come up with somewhat lower probabilities but actually it will cut its model if it is any good will come up with somewhat lower probabilities and then it will lead to loss so suppose the cop is pretty good at it and then the you know the thief is lousy then what will happen there will be a big loss right the gap between Y and Y hat will be big let me say this point more so let us say that the cop is pretty good the generator has this fake generator has produced a data has produced data then what will happen the fake will claim one police will say no no I'm not going to get fooled I'm claiming 0.1 means there is a significant amount of loss right likewise another image it will claim one and the police will say no I I believe it's just less than one person chances genuine right so now the loss is even more 0.0, like 0.99, I don't know, something like that. Something here, it is 0.9. So things like that. So a lot of big losses will be there. From the big losses now, the generator gets the gradient descent. It learns from the backprop and gradient descent. So now the generator will generate better fake images. And so you repeat this cycle. First part, you make the cop a little bit smarter, and the second part, you make the thief a little bit smarter, cop smarter, thief smarter. And this is the minimax, because they have one tries to maximize, one tries to minimize. And so end result is that right the cop will try to amplify the difference between the two and be able to figure it out and the thief will try to minimize the gap between a real repeat a real dollar and a fake dollar right and see notice the fact that the thief never sees the real dollar in its life right the thief is just generating out of it just cooking up but it is listening to the chatter in the police station essentially in other words that's a point we have saying that it lets the back prop flow from the police to it black proper gradient so the gradient flow it sort of listens to and learns from that and gets smarter at that and those are the two steps of generative adversarial networks with that we are guaranteed to reach a stable point reach a minima the trouble with the minima is it's not always there it will reach a minima so So for example, one of the problems with the initial GANs was that, let's say that you have 10 digits, zero to nine. First, let's say that the three producers are zero, or maybe three, and then the police tries to learn about three. Then the police gets better and better detecting three. And the generator, what happens is that the neural network generator shapes to produce better three, and it's not producing other digits. And soon it becomes very good at producing threes, right? But then the cop catches up and now it figures out some way to tell the trees. So then suddenly, because you're feeding in random numbers, the generator generates something that is not like a three. And all of a sudden the cop is completely fooled because all that the cop knows is how to detect a three. And the whole cat and mouse game starts all over again. And this time they gravitate towards four or they gravitate towards seven. And so they can endlessly be getting trapped in local regions, semi sort of a states of equilibrium and the game may never stop the algorithm may never converge it will just keep going around in cycles it will get stuck here spend some time then suddenly jump here spend time and suddenly jump here spend time and it may keep on going that so people have come up with ways to improve upon it and it's not completely solved problem but it's much improved so if you look at the dc gan it's a little bit more stable one of the nice qualities of dc gan is the gener the generator is actually in some sense a convulation neural networks run backwards right or architected backwards and the detector discriminator is of course a standard coordinate with the classifier at the end softmax in the end and so that it is they play one of the advantages of that is you get this property of yeah so here's the thing the way to understand it is suppose you have a duck and a cow what happens is that suppose this is the data distribution in one dimension you could be here or here the duck looks like this cow looks like this you point these red dots at the point sample from that but the generator in the beginning is just point creating absolutely uniform distributions. But after some time, what happens is that it learns to produce data only in this region. It picks up the distribution. That is generative learning. That's all it is. And now you can generalize it to higher dimension. That is generative learning. One analogy I gave is a bell curve. If you want to learn a bell curve, you can just figure out the mu sigma square then you can generate arbitrarily many points uh similar to the original distribution right and so it has a very good vector arithmetic associated with it you can actually take just as word embedding says a queen is equal to king minus man plus woman you can play the same game and you just saw us do that we saw the man with glasses minus man plus woman becomes a woman wearing glasses then also gated gans produce neural style transfers right you can take a picture and paint it any way you want I impose the style of a great artist upon your fairly average picture and suddenly it will turn out that Rembrandt has been has come out of his grave and started painting a beautiful picture of your children so that's that's the thing about these things you can do with so long as the output is images, these GANs do really well. You could give it inputs. You can say, here's the thing, give me a room with a window and a girl playing piano. And it can then start producing images like that. One of the questions is when does it converge? Well, quite simply because you can look at the loss function over time, over iterations of both the generator and the discriminator, you can see at what point to stop. That is it. And what it is doing is it is just learning the probability distributions. I give this example in two dimensions of a cow, let's say two dimension of a cow let's say uh yeah let us say that in the feature space of weight and size so from one dimension i go to a duck's are light and small cows are big and heavy and any points in between are basically very easily fake data right so that the cop will notice it so soon the generator learns to produce points in only these two corners. So suppose a green in the beginning these green points are what the generator is producing. It is all over the place very easy to get caught. Gradually it starts producing points only in these two corners and when it does that what it is doing is it's producing very realistic pictures of cows and ducks. Right. And that is generative adversarial network. Guys, I noticed that it is 9.43. I was thinking maybe I'll introduce the concept of graph neural networks, but I think may not be a very good idea. It might be information overload you guys want to hang in here till 1130 or should we call it a day I should review it and call it a day today we have 15 minutes call it today okay I don't know we can wait lately I think we can call it today okay getting tired okay guys so then today was the last theory session of part one guys so um i will just review what we covered in this course i don't know if you look at what we covered in this course you'll realize that we covered uh between all of the sessions that we did um already uh let me just i'm just now going to share a course website. Let's see. So please, just for your information, guys, be aware that all the videos are there. Do you notice how many videos are here? And this still does not include the recordings from Sunday which will come hopefully tonight the one on bird we have a lot of recordings actually one to 27 27 yeah and 28 and including today it will be 29 by the time we finish Wednesday to be 30 and then we'll have our quiz review sessions whenever we do and then we'll have our sunday when we go into the mathematics again the the original research paper of gan is a reading so we have covered a lot of material and this session list says it all we talked about deep learning what it is about we learned about transfer learning so let us ask ourselves do we know what transfer learning is guys what would be a good example of transfer learning so let us ask ourselves do we know what transfer learning is guys what would be a good example of transfer learning bird yes we use birds we understand that we created a first neural network using python we did all our labs in python once you understand the theory these libraries begin to look very simple you have the forward pass the the loss function computation the backward pass and then the back back propagation of the gradient and then finally the step in which you update the way it's gradient descent step literally four steps and in python is literally four lines of code if you look at the labs and go back to the code that i wrote the main loop you remember that it was actually four lines effectively four core. That's the inner loop of the learning in machine learning. Right? The word learning. We did that. We did quite a few of the quizzes. Every week we had a quiz or more quizzes. Quiz zero was to review your background. Quiz one was basically on transfer learning and on interpretability of models and all the things that we'll cover. We did not do a lot of model interpretability and explainability. That is for part two and three. We didn't have the time to do that. We'll continue on. We did a bit of Python. For those of you who didn't have Python background, go and please review those videos. There were help sessions I gave for the lab. Then we talked about the fact that why are neural networks so powerful? By now, today, we are in the sixth week and ending the fundamentals. I hope if I've convinced you of anything, I've convinced you of the awesome and fascinating power of neural networks. And of course, it's dangerous too. It's an amazingly powerful tool. and one of the basic questions that comes is why does it work so in that framework we talked about the near the universal approximator theorem with respect to neural nets we talk about the fact that a single layered neural net can approximate any function that you want. Right? It's pretty clever at it. We saw that in the lab examples. We created all sorts of functions and we could see that the neural net was able to get to it. Then we built our first neural nets. We did quizzes on universal approximators. We did all sorts of things, deep neural nets. And then we talked about Ola's blog that explained what does it do? If you think from an abstract perspective, the input space and the output space, why is it that you are able to capture nonlinear decision boundaries, our relationships? And when do you know how many nodes you will need in your layers? That's a lovely Ola's blog very nice then we went on to do regularization methods lots of them l1 l2 drop out at least early stop right and normalizations bash normalizations and so forth and all sorts of regularization methods they are at at the heart of deep learning. The reason we can do deep learning so successfully today is because of that, right? We can create really deep neural networks, residuals, short circuits, and so forth. And that's what makes it possible to make, these days you create neural networks that is a thousand layers deep, right? So that is it. We learned the concept of back propagation. We learned about activation functions, right? And then we did all sorts of quizzes and labs on them. Then we did the visualization of the loss landscape. I showed you what is the effect that regularization has or different things have, batch normalization and other things have on the loss landscape, right? We saw it as the examples and pictures. That one, at least I find very useful in understanding what is wrong or what is right with the models you're building. I'm surprised that not many textbooks or people talk about it, but it's a very, very useful tool to know whether you're getting lost or you're converging to a good solution. That is that that the problem of vanishing and exploiting gradients and what batch normalization does we talked about convolation neural networks which are this business of filters and then we did about we did this lovely paper which uses a transformer for object detection which I hope you guys liked it then we did this vector space representation of text df-idf and so on and so forth and word embeddings in particular we went into the detail of word to work quite a bit I one particular implementation script gram we did end-to-end implementation notes and then finally we went more into transformers actually we did the transformer lab before i gave the transformer theory and then we had the understanding of recurrent neural networks and transformers right rnms and lstms and gius and transformers and now we did today one big architecture namely generative adversarial networks so this is the fundamentals guys this course I focus fundamentals a lot more in making sure we understand all the concepts so that going forward we focus a lot on actual doing things we want to code we want to build something we want to get a flavor of what does it mean to do that and one of the most pleasant surprises will be that once you become fluent with the with the concepts and with pytorch first of all new research papers that come are easy to read if you have never seen read research papers in this area of deep neural networks, of course you haven't. I hope, we have read a few papers in the last few weeks. I hope you must be feeling that the papers are not as hard to understand. We are not talking of some deep super string theory paper. We are talking about very straightforward field. Great breakthroughs, but easy to understand. And so one of the things we'll do in the part two, part three, is we'll take the breakthroughs as they come. We will read the paper and we'll translate it into code. Are we together? Like, for example, this object detection data, we will literally take the paper and we'll implement the entire thing in code ourselves. And then we will take scenes, pictures, roads, pictures of the road and this and that, and we'll see how well can we detect things in that object detection we can do with that paper in that. And so we'll become much more fluent with the practice of deep neural networks. That in a sense has been our theory, guys. And in the next, in the lab, we'll finish that. Guys, to request, I'll send you a course survey. Please do fill the survey. Also, do keep taking your quizzes. It's important because that's how you will know whether you know or not. And don't expect to get very good marks in the first attempt. Whatever you get, take that as a baseline. Go read again, review again again watch the videos again and then come back and take the quiz again and keep following the topics in your textbook the main textbook it's very straight how many of you are finding the textbook now relatively easier to read after the sessions anyone and yet anybody has a feedback? Yeah, it is more easier to read now. Much easier. So guys, if you are stuck, reach out to me or reach out to people here. I would love to sit and explain again because I want to make sure that your foundations are solid so that when we go into the practice of things, it is a smooth sailing there. There your worry should be, why is my code not compiling? What is the bug here? This, that. You should not have worries about, I don't understand anything that's written in the code. It should be more like, I know the general flow of code, but what do I do to make it work? Or how do I make it work better? And things like that. Can I use it to solve this problem? We'll completely change our mentality in the second, in the next parts of our lecture. Thank you.