 Give me a moment guys. I'll get the link, share it and then we'll proceed. Thank you. I'm starting the recording now guys so please be aware the topic is recommended part one to recap what I said, in this series, we are going through a series of Sunday talks. Every Sunday, meeting around noon. Occasionally, I get a little bit delayed, but I'll try to stick to the noon time frame. We'll give about an hour, hour and a half to the topic of AI recommenders. These are all AI models. to the topic of AI recommenders. These are all AI models. And we will cover these recommenders from the beginning, from part one. Part one is the original recommenders as they used to be to the state of the art today. So this started out as quite an interesting problem. It has to do with the whole nature of the so-called web tool. When companies like amazon.com, et cetera, came about, the main value was they were selling things online, you could order it online and you could receive it without going to a shopping mall and so forth. Now, when you look at the history of bookstores, for example, there was the Borders, there was the Barnes and Noble. Many of us have fond memories of those and taking children to those. They used to dominate the market. You would ask, why would an online bookstore, which is what Amazon started out as, what possible chance did it have against these behemoths, the Borders, the Barnes and Nobles and so forth? Why would they succeed? When you ask that question, you realize that Amazon started with quite a few disadvantages. For example, you could not see the book, you could not preview it, you could not in those days at least, now we have a preview functionality. People, books are very tangible things, people like to touch it, feel it, smell it and so forth and then purchase books. There's a pleasure in browsing bookstores and so forth, and then purchase books. There's a pleasure in browsing bookstores. And so you deprive yourself of all that pleasure. Furthermore, if you purchase a book, it would be a week or three, four days before the book actually arrived in mail. So with all those disadvantages, it is worth asking from a very practical perspective, why did Amazon take off so well as a bookstore? And the answer, there are many, many answers given to it. Today, of course, those answers, they look sort of no brainer. The answers that are given, and this is where we will start our technical discussion is, see, first of all, a brick and mortar store can have only n number of books and can be based on the real the the sort of floor area that you have or the shelf area that you have for small bookstores you will take the world's bestsellers in each of the subjects so suppose you're keeping books across at 20 different subjects in each of the subjects you will look at the top end bestsellers common sense strategy says that you should populate your bookstores or your little bookstore with the top end bestsellers. Common sense strategy says that you should populate your bookstores, your little bookstore with the top end items from the bestsellers list. And the bestsellers used to be quite a thing. Everybody wanted to know whether a book has made it to the bestseller. Because once it makes it to the bestseller, there was a rich gets richer effect, right? Beyond a critical threshold, your book would make it to the best sellers list or would get reviewed highly and had a higher chance of getting to the best sellers list. And so people kept hoping and praying that the very few critical reviewers would review your book well, and it would have enough sale consequent to that to make it to the best sellers list. Once it made it to the best sellers list, it would actually show up in a lot more bookstores. And once it showed up in bookstores, it actually had a fair chance of being purchased. So that was the world before these recommender systems came in. Now, when you move to the online store, technically you have a sort of an infinite warehouse you can do you can acquire a book or ask a publisher to directly send it to the to a user to a purchaser now how many books can there be suddenly you have a whole world of books open and these books the first thing we notice about books and and and and in consequence later on we realize that the same thing is true for a lot of a lot of other things like internal touch functionality so the the if you look at the number of items sold number of items sold and think of this as popularity top or people often call it in the in the language of bi a business intelligence most of you must be familiar with it as top end or as pop popularity the way is the distribution follows what is often Power law or Zip-in distribution. What it means is that if you look at the popularity a number sold, let me just call it popularity of an item I based on its ranking, let's say that the ranking in terms of popularity is R. So you would see that the popularity, the measure of popularity is r so you would see that the popularity the measure of popularity how much you sold was approximately proportional to r to the power some n right it could be this sort of a graph is power law graph or zephyr distribution power law graph when you have a power law graph and the other observation in this is in this power law and then you could have various different shapes of it of the power law and so forth but but one thing that was definitely true which this picture doesn't convey is that if you look at how long the tail is you will realize that it actually dips very fast and it stays, well, I am sort of increasing and decreasing it. That is an error. It should be flat, but I'm not able to draw a flat line. This is called the long tail. You have a long tail distribution. What it means is that while very few books are hugely popular, there's a tremendous amount of books whose sale is not zero, whose sale is greater than zero. Or you can even say greater than, let's say, 10 or some very minimal threshold, let's say greater than some epsilon value, enough that it is profitable to sell it. So long as you don't have to store it in your store. I mean, it's not occupying and displacing some high, more likely to sell book from long as you don't have to store it in your store i mean it's not occupying and displacing some high more likely to sell book from your bookshelf but they are of value and this still is heavy in other words if you look at that it is also a long heavy tail heavy tail what does heavy tail means if you look at the area under this curve, it is actually greater than half the area under the total curve. It means, what it means is, if you could sell these items, which other stores cannot sell, which brick and mortar stores cannot sell, this is your internet opportunity here, right? And this is your, let me just call it the zone of opportunity. Zone of opportunity in the sense that you literally have no competition from, or very little competition from the brick and mortar stores because they simply don't carry books from this long tail. Now the question is how valuable are these books? And from books, let's generalize to items, to movies and to things like that, right? So one example that you could give in US is, you know that in the Indian diaspora, Hindi movies are very popular. Why do mainstream movie halls not show theaters, not show Indian movies. The answer is very simple. The clientele is very small. The clientele belongs to this long tail. And yet you imagine that there is quite a bit of money to be made by exhibiting movies or just showing movies from the Indian, the Bollywood, because you have the market of the Indian diaspora which is significant. It's just not cost effective to do it in a brick and mortar store. So this becomes the zone of opportunity for example for Netflix right and for Spotify, the songs of various countries and so forth. And even within the US there are many genres. For example there is a science fiction writer. It was actually, she's passed away recently. A writer that I like very much, Ursula Le Guin. She has written absolutely wonderful books, and some of you may have read it, like The Left Hand of Darkness, or A Rose for Lucy, and things like that. Hauntingly beautiful science fiction stories and she never obviously she is not very well known she is not like the author talking or the author of lord of the rings or something but at the same time her reader base has never died she has had a loyal amount of readership continually over the years. So never very popular but never out of fashion either. And this long tail is made up of such interesting writers and such interesting content that is for which there exists an audience, it's just that that audience is minuscule but the sum total of all this minuscule audience actually makes for a massive, massive opportunity. Now, with that as a business background, let us now ask, how do you sell to this long tail, for example? You realize that if you try to sell to the top end, the other side of it, the most popular, this has a rich is richer, rich gets richer mentality. If you only stalk and recommend the best sellers list, you will keep on perpetuating and exaggerating the sale of the best sellers. Isn't it? Disproportionately. And things that just about don't make it to the best seller but still are very good, they will be altogether ignored, right? So it is a feast of famine economy it used to be for authors and things like that. So now we live in a different world. The world is full of authors and lots of millions of authors and they all have a little bit of readership and they all are in many ways we live in a much better world as far as producing content is concerned so the crucial question is how is it what were the factors that went into making this zone of opportunity come about so one of the things is if i could tell that suppose you have a user user x iot user and there are many items so let's say i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. i.e. We are not even asking about rating. What you are asking is given user I and given item J at the intersection of user I and item J, what happens here? a website like Walmart or Amazon or Netflix or even YouTube and so on and so forth, any of these content providers in some sense that you purchase, what happens is they give you an ability to purchase things, which is the first thing, engage with it, engage with items. When you engage with items, engagement can take one of many forms. A very concrete form would be the user actually purchases that item, right? Or in YouTube, it would be the user watches that YouTube video, right? A less concrete form but still suggestive of a signal would be the user went and read the description of it, right, searched for it, and then drill down into it, and actually read the description of that item, right? On Amazon, it could be a book, it could be one of the items on YouTube, it could be a video, and just read through some of the comments and so forth. Another signal could be that the user searched for something and it showed up. It means that it is within the zone of relevance for this particular user, that particular item or that particular book and so forth. And some even stronger signals could be the user not only consumed or purchased an item, the user actually left behind a rating and a comment for it. And now what happens is if you look at the data, you have a lot of data of items showing up in searches. You have obviously lesser amount of data of user drilling down into items, of all the results that show up in the search result when you visit Amazon or YouTube. You drill down into only a few of those. And then once you do that, even smaller amount of data, you actually go and watch, right? And even smaller amount of that after that, you probably share or you share and perhaps write a review on. So what happens is give a rating on and even fewer you bother to take the time to write a review for. But to the extent that we are, I mean, the fortunate thing is human beings are a very social, very gregarious community. We like to express and say things. Fortunately, even though the comments are the smallest set of data that you will get from the user interaction user item interaction it is still substantive the initial question that that that was posed is can we predict so the basic question that we tell is given a user i given a user i given an item j so you have a fundamental thing can we have a function such that this function maps a user and an item to something either it goes to a boolean field it says will be interested thumbs up or thumbs down will be interested or not it could be one zero will or will not buy so yes now will buy or or not i'm taking an example suppose your goal is to sell this item to the user you can do this so this is one question the other way you can do that is would the user give a thumbs is, would the user give a thumbs up? Or would the user give a thumbs down? Let's say a thumbs down, thumbs up. Thumbs down, right? This could be the case where a user visits a YouTube video or something like that and gives a thumbs up, thumbs down. Or you could give a rating on a on a like art scale. You could say you often find this rating on a five scale, right rating. Now when you give a rating on a like art scale, like art is a formal word, like card, like I believe is it E or a i forget okay like art scale then you take three as neutral anything below two is below three is negative and anything above three is positive right and so these are the things and lastly of course you can have a detail, a comment as a rating. So if you have a comment, which is a text, what you can do is in effect comments can be translated using natural language processing, a sentiment analysis. So, and all of you have taken the deep learning course. So you remember we used BERT, the one of the earliest examples in natural language processing that we did is we use a transformer model, a BERT model. Remember hugging phrases? We used a hugging phrase BERT or transformer model. And there are many lesser models, many models, many different models, not just that, to do a sentiment analysis of the text. And you again come up with, you can convert this either to rating, to some form of rating, right? Either a like art rating or thumbs down rating, positive or negative sentiment, right? Positive, negative, or scaled. You can do a scaled rating. It's very easy. Sentiment analysis today, as you know, natural language processing has advanced enough sufficiently that we don't catch sarcasm, it has limitations, it doesn't get all the things right. But as far as business is concerned, we have good enough signal, right, we have reached the level that we have good enough signal for sentiment analysis, right? In a statistically aggregated works, though in the individual cases, it might not be so perfect. For example, it doesn't catch the nuances of humor, of sarcasm, of irony and so forth. But with all those limitations there, the state of the art is today useful, right? So this is it. And now the question is, this is what we want to do. This is the fundamental problem that we are trying to say. I will just leave it as a buy or not buy, right? Now, the history of this field is, before AI started proposing solutions to this, you can imagine that this solution is older than our Web 2.0, it's older than Amazon or anything. It's a fundamental question that marketers ask. Who should I sell this item to? And when you try to figure out who to sell the item to, there are many, many techniques. The broad class of techniques is that you do, but you don't, the traditionally, you don't, you were not using AI for, and let me use a different color to do this, before AI, what you would do is you would look at, for example, item characteristics. Item characteristics. So for example, if I notice that you're searching for computers, right? notice that you're searching for computers, you happen to read some computer which belongs to, let's say, Dell, a Dell machine. What is the next thing that you do? The user hasn't yet purchased it. It makes common sense at that moment to recognize that this particular item is a computer is a desktop computer with a certain characteristic so for example it's a mid-range computer and you have some demographic information this sort of computer is popular amongst you know certain income groups or students or something like that what do you? You immediately start showing more computers, more computers to the user. And you see this behavior very much in effect. If you walk into a store, you go and talk to a salesman and you say, I came looking for this particular computer. He'll take you to the computer, but very soon he'll start showing you all the other computers and all the computers are kept in proximity of each other. So that the user can potentially comparison shop, compare this model to this model to this model and their features. And obviously the stores will very helpfully put the characteristics, the features of those items in some little display that this is this has this much memory. This has more memory. this has even more memory, this has a bigger processor, this process, and so on and so forth. So there is a way to show a gradual gradation of computer abilities along various dimensions, and the user can select. So the same idea when taken to the web, it's literally lift and shift to the web environment. Could we do that? Yes, we can do that. The user is searching for shoes. So let's show all sorts of shoes. It's common sense. We can do that. So that is item characteristic based recommendations. You can recommend things or show based on that. And by the way, there is a continuum between search and recommend. If you look at Amazon, the lines between searching and recommendation are blurred. In fact, today we consider search to a large extent to be AI driven. It's not everywhere true. So for example, we, and again, those of you who did, none of you here in the audience did that project with me, But if you did it, we made a distinction between the search results coming from elastic search and the search results in which you have further augmentation from AI, deep learning based search and so forth. So the latter is highly sort of has a huge overlap or is interleaved with recommendation systems. So it will keep that aside. The second way is user characteristics and again i'm talking of the world before ai this is trying to segment the user into buckets so for example you wanted the joke is the last thing you want to do is sell a lawnmower to somebody looking living on the 10th floor right because you know that that would not be relevant. So you bring the user up into certain segments, market segmentation. And for each segmentation, you know what items to show, right? So for college students, the last thing that you want to do is show potentially minivans. They're not interested in minivans, whereas a different group of people, you do show minivans have a different market, isn't it? And so for likewise for luxury cars, it is pointless to show it to young people with a starter job, freshers with starter jobs and so forth. At the same time, you don't want to, and so the car industry, you know, there's a huge amount of segmentation and people have niche market, brands have niche markets that they go after. So that is user-based recommendation. You try to find the, where does this user fit? Which of the segments, market segments the user fits into based on the user characteristics. And now you can sell items to the user based on that segmentation. So that sort of thinking has been there. And you pick up any book on marketing in MBA course, and you will see these things talked about quite a bit in a more broad sense. Now, at the turn of the century. so now this is the history of it, and we will come now, so market segmentation, I'll just put the word. So now I'll come to the turn of the century, the 21st century, and now I'll start getting a bit more mathematical. See there was a big question. The whole value proposition of this so-called e-commerce companies that were emerging was from the long tail, long heavy tail. And somehow, we needed to find for a given user, what were those unique things sitting in the long heavy tail that this user would find useful right so once again we so the the function that we talked about let me use a different color we're changing theme here let's say i'll try this so the basic function is given a user i given an item j what is the value what is it is equal to what so initially people were looking for explicit rating they were looking for a number amazon started the tradition as a rating from one to five then netflix followed and if you look at this did it succeed it did succeed quite a bit. If you ask this question, why did Netflix, for example, completely break down Blockbuster, which used to be a video brand. We used to all, if you remember, some of you, we used to rent videos from Blockbuster, forget to pay back, or return the video in time. And then of course pay a tremendous amount of penalty. And in fact, they used to monetize a lot of large part of their revenue from the penalties and so forth. So that's the old history of this blockbusters and so forth. Today, most the young people have never heard of blockbusters and similar brands. Why? Why could they do it? Along with it is the fact that today, e-commerce companies, sorry for the . Today, e-commerce companies, you go to Amazon, can you guess what proportion of the sale happens because a user came to the site knowing exactly what they want versus purchases made by what amazon recommender system recommended so some of you of course have know i've spoken of the answer in previous workshops. So please don't speak up. But those of you who haven't, can you make a guess what proportion of the money or the revenue of Amazon actually comes from the recommender systems? Things that it has recommended to you that you eventually end up buying, as opposed to, I know that I need this particular device. And so I've come to the site to buy this particular device. Anybody would like to venture a guess? I think about 70 percent. 70 percent comes from? The recommender system. That is true. Most of it, a large proportion, much more than you would have thought comes actually from the recommender systems so recommender systems have come to dominate our world and in fact if you ask this question what is the singular value proposition of this e-commerce site first of all there are multiple things one is the wisdom of crowds whenever you go to an item you see the ratings on that item or the thumbs up, thumbs down on the item contributed by community. Now, if you remember the ensemble methods thing that we learned, the wisdom of crowds is generally better than the statement of one or two experts. It's more trustworthy for sure. So when you get critical reviews of a movie, you may or may not like the movie, but quite often you'll notice that your own review is much more aligned to some users. And some ordinary people who have reviewed the movie and given things, comments on it. So that is the wisdom of crowd kicking in. That's a huge factor. The fact that you can actually observe to study what others thought of the product of the item. And the second factor that is much more that is equally relevant is that it uncovers for you things that the moment you look at it, you know that it was in the back of your mind, you were thinking of purchasing it, and it is actually useful for you. So for example, going back to that situation of a Dell computer and recommending more, see, till you haven't bought a computer, seeing recommendations of a computer is very useful. But the moment you have bought a computer, you will realize that that old way of showing you more and more computer is absolutely useless because computer is the last thing you want to buy. It is probably more pertinent to show you a printer, a scanner, a monitor, anything but a computer, isn't it? So what is more vital is to know what else, what is it that you should be next looking to. So that brings up multiple dimensions from an AI perspective multiple ways the old way which was a did my in data mining we had something called market basket analysis so you would do associations you would notice the fact that computers and printers and monitors tend to be bought together and mice and keyboards they tend to get bought together so you would create these associations and there were all these algorithms like the air priority algorithm and so forth that you would apply on the other hand you uh you could look at it also and this this gets into the sort of ai we have been doing just a few months ago you can look at it as a sequence model, a time series. What is the next thing a person is likely to buy after buying a computer? So you realize that if you don't have evidence for this person buying, let's say a printer, and you can put it in a slight, in a buy-in perspective, let us say that the prior probability and this is how you would you would say that suppose the prior probability of buying a particular printer printer i is something right this is your prior probability now you don't this is in looking globally at the data and knowing anything about the user. Let's say that a printer tends to sell one for every 10,000 users. So the probability, the prior probability of the printer being sold is one in 10,000. Now look at the evidence. The user has just bought. Given the fact that the user has bought, given the fact that the user has bought, you multiply it with the likelihood of now function of, you have just bought a computer, right? And so now you have the posterior probability, the posterior probability, the posterior probability, probably probability of that printer. Would you say that the posterior P is greater than prior probability? Now that the user has bought a computer, would common sense dictate that you're much more likely to buy a printer? Are we together guys? Does it make sense? Anyone? Everybody is on mute. Yeah. That makes sense, right? So what happens is that there is a certain amount of sequential, you can build sequential model that given the behavior over time of your purchases and your interactions, what are you next likely to do? So you can build a model on user interaction, predict user interaction. Over time. And actually from an AI perspective, we are doing that with great success. See, you can do a static model, a model that is invariant of the time dimension. Or you can build an AI model, which is a sequential model, very similar. And when I take sequential, I mean it in the sense that we were talking about in the deep learning workshop series. For example, you know, LSTMs and RNNs and GRUs and, you know, stuff like that. So we can't be using LSTM model or any recurrent neural network model or even one of the classical time series AI model like ARIMA or something like that. You could use, though it's much more traditional, it's much more common to use today in 2021 to use a deep learning model for these things. So you could do that, right? So there's a sequential nature for that. So now we will ask this fundamental question, how do we solve this question in today with modern AI? So when people started looking at this question, the history of it is that Netflix had an algorithm in-house, the sign mark or something they used to call it, in which they would predict what movies a user is next likely to watch or would find interesting. And they would recommend it on their website. Remember, Netflix in those days was not streamed. It was still going out in envelopes by postal mail and people were returning those envelopes after watching the movies those dvds after watching the movies and yet it's a testament to the recommender systems that netflix even then completely demolished blockbuster and all the other brick and mortar video stores completely wiped them out the streaming service and then of course came the streaming service, which added a new dimension to it, which instant in time, you like something, you start watching it right now, aspect of it. But even before streaming came, Blockbuster was practically for all purposes destroyed and gone. The old world was no more, you know, we were in a new world. So that's the value of the recommender systems. Now, they threw a prize, a million dollar prize, a million and one, 1.1 million, I believe, or one, let's say roughly $1 million prize. And in that prize, they asked this question, can somebody beat a model by 10%? And many teams tried, and then the teams collaborated with each other and so on and so forth. And at the end of it, the team that won, which was a collaboration between two, three teams, finally beat the 10% boundary by a significant amount. And I believe they first went to 11% then they went to 12 or 13% or so. And they soundly beat Netflix's own model. That algorithm, we will use that as a beginning foundation today. How did they do it? How did they do it? And having done that, it sort of started this whole world of personalized recommendation. The solution that came out of the Netflix Prize is the context for today, I would like to give the whole time to walking through it. I have walked through it with some of you perhaps in a brief, more briefly. But now I'll go into the technical depths. So that started amongst other things. And I'll just end the historical preview here, or the prelude here, what it did is it started an explosion of possibilities e-commerce really took off at the same time you could see that it was affecting many adjacent areas for example online advertising this whole business of targeted advertising is nothing but the recommender systems coming and taking over. And they have been very, I mean, today it has become uncannily effective. If you own one of the affinity cards from a grocery store, it is today well established that when you enter a grocery store, you might have a list of things to buy, like most of us are responsible, and we'll have some things on what we need. And we think we'll buy this, the things on our list. But when you look at your shopping cart, quite often, for most people, not for some highly disciplined people, but for most people, what you come back with is somewhat resembling your shopping list, but it has a lot of other items. Now it turns out that the recommender systems are much better able to predict and tell what will be in your shopping cart when you walk out than you yourself were. And just ponder over it. You think we know, we all think we know ourselves. And when we enter a shopping store or we enter a grocery, we feel we know what we are going to buy. And yet the shopping cart is much more reflective of what the mathematics tells you will come out with. It is that uncanny accurate. And there has been many, many cases. I'll end in the prelude with one particular case, which was to do with Target. So Target started doing this targeted advertising, I suppose, it's a pun. So they got hold of a Silicon Valley company and say, Can you do targeted advertising for us? This was a much talked about case a few years ago. So this is why not Let's do targeted advertising. They started sending targeted advertising. What it meant is that the brochure that you got was different from the brochure that your and the coupons that your neighbors got in the coupon. So they were subtly different. Then one fine day, dad walks into a department store target, very angry and says, there's something wrong. What do you mean? What are you guys trying to insinuate? We noticed that in our brochure, there are coupons for, you know, pregnancy items for pregnant girls and for childbirth and things like that, you know, young babies and things like that. And we don't see it in our neighbors items. These coupons are not there in the brochure for our neighbor. And obviously the department store manager was very apologetic and was, he expressed a lot of, I don't know, concern and apology. And then if I remember the matter correctly, I only vaguely remember the whole story. If I remember the matter correctly, I only vaguely remember the whole story, what happened is a few months later the father comes back and now he was apparently perhaps angrier. He's like, how did you know? How did you know something that we didn't know? So it turns out that in that house there was a girl, a young girl, a teenage girl or something, who was pregnant and when you are pregnant and you purchase things in the department store, your behavior changes slightly, your taste in things change and what you pick up changes. And the AI had caught on to that because it had caught on to the signal, it was recommending different things to that household knowing that this was the user characteristic there now and obviously in physical terms pregnancy takes a little while to manifest itself and so the family became aware much later about it right and it just goes to show the uncanny power of the recommender so that is the end of the historic prelude. And that was a few years ago. So you can imagine these recommender algorithms today. There is a very interesting thing to it. On the one hand, they're really worth established. You can almost say that recommender is old news. It's boring. I've heard these phrases come. People are using recommenders and they say it's totally boring. All you have to do is take up some libraries, plug it in and it works. That is true, by and large. Today, the recommenders are a very mature domain and there are excellent libraries that do it. But there are two other sides also to this story. While you get a generic, very pretty good performance from a recommender, and they are in quotes boring, we have really mastered them from our artificial intelligence or deep learning perspective. And uncannily, they do far better than we do as human beings. And they guess things that we couldn't have anticipated. They're doing all of those things, and they're very mature libraries to do it. Yet there is a huge scope for improvement because every bit of improvement that you make to a recommender system translates typically in e-commerce to an untold amount of millions for every company, every large e-commerce company. It means a lot. For example, if you're recommending books to users or in the company that I work for, which is, we are, as you know, my day job is in the learning company, we recommend personalized learning. The fact that you can recommend personalized learning, you can understand what a user is trying to learn, what the user should learn and so on and so forth is tremendously powerful. At one point, I remember there was one client, a British client, a large British client, and they wanted to use it. They said that we'll purchase your product if and only if you can show us that it works. Now, obviously real life is more complicated. You know, in the application, there may be all sorts of constraints and whatnot. So we deployed the application absolutely without constraint. And we got a team, obviously, because it was a high powered, really big company, as in global conglomerate. So they got their best AI experts there in that place. These were all PhDs in machine learning and so on and so forth. And the whole test was that use it for three months and then we'll make your recommendation because it takes time for it to understand user behavior. They used it for three months at moments. They came back and then we asked, compare your recommendation. And each person in the meeting said, I asked them, what did this system recommend to you and what did it recommend to you and what did it and they all talked about the recommendation and i remember one of them saying what she said still resonates with me she said that these recommenders are so uncannily specific that it's it's practically it is actually a privacy violation to reveal what the AI has recommended to me. Just revealing it, just speaking it out as a violation of privacy. So this is the extent to which the AI recommenders have become good. And what we had done, actually, I would not by any stretch of imagination say we had done any rocket science, actually, for that particular system. What we had done was, yes, we had done any rocket science actually for that particular system what we had done was yes we had done some bit of original work in that but broadly it was based on well-established theory for the last 10 15 years right and that is how good it can be well there is a other side also to this story a recommender systems or the recommendations of AI, they don't exist in a vacuum. The recommendations, by the time they bubble to the UI layer, there are many application layers that come up. And that is where the cautionary tale is. I have seen here in my place, and I have seen in many places, because obviously in our trust, you see many people come from different companies and talk about the experience and so forth quite often two things happen when recommendations don't work people always look at the recommender system when they do work they they then no one obviously questions that you know it is like astrology I like to joke whatever. If you make one wrong prediction, it will be noticed. Google, for example, Google in search results produces something or recommends something hilarious, everybody will know about it. I think in India, there was a meme that became quite, quite famous, right? Somebody was ordering a fan. And apparently, I don't know how, whether it was a fake image or it was real, Amazon India recommended a stool and a rope. You know what it means, right? Did anyone catch the implication? Yeah. Yeah. Yeah. Yeah. Exactly. And so that instantly became a meme. Everyone in the world knew that Amazon is recommending a stool and a rope with fans, right? So it's dangerous. That's a danger of recommendation. You have to be cautious with the recommendation and be very sensitive to the application domain. You have to put enough safeguards around it because if you don't, you get into trouble. So that's that. So now, with all of that big introduction, let's go a little bit into the technical details. I will start with the approach. So there are many approaches to it. The dominant approach that I will start with, so let me break it up into two areas. There is a content-based approach. Content-based approach. Content-based approach, by content, the word content is used in a more generic sense, user-based, item-based, or both of them together or together what it means is you actually see what are the characteristics of the user it's sort of like market segmentation the old marketing market segmentation thinking user characteristics or you look at item characteristics you see that this item is this and so on and so forth. Like for example, if you're looking at user, you look at the age, the income, the gender, the such demographic aspects. If you're looking at item, you look at now all sorts of characteristics of the item, right? That it's a computer part and this is a tractor part, this is a tractor and so on and so forth so those are the item base and you put them together and you can do it and you can still apply ai techniques right so one very basic ai technique that you could do is you could just ask this question that suppose there's a user features a feature one of a user feature one let me call it the user feature and feature up to feature n. And let's say, and this could be the height, I mean, I don't know, the age, the income, the et cetera, et cetera, of the user. And then you could have item characteristic, let's say item n plus one, some feature of the item, let us say, of the item up to feature m, right? This is of the item, right? So item feature could be, for example, its price, its category that this item belongs to, sub-categories, the product weight and whatnot, right? So you can give all of that. So this forms your X vector, right? And so you're asking this function, will this user buy it or not? So if you say buy or not, now this is your classification problem. Or you could say rating, what would be the rating the user would give? And that makes it into a regression problem are we together guys right or what you could do is you could take this entire feature vector and you could look for clusters within this and do all sorts of dimensionality reduction also do unsupervised learning unsupervised learning, dimensionality reduction, or clustering, etc. So that is one approach to doing things. We will come to that. Today, I want to focus on the so-called collaborative filtering, which is the dominant methods, collaborative filtering. Within collaborative filtering, again, there are two fundamental sort of, there's a dichotomy. One is model-based methods. You actually build a model. You say the user buys this item because, and here is a reason, here is a model that you have that tries to capture the dynamics, capture the because of this, right? So what does the data show you? What is a manifest data? The manifest data is user, some users have bought some items, right? But you build a hidden, a latent model that explains why this user bought this item, but not this item and not this item. Are we together? So when you try to infer a causative model underneath it, or not, I wouldn't say causative, it's a generative model underneath it, or not, I wouldn't say cause it, it's a generative model. When you try to infer a generative model that can tell you whether this user will buy or not buy any arbitrary item here, that is a generative model in the AI sense. And you build a generative model. There is another way of doing it is to not build a model at all. It's an unparametric sort of a prediction. And those are memory-based models. Well, I wouldn't call it, model is sort of a not, people use the word unparametric model, but I will just call it unparam, memory-based approaches or unparametric approaches. So how would this approach work what you do a little so these are these again come under the category of item based or user based so what are item based and user base i and i always in my case i always tend to get muddled between the two. And then I realized I've said it the reverse way, but I hope I'll do it the right way. So what you do is, in the item based approach, you say, given an item, so you know that given an item, you would have records of many users who would have bought it, isn't it? So let's first go back to this big matrix, the interaction matrix is called the, technically in all literature, we call it the interaction matrix. And most of the mathematical literature in this space, they represent this with a giant M. It's your interaction matrix. By convention, the y Y axis is the user. The rows are the users and the columns are the items. Items, users, are we together in this matrix? So given a user I, and given a user, given an item J, what you're trying to do, item J, you're trying to find what is the value here in the interaction matrix. The value could be just the user bought the item, or it could be a rating. When it is a rating, thumbs up, thumbs down, or a rating on a Likert scale, a number, let's say one to five or whatever, one to 10. Generally one to five is whatever one to ten generally one to five is already considered too much suppose it is that then it's and you build a recommendation system in which you are trying to predict a number a rating then you call it the explicit recommender so let's get a terminology right in the basics. And today we're just setting up our mathematical terminology. We'll go into details from the next one. So explicit recommender predict a rating. So in some sense, it's a regression problem, right? You're predicting a number. You're predicting a rating which belongs to the set of real numbers. That is what sort of like regression, like regression. On the other hand, these days, increasingly we realize that it's far better just to answer the question, will this user buy it or not? To buy is a vote for that item. Most people don't. As I said, the data set for just people buying is bigger than the set of data sets for leaving explicit recommendation. So you can consider a person buying an item, watching a movie, taking a course, right? Listening to a song as a positive word for that item. And you don't have negative word, you don't know what the user is. So implicit recommendation just goes to, and which is what we try to do these days. What we try to do increasingly is we have leaned towards implicit recommenders more and more. Which doesn't mean that we don't do explicit recommender, but we give, let's say, more weightage to implicit recommenders because we know that owing to more data, they just tend to perform more. See, it's a first order. First question is whether you'll buy it or not. And then may or may not be important what you will rate that item, how much you will like that item. Because the fact that you'll buy it or not. And then may or may not be important what you will rate that item, how much you will like that item, because the fact that you'll buy it is an overwhelming your preference for the item to begin with. So sort of like that. So implicit recommender just answers a Boolean question. Will you or will you not buy this item? Great. And of course, it's more like classification, like classification. Great. And of course, it's more like classification. It's a classification problem. So for the rest of the discussion, I will assume if I mean explicit recommenders, I'll say so. But when I don't mean explicit recommender, then assume that I, for the largest discussion it it may be either of the two recommenders and in practice the examples that we will take uh will lean towards the implicit recommender in keeping with contemporary practice uh 10 years ago it was different 10 years ago we paid a lot of emphasis on the explicit recommender. I remember the recommenders, the initial recommenders that I was building more than, you know, when it was just hot about 10 years ago, these things were absolutely hot. If you could just do a basic recommender system, you got a job in one of the, you not just got a job, you could do a startup in Silicon Valley. At one time, I remember the joke was that every single startup was nothing but a, I mean, it was just a cottage industry of recommender systems or what they call targeted advertising. There were so many, many companies doing targeted advertising and targeted, and this recommendations and so on and so forth. It was, it wasn't even funny. It was odd to see some of the brightest minds of the universities graduate and all they were doing is they were all trying to make the recommender systems and each of them essentially looked alike because ultimately each of them was fueled by the state of the art in theory and the theory is well published the research papers are well published and it is just how you implemented the product development around those recommendations systems for different use cases and customer needs is what created a whole cottage industry of lots and lots of startups and lots and lots of companies there but today we have moved forward so where are we today there are many approaches from uh now let's look at it from a from an ai perspective we will use it we will use a family of techniques starting from the original paper which won the netflix prize where is that i wanted to show you guys the cover of that iconic paper yes here it is and those of you who took a workshop with me many years ago i would remember that i covered this paper in great detail. Does anybody remember me covering this paper? Well, it's not a paper, it's an article actually, but it sort of is a famous one in which this matrix factorization technique that won the Netflix million dollar prize was brought up and i will explain that to you so that's a linear it's a linear technique extremely effective very effective actually for a whole variety of problems the virtue of this technique is it doesn't use for example deep learning right so as all things considered it it is a track table problem. You can actually build a model quickly, relatively quickly. I would say relatively quickly. Building a recommender system is brutally, brutally expensive to give you a sense of how expensive it is. Where I work, I lead the AI team, we build recommend again for personalized learning and we have about 75 to 100 million, now we have close to 100 million users but it used to be 75, 70 million users and or even less actually. Before that the models we used to have 50 million users and across about 100 to 200 million items to be sold, which is nothing compared to Amazon. Amazon has 300 million users and maybe even more now, and about 100 to 200 million items. So the items in our case are still pretty huge, but the number of user base is bounded to about 75 to 100 million users. One training, just one AI model when we would train on a cluster, even today as I train, as we do this weekend is being trained, we are using close to 100 machines. Each of these machines is a super server and the computation finishes over seven eight days right the whole ai recommendation generation takes a lot of time so ai this these recommenders are enormously powerful but behind that power is also a tremendous amount of computational cost so which is far more than what you would get for typical classification or regression. We are, especially in the deep learning territory, today it's a given that you burn a tremendous amount of electricity to train a model. And you have to keep training a model because user behavior changes, new items keep popping up, new users keep popping up. So we retrain our model every two weeks. And now in the next generation architecture that we are just putting in place we are now we are going real time people apparently not happy that a model gets trained every two weeks it is a psychological thing if you say oh you know a model get rebuilt every two weeks they they have an unhappy face they would rather that we built model in real time right new user comes new item comes, it gets new user interactions happen. You factor that in real time and do that. Does it help? Yes and no. Yes in the sense that locality of effect. If we just found out that you have developed interest in, let's say, thrillers or science fiction books, you don't want to wait for two weeks to recommend. Just when we see you getting interested in science fiction books, you want to recommend more science fiction books. You don't want to wait for two weeks to recommend. Just when we see you getting interested in science fiction books, you want to recommend more science fiction books. So your model has to have locality, time locality of reference quickly to catch on to the trends. The other side also is true. Human behavior or human likes and dislikes, they fundamentally don't change, especially let's say that in my field of learning. I mean, obviously leaving aside the whole educational theories and cognitive styles, each of us have a well-defined mental makeup of cognition. And so you can pretty much tell when very early on you catch on to a person's cognitive style and how or what they would like. After that it doesn't change much. So there is some value in having real-time recommended systems and there is some value in not bothering because when you're doing computations at scale once in two weeks is enough. This is by far and wide I believe the social graph somebody told me I don't know how true it is that Facebook does its full model update relatively infrequently, but they keep putting incremental layers on top, incremental updates, like I'm saying, and then every once in a while only do the full social graph model updates, based learning updates and so forth. So your mileage may vary. But anyway, these computations are brutal. But the matrix factorization, as all things considered, today we consider it somewhat simple. So what is the basic idea behind a model-based approach? The model-based approach, actually, let me just first take the memory-based approach because they are simpler. So you look at this. Let me draw the matrix again here actually i'll stop today and i noticed that we are running out of time let me take this matrix so let's say that you have a user x i the whole it is a row right what ratings the user has given or bought or not bought the item. Now you realize that this matrix M is sparse. M, what is this quality? It's a very sparse matrix. How do we know that it's a very sparse matrix? Simple. Suppose there are 100 million items in an e-commerce store. any one user, you would have bought up to 100, 200 items. 100, 200 items are nothing as a data point compared to the 100 million items that exist. It's a one in million ratio, right? So in other words, only occasionally you will find that it has been bought. Most of the time, there is no data. There is missing data. words this question mark there is not even that you won't buy it is just missing vacuum every once in a while there is data that you bought some item right so for a user it is like that are we seeing that guys likewise for an item given an item and suppose there are 100 million users available or 300 million users available so it's a rectangular matrix and this is important when we go into the linear algebra of it. So given this item column, it's a column matrix. You would realize that this too is a very sparse. There would be only a limited set of things. Only some extra, extra popular items would be something that maybe even a million people have bought. Most items are bought by a thousand people or 2,000 or 10,000 people. At 10,000 people, when you compare across a hundred million people, it's still a sparse purchasing history, right? So M is very sparse. In a way, in a way, you can think that you don't know what these values are, right? And for each, what you're trying to do is you can look at it as a journey from a sparse matrix to an M hat. Remember, we put a hat for prediction, predicted matrix, prediction matrix, a prediction matrix of interaction, very sparse interaction matrix. You're looking at a predicted interaction matrix in which all the gaps have been predicted. So for example, this would say, will not buy this, you will buy, right? And so on and so forth, will or will not buy. Now, remember that in our classification theory, we say we don't like to predict absolute numbers because it's a probability it's a posterior probability probability what you're trying to find is the posterior probability in view of the evidence that this particular user probability that a user i will buy an item j and that belongs to the we we tend to put it in the interval from zero to one right it's a real number so it's a real number between zero and one so it's a certain probability so what you predict are not zeros and ones what you predict is the probability that this person will buy that item right closer to zero means it seems unlikely and closer to one means has a high probability of buying this item right so now look let's look at this the item based right what suppose you do an item based what you can do is pick an item let's say let's say pick a printer you want to determine who should you sell this particular printer to so what can you do you could actually look at or actually let me do it the other way around let me do user based user based so what happens is that given this user would have bought or i don't know it doesn't matter so you can you can do it both ways so you can find items item one item two given this user what items the user has bought right uh suppose they have bought key items some some key items now what can you do the items that the user liked most the top and favorite items of the user you can take those top end and then you can go to this for those item so let me just write item column vector you know the column vector here as y j right so you can find the similarity some sort of a similarity measure so this could be from your cosine similarity dot products or something like that but generally you have to be a little bit smarter than that so some similarity and a lot of the secret source of recommenders is in how you design your similarity function you can do a similarity measure between this one of the light items let's say that i y i is the reference light item and it's uh it's column vector and you do a similarity measure with all other items so you really the scale of the problem you have to compare against all 100 million items, find the K nearest neighbors. When you find the K nearest neighbors, you have to now you realize that that's a linear problem. It is order number of items, order in the number of items, and then only you'll be able for each user, you in the number of items. And then only you'll be able, for each user you'll have to do this. And suppose you take N favorite items, good items that user actually liked, the items that the user liked very much. And then you do the similar exercise, then it becomes order N times order N, which means that for a given user, you're doing a n times m computation for each user so this is again quadratic already computationally brutal and then when you realize that you are building such a model for each of the users and now it becomes Now it becomes even more intractable problem. It becomes an order. Basically it's a cubic order. So suppose there are N users, N favorite items per user, and M total number of items. You're looking at this particular order of magnitude computation, which is quite brutal. So people optimize, as you know wherever there is a problem there has to be a some sort of a solution worked out so people don't do nearest neighbors they do approximate nearest neighbors approx nearest neighbors and as we go into the technical aspect we'll go through a couple of lab exercises at some point. For example, we'll use dim sum and so on and so forth. You can use those techniques to do that. Then there is similarly, you could go the other way around, take an item. Let's say that some item, tilde, you can take. And for this item, what you can do is you'll get a lot of users right who have bought those items user one user two user and and now what can you do well there are many many things you can do but amongst the things that you can do is you can find users you can find those users which rated your item highly right which are exactly the kind of users you're searching for. And again, do a KNN search. You can do a KNN search. So by the way, here, what do you do? Once you find in the item space, the nearest neighbor items, you can pick those. Those are your recommendations. Those are your item recommendations to the user. And now for items, it gets a little bit more interesting. You find this users who are most like likely to, you know, who are most similar to your well, the users who liked your item. Once you find those items, now you have to ask the reverse question. What between those users, sorry, once you find those users, you find what other items is popular amongst those users you know amongst your uh favorite users what are your what other items for this item are popular there and then you again can recommend those items so it's a more indirect way now all of these ideas are based on fundamentally two things, three basic principles, similarity measure, you need a similarity measure, which you have to be really smart about. You need an approximate, approximate because it's computationally brutal. You need approximate KNN. And the last thing after that game is simple. You have to pick top end for recommendation these are again just to refresh it these are model based collaborative filtering and today colla colla we are doing stiff filtering methods and then finally it is 2 36 now uh should we do that then i'll just give you a brief idea guys of what we are going to talk about the next time then comes this famous algorithm the matrix factorization it is think of it as a granddaddy of our the more recent methods that we use which are much more deep neural networks and based methods for collaborative filtering. But it's a granddaddy of it. What it basically says is that suppose you have this matrix M, right? You do one of the things which in linear algebra is sort of the crown jewel. It is called a singular value decomposition. Those of you who took my the math behind data science workshop, of course, are familiar with it. Singular value decomposition. What it basically says is that this user item matrix with the dots that you have, imagine these dots are purchase dots. In this very, very sparse matrix, the dots are what they are. There is a causation. You can sort of hypothesize a generative model or a sort of a reason why it is like this and not like something else, right? Why the dots are not somewhere else, right? In other words, to hypothesize that these are not random dots, but there is a systematic, there's a reason or a dynamics behind it. What you do is you hypothesize a lower dimensional, and that's why you find the word low rank matrix. There's a lower dimensional, let me call it a latent square matrix. matrix right there is a lower dimensional matrix or such that it is and so I'll explain what it means by that it means this that let's take a take L of dimension I'll just take an example typically these are 200 dimension for Netflix 2 to 300 dimension and so forth but just for the sake of argument and the way this this original paper beautifully explains it explains it in very simple terms it says that imagine that this is your um two factors two latent factors not 200 just two factors and i i believe they use things like let me just say originally i read this paper i'm looking at this paper after almost 10 years actually, what particular thing they use, I don't have my reading glasses before me. Serious versus escapist and geared towards male versus geared towards female. Okay, let's go with this. This is a male male liked, and this is female liked. I don't know, so many years later in this world of diversity and inclusiveness, whether this example would be liked anymore, but it's the original example, right? And this is serious. And this is escapist or hilarious. Right? So let's try to cook up some example. Actually, this guy, this paper, make some examples. Let me see what examples he has. Again, I don't have my reading glasses. So let's just stretch it out. Ah, it says a movie called Braveheart. Braveheart is both serious and male dominated. And Escapist, somewhat escapist, is a movie like The Princess Diaries. Has anybody seen Princess Diaries? I'll ask my daughter what daughters what they think of it i hope this is correct then here would be sense and sensibility which is jane austen say ability ability and then a dumb and there's a movie here a dumb and d, which I remember seeing, which was pretty good. And Dumber, Independence Day, so forth. So in other words, guys, when you look at a scale of male versus female, and a serious versus just escapist, I suppose a documentary would be somewhere here like that documentary, right? And I don't know. that documentary. Right? And I don't know. Recently, I noticed that some kids from my neighborhood gathered together and they were watching an Indian movie called Dhoom. Would you agree that it is somewhere here? It is sort of an unrealistic, fun-loving movie. Well, anyway, so this is it. Or maybe it is somewhere here. unrealistic, fun-loving movie. Well, anyway, so this is it, or maybe it is somewhere here, whatever, somewhere it is. So you can place, you would agree, all the movies at some place in this page, in this coordinate system. Now, there are only two coordinates we put, but they are actually, imagine that there are lots more coordinates. So in this space, this latent space, or more practically on this writing board at this moment, two-dimensional writing board, every movie can be projected down because it will have some degree of bias towards male or female liking and a certain degree of seriousness versus escapism on that spectrum. It will fall somewhere. So you can put a point for a movie. Now what can you do for a user? So when you put a point for a movie, you see that the movies have those traits. This movie, Dhoom, has a trait of, let's say, 70% maleness. I'm just picking it up. I have no idea how much it has. Or the princess diary has a very high female likeness factor and then it is like, I don't know, the Braveheart is supposed to be a very serious movie, a high seriousness factor and high male-like factor. So you can give these two values to these factors and therefore coordinates, you get the coordinates. So items have traits. But what do human beings have? What do users have? They have certain affinity to the trait. By the way, I tend to use a different example, which to me, I suppose, is more intuitive. I like to cook. I like to enjoy cooking. So when I look into the kitchen, especially the Indian kitchen, you find a lot of spices, right? You find these ingredients and the spices. So let me just take spices. How much of chili you put into the food and how much of whether it is trending towards sugar or towards salt or how much sugar you would put. Think of these two axis. Now, if you were to imagine that, suppose you have 10 spices, right? Then, or 10 ingredients, like white spices, take all the ingredients, also rice and lentils and so forth, 10 ingredients, and you make lots and lots of dishes. There's an infinite combination of dishes you can make from the 10 ingredients by doing different combinations of these ingredients. And so you cook up a cuisine out of that. Now every user will have a certain specific affinity for that cuisine. And whether the user will like that cuisine or not, at the end of the day, you can sort of hypothesize that you can decompose that like based on their liking for certain traits their liking for sweet versus a spicy versus things like that you know uh you can do that you can think in terms of that and because you can think in terms of that you can say that cuisines have traits and users have certain liking for this underlying invisible traits like how sweet it is, or how you know spice it is and. Vipul Khosla, Ph.D.: So on and so forth, and so things like that that is the basic idea, so in the case of movies, you can project that each movie has a certain a user, let us say that this user is there the user likes silly movies like you know when you want to have a relaxation you want to just get a few laps um cool down and go off to bed right let's say so you are here and so you would gravitate you would agree to movies in your vicinity right and that's the fundamental idea of the recommender of the singular value decomposition is the mathematical technique from linear algebra that basically helps you do this you have a latent space of l dimensions and what it does and the typical dimensions is 200 and so on and so forth and what you what you do is you represent each user each user as a row matrix of l dimensions of l dimension each of those values here how much so suppose this is a dimension i a dimension k kth kth dimension it basically says how much does the user i have a liking for the kth trait are we together kth trait here how much does the user like this trait right likewise every item j can also be represented by exactly L dimensional, how much of traits does it have? So suppose you have the key. How much of this, for example, if it stands for spiciness, how spicy is this cuisine will be measured by this value here, this number, the scalar here. Right. And therefore, what you can say is that roughly that it is the inner product of this. When I multiply this with this or in more mathematical languages, when I do XI, YJ, inner product, dot product. When I do this dot product, I will then know how much it should give me the rating. Now, let's think about rating. If it is explicit rating, of course, you get it on a scale of one to five. But when it is implicit, you know that the user either has bought it or not bought it. But we don't do that. We give it a probability from zero to one, right? Probability from zero to one, which is again a number so it is okay rating is just the probability implicit probability of buying buying streaming reading etc so we do this and now having done this how do you so now comes the machine learning part we know that we are going to do this this is a linear algebra part how would we reduce it to machine learning it's actually very straightforward we are taking the dot product we know that if i do transpose you know because these are this is a row vector this is a row vector you have to take the transpose to make it a row versus column so you can multiply the two properly in matrix notation so this gives you the dot product of these two now if you're talking of explicit rating this is okay what are you comparing it to you're comparing it to the data to the actual visible or the evidence the evidence is you have some for some item i and j, user i has bought item j and put a rating, or just bought it. If it is just explicit, then it is okay. You compare these two, this will be number, this side will be number, and this is of course, this is number, and this will of course produce a number. On the other hand, if what you're looking for is between zero and one, like you want to make it into either the user buys or not, so the values are 0 or 1, these two. So then of course what do you do? Now we remember in classification theory, how do we convert this into a classification problem? How do we do it? For example, in logistic, so this is your logistic regression expression, right? your logistic regression expression right uh minus z so we we just apply that or let me just put a sigmoid function there because this is literally what it is you do a sigmoid and you're comparing these two and what are you doing this therefore square of this and obviously with the appropriate summation over i j this is your loss function right this is your sums mse the sum squared loss are we together this is your sum squared loss part to this loss you must add the standard regularization term and i will just mention it to you without further ado this is your sigma ij xi square plus this is actually minus typically you do minus because you want this to compete with this regularization yj i hope i'm getting the math right i'm just doing it from memory at this moment but in case i've made a mistake i'll tell you if a correction is required. OK. So this is it. You just go and add the standard regularization. Guys, you must be remembering this is your standard. What is this? This is your lasso. This is not your lasso. Sorry. This is your ridge regularization. If you remember the regularization theory, this is your rich regularization. So this is your basic loss function. Now, when you solve this loss function, this loss function, you can solve it by whatever means necessary, right? So long as you get. So in other words, your algorithm is to do, so because this is your total loss, you want to find that value of XI, argmin, YJ. That will do the argmin of this complete expression. Isn't it? The sum squared loss with regularization. Now this from rich regression should be extremely familiar to you. The only difference that we have is we have derived it out of the interaction matrix. So the moment you have this insight and it's amazing that, you know, this insight took a long time to develop. And the people who developed this insight literally were given the million dollar Netflix prize. And this moment was considered a seminal movement actually in the recommender system. Properly we say that it really took off from this mathematical expression and from this particular breakthrough. So the way you do that is it's a tough problem to solve. So what you do is you do it in two phases. First, what you do is you keep the wise's value constant. So when you do the gradient descent, and I hope you guys remember the gradient descent learning step. What is the gradient descent learning step? The value next, right? So theta. Next is the current value of theta, the better, minus alpha gradient of the loss, right? Do you remember this expression guys? This is your fundamental loss function. So obviously you don't do loss just like that. You will of course do mini batch loss, mini batch, right? You may do a full gradient descent. You may do stochastic gradient descent. So for example, if you look at Spark on a big data scale, it takes the approach basically of doing stochastic gradient descent. So I won't go into the trade off some mini-batch stochastic and full-batch gradient descent. Those are things that we have covered in extensive detail in the past, but you do the gradient descent. So what you do with the gradient descent is you do it in alternating cycles. First, keep Y is fixed, right? And update only the update Yj is fixed, update Xi to better value. Do a gradient descent on the x i the next step first next keep x i fixed update y j to better values right and then what happens is that you if you just look at this expression it pretty much degenerates to the regular, regularized regression that you're familiar with. And you keep alternating between these two, alternate. Once one cycle, you update the XIs keeping Y fixed. The second cycle, you update the Ys keeping XI fixed. So gradually the taste, you know, the values begin to get better and better the traits begin to fall in place of an item correctly and the user's taste begins to be more and more realistic that is why this algorithm famously was called alternating least squares. And for example, one of the popular frameworks for doing in big data space is Spark, right? So Spark's first implementation, the moment they got their stochastic gradient descent going in place, they implemented this Netflix prize in the alternating least square methods. And even today that is the dominant approach for matrix factorization based recommendation system in Spark and very effective. It works, it gives you a pretty good result and that's it. So today I'll end with that guys. Next time we go into, we'll keep going deeper. I'll give you a preview of what we'll do. We'll do various approaches using deep neural network. The deep neural network work is rich and vast. You have the traditional neural networks, feed forwards. You have, then we'll do it with autoencoders, with generative adversarial networks. Then we'll do graph neural networks. You know, how to create a graph of this, because users and items, you know, how to create a graph of this, you because users and items, you know, there, you have edges between them, and you can do graph neural networks, and then you can do sequence models, time evolution models. And, and then you can do some of the more cutting edge research geometric, for example, you can embed them into some sort of interesting manifolds, interesting geometries, and then try to predict things. It's a pretty long journey. Let's see how many sessions we feel like doing or how many sessions you folks are interested in. But anyway, thank you for coming to this session and I hope you all had fun. I'll open it up to questions. Any questions questions guys? Are you still here? It's dead quiet, so I can't make out. No, we're here, Rasif. Okay. Do you have a list of papers that you've planned to kind of share so that we browse through it for some context? That's right, yeah, we will. In fact, the reading review for today is this paper, the original paper and i'll post it to the slack channel right away okay you can read it and gradually i'll take time unfortunately i'm very very busy with my day job i didn't get time but i will take time out to make a reading list and they are all there on my shelf i just have to start pulling the paper printouts and then finding the equivalent electronic links and giving it to you. As we do topic by topic, I'll give you all the seminal papers on the various topics. If we're supposed to go through all the topics thoroughly, how many sessions do you think they'll take? See, it's an interesting experiment I'm doing. Of course, this is completely free and so on and so forth. We are not doing it at support vectors and we are just doing it like this. I am taking the lab is out of it. So in this Sunday sessions, of course, we can't do hands-on labs. So with the hands-on labs vanishing, it becomes an experiment I'm doing for the first time i would imagine that it will take about six weeks okay uh by the way did you see that uh paper by uh michael brownstein which one did you post it to my Slack? Yeah, I think they did like a survey of geometric deep learning. Oh, that one. That's like 150 pages. I looked at it. Yes, I looked at it. See, geometric deep learning. No, actually, I've forgotten about the paper, but I remember looking at it. See, Geometric Deep Learning is pretty close to me because my PhD, literally my PhD buddy, office buddy, is one of the deep experts in it. And we are at this moment collaborating on a bit of research in Geometric Deep Learning. So obviously for me it is particularly fun because we are bringing in ideas from, you know, you guys know right, but after my IIT and You know, you guys know right but after my IIT and and and before my computer science here in US, I took a diversion doing PhD in theoretical physics, mathematical physics. So we're trying to bring out ideas and ideas from topology, you know, and differential geometry and so forth into deep learning. So for me, one of the more fascinating things is how those more mathematical disciplines are beginning to make their way and enrich computer science in fact computer science a lot of the ai is nothing but those fields apply to computer science and you can see most of the terminology and things comes from that those fields by the way um i think you might want to stop recording. Oh yeah, let me start recording. That's true. I don't want these things on the screen. Yeah, like, are you in? I'll also remove this from the session. from the what session? From the recording, right? I'll have the QA session chopped off. Go ahead. Are you interested in like going over that sort of paper? Send it to me again. I can do that, but see, let me see. We are covering the topics in great detail, a little bit more detail than surveys surveys do right the point is to actually go through the math you know this this is basic math but now we want to go a little bit deeper into each of the topics and cover it so i don't know whether the paper covers it in the same detail if it does we'll do it uh it's 150 pages oh it's 150 okay remind me about the paper again and put it on my slide. Yeah. This topic, as I mentioned to you, it is quite a bit in my mind these days. It's a cutting edge. Right? Yeah. Yeah, I think Michael Brownstein is sort of one of the bigger guys in this so I have heard his name come up quite a bit in papers but I don't remember the specific survey paper remind me about it again okay all right guys any other feedback did you guys have fun by the way was it useful yeah it does it is useful i see um but this is introduction so this is very introductory yes this is just the basics you know, like, like, everything is interesting. But, you know, like, for me, it's like, market value, job value, you know. Can you spend like a minute talking about that? You know, talking about that, you know. See, recommenders are a mature topic. When you ask from a job perspective, I would consider it a necessary condition for any good job in AI. Like, for example, if you came to interview with me, I would certainly ask you questions in recommenders and check if you understand the deep learning based recommenders and so forth right it's a basic thing it's a baseline it's like you know you're hiring an engineer you want to make sure the guy knows calculus right if you're doing mechanical engineering or something like that or a physicist in the same way uh that's how it goes you know you need to have you need to know this now whether or not being if you are an expert at it you have an instant job your situation is different you are a starter the question is what is the minimum knowledge needed to get a job get to it and i have answered that question for you dennis you know what it is you you know what ai learning more ai is not going to help you you need to learn what you come from a non-engineering background so you need to focus on the basics of engineering like you know the google cloud and so yeah i'm doing that easy stuff that you don't know you know all the hard stuff so yeah it's one of the easy stuff, I'm planning to do the data engineer pretty soon. And I think the machine learning engineer thing, like it's not that different. So. Yeah. Yeah. So the engineering is engineering, but the AI is different. We call AI as a part of computer science, but as you know, AI has pretty much, it's a deep mathematical field. Frankly, for historic reasons, it's considered part of computer science. It doesn't have much to do with computer science. So you need to pick up the traditional computer science in your case, if you were to, so from the right answer for you is, should you go invest in recommenders at this moment? No, go and invest in picking up the easy stuff in which you haven't put in enough effort. Yeah, so certifications and lead code on the side. Yeah, yeah, yeah. I mean, silly as it looks, you need to do that. You need to get your foot in the door. Once you get a foot in the door, I have no doubt that you will do very well because you have mastered AI quite well. Okay. Alright guys, any other questions before I Is Patrick still here. Patrick. Yes. Yeah, we saw the single tag on right It was like this competition like seeing on Kaggle, right? It was like this competition, like. Okay guys, you guys can talk and I'll step off. Anybody else? Anybody else has a thing? Oh, by the way, I also noticed there's a YouTube recording on, I should stop it. Anybody watching?