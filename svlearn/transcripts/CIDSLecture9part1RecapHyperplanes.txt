 We will start with a review of neighborhood methods. So the basic idea of neighborhood is you are defined by the company you keep in a very simple terms. So if you want to know what is the value for regression, what is the value at a point in feature space, you just need to find neighboring points, actual data instances, which are in the neighborhood of that point. And in a sense, in the simplest term, take the average of them, right? When you take the average of them, that is called a K nearest neighbors. You're picking K of the neighbors. Now, how many neighbors you pick depends, determines the bias variance trade off, which we'll see in a subsequent lab, not today. In the bias variance trade-off, what it means is if you take k is equal to 1 and you're only listening to your nearest neighbor, then what happens is if so happens that the nearest neighbor has a huge error or it has noise because of whatever reason it happens to be on the wrong side, you will end up misclassifying your points much more. So the decision boundary is far more complex. On the other hand, so as you increase K, the decision boundary begins to smoothen out, as you will see. And beyond a certain point of smoothing out, now you start developing variance errors. In other words, certain point of smoothing out now you start developing variance errors in other words and not bias errors because what happens is for example in the limit of taking all points the k is equal to the number of data instances there will always be one answer why will in regression be just the it is essentially the null hypothesis it is the average of all the y values in the dataset. There's no relationship between the x and y, the predictors and the y target variable. Likewise in classification, whichever is the majority class, that class will overwhelm and all of your predictions will say that every point in the feature space belongs to that majority class or the dominant class. So that represents essentially you go to the limit of, in some sense, a null hypothesis. Right. So as if you're not sharing the screen. Excuse me. I'm sorry. Let me do that. Thank you for mentioning that. I am certainly getting absent-minded. All right. Is it any better? Yes. Right. So the two approaches, nearest neighbor, when you apply to regression, you just pick a few points around it and then you take its average. For classification, again, you take a few points, K points around it and look at the majority. In both these situations, you realize that if you just look at K is equal to one, you will have high variance errors because if i had taken a different sample of data to learn from the answer may have changed by uh by happen chance you might have data points with uh with more noise in it and so your prediction would be off so a basic smoothing method is to take sufficient number of points. Now, what is sufficient? What is good enough or a good number of points? That is the K, the hyperparameter in the model. You have to play around with it. By now, you must be realizing that machine learning, most of the algorithms are infested with hyperparameters. You have to not only train the parameters of the model if you have parameters, but you also have to then worry about the hyper parameters later on. In the neighborhood methods, generally there isn't much training to do. It's a form of instance-based learning in which you just hold the instances of data in memory and then when you have to make a prediction for a point in the feature space, you just find the k nearest neighbors and you use that to make decision. So k is equal to one represents in the bias variance trade off represents a situation of high variance because it changes, it varies a lot which sample points you happen to have taken. On the other hand, when a k is very high, for example, asymptotically, when it is the number of data points itself, in the case of regression, it essentially amounts to the null hypothesis. Basically, you're saying the prediction at every point will be the average of all by values, right? Because k is n n all points are the neighbors uh which doesn't help at all and the for the class of regret for classification the same thing holds i the bigger the k the more biased it becomes the decision boundary becomes overtly simplistic right uh so that is that's a biased way to straight up. One question, this does not apply for high dimensions, correct? Yeah, so we'll come to that. Let me just leave you in a moment. So now, the second question is, so you can say, well, that is bad. How do I mitigate the effects of the hyperparameter K, the change in the hyper parameter k you know just hunting for the best hyper parameter is a bit of a nuisance uh you have to run it again and again if there were some method that we need not have done that we could have arbitrarily picked some k and more or less the clustering i mean not the clustering the regression and classification would be robust against small changes in the value of K. That would be good. And the intuition for that comes from the fact that if you find K neighbors, you would like to give a stronger voice or a stronger vote to the nearest neighbor, a lesser vote to the next nearest neighbor, and a lesser vote to the next nearest neighbor. So in other words, you want to take a weighted average not not just a plain average in which a far off point has in the neighborhood it still has as much voice or as much vote as the absolute nearest point uh to the to the point in the feature space you're looking at the point. So the way to do that is to, well, of course, apply weights. Those weights that you apply are called kernels because to the kernel, we want to associate some well-defined properties. The kernels, these are called the distance kernels. Now, what are distance kernels? Distance kernels basically say that in a way, how much a point influences the decision at a point. So suppose I have a point X and I have points A, B, C, D around it. How much A influences the decision at X should depend upon, should I draw it out perhaps? What we are saying is, suppose you have a point X. This is the point you want to make a prediction on. And let us say that you have points XA, XB, XC. Are you able to see my writing? Yes. Right? XC. And more than that, you have XD. And maybe I'll take X b a little bit further away right so given this point you realize that you can order it the distance to x a x is less than distance to less than distance to x c x and which is less than actually from this picture, if you look at it, it is less than i'll draw these things out this distance is longer than distance of. X, B to X, would you all agree? Which is less than the distance of X, D to X, right? So the order of distances is A, C, B, D from this picture. Let's look at this distances. Would you agree? Yes. Right, and so what you want to do is you want to have how much influence they have on the vote to be proportional to how far they have, inversely proportional in some sense. What you want is further points have less influence. So to capture that notion of influence, you need a function whose basic quality should be that in the very minimum, the influence shouldn't increase with distance. You would agree with that, isn't it? So you say that you need a function that is monotonically decreasing and continuous. Continuity is obvious. I mean, if you don't have continuity, then you have zones in which you simply can't find the what influence points will have if they were there. So obviously, you do want a continuous function. So continuous is obvious, I hope decreasing makes sense. And monotonically means it has to slowly decrease, it cannot just suddenly go up and do like that, because there is no rhyme or reason reason it doesn't agree with our intuition of what we are trying to say that the further you are the less influence you have right so what are valid kernels well a flat line a flat amount of influence is also invariant of distance is also a valid kernel why it is decreasing not at all but it's certainly not increasing a step function says that all points within a certain radius you consider to be of the same influence but beyond that you don't do it so then you're essentially shining a torch light around that point isn't it so suppose you do this let's look at this blue line. What is this blue line doing? It is creating a circle around it. So, this, the blue lines kernel would be a step function, isn't it? Which is the radius. Let's say the radius here. When you go at a distance d is equal to the radius. What happens? You don't take any points beyond that so therefore the influence is zero outside it it's a step function it's a step function right if you haven't encountered step functions before just literally quite think of it as a step into your house that's what it looks like outside inside uh then it could be a linearly decaying function the yellow line it could be the pink line or it could be the light blue line it could be anything basically so long as it fulfills the condition that's a monitoring tonically decreasing continuous function so far so guys. And so what you do is that you take the values, the weighted average for regression and the weighted vote for classification. And these things, and today in a way we got introduced to this lovely notion of kernels, distance kernels. It turns out that distance kernels are a special class of kernels. Today as we make progress we will learn about kernel and kernel machines in a much more in-depth manner. In fact we may not be able to finish the whole kernel stuff. Today I'll introduce you just a touch of the main kernel machines theory, because it gets quite mathematical. It's a beautiful, beautiful mathematics, gets elegant, but it's usually not something I would cover in an introductory course. Usually the mathematical details we keep for the math of data science. That's where we do that. But I'll introduce you enough to this theory of kernels. So that is that. Then what are you dividing? What's the denominator? Sigma k? The sum of weights. That's what you would do, right? The sum of weights. Just to normalize it. So suppose you want to take the weighted average. Let's say that you want to weigh something by 2 and 1. 2x1 plus x2. So what should your denominator be? 2 plus 3, right? Because what you want to say is 2 third x1. 2 third contribution should come from x1. 1 third contribution should come from x2. Sorry, 2 plus 1. I take that back. 2 plus 1 is equal to 3. This is how weighted averages work, isn't it? Proportional. So that's a constant, though, right? That is a constant. That will be a constant. Yeah, yeah. It will be like the number of points that we have taken. No, not number of points. Remember, it's the kernels. Correct. That's why I'm kind of... See. Okay. Let's think about it. Given a point, x vector, let's say that around it are the points x1 all the way to xk. Right. Right. So now at each of these points, you have xki. So suppose you're trying to find the y value. So what will you do? At this point, you say that this guy, x1 is saying that the y should be yi, because that's what's the value of yi there. The second guy, so each of these guys is saying this right so now what happens some so total in now this k that is there suppose it was okay for for example let me let's make it simpler suppose there were only two points two nearest neighbors right this is x1 this is x2 so what will you do, you would say, and let's say that k x1, x and plus k x2 x. And this is, of course, y1. And this is y2, you do, but you realize that you cannot just add them up. Because suppose each of these are, you need to scale it down, you need to take proportional weight. So what you have to do is, so suppose this was equal to a two and this K is equal to one, right? This is equal to two and this is equal to one. So basically what you're seeing is this guy has twice the influence of this guy. And so the total influence that the collective they have is three. So what you're really saying is that you need to divide it by 3 which is equal to 2 plus 1 isn't it what have you done you have just said this is equal to summation of x xi k xi sorry a summation of k of xi that is what you just did. So this will vary depending on the neighborhood points. K points. Yeah, K points. That's what I'm trying to understand. It's not like a constant you can think of. Wherever you are, there is going to change. It's going to change, definitely. Definitely. Both the numerator and the denominator will change. That is right. And what it gives you is a more robust estimate at that point. It just agrees with common sense that nearer points should have a stronger influence, a stronger voice, isn't it? So that is the notion of kernel. That's what we did. The second thing that we did, which was a slightly longer journey, one of you asked me to explain spectral clustering, which was a long intellectual journey. So we went into our digression and talked about eigenvalue decomposition, which I won't repeat today because we are going to do it when we do principal component analysis in detail, but basically eigenvalue, just as numbers can be broken up into primes, matrices can be broken up into components. And there are many interesting decompositions of matrices. One of the very powerful decompositions is eigenvalue decomposition. And what it does is, if you take a matrix and apply it to a matrix which is like this, symmetric matrix like this, then you apply with certain conditions, if the matrix is in some sense well behaved, then it will transform a circle of points into an ellipse of points and in higher dimensions a sphere of points hypersphere of points into hyper ellipsoid and those ellipsoids will have major and minor axes and all sorts of different axes in them and those axes are the eigen direction of those axes are the eigenvectors and the degree to which they are stretched out are the eigenvalues vectors and the degree to which they are stretched out are the eigenvalues. Now the way this spectral clustering worked is you look at the k nearest neighbors, for example, that's one approach, and then you create an adjacency matrix, then you take a degree matrix, we have two different things, these are all coming from graph theory and we'll do it in greater detail uh someday uh then it turns out that degree minus a right uh gives you something called the laplacian matrix laplacian matrix has a lot of very nice qualities it's a good it's sort of a key concept in graph theory now given the laplacian matrix, if you do the eigenvalue decomposition, and it will project it into a space where the eigenvalues have an exponentially decaying set of values, and you can throw away some of the directions where the eigenvalue are small. And so what you're seeing is those directions don't matter. And you can therefore only look at those, look at the subspace made up of eigenvectors, the first few eigenvectors. So you have taken data and projected it into a lower dimensional space. Whether it is, whatever I'm saying, if it doesn't resonate with you at this moment, hold on, because we'll do dimensional reduction in great detail. But once you have done that, the long intellectual journey is you take those vectors and then into that vector, you project every point. And once you have projected the point, you can then apply your K-means clustering or whatever favorite clustering algorithm that you want to apply. k-means clustering or whatever favorite clustering algorithm that you want to apply. That is the whole story of the spectral clustering. So in simple terms, it differs. By the way, people ask how is it different from principal component. In principal component analysis, you factor the covariance matrix. Here you factor the Laplacian matrix, which is a core entity in graph theory. So that's the difference. Anybody here with a background in electronics and communication? So you should be remembering your spectral theorem. And so pretty much these things are all in many ways related, but we won't get there. So today we are going to start now with a completely new topic. I'll give a five minutes break, then we'll start with support vector machines. Kayal, I will record you. Now we are going to start on a new topic. This topic is called kernel methods and associated name with it is support vector machines which when it came out in the uh in the early 90s it was supposed to be quite a revolution and i have heard some quite quite a few stories about how it all happened i don't know how many of those are true and how many are not but well the breakthrough apparently was made by a brilliant brilliant person vladimir vatnik who made this breakthrough in the 1960s it was way ahead of its time when he submitted it to the math department or the math journals they all rejected it apparently then he tried neuroscience journal, and apparently from what I understand, even there it didn't get published. So it had to wait for many, many years, 30 years before in US there was some sort of a competition. I think it was the handwriting recognition competition or something like that. And where he submitted or his friend encouraged him to submit the results of his algorithm. And it was remarkably better than anything done so far. And obviously today, Vladimir Vapnik is a distinguished professor at MIT and has been there for a very long time. And I heard this story, so this is part of giving you an insight into how things are done in reality. When we look at some work in books today, when we look at kernel machines, kernel methods, it is so mathematically elegant and so absolutely powerful it feels. But things start slowly and Patrick Winston, an MIT professor who used to teach machine learning there for many years, he has passed away. He told the story in his lectures that Vladimir Vapnik discovered it over what he said, many cups of tea in a cold Russian winter. Apparently he had gone to, if I'm right, to a dacha or someplace in a remote area of Russia, cold, quite isolated. And then every day he would slowly build up the thought process. Because to get this thinking right, and I will take you on the same intellectual journey. And it's a breathtaking journey that takes us from a simple idea to the support vector machines. So do please pay attention. I would strongly encourage you in case you are keeping an eye on your laptop for messages from work or from social side. Today, I would request you because this is a slightly tough topic to please not do that. Give your undivided attention to this topic. It's very important that you do that. So with all of those statements of background, let me start. We will start with the distance function. If you remember, we talked about something called the distance function. Do you remember that, guys guys the distance function was distance from a decision boundary let's say that there is a decision or between at this moment not even a decision boundary between a hyperplane we're not sharing oh I stopped sharing okay I apologize no I am sharing why is this not showing up is nobody able to see my screen no i can see it as a speaker view or gallery or something yes so let's take this situation you have uh let me yes suppose you have a hyperplane like this hyperplane hyperplane we realize that the definition of the hyperplane in the way that we studied it was that any point the distance from the hyperplane of a point x is given as beta naught plus dot x vector do you remember this guys as a dot product between these two now these dots may sort of get i will introduce a notation that i like beta hat x so when i write this notation bracket bracket notation, I mean exactly this, dot product. It's a synonym. I tend to write it like this, this sort of angled bracket notation, because when I'm studying my own code or my own notes, scribbled handwriting writing it is much more clear to me that it is so if this is the distance to the hyperplane you would agree that the hyperplane is given by dx is equal to zero would be the set of all points points on the hyperplane, isn't it? Because points whose distance is zero are points on the hyperplane. So far, am I making a simple statement? Sanjeev, are we clear with this? And in fact, the definition of a hyperplane is the set of all points that fulfill this criteria. So we say that another way that we can define a hyperplane and just putting it very simply, a hyperplane is an affine hypersurface hypersurface affine hypersurface given by beta naught which is dx beta x is equal to zero right now what is the word if I mean now if i mean it doesn't necessarily go through the origin there's no reason that this there's no constraint that this hyper surface or this hyperplane goes through the origin are we together right a decision boundary remember your grapes and blueberries are blueberries in terms of weight and the cherries. So the decision boundary, if you remember, is this. Does this go through the origin? No. So this is your hyper, this is a hyperplane of what degree? Degree one, isn't it? So suppose your feature space, so some very basic thing, if the feature space is equal to r to the p, you know p-dimensional, the p features, so your feature space will be approximately r to the p, right? I mean, forgetting about categorical variables for simplicity for a moment, right? So they're all measurable so would you agree with that the cartesian space r p in other words suppose you have two features like in this case weight size would you agree that the feature space is r2 plane right real times real now generalize it to p features it is rp and then the the thing is what would be the dimensionality of the line in in two dimensional in r2 what is the dimensionality of the of the hyperplane or the decision boundary? R1 or just R. Isn't it? One dimension less. Would you all agree? N minus 1, yes. Yes, N minus. So in the same way, if the feature space is then the hyperplane, a plane plane is dimensionality is is is equal to r e minus one one one less in three-dimensional space it will be a plane like as a sheet of paper right and we can generalize to higher dimensions so this is the only mathematics at this moment I want you to remember. And just be familiar with my bracket notation. This, whenever I write this, I mean nothing but this is by definition. Now, why do I put a hat over beta? It's a unit vector, right? It's a unit vector, right? It's a unit vector. These are unit vectors, unit vector. And in which direction is this unit vector going? This unit vector is in a direction orthogonal. This is the unit vector, orthogonal to the plane or the line or the hyperplane, right? So it is the orthonormal vector. So we say a unit vector is orthonormal to the plane. Beta hat, so we'll just, this is all a review of what we have done, is an orthonormal vector to the hyperplane. And so you realize, so this is the way that you posit this thing to be.