 In the moment by my watch, on the Pacific coast, it is 138, but let's make it generously 140. Would it be reasonable to say that we regroup at 2025, no, 225. All right, guys. So let's regroup here exactly at 225. And we mean it, guys, because we have a long territory to cover. Have a quick lunch and let's make progress after that. To recap, till now we have done bar charts, or we have done scatter plots, each of these and we have done line plots. And we started with one liners in each of them. And we ended up with a sort of a milestone or a landmark visualization that we could reproduce. In the case of bar plot, we did the two roses of Florence Nightingale. In the case of scatter plot, we did the gapminder's famous visualization on the wealth of nations and so forth. And when it came to the line plot, we ultimately ended up with the famous stripe plot, which is used for, which has become iconic for climate change and the climate change conference global climate change conference so in the same spirit now let us move forward but before i move forward to other big visualizations i would like to take a pause i gave a talk this uh in during the week on kernel density estim. Now, the audience was very small and there was a request that this is important enough that I repeat it for everyone. So this is a repetition now for everyone. I'm going to bring up that visualization. If you have been through this, if you were in that midweek session, then this could be a refresher or this could be a time to go get a half an hour nap. So with further ado, without further ado, let's get started. I hope, are you able to see my screen or have I not shared my screen yet? Not shared. Okay. So let me give you the big picture here. See, suppose you look at data. Let's look at one dimension, weight of ducks or something like that, weight of birds. You realize that when you look at a continuous variable, you can have any value possible within these. For example, the height of any two people could be off by millimeters or even low. So you have a continuum of value for all practical purposes. You do have a range. For example, you don't tend to find 10 foot people. You don't find tend to find a one inch people, right? You don't have tamalinas and you don't have those giants that Guleva Travel speaks of. But in between, you have a pretty large variety of heights and weights of people. Then, if you had to guess, how is the weight distributed? It gives you some intuition. What is the, like, how likely is certain value to be present? For example, you would say that if you're looking at children or in high school, their height and weight distribution would be one way. If you're looking at, let's say's say sumo wrestlers their weight distribution would look entirely different the likely weights are quite different so today we will talk about this topic of probability density probability density is at that particular value how likely how likely is that value compared to other values? Normalize to one. It is integration of a density function. That's a basic intuition. So we will develop that intuition today through a notebook. It is one of the visualizations that people miss. So the main thesis that we are going to make today is that histograms are good, but histograms are flawed they are sort of a basic way of understanding the likelihoods of things and they are better estimators from data we can infer or build better models of how the data is distributed what the probability density distribution is so to motivate that what i would like to do is take this particular example. In this example, look at this picture here. I just took a picture from the web. This is from the Unsplash website, credited to the photographer, Jonathan Bennett. And I felt this is, this could be the pond behind your house. This could be, I don't know, is this Lake Elizabeth like that? Probably. Right? What we have in our neighborhood, Lake Elizabeth, a scene from Lake Elizabeth, anything is, it is pretty humic, it is seen as. Whenever you see local water bodies, you often find ducks in there, you find geese in there. So look at this picture and if you say, what can I infer about the populations of birds and geese? How likely am I to find them in the water? What would you do? You would say, all right, let's do one thing. Let's count the ducks, how many ducks there are and if you count it carefully you would probably come to 11 unless my eyes miss something i counted 11 and then if you count the group the the geese you will find that there are four beads like these four geese the geese are big white ducks are tiny little birds smaller birds so you could say that all right Raja Ayyanar?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi? R. Vijay Mohanaraman, Ph.D.: keep running it, and this is the data and this data might as well show it later. R. Vijay Mohanaraman, Ph.D.: There we go so it's it's as simple a data set as we can have looking at this picture I hope you would agree the 11 ducks I I counted for for groups. R. Vijay Mohanaraman, Ph.D.: So for these now zoom little zoom your screen in the picture or the the the code and the code actually the code yeah do you want us to go or type with you or or is this something we just want to discuss right now this one i don't want you to type. This will move a little fast because it is a core topic. I want you to get familiar with it. Let me see, do I want you guys to type? Not this one, but when we go to more interesting examples, then I will expect you to type. So, thank you. This is a motivating example. Just read and understand. I hope when you look at this code, there are no surprises. Right, here it is. So without further things, you know what I would do. I would like to produce a count dot and a bar dot, one bar for ducks, one bar for geese, in which I count the frequency or the count of this thing. Now remember, what we are looking at is the species of the bird and their all the usual cosmetic aesthetic elements that we apply we will apply at the end of it what we can do is we can use the sns bar plot c1 spot i just happen to choose that why did i choose sn c1 for no particular reason because here i want to focus on bringing out the idea i could have used plotly altair any one of these i randomly picked a seaborn and it's quite common bath not actually in seaborn is perhaps the most straightforward let's look at this thing we are used to barcodes so you would agree that this is by now if you have reviewed the jupiter notebooks of last week this is a no-brainer you create a barcode x is count i made it horizontally just in keeping with what we did last week y is species so i get this for this particular barcode right does this make sense guys at the bottom is the count how many there are 11 ducks and they have four geese quite a simple data set and quite a simple uh visualization of it then suppose I ask this thing now now we do modeling we I tell you that the world is filled with many ponds and many and huge number of ducks and geese practically uncountably large number of ducks and geese in the world and there are probably uncountable cows or ponds out there right or ponds or lakes so now if i were to ask you if this data if this picture is represented right if this is representative of what's happening all around the world, and we consider this, in other words, if we consider the birds that we see here in water to be all the ponds of the world, all the geese, all the ducks, and come up with the actual ratio. What is the probability of finding a bird in the water to be either duck or a geese, if you had only two choices? You would like to know the probability of that. So what is the best you can do? You can say, hey, you know what? At this moment, this is the data. If this what at this moment this is the data if this data truly and this is the word we have to remember that data is always a sample from a population always but quite often you don't know the population it's a basic statistical fact the population is for many more situations it is not it's not practical to go and grab the whole population. For example, if I ask you, make a histogram or something like that, a probability that people are a certain height, let's say between five feet and five feet one inch or something like that, you would say, all right, very simple. Let's take seven and a half billion human beings, them up and measure that is the population but that's not very practical so what can you do you can draw random samples to present it itself and that itself is an art how do i mean it's a science of how do you draw good samples so that you don't have biases because if you go and find a draw five little kids your your understanding of the height would be completely wrong if you because there's a bias in the data if you likewise you know if you pick a few a basketball players your understanding of height of human beings be again very biased towards the high values so you have to take representative samples right people often use the word independent and identically distributed you pick a sample it has nothing to do with the previous sample that you pick and it should be identically distributed means it should be randomly truly randomly picked from the population and if you then you try to get so this word iid you find quite often in data science and statistics independent and identically distributed but more broadly and then it's related to the topic of essentially representative data of the population but suppose this is it what can you tell given this distribution how would you make a probabilistic guess I ask you what is it telling you what's the probability of a bird being a duck as opposed to a goose you would say well that's easy I will take this Dutch the 11 ducks out of 15. so I will say 11 out of 15. 11 over 15. would that be a reasonable statement to say yes right so we do that and that becomes makes it into uh that makes so that's what i do in this code nothing no surprises i just divide it by the total it's a simple data set but now when i do that now what happens the x-axis becomes the probability since there are only two things you say that this is uh this probability and this is this probability into those values are and they add up of course to one now people i may say that these are probably lating right now for discrete variables it is okay you say that what i have is a probability mass distribution i'm saying that 75 or whatever percent happens to be ducks and whatever this comes out to be 26, 27 person happened to be geese, right? It's a discrete distribution. That's why people often use the word probability mass functions. I will generalize it now. Now let's go to continuous so i'm creating a data set this data set and this part i would like you to do with let's go and this part you don't need to be able to type it it is just my way of generating a data set which i know about right so let's go and where is it i will post it to or could somebody help me post do you want me to do code posting? Yes, that would be awesome. So I posted the first element guys. Just run this. Remember at the top of your notebook, don't forget to do this. These two things visualization imports, run the visualization import it will automatically bring in the styles and because i didn't feel like cluttering the top of the notebook with lots of imports so do that and then do this right very much like in your previous note once you do this now let's look at the first thing you could do and remember our topic here is to understand the probability density distribution, right, that is the core topic. Why is that valuable? You will see why it is a valuable visualization to have about data. It says a lot about data. So, the earliest work in this space was done by the great statistician Pearson, who was a student of Galton. We're talking about 18 somethings long, long ago, right? A very, very long time ago, almost 200, 150 years ago. He came up with a method, he says, take the histogram and just divide it by the total, which is what we did just now. He said, suppose you have a continuous variable, discretize it, make it into subregions. Suppose your range is from one to 100, discretize it into sub ranges from one to 10, 10 to 20, 20 to 30 and so forth. And you will get 10 different bins, right? Once you have discretized, you have bin. So then you do the binning of the data. You take the data and you ask this data point, this datum, which particular bin this datum falls into, whichever bin it falls into, put it there. Imagine that you're putting a bit of material there. And here is one basic intuition. Imagine you're putting one unit of sand in that particular bin. And think of each bin as a glass column. It's a column of glass into which, or cylinder of glass into which you can drop a one unit of sand so what will happen ranges of bins where lots of values are falling that column will start rising would you agree as more and more data is data data instances you encounter and you start ascribing it to the bins some bins will rise up in height faster than others so that is the counting part then what happens is that you can count all of these and then all you need to do is suppose you divide it by the total if you divide it by the total what will you get you will get the same probability distribution that we are talking about so now to do that this is the code i want you to write you you just became familiar with histogram before lunch remember we talked about density is equal to true it does it for you so the only crucial line of code is histogram data you can even decide how many bins there are here i've arbitrarily decided 100 bits i've said density is equal to true and the rest of it is aesthetics all of these are aesthetic elements let me copy that to. Yes, okay, your voice is coming out very faint. Oh. But yes, if you could, this bit of code, I would rather that we did no, no, no, don't paste this. Folks, can you please type this in quickly, and to your own satisfaction, be able to reproduce this thought? should i wait a little bit uh should I wait a little bit? Could you wait a little bit, please? So do you want me to paste it? No, no, don't. This way I want people to actually do it. By the way, some of these are aesthetic elements. If you're so curious, you can skip it. Like, for example, in the tight layout of 1.05 it is just to add a little bit of white space when you are trying to publish it into a book and some of you know i'm trying to write my own data science book and some of these visualizations quite literally will go in there i won't think can we move forward anyone else who's waiting. We still time. Yes. Okay, so you should get a plot like this, which looks aesthetically pleasing and all that. So we took 100 bins, and we saw this, what can you tell about it. What is it trying to tell you about the data? It's telling you that quite likely there's a peak here and there's a peak of probability here. These are the most probable values. So if you look at it, for example, as the weight of the negative values, so give me a little bit of a license. The negative value so give me a little bit of a license in. waves are not negative but suppose I have two species of birds ducks and geese. Maybe this is the spread of the weight of ducks, this is the spread of weight of the ease I don't know this may be completely unrealistic from a biological perspective in some units would say, and of course, negative values aren't that good. Think of whatever it is, maybe temperature. We are looking at temperature in two different regions of the world activities and mixed up data. So you get something like that. How do we, now, you notice that we said normalized, we probability dens is normalized. Typically, histograms are unnormalized. Most people don't do that. So I just want to reiterate the point that by default histogram, the y-axis is count. It's not probability density. But we can repurpose it for probability density by simply saying density is equal to true. So now here, the y-axis is probability density here as opposed to it. So you can remove that density is equal to true. So now here, the y-axis is probability density here, as opposed to it. So you can remove that density is equal to true, and you will then get another plot. And I won't let you, don't have to spend time doing it. You can take me on trust that this is what you'll get. Now, there's histograms are ubiquitous. All the books on data science, they bring in the histogram quite early on as a good descriptor of univariate statistics. Right? Or one variable. Univariate is one variable. Statistic means a property of that. So mathematical property of the data. Something that says about the data. So now the problem with histograms is it is too sensitive the shape of this is too sensitive to the choice of the number of. How many Ben should we take. There's a bit of scholarship on what is the optimal number of wins, so on and so forth. If you want, you can chase it out. People have talked and have comments and things about it. My own feeling is it's not really worth chasing those directions. Maybe a little bit worth it. Now what I will do is I will take this histogram. And I have made, and this one you don't need to copy it can take me interest what I have done is I made three plots exactly the same code but I have taken it for three different values of its number of bins so here I go I'm making one row three columns isn't it and I'm saying share the y-axis now do you notice that what am i doing with axis one axis two and so forth these are the subplots the first subplot i'm saying make a histogram in which the number of bins right here i didn't even specify i left it as a default Right, I didn't even specify left it as a default. In the second one i'm saying okay number of business 50 and the same code exactly the same, I say now the number of business 500. When you do that. What you notice is you, you see this, do you see that these things look different right in fact because here we have a lot of data we have 17 000 data points you begin to see the emergence of the real pattern your mind is already beginning to see a pattern you know that this rise and fall this very jittery or spikes they're not real your mind, you already start building a smooth curve of a probability density distribution, isn't it? Your mind now, through any one of these curves, you realize that first of all, the probability density doesn't change radically. This, it has to be a smooth function. The weight of one, likelihood of the weight here and just slightly adjacent they can't be vastly different that is unrealist that's some these are the main problems with histogram hey um they are they they are very sensitive to the number of pins which becomes a hyper parameter secondly they're unrealistic because you know that the underlying probability density has to be a smooth curve. Isn't it? It can't radically change with a slight change in the weight of a bird or something like that. It can't become so unlikely. Discrete jumps are not possible. It has to be smooth. If we agree with that, and this line of code, I hope now guys, I won't give it. Let me give it to you so you can play around with it. Or yeah, could you please, could you please paste it for everyone? Sure. Oh, you already did. Thank you. So you do that, repeat it so that in your notebook you have this. And now you ask this question that alright, right, these are the limitations of the hedge term. What can we do? Let's think from first principles. Suppose you found a bird whose weight is, let's say, 5 pounds. What exactly 5.0 pounds? What exactly can you say about the likelihood of getting birds? What is that data trying to tell? Surely it is not saying that all birds are five pups, right? Would you agree? If you just have one data point, you would surely say that no, that's not possible. If nothing else, they are instrumentation errors, they are errors in the measurement process and so forth. So the values are probably clustered around five pounds five pounds isn't it so you would say that it's somewhat five pounds is likely maybe it could be 4.9 maybe it could be 5.1 and maybe less likely it could be four pounds and six pounds and so forth and And you sort of decrease from that particular value. So here is one intuition you can take. Take a bucket of sand, one unit of sand. Are we together? And you say, if I find a value of five pounds, I can go to the x-axis on five and pour some sand in. Now, how I pour sand is the crux of how we do it. There are multiple ways of doing it if you think in terms of the sand analogy. You could have divided the x-axis into discrete bins and then put all the sand in that particular bin, right, which was the histogram way of looking at it. But now let's generalizeize suppose you take your sand and make it into a a square right one square or something like that and wherever you find the data you put it there means it is equally likely to be there and within a neighborhood of it but after that it's zero unlikely to be anywhere near after that now that looks unrealistic of course but it is an approach that you can take it is at least better than histogram winning because you are centering the data on centering your square of sand onto the main point the actual five pounds not putting it say suppose you have a bin that goes from four point a four point two to five point one you're putting sand and it looks as though it's more centered around 4.5 or 4.6 rather than five point exactly five points we have an improvement here make the block sit on top of five that particular way of doing it is called the top hat. And this putting sand and the shape of the sand that you put for each data point, the word people use is it's the kernel, right? It's a kernel function or kernel density estimator. You're seeing that each data point, the narrative it is telling you is that this is how the probability is shaped in the neighborhood or thereabouts or in the neighborhood of that point. Are we all getting the intuition? So now, if you think like that, that is more realistic. And now we have introduced the concept of a kernel, right? So we can say, let me put it now in a slightly more mathematical way. We see that each point Xi, now it says that in the neighborhood, so if you take up any other point X, the likelihood of finding the data there, we can say is depends on the distance from that point in some way, right? Then there is a hyper parameter here we'll talk about it in a moment um it is like and it is called the bandwidth which will introduce it ignore it for a moment this pouring of sand is called the kernel density estimator right it's a fancy word but if you think of a unit of sand a pore there that's what it is now when you pour sand you could pour your sand very peaked right if you are making a hill your hill could be really peaked it could be like a fort like literally square or it could be smoothly rising up and falling down if it is smoothly rising up and falling down you could be generous you could spread it like you could not make it so high but you could make it spread out more or you could sort of collect much of the sand near the point make it much more peaked right go higher but very easily not spread it far, right? You realize that you have a choice. So that is what that bandwidth parameter etches out. It's about how tightly you have packed the sand or how much you're letting it sort of flow in the neighborhood, wider neighborhood, right? Or how much you're keeping it, like how much are you respecting immediate neighborhood versus slightly outward neighborhood versus far. So one could be very generously you spread it far but then at that point the probability just barely peaks a little bit density peaks a little bit so that is it. So now let's create what we will do is we will create this function i call it delta which is the scale distance distance from the point divided by h so one of the most popular ways that you would probably want to put your side if you if you think in your mind i'm pouring sand would you probably pour sand like this symmetrically yes something like that so this this is a pretty good guess for a kernel function by the way at least there no wrong answer, the only thing has to be that Colonel function must be positive, it must. R. Vijay Mohanaraman, Ph.D.: be symmetric around that point and the area under the curve, of course, has to be one because the probability of finding that kind of what that point is saying is that the probability of finding it is there about summing up to one right so you would agree and you can create many many such functions perhaps the most ubiquitous is of course the gaussian curtain those of you who did machine learning courses pass with me are familiar with it how we use gossip curtains they are great so we use this now what do we do we do? Let's take the Gaussian kernel. And this is, by the way, just showing how a Gaussian kernel works. But there are quite a few kernels. I just itemized exponential kernel, cosine, top hat, now, which is basically sine wave. Then there is a tricubic and bicubic and whatnot. Anything goes so long as it has the right properties. And I didn't have the time, otherwise I would have shown you how each of those kernels look. But I think it's there. And now comes the question. Well, your dataset is not made up of one point. It's made up of many points. So every time you see a new data point wherever it is you say oh the values are likely to be here and you pour a bucket of sand another point you go and pour a bucket of sand so soon what will happen you'll have little sand hills all over the place would you agree along the along the axis you'll have created multiple samples are we together in the intuition so far right and so now we do now we ask this question collectively, what is this data set? What is the probability signal? We have a little bit of a problem. All of those individual kernels, their probability densities were integrating to one. So suppose I have D data points, right? Suppose I have a data set D whose cardinality is you know this is static mathematical jargon you put two parallel bars just say how many points are there in that data so suppose there are 100 points when you do the probability of a curve density estimation you want to say add up all of these things so at any given point how much that that so here's a thing so let me put it let me draw it out what we're doing is give me a moment so let's go with the intuition here. Suppose I have this value axis. I come across a data point here. And so what I do is I go and put a little sand hill here. I find another data point somewhere here. And I put a sand hill here. No for sand hills. So let me put a sand hill here. Let's say that I find another data point and put a sand hill here. I find another data point here and I put a sand hill centered around this. And so I keep going on and on and on. Let's say that I find another data point here, and I put a sandhill. I create a sandhill here. Oh, sorry. This is all. This for this, the point would be somewhere here. And suppose, oops, sorry, sandhill here. And you guys are getting the idea. Suppose I find another data point here and so I put a sandal here, right, at this point. And suppose, let me keep making some points here to see what's happening. So let me go and create sandals for each of them. What are these sandals called in the formal language what what are each of these sandals called in mathematical terms kernels isn't it so suppose this is xi so this is a kernel function x xi given some hyperparameters some bandwidth h which the bandwidth h controls the spread of the sun are we together guys so now it's a hyper parameter it's up to you and it's you know in machine learning you never can get away with that parameters right so we'll come to that so now comes this question i ask this thing what are all of these points saying about the probability of finding a value let me say here this is my sample point you know that each of these sand hills and all of these sandals one two three four five six seven eight nine ten all of these ten sign deals will contribute some little probability here so i will get a k1 plus k2 plus k k 10 isn't it all of them are saying i say the probability is this much at this point would you agree right the trouble is this value you can't take because it's not normal. So what you have to do is you have to take all the answers and divide it by 10. Would you agree to get the actual probability? Yes. Yes. Right. And that is probability density estimation. And just ponder over it for a moment. This is far better way of coming up with a probability density estimate than using a histogram would you agree right here you can pick what are the choices you have you can put a reasonable kernel function you can pick a bandwidth parameter hyper bandwidth is the hyper parameter thereeter. There's two hyperparameters. Which kernel did you pick? Which is the... Now, generally, what is happening is so long as you take smooth continuous kernels, right, not with sharp edges, generally, most of these kernels, they yield practically the same results, right? They're very close to each other. They give you results. Gaussian kernel is by far the most popular. You can pick it, right? Or you can pick another kernel and so on. And in the language of kernels, you would see a more popular in Gaussian is that Binning Kernel, right? Which is the histogram. But we just realized that it's not a good idea to do that. So what happens is when you do all of this, will get would you agree that the sum total of this that you'll get a curve that probably looks like something like this well forgive my drawing but this is how the px function would look the cumulative well you know it should have peaked here, but I made it somewhere else. Okay. Let me make it peak here. Okay better, this is the appropriate function, so that is the intuition let's use that intuition and see what happens, we go back to our Jupiter notebook. Can you stop me with this one? Yes. So you're saying that this cumulative one, the shape of it will look like the small baby's insect? will be influenced, it is the additive model. Correct. It adds up. But will it look like that or what will it look like? Oh, you don't know. It need not even look like a car seat basically. It will look different. It will certainly not look like a car seat. It will look like whatever the data distribution truly is. Okay. So you said that, maybe I didn't miss or you said that even if you take different types of these curves in the end they will all look kind of yeah with sufficiently many values they will all look pretty close to each other so you're saying that if you take gaussian or any of those one of those forces set yeah the end result of that formation pretty much they will look same what spread you take that is right that is right except for the hyper parameters so for example you could take something like this like uh this is your cosine right kernel okay so what happens is now you're building things like this right this kind of thing and so uh ultimately sorry this line shouldn't have existed where the point is so um you can play around with it but eventually the shape that arises will be very close so general feeling is that while you can change your kernels and see the changes in result if you have sufficiently many data points generally not worth trying to optimize with the best one of the things is the sine wave, the sine kernel that, what's the Russian name, I never can get a proper name, Ipanichnikov, yeah that's right, this guy. Ipanichnikov tends to, in general, outperform others, but I prefer I just work with the cross and be done with it right but there is a problem here like all machine learning situations there is a hyper parameter in the model how generously did you spread out or did you pick your um your kernels right that is the bandwidth so what i show here what i show here so i'm going to understand a little bit say suppose you had wild chicken right which kind of fed in the wild and things like that and you got all the small chicken babies together their spread would be quite different on the other hand if you are an arm chicken and you fed them exactly a measure they will probably have they will all be growing up at the same rate and things like that they will have this kind of a separation there will be a weak ones but you give antibodies make sure that they wait and all will you could do that you could for each group you could pretty much actually apply a different term so is that the reason why you have to differentiate no no no the thing is that in the industry, see, once you agree on the theoretical principle, there is no way we can say which is the best kernel. There is no way. So everybody will try a different one. That's all it is. But because . But it is not representative of the underlying data. It is, because see, it's a hyperparameter of the model which permanently issues and in a way data will tell you that this was the best kernel right it becomes a hyper parameter so how do you know what is the right hyper parameter data rules data speaks so now what happens is so this is the explanation of this little bit of a fact I hope this mathematics looks, it is just think of it as a cryptic month. I mean, this looks scary is basically talking about the buckets of sangria we are pouring all over the place. Now the problem with this obviously remains that it is sensitive to the hyper parameter bandwidth. In this case, if you take a Gaussian kernel, we have a bandwidth parameter. So what I have done is I put a bit of code here. I'm giving you, we can just paste it for you so you can use it. What it shows is how does the kernel change? Just the kernel, a change for different values of the hyperparameter. So you notice that this is very simple. If you keep the kernel very tight this is the shape of the kernel this is a very tight here you're saying the value is exactly zero or very close to it the next one you're saying well you know the value is around zero or even up to 5 10 is pretty likely and then you get more and more relaxed about how the values are spread around that particular zero isn't it so this is how generously this is a meaning of the fact how far you're willing to spread the data and let's go now oh i should have done something let me do this i don't know if i have run the previous cells and you can have a effect, let me do this share why. Because i've not run the previous cells i'm actually. gonna shake up sorry. yeah so well, of course, do you notice that when you keep the bandwidth very low, you get a high peak, right? The hill is high because the sand has to go somewhere. It rises up. Then as you relax, you make your bandwidth bigger and bigger, your hill begins to get more spread out, doesn't it? So that is what I was trying to illustrate. You can see the spreading out of the hair and the height has got down. Now, let me plot the same kernel, all of this actually, let me remove the share by. So, oh, sorry. Why am I, okay. So this is, let me keep for whatever reason I have to fix the size and get rid of it now let's look at this same thing I just plotted it out more beautifully with filled in the colors do you see how the four different kernels behave for four different values of the bandwidth you can literally see one so if you think of it as children playing with sand one child has spread the sand pretty far in white another child has peaked it and other children and they have people in between and how much they believe it should be spread so now that you have the current one of the questions is how do you compute the current the kernel density estimation so here the word is slightly abused what we are computing is kernel density estimation of the whole data. What i'm calling the px the probability density is using the kernel method so people often use the word kernel density of that. But remember kernels of the entire but remember kernels are the individual things but anyway be as it may be this part of the code i want you to do so please um because there's a bit of learning here uh let's compute the kernels i use this bit of code type it in. I will make it a little bit smaller so that it's readable. Is it readable enough? Can you guys please do this? So this is computing the kernel. It is one of those, I feel that this is one of those features that is rather underutilized when people do data analysis or data science. So I want you folks to learn it because kernel density visualization in my view is one of the strongest intuition you get into computer. You should never skip it. So please do copy these two cells as is and I want you to type it in tgs as it looks oh the first part harini has already pasted so the first part you can take from there and the second part you can fill it in this is the first time you're encountering the fill function so it's worth typing the second part out the fill function. So it's worth typing the second part out. So what, unlike a curve, like a line, the fill will fill, make an area product. So it will do this, it will make it a little bit smaller. Sorry, sorry for the fact that I'm jumping around. You notice that this visualization is what we are creating with fill not just the mark do you want us to type this plt.fill this cell yes yes please do that okay because this is your first encounter with the fill function Thank you. E aí Are we all done, folks? No. Okay. okay Now, I think we're missing that original first data loading pattern or the file or something like that. Oh, there is a there is a code that you'll find on slack that you paste it look at this where we generate the date one second it's a little bit of a shape of this. Here, I will paste, put it on Slack all over again. Thank you. There we go. This is how we generate. So let me know guys when you reach this point. And think about it, would you rather have this Do you think this is more informative or is the histogram more informative? This one. The kernel plots, yeah. Certainly, right? And it's also more honest, it gives you the real intuition into the data. So have we all reached here? Can I move forward? So the kernel plots, I have a question. So the kernel plots are nothing but a probability distribution, right? Oh yeah, ultimately they're probability distribution using the kernel functions, that's all. Okay. That is why when people use kernel plots, there's a slight abuse of terms. If you want to be really precise, you can say that is the probability distribution that arises when we use kernel density a kernel estimator so kernel functions right so kernel density estimator is density estimator using curves this one There's one way to look at it. Then it looks pretty precise. So are we all here so far? So statistically, what is the difference between Gaussian kernel and normal binomial distribution or any other probability distribution function? Normal and Gaussian are synonyms. Okay. Normal distribution function normal and gossian are synonyms. Okay, there are many is the mathematical nature, e to the minus X squared over two Sigma square is gossian. Right, but there's Laplacian which looks very similar log normal can be made if you take standard if you take the deviation as one looks actually almost indistinguishable from a gossian so there are many such functions that look very similar. And obviously that goes into things that we'll cover in the mathematics of data science. For example, there are all of these lovely esoteric functions like a beta distribution, right? There is a gamma distributions. There is Weibull distribution. In fact, all of reliability has to do with things like variable and gompers distributions, et cetera. You take a flight from here to India, that aeroplane goes over the oceans, right? Passes what, 10,000 miles or something like that. In the journey, the aeroplane is made out of what? At least a hundred thousand parts. And yet you don't worry about the aeroplane popping down into the sea the only thing you worry about is i hope i don't get a middle seat right yes why why do you why are aeroplanes so extremely reliable well it turns out that people have worked out the probability of a product failures each of the component failures and there are beautiful distributions like the variable distribution underlying that in fact, much of the engineering is foundational to these distribution density functions or distributions. So there's a lot of things here. And obviously, this is the visualization course, we won't go into it. But when should you take the math of machine learning or the math of data science course, or we deal with those things systematically. So you're saying that the maintenance of the plane follows that previous recommendations that come from the SPARC institutions. So you do preemptive maintenance. Because of the preemptive maintenance, your airplane has become quite literally the safest mode of transport. If you really think about it, your car, etc., they're nowhere close to that. You haven't worked out the equation of everything's free. Sort of we have reliability. So nowadays these things have computers that tell you that you need to do oil change, now you need to change this. At that time, the thing hasn't actually broken, but you have crossed a threshold, now it will break. So you preemptively change it. That's how you get reliability. Somebody once said that you can drive any normal car easily for half a million miles. All you have to do is do preventative. And it just goes on and on. Yeah. And most of the manufacturing things are based on Six Sigma principles, right? I mean, they are as statistical and it's a normal distribution that they base everything the preventative. All the parts that are manufactured are based on those principles their quality and everything so that adds up to provide you a better absolutely so the six sigma guarantees that when you buy a new thing it is already very reliable you know the reliability will go down as it decays and there are mathematical models of the response the hazard functions of how far it would have decayed and then you preemptively service it or change the parts so that it always remains at very high six sigma reliability gets preserved almost along the entire lifetime of the machine. That's how it goes. So that is that. So this looks beautiful. Have you all reached here? I hope you feel this is worth it guys one lesson to learn here is always do density you know kernel density estimation not to do that is practically right when you're trying to understand the data visualize these things now there is a trade-off remember Remember that hyperparameter about kernel? I mean, sorry, bandwidth? What is the bandwidth and how does it affect? So I wanted to show you, this is a dataset that I generated. You notice that, what is the bandwidth doing? If you look, and I won't, don't worry about the code, which is nothing fancy. I just keep changing the bandwidth from practically zero, because I've rounded it to two decimal places you can't see how small i started i believe i started from 0.0001 do you see this bandwidth and then i go to a bandwidth of 25 somewhere along this do you notice that this is too unrealistic the first one would you agree that it cannot be representative of the data the top one then as you flow down and down and down and down you come to this last you realize that this too is not realistic isn't it the reality is somewhere in between and you have to decide what bandwidth you would consider as good right here what has happened it's a high in the language of machine learning, forgive me for using that language, if you are not doing machine learning, this is called high bias situation. You have over smoothed the influences from each point of the kernels. Your hills are too broad, adding that here the hills are too peaked. Remember very very small edges like this very very peaked ones so when you take very peaked hills of course you'll get a visualization that is like that and so somewhere in there so let me just look along the diagram if i have to look along this diagram at what point do you have smoothness smoothness has been achieved you would say this looks pretty smooth right this where my mouse is 0.25 or something and beyond that you start losing resolution here you see the nice dip but now the dip begins to go up this is a place to stop where you achieve smoothness but you you don't lose enough information you know so i would say this is this these three are successively losing more information this is about as good as it gets maybe you could argue this is better i don't know it is subjective maybe this this so somewhere in the vicinity of this maybe this you can pick it doesn't matter because the intuition about the data has been carried now well can do it isn't it so this is how you would do it in first principles so density estimation is good doing because knowing the density you can use it for example it's a clustering technique called density based clustering then clue is literally based off this uh we covered that those of you who did the comprehensive introduction and and have spilled over to this class remember dentine. R. Vijay Mohanaraman, Ph.D.: So, for example, if I were to tell you where are the data clusters, what would you say, can you just visually tell. R. Vijay Mohanaraman, Ph.D.: Where are the data, where is the centroids or where is the remember the word that we used was attractors top of the hill of the density function so can you guys tell me where the two attractors are where the slope of the function of this particular curve is zero right the top the slope is zero so it would be this part maximized one and four isn't it yeah one and four so you would say those are the attractors in the density estimator in the language of clustering and dental we call them the attractors and we can use a hill climbing algorithm to get there okay that is about algorithms today it's all about visualization now it turns out that if you use c-born they give you kernel density estimator built you don't have to compute it or apply any code you can just give it the data and it will draw out the current densities so c-born makes it a one line i made it two because on one side i've shown you kernel density as a line and then the second side as an area and of course you can mix them both of them so i've made them into two figures one row two columns and this is doing it so now we have to move a little fast in the interest of time we have only two and a half hours left so i won't be letting you type it and move forward by now i hope you realize that this code is dead simple you don't learn much by writing this code you already know it the only thing is to know is that there is a seaborn has a function called kv plus so that now let's go and look at the auto data set if you remember we keep coming back to the auto data site it has miles per, weight, and so on and so forth. It's a continuous variables. Let's look at the data set. And again, you can start by creating normalized histograms. Do you see that after knowing kernel density estimators, how sort of not very satisfactory the histograms look after that. Would you all agree? Yes. Right? It's not, it's not informative. Surely this jaggedness doesn't make sense. So let's do the one-dimensional density estimators. We do this. There's a bit of code, nothing magical here. All I do is for for every features these are the numerical features miles per gallon the displacement the horsepower the wave the acceleration so i create a kernel density estimators for all of them and this is informative it show how it shows how it varies for example do you do you see that one one quality let's look at the lower post what does it tell you it tells you that it's's bimodal distribution do you see that there is big hill and there is a small hill here yeah right so this is the hill of people rational people buying rational cars not high horsepower groups right and this little hill is the car enthusiasm the sports cars and so forth people who buy buy high horsepower vehicles. Well, that was the 1970s. Today, these would be considered low horsepower. This pattern has stayed today. You have the majority of the people buying normal cars and a few people who are into sports cars buying sports cars. As a personal bit, I grew up actually when i was young i uh interesting history i was in graduate school doing my phd so a colleague of mine who came from a wealthier background had a nice sports car so i used to always look at it and celebrate of course i didn't have a car at all. So when I graduated and got my first job, my wife insisted that for all the hard work I should reward myself with a car. I said fine, and then she picked a sports car. She says, you have always been salivating over a sports car, here it is. So I got the sports car and then I got used to the steering of a sports car, the stiff steering and the very tight suspensions and so forth. So after that interesting thing happened, if I would sit in a normal sedan, I would just feel uncomfortable. It's too soft. So over the years, I've still stayed with sports cars. Though the age to drive sports cars is probably over, but I still drive it simply because i feel safer in anyway that's a disability but you look at the acceleration the accelerations of them obviously again you see it used to take 15 16 seconds before you could go from 0 to 60. today you go to 0 to 60 in good cars in three four five seconds weight of the car again you see most in those days 2000 ish 2200 maybe used to be the most likely car after that it was a slope downward so it's informative you can now group it by origin and if you group it by origin. And if you group it by origin, you see a very interesting pattern, very interesting pattern. Let's, for example, look at this. What do you notice? The American cars tend to peak at 18 miles a gallon. The European cars peak at about 27 or whatever miles a gallon. And the Japanese cars used to peak at 30. Already in the 70ss they were giving pretty good mileage isn't it say something about the different ways see car in US is a what somebody is practically it's a it's a level right I know that people I actually know people who celebrate the birthdays of their cars. They're so fond of their car, they celebrate and they'll always like lovingly wash it and so forth. It's their baby, right? So well, it shows in the culture. I don't think there are many other cultures that celebrate where people celebrate the birthday of their car and treat them like babies or pets. Anyway, look at the weight and you can see that the weight here is peaking again in the US in this location, whereas the Japanese car tends to be light. And when it comes to displacement, high, how big the engine is, clearly, clearly American cars tend to have bigger engines, more horsepower. So if you look at the mainstream cars of this, they're really not that different from the European and the Japanese, maybe a little bit more horsepower. But this hump that you see of American cars, this big horsepower sports car, you can see that that sort of market is practically absent in other regions. It is sort of there in Japanese also have a level of sports cars. You see a tiny little hump here, another hump here, but obviously they are still not in the same range, what is called high horsepower. For example, in India, what is considered a high horsepower example in India what is considered a high horsepower car today anybody remembers I don't know one of those big SUVs in India Mahindra or whichever what's the number of horsepower symbols anybody wants to quickly google it up yeah jeep look at the Indian jeep no no that is the displacement. CC is the displacement. But you can look up the horsepower. You can Google up the horsepower of the Jeep in India. Let me know how much it is. Well, it's saying in India, it's Bentley Continental. I guess they sell those in India too now. That is the US model. That of course we shouldn't take. Look at the typical Indian model, which is called a powerful model, like a Jeep. Mahindra. Horse power. Tata Suma or Mahindra. 72 horsepower. How much? 72 horsepower. 72 horsepower, right? Okay, let's take 72. Let's be generous. Let's make it 100 horsepower. What is the typical horsepower of a muscle car here? Much higher. I don't know about other cars, but mine is 610 or something horsepower. So the definitions are different and it shows up in this history. So anyway, what is the point that I'm trying to, not to brag about my car or this car, because I know that many cars have even more, 700, 800 cars these days, those are called getaway cars. So this is, what it shows is, these density estimators are great vehicles to tell a story about the data, isn't it? They're excellent means. They surface what the data is trying to tell you. These density estimators surface that data. Would you agree? Yes. So yeah. So with that, let's move forward. And look,, by the way, I haven't done it. plotly etc, etc, you can do that and leave that as exercise for you in plotly I hear it is called KG plot and plotly it would be called test plot. Almost identical syntax. yeah similar syntax. the y-axis if you go back to show the height and probability distribution for the weight one right the main one wait wait wait yeah so why is it like that y-axis is 0.00025 versus if you look at the other ones no no no the reason is that look at the weight scale the weight is the range of the data is vast from thousand to six thousand so you have to spread the sand far and wide right if you think of it as a bucket of sand right so at any given case the height of the side will be very tiny because you you have a mile a mile of x-axis to spread the sago. So, in this data… One second. If you normalize the data, then it would be a difference. Please go ahead. Somebody is asking a question. I was just saying that it is low because the total area for that y-axis times the x-axis has to be one. Exactly. That is why the y-axis is so much smaller given the x-axis is so much bigger. Absolutely, the same thing. So that's what I'm saying, you have only one bucket of sand. You need to spread it out. If your x-axis is a mile long, then you can't go very high. But I get that calculation, does it come to one? No, no, it will come to one no no it will come to one it will come to one definitely because it's like in the third digit yeah you do that you can do that actually you know that you can actually do the math because each of these kernel density functions if you use the kernel density of the scikit-learn you get a function so you can literally integrate use the sci-fi to use the integral function and integrate across the range, you will get one. You'll get not exactly one, you'll get up to four, five decimal places. I have one question I ask before you progress. So the histogram and these kernel density plots, right, they are related to each other, I would assume. I would consider this the histogram, the poor man's kernel density estimator. I mean, poor man's density estimator and kernel density estimator to be the thing. In fact, my general feeling is, I tend to draw the histograms simply because it's a ceremony to draw histograms. But I tend to primarily focus when I'm driving intuition into data, I primarily focus on the density estimators, the kernel density estimators. So if the width of the bin goes down to zero as its limit as the width of the bin goes down to zero, it would become a width of the beam goes down to zero it would become a kernel density estimate instead of a histogram the problem that normalize yeah in the asymptotic limit of data size going to infinity and the bin size going to zero it becomes the kernel density estimator not this gaussian kernel so there is an argument so they see okay here's the thing there are some subtleties if you take the asymptotic limit will it become the same as the current density estimate. R. Vijay Mohanaraman, Ph.D.: That is called the not it's some there's a whole paper on it says it's an idea of looking at it and so forth, broadly, the intuition is correct, but there's some subtlety there's a little bit of a subtlety there for why it isn't exactly quotient. Okay, thank you. If I remember, I've forgotten the full argument, but there is something there. But your intuition is correct. Imagine that the data set tends to infinity and the bin size tends to zero. What do you get? It will be a pretty good approximation. In realistic terms, it's a pretty good approximation in realistic terms it's a pretty good approximation let us go validate it see here is how uh the sorry uh here is how where is the histogram yeah look at this histogram this histogram pretty much looks like the kernel density estimator that we looked at before, isn't it? So this is the histogram in which we have increased the bin size to y1. So it's beginning to take the shape. The trouble is, in reality, because data sets are finite, this begins to get choppy. You see that how the lines are getting choppy? Yeah. So that is what kicks in. So you have to assume that the data set size itself is tending to infinity and the values, the number of benches is zero. I mean, the size of the bench is tending to infinity. So you've got that intuition. So there we go. Now, I normalize histogram. We did one day, one day is beautiful. Can we generalize this idea to two dimensions? So for example, if we look at the feature subspace in which we take two variables, let's say horsepower and miles per gallon. We're just looking at how miles per gallon and horsepower related. And we have data points. We predict all the data points and at every point that you find data you again create a cycle now this time a genuine cycle what is the how will it look it turns out you don't have to do much you can literally the same code will help you do this i pick a good color scheme here i chose plasma to show the red hot areas the really dense areas. Now those of you who are familiar with them, you would realize that these are your attractions. They're glowing hot, they're glowing yellow. These are your peaks of the hills. So do you see guys that this gives you some intuition, very crucial intuition into the data. Do you agree? very crucial intuition into the data. Do you agree? And these curves that you see in there, these are called the contour curves. Contour curves are curves of equal density. So all points along each of these curves will have the same kernel density estimates or values. And so you can see how the data is spread out. If you only want to see the curves, the contoured curves, you can, all you have to do is set fill is equal to false and then you would see that. Also, how many levels, how many curves do you have to see? You can set levels. You control with the number of, so these are minor technologies and how to use that okay so this is it right now you can say that all right why did we choose miles per gallon versus horsepower what about other variables i have five variables so i have ten possible pair of nonsense five c two five combination two is ten so how can i see all of them together? Surely I don't want to write code one by one, keep drawing it out and create a giant model. It turns out that Seaborn, and obviously Seaborn in particular, this is one of its strengths. It gives you something called a pair plot. And first let me show you what it does. I'll just shrink the size a little bit. Do you see that in a single visualization, you can get all of them, right? What are the properties I'm looking at? Do you notice that you get all of them? Look at the diagonal. What is the diagonal? The histogram. What is the top diagonal area? It is the scatterplot. If you look at the subspace of one feature versus another feature what what is the scattering of the data in that already when you look at it you can see the shape like this right for example here so what do you expect you expect the same shape to reflect in the kernel density plot here kernel kernel density plot, I've drawn it out, right, and the lower half are kernel density plots. I find that these pair plots are very informative. I hope you would agree that if you can draw out this pair plot. Now the code to do that is straightforward and in the pausity of time I will explain and not make you do it. Look at this. You create a pair plot in which all you say is, okay, what is the data? Here, I have reduced to the numerical, only the numerical columns of the data. Then I say that in the fig, map the diagonal to a histogram. Are we seeing a histogram in the top, in the diagonal? Then you say in the upper part of this here put the scatter plots scatter plot is also something we are familiar with and in the lower part put kd plots kd plots kernel density estimation plots we just learned about and so it will draw all of it together and you get this vision do you see this code guys now does it look simple when you see this uh here the reality here, does it make sense? Yeah. That is it. It's a very powerful way of visualizing this and I think this is one of the remarkable things about Seaborn that you could do it so easily and of course if you want to break it out by the category to say origin, you can do that. You can say it will tell you more do it so easily and of course if you want to break it out by the category to say origin you can do that you can say it will tell you more about it so here it is broken out by origin so all you have to do is add one more line here say you now color it by origin so so seaborn uses the word u for color yes in u which is a synonym of color. You see U is origin. And so you get this beautiful visualization. So I will, and with that, I'll end the kernel density estimators. We will take a 10 minutes break and then we'll learn a completely new topic. Maps, how to visualize data, geographical data, geobots. And then we'll do networks. And then we'll do presentations. And I hope all of you have had your project presentations. This batch has been extraordinarily quiet. I don't know how many of you have done any project at all with it, but we'll find out. 10 minutes break and no more. It is 3.54 at 3.55. Promptly, I will start.