 today for some reason going live on youtube doesn't seem to be quite working so I'll let that be. Alright guys, so just to recapitulate what we did last week, we covered the preliminaries for this paper, Attention is All You Need. So in that context, we talked about softmax. Somebody had asked asked this question why is there a softmax at the end of all the classifiers and why does the attention model use a softmax so the intuition behind softmax is that it is a probe if you have given a bunch of array or a list of numbers the softmax will exaggerate or will probe or point towards the biggest value. And the way you do that is first you take all the values, you exponentiate them. When you exponentiate the values, they all become positive. Once they become positive, then you can sort of take the proportions after that. And then you take the proportion after that, two things happen, explanations make them positive as well as they exaggerate the biggest value. And so the biggest value then stands out as we took an example and we showed it with that example that if you take a bunch of numbers, for example, if you take, let's numbers for example if you take let's say some some numbers which are in this proportion um one one point five and two those numbers are pretty close to each other so that it's a somewhat soft optics and then you take a k a little bit more well defined one ten twenty so then if you just took proportion, raw proportion, you notice that the way they divide themselves is 44%, 33, 22 for soft and for much more defined ones, it is still 65%, 32% and so forth. Now, the 65 stands out, but not that much. The other problem with just taking proportions is it wouldn't handle the negative values. You'll have negative numbers. Those don't look like probabilities. And if you want them to behave like probabilities, you need values between 0 and 1. And you want all of them to add up to 1. So softmax does the trick for you. Let's see what happens when you do the softmax. If you exponentiate them, you see that for K, this value really stands out, 4.85. It's really a big value. At the same time, when you make it into a percentage proportion using that, you realize that this becomes the big value 20 becomes 99.995 percent probability and the others are vanishingly small and the way to do that is actually a bit more let me just share a different part of the screen to illustrate this you know share, what can I share here? Let's do that. So I just wrote this code. I'll just email it to you guys to show what it means when you do that. The other thing is that, the other point that I made last time is that if you take two random vectors in a high dimensional space, most of the time they will be orthogonal. And the intuition that I gave is, there's way too many axes. And so if one vector has high value or is more aligned to one of the axes or something, it is unlikely that the other vector will be sort of aligned with that. You can be a little bit more rigorous about this. You can, for example, say that, let's say arbitrarily one vector, you take all of them as unit vectors. If one vector is pointing along the first axis, let's say X axis, and the second vector is pointing in some arbitrary direction on the unit sphere, the expectation value or the average is zero. And the mathematical way to claim that or show it without actually doing it is just to realize that it's an odd function. Because it's an odd function, obviously the expectation value or the average will be zero. But then the variance of this tends to one over n. One way to think about it is the variance is sum of squared components and if in the general case when you have a lot of numbers they all have to add up to one. So suppose they're n dimensions one can imagine that each of the components will be sort of of size 1 over n on average, right? It follows from the law of large numbers. And so you can argue that the variance is 1 over n, which means that most values are around 0 with a variance of 1 over n, so they're close to 0 for very large n values. You can see that very clearly by taking just two vectors so here i just create two random vectors and then of course i make them into unit vectors i normalize them once you normalize them and you take the dot product you come up with a value that is very very small so here if you notice the value is 0.00069 in other words close to zero you take any two random vectors let's do it again and this time it is again small and small and so you can keep on doing this over and over again and you'll keep on getting greater that are close to high zero which points to the fact that most most of these vectors randomly if you just take two vectors in a high dimensional space they will tend to be orthogonal sort of interesting take these bits of knowledge and use that to understand the the attention model and the transformers and we'll do that the last piece that we talked about in preliminaries was the concept of attention oh by the way were you guys seeing what I was sharing before just now maybe not no okay maybe I'll repeat what I said because you may not have seen what I was saying so saying that if you take two vectors, S and K, let us say, two array of numbers, sorry, two numbers, then what softmax does is it exaggerates the value here. As you can see, the 99.995, the K will pick out. It's a probe. I'm sorry. The softmax is a probe. It will look at a list of numbers and it will make the biggest one stand out as a probability Shubham Tulsiani, And that is sort of the value of soft max it does it in two steps. First is it exponentiate the numbers so that you always have positive values and also exponentiation exaggerates the big value. And then in the second phase, you just take proportion to make it into a probability between zero and one. And so there we go. You can see the other point that I was mentioning is that random vectors in high dimension tend to be nearly orthogonal. So this was the code. I was showing if you just generate two vectors x and y and If you just generate two vectors x and y and you once you normalize them to unit vectors and you take their dot product, you will realize that their dot products tend to be very, very small. Most of the time. Sorry. This one, they tend to be very small 0.09 and again I'll do it. It will be 0.0089 as you can see when the dot products are small it means that they are orthogonal to each other and that's an interesting observation as i said we will use it now in our work in understanding this paper these are the bits of things we need the third bit of thing we needed was about the embedding. We talked quite a bit about embedding. And the concept of embedding was, if you remember, that you take all these words in the language, by the way, last time I used the word corpus to mean the vocabulary, I sort of mixed it up. Corpus is a set of documents, a lot of English text. Vocabulary is the set of words. And I think last time I was mixing them up and I realized that later. But okay. So suppose you have a lot of words in a corpus, in the vocabulary for that corpus that you extract. Now, those words are not random. There's relationship between words. You talk about ripe tomatoes, but you wouldn't talk about a ripe scissor and things like that. King and queen are close to each other and things like that and apples and oranges are closer, things like that. So there are relationships between them. Whenever you have categoricals whose values, those classes are There is some relationship between them. The question that arises is can you capture the relationship for words? Can you somehow make a step towards capturing the semantic content of those words? So to do that, what you do is you take, you start with the one hot encoding. Now, once you start with the one hot encoding, your space will be essentially the size of your cardinality of the categorical features. So suppose there are three million words in the English language, let us say, approximately. That would be a wide vocabulary or maybe a smaller vocabulary, I think 300,000 or 100,000 words or whatever you take. If you one-hot encode it, you'll have a pretty large dimension space. And all of your word vectors will be orthogonal to each other. Their dot products would be zero because they will all have a bit or a value set at a different place. Now not very useful. So what you do is you search for a lower dimensional embedding or latent space in which the semantic meaning of the words becomes much more clear. And to do that and here we talked about it and so you can do it with words and with sentences with, and so you can do it with words and with sentences, with paragraphs and documents. You can create embeddings. When you do the embeddings, you come up with very interesting relationship in a lower dimensional space that is this, lower dimensional space where D is much smaller than the big D, which is the one hot encoded space, you find relationships. The interesting relationships you begin to find are things like, for example, Delhi is to India or Washington DC is to US. Then Kathmandu is to what? And it turns out that you can write it as equations. You can observe that Delhi minus India is approximately the same as Washington minus USA and therefore that and that also from that your input should be Kathmandu minus whatever else that X that you're trying to find. And if you solve for X you'll realize or what this thing will show is that the word Nepal is very close to that x vector. And so you can infer that Kathmandu is the capital of Nepal. These are interesting relationships people began to observe, and it created quite a bit of excitement when these things were discovered. So for example, you could find the relationship, I think, a king minus male plus female. Or maybe it was king minus man. And it was woman. I forgot. I forget exactly what the specifics are, but king minus, and it turns out to approximate the vector for queen. So that's very, very interesting. You find the embedding in two ways using supervised learning. What you do is in the process of a classifier. You take this matrix m cross d, like for every word you want to find an embedding vector, you initialize it with random numbers and then you train it as, as for example in a supervisor or a classifier. Let's say that given a word you're trying to determine the part of speech. determine the part of speech so what you would do is you would put the embedding matrix in front take the x feed it here multiply it with this then feed with a few dense layers you know feed forward layers and then obviously the soft max layer and then you predict the part of speech verb noun at work whatever it is right and then you back propagate the errors and then you try to make it more accurate as you classify a learns as a product of that or as a side effect of that you're embedding gets trained and so you have You have a means to create embedding for embedding vector for each of the words in your vocabulary. Now there are also unsupervised ways of doing it. They're lovely. Now, there are also unsupervised ways of doing it. There are lovely methods, papers, and we are going to cover it in the subsequent weeks. Now, perhaps next week we'll do the word embeddings, word to word, glove and fast text. So that what it does is it says that, see, suppose you have a corpus of documents, lots of lots of text. The relationship between words is right there, present. So let us uncover the relationships between words by just looking at these word sequences there. And so there are quite a few techniques that try to do that. And at the end of it, they come up with it. Like for example, I talked about glove. What glove does is that it looks at little windows of text with this. So suppose you have a word, a cat, then it will look at two words before and two words after. It will create a window size of five. You always take an odd number window size so that you have given a word, you have as many on the left as me in the right and then it looks at the co-occurrences like for example how many times the word cat and meow let us say a co-occurred versus cat co-occurred with any other word in the vocabulary what proportion of times that it sort of co-occurred, things like that. And once you take these ratios, they give you some sense of the relationship between these words. And then, and obviously I'm skipping a lot of mathematics here, but basically what you do after that is you look to create, you say that the embedding vector for cat dot embedding vector for meow will be the log of the probability, probability of cat meow as you find in this equations. So there are these mathematical techniques, they're very elegant actually. And in the next few weeks we'll cover that paper step by step but today we'll focus on attention so that's about the word embeddings right now a little bit about review of the recurrent neural networks what did they do recurrent neural networks are sort of time flow they have a sense a temporal sense you give it an input let's say that the work the sentence is this the cow jumped over the moon then you give it an input let's say that the word the sentence is this the cow jumped over the moon then you break it up into a sequence let's say word sequence x1 is the x2 is the cow jump just x3 and so forth and you take a rnn unit and it's the same unit into which you keep on feeding words over and over again the entire sequence words. First you feed in X1 and let us say that it produces Y1 or you do look at the output or you don't care, but it learns, it sort of, it creates a thought vector H1, a hidden state. People call it the hidden state or the context vector or the thought vector. The way I think about it intuitively is that in the mind of the RNN, the way I think about it intuitively is that in the mind of the RNN, it has somehow recognized now the word the, and then it comes across the word cow. And so what you do is you take the hidden state and you take the new word and you put it together. And now this time you hope that the hidden state that comes out understands the cow somehow and so you feed in the next word in the next word and at the end of this entire sequence what you will end up with is sort of a thought vector that captures the entire sentence the the cow jumped over the moon or something like that and what can you do you call it the encoding in in some. You have encoded the content into a thought vector. Now you can use it to decode. You can do many things, summarization, translations, and so forth. So now the limitations, if you remember, of the RNNs, and they're obviously they're much better, they're more powerful cousins, the LSTM and GRUs. The basic point is that, and I wouldn't go into everything we talked about last time, but the point is that LSTMs have memory, they have sort of things to remember and things to forget. And so GRU is a simplified version of LSTM. It's a more modern thing. At the end of it, they all help you remember things. Like for example, suppose the sentence is not just cow jumped over the moon. You can say the cow that was fed some carrots jumped over the moon. If you remember, we talked about it in joy, right? So the word jumped is by the time you reach here, you don't want this memory, the thought vector that's coming out, to have forgotten that it is all about the cow. The example that I gave is the character in Nemo, finding Nemo is Dory. Dory tends to forget very soon. So the same thing would happen in your memory vector. When you give it long sequences of words, the more recent words stay in the memory, the old ones begin to fade out, right? And they get subdued, which has been a big problem with the RNN architectures. So how do we address that? The point with attention models is they have an elegant way to address that particular problem. And the way they do it intuitively is this. Suppose you did not just remember the last thing that came out, the H at the end of all of this hidden state, the hidden state is not just H6, but somehow when you are decoding that, let's say into a new language and you're generating the words, let's say in Hindi, you have access not just to H6, but you have access to H5, H4, and so on and so forth. Logically speaking, suppose you had access to, not just the last state here, but somehow you had access to all the previous states. And that, then there would be no forgetting because you have access to the and the cow and the whole thing the trouble is that having access to all of those hidden states is not terribly scalable for very long sequences so how do you deal with that and we'll deal with that in the attention so that's the background and of what we talked about last time now the attention models uh the attention mechanism is quite elegant. First it leads to parallel processing, big thing, because that was a big drawback of RNNs. Data had to be fed in sequentially. In attention, as we will find, you can feed in data in parallel. You can do very fast computations or translations and things like that. That is one and the second thing is you don't have this problem of forgetting. It deals with long sentences much better, you don't have forgetting, especially in the longer sequences of words, long sentences and we are going to see that today. So the way I would like to do that is now that we have the background I would like to do that is now that we have the background, I would like to walk through this paper. Attention is all you need. This paper came out, as you can see, in December 2017 and was really the big thing in 2018. It led to things like Bert, Robertoberto and robert roberto and albert and so so on and so forth lots of architectures today we talk of cpt uh gpt sorry gpt one two and as some of you probably heard or saw in the news now gpt3 has been released by open ai released by OpenAI, which is a hundred times more powerful than GPT-2. So these things are working very, very powerfully. And it is interesting that so much progress has happened just in the last three years or two to three years. It's quite remarkable. So we are going to, not even three years here, two years or so. So we are going to now go over this paper, Attention is All You Need, to understand that. Now first is the title. What are they trying to say, these authors? Is all you need implies that perhaps people believed more was required before that. And that is true. While these RNNs were there, people were also creating the attention mechanism. They were thinking about it. There were a lot of paper. I think quite a bit of work was happening since 2013 or 14 and so on and so forth. Maybe earlier, I don't know. But this attention mechanism though was being coupled with RNNs and people were getting good results putting attention with RNNs or with sequence, the old sequence models and getting the results. The fact that you have RNNs always has the limitation that things have to happen sequentially. What this paper highlighted is that actually you could get rid of the RNNs all together. You don't need them at all. And you can do everything like this translations and things like that. Well, not everything, but quite a large number of things that you were employing RNNs for, you could actually do with attention only, which explains the title, Attention is All You Need. And this came out of Google research. All of these people were directly associated with Google research. One way or the other, either they are directly employed there or they're doing internship or something like that of visiting scientists. I think something is mentioned at the bottom somewhere about who did what and so forth. So I'm going to talk about this paper carefully today. It may be a bit of a long session. I've highlighted the dominant ideas. the dominant ideas. This is an encoder-decoder architecture. In other words, first you do build a representation and then from the representation you you come up with the new words, let's say you're doing translation. So, This other abstract is worth reading. I'll read it in its entirety. The dominant sequence transaction models are based on complex recurrent or convolation neural nets that include an encoder and decoder. Again, people sometimes use con nets, 1D con nets also and so forth. So both of it put together is the old way of doing it, RNNs or CONNNETs. The best performing models also connect the encoder-decoder through an attention mechanism, which is the point that occasionally they were adding attention into the mix. Now, this paper is essentially proposing a new architecture, the transformer. So he says, we propose a new simple network architecture, the transformer, based solely on attention mechanism, dispensing with recurrent and convalescence entirely. And that I think is the seminal statement. So then they go on to show that the accuracy improves. The last sentence is worth reading. It says that we show that the transformer generalizes well to other tasks by applying it successfully to English constituency parsing, both with large and limited training sets. So in other words, they say that it's a pretty powerful architecture that you can apply to many tasks. And there's a benchmark blue where it's sort of achieved very high scores and now this course two years later looks not so high because the the thing has evolved people have created all this wonderful GPT 1 2 3 and all of these models based on these transformers and the scores are much much higher so what is this particular architecture? This is a main diagram here. And I'd like to sort of emphasize which part is which. So this part here, well, that doesn't do a lot of justice. Okay, so this part is your encoder, I would say. Encoder. No, excuse me. Yes, this part is your encoder. And then the opposite of that is this part is the decoder. So actually this decoder. So actually this decoder part has two things. If you look carefully at this encoder, you will realize that the lower half of this, up to here, this part looks suspiciously like the encoder part. So I'm going to explain these things now step by step. We'll just focus on the encoder because this paper, when it goes into this, it says it does all this attention things and there's an equation. This equation is the main of the celebrated equation of this paper there. What is attention? So I'm going to explain that in simple terms so to do that let me use the space next to it let's see where where should I draw it out all right I'll go to the bottom and put it here for the time being. So the idea is this. First you take the word, let's say you have the word a very silly statement here that was fed carrots jumped over the moon. Over the moon. Enjoy. Let's say some silly silly some statement like that so what happens is first of all each of these words are let me say the what happens to the the first thing you can do and to the cow and that what can you do with each of these you can come up with word embeddings right So you have the embedding vector for the embedding vector for cow, embedding vector for that and so forth. Well, actually, yeah, okay. For that and so forth. You'll keep on going and somewhere it is the moon. It will have its embedding vector. So for each word, we have an embedding vector here, E, right? EI, let us say. Now there is a problem in this system. In a sequential model where you feed X1, X2, Xn, implicitly the position is defined of a word. This particular word xj comes after these words and before these words. So the position of each word is well defined. Now in the attention mechanism, what they do is the position is not clear. If you can't just use the embedding vector, embedding vector is position agnostic. So you need to inject the positions into this whole thing. So how do you put position information or it's called position encoding. How do you do that? The way you do that is actually the way these authors did it is quite simple and but it's a little bit let me show where they talk about it. The position encoding is being talked about. Here. Here they use this equation and I'll just explain what this equation means this equation. So let us say that the important thing is there's a sine and a cosine here. Let's take, doesn't matter, the argument is the same. There is a position part of it and there is divided by a frequency, sort of a 10,000 to the two i, et cetera, et cetera. so let me explain what this means intuitively because it may look confusing what you do is suppose you have waves like this a sign let's say a sine wave or a cosine is this is one wave now you take another one with twice the frequency so suppose you take this and then you take maybe something that goes like this and so forth and what you do is at this locations you start throwing in the words so the cow cow and so forth so for moon so what you do is you take these points these values so here you say okay this is rising this is in the increasing part this is also rising this is here and then if you look in the increasing part. This is also rising. This is here. And then if you look at the cow part, cow, that, or let's look at the word here, that, that is near the peak. This is decreasing. This is increasing. You realize that just by looking at these values, these points, you can tell where this word is relative to this word in this word this words position encoding is different from this word and two words that are very close to each other they will have very similar position encoding like the cow have fairly similar position encoding but the that have a different encoding by the time you come to moon here, the position encoding of moon will be different. And these points, you treat them as a point or the components of a vector space. I believe the author takes a two one vector space. I think we can confirm in the paper. And so the position encoding for a word, let's say the word cow, is again a vector 512 dimension. In the same way, the embedding space they have taken as 512 dim, nothing magical about 512 is just a choice that they did, and nothing very magical about this particular way of doing position encoding. You can come up with all sorts of schemes for doing position encoding okay but so now if you have these two things together let's say that you put the this part for a word let's say the word cow is there cow goes to this the e embedding for the cow and then you concatenate it with the position for the cow let's say that the position encoding that you have position encoding of the cow right and you create a large vector you concatenate these two and you create the final cow vector which is sort of these two things put together. So this is your input, input for the cow, right? Input vector for the cow. Remember that in all of machine learning, you're first trying to prepare your input vector. Now, if you look at this input vector, both these things are there. The semantic aspect, the meaning aspect of it, of the cow, to some extent is there in the embedding word and its position is encoded into the position vector right so both of them are there so then what do you do what you do is quite remarkable actually this attention mechanism think of this as a system. This X vector, cow goes in. And from that, you burst it out into, you create three separate vectors out of it. You call it the K vector, K cow, the V cow, and the Q cow. Now, what in the world are these three K, Q, V? And this paper talks a lot about this K and Q and V vector. In fact, they are the foundations of your attention mechanism. They are certain dimensional spaces, I believe. So first of all, when you take this, you break it up into these vectors, I believe 64 dimension if I'm right. But I may be wrong. We may have to look at the paper for the exact detail in a moment. But it's an x dimensional vector, all of these. The intuition behind this is that for every word, you are token in the more general sense you create a key vector right that sort of gives them the way intuition that I have is in a certain space it gives you the indexing or location of the word indexing or location if you if you will in some sense of the word the value vector is sort of uh literally it's the it's the way that you would think of the word vector itself i would say think of the word the the v is the value vector so it gives you what you're attending to and you'll see why we use the word attending to to when do you and this is called the query vector so now what happens is you feed it the entire sentence now how does it do that by the way it's very simple you take this is standard matrix multiplication and that is that is again the remarkable thing. Suppose you have a matrix for Q, right? This will produce KQ, right? And then likewise x dot w of v will produce K V and then sorry vector. So let me just call it. Well, let me just call this. Okay, let me just call it Q. Q for this word Q I I think the notation used in this is this like this and we and v i and likewise xi times multiplied by another matrix which is the key it will produce the ki so every word now gets burst out into three vectors one vector in the k space one vector in the v space one vector in the q space, one vector in the V space, one vector in the Q space. Now, what is the value of that? That is the thing that it comes. First of all, you notice that you can feed in, because it's just matrix multiplication, you don't have to feed words one at a time. You can take the word moon and the entire sentence. You can take all the words in the sentence and you can essentially in a pass it through this mechanism right so I hope that is clear that there is no reason to do it in parallel you can pass all the full sentence at one go through this system so now you have all these vectors one for each word then comes the interesting thing what happens is that so suppose they are all of these vectors one for each word then comes the interesting thing what happens is that so suppose they're all of these vectors xi one x two x three oh sorry i should use the word k1 k2 k3 K1, K2, K3. Now what you do is you take a query, you take a query vector QJ, right? Now, suppose your query vector is this, QJ. You will realize that this query vector will be closer to some of the vectors and far from the other more or less orthogonal to the other So if you do this times K For all the I values you take this query vector and you so you let us say that I is equal to 1 I is equal to 2 I is equal to 3 you do this What will happen is most of these values will be small so we'll end up with a vector quality will be pronounced is pointing you what it is doing is it's it's taking the q and it is pointing it towards another word let's say that this word k2 was moon uh cow or i don't know moon right and this i don't know maybe cow needs to pay attention to the moon or something like that something will count shouldn't something will count shouldn't okay so I'll just use the word so it needs to pay attention to the moon or something like that it becomes like that so then comes this interesting thing now that you have taken the dot product and by the way let me write the dot product as this in this format so you have a dot product all of these dot products what what can you do? You will get some positive, some negative values. The thing that we learned about just a little while ago is you can exponentiate it. By the way, there's a normalizing constant. You put square root of d, okay, which is the number of dimensions that is to prevent the gradients from exploding and so forth. It just makes the computation better, I think. So this is it. And now what you do is, but let me, while we do that, let me not focus on that. That's a technical detail, this dk square root of dk part. Let's focus on this part. What you do is it produces, once you exponentiate itated and then you multiply it by VI. So when you do QJ KJ Right up to this constant decay and we are so this is something I want to Illustrate and show in the paper that this is the main attention equation in the paper If you look at this This is the main attention equation in the paper. If you look at this, this is it. I hope we can color this part. First, what we do is we softmax this. What will softmaxing the Q with a K do? It will single out that particular word. So suppose you have a query word moon and let's say that you are trying to find out what thing it should attend to by taking the dot product with the k vectors it will sort of and then soft maxing it it will single out the particular word it should be paying a lot of attention to maybe one word or two words it should be paying a lot of attention to maybe one word or two words it should be paying a lot of attention to and then you're multiplied by the value of that word right so in a sense what you do is you're giving different amount of emphasis to different words each word goes and picks out some other words in the sentence and pays more attention to those words. Now the whole question comes, well, how would it know how to do that? And that is the learning part in this whole game. Remember that to generate these vectors, a QKV, we had to multiply them by these matrices, kv we had to multiply them by these matrices right the w matrices wq wv wk matrices and those are weight matrices we learn them through the usual back propagation in neural networks so the same gradient descent and back propagation and all the theory holds and the supervised learning theory holds but at the end of it you feed in a lot of sentence. Let's say you're doing a lot of translations and you're back propagating the error. You know the translations. Ultimately, you'll end up training or coming up with good value for the weights. Once you have good value for the weights, you can burst out each word into its KQV. And then for any query word, let's say the moon, you can see which other words in the sentence, self-attention, which other words in the sentence it should pay more attention to. So that is the encoding part. So what you do is telling the j-th word to pay attention to the i-th word in the sentence. Now, once you get that, you, and that's why this is called the attention. And the authors call it the attention. And attentions were understood before, obviously these authors didn't invent attention they invented transformers this is your equation for the attention now that you have a way of making a word pay attention to some other words one or two words and more attention to a few words in the sentence what do we do with that what that? Actually, the rest of it is so actually we can illustrate this like here. The cow, I'll just write that again. Let me write a simpler sentence. The cow jumped over the moon. Right? Let's say enjoy. Now what happens is all of these words, you realize that you will end up with the attention mechanism. And for each of these words, you'll end up with your KVI vectors. Then, therefore, for each of these words, you can do the attention and so forth. You first thing and the now comes the transformer architecture. What they do is you feed in the x vectors, the xi and you get the qi and qkvs. And then what happens is they don't do it in only one layer. They treat that as an input and they do it again and they do it again and they do it again. And they do it about six times over. Nothing magical about six times, but it so happens that they do it six layers. You can think of it just as in a deep neural network, sort of a feed forward dense network. put six layers well here we go think of them they are doing it six times over once they have done it six times over they then they're not they have a bit of normalization the rest of it is mechanical aspects norm plus I and so forth and then so we may as well go to the paper and look at the diagrams. So look here. What you do is they feed it to this attention mechanism. From here it goes to the norm, add and normalize, and then you pass it through a standard feedforward network. then you pass it through a standard feed forward network just as for a cnn you would do it for you know at the end you would put a few dense layers in the same way you put a few dense layers and again add and normalize and so forth now one thing to pay attention to it is that they are not talking they're not talking about attention they're just talking multi-head attention which is quite interesting and let us talk a little bit about what is multi head attention. See, Let's go back to our picture here. What we did is we took a word, let us say a word cow. a word cow or rather it's embedding both embedding and positional vector put together and from there we created a K cow we cow and you cow now what happens is the way those weight matrices work they try to give meaning or they try to break it up into those vectors in a way that attention is paid. But when you talk of attention, there are many ways of looking at the word cow and its relationship to other words in the sentence. So to give an a very rough analogy, one may be paying attention to the proximity aspect of the words. Another may be paying attention to the grammatical aspects of the sentence and so forth from that perspective. So what you do is you don't have one head, one block that burst out, a set of weights, W, K, W, Q, W, V. You don't have that, but you actually have many of them. Let me just call them I, I. You create many of those things. In fact, in the original paper, they used, I believe, eight heads. And the idea is, the idea is very similar to using, in my mind, using filters and convolation networks. When you take an image and you convulate over it, you don't just use one filter. You use a stack of filters. And you expect that each filter will learn to pick out something different in the image to make it very intuitive and a rough analogy would be that if one is learning to pick out edges another is learning to pick out crosses and there's learning to pick out lines they all try to pick out or learn different things to pick or features to pick from the image or recognize in the image. So having many heads has an approximately similar analogy. What you do is you have many heads and you hope that each of the head is sort of doing this attention stuff, creating this K, V and so forth, these vectors, bursting it out in a slightly different way. It's paying attention to different aspects of the sentence. And it works out wonderfully, actually. So that is why the word in the paper is multi-head. Multi-head attention. So there are eight of these layers. Think of them as eight filters or eight stacks or something, if you were talking about. Convulational neural nets. Is that meaning clear? attention. This is nothing but this equation, you know. You take it qk dot product and you scale it and so on and so forth, apply the softmax multiplied by v, that is all right. What is the multihead aspect? It is the same thing. you do it by across multiple of these heads and then at the end of it you sort of concatenate all of those values together to create a long vector which is your attention vector at that particular moment now once you have done that what happens now So let's say the word you're trying to translate the word and write the word right here. The cow. Well, I better take a dark color. Yeah, let's say the cow jumped over the moon. What will happen is you feed the entire thing, it will all come here at this moment into this, right? All these key values are coming into this. From the encoder, they have gotten encoded and they're coming to decoder. Now what happens is you look at the translation, let's say translation isn't in the first you need to create a certain number of say words that will represent How would I do that? Something like that. My Hindi is pretty atrocious, but tell me if it is very wrong. or something like that. So if you notice something very interesting here the order is very different the let's say that work workout you make it like this cow maybe these two sort of map but then the third word jump doesn't come here instead the moon word comes here so what what should happen is when you do this attention model, the word jump should pay attention to the Hindi word chant, right? And how does that work? That is the decoding part that we are going to pay attention to. So all of these things, when they come here, then for the first, you don't have any word, you have just blank. So you use this to produce the first word, let's say, ver. Then what you do is you put it here, you feed that word back into output. That's why this is called the output encoder. You put it through this word ver, through the same treatment that the input words went through. It will go through the embedding, it will go through positional encoding that the input words went through. It will go through the embedding, it will go through positional encoding and so on and so forth. And it will go through the same process to again create the attention vector. And all of these will then go into another multihead attention layer, the third part, which is really the crux of the decoder. And what it will do is it will take this attention over all of this word plus this one, the first word. And it will produce the next one guy. Then what happens? Now you have two words. Let's say two words. And this is Hindi guys. There's a few who, and I apologize, a lot of you are either not from India or are from South India in which case Hindi is a bit of a no-no but I don't know Tamil so you'll forgive me for that so here we go once you have these two words now from here all the words have gone in these two words will again go in so this this is why the word mast is there. It means that the words that come afterwards don't exist in some sense. They have been mast out, right? You're paying attention only to the words that have been translated so far. Work guy is fed in along with the whole attention on the first sentence to produce the next word and hopefully the next word that comes out where attention is paid after taking this and all the other English words is the moon, the translation for the moon. And so step by step, this sentence is built up. Are we understanding that guys? So that is why the word mast is here and this is called the output encoding and the rest of it is very, very similar. If you compare what is on the left-hand side and what is on the right-hand side, you see in some sense a repetition here. This thing, this part looks no different than this part. And that is it. That is the gist of the transfer or the transformer. It uses these attention heads or attention mechanisms to get its work done. Interestingly, the decoder is nothing but sort of the encoder with appended a masked multihead right here, because you have to only look at the words that are translated so far and then rest of it is the softmax why do we need softmax of course we need softmax because we need to decide which of the translate which of the words in the hindi language to ultimately use right so what that's that it will pay attention it will ultimately end up producing this particular word so that is the attention mechanism actually if you understand that the rest of the paper is very sort of easy to understand this picture I hope is easy to understand and there isn't other things it talks about the fact that this is a scalable, this is a very fast and parallelizable way of doing it. How would you train it and so forth. And they show that in the original paper itself, the results were quite impressive. The blue results were quite impressive. And now I invite you to look at the latest results. They are much more impressive. Next time,, next time I think we'll talk about embedding but after that we'll take up the paper, the BERT paper. And BERT is very important in natural language processing that represents the next milestone and let's talk about that paper. So guys I haven't been paying attention to the chat. Let me see if there are any questions. Where do I see those? One second. Guys, feel free to ask me questions. Is there anybody there in this? I don't know if I've lost the whole audience along the way. Is anybody listening, guys? Yeah. Oh, you guys are there. Okay. Wonderful. Any questions, guys? If not, this is it. This is the attention mechanism. This transformer architecture using attention is very powerful. Its biggest contribution is to show that you can do all the sequence to sequence models or stuff translations and all of these things without needing RNNs and that's therefore the title attention is all you need I would I would invite you to google up this paper and go through it it's very it should be very readable after this. Please do let me know your feedback. And that is it guys. With that I'll end. Next time we'll talk about embeddings. A couple of these embedding techniques that I alluded to Word2Vec and GloVe and fast text in detail. And then the week after that we'll do BERT. One more thing guys, as a request, if you could please go to the Support Vectors YouTube channel and subscribe to it. I'll appreciate that. I don't know why today the YouTube live didn't work. I have to figure that out. Last time it worked, this time anyway i'll upload this video to youtube so please do go and subscribe to this channel and that is it if you oh i see a word here how does the word question how does the word chan come next instead of the word equivalent of jam yeah so this part the word equivalent of jam yeah so this part actually this is the main part is good that you asked that like let me and the other question is can I explain the Q K B back and how the weight are being calculated so let me review what I said these are the crucial questions see the point is that at the end of the day let us say the cow jumped over the moon enjoy right or something like that when you try to okay that because I did that thing over the paper itself wrote the notes over the paper let's go back here what happens is that the encoding is here right encoding is being fed in and what is being fed here also is the encoding of the output the cow interestingly at this particular moment if you ask this transformer what is the next word? Like what is it paying attention to? It would turn out that the thing that it would be paying attention to in the Hindi language is it would be paying attention to the word chand. In other words, it's very interesting. It's not a literal word, a sequential translation, because if it was a sequential translation, you would actually not need deep learning. All you have to do is a vocabulary in which every word translates you look at the translation and you put it together so for example if sentence if you translate this into hindi literally it would be Kudi upar vachand. That would look pretty contrived and it wouldn't look like a translation. The point of these networks is neural nets or deep learning is that it translates from a natural text in one language to a natural text in the other, which means that the words that they translate to, they have to be reordered. They have to be put in the proper context. And that is what these things do. This word will pay attention to moon, and it will point to the moon, roughly speaking, through all of this mechanism. It will point to the fact, and all of the sentence put together and the outputs of it will all start pointing that the next word is the moon.'s not jumped it's not the translation of jumped but it is guy the uh sorry chand it is chand the hindi translation for moon and that's how it goes up it it all happens because of the way you train it so now the whole thing is that when you train it what does it learn it learns the W matrices so in other words when you give it X I it will learn that given an X vector it will learn the Q vector now there are many Q sitting here there the eight of the Q sitting in this head there lots of cues sorry weight vectors sitting here there are lots of weight vectors sitting here so like any modern neural network there are a lot of weights that are floating around weight matrices, sorry, not weight vectorize, apologies, weight matrices that are floating around in this situation, right? In the architecture, there's lots of them. And they all get trained by how? They learn through back propagation by feeding it. For example, if you're training it, you're training it to learn that the translation, let's say that if this was a learning exercise in the learning phase, you would say the cow jumped over the moon, you would give it as input. And then you would tell that the right output should have been, . So it would be the Hindi of that, properly said. And so what will happen is in the beginning, the neural net will make mistakes. Those mistakes will back propagate. And then gradually, those weights will readjust. And it will learn how to pay attention to the right words in the right context. And so at the end of all the training, hopefully, when you ask it to now translate another sentence, it will do it the right way. So that's the explanation for these weight matrices. These are the learning parameters that it learns sort of in the model. Obviously they're not the only things, those are the attention components, but then there are all of these, the standard feed forward network and their weights and so forth. So the rest of it, of course, we are familiar with from our previous, you know, understanding of dense nets any other questions guys all right guys so if not then oh yeah so patrick is raising his hands please go ahead oh am I not allowing Patrick to speak I think I allowed everybody to speak I may not have Shankar guys just type your question I think this webinar thing is a a bit of a nuisance letting people where do i see the attendees here are the participants allowed to talk and i'll try to allow to talk everybody patrick allowed to talk. So guys, you can ask your questions now. I've unmuted as many of you as I could. Asif, this is Balaji. Yeah, go ahead. So the lecture started at 12, right? Yeah, it started around 12, 10 minutes past 12. okay i just wanted to ask you uh did you introduce the last week uh questions or last week lectures yeah i started by reviewing last week's okay i missed that part do you have a recorded version of this oh yeah it's already there the previous lecture is already on the website awesome thanks thanks and this one will also go on the website the last one was live YouTube live today for some reason my YouTube live didn't work so I'll have to physically I mean I had to manually upload it there but I will affect it thanks any other questions guys otherwise I'll stop yeah I have a question sir go ahead pretty much so other than finding the you know the sequence how to put the in the translated language how to put the words in sequence. Where exactly the word like the translation is coming from? I mean, the word from English to Hindi. So in the dictionary. So where is that part in the model that I'm trying to understand? Oh, where is that in the model? Okay. Good question. Let me go back to the document. Hang on. See, you're looking at this, right? The output probabilities. Right. What is it, the output probabilities? This is a softmax. So what is the target categorical variable that it is looking at? It is looking at the category of all Hindi words. Right. is looking at? It is looking at the category of all Hindi words. And it is picking out that Hindi word that should now follow a word guy. And it will pick up the word Chand. So I understand that part, sir, but where is the dictionary embedded here? Like, I don't know if I'm able to frame the question correctly but see when you just think of a normal classifier when you train a classifier you give it the input and the output no all right give it the output labels why your output label y set of labels now y i correct the hindi dictionary okay i see that's that so as if yes go ahead okay so so when we do the multi-headed attention so this this can also relieve the the issues of words that have that can hold multiple meanings that is okay so so if it's used as a. So if it's used as a verb, or if it's used as a noun or a predicate, it would be able to pick it up in a multi-headed. Right. Okay. Absolutely. In fact, it does really well, words that have multiple meanings, right? Apple computer versus Apple is delicious, which is completely able to pay attention or capture the context well sample sentence in the chat something like can you date the date of our date if it's all in the same sentence would it be able to actively actively single out that date is used three times in three different content actually there is a website let me see if I can pull it up there are many websites one of them is talk to transformer let's see let me share that screen so we can we can all play around with it together just one second this thing how do i get to that one second share new share and what do I want to share now talk to transformer this so are you guys seeing my browser now yes let's try that this is by the way using GPT-2 and as you are aware this week, so GPT-3 has come out, which is supposed to be 100 times more powerful. So we'll try this. Let me try this. The cow jumped over the... well moon is something uh odd but see here it is it's trying to complete it this is completion it's not translation the cow jumped over the ice eyes are back right over now that doesn't seem very useful uh let's say moon you would say in delight or enjoy let's see what it does let's see what it does oh goodness right so doesn't quite always work with the completions the translations are much better Let's try that. Oh, sorry. And yeah, that is it. And so you can see, see, this is not a perfect technology at this moment, but as of 2020, this is the best we can do. In sentence completions, it's not very good and in some sense but in in terms of this translations it does a pretty good job and let's try that translation transform Google uses transformers now model so So let us see if we can do that online version of the transformer. I suppose the best way to do that, the cow. And suppose we say, let's pick Hindi. Not bad. It missed the over part over it translated as but, but there you go. But once again, do you notice that the word Chand has come right after cow, which is the way it should in Hindi. Also notice that this English sentence has six words, the Hindi sentence has only five words right why because after this it decides that it needs to put a full stop it doesn't need to generate any more words and now you can you can try to confuse it you can try things that would be hard to call the basket of let's try that. And so the time just, it couldn't do that, just here meant right now. So it said Sif, it translated that wrong, but the rest of it, it seems to have done it pretty good. And you see the attention mechanism at play here the order of the words are completely different and it does manage to keep the attention in spite of a fairly long sentence or longest sentence so that's the nuance how do you carry the meaning or hold on to the context when your sentence is really long? And you can do that. You can try to confuse it even more that had been fed basket of carrots. And you can see there's a few who know Hindi, that this looks like a pretty good translation. Maybe I'll translate it also. Anybody, what other languages would you like to translate? Well, in the audience, I see anybody would like to have a preference? Asif, I translated into Tamil and it was awesome. It was awesome, yes. Okay, so I can't tell you, all right, there we go. And this is it, this is the Transformer Architecture at Play. Obviously, we'll get more into it, into BERT and GPT and so forth in the coming weeks, but have fun guys, this is it. This is the foundation, this paper is the foundation for a lot of things that followed in the last two years. And the N don't know how many of you are listening today. Let me see, is there a way for me to see that? 19, we have 19 participants. Thank you for being here. Next week we'll take up another topic. How many of you found it useful by the way? Did any, do you guys have any feedback or anything else I should do to make this better very useful as if thank you this is much again oh okay nice now she get nice to see you here oh yeah I attended last one too and thanks for doing this we we missed the the NLP all together in the last workshop yes yes we did remember we kept it for the deep learning boot camps the deep learning boot camp by the way all of these things that will do from a practical perspective the date was first I was thinking of early July I I'm not thinking of mid July. I'll start the registrations open. And by the way, if you are planning to join, let me know. Those of you who are planning to join that bootcamp, do send me a word. But all of these things will become very real when we do a lot of lab exercises with them and we code it up and so forth. Awesome. Since the webinar is starting next June, is this tomorrow? Yes. How good would it be for someone to join that deep learning bootcamp? See, the way I'm putting the deep learning one, by the time the 200 finishes, the deep learning would be catching steam because the first couple of sessions on deep learning, the only two sessions I think that overlap. And they are orthogonal to each other. They're different topics. You won't need, see, you need ML 100. You don't need ML200 to do deep learning. ML200 takes you deeper into all the standard algorithms except deep learning. I see. Hi, Hasil. In that case, I would be willing to get a link for that. Sure, you're welcome. By the way, ML 200 is starting tomorrow. So if you have any friends or anybody to recommend, please let them know. We'll start tomorrow. Go ahead. I think somebody was trying to speak. Was it Hari Priya? This is Hari Priya. Yes, please go ahead. Very useful. Nice. Was it Hari Priya? It's Hari Priya. Yes. Very useful, nice. So do you also take some inputs or suggestions for future topics or you already have a line? No, no, no. You can send him some suggestions. The only thing is that, see, in a way, right, I'm preparing the ground for the boot camp. The idea is that if I give a lot of these talks, partly when we get into the boot camp, we can move much faster. That's one of the motivations that we funnel people in there. And we spend a lot more time doing things rather than going over theory and we can refer back to this for theory. Sure. I will do the theory, but I'll do it at a much faster pace. Sure. So I think if other people are also interested, we probably can suggest time series analysis. I'm going to be in the list of these. Time series, yes. I will be doing time series, but much later. Okay. Much later. At this moment, I'm focusing a little bit on the NLP side. Then I'll be coming back and I suppose, see the first workshop, the bootcamp is the core thing. We'll do a lot of this optimization methods, deep learning, feed forwards, CNNs and NL transformers. So we have like a very focused time in which we'll go deep into it. So we will deal with time series, but as a sequence model, but limit ourselves to it. The reason for that is there is a dedicated workshop only for time series that is coming later in fall. I don't know how many of you are interested but and by the way you'll be surprised to know that this all of this is very relevant there also. Anything else guys? When are you planning for deep learning classes? Quite likely the third week of second or third week actually of deep learning so what I'm going to do is July I'm going to announce the bootcamp and take in registration but without being firm on the date I'll put a tentative date okay right it will be all Saturday and one weekday evening. So it's quite likely to be Thursday evening and all Saturday. The all Saturday part will be the lab part and I'll elaborate it. And Thursday evening I will do the theory, but I'll do it. I mean, obviously if I've covered it in these Sunday sessions. I'll go a little bit faster there. And the workshops. Go ahead. Hello. Yes. This tomorrow is the practical method, right? Not the theory part of ML 200. Yes, it is the ML 200. I have changed now because the subject has moved forward. So I'm going to do twice the number of algorithms. I will do some theory, but a lot of that will be oriented towards a practical approach. So in other words, it's a completely different course now. I see. Yeah. And that's what I wanted to be. I would take a few of these and go deep into it. This time I will move much faster so that I can cover a larger collection of algorithms. But more than that, how do you use those algorithms to solve your problems? And also we live in the world of auto ML and so forth. Right? And obviously, when I said, Okay, yeah, there's something I should mention, guys, the deep neural networks, those of you who have done it before, maybe it'll be for you to know, and there's a few who haven't wondering whether you should do it or not. See, traditionally, the way deep neural networks are taught is you're taught you know the usual back propagation optimizers then dense nets con nets and uh this uh the cnn's rnns and this now transformers etc actually i'm going to teach it in an entirely different way i'm going to start with the state of the art the state of the art at this particular moment are things like auto ML, automated ML and a neural architecture search. In other words, instead of scratching your head and designing a neural net and wondering which one will work best and continuously tuning the hyperparameters, there has been a lot of research these days and implementations in which you don't do that anymore. You train a neural net to essentially discover the best neural net to solve your problem. And that thing is true, not just of neural nets, but also of traditional, all the machine learning algorithms also. And we are living in a world of automated ML now. and we are living in a world of automated ML now. So the way I'm going to teach the deep neural nets will be from the perspective of automated ML, automated machine learning, and it will be from, like for example, deep learning will go heavily into neural architecture search, right, search architectures, and we'll also do things that are becoming very topical for example graph neural networks they're getting a lot of attention we are also talking we'll also talk about things like optimal transport theory and so forth that are gaining a lot of attention so um it's experiment i'll be teaching from a completely new perspective and teach different things. I will teach a back prop and all of that but I'll go much faster and I'll teach from a different perspective and different examples. Asif, I was wondering what your thoughts are with inferencing on low compute devices. I think aside from aside from other ML there's also move to tiny ML oh yes yes of course see once you have learned learning is the expensive process learning is where you need the big iron once the model is built for inferences you can put it on a, you know, those little embedded systems and devices. Totally makes sense. So for example, TensorFlow has a TensorFlow Lite and PyTorch can be scaled down. Inferencing is very, very easy. In fact, you don't even need frameworks. If you just know how to matrix multiply forward, technically you can just write it yourself, but those frameworks just make it easier for you. Inferencing is not computationally expensive. It's just matrix multiplication. And direct multiplication was never a problem. But can you teach, can you teach us or have a workshop on how to translate from a, from an implemented model to move into inferencing and low compute? In the deep learning, yes, you will do that. In fact, it's very project based. So those projects will make you do the end to end pipeline. See, that thing is broken up into three parts. The deep learning core, which is the first thing I'll teach you. It is, you learn about this automated ML. You learn about then the four architectures. You learn the basics, the backprops and optimizers. Then you learn about dense nets, RNNs, CNNs, transformers, and a bit about graph networks. We'll stop there. Then the second part is dedicated just to vision. For those people who are interested, there's another workshop only for computer vision, which is predominantly these days, obviously, convalescent neural networks, though now we are beginning to use transformers there too. So those two. Then Jay Shah, Dr. Vinesh Pramlalli, He or He project-based, keeping the theory to a separate evening and keeping a whole day for practice and doing it. So then there's a whole thing for creating production pipelines. This is where your stuff comes in. Once you've trained the model, how do you take it to production? And especially how do you take it to production on a whole slew of devices from big Ion to embedded systems. So you learn to do inference in JavaScript, you learn to do inference in very small devices, et cetera, and so forth. You learn to do it using Kotlin. That will be very useful. So we'll do all of that the first that's coming in July right that's that is not anything so it will have see the thing is that it is four days of practice morning to evening for Saturdays and four days of theory on Thursday evenings. So NLP will get one day. Okay. But only one day. But there is a dedicated workshop for NLP where the entire month is given only to NLP. In this workshop, you will in one day, you will do everything. You'll do text summarization, sentence completion, translation. You're getting the idea, right? You'll do a whole lot of things, all of it in one day. Whereas in a dedicated NLP class, you will do a lot of projects based on, for example, an example how this differs in the core part of the deep learning we will start with actually one of the reference things we'll start with is the forward net these days is a hot topic so we'll start with the architecture for covid net finding whether a patient does or does not have covid based on x-rays we'll study that architecture and we'll study more importantly how that architecture was created using AutoML, you know, a neural architecture search. Those are things that will be a start. Then the BERT architecture would come in the next dedicated MLP work camp? No, BERT will be introduced in the core, but we won't go... See, we'll do only BERT, right? And a little bit of GPT, ELMO. All of it will be, you know, one problem. You'll solve one problem using them. What you won't do is go deep into it. See, there's a lot. There's BERT, there's ELMO, there's GPT's, there's a whole slew of things you can do. But everything will just get introduced, you'll have enough to get started. But to do NLP, there's more to NLP than just this, cleaning the data, lemming, stemming, dealing with stuff, creating a whole NLP pipeline, et cetera, going to full production with an idea to a product. How do you do that? That will be in the NLP class. Correct. Thanks, Asim. Anything else, guys? All right, guys, thanks for your your time I look forward to seeing you next week and hopefully my youtube life would be working and once again a request if you have not subscribed to the support vectors YouTube live channel please do I'm stopping the recording thank you