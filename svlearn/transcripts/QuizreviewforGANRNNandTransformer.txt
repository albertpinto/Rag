 This is Saturday. We are reviewing the quiz on generative adversarial networks. So before we go into the quiz itself, I want to do a very quick review of generative adversarial networks. And obviously, this will be a very quick one because we have had a detailed session. So see, in machine learning, there are two kinds of models when you build. There are so-called generative models. There are so-called generative models and there are discriminative models. Now when you took ML 200 with me or till now when we did classifiers and so forth, almost always we have been dealing with so-called discriminative models. What is a discriminative model? See, when you have a feature space, so imagine that you have a feature space. This is your classifier. Are we together? And let us say that you're talking about cows and ducks, the illustration that we keep taking again and again, just to simplify. So let us say that these are ducks. Or you can think of it as a weight and size axis, the two axes. And so ducks will be clustered around here. Right? Somewhere here. A few large ducks. And then the cows would be clustered somewhere here. Big, big heavy cows would be clustered somewhere around here. So when you think of a classifier traditionally, let's say that you think of logistic regression, what they try to do in logistic regression, you don't directly try to understand the data distribution in the space. What you are searching for is the best decision boundary. So if you remember what you're searching for is this decision boundary. This decision boundary. Sorry, I should not keep this like this. Yeah. So decision boundary. Boundary. So long as you can make the decision boundary you can say that this side is the ducks and this side is the cow isn't it the purpose of discriminative models and classification is to find a decision boundary. The way that you say that is, given a point, so I'll first write it in English so that these notes can be with you. The purpose, the main goal of a discriminative model in the context of classification. And this is it. But another version is true for regression. The statement is broader than that. but we'll say it like this. The main goal of a discriminative model in the context of classification. Actually, the word in should start from here in the context of classification is to Find Decision. Boundary or decision boundary. More specifically,'s badly spelled. Feature space, Xi, given this point that belongs to your d dimensional feature space, what your main goal is to find the probability of any of the classes, so the probability of a class c alpha given this point in the feature space. So what you want to do is at this particular point, let's say that at this point, what is that? Suppose you take this point Xi in the feature space. The question that you want to ask is, for example, what is the probability of a cow, probability of a cow at this point in the feature space versus the probability of a duck at this point in the feature space, at the same point in the feature space. Are we together? And you compare these two probabilities, which is it more likely to be that at this point it's a cow or a duck? Whichever has a higher probability, you will say it is that. Is this a very simple review of what we have been doing, guys? This is it. But the way, so this is how a discriminative models work. So the main formula, I'll write it as that. And removing the subscripts, I will just say that the main goal is to find this. So discriminative models search for this. Discriminative models or algorithms search for this. That is the main way to think about it. Now, why am I mentioning this fact? Don't all models search for this? It turns out that there is another class of models. They are called generative models. And the generative models look at the world slightly differently. The same world, now let me pick a different color. Yellow I put for discriminative models, so maybe I'll pick blue for generative models. Let's take blue. So what are generative models? Well well a little bit lighter perhaps generative models models they ask actually a different question they say can we can we discover the underlying data data distribution distributions for each of the classes for each of the classes. This is the question they are asking. So they're basically saying that see if you look at this image and you have your cows and ducks, right, this is for cows, this is for ducks, and let's say that this is for cows, right, can we find this? Can we find and say that the probability, let us say the probability of cow, sorry duck here, we are looking at the duck here. So let's say that this is the duck. And this red one are the cows. So what you're basically asking is, what exactly is the probability function in the feature space where X again belongs to Rd? If I know the probability distribution, what are probability distributions? It is like where it's a measure of where things are more common. We are familiar with, for example, the bell curve distribution, isn't it? It just tells you that data is much more likely to be at the center, right? And it has a standard deviation sigma, variance of sigma square, right? So think of it like that. Bell curve is not the only distribution. There are many distributions. The distributions could be like this, you know, there's a right skewed distribution. It could be left skewed distribution. There are many distributions the distributions could be like this you know there's a right skewed distribution it could be left skewed distribution there are many many distributions it may be much more peak distribution and so there are many probability distributions it just shows that if i do a frequency plot of the points in some sense over the entire space where is where are the most frequent where are the ducks most likely to be in the feature space? And then you divide it by the total. So you get the probability function. So what generative models try to do is they don't ask for, is this a car world? They say that when we learn, we know where the ducks are, can we discover this function? And likewise, can we discover a similar function for cow, let us say? So you can see that these two will be different functions. But if you can do that, then so the question that you ask is that what is the probability of a probability that a given point in space is a cow, right? Or is a cow or a duck, C alpha, right? You're just looking for a probability distribution for each of the classes. Given a class, what is its probability distribution for each of the classes given a class what is its probability distribution are we together so you say given a class you search for generative models search for search for distributions that describe that reflect the data reflect the data or is most likely to have produced the data. To have produced the data. So this is the goal of a generative model. Now, what is the benefit of a generative model? Given a generative model, you can still do classification. You can still say that if you give me a point here, I can check the probability that it is a cow, I can check the probability that it's a duck, and I can still answer whether this point is a cow or a duck based on which has higher probability. But there is an advantage to the generative model. The generative model advantages, once you have figured out the probability, the advantages, let me center this. Advantage. Once you have figured out a distribution, distribution, you can now generate, generate as arbitr arbitrary amounts and arbitrary amounts of new data. Isn't it? Because you have a probability distribution, you just go sampling into the probability distribution and you can generate new data. Are we together? So that is one advantage and that helps you create new data from what you have learned. And this is the source of the fact that we could create new pictures, new things, etc. Once we know the distribution of pictures or images. So suppose you show a model, a generated model, a whole lot of faces, right? So it has figured out what do faces look like in the feature space, in that very high dimensional space. And so now it can produce a new data, a new face that is completely new. In other words, it didn't exist. And it is what it has learned about the data. That is the point of a generative model. The disadvantage of generative models usually is, it takes a lot of disadvantage usually is, usually computationally more expensive to train, more expensive to train, and usually needs more data to train. Compared to discriminator network? No, this is for the generative models. Generative models in general, compared to the discriminative models, usually need more data to train. So generative are a bit more expensive to train in terms of computational power and in terms of data needs. But the one additional advantage you get from generative models is that they can they can generate new data for you. Which has all sorts of advantages. For example, if you if it has really learned the data distribution well, you can now generate a lot of faces. You can generate a lot of new words, new things, and new paragraphs of text and so forth. And then it can help you in data augmentation for some other tasks that you are doing, things like that. So that is it. That's the advantage and disadvantage of generative models as opposed to discriminative models. So any questions? Sir, so when we give noise on what basis is it generating the output for the discriminator? So I'm not, first I actually thought the, because in the practical example, we use normalized noise as input rate. Yeah, so let me give you a review of GAN now. Generative Adversarial Networks. Let's look at this word. So a generative adversarial network is quite interesting. It takes a discriminative model and a generative model and it sort of makes them compete against each other. Right. So just to give you an idea, just to draw the picture that we drew, you have a generator and here you do something even funnier or or even more interesting, rather, not funny. You tell the generator, in a typical generative model, it will see the data and learn the probability distribution. What makes GAN, generative adversarial network, so remarkable that this is a very unusual generative model. You never show it the data. model. You never showed the data. So it never learns from the data, right? So it is like, it says, okay, go fake a currency. So, you know, if you have never seen what a dollar bill looks like, then you will put all sorts of things. Maybe you'll put a tomato and a camel there in the picture, right, or something like that. You'll put some random sketches or drawings, whatever, like that. I'm just sort of trying to oversimplify. It wouldn't actually put tomato or camel. It will be some random noise. But think of it like that, because it has never seen a currency. But then what happens is you ask the discriminator to decide whether it's genuine or fake and based on the mistakes you back propagate right so let's see what happens this is it going into the discriminative discriminator model and the and the and if you remember that the picture that I had given to you is this is the thief and this is the police. And so what you have is a pile of real data. So from this, what do you do? You take many batches of it and you feed it in here. So the discriminator gets data and it has to predict. Either it has to say real or it has to say fake. It can predict either one of two things, real or fake. Isn't it? Now what happens is, let's go through the steps of it what what input you give you give it some random random just imagine random so how do you need to your question why random and explain in a moment that's what I'm coming to so just as a review of what it does what happens is that step one so one step of learning is made of two sub steps two sub steps a one discriminator learns Okay, one discriminator learns, learns and the second is generator learns. And they learn when one learns, the other doesn't learn, right? So you do step by step. In step number one, so let's say that you are doing step number one, step number one, what do you do? Step number one, take a 50-50, 50-50 ratio of real and fake images. Fake images, who's producing them? The generator is producing images. And feed it to the... By the way, I shouldn't say images now, I'll be generalizing to data and fake data, ratio of data, mini-batches, to be more precise, and feed it to the discriminator to D. Right? Look for the loss in the output. Output. And use it to back propagate gradients, back prop the gradients to gradient descent learn step in the D. So this is what you do for the discriminator part and once you have taken this step for the discriminator then you take, maybe I'll put it here, number two. So D step, let me call it D-step. Then you take the G-step. In the G-step, what do you do? Let the generator produce a mini badge of fakes, fake lookalikes to real data, fakes, right, that are true data lookalikes, true data size. I mean, you should at least agree with the same size, otherwise it wouldn't work right so if the real data is 64 pixel by 64 pixel by rgb channel then you fake data also you have of the same size right pretend that it is true data and feed it to the generator. Feed it look at the lock look at the loss and use it to backprop gradients in the generator in G and and then and then gradient descent step namely update the parameters of g parameters of g so is is that clear and here also the ie update the parameters of d right so this is your classic gap, right? Are we together, guys? This is the gist of the idea. So this sort of way of looking at it, the loss function is a mini max loss function so those of you who don't have a background in mini max in simple terms what it means and tomorrow we'll do the math is it basically says that the one tries to minimize the difference between fake and true images the generator whereas the discriminator tries to maximize the difference between the two isn't it so they have opposite goals they have they they try to one tries to be very good at telling them apart, another tries to be very good at creating images that you can't tell apart. That is why there's an adversarial relationship between the generator and the discriminator in this particular neural architecture. So that is our overall summary and we'll use this now to go back and look at the quiz. Any questions before we continue? Sir, so it's clear now I understand that and these discriminatory function will find a decision boundary maybe one dimension two dimensional. It's just a basic classifier. Yeah. And the generative model will find not only decision boundary. It doesn't have a decision boundary. It finds it. It's looking for distribution. I understand, sir. So imagine that there are two letters, 1 and 2. The letter 1 is distributed in this part of your feature space yes sir right and the letter two is distributed in this part of the feature space yes discriminator will figure out is that it will figure these two distributions out yeah and now what happens is and now the purpose of the random so random what it does does is it will give you a line. And it will say deform. So what will happen is based on the values here, either this line is closer to this guy or it is closer to this guy. So the generator will find it easy to deform it to this or to this. And that is why you need it to be random. If the input is random and you have to shape it deform it into either this or this right whichever it happens to be closer to it will deform it into that that no that that i understand clearly sir that generative model will find distribution yes yeah so i was just getting a little confused. To find a distribution, how it will find the density of the data? Like, is it using something like we do in clustering? I can wrap data into a manifold. See, it is something like that. What happens is that, think about it this way. The probability distribution is a function function isn't it yes it is a function of the feature space where x r x is a point in the feature space d dimensional feature space this distribution is nothing but a function yeah so yeah how does it find sigma yes point is, and the whole thing about a neural network, remember that they are universal approximators. So you can take a neural network that it will approximate any function. It will learn any function. Right? Remember the universal approximator theorem. And therefore it will learn it. Give it enough time, a box of learning, and it will finally go and learn it. And this is the heart of it. That is all there is to it. The heart of this field. Actually, all of neural network is very exciting areas of research. And as with all great research, you know, once something is discovered and you look at it in hindsight, once you've really understood it, it all looks very simple. Asif? Yeah, go ahead. Could you briefly go over seeing Minimax last function? Tomorrow, tomorrow. That's what it is. All right. All right. Guys, I think there are no more questions. I'll go back to the, to our, yeah, go ahead. So my question is, well I'll reserve it for later, but my concern was just, can you show a regression for GANs for generative models later when we discuss the question? You know, there are many generative models. Naive bias, for example, is a generative model, hidden Markov models, and so forth. So we'll do that. There's nothing mysterious about them. They're very, very common. So we'll do that. All right. So now let's look at it. The input to the generator in a generative network can be noise with any random distribution. The main purpose of the noise is just to make sure that it hops around in the feature space, except from one distribution or the other somewhere or the other it picks it up that is that it picks a point from somewhere in the distribution space that is it the second is oh look at this this looks uh by the way when you look at this image, how many of you, before you learned about GANs, would have doubted that this is a real person? I would. Looks legit. Very legit, right? You wouldn't realize that this person actually does not exist. And how realistic it is, isn't it? So this is the state of the art. It could be either a fake or a gann. It's very hard to tell. I mean, the even thing is that one thing that I found remarkable is you can even see imperfections. For example, if you look carefully on the nose, they're like all human faces have minor imperfections. It has managed to put some character or some uniqueness to this picture too. The haircut totally looks like a Corona haircut, self-done. Self-done haircut. That's true. Left eye and the left side. Yeah, that balanced. That's true. But isn't it amazing that it can do that? Consider data. I mean, it's good, amazing and frightening actually. Frightening because there's so much of fake news, fake images, fake messaging that's going on all the time. It's getting harder and harder to tell what is true and false. In fact, there used to be a saying that do not believe, believe only what you see and only half of what you hear. Now we're in a world that we can't even believe what we see. So consider a data point x i, this is again using a standard notation, a data point in a point in a PD point in a D dimensional feature space, like y be one of the category classes I it's a categorical variable taking. Oh value, one of the classes, one of the class values see alpha I think I should fix the English a generative model tries to directly learn the decision boundary between the classes in the future space without needing to learn the data distribution of the class is that true guys that is false because it's not direct in fact that is what a discriminative model does not a generative model it is a discriminative model that tries to find directly a decision boundary and determine that in a given point what class it is when training the generator of a generative adversarial network, this is just knowing the architecture. You know that the generator just takes random noise. It doesn't matter what kind of noise it is. A generative model cannot be used for classification or regression faults. Of course, we can. Because if you have a probability distribution for each of the classes of course at any given point we can compare the two probabilities from the distributions for the cows and ducks and we can use it for classification likewise arguably we can use it for regression. GAN is a model of the type That tripped me up. It's a generative. It's a generative model. Why? The end result is it generates. Though I would say that I should have formulated the question a little bit better. I thought it was both because you got a generator question a little bit. You call it overall generated because at the end of it you discover a probability distribution and only generative models can discover a probability distribution but it is true that this there is a discriminator sitting in there but the overall purpose is generated okay. The input of the input to the PI really should reword this question. The input to the discriminator in a generative adversarial network can be either a mix of actual data and fake data that is remember the step one in which the generator learns or it could just be fake data from the generator, which is step two when the generator learns. So easy one while training the discriminator of a generative adversarial network, obviously, the discriminator input is an equal mix of a mini batch of real data and one of the fake data for the generator. So remember, we are training the discriminator. This is step one. So you give a 50-50 mix of real and fake data. The discriminator in a generative adversarial network is a binary classifier. Of course, all it is telling is fake or not or real. Generative models, when used as a classifier, directly search for a decision boundary between the classes. No, that is what discriminative models do. Generative adversarial networks use a minimax loss function. That is again true. I haven't gone into the mathematics of minimax. Consider data as this. This is very much similar to this. A generative model learns a specific distribution of each of the classes in the feature space. Very much so. That is literally the definition of a generative model. Next is 11. Hang on. This is okay. True. Distribution 13. A generator is a binary classifier in a generative adversarial network. No. That is what a discriminator is. is and and next is which of these is the classifier in a generative adversarial network the discriminator of course it is the one that discriminates and classifies while training the generator of a generative model the discriminator input is a mini batch of fake data with the labels pretending to be true levels right and that is it that is the quiz so i i want to understand sir you know so far uh whatever we have learned i understand that we can find the boundary right very quickly with the model but now we are talking about here dissecting the data and finding the distribution like it's a sphere see that is a deeper level thing you can do that's why it's computationally more expensive but if you can do that you know it's almost like you've figured the data out and you can get more data you know of course if you know the distribution and then that's like you can create you can do something. Yeah, but what what algorithm is doing it that that Oh, there are many algorithms like for example, this one you saw it was just using a neural network and you're using a discriminator to train the generator. But simple things like naive bias is very easy algorithm. I'll talk about it. Hidden Markov models and there are many many generative models i i have this roadblock here i keep on thinking that it once it reaches the surface it won't be able to poke through the thing and but here you're saying that it will not only poke through the data and it will find out like central has more density and then the you know as we go it is that yes. Vaidhyanathan Ramamurthy, it's very interesting and that's why you also say see generated models are essentially like gan for the GAN fundamentally right when you figure out the distribution in many ways, what you're saying is. Vaidhyanathan Ramamurthy, It is more like unsupervised learning in some sense. Because for a GAN, you don't really care whether it's a picture of one or two or three or picture of a man and a woman. You just say, create a picture that is realistic. So that's why generative models initially are considered in its basic form are unsupervised learning. You just say, go learn from the data this particular again then later on people have used it for semi-supervised and learning and reinforcement learning and so on and so forth so we'll get to that all right so let's go so this is the paper let's go and look at the other quizzes quiz number okay so this is here so guys we have seven quizzes in this uh first workshop the first part we have had seven quizzes zero to six go ahead as if uh just to piggyback off of what supal said when would it's considering that generate these generative models are computationally expensive when would it make more sense like for for regression problems that you mentioned in the quiz, when would it make more sense to focus on a generative model? See, today, we don't worry about the computation cost because there's a lot of machinery available, the computation power available. We do worry about the data need. Typically, if you're trying to train a GAN, unless you have some pre-training, remember transfer learning, it takes quite some amount of data or iteration before it figures out. So it needs more time to learn. I wouldn't even say it needs a whole lot of data, but if the dimensionality of the data is high. Images are very high dimensional. So you can't train a GAN with just some 10 images. Maybe it will pick up some little bit, but it won't be that realistic. We need a lot of data to make it realistic. See, look at it. Why are people finding what are the stellar use cases of GAN? The earliest stellar use cases of GAN were with images and with words. Why? Because images are plentiful, you know. They are quite literally trillions of pictures online. And likewise for words. Words are plentiful. The whole web internet is mostly words, right, and then videos and pictures. So it's easy to train. But if you suddenly say that I'm going to use GAN to figure out something where the data is sparse, use it for cancer research or something, and for a rare form of cancer where there are just 30 patients available in the world, now it's a little bit trickier. I mean, you may, maybe they can find a clever way of doing it, but it's trickier. So when the data is passed, for example, the biation methods do much better. You take a sensible prior and then you work with that. Or you use some form of pre-training. In many ways, you know, the transfer learning that we do and fine tuning it, you take a pre-trained model and you do a last mile fine tuning. In some sense, it is a form of bias in learning because you're taking a very informative prior. You say that the solution is something close to this. And then with a small amount of data, you just do the last mile learning with the fine-tuning of the model. That is my perspective. So that's when we are looking at it. All right, guys, so let's go ahead. That's one last question. But for, let's say, weather data and regression, something like that, can we still use GANs efficiently? Or is it a technique that is better left for other methods? You typically wouldn't use GANs. You could. But see, generative models are used often, but not necessarily GANs. GANs purpose is to fake it, fake a weather. You could do that if their use case requires it, but generally your main purpose is to predict, right? So you can use a generative model or a discriminative model. It doesn't matter, whatever? So you can use a generative model or a discriminative model. It doesn't matter, whatever works for you. Now, typically what happens is weather simulations, they use both actually. They try to figure out the probability distributions of some things and they try to use discriminative models also. Weather prediction, in a typical weather prediction, you would be surprised. It is an ensemble of thousands of models. It is never one model. These models are computationally brutal, and it's an ensemble of 1000s of models. That is why we can make such such accurate weather prediction. You know, the joke used to be 25 years ago, that on a sunny day, only the weatherman is walking around with the umbrella. right? Because his model say it is going to rain. So, and whatever he predicts never comes true. But today, today, today it's not like that because with the AI coming in and all this computational power coming in, the weather predictions are very accurate. But once you have the actual weather data and then don't, and you know the distribution then then you can create lots of data to train the model right yeah see weather is one of those things yes you can so you do a lot of weather simulations in some sense they are in in some sense they're generative models you know the parameters of the game you have some equations see whenever you say you have an equation for something what do you have what you have in in a very rough sense is you have the distribution the probability distribution and you know how to now generate data yeah right so that's one way to map it it's a generalization of probability distribution think of it like this if you know the equation behind the activity now you can generate as much data as you want yeah so there is nothing left that that's the scary thing here yes yes but there is a catch to that so i'll tell you a story uh when i was in college in the iit but there was you know people are young and they don't feel like doing the lab so there was a physics lab You know, people are young and they don't feel like doing the lab. So there was a physics lab. And this was first year and people are supposed to do the simple, you know, the simple pendulum experiments and compute the value of gravity, the value of g. So most people were doing all sorts of pendulum experiments and creating a lab data set. But there was one guy who was goofing around the whole time and then suddenly he just sat down and cooked up data. Then he was going to submit it and even the lab assistant, you know, the lab boy who was barely educated, he looked at the notebook and he said this this is not good so he said why is it not good he says well this is not good right the professor will catch you so he could figure out that this data is fake and why can't he figure out because he used the formula exactly there was no error in the data i got the 9.8 i i in my class the 9.8 in my class grade yeah see when you take the value 9.81 what people forget is that value is only true on the equator at sea level yes it's not true but it was great i don't know what we did but i got knife put it but when you yeah when you're reading the data exactly uh be careful all right am i sharing my screen now once again i unshared it because just to preserve the privacy of people who have taken the quiz but uh am i sharing the screen anymore we see you okay now you see the screen okay yes you see the screen. Yes. Right, so this is for the transformer. Once again, this is a straightforward quiz I hope. Recurrent neural networks inherently do not have the potential problem of the potential of vanishing gradients while training. Actually they very much have that problem, right. The opposite is true. You have to really worry about vanishing and exploding gradients and do something about it for answering questions based on a given text rnms is the preferred approach since transformers do very poorly on the task well if you learned anything we learned that transformers are absolutely rocking the world at this moment and doing just about everything better all right so it's false. Recurring neural networks have the potential of exploding gradients, very much true, and therefore one must put measures to address it. You do clipping and so forth. A GRU is a simpler, newer, faster gated RNA compared to LSTM. This is something I mentioned in the lecture, it's true. But it might have slipped you. So if you go back to the lecture you see that statement there next is give me a moment and bring this here one of the limitations of RNN is that input sequences such as sentences must be fed sequentially, one word at a time. We're not seeing this. I apologize. I moved it to my writing pad. One of the limitations of RNN is that input sequences must be sentences, sentences must be fed sequentially one word at a time does RNNs do not benefit from any massively parallel computing hardware available that was a core limitation of RNN people really didn't know how to take care of that and it was obviously the NLP crowd was looking at envy with envy at the computer vision crowd which happily were doing the computer vision task with these cnn's in a massively parallel way you could patch you could feed batches of images and one image would be processed in one shot transformers can be used to predict the or missing word at a location in the sentence. True, false. It's one of the tasks you do. Remember we did that, fill in the blanks. We actually did a lab on that, so this should be reminding you of the lab. If one is using an encoder-decoder implementation, implemented using RNN to perform machine translation of text from one language to another, which of the following is true? A sentence is fed one word at a time sequentially till the end of the entire sentence is fed into the encoder. Now that's not true. Entire text is fed in now, hopelessly not. For image processing tasks, the appropriate tool is CNN. Therefore, while transformers are awesome at natural language processing, they cannot help or be part of an architecture to, for example, detect objects in images. You guys know that that is false. We, in fact, covered the data paper, right, object detection paper using transformers, detection using transformers paper. So on, by the way, do you guys remember us covering that paper? Yes. Yeah. Yeah. Yeah. In the attention is all you need paper that introduced the transformer architecture for deep learning. The transformer contains the following components. Well, it is, this is a tricky one. When you're doing it, my expectation was that this will force you to review your transformer, pull up the transformer diagram, and then inspect it to see which of these things are present. So obviously it doesn't contain CNN or RNN, but it contains all these other things, layer normalization, feed forward layers, multi-head attention but lstm and gru are augmented forms of rnn true false true in an encoder decoder architecture for machine translation for first the entire sentence is encoded into a context or thought, then the decoding of the sentence happens, which is true, irrespective of whether it is RNN or transformer. Then, an LSTM is essentially a form of gated memory where there are gates to help in update, remember and forget. Updates, remember and forget. This is literally the definition of LSTM. So it was an easy one. We can use the transformers to classify text into a finite set of classes. We did the lab, so they should be straightforward. Recurring neural networks perform very well. Sorry, very well. Sorry. Very well. I had a pretty long Friday at work, so today I'm a bit recovering from that. Recurring neural networks perform very well at encoding and decoding very long sequences. Now that's a limitation. The Dory problem. Yes, yes. They forget the Dory problem, exactly. That's a very easy way to remember Dory. RNNs are Dory. While performing machine translation, one of the great strengths of RNN is that it can be fed batches of sentences in one step with all the words of all the sentences fed in parallel and produces the translation. This is exactly what an RNN cannot do. If one is using an encoder-decoder implemented using a transformer to perform machine translation of text from one language to another, which of the following is true? So here you have to pick. So let's see why the other ones are not true. The entire text with all the sentences, however many, must be fed at once. No, transformers don't have that requirement. You can feed many batches of text but not required. A sentence is fed one word at a time sequentially till the end of the sentence is used. Interestingly, while in RNN that is all you can do, feed one word at a time, a transformer simply will not take one word at a time. It won't be able to learn because it's a self-attention mechanism. You must feed it all the words of the sentence because the whole point of self-attention is it needs to figure out which word a word is paying attention to in the sentence. So unless you feed the other word, how will it pay attention to? So that's the point. 17. Transformers are not capable of sentiment analysis. Of course, that's false. You guys saw in the lab. So I tried to make some arguments to confuse you. But you know that in the lab you did it. NER in NLP stands for, well, it stands for named entity recognition but i tried to confuse you with no error rhymes and newly enhanced recurrence and whatnot i think this one none of you got confused any are most of you got it right question 20 transformers can brew coffee and serve it to you after its inference of your specific taste it to you after its inference of your specific taste. Not yet. Transformers are magical, but they haven't reached that stage yet. That was just supposed to be a joke. While transfer learning works well with CNN, architecture such as R will be task for me to be tested is not possible to reuse a pre trained transformer representing one of the significant limitations of this architecture. Dr. G R Narsimha Rao, If you did the lab. You know that all the time we use pre trained transfer learning and featuring transformers. So obviously, this is false. So all of these questions, if you have been through the, see, these are not easy questions. If you imagine that you were asked this sort of question in a job interview or in any scenario in which you just had a superficial understanding of these things, would you agree that it would be very hard for you to answer these questions? If we hadn't learned about it, yeah. And in fact, just superficial learning by skimming Google doesn't take you that deep. So I was hoping that... And that's how I framed the quiz, to prove how deeply you have learned the topic. So if you have learned deeply, then you'll do well. Can you go to question 16? Yes. Now, so the concept here is for the transformer, you feed the entire sentence all at once because that's how the attention... One sentence must be fed completely at a time. Okay. So now when we go to that uh option two which is a batch of sentences can be fed to the encoder yeah it is just a mini batch of data remember one sentence is one data you can feed it a mini batch of data because it is processing each sentence separately yeah but the point that is trying to uh make sure i understood well is so when you feed a batch of sentences, the attention part of the algorithm still works on a sentence at a time. It cannot go beyond sentences. Yes, it does not hop sentences. Except for one thing, when we use the word sentence in natural language processing, our definition is a bit more general. See you define what a sentence is by putting a separator at the end. And I see a list at the beginning, a separator at the end. So you can actually treat a paragraph. What in English would be a paragraph made up of many English sentences as one sentence. Okay. Right. But that'll make the network bigger. That will, that will, yeah yeah that will cause a lot of the data size is bigger so the attention will be all over the english sentences but from a as far as the transformer is concerned to it the paragraph is one sentence and so then you can pass in a mini batch of many paragraphs each acting as one sentence and that is one interesting aspect so for example your definition of sentence because is based on the notion of a separator, right? So if your separator is full stop or exc a file then well entire file is a sentence right so that is a generality of definition that you have to be aware of that when we say a sentence in nlp we may not mean exactly the same thing as an english sentence and since our coming six weeks are all nlp you will do a lot of nlp, at least for four weeks. Okay. All of these concepts will become deep. In fact, what you will do now is nothing but NLP for four months so that at the end of it, I mean four weeks, so that at the end of it you come out strong in that. Okay. Thank you. All right guys, so this is it. This is quiz number two and I will stop the recording for this you