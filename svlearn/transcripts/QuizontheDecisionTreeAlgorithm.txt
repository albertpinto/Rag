 Let me know when you guys can see my screen and let me make the fonts as big as possible. Yeah, I can see my screen and let me make the font as big as possible. Yeah, I can see. So you all know the answers, but I'll go over it. What are the typical uses of a decision tree? A decision tree we use for both regression and classification. That's why we call it, people often call it CART. Classification and regression the algorithm right it can not only be used for classification and regression it can be used for a dimensionality reduction and for clustering though users in those contexts are not so widespread and generally people tend to use other algorithms for classifier clustering and dimensionality reduction. Word for classification is very very popular. It is also used for regression. So that's that. The second question is what does Gini in the decision tree algorithm refer to? So by the way how many of you went for C in honor of an old school chum named Ginny? That was just entertaining. Right, okay. And so it's a thing that actually I myself learned by teaching this workshop in a previous time. I went with Ginny and then I learned about this great mathematician, a statistician and sociologist, Italian, in 1800s. So it is his work. It's an impurity index. Now what is information gain? I'm very proud actually to say that the information gain question now all of you got right because in the previous survey it was the question whose understanding like when I said how well do you understand that it had gotten the lowest score so I'm very happy to see that we finally got information game nice the even this loss function very pleasantly surprised that most of you got it right so in the loss function the terms have this term is the sum squared loss you see the residual squared and summed and then alpha is the regularization parameter and P is the number of nodes in the tree what you are basically saying is that the more the nodes in the tree the bigger the penalty oh by the way is it plus or minus okay I might have made a mistake it might have been a plus depending upon how you interpret alpha but basic point is that if you increase T too much you will actually it should have been plus most likely okay if you if you in the number of notes in your tree is high you penalize for that this is just a sum squared error no it is not enough because then it will overfit this is the part that introduces pruning in the tree then this is a question that keeps coming up people say decision, is it inherently more powerful? And in general, will this outperform a linear or polynomial regression? The answer to that is actually no. There is something I keep talking about, the no free lunch theorem. No free lunch theorem is one of the, I don't know how much I talked in this particular season, but it is one of the uh i don't know how much i talked in this particular season but it is one of the great landmarks not just of machine learning but in the theory of human knowledge itself like it is in the same class as gordel's incompleteness theorems very very powerful it came about towards the end of the century and what it basically says is that all these theorems say is that no one algorithm is more powerful than any two algorithms if you compare there are situations where one will do better there's situations where the other will do better in the case of a decision tree decision tree you notice that it divides the feature space into regions so any decision boundary it makes will be sort of staircase like step like and so if the underlying ground truth is a linear will be sort of staircase-like, step-like. And so if the underlying ground truth is a linear decision boundary, it will do extremely poorly. Well, I don't say extremely poorly, but it will approximate it. I take that back. It will approximate it. And in the asymptotic limit, where you have a very deep tree, it may get it right, but a simple straight line linear regression will outperform it. The same is true when you do polynomial regression. Poinomial regressions represent, well, some sort of a surface or shape in the feature space, a small differentiable shape. Dec decision trees make edgy they have sharp edges their decision boundaries are anything but smooth and differentiated is the opposite so that is that both for and this thing is true for classification as well as regression if the underlying ground truth is a straight hyper plane that relationship is hard for their decision tree to capture though in the asymptotic limit of a really deep decision tree it can it can make very small steps and get across so ask if what is the asymptotic limit oh asymptote means in the limit of very large numbers so that's like minus infinity to plus infinity or? The depth of the tree you know. Oh okay. It goes to yeah I should clarify that. When the size of the tree goes to infinity, very very big tree, you realize that then those steps will become very tiny steps. They will begin to approximate a straight line. Do you want me to illustrate it? I could do that by making a drawing of it. Yes. Yeah, let me do that. I'll share. I have some doubt about this question. So basically, yeah, I mean, you can go ahead. I can put my question later. Give me one second. Let me answer this one question and then we'll come to you so what we are saying is imagine a feature space like this and imagine that your decision boundary should be aligned I'll just take a different color to be more so suppose you have a decision boundary that should really be this this is the underlying So suppose you have a decision boundary that should really be this. This is the underlying ground. You realize that a logistic regression will quickly discover it. cover it but in the case of a tree what will happen is but for a tree see what will happen it will start create because it makes rectangular decision boundaries it will go like this. Remember it can only split along the axis. Anil, do you see what I am talking about? Yes. So what happens is if you increase the number of regions the splits then there will be more and more tinier and tinier steps and They will gradually approximate the red line. So the steps here The steps in the Limit of approximates the lines, the true decision that's that's what I'm saying I think you had a question yes so I was thinking that in decision tree like in general it will be more powerful means you will get a better accuracy factor but of course in a particular case like we saw in case of the river data set right that your basically polynomial regression and doing all this feature extraction right in were the best accuracy. Right. In fact, if you remember what happened in the decision, in the river data, it was the worst algorithm. Right, right. It was the worst performing one. And so that is a very deep truth that unfortunately most people doing data science don't know. The no freelance theorem. It's a very powerful result in this space I would say it's one of the things I emphasize a lot and should be emphasized a lot no one algorithm is more powerful than another get out of the mentality that people write research papers as such as and say look how much more powerful is my algorithm compared to the other existing algorithms it shows a level of ignorance contextually it can so for example for a particular task let's say image net image classification some algorithms certainly outperform for a specific context you can have a hierarchy of i mean you can have a ordering of performance you can have the best performing the next best and so forth so for example for image net a lot of performance. You can have the best performing, the next best, and so forth. So for example, for ImageNet, a lot of innovations have come because every year somebody comes up with something better. And now ImageNet has become useless because accuracy is almost near perfection. But for example, when Microsoft came out with the residuals neural network, it was a breakthrough. Then when BGG came out, it was a breakthrough. Right? But then when BGG came out, that was a breakthrough. Then when, you know, all of these ideas as they keep coming out, they are breakthroughs. So yes, for specific situations, one algorithm can be more powerful than another. But in the general, you cannot say that this algorithm is more powerful than the other without the context of data. Asif? Yeah. What about the CART algorithm? CART is decision tree classification and regression tree. Isn't it better than just simple regression and logistic regression? That's what I'm trying to tell you remember the lab we did on decision tree on River what did it show a decision tree underperformed our feature engineering and it underperformed the random forest for that data set and for the flag data set and for the I think also for the California housing California housing I think it came to us with the same values. Why doesn't it just like not divided into regions and just reduce it into like a simple regression? See, the thing is, this is what I'm trying to say that because it sticks along the feature axis it has it can never never beat a diagonal decision boundary if the diagonal decision boundaries the truth or any any slanted decision boundary because you'll have to see you'll have to approximate it here do you see that you'll have to approximate it here using steps. You see that, right? Yeah. That's the main problem because underneath it there you can imagine that there are these regions, right, regions in which suppose it was a classification problem you're saying this is plus plus plus and the other side of it you're saying minus minus minus so that's how you rectangular you have to deal with rectangles and the rectangles have to be aligned by the axis so you don't have a choice that's a problem okay Okay. All right guys, so should we continue? So Asif, similarly related thing. So like for the random forest, we said that, so like the split, like no, so for the decision tree, like we made the split along the axis so there was a case of like in our data science like the Saturday data set class we studied about isolated random forest where instead of making the split along the axis like we made it along an angle yes yes so that is is that like similar intuition or yes so the idea is that like similar intuition or yes so so the idea is that you can actually split along different directions and you have adaptations of a decision tree but that makes it another algorithm which we didn't cover okay and that algorithm may be more suitable right so the point is that you know no one algorithm so suppose you take that algorithm i can easily show situations where it could not work even with slanted decision boundaries you realize that any smooth curve is never representable with this because it will always make these edgy decision boundaries step like decision boundaries isn't it it's the nature of it. It will not do very well. Suppose I give you a curve like this. Now what do you do? Or a hyper surface like this as your decision boundary. You realize that now you're stuck. You can never do that because the decision boundaries are inherently non-differentiable at these points, you know at this edge points You need a classifier that builds essentially differentiable Decision part of the smooth decision boundaries in simple terms and this is and this class of algorithm doesn't do that Passive yes, go ahead Yeah, so Is it fair to say right if you see a data set but somewhere locally slanted other where it's straight decision boundary here you can apply logistics other you can apply decision. That is called piecewise. Piecewise what you do is you do that quite often you apply one set of algorithm in one region of the feature space another set of algorithm in another region of the feature space so I'm giving you a big hint look at the California housing data set yes remember last time I talked about our outliers we did not do outlier treatment but since you took that boot camp if you recapitulate what we did in the boot camp was that we did outlier treatment and quite often you you you not only detect the outlier you say that for outliers I'll build a different model yes right or for certain or you can do piecewise division or even of the in layer points if you notice that the behavior is materially different in different regions of the feature space, you make different local models. That maybe is a good segue into a topic. Suppose you have a decision, let's say that suppose you have this as a curve. What you could do is you could divide this into regions, piecewise regions. And for each region, you can make a straight line. Do you notice that? Suppose I do this, this, this, this, this. And so you piecewise, you put these things together, small regions of the feature space. It is a technique, I didn't teach it because, see machine learning is filled with so many, many algorithms. Do people use it? Very much so. In fact, the approach of Lois and using a kernel, when we do kernel machines in this case of output in case of classifier the output is majority of or output is an average of classification you can't take an app majority good so guys I can I now go back to the quiz any other questions so yeah just one common say in the previous case where you are drawn that linear when with the rectangular like steps so in the limit that this width of the rectangle goes to zero then it will approximate to the straight line but you realize that you have severely over fit to the trading data and that and hold that thought in your mind because we are coming to that question, so the next question is literally that the question was one of the inherent advantages of the decision tree is its robustness against overfitting to the data. In fact, it is one of the demerits of a decision tree when decision tree came out it overfit people realize that it could fit to nonlinear data. to overfit people realize that it could fit to nonlinear data nonlinear like it could fit to nonlinear surfaces in our decision boundaries and lines for regression surfaces for regression but it overfit so they said no problem will prune the tree and there was a huge set of activity creating smarter and smarter pruning and faster pruning implementations the trouble with pruning and faster pruning implementations. The trouble with pruning is even if you prune, you tend not to get rid of the variance errors. It still remains residually overfitting. If you try to over prune, then the bias error shoots up. So you're left at a hard place. Your minima between, you know, the optimal point between the bias variance trade-off still leaves you with a lot of errors. And most of those errors come from your variance part, the overfitting part. Which is why, if you go back now to our random forest, which you covered this week, what is the advantage of random forest? You take a lot of trees. And because you take a lot of trees and because you take a lot of trees it has you know it blurs the optic it is almost like you're looking at the picture through a camera in which you deliberately defocus a little bit right so you smooth out the very very small steps or things like that so you end up decreasing overfitting you never get rid of it actually. The inherent problem with this trees and tree based algorithms, even ensemble of trees is, it is very hard to get rid of overfitting. So there's random forest, better than that is extremely randomized trees and so forth. There's a whole set of improvements. It was a cottage industry, like lots of people created lots of variants of the decision tree and samples. Vipul Khosla, Ph.D.: And they all do different degrees address the problem of Vipul Khosla, Ph.D.: Overfitting their work. I mean, in very popular particular data. If you go and look at cable, you notice that most of the winners of tabular data. notice that most of the winners of tabular data are random forests and boosting algorithms like Gbootcat, this, that. They are all tree-based algorithms. So trees are here to stay, but you need to not just use the decision tree, you need to use ensembles of them. How will you know all this once if you start doing black box model like deep learning you will never realize all this so now I'll share back to the quick oh am I already sharing the quiz no you are yes so here we go so the next question guys is decision trees and inherently more as it will outperform logistically decision boundary that is not oh I said we lost you in between sure can you hear me now yeah okay thing is this question that decision trees inherently more powerful that in general it will outperform logistic regression I hope I don't need to explain anymore we saw that it's not true if your decision boundary is a hyper plane at a slant decision tree will never outperform a simple logistic regression and it again goes back to our no free lunch theorem and now to the last question guys which is entropy the probability of is P then what's the definition of entropy? So entropy is defined as minus P log P. The minus sign is to this because the log of a number of probabilities are between 0 and 1. So the log of that will always be negative. So to make the negative into positive you use this minus sign a P log P. Now log to the base what? Depends upon which community you're talking to. If you're talking to the decision tree or this sort of community, they'll say, okay, log to. Or if you talk to the physics community, they'll say log, natural log. Information theoretically, they'll say log to. So those things we can sort of minor aspect, but that's about it. So guys, that was the quiz for today I hope you found it