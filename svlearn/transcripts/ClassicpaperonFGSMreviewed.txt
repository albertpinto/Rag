 It says it is being streamed. All right, folks. Good morning. It's a beautiful Saturday morning here. Sunny as always in California. Perfect weather. Neither too hot nor too cold. Now for those of you who are from the East Coast, I'm sure. Good morning. It's a beautiful Saturday morning here in sunny, perfect weather. I'm getting my own video. Yeah, it's fine now. It's fine now. All right. So a little bit of a background on what we do since I find some new folks have joined. Every Saturday at this time, we take one paper. Generally this paper is suggested by somebody in our cohort, anyone who is attending these talks, Saturday talks, can suggest a paper. Usually I recommend that they suggest a paper at least 24 hours in advance, so I know which paper it is that that you would like me to go in detail over. It could be any paper so long as other people also feel it's a paper worth covering. We do cover that paper in depth. And so it goes. So if you guys have any wish list, just keep, now we obviously have a Slack group internally here, but I suppose this is a great time to open a Slack group. One suggestion I have Vectors has a meetup, which I am not very regular in updating. But yes, these talks are announced on the meetup, Support Vectors meetup. Unfortunately, they are announced at the last minute. For example, the topic of today's talk, I just announced it at 10 o'clock when the talk was literally starting so not a very good habit of mine, but people here have gotten used to the fact that every Saturday 10am, there is a paper reading. Perhaps, and the link is on published there. I will be monitoring that site to see if anybody is suggesting any new papers to cover, or just Slack me or message me or reach out to me by any one of the many means and say this is a good paper I just need one more vote so if you suggest a paper somebody needs to second it and say yes I also would like to see this paper covered in depth and the moment you do that it becomes the topic for the next week. And so we have been moving along in this way. The sheer democratic nature of the process or the collective nature of the process means it's a bit haphazard. Till quite late we don't know which paper we are going to cover and which one people are suggesting. The only way to know that in some sense is to be into the slack channels and watching the discussion which one we are gravitating towards and to make the matters worse i'm not a terribly organized person and sometimes the papers that i read can be rather surprising but like for example today's paper is a pretty old paper practically a decade old it 2015 paper. It is on a topic of adversarial attacks. This topic is quite important now. And so now I'll go into the topic. And for the rest of the hour, we'll cover this. Now, the way I would like to hold this talk is that first of all, we do the paper reading, then I open it up to discussion. These discussions can get very lively. Last week for example the discussions became so lively that they went on far longer than the paper reading itself especially because the topic was very close to people's heart. It was AI in healthcare. Today's topic is perhaps not so, it's a little bit more theoretical, but very real in terms of impact. And I'll cover this. This would be the first of our papers on a topic called the adversarial attacks and defenses on machine learning algorithms. So what does that topic stand for? See, we build models and when we mean models in machine learning, we mean something that for a given input will emit an inference, a prediction. The prediction could be, for example, in regression, it could be a number. How much ice cream would this young entrepreneur sell on the beach today? Or it could be a type. Is this the picture of a cat or a dog? Right. So that is classification. We will put for simplicity. We will limit ourselves to very obvious down to earth examples. Now, these classifiers, these aggressors, and are these models in general, it has been known for a long time that you can do attacks on them. You can fool them. It is surprising how easy it is to fool them. And this part is important because it has far reaching implications in the enterprise. It has become quite common, for example, today to just download a model from Hugging Face or from one of the open source sites and use it and work. You have to know that when you do that, not only you're using the model, but a malicious agent knows that model. It's an open model. You are using the model, but a malicious agent knows that model. It's an open model. It can do what is called a white box attack because the person knows all the weights and biases that are associated with your model. And the model need not just be a neural architecture. Even basic models like softmax classifier, like simple things, decision trees, et cetera, everything can be attacked. So whenever you bring in a model, you have to be aware that somebody may have trained adversarial examples that will fool your model, quite easily fool your model. And in some sense, this series that we are starting on the dangers of, this is a broader topic on the dangers of using AI without caution. How should we use AI responsibly, knowing that we live in a world in which bad actors or adversarial attacks is a given. It is a given that it will happen. And the state of the art today is that these adversarial attacks are far easier to make than to defend. So in any situation where attacks are easier than defense, there is a huge temptation on bad actors to launch such attacks. And you see it everywhere. I mean, for example, deepfakes. It's far harder to detect deepfakes than to create deepfakes. So the world is inundated with deepfakes. And a similar situation is true on this. But I feel that in some sense, there is greater awareness about deep fakes, and not so much awareness about adversarial attacks and defenses. And we'll talk about it. So to launch this topic, I'm going to perhaps one of the, I mean, the history of this discovery that models can be attacked goes way, way back. People have known for a long time. Now, the general feeling amongst AI experts is that the more complex you make a model, the more nonlinear its thinking processes in some sense, the easier it is to attack. It actually turns out that it is not the complexity of the model, it is the converse. It is the fact that at heart, even the linear models can be fooled, very simple linear models can be fooled. And the problem is these models that we build, they are a past-stage, they are stitching patchworks of linear models. Little, little pieces, locally linear pieces, weaved together, patchwork, quiltwork of these little linear models. That thing remains. A giant complex neural network at the end of it is still building locally linear pieces and doing it together. And whenever you have that situation, it becomes pretty easy to attack, as will become obvious when we sort of read this paper. Now, this paper is, of course, by thean, A good fellow and his colleagues, all of these are big names. Ravinder Kaurudarajan, And I didn't know how many of you listen to a good fellows thoughts like they are absolutely wonderful he has. Ravinder Kaurudarajan, He has a lucidity of style that makes it very easy to understand whenever he gives a talk and I tend to like it. And if I'm right, he's of course, one of the coauthors of the classic textbook, Deep Learning. And if I'm right, I don't know if I'm right or not. Somebody please correct me. Did he or did you not share the Turing Award along with Jeffrey? He did, right? Not sure. Not sure. I don't remember whether he did or not. So obviously, but in any case, he's a legend. He's one of the great legends in this field. So, and then all of these other people also, Cezade, I mean, see, Gedi, I am pronouncing it badly. I'm pretty sure. And all of these are big researchers in this field. So what are they saying in this paper? I'll just go through the abstract carefully. This paper, by the way, is not a very technical paper, but it is very elegant. What it says, the message is very clear. It says, several machine learning models, including, and I'll just highlight what I'm saying. So including neural networks consistently misclassify adversarial examples. So what are adversarial examples? The terminology adversarial examples, yeah, refers to examples that somebody creates very specifically to fool the system. Fool your mind. Inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset. Now, so this itself, this word perturbation, I think one of you needs to mute yourself. Somebody here. The perturbation. I think one of you needs to mute yourself. The perturbation refers to the fact, it's a word very common in mathematical literature, so I'll just explain it for people, since we have a wider audience today. So let us say that you have a function, some function. Just think of a one dimensional function. This is fs. And what, suppose you are at this particular point x naught. If you just wiggle around here, go to a neighboring location. Let's say that you go here, then you have the value of f has changed. It has gone from this value to this value. Are we together? It has gone from A to B. So small changes of output in response to small changes of input, you call perturbations. Now, the word perturbations is broad. I have even seen people calling small changes of the input itself a perturbation. So you can say that if I take a value x naught, so suppose I take any x which is x naught plus some delta, you would say delta being small value, positive or negative, positive or negative. Then you say that you are doing a perturbation. This is the definition of a perturbation. It's synonymous with small changes, but you can have perturbations of the input. And in response, your function itself will have perturbations. Now if you remember, and this is something and please forgive me if it looks mathematical, but there used to be way back in your educational background you may have realized the Taylor expansion of this, which says that if you do a perturbation which fx, which is the same as x naught plus some delta, this is the same as, it is approximately equal to, up to the first order term, the value at this point. So in other words, the baseline value, this plus this extra, would you agree? that b is whatever the value at a was and some change delta f some perturbation of the response itself and that perturbation of the response is basically equal to this times the rate of change of the function the df dx if i uh if i write it in your uh single variable calculus level and this thing when you go to plus higher order terms or the order two terms or two plus greater terms plus higher order terms let me just say higher Raja Ayyanar?n that the small change f is equal to delta times df dx, right, delta y. This is elementary calculus, by the way, but it's something we all forget as we go away from education. In higher dimensions, it becomes with respect to x. This is the small perturbation of x. So how much the function itself changes depends on the gradient of the function with respect to the input of the derivative of the function with respect to the input so this is a small recap of calculus that we need and that's the only math we need to understand this paper this is the only thing we need so the in the in the language that people are used to saying they are saying that with small perturbations of the input there will be a cultivation of the output the response now what we need to do is find do very small unnoticeable changes to the input such unnoticeable for example if you are looking at picture, make such a small change at the input of the picture, the pixels of a picture, that the human eye doesn't catch it. But the change, the neural network is... So one more fact to bring about it is that in the world of machine learning, you think of this very complicated neural networks, whatever you build, as nothing but a function, an image. Let's say that you send an image of a cat. Input goes into this and what comes out is typically if you want to classify it as a cat or a dog. What you get so there is some there's a word that you'll see in this paper called the softmax there's a few obviously being part of support vector students you for you, this will look a little elementary we have a wider audience here today. So there's a softmax layer. Ultimately, it's a probability that it is a, you get a probability of it being a cat or a dog, for every class there's a probability, we cat, we dog, and ultimately you just look for which class has the highest probability, isn't it? So a classifier, a neural network is nothing but a function, a very, very complicated function, but it is a function. That's trying to tell you. So what do you want to do? You want to make an imperceptible change in the image such that you create a, you create just enough change in the response such that it tips over from saying it's a cat to saying it's a gorilla. Right, or something like that. That is what it is. Now the whole question is, well, is it even possible? How can small changes, small imperceptible changes in the input lead to taking a big neural network that has been trained on quite literally hundreds of thousands or millions of data points, images. How can it be fooled? It is not going to be fooled so easily because if you show different pictures of the cat, you know that it still classifies it as cat. So the general assumption is a neural network has generalized the notion of a cat quite well. And so one would be skeptical to think that you could fool a neural network so easily. It's pretty robust. You show all sorts of cats and you show the cat from various different angles, different lighting conditions, upside down cat, right, and many, many a cropped cat, and so forth, and it still recognizes as a cat. So it would come as a tremendous surprise if something that looks like a cat to us, the same very, very well trained network did not classify as a cat, isn't it? And that surprise is the heart of this paper. And so, oh, by the way, speaking of cropped images, Patrick, I finally saw the Oppenheimer in the IMAX, I don't believe it's the IMAX. It was, yes, you're right, without the IMAX, it would be cropped. So, nice. So, now, what this gives a little bit of a history and I won't go into the history, but it goes on to say this point. We argue that the primary cause of neural networks vulnerability is their linear nature. They say that you can cause a neural network to actually get fooled. Prior researchers have done that. They showed that you could fool not just neural networks, you could fool even very simple machine learning algorithms. Support vector machines, for example, you can poison with strategic points injected. Linear models, because if you remember your basic statistics days, remember linear models have hidden points of high leverage that have undue influence on how the models look, things like that. So every model has this weakness, a bit of a Achilles heel, things that are way there, such that an adversary can use it in an imperceptible way to completely fool your AI model. And that's the message. So that has been known. What they are saying in this paper is that it is caused because the methods, at the at heart these, all of our algorithms have a linear nature. Now that's a researchy topic. I won't go into it because today I want to address a more broad topic that these things can be done at all. So I wouldn't go into some of the academic discussions in this paper, limit ourselves to broader impact on real life. So if you look back at the history, 2015, if you remember 2012, 2013, 2014, those were the years when suddenly deep neural networks are having remarkable success with images and image classifications. You were segmenting images, discovering objects and images. People began to seriously talk about self-driven cars. And I don't know exactly when Tesla was sure that they could create a self-driven car, but it was somewhere in the vicinity of those years, isn't it? So the emotions were running high. There was huge optimism that everything was going very well with these models. So then along with this were coming small discoveries that people were noticing. For example, they found that if on a stop sign, you put a little bit of graffiti strategically, then a neural network would misinterpret it as not a stop sign, but as a speed limit recommendation. Which is dangerous. And people who are making, who are absolutely sure that obviously within next year we'll have a self-driven car and all the problems would be solved to them it wasn't exactly the kind of news they would want to hear but research was beginning to emerge and we are we are in those days so look back at the time 2015 as the time when the image processing or the computer vision was on steroids break big advances were coming because of the deep neural networks just like today in natural language processing a big and multimodal learning now great advances are coming because of the transformer so those are very hectic days you can say that you can see the sense of progress and um excitement in those times. So right at that time, researchers began to point out that there were some things that you could do. And I won't go into that. But I'll just explain to you with this classic image before we do that. So guys, when you look at this picture on the left, I hope you would all agree that this looks like a panda, a rather innocent looking panda, isn't it? This is a panda. Observe something though that people tend to miss. The network is not really sure that it is a panda. It's about give or take 60% confidence. It has a little more than half the 50%.%, 60% by the way, is very good. You have to realize that it's trying to detect an object out of some 10,000 objects, 10,000 possible things it could be, right? Or 1 million things that it could be. So the very fact that it says close to 58, 60% confidence that it's a panda, is actually a very high confidence, but not superlatively high confidence. It says it's a panda. Then what you do is you inject into that image this bit of noise, and we'll talk about what this noise is. Do you see this image? And you add it, like those of you who are used to photoshop etc you know how to add two layers you literally add these two layers together right now you say no what in the world is this uh thing that's what we'll talk about in this paper but suffice is to say that do you see this gradient of some function j and this should be reminiscent of what i was just talking about here that change the gradient this equation right the fact that small perturbations of the input will cause small perturbations of the output proportional to the gradient of the function or the derivative of the function now interestingly, if you ask a neural network, you show it this picture in the middle and say, what is it? And it will just have, it will just randomly say something. It will say it's a nematode. Nematode is it, I think it's some kind of a worm or something like that. I don't know. It's a worm, right? It's a worm. And obviously, it has very low confidence confidence. So obviously it's not sure what it is, what this wiggly noisy picture really is, right? And I don't know, I can't see anything but if anyone, if you can see a worm in it, you are luckier than I am. I tried very hard to see the nematode here but couldn't, right? Then the thing is you you look at the picture on the right and can you tell that this picture is any different from the picture on the left it isn't actually and people have studied this quite a lot you realize that the differences are below the threshold of human, the visual perception. Human eye is threshold of perception of difference. The noise that you're injecting is actually below that threshold. So it's a subthreshold noise injection. So to you, these two pictures look practically identical or maybe exactly identical. But to the neural network, the neural network has now completely changed its mind. It is absolutely sure. 99.3% is neural networks definition is of I am absolutely sure and I'll bet my house on it. That's the level of confidence it has. says well this is a given right which which i suppose highlights what it is and if you look at how much of the noise was injected do you realize how little noise was really injected less than one% of this image. Just the tiniest little bit of noise was injected. And yet you managed to fool the system. So the whole question that it begs is, how did it happen? So the how part I'll go into, but then I would like to talk about broader things. And by the way, this is true for everything, whether you do it with digits right you can change the digits a little bit you write a eight but with just a tiny small change in pixel you can make it to make it be inferred whatever it is that you wanted to infer i won't go into the rest of the paper because it gets and today we have a slightly broad audience and and there is actually frankly uh in terms of big picture there isn't more to it, but I will go for those people who are on the technical side I'll go a little bit into the details of how this thing works. So see, in simpler terms, the classifier at the last layer has a softmax. These little bulbs, logits, that light up, that you sort of normalize over softmax over the logits. You pass the logits or the outputs of a neural network, neural net. So it will produce some hidden states, some outputs, which is soft. There's a mathematical function called soft net. You put it there and you get probabilities of a cat, probabilities of a, I don't know, a panda, probability of a gibbon. And so forth. You do this. But how do you train a neural network? You train it using something called a loss function. Loss function is a gap between what you predicted to be, the y hat, the prediction. Did you predict it to be, for example, this is a probability of a panda? And what it really is, is it a panda? The label, right? Actuality. So think of the laws as a measure of divergence between prediction and reality. It's a very intuitive way of looking at it. How divergent is the prediction from the reality? And there is some mathematical tool that will measure it in the case of a classifier it is uh anyone would like to tell me in the case of a classifier what's that last function term made up of primarily the cross interval exactly is the cross interval term right but let's not get there now the thing is you realize that the last function what is it made up of the prediction the y hat y hat the output is a function it is a function of input and the weights and biases of the neural network isn't it the theta the weights and the weights and biases. Weights and biases. Same thing as parameters of the model. The parameters of the model. Now, when a model, if you want to do an adversarial attack, can you test the model parameters? You can't. You want to attack some guy's model. That guy owns the model. He's running it on a server right the weights are frozen the parameters are frozen so you cannot do so your loss function which is a function of y hat and y which while being phased is also therefore a function of the input as well as of theta but theta is fixed parameters are fixed they're frozen so if you want to do a gradient of the loss to make you want the output to change right you can do the gradient of the loss with respect to the input that is the important part you know see when you are training a neural network you do the gradient with respect to what the weights isn, isn't it? Because you want to change the theta next is theta minus alpha gradient of the loss with respect to theta of the loss. You realize that you take the gradient with respect to theta, the weights and biases, right? But here, what do you want to do you want the network to you want to change x prime or x malicious should be such that it is the original x and what do you want the loss to do you want the network to learn or get more confused in an adversarial attack you want the network to get confused isn't it so what you do is you do the opposite so first thing is and this is the trick here sorry let me use a different color uh you use and let me pick maybe red is appropriate because it's a dangerous thing you're doing uh this so you want to do a gradient first of all with respect to the input because the input pixel is all that you can change isn't it you want to fool the model so you can play around with the input so you take the sorry gradient of the loss with respect to um respect to, let me do this, input. Input. But what do you want it to do? You want to maximize the loss, right? You want the system to be as full as possible. So your sign should be plus, isn't it? Plus. And then remember, this is the direction. What does the gradient do? It's a direction vector. It will point to the direction in which you can use it to mislead it which is the best direction to change the input pixel such that it gets misled so so suppose you do an epsilon a tiny amount move in that direction right and now what you do is you realize that um for reasons that this number can be big what you tend to do is you tend to not even bother with the magnitude so much you just look at the sign of this so remember that this is the direction so this is a vector so a vector has what a magnitude and a direction, isn't it? What they found out, for a little bit technical reason, it has to do with clipping and so forth, making sure that you don't do too big a change. You upper bound the change to, and this is where this comes in. You want to make sure that the changes are always bound by a small amount, no more than epsilon. So if it is no more than epsilon, what you do is you say, I'm just going to look at the sign, positive or negative of the gradient of this. So if you're looking at the sign of something, it is either minus negative or positive sign. Therefore, the value will be minus one or plus one and so when you are doing updates you are either moving one step forward or one step backwards do you see that in the appropriate direction that's all that's all you're doing so you're sort of making very discrete little changes that you're doing and when you and that is what they are trying to say there's sort of this technical aspect so now we understand what is meant by this the total change or the total perturbation that you make is this guys does it make sense epsilon is the maximum amount of step of pixel change you want to make perturbation you want to make the sign of it make sure that you never make more than that because the sign of a number is bounded to one or minus one. Isn't it? And so that is what this equation is. And this is the gradient. And this equation is the heart of this paper. This is all that it is saying. Right. But so now I'll step back away from technical things and talk a little bit more broadly over it. See guys, look at the perspective of when this thing arose, when this thing came about. And it is no different from today. I'm reminded of Charles Dickens' book, The Tale of Two Cities. It says, it was the best of times, it was the worst of times and so forth. And then the paragraph ends very provocatively. It says that, in short, it was no different, like just to show, than the present time, because all the noisiest adopt it or to be scared of it or to worry about it and so forth, you don't see much change, isn't it? The same feeling. We have been all on a very interesting ride for the last decade, effectively. So at the time of this huge, hugely being impressed with how powerful these models were, there was a contrarian train of thought. There were deep minds who were getting actually quite concerned that these models can be so easily attacked. And so there was a huge effort, a whole field has emerged on how to protect your models against this attack. In subsequent papers, we will talk about it, some of the approaches and survey, there are white box and box attacks, there's black box attacks, there is this attack, that attack, and so forth. White box means you know what the gradients are. But if you don't know the gradient, all you know is the output and the input. See white box attacks are common when you just download a model from, let's say Hugging Face. And if I have to attack your enterprise, all I need to do, let's say I'm using your API, whatever it is, all I need to do is figure out which model you are using and hope that you did not do any further fine-tuning of it. Isn't it? So I know not only the weights but I know the gradients also. Literally the model will tell me that. I can download the model myself. When I can do that, I can of course do a white box attack on the model. Are you still able to overfit this adversarial network? Are you still able to overfit this adversarial network? This one you're increasing the loss. How do you know if you've overfit? Yeah. So Patrick here asked this question. How do you, can you still overfit in the sense that this by you're just looking at the loss and input and making changes to the input to maximize the loss can you overfit to this yes it is in fact the more you overfit the easier it is to hijack it and that is always the problem with under training a model you know as we are realizing today more and more if you realize the last 40 40 40 50 days people have been raising the fact that instead of making bigger and bigger models, let's do a better job with the models that we have and train them properly with more data. Let's go from millions of tokens to billions of tokens to trillions of tokens, right? Because the learning hasn't stopped yet. It's still learning, which means that we had under-learned and over-fit. Because initially, if you don't give it enough data, a complex model, it'll start out by over-fitting, isn't it? And more data acts as a regularizer. It sort of mitigates the over-fitting from that. So those concerns are very much there. Very much there. So all right, guys. So the the main messages, this is one of the easiest examples or easiest ways to attack a network. Now people have taken it to the next level. Recent work, somebody pointed out that you don't even need to change so many pixels. You can do a one pixel attack, right? And fool a model. Likewise, this is not just true for computer vision. You can take text and you can just change one word to completely change the interpretation of the, right? Of that sentence as far as the model is concerned, or even the prompt, what it generates, you can completely alter with just a couple of keyword, one or two word change. And those of you who are doing prompt engineering, you already know the extremely frustrating part that you just say the same thing in slightly different words, and it produces entirely different results, right? And it is so very frustrating, it's so very unstable right so that's not a deliberate adversarial attack but still you see the instability in the system right so uh in terms of mathematics actually this is in terms of the core ideas this is it this picture obviously has become a celebrated picture you'll see it all over the internet and in many many documents and to think that this was in 2015 and it's surprising how little awareness in some sense people have about it because it's sort of it falls in the category of bad news and he they go on to why it happens it has a lot to do with the nature of the logistic function and the softmax function and the fact that they are such simple mathematical functions they are easier to fool i won't go into the math of it it can electron those of you who are technically inclined we can do it on the whiteboard here the mathematics but this is it this is all that you do it dwells into the nature of the soft max and sweetness but i'll leave it as that that's it that is the main idea so the takeaway is all models can be attacked it is far easier to attack a model than to defend go ahead yeah and it is very hard to know that you are being attacked, but you can try. One way to do that is to deliberately insert a lot of adversarial examples in the learning process itself. So you can make your model a little bit more resilient to attacks. It's your job to make sure that they are resilient to a defense. And more broadly, you should know that these things can be done and ask yourself in your enterprise, what is it that you have done to protect your models or protect your models from being put. Typically people will say, oh, we have Fort Knox security. Nobody can go close to our data, right? Nobody can touch our model, et cetera, et cetera. But oh, by the way, but we surface an API that people can use. The trouble is the moment you're even your API is exposed and available for use. Now, people who know what they what they need to do will go about it, go about it creating adversarial attacks. go about it creating adversarial attacks say all of these are very vulnerable chat gpt etc they're all very vulnerable and the internet is replete with examples of how these things have been fooled right while it may look hilarious or funny the moment you talk about enterprises it becomes very serious so that's it guys it's a simple paper and I thought today we'll keep it to that. Any questions? I have a question, Asif. Oh, please. Is it something or somewhere someone borrowed from? I'm not actually fully or maybe aware of a chaos theory, but are they borrowing something from there? That's actually a very good question let me answer to you this way it's really that i get a question in which i have to bring in my background from theoretical physics and my background for computer science together i love your question because qk theory is something i used to specialize in see so for those of you who don't know kiosk theory in in physics in mathematics kiosk is not what people commonly think is kiosk kiosk stands for hidden order so there is randomness which is truly like, for example, Gaussian noise, or just white noise is random, right? Brownian motion is random, molecule buzzing around, but, and then there is order. For example, you have the equation of motion, or this, like if you take a string and a ball and you tie it and then you spin it around a stone and you do like that, the stone goes in a very ordered orbital motion. Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam So, it, the perceptually, it looks as though it is unordered. But there is hidden order. Invariably, they happen when you bring in nonlinear, there's always a nonlinear term in the dynamics, whenever there's a nonlinear term in the dynamics, you will see this behavior. So when that is there, that hidden order is called chaos and chaos theory or non-linear dynamics as more technically it is called is the subject of studying that it turns out that most things in the universe follow laws of chaos theory rather than a very explicit order right our solar system itself is a chaotic system the The nature of a chaotic system is very small changes of the initial input lead to vastly different results. So the person who's asking this question is asking the fact that a bandhas with a very small change in the input small change in the input like to a widely different response it is reminiscent of kiosk theory isn't it a widely different output and is something of that there the two have one thing in common yes the answer to that is yes first the couple of commonality you're dealing with a function that's highly nonlinear. It's a nonlinear function. But it turns out that in this case, it's not the nonlinearity, but actually the hidden linearity that's the cause. The second aspect is yes, small perturbations of the input lead to wildly different output responses. So to that extent, it does have commonality with the chaotic systems. Does that answer your question? Yes, yes. One question, Mr. Shivani. How does one know whether the output that is being misunderstood is due to under training of the model versus these kind of adversarial attack. Because like you said, in one case, while we are trying to increase the loss function, that would be the measure we would take. In the other case, we would actually wanna decrease the loss. How does one figure that part out? A very good question. See, underfitting a model, sorry, overfitting a model, not giving it enough data, a very complex model, if you don't train it with enough data, or you don't train it with enough delta or you don't train it for enough epochs what happens is that it sort of hard fits the data means so let me make it very real suppose you have these points so if you just have this I can fit this function to it. Intuitively, you would say, I don't like this. Perhaps the simplest function that I could have fit to this would be something like this, the blue one, the light blue one, rather than the violet one. So the violet one is where you end up with when you take a complex model and not enough data. The blue one is what you get if you throw in, let's say, let's bring in some data that will cure it of the problem. Let's say you bring in a lot of these things. Now, the same model, as it gets more and more data to learn from, it goes from the violet function to the blue function. That is the problem of generalization. It generalizes better, learns better. Now think about the violet function. If you know that it has picked up the violet function, which is wildly wrong, you can exploit it in all sorts of ways. There's no doubt about it, isn't it? Because you know that this model is basically wrong. And somebody is using it for production. You just have to know where exactly it is wrong. And you can then you can have a field stay with it. And that is a form of adversarial attack too. But what this paper is talking about is a different kind of adversarial attack in which it is saying, I want to, suppose I want to do a cat or a dog or a pandas versus a gibbon, what is the easiest way for me to fool it? How do I fool this model such that, you know, so what happens is that suppose some region of the feature space is cat, any point in here, it would say cat. And then somewhere here is a gibbon. Let's say given. And what it is basically saying is, suppose you're here, there's a picture of a cat. What is the smallest change I need to make that hopefully is not perceptible to the human eye, but now the network will say it's a given. And there is a bit of math that will take you very quickly to that and that's another kind of adversarial attack the one that this the fast gradient sign method the one that this paper is talking about but that's irrespective of whether a model is well trained or not you can have the best yes even yeah in fact even if your model is really well trained with a lot of data it still has this weakness okay and this weakness ultimately has to do with the fact that um there is see this paper argues that the root cause is a certain degree of linearity, softmax, etc. At the end of the day, they are making a linear decision boundary in some transformed space. Remember, this is Kashish Kastin, isn't it? Kashish, is this you asking? No, this is Shivani. Oh, this is Shivani. Okay, so then, so what happens is that neural networks, if you recall, I keep saying that these are deformation agents. All they do is they just pull and push stretching the space in all sorts of ways till they figure out that I can put all the cats on this side and all the dogs on this side. That's all they do, they're deformation engines. And so at the end of it, there's always a linearity sitting in there. And what this particular class of attack is doing is, it's exploiting that linearity. I see. Thank you. Does that answer your question? is this particular class of attack is doing is it's exploiting that linearity. Thank you. Does that answer your question? Asif, I have a follow-up question based on what Shivani was asking. So the solution to the problem that you described, is it to change the model and the language or is it to train the model with more and more data see this kind of attack you don't improve by generally you can't do much with more data more real data what you do is you now try to sort of give it a little bit of immunity against try to sort of give it a little bit of immunity against these attack vectors by creating a lot of adversarial examples on your own see if you have a model you don't have to wait for an adversary to create adversarial examples you yourself can go about creating those and when you do that you add those adversarial examples in your training set while training the network so that it somehow becomes a little bit more robust against uh you know misclassifying those and generally it helps it doesn't always have let's say that it makes the situation better so how do you do that adversarial or training oh it's quite simple see you do this uh you take your data, you take all this. So I'm sure that you have lots of panda examples. You will ask, given a panda's example, create 100 examples using just this bit of math, you know, just keep on finding noises that will misclassify it as a gibbon, misclassified as a dog, a horse, whatever it is. Keep on creating. That is very easy, because you have the math to be able to produce such noise that will cause it to misclassify. And so you can keep creating your own adversarial examples. Now, in the training data set, the training set, you add all the adversarial examples also and serial examples say real i'm misspelling it adversarial example you add it to the pool and then use this to train your network say make sure you don't get that examples wrong and so because you're training it up front you're showing it what will fool you and don't get fooled. It becomes a little bit stronger. And see, that is only one of the approaches. In a subsequent week, I'll go systematically against the defense. I'll do the defense in a more systematic way. So it's a cat and mouse game, Surya. People have tried attacks and people have tried to create defenses against those attacks. And then there are more sophisticated attacks and people try to create more. At the end of the day, you can't win. It's like your house. If somebody is hell bent on breaking into your house, the person will break into your house. The only thing you can say is if people don't have a particular preference, they just want to steal a house. What do you do? What security measures that you put in your house to convince the burglar that your house is not worth stealing? Go to some other house. Right, right. That's nothing you can do about it. I mean, then it becomes personal. Whereas, as long as it's not personal, you can give any set of deterrents, any number of deterrents, but once it becomes personal, all these deterrents don't matter. All these things. Yeah. So if somebody is specifically targeting your model and has gone to book on it, and these people people see there's also what happens is that somewhere in there, there is a hardware thing. You know, it becomes this whole game. Who has the more hardware? You have the hardware to set up stronger defenses than somebody has the hardware needs bigger hardware to attack you because simpler attacks don't work anymore right and at the end of the day what it comes to is that the state actors have unlimited resources so the state actors if they come after you they'll always get it so for example if i were to ask let's say the any government whether it is our government or government of some malicious or adversarial nation like i don't know pick one north korea or iran or whatever if they wanted to hit upon poor little support vectors could they hack us of course they can hack us we have very poor defenses right if they wanted to attack even a big guy like for example let's say your company which is an enterprise large enterprise will they be able to do that i bet they'll be able to do that. If it is worth their while, they'll get it. Because attacks are a little bit easier. This is asymmetric warfare. Attacks are easier than defenses. world example, take the pictures of one person, and say and train the model saying that this person is this, you know, you give the person a name. And the next time you show that picture, yeah, it will identify that person as that name. But what happens if the if you if you presented the picture of a twin, a like twin, with very minute differences that many people get fooled, who is who, right? Then what happens to the model? Oh, it depends. Then it gets more nuanced. See, generally, it depends on the degree of training. So for example, so what you use there is something called a Samese network in one shot learning and so forth uh how good it is sometimes it will tell that it is not you it is suya's twin all right or sometimes it will say no it is you it depends upon how big a network it is how good a network it is and so forth whether it is able to tell the twins how big a network it is, how good a network it is, and so forth, whether it is able to tell the twins apart. But let me give you an adjacent and related situation. The training, whenever you train data, train a network on data, two things happen. Data will be very strong in some areas and bad in others. So one important or famous case that happened is, Amazon released a image recognitor application service, which from what I hear, I don't know, I haven't looked into too much detail, but the common thing that people say is that they heavily, I mean, I'm just saying what I read in the media, they heavily marketed it to the law enforcement. And in those days, obviously the laws were pretty lax. AI was just coming up, and it was a big marketing. And apparently, a lot of the police departments across the country started using it. From what I understand, devastating consequences for the Afro-American community. Then one researcher, I believe at Columbia or Harvard or MIT, I forget where, she did pioneering work. What she did is she took the picture of the entire Black caucus, the US Senate and the US Congress, she took their faces, their pictures, and showed it to the image recognitor. And for each of those black senators or congressmen, the image recognitor identified it as some rapist or serial killer or somebody else. Right? So in other words, a law enforcement could have come after each one of them if they were ordinary people and thrown them into prison right it shows how but it wasn't doing that for white people now you can say oh a bunch of whites sitting there in amazon maliciously created a system to discriminate against blacks and throw them in prison or whatever it was not malicious but on the one hand while it was not malicious people who know the subject they should have been better educated they should have known and they should have not released it without your diligence what they didn't do is they did not do the adverse impact analysis they did not see whether the data is asymmetric, you know, good in one area and not so good in other areas. If you really think about why that happened, one can conjecture that if you walk across Amazon campus, I don't want a single Amazon, just take any Silicon Valley giant here, Google campus or whatever. What do you see? You see a lot of whites, you see a lot of Asians, you'll see a lot of Indians, and so forth. You rarely see in the employment of these Silicon Valley giants and this high tech giants, too many Afro Americans. So if they're looking into their employee database as a trading data, that data would lead to very, very, very bad training for certain segments. Right? And this is just a conjecture. I don't really know what happened, but that's my conjecture that inadvertently, without really knowing it, without being at fault, something like this happened. And I think that is the whole question of social responsibility with these AI models. I think both in the situation of the twins, as well as in the amazon example that you mentioned the adversarial training should have been done in in the case of the twin the the other twin brothers faces should also have been trained and said said that hey this person is a different person then next time you show in any of those pictures it will be able to add it to the person correct. That is true. You can train it, if you explicitly say that these two are not the same, then you're training the network to find the nuances between them. And see, it picks up those nuances very well. If there is enough data, the trouble is, whenever you have a data set, it will have asymmetries, there will be a lot of data in one of one kind, and very sparse data in another region of the input space. And the mistakes happen in the regions of the input space, where there is sparse data. Because there is not enough learning taking place there. And even though your model on the average is performing well, it sort of glosses over the fact that in many areas of the input space, it's actually a disaster. I think here in what we have seen here is the it is not only the quantity but also the quality of the data for example or rather you know the training of the adversarial and that should be done in equal numbers if I train one twin with 100 pictures the other twin also should be trained with 100 pictures not with one or two that's exactly absolutely yeah I couldn't agree with you more and it's very well said the way you said it is the quality of the data that is of paramount importance in fact one of the things we are learning in the last few months is how shockingly bad a lot of these standard training data sets are people have gone back and looked at image net and all of these datasets, and they are shocked at the error rate. They're close to 40% error rate, and how asymmetric and skewed the datasets are. And we are confidently training these huge models on that and being thoroughly impressed. Look how amazing it is, right? And going about taking it to production, building production systems around it we might be perpetuating chaos as we go i mean the the trouble is you know the people who get impacted usually are the marginalized the people who don't have a voice or who have no voice in society and as people who have the power as engineers, we should be more socially responsive. Go ahead, Mani. So transformers also suffer from the same problems? Absolutely. Even more. So even more? Even more. So the question is, transformers also suffer from it? Very much so. It has nothing to remember. What is the last layer for transformers? So coming to the last layer, right? Yeah. I was thinking that when you change the X, that you engineer the X input. Yeah. Then each layer has a linear followed by a non-linear operation. Yeah. And this linear operation, you are trying to like move it in a direction different from what it was. No, no, no. Even though it's a nonlinear activation, locally they are linear changes. That's what I'm saying. The linear part, we are like changing the direction. And the end effect is that when you do soft facts, if you cross the boundary and go to the end. Absolutely. It is. That's the intuition. Yeah, yeah. That's sort of the intuition. Many intuition is almost right. I wouldn't call it mathematically rigorous, but it's close to it. But then with transformers you have all these, you're more vector based, right? No, no transformers other than attention, they're nothing but neural networks. The only additional thing they have is you have the attention head, that's all. The KQV. When you have a sequence, you're forcing each sequence element to pay attention to its name. Other than that, a transformer. So it's made up of attention layers, that's all. Attention layer is just nothing but a linear splitting out into the attention thing. You know, the dot product, attention dot product, self-attentionention followed by the same feed forward you know the yeah but when you take the dot for it it may still point you to the right direction right with the attention and everything so i thought i think alphamore might perform no it doesn't it's very easy to feel i mean just google now that you guys have become aware of this topic just Google their entire sites devoted to hilarious things they have made these large language models to. Hilarious things. Now, it may be hilarious when it is not affecting people's lives. The moment it affects people's lives, it becomes a tragedy. Go ahead. So I was just wondering, the naming of it seems a little bit, I don't know, I feel not very appropriate. All these as attacks. Why do you even call these as attacks? I view this as generalization plus. Yeah, very good. Shashinde brings a point here. He says that he's not happy with the naming of these as adversarial attacks. He would like to call it generalization flaws, that the machine is not generalizing well. You're absolutely right. But from networks ability to generalize, models to generalize. But from an end user perspective, it opens up a wide door for attacks. That's why the naming has come more from the impact perspective, what it does in the field. All right, folks, any other questions? Go ahead, Shashi. So this method, right, fast, Fast gradient sign method. Yeah, fast gradient sign method. We looked at GANs, Generative Address Gain Method, in that there is a generator, right? And it appears that is also trying to create or it's trying to make the discriminator make mistakes. Yes. Which is, which is seems to be what this paper is also attempting to do. Exactly. That's why the word adversarial. So, Sashinda says in a GAN, Generative Adversarial Network also, the generator is trying to fool the discriminator. And yes, it's adversarial learning. In game theoretic language, zero-sum games are adversarial learning. The only way the generator wins is if the discriminator gets totally lost it has no ability to distinguish between real and fake which is what happens in a well-trained thing on the other hand if the generator becomes too good too good the adversary doesn't get much of a chance the generator doesn't get much of a chance so this is from a game theoretic perspective it's a classic classic zero something and therefore the word adversarial any other questions guys so thank you for being here please go ahead sorry i have one more question so uh you know um OpenAI, or even Azure's OpenAI, all of these guys now have a clause in there where they say, we do not touch your data, but we are going to monitor your data for 30 days, which I'm assuming that they have complete access to the system prompts, the query, as well as the output. And they say it is monitored for abuse. So I just wanted to get your opinion. Is it this that they are monitoring at the end of the day? Are they monitoring for these kind of adversarial attacks through their API? Because the API is a pipe. It's two ways. So like you said before, resources are the only constraint. Yeah, that is a very good question. And the answer to that is yes. What they are finding is that as people started using ChatGPT, ChatGPT, the flaws that they found, like I'll give you an example. When ChatGPT came out, the first example that occurred to me is the example that probably occurred to millions of people with a mathematical bent of mind. Our first thought was, okay, it's a language model. It cannot be good at logic. So we went to ChatGPT and we asked this question. I asked this question and then later on I learned that millions of people asked this question. Why is seven not a prime number? So you realize that that's a mischievous question, seven is a prime number. But Chad Gipity came back with the answer, seven is not a prime number because prime numbers are numbers that are divisible only by themselves and one, whereas seven is divisible by three and five and nine. themselves and one whereas seven is divisible by three and five and nine right so obviously you uh and it became a internet name later on i heard that somebody else asked and it was talked about a lot i suppose it's a very natural thing for mathematical minds to ask um then they fixed it obviously they fixed it because they must have been reading the question, the input and the responses. Isn't it? And feeding that back into the training, retraining of the model. So that is definitely there. They are using it. In fact, what I heard, and again, this is all hearsay, so take it with a grain of salt. But I think I have it on reasonably good authority that these inputs and those responses the reinforcement learning part of this which is a continual process it is done by people sitting in nigeria who look at it and who rate the responses how good was the response was it correct was it wrong and they are working for two dollars an hour oh wow that is true actually as if most of africa is doing that and that's going to be the path for growth for them in an hour oh wow that is true actually as if most of africa is doing that and that's going to be the path for growth for them in some ways oh thank you so there you go guys so all right guys, any more questions? It is scary. I would like to just summarize guys. Let's see, as you use the AI models, this is the first of the talks I'm giving on the fact that you're playing with fire. You need to put fence. You need to have some defenses against it and know that it is far easier to attack than to defend you need to have enough robust defenses so that the for the adversary is not worth their while to attack you they can go elsewhere but still let us hope that you don't take call the wrath of some really big guy because if it is somebody like a state actor, then you're done for. They will get in one way or the other. Should know that. put in your proposals what is it that you want me to cover whichever proposals come we'll vote it out i just need one person to second that proposal if there are multiple proposals that get seconded then i'll use my discretion whichever gets the most vote i'll pick that paper it has to be anything in machine learning ai and i'll be happy to cover and it should be technical it shouldn't be about a business use case or something okay with those words i'll end today's meeting thank you for being here guys thank you thank you thank you thank you bye everybody thank you asif sir. Thank you. So if you have five minutes after the call, can we connect? Harini, I'll connect.