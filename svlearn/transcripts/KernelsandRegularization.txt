 One more second guys. I need to put the YouTube on mute, otherwise I'll start hearing myself back. Oh, why am I not seeing the new page? So we have two things there. So let me recapitulate what we did the last time. We learned about support vector machines. In support vector machines, we have, okay, sorry to interrupt. I had a small announcement to make. This Sunday, we are going to have the seminar that I talked about the last, actually I was planning to have the last Sunday. It's going to be there with that person in India who's been doing a lot of grassroots engineering work, small innovations that will help the common masses and so forth. So that is for this Sunday. It will be at noon this Sunday. So what it means is that we'll have a quiz on on Saturday. And we'll have our extra session that we do right after that, after the quiz session. So we'll put both of those sessions together so that on Sunday we just keep the seminar just as a clarification. Alright, so with those things, let's get started. Last time we talked about support vector machines. The support vector machines are actually very simple machine learning. It's an algorithm. It's quite simple, actually, and based on the notion of a kernel. Now, there are many kernel-based methods. If you look at the literature, this whole field of kernels, using kernel methods is pretty vast and pretty deep. They use it for a lot of things. So today we'll use it perhaps a little bit more, but broadly it's used a lot. Support vector machines just happens to be the simplest and the first one of them the basic idea in support vector machines is that we have the notion of support vectors these are lighthouses essentially the idea is that throughout the data you find some inline points some specific points which you can light up and you can think of it as sort of lighting up those points, declaring them, or these are your support vectors, these are your lighthouses. Once you have discovered these lighthouses, their purpose is they help you draw a decision boundary, a nonlinear decision boundary. So how do you discover these lighthouses, these support vectors. The way you discover these support vectors is, the implicit idea is that you take data where the decision boundary is non-linear or potentially non-linear and you project it into a higher dimensional space. The underlying belief is that in a higher dimensional space, every decision boundary, no matter how complex it is as a hyper surface, there will be a higher, even higher dimension where or some of the sometimes you don't even have to go to a higher dimension. Sometimes you just need to go to another dimension, another space of the same dimension sometimes. And when you do go there, the problem simplifies itself. It becomes a linear problem. Once it becomes a linear problem, then everything that we learnt about maximal margin hyperplane, the soft and the hard maximal margin hyperplane, comes in. It's a straight river. So the basic idea is you want to flow the widest river possible through that data set in the higher dimension, a straight river. When you project it down to that, in the original space, that straight river is obviously a very river is obviously a very very curved river potentially it doesn't it need not be straight now those points that are there in the river or on the banks of the river those lighthouses you find those in the higher dimension space and then you come back when you come back of course those points those lighthouses will become will be sitting along the very non-linear decision boundary in the original space. Going from one space to another to solve the problem is a classic mathematical concept. It is often used and today I realized that there was a simple motivating example that I could have given which I didn't give to illustrate this point and I'm going to start with that so you see why it is true that in higher dimension it gets all things can be linearized now that now that's a good news the bad news obviously sometimes can be it's not really a bad news but it is a fact that you might not be able to visualize the dimensions very easily. Sometimes these are infinite dimensional spaces. So, and that is a little bit hard to get your head around. How do you deal with, how do you imagine infinite dimensional spaces? For that matter, how do you even imagine a four dimensional space or five dimensional space or something? And the way, well, the joke is, the way you do that is you think in terms of the basic two and three dimension that you can but loudly you say as you can see in n dimensions or something like that because quite often your intuition carries forward so long as you know the traps places where the the higher dimensions are radically different from the three dimension and you understand those differences, in all other situations quite often your basic intuition of three dimension does carry forward to higher dimension, even to infinite dimensional spaces. And this is what we do, like I remember when I was learning general theory of relativity and like i remember when i was learning general theory of relativity and even the special the basic idea is that space time space time already is four dimensional it gets a little bit hard to understand that and then comes general relativity which goes and bends the four dimensions into a fabric which has all sorts of bumps now now it is even harder to, but what you do is you imagine in two and three dimensions, and you from there, you carry the intuition forward to the higher dimension. That's how it is. And the same is true for string theory. String theory takes it even further to 10 plus one dimensions. We all carry our intuition in three dimensions, And then we go there and we sort of manage with that because we haven't evolved ability as to get intuition in high dimension. But usually that's not an obstruction. Now I give the example, if you remember, of a polynomial, I mean a curve, which could be represented as a polynomial. And the moment you could represent it as a polynomial those polynomial terms become the axes of the higher dimensional space but sometimes you don't even have to go there I want to give you an example which is quite simple to illustrate the point that going to higher dimension can be need not necessarily is not necessary the higher dimension could be the actually of the same dimension rather so I'll take this example imagine are you guys seeing my screen imagine that somebody gives you a unit disk inside the disk let us say guys could you one of you please confirm you can see the yes okay that thank you so suppose you have points like this and this thing usually in the classroom it's so much more easy to illustrate, let me call it the interior points on the disk, and then associated with it are some exterior points to whom, let us give another color. Oh, this is rather multi-colored all right we'll go with it anyway then you have points which are outside it the yellow points are outside it and we can sort of generalize an extra color but this is if you remember this was actually one of your labs in ml ml100 in fact classifier 3 was this and might want to, it would be instructive if you revisit that and apply a SVM on it, support vector machine on it to solve it. Because that problem is essentially tailor-made for a support vector machine. I'll post the solution with the support vector machine. Today, guys, remind me, I don't know how long we'll continue with the support vector machine today guys remind me i don't know how long we'll continue with the session but i also want to get you started with something called automated machine learning uh just as a reminder it's an off-topic thing let me so let me introduce some noise points here make it look more realistic so assume the data is something like this. Are we together? And it's something like this. And some points are outside too. So there we go. We have this data. What can you say about the decision boundary? You would agree that the decision boundary is quite manifest. It is, it happens to be this thing. This is your decision boundary, isn't it? This is your decision boundary. Let me make it very, very clear. And Oh, what just happened so these are you decision boundary now this is evidently nonlinear decision boundary isn't it so I'll just say nonlinear in the original space this original would you agree this is a r2 space let us say that this axis is x1 and this axis here is the x2 axis it is that now could you guys see an intuition of something that will make it linear? When you look at a problem like this, you ask, OK, can I use logistic regression straight away? And the answer that comes to mind is perhaps not because it's rather a curve decision. It's a circular decision boundary. So actually, this is one of the interview questions I give to data scientists, should they have the misfortune of being interviewed by me. So now I'll tell you something very interesting. Suppose you go to a new space, let me call it x prime is equal to x1 square x1 prime x2 prime is equal to x2 square right and see what happens let's see what would happen there in that particular space so you realize what is the equation of a circle guys the equation of a circle is x1 square plus x2 square equal to yes radius square excellent of the decision boundary if you write the decision boundary as an equation it would follow this constraint it would follow the constraint that x1 square plus x2 square is equal to some value r squared like some sum this let me just take this as r it would be r squared the radius squared but when you go to coordinates like this what does this equation become if x1 square is x prime, it becomes x1 prime plus x2 prime is equal to some value. So suppose the axis is x1 prime and the axis here is x2 prime. Do you see that guys? So what will happen? It's a line. It becomes a line. A line. In fact, this is, if you go here, if this is r and here also r, it will actually be this. This would be it. This is the equation for this is x1 prime plus x2 prime is equal to r squared which essentially is the same equation and so what you will end up with in the transformed space are it is things like this all the blue points would be inside the decision boundary here in the inner triangle from the origin to this this little triangle and all the outer points will become like this are we together guys so it should be R square yes in fact this. In fact, this is R squared. You mean the coordinate? You're right. You are absolutely right. That correction, thank you for pointing out, the correction is R squared. So does this make sense, guys? So do you notice that you actually went from here to here, one space to another space, not necessarily in a higher dimension, you are lucky enough to get it in the same dimension. Are we together? Right? You could. So that's the intuition that you can use to do that now there is actually another way of looking at it what you do is suppose i created one more measure of it the measure is i looked for every point this is two dimensional guys this this thing is two dimensional If I look at it in three dimensions, let me look at it here, three dimensions, and see what happens. If the z, I create a x3 dimension, think of it as z rather, actually, typically in school we think of not x1, x2, x3, but x, y, z, we think of it as z this thing suppose i create a z 3 by definition it is x 1 square plus x 2 square let us see what it will look like what it will look like is uh It will look like a surface that goes to through this. And so let me make it. Convince yourself that this is how the surface looks. Parabolic. Parabolic surface, isn't it? This equation. It's very easy to see if you just forget X2 and you just think in terms of intuition if you want intuition just think what what does Y or Z is equal to What is this equation y is equal to x square look? It looks like this y is equal to x square that's intuition i just generalized it to our three dimensions so this is it are we together and if you look at this you will realize that your blue points were actually on the surface here it is on the outer surface here actually this, this begins to look like solid, but isn't solid. Let me emphasize the fact that this is this. And all the yellow points, if you think about it, they will be again on this surface, this parabolic surface, but they will be up here. on this surface, this parabolic surface, but they will be up here. So guys, just you could do this in our in the physical classroom, we do that. You can take a page and you can just put a point like this. Just draw it, color it in a piece of paper, the inside circle, and all you have to do is if it is a think of it in a piece of paper, the inside circle. And all you have to do is if it is a, think of it as a rubber band page or sort of a sheet of rubber. And then imagine that you're deforming it into the shape. You would agree that you could do that. And this will become, this becomes this. And when it becomes this, where is the hyper plane that is the decision boundary this is the hyper plane hang on let me use a different color this is the hyper plane I look you use well I have used white what can I use for the decision boundary blue is green let's go with green yes let's go with green. Yes, let's go with green. You would agree that the decision boundary would be this plane that sort of cuts here. Do you see that? This plane that is cutting it right here. Do you see that? This plane that is cutting it right here. Imagine a horizontal plane. The green is a horizontal plane. Plane cutting at some place at z is equal to r squared here. Sorry, x3 is equal to r squared, if you think. All the blue and yellow data points are on the surface of that three-dimensional parabolic object. They're not inside it. No, no, yeah. So yeah, they are on the surface of the parabolic object. Yes, that is true. And so in other words, that parabola is not solid. It's just a surface. Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam It sort of looks like a telescope or parabolic, the surface of a telescope, a radio telescope, parabolic or something like that. Yes, it is like that. So guys, I hope you can see how you can take data. Do you notice that in this space, the decision boundary was circular. By going to a higher dimensional space what have you done you have lifted you have created another dimension in which actually on x3 there is just a cutoff point above that it is so when you write an equation x3 is equal to r squared what is it x3 is equal to some constant. So it means that's your decision boundary. All points above the plane on the parabolic surface will be yellow. All points below it will be blue. I don't know. Is this intuition clear, guys? Oh, yes. So this is your main intuition of going to a higher dimension and solving it and sometimes going to not necessarily higher dimension but going to some other space and solving it. If you want to put it a little bit more poetically, you can say that for every data there is a native space in which it's much easier to understand it. It's very simple. It simplifies itself. So that is that. So that's the intuition of the support vector machines. The job of the machines is to discover this high dimensional space in a systematic way. But what is more interesting is you can say, all right, now it is complicated because i need to go from x1 x2 to this find this complicated mapping mapping that will go to some other higher dimensional space let me call this x1 prime x2 prime and x let's say d prime and you don't know uh what d is first of all and you don't know what the what the complicated rules of psi are actually people use not psi they use phi sorry i apologize phi r right that takes a point to a higher dimension yes go ahead please i'm just it was a little difficult to grab the three dimensional thing so. R. Vijay Mohanaraman, Ph.D.: For me, intuitively, it was a paper and I didn't like a rubber platform I folded it do a nice parable. R. Vijay Mohanaraman, Ph.D.: yeah. R. Vijay Mohanaraman, Ph.D.: So it's quite less so excellent square plus one square equal to r square was the initial two-dimensional circle so at a distance of r is the from the center is where the separation boundary is the decision boundary but when you fold it up you are still going up to r square or r so that that part I slightly couldn't understand. I know you're projecting it in the third dimension, but intuitively I was holding the paper up into a parabola and then I have to go R square distance up in the X3 direction to hit the decision boundary. So that part I was looking. So this parabola, very good question actually. So this, let let me so see this decision boundary that is here this thick decision its height everywhere everywhere would be exactly R square R square plane. This line is sticking up exactly at the height of R squared. Hey Asif. Yes. Will the height be R squared or you know from the origin to the point where the plane is cutting that is R squared now think about this you know the on the surface of the surface of the parabola yeah see every single point here right this point corresponds to the x3 value right we are x3 okay is r squared which means that x3 this plane what does this plane represent? This plane represents x3 is equal to r squared, a constant. So think about, just think about this. It's a horizontal plane, isn't it? On this horizontal, every single point is at the same height from the ground, of the ground made up of x1 and x2,'t it and so all the points on the parabola the decision boundary points there wherever this plane cuts the parabola the whole decision boundary is at height R squared because every point on this plane is at the height of r squared right r squared are we making sense this is i i was just thinking definitely they are of constant height but will that be r squared that's what i was thinking it is no because uh this is literally the definition of this x3 the way we have defined x3 or we have defined x3 like that okay greatly like that if you choose you could define x3 square is equal to x1 and x2 square because then it looks more uh sort of symmetric but you don't need to i mean it doesn't matter this is it so so the point is that your real boundary in higher, so now the intuition is, look at this, in higher dimension, I'll put it, in higher dimension, in higher R3 space, the decision boundary is is the linear is the linear if you look X R squared plane. It is this plane. Isn't it? And in this, in our R2, original space is a circle. Isn't it, guys? And you can literally see that happen. This plane is touching the hyperbola, or sort of parabola, I take this, but parabola, parabolic, well, not parabola, but parabolic surface at this circular place. Isn't it, guys? I have a question, sir. Yes. One second. Let me finish this out. So this circle, when you project back into, you look at the projection back onto the two-dimensional surface, x1, x2, you can literally see a circle on the ground, right? This thing projecting down to a circle in the ground, right is the original circle that we have go ahead so sir circle in three dimensions isn't this sphere but we did not we did not choose to lift it into a sphere we did a very specific lift you remember x3 is x1 square plus x2 square i didn't say that x3 squared is equal to that right that would have made it into a sphere yeah but in three dimension our data uh distribution looks like this so here so i'm a little confused like how it doesn't. Well, think about it. See, here is the illusion. The height of a point in three dimension is proportional to the square of its distance from the center, from the origin. origin think this way okay for every point the x3 value the lift right the going up in x3 is equal to the square of its distance distance from the origin right from the center so because it's a square it will look like a hyper it will look like a quadratic surface. So the distance at every point is x squared plus y squared plus g squared. No, it is not. It is not. We are not talking about that. Every point is given as, see, this is the square of the distance. Sir? The square of distance and we are saying that the z axis value the x3 axis value is simply the square of the distance what we are not doing is x1 square plus x2 square plus x3 square is equal to some constant because that would be all points on the sphere yeah surface area yeah that's right and we are not doing that we are declaring that the x3 value the lift or the height of a point from the ground is proportional to the square of its distance from the origin right so when it's proportional to square it looks like a curved surface right then it's curved yeah yeah then it's a curve so so g so z uh is g height is x square plus y square yes exactly the z height and here i call z by x right yes and the basic intuition is you look at this plane this boundary is very obvious it is actually the hyper is just the plane the horizontal plane where x3 is equal to r squared that certain height or distance from the decision boundary right from the origin at a certain distance from the origin uh is that plane x3 is a plane is that obvious x3 is equal to r square is a constant decision boundary it is a flat surface in three dimensions and therefore the decision boundary is a linear decision boundary that green surface that you see uh the green that you see is the decision boundary. But when you come back and look what does it look in two dimensions, in your two dimensions, it is basically the projection of the intersection of this plane with this a parabolic surface. And that is this circle. And when you project it down to the ground it is still the circle which is exactly the circle that you are looking at so in other words in what in three dimension is a green plane in two dimension becomes this wide solid line you see that right are you is anyone else are you guys getting the intuition it's a simple geometric intuition exactly all you did is you lift it all by the equation x equal x3 equal to x1 square you lift all the points up and then slice it by looking from bottom yes you just literally put a horizontal slice imagine just taking a knife and going horizontally and cutting through this thing isn't it yeah now i got it thank you yes it's a very simple and beautiful intuition and this is the foundation of this idea is the foundation of support vector machines now this idea this idea, intuition is great, but there is a further fact which makes it computationally possible, which is that there's a kernel trick. You will learn about the kernel trick in the math class when we get to that, because quite literally, it's about dot products and vector spaces and all of these optimizations, constraint optimizations. So this is literally what it is. But we'll leave that for the next class. But the basic intuition is you can do that with the kernel trick. What happens is you don't have to truly find this phi, this phi that you have. You don't have to find because there is a trick. I won't write the equation. because there is a trick. I won't write the equation. The dot product between x and, let me just say x and x, well, I used x prime for the different space. So let me just say that the dot product between two vectors, a and b, right, it has a certain relationship. If these things get transformed into the other space we could say that there is a phi a phi a b in the like let's say that this function is there this can be computed this kernel can be computed without necessarily very explicitly understanding what in the world is this phi in closed form you don't have to do that it's sort of the thing that makes this magic computationally possible so that's a computational detail this let's not get into that too much for the time being till we get to the math class so I hope I summarized the phi and the k here for this going from two to three dimensional yes so here so in this particular example let us see that see i give you two possible journeys from here to here and then from here to here the the for this one the phi of from here to here. For this one, the phi of x, actually, why don't I leave this as exercise, because this is actually one of your very, very basic exercise in the math. I don't want to give out the solution right away. I'll let you find out. How do you do this? It's taking you in the face, but I'll let you do that. It is, well, okay literally x one the vector spaces are x one so suppose you have five x one x two is literally x one squared x two square a new point in a higher dimension, this is it right next to square. square the square of the points and then likewise this one is very simple you have the new points are five new points of x1 x2 are this set x1 x2 x3 where x3 has this relationship the constraint x3 has this relationship, the constraint equation x3 is equal to r squared. Thanks, Ashish. I'm getting the hang of it. And k is just the dot. I mean, in the simplest form, think of k as just a dot product, a dot b. It's the simplest kernel you can, you know, the dot product or in basic notation a dot b. Think of it like that. In a simple example of a k could be simple example. It could just be a dot product but actually it could be any function of the dot product altogether. So for example in this the polynomial kernel it could be a dot b to the power some polynomial power in the case of a radial basis function minus gamma a dot b. So you notice that k's are these kernels are just functions of the dot product. They're just functions of the dot product. That's all it is. There's nothing very magical about this. We use the word kernel. It's sort of the word used in the literature. So now I'll tell you about something very interesting. In the last ML 100, we talked about dimensionality reduction. And we learned about a technique called PCA. Does anybody remember what PCA stands for? Principal Component Analysis. Principal Component Analysis. Without recapitulating the whole theory of it, let's just say the basic idea is if you're looking in two dimensions, if your data has a linear has a direct linear correlation it is a correlation or there's a linear relationship like this in some form it could be in two dimensions or three dimensions for four dimensions there is a ellipsoidal envelope you can wrap data basic intuition is you can shrink the data in an ellipsoidal envelope. Envelope. Isn't it? So here it is the ellipsoidal is not actually necessarily three dimensions or four dimensions. It's very simply, this is the ellipse, isn't it guys? It's a simple two dimensional ellipse. Yeah. When you can do that, an ellipse has the major and minor axes you know the principal axes this is one axis this is another axis those axes so then this major axis uh let me just call uh this axis as u1 u2 let me put unit vectors along that so you can you have directions u1, u2, etc. And then you will have a certain amount of stretch along those axes. And by the way, these axes are also, when we did the little bit more on the theory of it, it is also the eigenvectors of the covariance matrix, covariance matrix x1, x2. There's a matrix. So we wouldn't do the whole thing, but just say that there's a bit of mathematics that says that if you take the matrix and you do something called an eigenvalue decomposition, then these are your eigenvectors. But forget the math. Just think about the intuition that you can shrink wrap it into an ellipsoid it has unit vectors along the principal axes those are your principal components and the degree that that you stretch along each of these the lambda one lambda two lambda three so forth These are called the eigenvalues. Values. And the beautiful thing is that they have an order. Lambda one is greater than equal to, Lambda two is greater than equal to Lambda three. It is like that. And not only is it like that, but it follows an interesting behavior, sort of a power law decay behavior. Quite often, you will notice that lambda 1, lambda 2, lambda 3, lambda 4, there will be lambda 1 will be pretty high, lambda 2 will be, let us say if you're lucky like this, lambda 3 will be, let's say like this, lambda 4 will be like this. So what will happen is if you connect these lines, right, you notice that there is a elbow here right so this is the value of lambda 1 2 3 4 lambda 1 lambda 2 rather a value the magnitude of it so elbow point And you basically say that if a situation is like that, I can keep only these three and throw away the rest of the components, right? The rest of the dimensions I can throw away and I'll get a pretty good approximation of the data. Just like in this case, you could argue that you have, I can keep only one axis of the data. And for any point, I just forget about this perpendicular axis. I just tell where a point is along the main diagonal axis. And that is the coordinates in one dimension. And I don't care about its location along the other direction, you can sort of ignore it, because it doesn't matter or you don't lose much by that. To do so is to do the dimensionality reduction. So that was the intuition that we have. Guys, do you remember that all your videos are still there? If you go to the page that I created for ML100, I hope you all have the hyperlink to that page. So all those videos are there for you. You all have the hyperlink to that page. So all those videos are there for you. You all have access to it. If you want to review it, please do go review that dimensionality reduction session. I think it was the last session that we did in the previous workshop. So now what does that have to do with kernels? So for example, what does that have to do with dimensionality? What do kernels have to do with dimensionality reduction? See, let us say that you're talking about data which is on a surface, something like this. Now, how do I draw a three-dimensional surface or two-dimensional surface? Let us say that most of your data is along this surface, this curve. This is your dataset two. It is like this. If you go back and look at your, well, obviously I did not make it symmetric, so let me make it a little bit more symmetric. Not that it has to be symmetric, but okay. Data set two look like this. So I'll put it like this here. You remember this was your data set two? From your ML100. So in the previous class, introductory class. So if you look at this curve, if you try to create a covariance matrix between X1 and X and Y, a covariance matrix between x1 and x and y a covariance of xy you will get something approximately zero actually correlation the covariance would be zero or the correlation would be zero why because the relationship is not linear at some point they seem to be positively correlated at some point they seem to be negatively correlated and again positively correlated and the net effect is probably a zero correlation guys are we together is the intuition obvious hey so now the question is is there a way to to linearize this can be linearized this linear yes sir this this so that this begins to look like our old problem of data along let us say something like this can be converted to it doesn't matter what I'm sorry what did I just do apologize I was up to something give Give me a second guys, I might have closed accidentally the OneNote. Too many notes on the desk. Yeah, okay. Sorry. So is there a way, and it doesn't matter at what angle or tilt it is, but can we somehow convert it into a linear problem is the fundamental question. How can we go from here to here? You can say, well, that is very easy. We can apply a kernel trick, the kernel mapping. We know that there is a way to go to a higher dimension where all curves and surfaces get linearized isn't it but once it gets linearized then the old technique of PCA comes in isn't it guys you see the two-step process we can still do dimensionality reduction on a nonlinear curve like this by first linearizing it and then applying our PCA to it. Let me write down the two steps. First, a, first, linearize the problem. Rise, lean, this is terrible. Linearize the problem by going to a higher dimension. So X goes to some phi of X, right? Some this. And B, then, do the PCA. Then do the regular PCA. What say you guys? Is this more straight? Is this obvious, this two-step process i do some feedback guys why we are doing pca once we uh later like the data then the whole point was that we need to do dimensionality reduction we need to let's let's say to just, here it is two dimensional data. We want to use it to one dimensional data. Got it. Right? So you realize that we are doing something very counterintuitive. We want to reduce the dimension, not increase it. But the way we do it is we first increase the dimensionality. We go to a higher dimensional space where the data is linearized and from there we come down back to a much lower dimensional space right so you're effectively sort of going from two dimensions to three let's say three i'm just taking a hypothetical example in this case of a sine wave which is let's say a five dimensional space or five dimensions and something and then coming down to one dimension which you at the end of the day you still have come to a lower dimension the only dimension that will matter is that the straight line dimension that and that is called kernel PCA. That is the kernel PCA. So you can generalize a lot of these linear methods using the kernel approach. People call it the kernel method often, that going to a higher dimension, solving it and coming back. The kernel trick is more widely applicable beyond just support vector machines. So there's a large body of literature on kernels. Kernels are still going very strong. They're in many, many areas of machine learning. So guys, before I move on from here, I would like to know, are we clear? Did we understand what we just did? We used the kernel to go to a higher dimension which is linear and from there we actually found a shortcut to a much lower dimension, right? So it is almost opposite. Kernel takes you up, PCA brings you down, but the net result is down, lower dimension. So the kernel we'd want to use in this example be like a sine wave or? Yes, it could be a sine wave kernel. You could use a, you could just custom make it or you could use a, actually polynomial kernel would serve you better because remember we did the fifth degree polynomial and that served us quite well in ML100. So if the data distribution is like three dimension in kind of blob or like, let's say amoeba shaped in three dimension, is it possible to reduce the dimension in that thing? The problem is mathematics is insanely powerful. So the idea is that, yes, you can do that. The question is, so there does exist some space where the problem is linearized. The question with the kernels that you have implemented in code, see the code doesn't, the libraries don't go and try every possible function mapping. They have just a few tricks up their sleeve. They will use a linear kernel, they will use a polynomial kernel, or they will use the exponential kernel, you know, the radial basis function kernel, the gossip kind of kernel, these three mappings that I mentioned. Where did I mention? It's these three mappings, right? These typically are the ones that comes built in with most libraries. Now libraries are extensible. So to kate your problem could i have just used a sine wave kernel by all means but you can you have to plug it into the library it gives you a way to plug in your custom kernel so asif can we use like a db scan because it is like a different uh type of a non-globular structure? No, that is for clustering. Oh, that is for clustering. It doesn't have relevance here. Here we are doing dimensionality reduction. So, sir, I'm sorry, Patrick. So I have actually a real data set and it's a clustering problem and we are trying to reduce the dimension by PCA. Exactly yesterday we were trying to do that. So as Anil said you know in clustering problem where we don't have the level for the data can we still do dimension reduction right see dimensionality so it's a very good question let me can i rephrase it in simpler terms so irrespective of whether data is meant for classification regression or clustering does it make sense to do dimensionality reduction? Does that summarize what you're asking? So yeah, so that is the question because there is at there is one point where where we have overlap of something and then we have to pick up you know through PCA like what are the most important features and based on those features we can further take it to like uh into consideration to develop some you know something like that so uh yes so let me answer that question for you the simple answer for dimensionality reduction is as a data scientist it's your responsibility to always try dimensionality reduction with complex situations if the problem is very simple it's alright you know but in all come in all situations you should try to reduce the dimensionality of the problem because a it leads to better understanding B it makes the problem uh computationally attractable means manageable yes but you should always do invariant of what problem you're trying to solve you must always try out dimensionality reduction now comes this question of what for technique of dimensionality reduction will actually work if the If you're lucky and the problem is linear, then direct PCA will work. PCA has been there since the 1950s. In fact, it was independently discovered by a lot of people, including Kosambi from my university. So it has been there for now, what, 70 years. Then over the years, the data has nonlinearities. You can then try other more powerful techniques. Today I talked to one of them, kernel PCA. Now, when you try kernel PCA, there are two ways of doing it. Either you let the, for example, the scikit library do it and you tell it, you know, the three kernels are linear polynomial and the rbf this gossip so try all three and see how does your data look see suppose one of the tricks you do is you you you deliberately force it to look at two dimensions like x1 and x2 project down to two dimensions then two dimensions you can visualize, three dimensions also visualize. You can see how well the separations are. You draw the scree plot, you see where is the elbow in the scree plot, how many dimensions do I really need? And then there are mathematical measures to see are they enough. There are words like proportion of variance explained. We did that in ML handwrite, so I won't repeat that. So it just says that how good is your model, is your lower dimensional model in capturing the nuances of the full data. It gives you that. So you can try kernel, if a linear model works fine, if a kernel PCA works, kernel PCA quite often works. The trouble with kernel PCA is that the three kernels that are built in may not be sufficient. You may have to plug in your own kernel and when you try to plug in your own kernel, now we are getting into custom business. You need your own intuition into the data. That is where a good, it is essentially a part of feature engineering. You're thinking about data, you're getting an intuition of what sort of kernel I should apply, and you can get away with it. A lot of people don't. But there are more techniques of dimensionality reduction. For example, here in my bookshelf, I'll give you a whole book, this book. It's a very good book. I don't know if you can see this book. So Asif, can we use like T-SNE and UMAP? Yes, please hold on to them. You guys, do you see this book? Partially. It's a transparent book, sir. No, we can't. No. It is a book that I'm holding on and look into my face. Yes. Right. So this book actually is one of the many books and one thing that I like in this book is it has a pretty good section, let me go here, on non-linear dimensionality reduction and manifold learning. So what is the book name, Asif? Can you post that in the Slack please? Picture of it and post it in the Slack. I will do that. It's a modern multivariate statistical techniques. In this, there is actually a chapter called Nonlinear Dimensionality Reduction and Manifold Learning. This book is very dense in the sense that you have to know our math of data science before you can even begin to understand begin to study it when you do that you will see that it has a lot of technique for dimensionality reduction so if you can see that and most of these techniques uh have to do something called manifold learning you try to learn the underlying surface to which data hugs. If you can discover that surface, you can linearize it. There are all of these techniques. So like Anil was using the word T-S-N-E and UMAP, those are all techniques to do exactly that, to figure out the deep inside, what is that surface to which data hugs? And then to find a way to linearize it, bring it down and separate it in the spaces that you look at, for example, two dimensional or three dimensional spaces. So the field of dimensionality reduction is very rich, especially this whole thing about manifold learning and so forth and LLEs, locally linear embedding. I won't use those words now. We will do those perhaps sometime in the deep learning workshops. Yeah, it will be there. TSNE and all this will come. There's a whole vast literature on dimensionality reduction. So what we did today is we went beyond just PCA and I used the kernel to go to our first step in handling nonlinear data so do that guys why should you do dimensionality reduction because simplifies the problem not only that you gain understanding of it you gain understanding of the data so to answer somebody's question should we do that I said well your question dimensionality reduction is something you should always try you have to know see the point question is will you succeed in the abstract from a pure mathematical perspective there does exist a space right really okay yeah where the data is linear. And now the question is, in that space, can you come from that space to a lower dimensional space? Like in that, is the data, sort of, can I shrink wrap in that space in an ellipsoid? And when I do that, can I throw away most of the axes of the ellipsoid so that the remaining axes that I'm left with, the sum total of those axes, is less than the original space from which I started? If it is true, sometimes it may be true, sometimes it's not true. If it is true, well, you can achieve dimensionality reduction. If not, you cannot. So for example, if you imagine data in a unit desk, like this one, let's look at this data. Do you realize that this data, there is no way... I can't hear you. I think we lost you, Asif. I can hear you guys. Tell me when you can hear me. I can hear you now. But, we can hear you now. But sometime like 5-10 seconds, we lost you. So you know, Asif, when you moved to this circle and started at that point, we lost you. Okay, so I'm saying that when you look at this data, for example, you realize that we created an axis, x3 is equal to x1 squared plus x2 squared. Yes, sir. And so as far as classification is concerned, just one dimension x3 is enough because at a certain value of x3, we could draw a hyperplane and classify. So, from the classification perspective, we actually reduce the problem to just one dimension. Do you see that guys the x-ray dimension that is sufficient to solve the problem yeah so that's the blame sir yes that's it and so and that is just the height from the ground plane from the ground of the point any point just ask how high is it from the ground and that will tell you whether it's a blue point or a yellow point, this is a classic example of how you went to a higher dimension. R. Vijay Mohanaraman â€“ UKRI BASAVATI BHABHATRI MOTUNU, Right only to come down and discover a much lower dimension that will solve your problem right just one dimension that's the beauty of it yeah go ahead so sir uh small question maybe silly question so feature engineering do we do before pca and after pc does it make sense both see these things you know i wish i could answer it one way it goes around in See, machine learning is not a linear process. People think of it as a pipeline. They say, in production, it's a pipeline, but the discovery process is not linear. It is, you know, you cleanse the data, you scale the data, you fill in values, you do outlier treatments, anomalies, and so forth. And then finally, you do you start doing, let us say some feature engineering, some dimensionality reduction, then you do modeling. It looks as though it is like that, but actually it's not. What happens really is, once you have done the first few data preparation parts, the data handling parts, you bring the data. First of all, data doesn't exist as vector spaces, right? The X, we always represent the X vector space, a column vector, an n dimensional vector. It doesn't, you look at the data, it exists as what? It exists as text, right? It exists as an image, it exists as a sound, it exists as columns in a database, a table. And the data is scattered across many, many tables. So one of the first things in your journey is to do those data transformations or what people used to call ETL jobs or things, to just bring in the pieces together and build the X vector. Once you have built the X vector, then you do all of these. You do the cleaning of it, the anomaly detecting, the missing value treatment and all of that. And then you prepare that into it. You create the cleaned out X vector. But you still don't know if these are the right feature vectors to go into your algorithm. You don't know, right? What do you do? Then many things can happen. Maybe there's a lower dimensional space that represents it. Maybe there is another vector that we can build out of it, the feature engineering like we did for river. Or like you see in this problem, it is not x1 or x2 that matters, it's X3 that matters. So how do you discover X3? Then you go start going around in circles. You partly use intuition, partly try SVM, partly try different techniques. And you see that when they begin to work, you ask, why did it work? Can I, for example, use it, go back and do something? So in reality, after X, there is a journey to let us say, X prime, the feature engineered vector, the feature vector, and this is the thing, usually the input vector and feature vectors are different. Right? So this is sort of an iterative process, you go back and forth, you go all the way to the classification and model building and you keep coming back and it is the circle of learning you know somebody used to some i suppose in us we teach children a very simple phrase that science is captured the scientific spirit is captured in three words that form a cycle you ask a a question, is this true? You start with a hypothesis, then you measure, you gather data to test the hypothesis, get the data in, learn from that. What did you learn? Did it support your hypothesis or not? And then you come up with a different hypothesis. You go back to asking the next question. So you are forever in the circle of ask, measure, learn. And that's how you build better and better and better models. And this is the classic journey of science. So you keep going in circles there. So any non-trivial science project you do, that is the reason it takes a long time. See, I'll give you a real life intuition. In your career, for most data scientists to be successful in their workplace, they don't work with too many data sets. Here we are doing different data sets at a time. We're getting practice. It's good for learning. In workplace, you will just get one kind of data set. Maybe you're working for, I'll take a hypothetical example, Zillow. All the data set will be about real estate, isn't it? And one team will be trying one sort of algorithms or variations of it for a long, long time before they have a breakthrough idea and start trying different algorithms. Quite often they'll be going around in circles trying to optimize, make it better, the predictions for just one problem. In the case of Zillow, it's like better predicting the house value, let's just say. And then all sorts of other sub-problems associated with it. You can keep building models. So your entire career there in a team can be associated with solving one problem. So obviously it can't be a linear problem that you went through the journey correctly and you have a model. No, you keep on moving around in circles, getting better and better and better models, which also sometimes means finding the right features or right inputs and then asking people to go gather those inputs right so so that is how this means are we together guys so all right today i covered the second topic which is kernel pca I would like to, before I go into the break, cover one more topic, which is using the distance kernel for KNM. So I'm closing this topic, guys. Should I, do you want to take a break before we start on the next topic? Or can I start in the next topic and then take a break? That's a a question. Go ahead. If we use a kernel to express our data on a higher dimension, does this simply translate the understanding of the data into something that's easier to wrangle? Or will this transformation introduce a likelihood for variance errors? No, not really actually. When you go in a disciplined way up to higher dimensions, you don't necessarily need to know to introduce a lot of variance into the problem. Look at this thing. From here, the circle, when you lift it up to the parabolic surface it was a very controlled function isn't it it was just not a half as an expansion of the dimensionality of the problem you don't you know you're not introducing more variables more flexible parameters it's a very disciplined approach to go there so this won't affect your directly your bias way straight up like it's not in the obvious way okay thank you yeah all right guys so would you we did something uh i i can see that we did something conceptually a little bit heavy at this point would you guys prefer taking a short break to absorb all this think about it and then we'll start in 10-15 minutes yes yes break yes that works let's do that then we'll come back so today guys we'll have two breaks perhaps or maybe one break and a very long session after this because we do have a long session after this i'll pause the recording. See you guys in 15, 15-20 minutes. Thank you. Grazie. Gracias. E aÃ­ Thank you. E aÃ­ Gracias. Thank you. E aÃ­ Thank you. Gracias. Thank you. um um Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. . you Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. so All right guys, let's start getting back. Hello. Yeah, we have pretty good participation today. Nice. So guys, this is a question that came up. It's that how many contact hours are there in the boot camp? And how does it differ from the workshop? So here's the difference. In the workshop, officially, there were every week, five contact hours, two and a half, two days a week, two and a half hours, 7 to 9.30. That was the official or the promised, the guaranteed one. In reality, we are doing this, we are obviously doing the two evenings. Let's say five hours, sometimes it goes more and then we are doing the Saturday and the Sunday sessions. So I would say that we are probably doing about seven, eight, would it be fair to say about eight contact hours a week in workshop yes so promised was five we are doing about eight in the boot camp the promised number of contact hours is we are going to do a two weekdays evenings what we are doing in the workshop and all of Saturday right 12 hours on Saturday I will be there in other words I will be helping you guys from 10 a.m till midnight but if you finish your projects earlier you can leave so that will be the actually more than 12 more like 14 hours but still let's call it 12 hours 10 p.m uh goes and we will be actually more than 12, more like 14 hours, but still let's call it 12 hours, 10 p.m. And we will be doing the five hours during the regular week. And then those extra sessions will still continue. The reason I made it two weeks, two days, is because I expect that the extra sessions will be the other two days. So in practical terms, when you're doing the boot camp, you will be spending a whole lot of time with me, you will be spending more like every evening other than Friday evening, you will be spending with me. And the Sunday, I'll again be giving extra time so it will be more like 20 of two and a half to the five ten ten hours during the week and uh 12 hours during the weekend that's 22 hours and three hours on sunday so 25 hours a week so all of it put together will come to 25 hours a week so it will be compared to eight hours that we are giving here, it will be three times more intense. So be mentally prepared for that. When we do the bootcamp, we will take it very seriously. We'll do a lot of work here. We develop the theory there. What we will do is on the weekdays, we'll develop the theory. The way it will work is on Tuesday or maybe actually I'll move it to Monday and Wednesday. On Monday, we'll develop the theory. The way it will work is on Tuesday or maybe actually I'll move it to Monday and Wednesday. On Monday we'll do the theory just as we do now. On Wednesday we'll do the lab, walk through lab, guided lab. Then in between Tuesday we will do our quiz session. On Thursday I'll do the extra help session, coding help sessions and so forth. And so that all of it is preparing you for the actual project session which will take all of Saturday. So it will be 25 hours of engagement that you'll get from me. It all amounts and you will have to put in those minimum hours. Of course, you'll have to do more work in the background study and so on and so forth in the material. So all of it put together, it's a hundred hours. So it is hundred hours. For those of you who are taking only workshop, hundred hours at $1,000. So that comes to $10 an hour. I suppose that's a fairly below market tuition rate in Silicon Valley. But so that's that. But if you're taking it is below like a federal rate, minimum rate is below the minimum rate. Yes. So that's that and if you of course are going to take all three of the bootcamps, the deep learning, the whole sequence, then it is even less than you're paying $800, which comes to about $8 an hour of engagement. So that is the way the economics of it works out. Now, so that's the economics of it. But then guys, when you come to the bootcamp, you have to be mentally prepared. It won't be like the workshop. It is the workshop plus much more. It'll be intense. You'll have a lot of fun. The projects will be there every Saturday, twice a day. Your team will be making presentations. So use this time in ML 200, and deliberately there's a one and a half months gap to brush up on your programming. The people who got the most out of the workshop last work the boot camp and all of them got a lot out of it most of them I would imagine they all did become very comfortable with coding and then they so they had fun you know solving the problems and presenting their own team solutions and so forth so that the the pleasure of boot camp is that it is not just one-way traffic it isn't that i'm teaching you that will happen during the weekdays the sheer pleasure of the boot camp is what you bring forward under guidance of course what you bring forward, under guidance of course, what you guys bring forward as a team and what you bring out as presentations. It builds up your confidence, it builds up your resume and it helps you create good notebooks and blogs and things like that. It essentially establishes your confidence. So you will do half the work, more than half the work. You'll do the projects and I'll be guiding you on Saturday. In this workshop I'm not really, I'm expecting you to do the project. I am there to help you but it's not a mandatory part. I mean I know that some of you haven't yet gotten started with the project. That is not good. You should start but in the boot camp project is a central part of what we do. You'll do fairly long projects you learn a lot learn amazing things from images imagine looking at x-ray images and being able to tell go with being practical things being able to tell a tree from another tree or something like that you know with classification being able to do a lot of this uh textual analysis summarization many things or being able to detect whether a face belongs to a person or not right so for example if i have just one picture of you can i make sure that the person that the camera is seeing walking towards the door is you. Those are very practical situations. So those will all be in the projects. And we'll do them obviously at the end of the workshop, I'll give you the solutions to the project, but usually you don't need to, because all the teams succeed in creating some solutions. And they all showcase it like they present it at the end of the day to each other. So I hope I answered this question that was coming coming how is it different from the boot camp i have two questions actually one is on the saturday session you say will that be in person oh unfortunately yes and here we know we live in the kovit world is exploding it won't be jay we know we live in the kovid world is exploding it won't be in person but one of the things that has happened is in the last few months we have all in the educational field been developing means to make it as real as possible so it will be very real for you what you'll miss yeah the other question i had is um would you have some break you know after the first between first and the second month and the second and the third month or would you go like three months boom boom boom no actually what happens is at the end of each month we will give a small break of two weeks is because a lot of people for example the experience was that they like to clean up their notebooks because during the during the boot camp you're rushing forward then people like to publish it or you know put it on github and so forth showcase it public github so in showcasing they ask for time to to dress it up essentially showcasing they ask for time to to dress it up essentially right so we will have a small gap between the sessions but very small gap i don't want the continuity to break so it will be it will feel like contiguous we'll just miss a week in between that's all yeah yes go ahead so this uh between this so this you are talking about the deep learning bootcamp, right? That is true. So this is so is there going to be like, are you going to have the deep learning bootcamp after the math? Or it will go in parallel? Or how long the math context basically, the math is going to be a whole month of intense, like it will be a little bit more intense than our current workshop. Math is going to be a whole month of intense, like it'll be a little bit more intense than our current workshop. See our workshop was six weeks, right? So we'll squeeze the same number of hours into a month. Okay, we will have so that you can have a little bit of a gap between math and the next one, the boot camp, but it will be one after the other. Oh it will be so you will do math and then your deep learning will start. Actually the math will be five weeks but we'll squeeze a little bit more material than the ML 200. See as you notice right in ML 200, are you guys getting the feeling that we are covering more than ML 100? Anybody? Or that was the intention that we cover more territory. Anybody feels that we are covering more than ML 100? Yeah, totally covering more material. So I'll dial up the pace. Every successive workshop that you take, the pace will increase the material. Not just the it will be different material but the quantity of material will be more. So the ML, the math course will do the entire 400 page book. But if you remember in the ML 100 and ML 200, we took, we are taking essentially two courses to do the 400 page ISLR book. In the second, the math one will cover the whole book well not exactly the whole book but large part of it uh in like five weeks essentially four to five weeks and then deep learning book can start then deep learning so in other words a gear up for momentum guys these may be the most intense years or months you have gone through after your college but that is the whole point you want to learn it deeply and thoroughly right so get up for that and you'll have a lot of fun people had a lot of fun in the last week last bootcamp so as if approximately when deep learning is starting is it so remember It's first September. There is even a sign up link. Prachi, could you please put up the sign up link again on the Slack? Yeah, I'll do that. I don't know how many of you have registered, but I would encourage you. I mean, personally, right, when I was learning all of these things, obviously, I come from a different culture. I mean, we used to read, we used to spend, I took a lot of time and I'm talking about 25 to 30 years ago, picking up these things but it was still not a very big field but whatever it was, it was mostly through self-study and asking and the professors and begging them for time and so on and so forth. That used to be the the world now i am essentially this trying to be i don't know how successful i am but i'm trying to be the professor i wish i had when i was learning these things so go at it guys see the return on investment is humongous i mean even if you look at it not from the joy of the subject, but from a pure financial perspective, a typical software engineer these days makes what? In the range of 130 to 160 base salary. A typical person who has skills that we are learning, their base salary starts north of 200k. What does it take? A little bit of money, but more than that, a lot of just six months of your life. You give up six months of your life to working very, very hard, generally working and picking up the material, not slacking. And then you come out with the other end and would you rather not earn 50% more than you're earning today? So even financially, it makes sense. So, more than you're earning today. So even financially, it makes sense. So the boot camp, I would say that I mean, and I'm not saying it's a carrot. Come again. I'm saying that's the carrot. That's the carrot. Yes, that's sort of the carrot. Yeah. So guys, I'm not because trust me that I, my day job compensation is pretty good. I'm not doing it because trust me that my day job compensation is pretty good. I'm not doing it to earn money. Though money is a consideration, support vectors is tanking at this moment, it's negative. But I do want to break even and all of that, there is a bit of financial incentive. But that's not the real reason I'm asking you guys to register for the bootcamp and so forth. It will genuinely you so if you and this math and all of this if you have started on this journey finish it properly it will benefit you a lot um it will and you can you can those of you who are in college you can give it a contrast with what you learn in college it's nice to see you here santa barbitvik can you hear me? You're in mute. All right Ritvik isn't here and all right so we could have asked him how so he'll tell you like how does this course is compared to the ones in college it's a different experience it's much more intense. All right, guys, so with that, I'll get back to the subject. So yes, we'll have math and we'll have, math actually I have to announce, Prachee do remind me to start the announcements and then for math immediately. I've been too busy with all of this. Asif, how long is the, what's the duration of the boot camp class in September? Four weeks or six weeks? No, no. The boot camp is four weeks. Four weeks. But it is a hundred hours. So let me put it in perspective. Yeah, I got that. It used to be 30 hours, five hours a week, six weeks. That is a hundred hours. Sandeep went through the boot camp pass cam yeah hence it was right so but that was just one saturdays only on saturdays right you tried to pack everything in on saturdays no no no no no no no let me guys let me re uh emphasize you will have monday wednesday and saturday Saturday formally Tuesday will be quiz and Thursday would be the coding help sessions and so forth that I gave see at this moment what do I have I have two official sessions and two unofficial sections official sessions that is the promised ones are the Monday Wednesday Monday theory Wednesday lab then we have Saturday for quizzes and question interview questions and Sunday we do or Saturday sometime we do another coding help and Python or something like that so these four sessions will continue but the dynamics will change Monday Wednesday will remain theory and lab the quiz will move to Tuesday the extra help quiz will move to tuesday the extra help sessions will move to thursday so so the thursday will become optional but at least the first three days monday tuesday and the wednesday will uh strongly suggest you attend then all of saturday so the only day really you are free i got that that. I was talking about when Sandeep did the boot camp, it was just only Saturday. It is not the same. This is much more intense. It was spread over 12 weeks and it was only Saturdays. That is right. And this also spread into a 12 weeks college. It is three boot camps. But each boot camp is way more intense than the October boot camp. In other words, you'll have a lot more to learn, a lot more engagement, and so forth. ASIF, did you consider six weeks instead of four? Just checking, because this COVID day is 150% now. So we will calibrate it. Let us start. If you guys feel it's very intense, it's our workshop. It's our bootcamp, guys. If all of you feel we need to slow down a little bit, we'll slow down a little bit. But let's not give up already. Let's start and see if the intensity is too much, we'll slow down. How about that? Sure. It's more about finding those many hours to spend on that that that's that's where the concern is yeah i'll give you a tip you know you when you are in grad school and doing research in one of the top places in u.s there is a basic rule if you if you get a good night's sleep you should pretty much feel ashamed of yourself anybody who has been through that experience in grad school shanka you must have gone through that sorry i missed the question asif what is it something about grad school that's cool right i i got through that experience in an undergraduate math class. Oh, you did? Yes. Definitely. Yeah. You learn most when your mind has no space to think anything else. Like you're a hundred percent occupied with one subject, one topic. That's why in grad schools here in US, the good ones, you are working on something all the time and you practically feel guilty if you get a good nine hour, eight, nine hour sleep. Most of the time you're working very hard during the weekdays, like getting five, six hours of sleep and getting back to work. And you're sacrificing weekends to study and so forth. Oh, the first weekend I took was on my birthday after one year. After coming to the US and working all six, seven days in grad school, the first time I took a break on my birthday the following year and I felt guilty about it. Yes. That's how it is. So guys, obviously I won't push you that hard. I know that you all have families. Working is hard with kids at home and work expectation paradoxically has gone up. Employers have found out that you can, it's an unfortunate fact that employers have figured out that you can push people to work harder when they work from home. They're getting a lot more productivity out of people and people are having struggling because they have kids at home they have many other everything the maid is not coming for those of you who do use a maid and so forth so everything falls on you let us put it this way let us aim high and if we need to slow down we'll slow down how about that guys does that seem a good framework Does that seem a good framework? That sounds fair. Yeah, let's start there. And we'll take it. We will know how much we can push through. It is, I would, sorry, to your question, I would easily expand it to if needed six weeks, if needed eight weeks per workshop or for bootcamp, doesn't matter. The point is I have a certain quantity of material that we have to cover that we absolutely have to cover the syllabus right so whatever time it takes to cover the syllabus properly we will take that time and I was doing magic I can just keep at it because yeah I said this time can you just publish what are the you are covering for each of the bootcamp three that will be easier to uh get settled on on that yeah yeah so yes see how she did some updates in the slack right yes i could drop some updates so basically guys bootcamp one is foundational it will give you understanding it will get you started in every area or most areas of deep learning it will get you started in every area, or most areas of deep learning. It will get you started with natural language processing, text processing, the transformers, the RNNs, and so on and so forth. It will get you started with image processing, the CNNs, Convulation Neural Nets, and so forth. It will get you started with how to deal with embeddings, you know, word embeddings, graph embeddings. Yeah, you'll get started with graph neural networks and graph embeddings and any embedding of any categorical variable and tabular data. How do you deal with tabular data? And it will get you started most importantly with something that for some reason nobody seems to be teaching, but it's very, very important.'s autumn automated machine learning how do you create you know okay so let me give you a bit of a background see it is actually part of your next lecture but let me give you a hint about it do you notice that we support vector machines with trees and already you're getting algorithms which have a lot of hyper parameters do we understand what hyper parameters are guys like for example in forest, how many trees to have? How deep should each tree be? How many features should each, you know, at each split point a tree should look at? These are all hyperparameters of the model in support vector machines. Which kernel to take? What should be the parameter of the kernel? Like what should be the gamma for a radial basis function, what should be the degree of the polynomial, so on and so forth. So in every complex algorithms, there are all these hyper parameters. And the difference between tuning the hyper parameter and not tuning the hyper parameter is vast. So it leads to all sorts of theories on how to find the best hyper parameter. If you look at the books, they tell you that, oh, the way to do hyper parameters, find the best hyperparameter. If you look at the books, they tell you that, oh, the way to do hyperparameters, find the best hyperparameters is to do something called grid search. Maybe that's sort of the kindergarten of the field. Then you can do a randomized search and so forth. But actually the state of the art has moved ahead. Today, we have an ability to do very efficient search to the hyper parameter space and find the best classifiers or the best regressors or whatever it is you want let's take classifiers as an example the best classifiers you can tune them and the performance gains are like huge very very impressive performance gains of the model. And not only that, I've taught you so many algorithms now. We have learned about decision trees, random forest, support vector machines, logistic regression, linear discriminant analysis, quadratic discriminant analysis, and this and that and many things. So the question is, and each of them will come with some hyperparameter tuning. So how do you search for the best model for your data? And therein comes this field of automated ML. There's a whole body of research and this breakthrough work happening for strange reasons at this moment. It hasn't made its way into textbooks yet. It will. Maybe soon you'll hear that this is the latest. So we will cover automated ML in a big way. So again, deep learning foundations, textual, the visual, the audio visual, and graph and automated ML. These are the six topics of the first workshop. The second workshop will take it far deeper. Second workshop will focus on convolational neural networks. And we will do all the powerful techniques that are important today. Like we'll bring in the residuals and the steaming. A lot of things we'll do focused on convolations. The third workshop is completely focused on transformers. Actually, I might do transformers before I do the... So second and third are interchangeable. I'm actually inclined to do natural language processing before the transformer before. Those of you who came to my attention is all you need to talk. You pretty much have an idea of what we'll be going through. We'll do all of that attention, transformers, RNNs, all sorts of text processing, embeddings, and so forth. That will be the third workshop. We'll go pretty deep and do pretty large projects or rather boot camp. So these three workshops, ML 400 is all of those, two of those? No, no, no. ML 400 is the first. 410, 420. There's 400, 410, 420. Ah. Dr. G R Narsimha Rao, M.D.: A 410 420 there's 410 420 Dr. G R Narsimha Rao, M.D.: Right. So the one that I just released is only the ML 400 which will cover this five topics. Actually, there's one more topic which is Dr. G R Narsimha Rao, M.D.: Anomaly detection time permitting will do that. So every day like a first day will be a bit lighter first week I'll get you through the whole foundation but every three successive weeks we'll cover about two topics a week right we'll move fast whereas in the subsequent workshops we'll move deep as if in the the subsequent workshops the second and the third one what are the domain verticals if you can tell us that you're covering in terms of like NLP or others. Yes, yes. So this one workshop after that will be purely NLP, completely NLP. Okay. Are we together? Yeah. Not, see it will use transformers and so forth. They can be used for even for images. so let's say that 90 percent of it is people are finding non-standard users for all of those those theoretical concepts so we'll do all that then another workshop will be another boot camp will be entirely devoted to going really deep in the computer vision part right see when we do deep learning right i won't be covering much of computer vision theory okay but you really want to do image processing, it isn't just a neural network. You have to know the subject. Okay. So we'll also do the theory of computer vision, the formulation of computer vision. So we'll go deep in that workshop. See, the idea is, guys, that my goal is, and I hope I reach it, is that by the end of all of this, when you are done, you guys come out in the top, hopefully, at least the 10% of the data scientists are anywhere with any level of experience, and at least 10% and hopefully better, more like the top three, 4% of that, or 5% of that. You go through a rigorous training, it's really worth it. Andrew Ng, the guy who founded Coursera, he used to make a very interesting joke at Stanford. I haven't seen him make that joke now, but when he was younger. When you would take your Stanford course in machine learning in those days, that course is still there, I believe, what is it called? 223 or 225 or something like that. When you would take that machine learning class, you would submit your first homework on linear regression. By now, guys, you must have realized that linear regression is the kindergarten of this field. Everybody starts with linear regression. It's very powerful. It gets a lot of things done, but it's straightforward. The theory is relatively straightforward compared to all the things that you learn after that. So you would submit your homework and then Andrew Eng used to congratulate students in the class. He says, congratulations, you are all now bona fide computer scientists, like AI specialists or machine learning specialists. And they would say, what, are you kidding? This must be a joke. And then he would say that actually it is a joke, there's truth to it, because most people who walk around Silicon Valley claim to know, claim to be machine learning experts, happen to know less than what you know. Which is the tragedy is still true, I believe in this field. Most people are, you know, they have picked up scikit-learn, they have picked up the libraries, TensorFlow, Keras, etc. They are all people who are mechanics, you know, who are just using the tools to get the work done. They are not people who understand machine learning at all, deeply. What I'm trying to do is build you guys up to be permanently in this field with a deeper understanding. So that's the basics, the foundations. Yes. See, what's the difference between a doctor and a nurse? There are many ways the trade-offs are there. Nursing tradition starts with Florence Nightingale. It's focused on caring. But if you look at the, so the emotionally the nurses take care of you a lot more than doctors do. Doctor will come look at you, give a prescription, walk off, give a consultation and walk off. But I say that as a child of a doctor. But if you look at the effect, nurses are trained in medical sciences, doctors are trained in medical sciences. The difference is the depth and rigor. Doctors go through far deeper, more rigorous training in the subject. By the time you become a specialist in cardiology, let's say, or anything, neurology, you have gone through years and years of rigorous training and study. So that's the difference. And the point is that if you are in a field, be a master of the field, don't be one of the, you know, one more guy on the street. So that's, anyway, that's a long interlude guys. So I'll get back to the subject at hand. Any other questions before I start the recording and we get back to the subject at hand? As if regarding the coding in the boot camp. Yes. Will you also talk about like quantized aware training, like for edge computing? Or is that another topic on its own? Oh, no, not in the first boot camp. Oh, goodness. But you do remind me of something that I should cover. In fact, Madhu keeps asking about this. See guys, till now you have just been building the model and testing it out, right? And you're feeling good, okay, I built a good model. But at the end of the day, in real world, you take your model to production. You haven't, because we had limited hours, just five official hours a week in the workshop, you have not taken your models to production, isn't it? So, okay, actually that's a very important part of the bootcamp. In every bootcamp, all the models that you build, we will take it to production in the cloud. Your team will have to deploy it to the cloud and I'll take you through the whole process of deploying to the cloud you will have to have a full end-to-end application a front end to it right simple front end it need not look pretty but you will have a real experience end-to-end experience of what is it to build a machine practical machine learning or an ai application or what is it to build a practical machine learning or an AI application, end to end. So the journey will not stop by saying, oh, look, I'm getting good predictions. That is all right for ML 100, 200, but those days will be over now. Now we'll make it very, very real. So at the end of it, your transition from the bootcamp to the job should feel like a no brainer. You will be doing a lot of real life, what you do in the workplace, but you'll do it at a much faster pace in the workplace. You do it in six, nine, one year time frame. Here you'll do it in a month time frame. We'll take, in fact, weekly time frame. Every single model that you build, you will take it to production all the way. You'll create an inference engine. For example, you should be able to, from your cell phone, use it. Are we together, guys? Yes, that will be awesome. Yeah, that is actually a core part of doing it. Boot camps gives you more time, like I said, right? 100 hours of engagement gets you a lot of time to learn a lot of things. So we'll learn a lot there. All right, guys. With that, any more questions. Otherwise, I'll get back to the main thing we should get back to our stuff now. Should I start the recording now, guys. Sure, Okay. So guys to recap, what did we do in the first part of the session? The first part of the session was that we were trying to get an intuition. Why is it that when we go to a different space from the original input space quite often, and usually a higher dimensional space or just a transformed space, you can see that decision boundaries for classification and for regression of course you can do a linear regression in effect and for dimensionality reduction your old technique of PCA for example can be applied you just need to take a kernel you need to go to a higher dimension where the problem has straightened out so you can wrap an ellipsoidal shrink wrap around your data and then go about doing your pca thereafter does it always work not necessarily but it does work a surprising number of times you should certainly try it one of the questions that was raised is if i do kernel pca will will i always succeed the answer is different First is that the built-in kernels in the libraries are limited, linear, polynomial, and the RBF. If you're capable of growing your own kernels and thinking through the data, this is again, feature engineering. You might have much more success, but if you use the vanilla one, you may not succeed. Yeah, you may not have as much success. The second aspect that comes to it is that the problem truly cannot be reduced in dimensions. It takes the whole space. For example, you would agree that this problem is something that looks as though it's too... Actually, this is a simple problem. We could reduce it to one dimension, the z-axis, the x3-axis, but there are problems which cannot be reduced to lower dimensions truly information is scattered in the entire volumetric space of the feature vector space like in the entire volume of the feature vector space then you can't do dimensional reduction and lastly not every technique has its limitations kernel PC, while it is more powerful than PCA perhaps, well, powerful is a word, not always more powerful, sometimes more powerful, but it's not so powerful that it can handle every situation. The field of dimensionality reduction is rich. There are many algorithms, just like for classification and regression, there are many algorithms that you use to be able to classify data. So the many techniques for dimensionality reduction, it's a very rich literature. There's this whole field of manifold learning and so on and so forth. We'll learn it and they're beautiful visualizations, the TSNE, the UMAP and so on and so forth, which you'll get very hands-on experience in the next class that we can but here we won't have the time for that but i'll stop here what we will come to now is we will see how we modified a pca to get dimensionality reduction we can actually modify knn to make it a little bit smarter we'll use distance kernels so the the idea is this So the idea is this. See, I told you that if you have a point here, where was I? Let's go back to our motivating example, the cows and ducks. So suppose you have your ducks here. And now you guys are familiar with this example, so I wouldn't draw too many points. And let us say that you have cows here in the feature space. And obviously, the way we motivated this is by saying this is weight. X1 is weight. And X2, let us say, is the size of the animal. Very intuitively obvious that ducks are small and lightweight relative to cows. And cows are rather big and heavy creatures way out there in the feature space obviously spread will be more so I'll spread it out okay something like this so the question is this renders itself very easily to birds of a feather notion if you want to find out what is this point this guy it is you look at the neighbors or this this, you look at the neighbors, it's pretty obvious that things this size and this weight are much more likely to be ducks rather than cows. You look at this point, you don't have data for this point, you get an animal whose weight is that and size is given by that value. And intuition says that it must be a cow because things similar to it are cows in the feature space, the neighbouring points in the feature space. That is the intuition between K and N, K nearest neighbour. K nearest neighbour. But then I also pointed out that this has a limitation for it. So the limitation comes from the fact that suppose I bring in some things that are bigger in size but not as heavy. I don't know. I cooked a giraffe but biologists will please excuse me if this analogy is wrong in terms of weight and so forth. So let us say that this is it. So how do you know whether something is a duck or a giraffe? You can say, well, I'm going to look at the neighboring points. But then the one problem that remains is how many neighboring points. If you take k is less than 1, I'm just recapitulating what we did. If you take k less than 1, sorry, not, what am I saying, silly. If you take k is equal to one you you can often get it wrong isn't it accidentally or by chance more likely that the nearest neighbors may be of the other kind isn't it so let's look at this example suppose you are here which point should i take at this point do you see this point that I drew with the small, where my mouse is, guys? Yeah? This point, if you ask what is it? Is it a giraffe or is it a cow? If you use k is equal to 1, the answer would be it's a cow. But if you take k is equal to let us say 3 now you'll say giraffe isn't it and let us say that you have in this particular meadow you have a lot more ducks around let's say that in your data set there are hundreds of ducks. So now suppose you take k is equal to 200, let's say. What do you think your answer will be? It will be the entire data set. And what is the majority here? Duck. So you will end up with an answer of duck. So you notice how your answer varies as you go from here to there. This is the traditional journey from high variance to high and so let me connect it to this concept of high variance to high bias. At k is equal to 1, you are in a situation of high variance overfitting to a situation of k is large. Let me just say k tends to infinity, high bias. In fact, k is equal to infinity. Obviously, you won't reach infinity because you have finite number of sample points, but in practical terms, whatever the sample size is, n, you will end up with a situation of bias, and it will degenerate into the baseline or the zero-R classifier. no more terrible spelling I have a degenerate rate into the baseline or zero r classifier what does the baseline or zero r classifier say its answer to every question is the majority right whichever is the most common so in this space space, cows, ducks, and giraffes, it turns out the ducks are most common. So a baseline classifier doesn't care about anything. It won't even care about the neighborhood. It will always answer the answer to any question which what animal it is, it's a duck. Is k the number of neighbors? k the number of neighbors k the number of neighbors yes indeed thank you for asking that if i didn't clarify that k is the number of neighbors so it is the hyper parameter in this model so i mean the value of k the k equal one give you high bias too simple no high variance you will see why it is the the decision boundary is very complex and very sensitive to uh and if you take another sample of data your answer will change and i will illustrate that point in a moment right this is something you should know actually this is it so this other end is easy to understand isn't it if your answer just simply doesn't change irrespective of what you do you're in the state of obviously an extreme bias and there is no variance at all then you answer isn't it zero variance so um it's i remember if i may just say, there was almost a silly joke. Once I encountered a fairly silly book that somebody had written, and the title of the book is Quantum Explanations or something like that. I was very fascinated because I used to love theoretical physics. I picked up the book, and in the book, the explanation of every phenomena was this mythical thing called quantum form so everything happens because there's a quantum right so obviously rather asleep because disappointed and i felt like a fool spending money on it but that's that so a high buyer situation is like that the answer to every problem is just duck right I have an animal. This is the weight. This is the size. This is it. This, that. All the attributes. And what's the answer? It's duck. So it is your point. It's one extreme. It's a 0R classifier. This doesn't depend on the data. Absolutely doesn't depend on the data. That's why it's called 0R. It doesn't look at any of the features at all. One answer for the entire feature space. So now what is the journey from k is equal to 1 to k is equal to n and how do we illustrate this? So I'll illustrate this with an example and in the notes I give more examples but here let me take some simple motivating example. I'll make a different picture here. Two, three, four, five, six, right? And now let me take some points, one, two, three, four, five, six, seven, eight, nine. And let me just bring in some errors, this and while we are at it, something like this and then blue points also let me throw in something here and a couple of things here so guys let's try to buy build a decision boundary let's take an arbitrary point suppose i take a point here what will this be this little point what do you think it will be but clearly it's a yellow point isn't it the it seems supposed any we are taking K is equal to 1 1 nearest neighbor so what is the nearest neighbor to this point nice it's the yellow and then it will be yellow yellow and the nearest neighbor to this point is blue. You would realize that if I take the perpendicular bisector of this, the decision boundary has to be here, isn't it? Then you look at, then again here, any point that you take here between these two, once again, you'll build a decision boundary somewhere, actually somewhere close to this triangular space, somewhere here the decision boundary will go. Then it will go through here. And then it will go through here. Do you see how we are doing the decision boundary? And it's getting complex, but okay. I will sort of, and then it gets even more complex i don't know how to deal with this but we'll deal with it at some way it goes up here it goes through this and then it goes through comes back and then goes through like this something like this you get your K is equal to one decision boundary. Keep in mind, I'll just make it a smooth curve even though it's actually made up of little. Many perpendicular bisectors. Exactly, sort of those lines or those edges. So this is your decision boundary. Now, keep the same thought in your mind and I'll use a different color color let me use the green color perhaps for k is equal to uh let's take k is equal to how many points are there three seven k is equal to three let's look at k is equal to three three seven k is equal to three let's look at k is equal to three go back anywhere you realize that up to here right very close to it it will actually be the three nearest neighbors will be if you look at this let's ask about this point what are the three nearest neighbors one two three so it will be declared as blue isn't it this point is blue what about this point it will be declared as three nearest neighbors are one two and which one do you want to say this one three nearest neighbors so what will you declare this point to be blue right this will still be blue what about this point at this point itself any in the vicinity of this point would still be blue do you realize that even if you are sitting on a yellow point if you take the three nearest neighbors it would still be depending upon neighbors you are taking these neighbors uh it is still a blue point so what is happening is that the decision boundary is going much more smoothly right now let's look at this point what are the three nearest neighbors it is yellow so it will go something like this right so far and then uh here these are the blue neighbors, this is yellow. Let us look at this point. Where are we? Three nearest neighbors are yellow. Right? So it will turn around. It will. Now we ask this question. What about this point is yellow? What about this point? This point is blue if you notice. The three nearest neighbors are, I don't know, depending upon how you look at it. Let me cheat a little bit to simplify that. You would say that this is blue. This is blue. Do you realize that? Okay, suppose let me move the problem a little bit more to make the meaning clear. And there's so far. I had some more point. So you realize that you're, I mean, obviously I'm cheating a little bit here. Your decision boundary might be like this. What can you say about that versus the white decision boundary? Do you notice that the white is far more curvy? Isn't it? It's more complex. Yeah. Now let's take K is equal to six. Let's take another color. I'm just taking illustrative examples here and let's take this. K is equal to six then what happens the problem begins to simplify quite a lot actually uh i would like to believe that if you are here most of your neighbors let's look at the six nearest neighbors one two three four these two would be the nearest neighbor. 5 is, and whether this is the next neighbor or this is the next neighbor depends upon where you are. Let us say that this is it. So this point is still yellow. And if you draw this decision boundary, perhaps it would be even more smooth, actually. How would it look like? I don't know. It would probably look like something like, let me just simplify it. I don't know if this is something, but I'm trying to just develop the intuition and perhaps cheating a little bit in my mind, but we'll do it actually in code and you'll see it much more realistically so what can you say of the decision boundaries do you notice that the decision boundary keeps getting less complex keeps getting less complex isn't it in the asymptotic limit of k is equal to, let's say that k is equal to the maximum number of points whatever it is, let's say 20, what would be the answer? Majority. Which points are more? One, two. I clearly see blue points are more. So what will be the decision boundary? Here. It will say that all points are blue because I have more blue points. Are we together, guys? If you look at k is equal to 20 and you take 20 neighbors or 25 neighbors, your answer will always be that all points are blue and you have a very simple decision boundary. What is the decision boundary? Everything is blue. You can just put any line beyond all the points and put all the points on one side and that becomes your decision boundary. Actually, it is even more extreme. Your decision boundary can be anything like something like this. Oops, sorry. That is, let me use the right color. It could be anything like this. So all the points on this side are blue is this is this obvious guys is this are we making our seats clear right so decision boundary becomes less complex and so now what happens think about it this way if i take another sampling of points this decision boundary will not change but on the other extreme k is equal to 1 will change you can you would agree that if I took a different position of the blues and yellows the decision boundary will immediately shift is this point obvious guys yeah it's a different sample will have this blues and greens a different place and so the decision boundary will change. So there's very high sort of a variance of the decision boundary. As you keep taking different samples, your answer keeps changing. It's a classic example of what is it a classic example of? It's of example of variance. High variance. High variance. The high variance or overfitting. This k is equal to 1 is listening too much to the data. It's a very, very overfit solution. So that is the journey from overfitting to underfitting at the other extreme. So now one way that you could remedy this is you can bring the kernel to the rescue. You use a distance kernel. Let's bring in the concept of a distance kernel. So remember, what is the notion of distance? Distance between two points is what? It is anything that follows these properties. It is anything that follows these properties, d x, x prime is equal to zero, if and only if x is equal to x prime, x and x prime are the same. And just recapitulating what we said is the definition of distance. And furthermore, we said that there's a triangular inequality, d x to some other point so you take a detour plus D obviously let me put this as vectors in some space or something like that a DZ to DX prime will always be more than or equal to the distance from direct distance from X prime in very practical terms when you go to, quite often your journey to your desk always involves a Z, which may be the coffee machine, the water cooler, or something like that. But your journey home is always a straight journey back, and which feels shorter. Obviously the journey back. Anyway, I'm joking so this is this is this defines a concept of distance a typical distances that we talked about at the Minkowski distance distance and just as a recap what we talked about is the distance was that there is the D the norm dn of x between x and x prime. So let us define x, delta x is equal to x minus x prime. And the mod of this to be this, absolute difference. So by definition, the Minkowski norm of n is equal to n1 to infinity is defined as delta xi. So the component wise, you could look at the component delta xi is the ith component. So the component wise delta xi to the power n summed over all of them in other words it is delta x1 n plus delta x2 n and the whole thing summation over i and the whole thing to the power 1 over n. You have to take the nth root of it. 1 over, oh, sorry, this is n, n should be here, nth power, and 1, 1 nth. You remember that d is equal to 1, if you recapitulate, was the, what kind of a distance was it? Manhattan distance. L1 of Manhattan, yeah. Yeah, n is equal to 1, yeah yeah yeah n is equal to one sorry the n is equal to one and people often use the word l so let me use the l l is equal to one l uh depending upon which community of crowd you come from but machine learning crowd people tend to use the word l l norm the word people use is l norm When they do that, they always are referring to the Minkowski distance. This is your Euclidean distance. Your Euclidean distance and go ahead. So one thing is, I'll give you an intuition of how it looks in space. Take a unit. What does a circle look like in Euclidean distance? L is equal to 2. Actually, let me just put it here. L is equal to 2 is here. L is equal to 2 is what? You realize that if you make all points, so let us have a definition of a circle circle is definite defined as all points and let me call it the unit circle so that i don't have to worry about radius radius is one all points at unit distance from origin so this is the definition, right? We will define the circle as that. Anybody would like to argue with that definition? Is that self-evident? That's how we define circles? Yes. So now let's think what happens when we do it for L is equal to 1, the 1 norm. So points which are at the same distance are here but the surprising thing is because we say that delta x so x there is no x x1 norm x norm 1 plus x2 norm is equal to 1. what does this equation look like this this is the equation of a straight line it actually looks like this and let me give you the intuition. See, you can go unit distance in this direction, right? Or you can go unit distance in this direction. Or you can do this way. You can go halfway here and halfway up. Or you could go three fourth way up here and one fourth way up. Or you could go one fourth way and you could go three fourth way up. But ultimately, if you are a taxi driver with only, let's say that gas in your tank for one unit distance, this is pretty much the envelope of how far you'll travel. Do we see this guys? Yes. Yes. We say that a circle for L1 norm is actually diamond shaped. It's not round, it's diamond shaped. Do you see this diamond shaped circle? This is how it is. And that raises a very interesting issue. Well, the first time you see this, you're a bit surprised, because a fundamental notion of circle has always been round doesn't it but in a different norm the circle looks differently in fact what happens is if you go to L is equal to half it will become like this much more pointy and we will need this intuition when we do regularization it will begin to look like that on the other hand if you do so this is a L is equal to put the word here L is equal to 1 this is L is equal to 2 what does L is equal to 3 and so forth look like so remember that we had a unit circle for L is equal to 2, the L is equal to 3 will develop a little bit of a bulge over this. It will look like this. Are we together? It's bulged out a little bit. This is L is equal to 3. And then when you remember, we talked of the infinite norm. L equal to infinity right and that devolved to the largest value the largest Delta Delta X max right so that looks in this triangle looks like this so it looks actually like a unit square actually. The circle devolves into unit square. Let me explain why that is so. What is L exactly? The L is, see we talked about the L-norm. The L is this L to the power L. So let me give you, let me okay recap this what L is. So recap. L in Minkowski norm. Minkowski's definition of a distance at Minkowski norm. It is called the norm. What it means is, look at this expression. The ellipse norm of a distance between two points is a generalization of Euclidean distance. You take the x1 minus x1 prime absolute value to the power L plus x2 minus x2 prime to the absolute value L. And you keep doing on, but let's stick to two dimensions. And to the 1 by L, the square root, the L-th square root of this, where L can take values from 1, 2 one two three all the way to infinity so now come again like yellow's dimension no l is not dimension l is the measure it's a way of measuring distance that's why it's called norm your data may be in two dimension but you may still take the infinite norm in fact let, let's look at two dimensions. So if you look at it, L is equal to D2. What does it become? D2xx' is our familiar Euclidean distance, x1 minus x1' square plus x2 minus x2' square, square root. Now you realize that that for even powers you don't even need the mod so this is nothing but X 1 minus X 1 prime square plus X 2 minus X 2 prime square this you remember from your basic high school pre-calculus or algebra what is this this euclidean okay this is your euclidean distance now d1 the first norm of x and x prime is equal to the same expression becomes x1 minus x1 prime mod plus x2 minus x2 prime mod. That's it. And because the 1th root of a number is 1 itself. What does this look like? This looks like your Manhattan distance or the taxi driver's distance. The idea being that in Manhattan, cities are on a a grid so you can only go along this direction right so this was from a previous lecture so I would say it may be helpful to watch the video of the last lecture which I just haven't posted let me the YouTube recording is there the live recordings are there you should do that and I'll put a cleaned up portion also but it's all there guys are you aware that all the lectures like including this one it is being live broadcast on youtube you can actually watch this entire class sitting comfortably on your sofa and watching it on your television and not only that after life those recordings stay public And not only that, after live, those recordings stay public. All the class recordings are at this moment public and present on the YouTube support vector site. If you haven't gone there, please go there, subscribe to it, watch those videos, and you will get that. So this is a recap of what we did before. So this was, who was asking this question? Sonal, was that you? Or Kamya? Sorry, who was asking this question? About the L, yes? Yeah, that was me. Me as in? Sonal. Okay, good. So, Sonal, did you understand this idea now? Yeah, I did. So this is, so now the peculiar thing is, see what mathematicians do is they take intuition from real life and then they abstract it, you know, they make it more general. They go beyond that, try to go beyond that, right? So that is what we are trying to do. So this is basically just a different distance formula, right? This is indeed just a different distance formula. And guys, one second, the recording somehow got paused. Has the recording paused? I don't know. I'll just pause and start it again. start it again. Okay, so this is just another distance function that's absolutely correct. So mathematicians keep generalizing from simple terms. So then it leads to beautiful consequences. For example, the d is equal to 2 we are familiar with. It a nice round circle that's a practical day-to-day intuition d is equal to one this is uh sorry not d l is equal to two the second norm is a familiar euclidean uh the l is equal to one takes a bit of a surprise but then if you think of taxi drivers in manhattan they can so suppose they have to go unit distance they can go unit distance here or they can go three-fourth of the way and one fourth of the way up or they can go half distance here and half distance here or they could just go along this axis all four units right so if you look at the unit distances their former they will form a diamond shape or rhombus shape. Right? So, okay. So, sorry, this picture I already do. It will form this shape, right? And if you, this is L is equal to one. And if you can even go to fractional values of L, and when you do that, you will begin to look this, like this beautiful shape. So you realize that a circle in different norms looks, let's say one half looks like this at the other end if you keep on doing it do you notice that it's beginning to bulge the bulge increases this straightens out it begins to develop a bulge and at the other end of this extreme it becomes this square how does it become a square think about this point when you are here well the distance is of course equal to here what about here when you are when you are here like what is your distance at this particular moment? You will realize after some time that this point is at unit distance because this distance, the y distance is to be, the x2 distance is to be ignored. The L is equal to infinity just looks at the bigger of the two distances. So which is the bigger of the two here? This distance. So it is still this, this still this this this this till you reach this point at which point now you are equidistant both these two distances along this along this are the same and after that it is the distance along this so the the unit circle becomes a square it will take you a moment to think it through so this is the Minkowski norm. Well, why are we bringing this? We'll use Minkowski norm a lot in machine learning, but here we'll use it in using something interesting. Just keep this thing in your mind and don't worry about it. And now, distance. Let's bring it to our neighbors neighbors oh it's uh here nearest by the way do understand minkowski norm because we'll need it in regularization today i was going to do regularization it's 10 i want to let you go in another happener but so we'll keep the topic for for next time actually or for an extra session. By the way, I'm ready to give you guys an extra session on the, on what? On boosting. So maybe we will need two extra sessions in this workshop to finish off things. But alright, let's look at this. K nearest neighbors. You realize that suppose you have neighbors which are far off. Look at this. Where are we? Where is our intuition? Yeah, look at this. If you look at this problem, you agree that the further going far a little bit makes sense. You know, taking three neighbors, four neighbors makes sense. But if you go too far, you again begin to lose it, because far off neighbors are not representative of the local situation, isn't it? If you take a K is equal to 200, all you get are ducks, isn't it? So the question is, can you bring some intuition to improve upon it? So one intuition that you can bring is to say, hey, you know what? People who are my close neighbors are more representative of me than neighbors which are far off. Are we together? So you can say that somebody who lives on my street, give that person twice the weightage compared to the guy who lives in the next city when you want to know something about you does that intuition make sense guys yes uh yes so so something like your attention model right oh yes yes it's like that but don't bring that here at this moment okay so uh so at this moment it's a simple intuition that the further off you go the less those neighbors are representative of you so what should we do it's okay to go to those neighbors but we just need to weigh it in we have to take a weighted average let's say that we take the vote you take their votes a bit less value than the words of the people immediately to you right or if you're doing a regression you're taking the average take a weighted average then weigh it by neighbor so suppose you have three neighbors w1 x1 whatever the value for this is right let's say that the value the y1 actually the value let's say that you're trying to predict y hat and based on the average of three of three points x and there are three neighbors to y2 y1's value y2 values y3 value does this make sense guys suppose you're trying to find the weight of a duck you look at three neighboring ducks and see their weights those are the Well, weight is not the thing. But something, okay. Whatever it is, you want to find the weight based on their size, right? So you take three ducks nearby and you take the average of their weight. Does that make sense? But the thing is, you weigh the nearest neighbor more. Let's say that the nearest is one, farthest is three. So for example, you could say, let me weigh this four times, let me weigh this two times, and let me far off by, let me weight once. Distribute the weight a little bit so that the value that i come to and then of course divide the total weight by seven so a four seventh two seventh and one seventh right uh is the weight that you associate with each of these readings so then what do you get you get a weight that is more representative of this than this are we making sense guys yeah but at the same time you you're not only looking at this guy's weight you're looking at other people's weight also so when you do that what you're doing is you're doing a distance aware or distance weight aware or distance weighed nearest neighbor. K nearest neighbor. And you may still take K nearest neighbor. K is still there. How many neighbors you take. But one thing you realize that suppose you take K as very big. K is equal to 50, but your 50th neighbor is very far away what will be the coefficient here it will probably be zero zero one something very small value the idea is that you make the far of neighbors very small they they influence very small so that they don't they don't have as much vote as the first guy. Right? So what you do is the intuition is that you take the votes or the weights or this regression or this thing, and you create a weightage function, a weight function. Actually, you call it the kernel. So actually, that also has a K value. So I won't use the K. Let me use the word weight, right? A weight that is a function of a distance between two points, Wd. More formally, it's a function of distance between two points, xn. Distance of a point to all of its neighbors, Di. It goes like this. Some decay function you need something that decreases it may decrease like this it may decrease like this it may decrease like this do you realize that each of these will serve the purpose right you just say that the further off you are the distances the less the weight you will give to that point in making a decision about thousand ducks or taking the weight of the denim and so forth. Is this intuition clear guys? You need a distance, any function, any function which fulfills the criteria, it is monotonically decreasing with distance. It is decreasing with distance. It cannot just go increase. This function will not do. This is not a good weight function this is meaningless right yeah emphasis like it goes up and down etc it won't do so we won't do that actually let me just put it here and this is not you need something that decreases it may not even decrease it may decrease very slowly or it may not decrease at all it may just remain so if you notice that if k if the weight doesn't decrease at all with distance it is your k nearest neighbor without a kernel without a distance awareness do you realize that guys that the standard knn is nothing but in this terminology distance awareness in which the weight function is constant. Is this intuition making sense, guys? Yes. It's straightforward, right? But you want something better than that. So sometimes people even use a step function. They say that if you are within a certain distance, I will consider you uniformly. After that, I won't consider you. So what happened is that soon people began to ask, these weights were called, W's are called distance kernels. And it's the same idea of the kernel. So I won't go into more mathematics. So the thing is, so long as it is any function that decreases monotonically, or at least decreases, never increases at all, monotonically, is a valid distance function. So there was an effort to find all sorts of distance functions. What is a good distance function? What is a good kernel? And there was a cottage industry trying to prove that this kernel is better than that kernel is better than that. And so there are many kernels that people began to use. Anything could do a sine wave. You know, the what is it? The cosine goes like this. That is a perfectly good. Again, it is decreasing. It will do. You could use anything that you want that decreases. You could use a bell curve, this one. This too is a good kernel. So people came up with a lot of kernels and they began to investigate which kernel is better than the other. At the end of the day, they found out actually, surprisingly, that the choice of kernel doesn't matter much. It does matter a little bit. But in most situations, changing from one kernel to another gives you a tiny bit of improvement in your classification or regression it doesn't make a tremendous difference so at this point what i'll do is let me just move on to the other computer and i'll show you examples in from the notes by the way there's no cell give you of what I mean in this context give me a moment so where am I let me share this screen here my desktop this is it. Let's open my... What is happening here? Okay, my system is, OK. Much better. So distance we've gained. So I'll give you guys this chapter. Now it's from this chapter. How do I minimize this? OK, so guys so guys look at this are you all able to see my screen yeah suppose we are writing a classifier that distinguishes between the blue point and the uh what color would we call it orange points orange points is that close orange points here let me zoom in a little bit and go to a single view, single page view. Page now, view more single page. Okay, guys. So are you able to see this picture clearly? Yeah. There's a lot of noise here. Points the orange and the blue are mixed up, but still you can say that there is a separation. So we will take different values of k and see what happens. If you treat it as a simple linear regression problem, a line would just go through like this. Now, what happens when you take k is equal to one nearest neighbor, one nearest neighbor? Do you see how complex your decision boundary is? And I'll give you a moment to stare at it and convince yourself that it is the right decision boundary. Obviously, I did it programmatically, so it happens to be right. I would quote regenerate it. So in this situation, you have all these points and you have this K nearest neighbor decision boundary. You see all these islands and peninsulas and so forth and this sort of backwaters and whatnot. If you look at this, if you treat blue as the sea and orange as the land, you see all sorts of geographical features. You five nearest neighbor do you notice that the decision boundary is simplifying itself does this look simpler guys yeah yes right then you go to k is equal to 15 and it has simplified itself even more you take k is equal to 100 and now it's this straight You take k is equal to 100 and now it's this straight line. Not straight line, but okay, much more smoother, much more stretched out line. So do you see the variance decreasing as you increase the number of neighbors? And the question is, which is the best value of k to take? And the answer to that is whichever value gives you the best predictions on the test data isn't it on the validation set whichever gives you the best predictions that is your value of k so proof of the pudding is in the eating you have to validate it against data see what the predictions are how do they come out and base it on that now i just mentioned that we have two kinds of, I mean I'm just taking the impact of norms. How does Euclidean norm and the Manhattan norm affect? For the same set of points actually the Euclidean norm and the Manhattan norms, do you notice that they build different decision boundaries? They will break it up differently. It's just something to know that you can just play with the notion of distance and change the decision boundary. One question that people ask is, hey, a K-nearest neighbor, assuming that you have a notion of distance, assuming that you have a dimensionality of the problem is small and you don't have the curse of dimensionality that we talked about last time. Isn't this a lovely thing? Won't it always outperform other atoms? Yeah, sodium chloride. Yes. So I take this example of sodium chloride. Actually, it's not true. But because if you look at a lattice structure in an atom, in this lattices, so salt is a lattice structure you will always get it wrong k nearest neighbor will always come out wrong irrespective of what value of k is because the nearest neighbor of a sodium atom will be a chlorine atom always and always of a chlorine would be a sodium atom and if you think whatever k you take you'll keep getting it wrong right again goes to the no freelance theorem that there is no way out you know nothing is perfect everything has its flaws now when you get this knob so you'll review what I just taught you about the P knob by the way instead of L I've used P mathematicians tend to use the word P and the data science community we tend to use the word L just trivial distances but if you notice this diagram that you have here in this box and some of you are repeating the class already have this note but do you notice how it goes from very small like concave like structure to convex diamond and convex in circle and finally to a square that's how it behaves now how do you this distance function that you have what is a distance function it follows this inequality now this is an example of the kernels people have one easy kernel is just one over r simple isn't it another popular kernel is just take the positive half of the bell curve. Distances are positive. This is the positive half of the bell curve. By the way, what is this constant in front? It is just a normalization constant so that the integral comes to one. This is called the parabolic kernel. It was associated with this gentleman ipane chiknov chikov i i can never quite get the pronunciation right how my apologies to this great man in the technical yeah if i need to go for it yeah so this is this kind of yeah parabolic kernel so it's it's like this. A simple triangular kernel, it just goes like this. Well, people are creative. You can use cosine as a kernel. You can use a tricube kernel, which is this, which goes to the third power. Then a weighted tricube kernel. So you can see that at one point people went crazy trying to come up with better and better kernels. And they thought that the kernel, the actual shape of the kernel mattered a lot. Today, we know that it matters a little bit, not too much. And so here are all of these kernels, you know, put here. Throw away the negative side, just look at the positive side. You see how they decay. So the bottom line is it doesn't matter much, but it does matter a little bit. So now you have two hyper parameters in your model, K, the number of nearest neighbors to take, and which kernel to pick. Generally what happens is if you use kernels, K doesn't matter so much. The number of neighbors, it gets muted. It is not something that affects the model in a huge degree. It does matter, but not too much. It doesn't matter that much. So something to remember. You tend to take rather large k's, but let the kernel take care of them. So this is it. This is just saying whatever I said in words in a more formal language. So how do you do regression with it? Very simple. You just take the nearest neighbor and take the average of the output, average of the predictions. So you can do that. If you apply to Hill dataset, you'll realize it makes a pretty good value. This is a lab. It is done in R. So I'll hand out the solution in R. If you apply it to the river data set, let's see how well it does. And I have the prediction mapped out for different values of k. When you do that, you realize that it is the maximum accuracy that you get is for eight neighbors for the river data set. You should take eight neighbors. So we did a search for the best model. Eight neighbors turn out to be best by the way if you rerun it and for different people it is sometimes seven or sometimes eight or something like that so and the accuracy is 87.3 which is what we expect we remember that the maximum accuracy you can get is around that 89 or something like that right so it's pretty good model isn't it the nearest neighbor algorithm especially with the kernel then how does the accuracy of the model vary with k you can see that you know afar in the beginning accuracy improves as you increase k and after that it starts decreasing as you take more and more neighbors you're considering further of neighbors and so your accuracy tends to be true your point of best accuracy is around eight you get that one way to simulate higher dimension of course you can't see me retire dimension is to take sparse samples of the data to learn. Because in high dimension data becomes sparse. So with the river dataset, you could just take very few samples to learn from and so it will simulate high dimension. When you do that, you realize that once again, the more you go to higher dimensions, the accuracy is better. Like, is like the bigger the sample, the more the accuracy, which means that the lower the, in lower dimensional spaces, uh, KNN works well in high dimensional spaces, you begin to get into trouble. It's the curse of dimensionality that we talked about. Right. All right. So this illustrates that the same thing for the flag dataset. I'll post this notes, by the way, I put it on the website, all this illustrates that. The same thing for the flag data set. I'll post this notes. By the way, I put it on the website, all of these notes. I know that Slack is getting a little disorganized. There are too many things here. I'll post it to Slack also, into this. So we have the flag data set here. Once again, the same accuracy curve that you see. It peaks around, what is the peak value here? You get a peak around k is equal to around k to 5 to 10. In this plateau range you get maximum accuracy of 57%. The Hill dataset, what does it do? Hill dataset gets the predictions right with very high degree of accuracy 99% coefficient of determination, the r squared. Very high. So these things work. Now I give this as a problem. Look at this problem guys. I can give you the data if you guys want the challenge. The code to generate the data is here. You look at this data and you realize that building a regression model for this can be pretty hard especially a linear regression model do you agree guys because the surface is anything but linear do you see that guys anybody yes's a highly nonlinear surface. And yet, if you have rich enough data, it fulfills our criteria, low dimensions. If you have enough data points, key and nearest neighbor can do extremely well. And I leave that for you as an exercise. I'll give you guys the data and give you the formula. This is basically a modified form of what is called a sinc function. I call it the flower data set. And so along with your thing, there's also the flower data set. I'll release it. And so you can try it out. You'll be surprised. Kernel KNN works like a charm, right? But it's very hard to model a parametric model like a linear polynomial equation to this. It's a very complicated polynomial. Do you notice that? Looks like a flower. So, well, guys, today we had a long session. I'll end with that. We did not get to regularization. I will consider whether to do it in an extra session or to do it next week. i will consider whether to do it in an extra session or to do it next week right we have one theory session left we can choose regularization we can choose a recommender system recommender system actually in second thoughts we are going to do in the math class very deeply so let me skip that and keep next week for regularization. This week, one of these days, I will give this week or weekend, perhaps the extra session, we'll give on boosting. Let's do it. So when would you prefer guys, during the week or during the Saturday? Boosting. Let's do it on Saturday. Let's do it on Saturday. Let's keep it on Saturday. Let's do it on Saturday. Let's keep it for Saturday. Yeah, boosting Saturday is okay. And Monday we'll do regularization. That will be the last topic for this course. And after that, our fourth week, we'll get into the math of machine learning. Maybe a week of gap. We'll start. So with those words, I will end today's session. If you have questions, stay back. I'll answer those questions. Any questions, guys? Yeah, there's another method where the number is for the number of clusters as opposed to the number of neighbors, right? The centroids? Yes, for the second one.