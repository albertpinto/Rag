 Welcome back folks. Today is Thursday of the, I believe, the fifth week and that makes it, and it is August 11th. This is Thursday evening here in California and perhaps Friday morning in India. And obviously for people in the Midwest, it's probably 9pm. So let us start by reviewing what we learned the last time. Last time, we learned about classifiers. In particular, we learned that in a classifier to model or to train a model to be able to successfully classify, geometrically it is the same as searching for a decision boundary, isn't it? So what we are doing is we are looking into the feature space and we are saying, where is the separation between the two classes? Now, the two classes may be well separated. For example, the ducks and the cows, if you look at their weight and sizes, it is just very unlikely that you'll ever confuse a duck for a cow or cow for a duck. So finding a separating hyperplane in simple terms, separating either a plane or a line in one or two dimensions is... Could you please not do that? Turn off the lights at the bottom, please. Yes. Yeah. So finding a decision boundary is quite straightforward. Right? Now, when you go to higher dimensions, it generalizes quite intuitively. Beyond three dimensions, our mind can't visualize. But logically, it is exactly the same whether it is four dimensions or 400 dimensions you are still searching for a separating hyperplane between the data now not just a hyperplane a separating decision boundary the simplest thing is it could be a hyperplane but it need not be we went to the concept that it could be any It could be a hyperplane, but it need not be. We went to the concept that it could be any hypersurface. It is not necessarily required or constrained to be linear or planar in structure. It could be any hypersurface. We do tend to, in most of the classifiers, we do tend to require that those hypersurf surfaces be smooth and continuous, which is a fairly common sense requirement. So those such hyper surfaces in a very rough way, people in the machine learning literature, you'll often see the word, some people will call it a hyper surface, some call it with an embedded manifold, a submanifold as the decision boundary. So submanifold is literally that. It sounds like a big word, but actually once you get familiar with it, you can think of it just as a hypersurface. There is a little more technicality to it. What it means in a manifold or these hypersurfaces to say that they are smooth and continuous is to say that if you're a tiny ant and you're sitting anywhere to you the world looks flat it locally looks flat for example for the longest time on earth we used to think that the world is flat now most of us are persuaded that it is not. We live on a giant ellipsoidal solid, but that comes through the knowledge of physics, right, or going to the moon and looking at the Earth. But if you were sitting somewhere on Earth, it is quite understandable that you would call it flat, approximately, right? And so they can say the surface of the earth in a very rough approximation is a manifold, though it wouldn't strictly follow the definition of manifold because there are points at which it is not smooth. It is pretty rugged, right? Mountain tops and so on and so forth. But yes, that's the intuition. So that is the notion of a decision boundary. Now, you can find the decision boundary using many approaches. One of the approaches was logistic regression, which essentially says that if you take the log of odds, the concept of odds, of something being a grape or not, it is directly proportional, and so you set it as identical, to the distance from the decision boundary, how far you are from the decision boundary. So for example, if you go from the deep blue territory and you keep driving straight towards the decision boundary, and you go past the decision boundary and keep going the further you go the more you are into the grape territory the more likely that the fruit you're seeing is a grape that is all there is to it now that is the that this notion of distance is inherent to all linear classifiers in ml200'll encounter another lovely idea called support vector machines. And those support vector machines, once again, this notion of distance from a decision boundary will become critical. And this expression that we wrote for the distance will come in. Now, without explanation, I claim that the distance is given by this distance function. So today, what I will do is, before I go into the next topic, I will speak to it. Then there were many metrics for deciding whether a classifier is good or not. At its root, you could just look at accuracy or error rate. Trouble with accuracy or error rate is, it is misleading, especially to the common man, when there is a huge asymmetry of data. We took the case of rare diseases, or relatively rare diseases, like for example, a breast cancer, or prostate cancer, or pancreatic cancer. Most people don't have it. So if somebody claims they have a diagnostic tool and they most people don't have it so if somebody claims they have a diagnostic tool and they can tell whether you have it or not and whosoever comes you just say you don't have it right but that quack will still be able to claim a very high degree of accuracy because the number of people who do have it in a population is probably 0.1 population is probably 0.1 percent and so the the the bogus diagnostic tool will still have 99.9 percent accuracy which is the danger of using accuracy so beware of just considering one metric especially accuracy or error rate right you have to first look at the dummy classifier. Dummy classifier will just say, we'll make the bogus prediction. Whatever is the majority or the dominant outcome or output or response, it will always claim that to be the answer. So you look at the accuracy of that, and any system that claims to have learned from the data must beat that dummy classifier, right, a bogus classifier. And that is important. Whenever you start your analysis, start with a dummy classifier. It sets a baseline for you. How would you know that you're improving? That would be the way to know that you're improving. So think of it as somewhat equivalent to the null hypothesis in regression. The null hypothesis states, I'm going to ignore all the variables because none of them matter in predicting the response. And the response is always the mean or the median of the response values. Now, that is for numbers. Now, for classifications with targets are cats and dogs or cows and ducks. You just find out which is more. If there are more ducks around the meadow, call everything a duck. Predict everything as a duck. That would be the equivalent. Here, it's a dummy classifier. Means you ignored all the input. Then we talked about two other simpler algorithms linear discriminant analysis and quadratic discriminant analysis what do they do they make a fundamental assumption they say that the each of the classes in the feature space exists like it is concentrated into uh specific regions of the feature space. Like for example, if you look at the ducks, all the ducks are small and feathery things. Their size are small, their weight is small. And all the cows, they're big and heavy things. And there is a vast separation between the two. And when you look at the size and weight of ducks, that together they form a bell hill. The more technical term would be a multivariate Gaussian distribution. And we talked about it, 2D normal distribution or 2D Gaussian distribution. And let's generalize from there to multivariate Gaussian distribution. But if I may suggest, use simple terms, use Bell Hill, just a hill, nice round hill, right? So you will have two nice round hills between the blue and the, between the cows and the ducks. And what do you do? Connect the tippy tops, the hilltops from both of them and in the feature space, the center, or more technically speaking, find the mean vector of the ducks, the mean vector of the cows, and then join the two and find the perpendicular bisector. And that is a good decision boundary. That is the linear discriminant analysis. Now, we can say the same thing in pretty complex language. We can see, assuming that each of the classes is a multivariate Gaussian distribution, right? To take the perpendicular bisector of the segment connecting the two means, you get the decision boundary. Right? But I hope we can say that without jargon in simpler terms. Now, the fundamental assumption with linear discriminant analysis is that the means are different. I mean, each of the hills are centered at different places, but their variances are the same. Now, you might say, well, that's not quite accurate. The variance of weights of a duck or size of a duck is much, much smaller than the variance of the weights of a cow, right? Cow weights are much more spread out, right? But the weights of the ducks are not. Now what happens with machine learning quite often is you can make simplifying assumptions and you get away with it. Like for example, in the case of cows and ducks, you will notice that you get away with it because your decision boundary, even though it sort of ignores the variance, the separate variances, it still works. But sometimes it doesn't work. Sometimes you have to be sensitive to the two variances being separate then your model has one extra parameter first you took a you had two different mean vectors so you know where the two hills are centered and a common variance vector commons variance value okay or in multi-dimensionals you can say a common covariance matrix right but let's not get into complicated language let's just say the two hills look equally fat right all around but our shapes look similar but in quadratic discriminant analysis you say well even the shapes look different they are normal, but they are spread out differently, which is acknowledging the reality, especially for cows and ducks. And when you do so, then you end up with something called quadratic discriminant analysis. Its characteristic feature is the decision boundary will be a quadratic shape. What are quadratic shape examples of parabola, ellipse, and so on and so forth? And we saw that in the lab. So that is LDA and QDA. Now, I mentioned the LDA word in machine learning is often considered as two separate algorithms. There's another algorithm which has the same name, LDA, acronym, same name. That is the latent Dirichlet allocation, often used for topic modeling, one of the simpler methods of topic modeling. And quite often these days when people say LDA, they often mean topic modeling, latent Dirichlet allocation, because natural language processing is very hot these days. But remember, we are not talking about that. I see. Yes. Would you say that Gaussian mixture models is a form of discriminative analysis? Pretty much. You are basically saying with one, it is a Gaussian mixture model with one constraint. The instances of one class forms only one Gaussian, not a mixture of Gaussians. The whole dataset forms a mixture of Gaussians, but suppose you are talking of cows, ducks, and horses. The only the cow data condition on the cow will be one Gaussian, not more than one Gaussian. On the duck will again be one Gaussian, and on the horse will again be one Gaussian. So the space, the feature space does therefore have a mixture of Gaussians, right? Like for example, this is, where are we? Where is my cows and ducks? Look at this. This is two Gaussians, isn't it? So you can say it's a Gaussian mixture. But when people talk of Gaussian mixture, sometimes they broaden it to mean that they generalize it to say even each class could be itself a mixture of Gaussian's. So it could be bimodal, it may have two Gaussian's or three Gaussian's. So that thing you have to be aware of okay yeah but it's a very good question all right folks so with that was a summary of last time now uh i would like you to check uh yeah or precision and recall uh we talked about it uh what is what does recall do we saw the picture of the recall, but intuitively, in simple terms, how not to miss, like a high recall is how not to miss positive cases. Let's say people have, you have a diagnostic tool that is checking for cancer. A high recall tool would be a tool where, which you would want as a screening test. recall tool would be a tool where which you would want as a screening test at the expense of having some false positive you want to make sure that all the positive cases are caught isn't it so that will be a high recall diagnostic tool and you want screening tools to be on the other hand just before putting somebody under the knife for an invasive surgery or let's say chemotherapy or radiation therapy, you want to be absolutely absolutely sure that the person does have cancer isn't it? So then what you need is high precision. You want to be able to tell a person who genuinely has cancer apart from a person who does not have cancer. So which goal you shoot for depends upon what you're using the algorithm for. For screening, you want high precision. As a diagnostic tool of last resort before taking invasive measures, it has to be high precision. That's one way to think about it. That is, by the way, the mental model that I carry in my head. So another way to put it is that when there is a, let's say there's a riot in India, those are very common. And here comes the police and it rounds up all sorts of suspects to be troublemakers. There's unrest. So at that moment, they don't have, because it's a very blunt instrument, they are rounding off a lot of people. They just want to make sure that all the troublemakers are also there in the pool. They don't miss a troublemaker. But they do know that they're also catching a lot of innocent people who then are brought to a police station and further interrogated. And hopefully the interrogation is a more precise tool, in which case they're able to tell the guilty, the miscreants separate from the innocent bystanders. Isn't it? So that is again again another way to say precision and recall rounding a lot of people quickly is a high recall you hope to just have high recall catch hold of everybody and then finally questioning them carefully one by one and that tool has to be high precision right so that's how it goes and i'll let you think of more examples of how you can do that. So then when you don't care either of precision and recall, but there is data asymmetry, a good way to go about it is to take the harmonic mean of precision and recall. That is called the F1 score. Shoot to optimize the F1 score. Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam classifier. All right. So with that, today, I will cover two topics. First, there is a residual topic we didn't cover. I didn't explain the mystery. I just stated something to be true without giving, frankly, any justification for it. I claimed that, let me see what I claimed, the unfinished business is the distance from the decision boundary I claimed is beta naught plus beta one x one plus beta two x two right and to generalize it to higher dimension let's write it as beta naught plus beta vector, right? So where beta vector is a beta 1, a beta 2, and x vector is, of course, x1, x2. And now generalize it. So generalize it to a beta 1, beta 2, a beta 3 in multi-higher dimensions, beta n, right? And likewise, you can generalize this to x1, x2, xn, right, by generalization. Now the thing is, how would you prove this? And let's go back to our coordinate system. Suppose you have this. Imagine that you make any line through the origin. So I'll give you half the explanation first. Imagine that you have a plane. Let's say you have a plane that goes... suppose, well, in two dimensions, your plane is just a line. So let's call it just a line for the time being, x1, x2. And so imagine that this is a decision boundary, right? Or this is a line or the plane. Now, suppose you have a point x. Any point, let's take any random point. I'll take this point x. This is an x vector. It will have some coordinates x1, x2. We agree. Now, draw the vector here and ask yourself, what is its distance from the decision boundary? So you would agree that if the one thing that you want to know before you can do that is, it would be lovely if we could know what is the unit vector. You know the unit vector everywhere pointing like this? The unit vector. Let me call that vector for mysterious reasons, beta hat. Unit vectors are, you know, you put a hat over unit vectors, which is literally, let me just call it beta one, beta two, like this. Suppose this is your unit vector. Suppose this is your unit vector. Unit, of course, and the word is orthonormal. So this is redundant. Orthonormal, by definition, is unit perpendicular vector. Orthonormal, unit perpendicular vector. So I'm using a lot of redundant terms, but simple terms. Intuitively, I trust you see that this vector is perpendicular. You would agree that if you take this vector, you realize that this vector and this distance is the same as this. And you would agree that the dot product between this vector, beta hat, and the x vector, beta hat dot x, is what? It it is equal to let's think about it this is theta this is a beta hat the size of it x the size of it cosine theta isn't it and what is that that is the base of this triangle isn't it guys right that is the base of this triangle which is exactly the same as this which is actually the distance of this point from the decision boundary isn't it the base it is the base of this triangle the same same angle is theta here does it make sense guys it's a very simple geometry. I'm saying you have this situation, perpendicular theta. If this is x, this direction is a beta. So if you know this angle, then x dot x cosine theta is literally the base, is the distance. So the distance would be this. Of course, the size of this is one. The size of this is whatever. And so what you realize is distance is truly distance of a point from this decision boundary is truly beta hat dot x. So you say, well, that is good. But now decision boundaries don't always go through the origin. What can we do further? So it's just a small shift now. Suppose your decision boundary shifted. It is now like this, right? What are we saying? All we need to know is that suppose you still have the unit vector. Suppose you have a point here, x, and you have the unit vector. Now, you realize that if I take the unit vector, the unit vector beta naught, and I say beta hat dot x this, what will it give me? Unfortunately, it will give me this distance. There is this, not only will it give me the distance from the decision boundary, but it will add this distance, which is the distance of the plane from the origin. Let me call this some value x, or let me just call this D, capital D, which is the distance of the plane of the plane here it is just a line from the origin so what happens is that this this total this base if you do this is equal to a d plus dx is this obvious from here guys this part is d and this part is dx. And the base of the triangle, the dot product, is these two things put together. So you can say dx is beta hat dot x minus d, isn't it? And now it is conventional just for reasons, because you can call this It is conventional just for reasons because you can. You can call this beta naught. And so your distance becomes dx is equal to beta naught plus beta hat dot x. And so you get your distance from the decision boundary. Do you see how it is? It's a simple derivation, right, which is the same as in two dimensions. It becomes beta 1 x1 plus beta 2 x2. And therefore, we have proved that the distance from the decision boundary is indeed given by this. We will use this for support vector machines, for maximal margin hyperplane classifiers, and for many situations, and certainly for logistic regression, this applies. So it's a thing to remember that this distance function is sort of overlooked, but it is worth remembering. Now, guys, I hope this derivation is simple if it is not please ask me does anybody want me to repeat it i guess we are okay yeah okay as if i have two questions so what if the line was away from the origin then it should be plus d right you mean uh it is away from the origin right this is d capital d no in the negative territory i'm speaking you make it and let's call this distance minus d so then it will be so it doesn't matter, the point is that whatever it is, like in this particular case, I took d as positive. And so I put a negative sign. If it was negative, then the negative of negative would be positive, right? And my last question was, in the distance formula, right? According to this distance formula, distance can be negative. But according according to what i know distance cannot be negative right no no in our notation distance is negative we admit in fact that's the whole point no the distances go from minus infinity to plus infinity remember grapes and uh let's go back to our grapes and blueberries. Where are our grapes and blueberries? They will come sooner or later. Oh, here are our grapes and blueberries. So look at this. We agreed that if you go up distance here, you see where I'm writing? This distance is negative. So we are not taking the distance in the traditional sense. We are saying our formula for distance admits negative values. In fact, blueberries only exist in the negative direction from the hyperplane. That's where all the blueberries are. see look at this all the blueberries exist in the negative territory for distance all the grapes exist more or less on the positive territory and there is this in-between region where there is some level of uncertainty but more and more probability as the distance gets closer to zero or more positive, it begins to look more and more like grids, doesn't it? That's it. Hari, you got it? Yeah, Asif. What is your second question? Sorry, my second question was the distance question because I got confused. How can distance be negative? In our language, it can be negative okay now you are thinking or you're putting it together with like typically you say that given two points euclidean distance is always positive. But we are using, we are deliberately choosing a notation distance away from the decision boundary. And we chose that on one side of it, we'll call it positive by convention. On another side, we'll call it negative by convention. So it's a choice. All right. So that wraps up the topic of classifiers. So that wraps up the topic of classifiers. Today, we will go back to a question which has to do with the bias-variance trade-off. Remember, so let us recall, we talked about bias-variance trade-off. So let me put an asterisk here, because actually it turns out that bias variance is often a trade-off but not necessarily true. You can have a situation with high bias errors and high variance errors also. So you're not always so lucky that if you decrease bias, your variance will increase. But for simple situations, let's assume that so i mentioned that we can have we can think of the complexity of a model and the intuitive picture that we talk is say the degree of the polynomial for polynomial regression like polyreg for polynomial regression like polyreg how many or how many terms you consider when you write y is equal to beta naught plus beta 1 x1 beta 1 x so let's just take one dimension for the time being plus beta 2 x square plus beta 3 x cube how many dimension how what is the degree of polynomial you're going to do because each degree is becomes effectively a new feature so for example this is like your x2 this is yet another feature x3 so those polynomial terms become you treat them as independent features even though they're not really but it does work. So now what happens is, if you look at that complexity here, n degree, let us say, what happens is that when you build a model, you find that your bias errors decreases as your model becomes better, higher and higher complexity, this is bias error. Bias error, total bias error. I'll just use the word bias error. And then what happens with the variance error, in the beginning, if your model is very simple, there'll be very small variance error. Then, but on the other hand, as your model gets complicated and it becomes flexible, when it model gets complicated, it becomes flexible. When it becomes very flexible, what happens is that it begins to oscillate. And we will see the illustration right away of how it happens. So then you start having high variance errors. Variance or overfitting errors. So think about it this way. If you have only five points, but you create a polynomial of degree six or seven. Now, the system needs at least those many bends, isn't it? So a system with seven degree polynomial will expect six bends. So you'll have to somehow introduce those bends. And what will it do? Those bending will represent oscillations through the data. Do you see that, guys? If you introduce all of those bends, they will show up as oscillations in the data. Do you see that guys? If you introduce all of those bends, they will show up as oscillations in the data. So you will introduce variance errors and that intuitively compared to the real value, you're having y minus y hat will be pretty large. And so you start having high variance errors. Now the total error, if you remember to recap, our total error used to be total error is equal to bias error. Well, for reasons, because variance is a quadratic term, so the unit of bias, you have to square it to make it in the same category. Variance errors, you have to square it to make it in the same category, variance errors, variance errors, plus this epsilon. Now, this is the bias squared plus variance errors, what in the world is this epsilon? Do we recall that? What is this? A residual error? Yes, it is the irreducible residual error. And you just call it the irreducible error and this irreducible error, where does it come from? From the machine, right? It's the machine error. Partly. That is one part of it. The instrumentation error. And the other part, it comes from what you don't know. Remember, I said that. So let's go back to the situation of entrepreneur trying to guess how much ice cream will sell today and deciding how much to buy from the wholesaler. buy from the wholesaler. If the model is based only on the temperature of the day, predicted temperature on the beach for that day, then the entrepreneur has not considered other affective factors, causative factors like wind speed, right? It's not, or how, and whether it's a weekday or a holiday, workday or a holiday, right? Because those factors affect how many children will show up on the beach who last for ice cream, right? And the fact, so irreducible error is also an acknowledgement that there are things the model doesn't know or hasn't accounted for, right? So the total error, now we need a lovely color for this. Let's say that the total error, therefore, which is the sum total of these two, now this color is very similar to the, let me go with green here, so the total error goes like this, and this part represents your goes like this and this part represents your irreducible you can't go below the civil error there's an irreducible error part that is always there and you will get to a minimum you'll sort of reduce the bias errors why variance error or actually i wouldn't plus minima plus a bit plus the minimum bias or residual bias and residual variance that you will still be left with plus small bias plus variances those terms will still remain but this is your smallest error that you'll get so what happens you realize that there is a Goldilocks zone here. If you keep your complexity too little and is equal to, let's say, one, then you're making a very stiff model. If your ground truth is like this. See, if your ground... So let's take two situations or three situations. Suppose A, the ground truth is... Data like this. Let's take another ground truth is data like this. Let's take another ground truth where the data is like this. And let's take another ground truth where the data is like this. So Now think about A, B, C, the three data sets. For A, what do you think, what is the ground truth saying? What could be a more effective model? Which degree of polynomial will give you an effective model? Basically a linear line. so here n is equal to one would be so what will happen is you have actually very low bias errors to begin with right and just the variance errors will start shooting up if you go beyond one what about b what do you think you'll have bens plus one bens plus one so third degree polynomial but actually because we remember it's a odd function so maybe fifth degree polynomial but beyond fifth or seventh degree if you go to n is equal to 20 you will start seeing what the bias errors will pretty much start by variance because your solution will start oscillating. So what happens is that your solution curve, let us say it begins to look like this. You notice that there is a huge gap between prediction and reality. And so the sum of the squared gaps, which is the total variance, it begins to show considerable variance. At the variance, high variance manifests itself says there's oscillations. And there's oscillations is because it's trying to fit more and more bends to the data. Now, if you take, so let me just mark these as boundaries, boundaries here. So if you look at data sets C, it has more bends. So you will start out with n is equal to 1, a high bias, because a straight line is hardly a model for it. It's a very biased thing. But you go to maybe 1, 2, 3, 4, maybe. So let's say that 7, 8 degree polynomial, and it begins to fit, So for, let's say that seven, eight degree polynomial and it begins to fit, right? Depending upon how many bands you will see. So somewhere between four, between five and, between five and eight or whatever it is, someplace in between, you will find that you have the best model, isn't it? So, but in general, this is how it goes, right? Bias way, it's straight off. So now it raises one question. It's quite a, like, how would you know, you don't know the ground truth. You are still discovering the ground truth. How would you not have to do this whole scanning model after model after model to find what is the right complexity. Could you, for example, start with just n is equal to 1, a line, and hope that that will work with more data? Suppose I throw more data at it. Do you think that will make it work? So if you take a biased model, suppose you have a data. Let's take this situation. This is your data set 2,, I believe in your lab. This shape, right? So there is, you know, and you did this lab, that your model tends to look like this. Remember, this was your one dimension, the linear model. Now tell me what happens, observe what happens if you throw a lot of data into it. Suppose you add more and more data to it. Do you think that your model is going to change appreciably? No. We learn a lesson with high bias models, simple models, simple models, these models are rigid. More data generally does not improve improve the model much. This is a thing to remember. Sorry, where did I go? This is a lesson. This is a lesson for us to learn. On the other hand, suppose I take a model. Let's take the same data set, and I will reduce the number of data sets. I will take 1.2.3.4.5 point. As you can see, I've taken very few points. And I give you a polynomial of degree, let's say 10, or let's say nine. You have to make eight bends. So what will happen is, you should be seeing quite a lot of oscillations here. Somehow you have to fit those many curves to the data, right? And the system is flexible enough that it will go through every of the points are we together but now think what will happen? This curve is trying to fit through the data as closely as possible. What do you think will happen to this curve? It will improve as data increases. Improve as data increases because it will gradually begin to look like this. It will still wibble. It is still wiggling, but it is wiggling much closer to the data. The oscillations are suppressed. Isn't it? How are the oscillations suppressed? Because it will still need that four bends or eight bends. Those bends are smaller. Those eight bends are much smaller. Ultimately, it is flexible and it is trying, all the points are attracting the, remember the curve towards itself. So the equilibrium state will be, it will still retain its number of bends, but there will be minor perturbations or there'll be smaller bends, biggles, rather than big oscillations. So we learn a lesson. Complex models right so we learn a lesson high but complex models improve with more data right generally i mean unless your model is totally screwed up and so we can just put as a cautionary word, may improve with more data. If you have a complex but thoroughly useless model, it won't go anywhere. Thoroughly useless hypothesis, then it won't go anywhere. So do we see this, guys? So what has happened is the fact that your model is better fitting the data and it is closer to what your intuition tells you that it should be you say that you have regular so what has taken place is there is a term for this regularization has taken place. So you say that those oscillations are rather irregular or it's sort of not nice. Now things look more regular or better when you throw in more data. So now a couple of words I will say. People often say that more data makes models better. You realize that that's not always true. With a biased model, high bias simple models, you don't get much improvement with high data, more data. On the other hand, if your models are complex but still a sensible model, they get greatly improved with more data. So that's the good news. So more data. So a lesson we learned from that, that a simple way to regularize a complex model, regularize and improve, as you can see, when you regularize, improve as you can see when you regularize you improve it a complex model is with is with more data so more data is a method of regularization it's a regularization method our first lesson but we are going to learn more interesting regularization methods today now before i go there I'll mention one thing. When we, there's an interesting observation these days that if you look at algorithms, let's say linear regression, by now you're convinced that linear regressions are basically straight hyperplanes, right, in whatever number of dimensions. So if the data truly is non-linear, the performance of a linear model may improve for a little bit, right, but it may saturate after a little while. When you throw more data into it, it may saturate very soon, linear model. Then you try more complicated models. And we haven't done those. But when you do that, you notice that more data improves the performance. This is more data. Then more data, it keeps on improving. And then it also saturates a more complex model. And then you take an even more complex model and you see maybe this thing gets pushed up a little bit. You may see behavior like this. And it saturates much later. So do you notice that the saturation point of this was here, the saturation point of this is here, the saturation point of this is here, the saturation point of this is here. One of the things people observed these days is that is models that are bizarrely complicated, that people deal with these days. These are the so called deep neural network models. And one of the reasons that we like them so much for only for complex situations, guys, they actually underperform for simple situations. A linear regression will beat them hands down sometimes, quite often, actually, for simple situations. is that they tend to keep on, their performance keeps on increasing with data, like almost keeps on going up and up and up. And it takes a very, very large amount of data, like something like hundreds of billions of data points before you start seeing saturation right so these are deep learning models right which is why these days they are being considered very very seriously for a broad class of problems to do dealing with computer vision, dealing with text and so forth, and anomaly detections and so forth. But then it is also the limitation. The more complex model you take, if you have a small data set, it will not learn anything. It will just go memorize your data and overfit to it. It has just way too many knobs and it will just turn and say, oh, I fit the data perfectly so you train it on a thousand data points randomly it will build any you don't know what model it will build but it will be a model that will work just perfectly on the training data and horribly on test data right so you can see that this light blue line i mean this line this line would you agree that this line will perform this oscillating bad on test data why will it be bad on test data because suppose you give it a com you give it a real test data which happens to be this later on you can see how big a gap there is between y minus y hat, isn't it? This is y, this is y hat. It is the predictions are way off from the actuality of the ground truth. So complex models, not given sufficient data, they are very irregular. They give you terrible performances. You need to feed it a lot of data before you dampen it down. They are very irregular. They give you terrible performances. You need to feed it a lot of data before you dampen it down. And I would like to illustrate this with a simulation. I'll see if I had a question. How do you know just by looking at data, whether it's a complex data and you have over you know had more bands or it's a simple data but with high bias see so i'll give you a purest answer and then a practical answer the purest answer is the ground truth, the very fact that you got data, you don't know the ground truth. So you are in a state of essentially, you can't make an informative prior or you can't induce a bias onto your model. Every model that you pick up certain complexity, it has an induced bias about what your guess is towards the complexity. You have no right to do that. You have to try everything out. That's a common way of saying it rather than using words like non-informative prior and induced bias. A simple thing you don't know so go try everything a more practical example is see whenever you enter a domain in that domain people have been working you you generally know something about the data set so for example when you deal with computer vision and you're trying to detect the objects in that image that the the optical eye is seeing. You know it's a complex task. Linear regression, et cetera, won't work, isn't it? So you have that what you know from the past, from your experience, helps you create essentially an informative prior. You start with a model that won't be starting with n is equal to one, let's say. I'll give you an example. Suppose you're trying to model, or let's say that you're trying to do a very complex modeling of a dynamic system, and it's oscillating, it's moving, it's responding to forces. You know for a fact that it's not going to be a straight line. You have seen the graphs or something like that. So you you you bring in experience when you bring in experience and you start you don't start from ground zero you start from a place that says you know i should at least start with a complexity this much a model this complex for example if it is polynomial regression let me at least start with n is equal to 5, let us say. So that is an informative prior. You say from there I'll scale up, because I'm pretty confident going below that won't help, because you see all those wiggles in the data. The other way to look at it is models always have an underlying, again, another way to say is an induced bias in the model. You always say that every model has some hypothesis like for example polynomials believe it's curved what if your data truly is like this you realize that here you can fit it with a polynomial but these edges that here you can fit it with a polynomial, but these edges, you'll have to get very high degree polynomial to be able to capture those, what looks like steps, staircase, isn't it? And so you say that there's a different algorithm, for example, decision trees and that work better for this. So when you get data, the model that you pick always induces a bias, right? Implicitly induces a bias about what you think the reality is. When you take a polynomial regression, you always believe belief that, well, it is best approximated by very rectangular decision boundaries, staircase-like decision boundaries, right? Or boxy regions, rectangular region like decision boundaries, or semi-rectangular, straight linear plots of land. So that's the induced bias. So the simple answer is you don't know, but usually the domain gives you some intuition. Does that, I don't know if... Yeah, yeah, understood. That is pretty much it. So, so guys, now comes a deep question. Generally, getting more data is a far easier said than done. Let us say that you're trying to predict. You're a young entrepreneur. You have temperature readings on the beach for maybe a month. Now, you can't wait to get 1,000 days of data simply because in 1 days, you know, three years, you're just sitting there on the beach not making money. Isn't it? So what do you do? Wouldn't it be lovely if you could build a model, a reasonably good model? And let us say that the model is not linear. So now we'll complicate a model and say that, let's say that your data, 30-day data, just shows that, and let me just make 30 points. It is saturated. So in your mind's eye, you can see that a curve like this perhaps fits to the temperature. Some part is linear, some part is bending, and again gets linear, whatever it is, temperature and ice cream. Because after a little while, as many people as have to come to the beach are there. Let's say this is your X, this is your Y. You don't have the luxury of gathering lots of data and then still being able to make an effective model. Looking at this, you know that a straight linear model would be wrong. You would be way, way too optimistic on really hot days. You'll end up buying a lot of ice cream that will go waste according to this model. And so how do you do that? And that is the wonderful, wonderful subject of regularization methods. The very fact that it exists is so absolutely marvelous. It is the regularization methods. So there are techniques that help you methods. So there are techniques that help you regularize a method, suppress the oscillations, even when you don't have enough data. Are we getting it? That seems almost magical, but we'll learn how to. It turns out to learn how to do it, the practical part is very easy. In scikit-learn, your library that you're using, every single method has scikit-learn, your library that you're using, every single method has literally a regularization method built in, regularizer, and you can give it the value l1, l2, elastic, elastic net, and I'll explain these things from a theory perspective. Practice is extremely easy, okay, but that it is worth, today we'll pay attention to the beautiful theory behind it. So, and of course we'll do a bit of lab on this. So look at this, I created a little visualization. By the way, this is all there on the course page, please download and run. And I can see that the library has advanced and some things that were beta are now becoming standard part of it. So I'll update the code. So look here. What this shows is how many points do you see here? Five points. If you make a very simple model, would you agree that a linear model would look like this are you guys seeing my screen it would look like this isn't it yes let's try a quadratic model ah it's slightly bent and maybe a little bit better because it's a little closer it is an improvement so what has happened bias errors are slightly decreased let's go to third degree polynomial. Oh, what about now? Now it is doing famously, isn't it? Can you add more data points and see if that's the right curve? No, no, so just wait a minute. But let's keep to only five data points. So at three, we seem to be doing pretty well, right? By now we are pleased as punch with ourselves and how clever we are. Now, let's go a little bit higher. Oh, four gets it even better. Now it literally goes through the data, isn't it? Now you may be tempted to say, let's go higher, five. But then five is still okay now see what happens after that what do you think happened at six would you believe this model is better look at this region i don't know where my mouse is do you see this region here you don't know whether this oscillation is real or not there is no reason to believe that this big oscillation is truth isn't it do you see that guys so keep this oscillation in your mind and see what happens suppose and then see what happens i go to seventh or eighth degree do you notice that at the end the oscillationation has become huge? All these data values are between, if you really go and look at the range of values, it used to be between 0 and 1 on this side and minus 0.5 on this, maybe minus 1 to 1. But by the time you go to an 8-degree polynomial, it is building a model whose value goes up to 15. Do you see that? This looks hugely suspicious, I hope. And remember, where are you seeing it? Towards the tail, the end of the data, isn't it? This is your wagging tail that I was talking about. What is this wagging tail phenomenon called? Do you remember what is this wagging tail phenomenon called? The oscillations in the periphery, oscillations towards the outer boundaries of the data, what are they called? A wrench phenomenon. Remember, we talked about it. What we are seeing is wrench phenomenon. R-U-N-G, wrench phenomenon. We talked about this in the labs. And it gets significantly worse. I go to 10th degree polynomial. And, oh, goodness, look at the oscillation. From 15, it has gone to more than 200. That is rather unbelievable, isn't it? Throughout the range of the data, it seems to be more or less small variations. And towards the end, there is a vast variation, right? And what happens if I go to 20th degree polynomial? Right? It goes to 1.4 into 10 to the so what is it 140 or 1400 million value here so obviously this entire region becomes hugely suspect because ranch phenomenon takes over now see what more data does to it let's take this 20 degree polynomial and add more data. I will just randomly add from five points to, well, I'm trying to go to 10. Let's see what happens when you go to 10 data points. Ah, what you notice is 10 data points, this thing goes through, but you still have the wrench phenomenon at the end, this huge oscill oscillation but it has decreased do you see that it's 400 000 now or 500 000. let's go to maybe 20 data points give or take ah what do you notice at 20 data points you do have a 20 degree polynomial, means it needs all of those wiggles. But do you notice that it seems far more manageable? The oscillations are peaking at about two and change, isn't it? Look at this value, two and change. A much better model, but you would still suspicious whether this is really true and whether all of this extra oscillation here really means something. And all of these bends, they really mean something. Let's go and go to 50 data points just for this final thing. What do you see now? What's happening to your model is getting simpler and simpler, isn't it? The oscillations are being suppressed. Do you notice that the wiggle in the middle is gone, but you still have wiggle towards the periphery and that comes from your range phenomenon. But look at the data ranges from minus 1.5 to about plus one in change. Let's go to lots of data. And what do you see there? Do you see that those wiggles are still there, but they are small perturbations, isn't it? They're small wiggles. And you do have, if you really look carefully, this, the tail is still wagging at the end. Do you see this towards a value, X value of six? The model still has a little bit of a wiggly tail, but it's far more tamed, it's far more regularized. That is it. But then, suppose I don't make it such a complex model. Let's make it a model of degree 10. What happens? it such a complex model let's make it a model of degree 10. what happens ah look at that what has what has happened your ranch phenomenon has gone away do you see this guys so a more complex model is not necessarily better you have to find the goldilocks region what is the best model you can build given the data right you should always build the best model you can build given the data? You should always build the best model you can. The simplest model that agrees with the data. You should always go with that. Given two models which both fit the data are simple in a complex, always prefer the simpler model. It's sort of like the Occam's razor principle. I go to a degree five and you notice that more data gives you pretty much the same model as you get with let's say 50 points of data. So by the time you come to some degree of simplicity, let's say 40 points of data, more or less the same curve. You could go down to maybe even 10. It's still more or less the same curve, isn't it? But for simpler models, you don't need so much data. So this is an example of regularization. You're regularizing through data. That's the first method you learned. Now, we're going to do some more clever ways of regularizing. Those are called lasso. So they're fancy names for them. Lasso. You know the loop that you throw to catch a horse or something like that? Lasso. And then ridge. And then there is the elastic net right right we will learn about these three methods uh but let's take a 10 minute break we have been talking for an hour let's take a 10 minute break any questions guys before we go Yeah, I have a question in this graph, not graph, the visualization that you showed, which is it then the degree should be five or 10 both seem to be fitting the bill or? It depends. See, for example, if the number of data points you have is about 10 data points, a 50 degree polynomial is more than enough to fit this data. Okay. Go with the simplest model, because with this less data, you always run the risk of overfitting. See? Already, just going to seven has screwed you. You see on the left-hand side, you can see the wrench phenomenon, and on the right-hand side, you can see the wrench phenomenon and on the right hand side also you see the wrench phenomenon kicking in. In simpler terms, I call it the bagging tails. So in the ice cream example, the boy selling the ice.D.: beach will will he be okay with getting two months of data and fitting the model or he has to wait for one year of data. So let's let's bring it to it if there is sufficient variation of temperature during the month 30 data points are more than enough for him to make a at least a good enough model to start with it start his business isn't it a month of data now another month of data gets your model better and better the model can keep increasing it's not like one model and i think i understand now yeah depending on how much data you have you keep selecting or improving the model that's right okay have you keep selecting or improving the model that's right okay all right guys 10 minutes break those of you who are here there is food in the in the break room and food and juices and fruits grapes and other stuff please help yourself to them I'll pause the recording for a while as as if so does deep neural networks also have this warfitting issue and the trench phenomena happening for them when they more data increases since they are able to increase or they are somehow able to mitigate this issue so answer the question do deep learning have overfitting issue horrendously horrendously right they have horrendous overfitting issues, you need to give them a they're very voracious about data. If you don't give them enough data, God help you. No, no idea what predictions that they'll make. Right. So, but then, because of these techniques called regularization, when you add them to deep learning, the situation improves a lot. So these techniques that we're going to learn regularization method, add them to deep learning the situation improves a lot so these techniques that we are going to learn regularization method they are a godsend even with complex models you are still able to make a reasonably good model even with complex hypothesis good model with limited data right but if you don't regularize those deep learning models and you give it limited data you'll have horrendous overfitting. That's one part of your question. Second part, will it exhibit the Runch phenomenon? So the answer to that is, see, technically, the mathematics of Runch phenomenon is applicable to polynomial regression. But if you don't use the word word ranch phenomenon, the thing is true. See what happens is the data is sparse at the periphery by definition towards the boundary. There's this vast region in which you have no data. And then little bit inside the boundaries of the data range of the data. You have some data. So any model has very little information what is happening near the boundaries. And so most models, especially complex models, will fail. Near the boundaries, it'll do poorly unless you regularize it, and so that brings us again to the topic of regularization. You'll see how regularization mitigates. those sort of problems. And that is true not just of deep learning, but all models. Does that answer you? Yes, I got it. Asymptotes basically. Okay, so I'll pause the recording and we'll take a little bit of a break. Before the break, we talked of two things. First is models that are overly complex, they tend to suffer from a problem of high variance errors. We took the specific example of polynomial regression. And what we noticed is that the measure of complexity is the degree of the polynomial. If you try to build a model with excessively high degree of polynomial, then you start seeing all sorts of pathologies. You start seeing oscillations in the data, oscillations in your modeling. The curve will oscillate a lot. It will overfit to the data. You'll start seeing wrench phenomena. You're wagging tails near the peripheries. So all sorts of bad things. And then the question comes, one way to fix it is just get more data. Enlighten your model with more data. It will be forced to fit to the data. And so it will still wiggle with too many, too high degree polynomial. It will still wiggle. But those wiggle will be more like small perturbations rather than wild oscillations. So more data is one way to regularize a complex model. However, more data is more easily said than done. In many many situations data is insufficient, many real situations. So the question arises is there some mathematical magic that could let you have the cake and eat it too? Have a model, start with a relatively complex model, but tame it down to a model of the appropriate complexity, where the residual complexity has become small perturbations. But it more or less is the right But it more or less is the right because you don't know the complexity of the ground truth you're trying to match the complexity of the ground truth to the model. One way is that you keep on dialing up the complexity up and down till you get the best model, which is the gold standard but it wastes a lot of time. So what you could do is you could start with a relatively more complex model. And if only there is a method to suppress the complexity, it's not completely, but enough that they don't matter so much. They are muted down, big oscillations are muted down to small perturbations. And then you say, ah, okay, now I have the cake and I've eaten it too. I took a 10th degree polynomial, but still, even though it was a sinusoidal curve, it still well sort of looks sinusoidal, isn't it? It has some small perturbations around the sinusoidal, and it still makes effective predictions. And that is the pursuit of that. The first method that was discovered in this category was discovered, I believe it was discovered by people who were studying, one of the earliest cases they were studying was geology. But let's go through the, I'm a little bit hazy about the full history of its discovery, but let's go and see what it means. It is called the ridge regression. So we synonyms are ridge, regression. But actually it's applicable to classification too, but I've rarely seen the word ridge classification. Rarely seen, rarely if ever, rage classification. This is more common. The more technical term that I will use for mysterious reason is I'll call it the L2. This is often called the L2 regularization. Now this funny calligraphic L with the two underneath it, subscript two, it is part of the lower, the jargon of our field of machine learning so today we'll learn about that in some way and so we'll learn about some beautiful geometry along the way and then there is another regularization we'll learn which is actually l1 and that is called lasso regularization. The lasso came much later, it is applicable again to regression and classification problems equally and these are L1 and L2 and a hybridization is called elastic net. L1 plus L2 is elastic net. So today we'll learn about three regularization techniques. There are many regularization techniques, but by far these are quite common, quite prevalent in the field. And as you make progress in this course, as we go to ML 200, 300, 400, there will be lots and lots of other regularization techniques that we'll encounter, especially in the context of deep neural networks when we get to those. And when we come to decision trees and forests, we'll learn all about pruning and many interesting regularization techniques. So what is rich recreation? But before I do that, I will state a fact. And this fact, I won't get into the proof of it because it gets a little bit mathsy, but take it as a fact. One of the signs that you're having overfitting, and Prabhat, it goes partly to your question, how do we know that we are overfitting? What happens is when you look at these coefficients, y is equal to beta naught plus beta 1 x1. Let me just take one dimension first, plus beta 2 x squared plus beta 3 x cubed beta 4 x4. Let's say that we go to beta 10 x10. And let's say that we go to beta 10 x 10 and let us say that the ground truth was much simpler let's say that the ground truth was was like this so you know that this is so you know that this model model will overfit with modest data with modest data, with modest data sizes, isn't it? Would we agree? Take 30, 40, one month worth of data and your entrepreneur would be really in trouble if they use a model like this. We saw that. Now what happens is there is an interesting observation. You fit the model to this high degree polynomial model to the data, and when the training stops, you observe the value of beta naught, beta 1. So what you will notice is beta 1, beta 2, beta 10. And you will be surprised. What is happening is that as you dial up the degree of the polynomial, these values are ginormous. It will be like minus 186, 233 plus 1.45 million minus something huge, another huge number. Another minus yet another huge number. So let's think what is happening. What is happening is each term is saying I'm really important. Remember that if the data is normalized, then your coefficients tell how important that term is, isn't it? And each term is saying, I am really very important. You need to consider me seriously. And its importance is inflated, except that all of them are in opposite directions. is inflated except that all of them are in opposite directions right so it is like you know you go into a meeting room and everybody has extreme and opposite opinions which they're expressing loudly right so there is no that you you won't get anything productive out of it the first thing you need to do is mute it down all factors cannot be so important and completely diametrically opposed to each other one is trying to increase the value one is trying to decrease the value are we seeing this guys right so this is a clear sign when you're with regularized when you normalize data when you see your coefficients blowed up regularized day when you normalize data when you see your coefficients blowed up it is a telltale sign that you have overfit in the case of polynomial regression so what it means now let's look at it geometrically what it means let me just take beta naught beta 1 because we can visualize in three dimensions. Let's look at this. See, this is beta naught, this is beta one. This is the plane made by beta naught, beta one, the ground. Let's just call it the ground. This is the hypothesis space or the parameter space parameter space i call it the hypothesis what does that mean every point here remember just a better recollection every point here represents a value of beta not beta one let's say the two dimensions right and like just for argument let's say that there's beta 3, beta 4, beta 2. They are all orthogonal to it, though on a page, I don't know how you will show other betas perpendicular to these betas. It's hard, right? Especially if you reserve this direction for the total error of the model, right? The sum squared error. So now what happens is that suppose the model the ground truth was simple your we always said that your error surface is a bowl that looks like this it achieves this minima here this is your beta start the optimal solution around it you have sorry you have the contour lines. So let me remove this circular region because that circle is not meaningful. So these are your contour lines, which are the shadows or the projections of these contour lines here, equi-arrow surfaces there. And at the center is this point, which is your beta star, optimal solution, optimal hypothesis for the data. So when your model fits to the data and the data truly is of this thing, it'll be like this. But observe something, the distance from the origin. This point is zero, zero, zero. So in the hypothesis space, just check how far you are from the origin. You will notice that for well-behaved models, let's say, you know, data is like like this and you have built a polynomial of degree three and you have fitted. You will notice that all you are five, all your betas are small. And by the way, this is a lab, go back and look at the coefficients. And you will notice that all the coefficients are small, with normalized data, they're all small. What happens is when these coefficients become big, what does it mean geometrically that these coefficients become big? Means that in some direction, positive or negative, let me just call it beta i, beta j. You have gotten, suppose I take beta 1 is 1 million. What it means is way, way far off, very far from the origin. Very, very far from the origin. There is a the origin there is a point it is claiming to be this to be the beta star very far from the origin oh sorry what did i do uh isn't it does it, guys? This will be like minus one, like plus one point, something 1.45 million. The coordinates would be that and maybe if this is maybe something else, big values, and this will be a huge distance. isn't it so you would say that this manifests itself as the fact that the euclidean distance which is the beta by itself beta star optimal beta squared square that is the i mean square root of it you can take the distance square is this isn't it beta beta dot beta that this thing is huge and so your solution is like this it is achieving well this is a hardly good example and it is achieving its solution way far from the origin so then comes a beautiful bit of theory it says okay one way to solve the problem is we are not going to run away from the origin we will declare in this space so suppose i just look at beta 1 beta 2 space right so remember let's look into beta 1 beta 2 just two coordinates for the timing of beta i beta j space there are lots of betas here so what happens is if you just take any two of these and you project this goal down you will notice that the optimal solution is somewhere here and the contour lines are like this. Are we together? They are like this. And they keep on going like this. And they keep on increasing. These are contour lines. Do we understand that these are, the further you go, the more you come to a region and are lifted above it, right, above this, look at this, above this here, which are all high error regions isn't it there's a high error regions the further you go from this point each circle is like circle a circle b circle c the error of circle c is greater than error at circle b is greater than error at circle A is greater than error at B star, the optimal solution. This is obvious, I hope, from the picture, isn't it? So now projected down to two dimension, this is how the same thing looks. I just made a two dimension. Just pick any two beta i beta j and you get something like this. And so the hypothesis is that what if we did this we say we're not going to go so far because we know if we go hunting for the solution that far we get into trouble so we will take a solution that is i make a circle circle of distance, some distance, let me just say some distance, right? D, some distance D. And you can say that I will look for solutions only here in this region. Right? So then the question comes, what is the best solution I can build? I will look for solutions only here in this region. So then the question comes, what is the best solution I can build? See, this is too far. It is outside your region. Let's look at curve. Let's look at this. Curve A, curve B, and I'll also draw a curve C here in between. That is just touching, just glancing, a curve C. Now C and curve D. So D not allowed, D, E, F, G. So D, E, F, G. All of these not allowed. You see, you can't go. And C, B, A allowed. Now, wonder over this. Which of these has the least error in the allowed side, which of them will give you the least error, A, B or C? I think it would be C. It would be C because the more outward circle you go to, the bigger the error. So the best solution will be point at which your, this green thing is called the circle, this disc, is just glancing against the contour of the air surface. Isn't it? Air surface contour. They're just glancingly touching. It's like, how glancingly touching? Look at this and this. They're glancingly touching. Now, what do you observe where they glancingly touch? Where they glancingly touch, we are now going to look at this region more carefully, this region. Let's blow it up and see what it looks like, what this looks like. So, this will look like, you see that you have this, a part of a circle coming through, and then you have a part of the contour line, right? The C coming through, right? So you would agree that their tangent planes are the same. They have the same tangent plane, right? They're glancingly touching each other. See, guys? The tangents have to be the same, isn't it? And what does that mean? It also means that the perpendicular orthonormal vectors of the green, orthonormal vector of the green is in this direction. The orthonormal vector of the contour line is in this direction, isn't it? The contour orthonormal vector, perpendicular vector, perpendicular to that thing is in this direction. Would you agree? Remember, we always look at the unit vectors that are perpendicular to the surface to give a sense of where the surface is heading so this guys a ponder over it and tell me that this is obvious isn't it is that obvious guys yeah i think it's obvious it is as if how we decided the origin of the green circle oh that is at zero zero zero zero by definition from the origin you know see the coordinate system needs uh the convention in math is you call that the origin you give it the value zero zero okay okay so here we are now comes So here we are. Now comes an argument that, okay, you observe also that at any other point, like for example, at this point, and at any other value, you notice that those vectors are potentially pointing in different directions. They may or may not be pointing in the same, but quite likely they're pointing in different directions. But this point is unique. The two orthonormal vectors, they will point opposite to each other, right? Now it turns out that these directions are not actually orthonormal. Orthonormal means unit, but these orthogonal vectors. There is a special thing called, and we talked about it. Remember, we talked about the gradient descent, isn't it? And I told you that the gradient of a function, gradient of a function always points, points perpendicular to its constant value surface, to its contour surface. In other words, what is a contour surface? Equal value or constant value. So if you say fx is equal to some constant value, c1, then the gradient of f at the point x will point perpendicular to the curve, this curve, perpendicular to the curve given by this equation. Now, this is a recollection. We talked about it. That gradient points perpendicular, it is the direction of maximum increase isn't it remember steepest increase steepest descent that's why we do gradient descent to go home quicker so just recapping so what it means is that this thing this vector is the gradient of the contour the actual error surface at this value, right? And this green vector is the gradient of the, sorry, and this green vector is the gradient of this circle, circle function, gradient of the, let me call this a function g, g being, what is g? G is, let's say here, the coordinates are beta 1 square plus beta 2 square. Remember, betas are the variables. If you think about it as x square plus y square is the equation of a circle, it's some constant value here. In this case, it's d square, right? If you say distance d, would you agree? So in this case, we have beta 1, beta 2 as the xy. So this is d square. So let me remove this. Would you agree the equation of a circle is this, right? Let me remove that. So in your imagination, you can think of it as x and y, and this becomes your high school coordinate geometry. It's x square plus y square is d square is the equation of a sector so this is what it does right and so this thing when you look at this you come to a pretty profound observation which was first so this way of arguing by the way was done outside machine learning in the more general case by a great, great mathematician named Lagrange. And that was the age in which in Europe, there was a practical explosion of mathematical values, a lot of what we call calculus, functional analysis, analysis and so forth. In a very compacted period of about 100 or so years, all of this mathematics came about. It's just an explosion of mathematical, the field of mathematics. And so Lagrange was the person who observed this fact that if you take any two surfaces, this will be true. Now, if the disc here it is the constraint surface isn't it disc here the disc that's right the the green disc this ah sorry the why am i still getting red okay yeah this disc Okay. Yeah. This disk is the constraint, the word is, and we're introducing a word, constraint surface. You're saying you must find the solution in the disk, So I often give this example which is a bit cheesy. So in the story of Ramayan, there is an episode that the king Ram and Lakshman, his brother and Ram's wife, see that of course the three gods, they are in exile. They are the good guys and they are in exile in the forest. The forest is fraught with danger but Ram and Lakshman, the two, have to go out and gather, look for food, hunt and look for food. So Sita is alone and the wife is alone in the home. So apparently Lakshman draws near the hut of Sita, he draws something famously called Lakshman Rekha. He draws a circle, a magical enchanted circle around the house and says within the circle, that is this disc area, so suppose your house is the origin of that Kutti, then this is the enchanted land. So long as you don't step outside the enchanted land, you're absolutely safe. Right. And well, of course, the story goes that the devil, well, in this case, Ravan, comes, tempts her, begs for food and tempts her to just step outside the magical land and all trouble breaks loose. So there's an analogy here. You step out of your disk of constraint, then your model will start overfitting. All hell breaks loose, sort of. Right. So that's the way of giving an analogy to this. So you look at this point, but you notice that you can have a constraint surface and you can write an equation. Looking at this, you would say that grad of e is the negative up to a negative it is some multiple of the grad of g this vector these two vectors are related by the fact that one is pointing in the opposite direction to the other but you don't know how their sizes are related so you can put a lambda here some proportionality constant would Would you agree with this? This is obvious from this picture. What is geometrically obvious? We are writing it as an equation. Yeah. And so this is the famous equation of Lagrange, what we just did when we wrote it like this. So we can rewrite it as this. Grad G is equal to zero. Lambda has a famous name. It's called Lagrange multiplier. Multiplier, right? It is the Lagrange multiplier. And of course, because this is a vector, this is a vector. So what happens is that there are various components to it, like the lambda 1, lambda 2, lambda 3, et cetera, will come in. But this is the basic idea that youed the theory of constraint optimization. In other words, if you have a constraint function that must be respected and something you're trying to maximize, now what you're saying is that don't find the best point for lowest point of E because that will get you into trouble. So this equation we can write now as gradient of E plus lambda G is equal to zero, right? So what does it mean? You're finding the minima of this function, isn't it? Gradient vanishes at the minimum. You're trying to find the gradient. And this is the word, this is the appropriate law. The word that you use is loss function. And for the first time, we are encountering a loss function that is not just the error function. So the loss function is e plus lambda g, right? Now, this is this. Now, let us think about it. What is it? What is the loss function? Typically written by this is this. Now, let us think about it. What is it? What is the loss function typically written by this very scripted calligraphic L? It's a convention in the field to do that. And of course, loss is a function of the parameter space. Of course, it's a function of beta. Now, what was E? Let's say that it's some squared error, right? This is your y minus y hat. And if it is mean squared error, then you can divide 1 over n. That is okay. Pick some squared error, mean squared error, whatever you want. So this is your yi minus yi hat squared summation over i. I'll just, if you want to divide by n, it's your choice. But it makes no difference. Now, lambda. Now, what was g? g was beta 1 plus beta 2 squared. But then let's generalize it to higher dimensions. It will be plus beta n square for a n degree polynomial would you agree and this factor you can write it as beta the beta vector right dot beta or you can write it as beta square both of these are correct these are different ways of writing it and so in your literature you'll often find this written as uh the various literatures you'll say y i minus y i hat square plus lambda beta so people just say summation of beta i i mean we can write it and this is also let me use because i we use for this let's just use j Raja Ayyanar?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na?nilavah?na? beta j squared j, or rather another way is the first term will remain the same and it will say lambda beta dot beta, or it will say the same thing plus lambda beta square. But the more important thing is it's a quadratic function. A circle is a quadratic function, right? Quadratic in beta. Right? And specifically a circle. A constrained surface circle. So now comes a very interesting thought. This lambda, how much is the lambda? It turns out that this is the regularization, strength of the regularization regular right position and you will say now how is it are you saying that the bigger the i make the lambda yeah so the intuition is this first let's give you the intuition see guys what happens if you take a very relaxed circle like if you're if you're allowed constraint surface is this big then you you will end up finding a solution very far from the origin so have you reduced the overfitting much have you regularized much no right no you haven't but if you make the circle too small then you increase the error that is right and so once again there is a trade-off lambda is the this lambda this is lambda therefore is a hyperparameter. Too big lambda, too big lambda will lead to a very tight circle, a small circle will too much and lead to more errors. The same thing. Too relaxed or small. Lambda, what will it do? It will make the circle go up and then again high variance errors, overfitting errors. So what happens is somewhere in between is the Goldilocks region for lambdas. There is a best place to how big the disc you want to make or what's the radius of your, to use our example story there, to the radius of the Lakshman Rekha around the hut. So sort of way of thinking. So you wrote the lambda. See what happens is, given the sum squared error when you are minimizing it, if lambda is big, to keep this term small, you'll be forced to keep the betas very small. Otherwise, you'll be in trouble. So the bigger the lambda, the smaller the betas have to be to compensate. Means the closer, the tighter you are willing to accept the solution to the origin, the smaller your disk. The smaller the lambda, the bigger you can make the betas in size. the bigger you can make the betas in size right because ultimately it's this term total that adds to the loss function and so you have some leeway on how relaxed it can be right and this is it we just discovered that this particular form of regularization is called rich regularization or L2 regularization. Now where is this word L2 coming from? So this brings us to, so now I'll tell you, I'll generalize this. By the way, does it work? It works very well. Indeed, L2 regularization should be a basic Indeed, L2 regularization should be a basic go-to regularization in most situations. You should always sprinkle the L2 regularization salt over whatever classification, regression, whatever you're doing. And scikit makes it very easy. We'll see it in the lab. Literally, dead simple. Reg is equal to L2. One more parameter. I believe it is right. Now, here comes an interesting thought. And this is one of those lovely things that mathematicians do. They say, what is a circle? So now, let's ask this thought. And this thought was asked by a mathematician called Minkowski. He asked, what is a circle? Well, to most of us, a circle is obvious, this nice round thing, isn't it? But he asked this question and asked, could we generalize the notion of a circle so he said circles are circles a circle is a set of is and tell me whether this is obvious or not is the set of all points equidistant all points equidistant distant from the center. And for simplicity's sake, let's take the origin. Would you agree with this? You take this and this point, this point is the same distance, this point, this point, they're all same distance and then likewise you keep on going. All of these points that are at the same distance, the set of all points, all points equidistant from the center. Would you agree that that's the definition of a circle yeah in a given plane yeah that is right so this point for example how would you go to you would go like this right as the bird flies right so now we come to then we will explore equidistant the trick is what is distance? So your basic intuition of distance is as the bird flies, isn't it? From the origin, if you were to take a direct flight to that point, that is your notion of distance. So that is called the euclidean distance because that is how we think of distance right put a put a thread here and then stretch the thread over to this point and see how long the thread is euclidean distance is let us say the gap x y suppose the coordinates are x y x square plus y square the square root to the power half isn't it the square root of this would you this is how we define in basic school the notion of distance. But you know what? All the Lyft and Uber drivers will disagree with you. Because when they go, and let's say that your city is laid on a grid. And many cities in the US are laid on a grid. Then suppose you're here. And you want to go here. Can you go this path? Can you take this dotted path? And these lines represent roads, lines are streets. Can you take a path from A to B like this? If you do that, you would be literally running your way through buildings, isn't it? So what will an Uber driver, taxi driver, a taxi driver will be forced to do? This is one path that is allowed. This is another path that is allowed. This is another part that is allowed. This is another part that is allowed. And you can see there are many parts. You could do like this. Or you could do, I can make some more parts. Let's say you could do. You can find many parts, but all of these parts, they have a quality to it. They will have one unit, two unit, three unit, four unit, five unit. So let's say that each of these distances are equally spaced or the grid is equally spaced. distances are equally spaced or the grid is equally spaced would you agree that the distance is actually five units let's say that this distance the x distances and the y distance this is the y distance right so what you're doing is the size of x which is three units three the size of y here in this picture is 2. And the distance, because ultimately, that is the amount of gasoline or petrol that the driver has to put in the car to take you from A to B, isn't it? So the taxi driver will assure you that the point guys do the taxi driver who cannot go fly through the buildings magically but has to be constrained to the streets the distances first the horizontal distance the distance north south and the distance east west are we together guys tell me that this is obvious or not if it is not i'll repeat yeah but it's the same distance whichever path you take exactly so you that is the interesting thing all paths the all shortest paths All paths, all shortest paths. So, of course, you could take a bizarre route. Like, for example, you could go like this. Right? We are not counting. That is not the shortest path. All shortest paths have distance. So distance is always the distance, implicitly the distance of the shortest path. Distance between two points is always defined as the shortest distance. You don't keep on saying the shortest distance. You just say it implicitly. It is this, right? This is often called the Manhattan distance. Can you guess why? Since Manhattan is in the form of grid. The city of Manhattan. Manhattan. I'm doing the spelling here. Let's say Manhattan street, you find the north-south distance and you find the east-west distance and you add the two up, Manhattan distance. So you say, huh, this is another notion of distance. And here is another notion of distance. And Minkowski asked, what is common here? Can we write a formula such that both of these are special cases of that formula? And that was a brilliant thought to have. And he sat down and wrote that equation, which now will make sense. distance d of the order n between two points x y is the size of x plus this the size of x to the power n plus the size of y to the power n the whole thing to the power 1 over n now let's see if it works what is d1 d1 will be x to the power 1 plus x plus y power 1 to the power 1 this is nothing but in fact in other words it, it is the Manhattan distance. Would you agree? d1, right? For n is equal to 1. It's literally this. this d is equal to 2 x y is equal to x square plus y square to the power half now you realize that the size of x square is the same as x square plus y square to the power to the square root and what is this yeah distance and now the whole thing begins to sing now because Minkowski has just generalized the notion of distances with this notion so we call this the Minkowski distance and now because we have a formula in our imagination we say well we can think of n is equal to three n is equal to four n is equal to a half, right? We can do all sorts of things. So you start asking, what do these things look like, right? So let us take n, a very high value. Let's say 10. And let us say the x distance, not south distance, was, let us say, 3. And the the y was so let's i mean i'll just exaggerate it let's say that this was seven and this was two right so what happens when you do this you will do seven to the power 10 plus two to the power 10, the 1 10th power, isn't it? D10. Would you agree? Right? Now, 7 to the power 10 is hugely more than 2 to the power 10, isn't it? So another way to say it is that this will be the same as 7, 1 plus 2 over 7 to the power 10, 1 10th. Would you agree? Now 2 7th is a fraction. It's a number smaller than 1. What is 2 7th? 1 over 3.5 is about point, let's say 0.3. Let me just guess. 0.3 is a reasonable guess, 0.3 something, 0.3 and change. So 0.3 squared, it will be 0.09. Do you notice that the bigger the power you take to the 10 will be approximately equal to 0, right? And so your d10 will be approximately equal to 7. Now look at this thing nothing is special about 7 so what happens is that the high value d so at as d infinity is just the max of x and y the max of x and y whichever is bigger will dominate isn't it so you say that the infinity norm and the word n that i deliberately use is called the norm right minkowski this question with sorry go ahead go ahead go ahead the equation that you uh did when you take seven out how did you get one there it should be seven to the power nine seven to the power ten to the power both of them by seven to the power ten and then multiplied by 7 to the power 10, and then multiplied by 7 to the power 10. Okay. And then the 10th root will make it only 7. Okay, got it. That is a point, exactly. So we learn a lesson that the infinite norm, infinity norm of the L infinity. So this is often called the L infinity. Now, it has a value, it picks the max magnitude. So means if you were doing in terms of parameters, what it will do is it will pick only one parameter, whichever is the biggest. Represents a pretty extreme case, isn't it? Now, how does it look geometrically? I'll tell you how it looks. You say, well, a circle we all know how it looks. The Euclidean distance, the Euclidean circle, the n is equal to 2 looks like this, isn't it? Our honest to God circle. But I'll let you convince yourself that if you are here, the Manhattan circle, the so-called circle, remember, what is circle? The set of all points that are the same distance from the origin. So if you use Manhattan distance, convince yourself that this is it because you can go either here or you could go some distance here and go up or you could go some distance and go up and you could go some distance and go up and so forth. And you would agree that all of these distances add up to the same Manhattan distance as a taxi driver, isn't it? So a L1 circle, so L1 circle that is n is equal to 1 is diamond shape. Isn't it lovely idea that, well, the shape of a circle is in the eyes of the beholder. Depending upon what norm you bring, it will look different. But it is still a circle. It's a diamond shape, isn't it? Now what happens is, if you take a norm, let me tell you, if you notice that the green is inside the circle. So from that, we develop the intuition, probably, that if we go to a half, it will become like this. And you can keep decreasing it, 0.25 and so forth and it will become even more like a pointy sword right it can become like this this could be n is equal to 0.25 and it keeps up by now you get the idea right what does it become it gets more and more pinched towards the origin isn't it it becomes and it becomes develops pointy, rather pointy tips. Isn't it? Guys, is that obvious? But a circle is nice round. Now, what about n is greater than 2? Let me take a color that I haven't taken, maybe this color. if you take three or four or something like that this will begin to look like this so what is it is bulging out and is equal to three let's say right I've exaggerated it, draw it out. It will be something like this. Go ahead. What else? There will be like inside the thing? No, no, no, no. Do it. Do it and see. The shape begins to look strange. It is beyond the circles. It's outside the circles, kind of weird shaped. So, well, try this out, guys. By the way, why don't we do this? Let's do a lab. Let's draw out all this curve. Let's draw out these curves. Let's take it for n is equal to 0.25, 0.5, and we'll learn. In case I've made a mistake, we'll learn. We'll all learn something. All right? So now, what does it have to do with this lovely digression and this thing? So Minkowski norm is all about machine learning, guys. So we generalize for the first time, we generalize the notion of distance. We realize that Euclidean distance can be generalized to Manhattan distance. And so it turns out that in machine learning, there are many, many creatures, fascinating exotic creatures that are all measures of distances. In fact, Euclidean distances just fails in high dimension. It is called the curse of dimensionality where something very mysterious happens. Everything is very, very far from everything else. There is just way too much space in higher dimensions and data gets lost. And we are only now, in recent years, beginning to understand the geometry of higher dimensional data, higher dimensional spaces and learning how to sensibly do machine learning with very high dimensional spaces. But as we build models with high number of parameters, we are necessarily dealing with very high dimensional spaces. And so to know that Euclidean distance would be useless. Most points are very far from each other and it's hard to find neighbors begs the question. So what notion of distance can we take? And there are other notions of distances. For example, in higher dimension space, two points, and this is the origin, let us say, what you could do is you could just look at the angle between them. So you could look at cosine theta. And you can say, if they are close to each other, then the distances between them would be small. Because anyway, far off points are all sitting far, more or less about the same distance from each other for reasons that I'll explain later there seem to be so what you look at is the cosine distance so this is called the cosine distance right it's one of the things that comes as a surprise that Euclidean distances actually fail in high dimensional spaces all of Minkowski distance begins to fail. In fact, you'll have to use cosine distance or other notions of distances to get around it. But that's a longer journey. And as we go through this machine learning course, ML 200, 300, 400, you will encounter some pretty exotic beasts that are all distance measures. So far so good guys. Now what does this long digression have to do with our regularization? You may ask. You say well you know I took the disc here. Do you see the obvious thing? I could have replaced the Euclidean disc with the Manhattan circle, right? Can I? Is it legitimate? What if my constrained surface is actually a Manhattan circle, right? If it is a Manhattan circle, let's redraw the picture. Let me say, Let's consider a Manhattan circle. Remember, it is diamond shaped. And so you end up with the beta space, let's say beta i, beta j space. You end up with a constrained surface that looks like this. Isn't it, guys? So now let's go back and look at your contour lines. You notice something very interesting. When you draw the contour lines, it looks as though there is a high probability that this tippy tip, this pointed tip of that disc will penetrate the contour surface first, isn't it? It doesn't always have to be true. It could have been that it hit it here. Let's say that if it was like this, then it could have hit here. But with this thing far off, there is a pretty high probability that this pointy tip, the pointy tip seems to puncture. If you think of it as a balloon, the error contour balloon. That's a picture I hold in mind that this nice pointed tip is going and poking into the balloon. And it's likely that the pointed tip will hit the balloon. Am I making sense guys? Just look at this picture. There's a fairly high probability, not always but fairly high probability to do. But when it does that, what does it mean? What does it mean that it punctures the optimal point is along one of the axis, isn't it? It is along the beta i axis. Now, what is true along beta i axis? What is the value of beta j? Zero. And so what does it say about a model? So your model was y is equal to beta naught plus something something beta i xi plus beta j xj. What does it tell about this feature? Let us say that this feature was height of the waves. Height of waves in the beach. What is it telling you about that? If beta j is zero, means from the model, you can just throw away that feature. Am I making sense? You can throw the feature away from the model. It doesn't matter. It's what the machine is telling you after learning is that beta j is zero. It doesn't matter. That feature doesn't matter. And so from the feature set, you ended up throwing away some of the features. Do you see that guys yeah but the same could have been true if it was a euclidean circle you just take the axis yes so now let's think the same thing i will draw it out along the euclidean circle so it could be think about it it could be by coincidence here right that it would hit at this point but it a little bit of thing will convince you that it is somewhat less likely right just convince yourself by playing around with it that what will happen is it will probably tangentiate here close to it it beta j so what happens with l1 norm is with l2 norm with l2 norm which regularization beta j will become small small values but with l1 norm beta j will actually be zero right so literally it will be zero and so what it does the way we say it is that l1 norm has one additional benefit. It leads to dimensionality reduction. Means, suppose my feature set, I had five features. So what was my feature space dimensionality? Five dimensional, isn't it? But let us say that I punctured along some axis. And let's say because I punctured along that axis, two of the coordinates disappeared, Let's say because I punctured along that axis, two of the coordinates disappeared, right? Two of the meters were zero. So what did I do? I reduced the dimensionality from five to three. Now only three of the features matter. The model is saying only three of the features matter, right? And so that is dimensionality reduction. We just got introduced to a topic, which is for next week, but we learned one method of dimensionality reduction, which is actually a good thing, dimensionality reduction. So you say, well, L1 norm or this LASSO, LASSO is of relatively recent discovery. Ridge has been there for much longer. LASSO is more recent. In fact the authors of your book, textbook, Hasty and Tipshirani, they did quite a bit of pioneering work with the LASSO regression or LASSO regularization. There have been significant researches in this space. Now, and dimensionality reduction is one of its core benefits. But the problem, and now we are running out of time, so I will be a little bit brief. And I'll tell you something that take it as a fact. It sort of is too aggressive and it is too pointy it takes away too many of the dimensions what happens is that as you increase lambda as you increase lambda so and these are the values of your different betas let me say different betas, beta values. So let's say beta 1 is here, starts out here, beta 1, beta 2, beta, and this is beta 2, this is beta 3. And then what happens is, as you increase for different values of the lambda, this regularizationization parameter you will find that gradually these things disappear so what does it mean suppose they both disappear here if you set your lambda to be anywhere in this region here at this point only beta 1 disappears beta 1 is equal to 0 here at this point only beta 1 disappears beta 1 is equal to 0. at this point beta 1 beta 3 both disappear and at this point even beta 2 disappears and beta 1 beta 2 beta 3 also are equal to 0. so this point do you think a model is useful if all the parameters are 0 no right you have you have gotten down to the null hypothesis isn't it because your model just says that y is equal to beta naught right that is essentially your null hypothesis so what you're doing is as you're increasing lambda, you are gradually making the model more and more rigid by throwing away some of the less important features. But what you call less important is a matter of degrees. Soon you'll end up with no feature at all. So now how do you find the best, like, how do you know, for example, that the best solution is this, throw away beta 1, beta 3, but keep beta 2. That you have to do by, this is the, so therefore lambda, again, is a hyperparameter of the model. Unfortunately, there is no golden recipe. You'll have to play around with different values of beta and in the lab we will see how do you do the search they're lovely methods grid search randomized search then and now we have we are in the world of bias and inference which is a very powerful new method that has come up more recent prevalent method has been there for a long time but it has become more commonly prevalent recently so guys this is it we learned about two forms of regularization now what happens is l2 will never set any of the features to zero it will just irrelevant or less important features will just have smaller coefficients and so you can just visually inspect and set them to zero if you want right and see what happens lasso will explicitly set to zero now what happens is both of these represent extremes lasso is too extreme in chopping off features aggressively in chopping off features aggressively. The rich is too generous. It almost never sets anything to zero. Very rarely will you hit along the x-axis, like Raj was saying, Raj's example. So somewhere in there is a happy medium, is a Goldilocks. And that Goldilocks is elastic net. Elastic net. So hang on. So what would be the equation of the loss function loss function would be y i minus y i hat squared the standard error term plus lambda beta j absolute value isn't it the absolute value of beta j's this is the loss function lasso because you're looking at the diamond shape remember your circle is diamond lasso regularization and what i just did for regression is also true for for classification also same is true same is true for classification is true for classification so what it means is that suppose I do, remember the loss function there was y, y log y hat for cross entropy, y i minus minus yj, 1 minus yj. For the negative case, log 1 minus yj. Remember, we wrote it like this, right? So this is your error term, e term that we are used to, error term. All you have to do is plus the same thing, the lambda and your sigma over i. Let me write it k let me say k here beta k absolute value to the power n where n is the norm right l1 or l2 norm the same thing is remains true for classification also you can regularize classifiers also the same way so now there is a method that people use if you really matters to you use elastic net it makes your training harder because you have to use l1 and l2 together your loss function will have let me just write it as the error term plus lambda 1 beta j absolute value over j square plus lambda 2 k beta k square right so you notice that you have both the Lasser term and the ridge term. This is the L1, this is the L2 term, both of these. Now what happens is if you minimize this loss function, you'll be forced to learn both two hyperparameters. And so it makes the model more complex. But what happens in reality is that it works better simply because it's sort of the Goldilocks region between lambda one, lambda two. It figures out which works more, which works better. It sort of weighs it towards whichever works better for this dataset. Are we together? And that is the world of regularization. So let us summarize before we end today, the last half an hour. What did we learn? We went through a quite interesting journey. Today, we said that, where did we start? Okay, we said that the simplest way to regularize is just to get more data. Lovely, but that is very easy to say, very hard to do it in practice, because data is precious, and often very, very hard to get. So you need a method, some mathematical magic that will let you have the cake and eat it too. That will help you regularize even when the data sizes are modest, right? Help you. And one way that you do that is by noticing that for, when you do have overfitting, your, for example, in the case of polynomial regression, your coefficients just explode. They just go out of hand, become very large and run away from the origin. And the bowl practically runs away from the origin, error bowl, error surface. So one thing you could do is say, no, I'm not willing to go further from the origin. This is as far as I go. You decide what's your constraint, further from the origin. This is as far as I go. You decide what's your constraint, desk, how far you're willing to go from the origin. When you do that, you say, then you notice that there's a lovely bit of geometry here. At the point that they glancingly touch, obviously the two orthogonal vectors, perpendicular vectors will be pointing in opposite directions. So you can write it, error surfaces, orthogonal vector is grad E, and the constrained surface G's orthogonal vector is grad G. We learned that. It's the direction of steepest increase of both of them. So they are in opposite directions up to a proportionality constant. And thus, we discover Lagrange, the theory of Lagrange, like constraint optimization. Lambda is the famous Lagrange multiplier. Right. And so we work it out for the standard disc. And then we realize that use the Euclidean notion of distance. We can generalize beyond the Euclidean distance. So now what happened to this? It will take a little while to show up. Yes. We went on a digression, called them, and learned about Minkowski distance, a Minkowski norm. And then we realized that a taxi driver will vehemently disagree with the notion of Euclidean distance because he doesn't follow, she doesn't follow the Euclidean distance. She has to go by the street, by the city grid, and the amount of petrol or the gasoline you consume is a testament to the fact that the distances are not Euclidean, right? So the distances are Manhattan distance. The generalization of that into a single formula is the Minkowski distance. The mathematician Minkowski wrote this expression. And then Euclidean and Manhattan become special cases of it. But that opens the door to more and more norms. In your book there is a picture, a lovely picture at the bottom you'll see, of the different norms and Wikipedia also has that. Go try it out. What does a circle in different norms look like? We also notice that the d-infinity norm always ends up picking the maximal value, whichever is the bigger. Now whichever is the bigger. Now, the L2 norm is nice and round. So most of the time, the contoured surface will hit it at an arbitrary point. But for L1 norm, there's a greater probability that it will puncture along the axis. It becomes even more likely that it will punch along the axis for less than for fractional norms, half, one fourth and so forth. So which norm you take becomes a hyperparameter of the model also. And it's a very interesting thing that you can sometimes improve the performance of a model simply by knowing that you can play with the norm and taking different norms and doing a bit of practice with it. In practice, the built-in libraries actually give you L1, L2 and elastic net out of the box. If you want to do anything fancy, then you have to do it. It's a custom norm. So this is it. And it does work, guys. Regularization is practically a de-rigger, especially in this world of very complex models. We build complex models. We never have enough data. So we always use regularization to pretty much discipline it, tame it down, mute down the wild variances, high variances, wild oscillations. And that is it. Now I will take questions, guys. That is it today. By the way, is this whole thing clear to you guys? How many of you felt we were totally lost today? No, okay, so everybody is quiet. So is there anyone who felt you understood what we talked today? The opposite question. Yes. It was straightforward, isn't it? Yeah. These are new ideas, guys, isn't it? Yeah. These are new ideas, guys. And the notion of distance, so what happens today is I took you beyond the notion of Euclidean distance to other distances. Machine learning is the whole notion of distance of metrics is rich and beautiful. And many, many distances notions are there. I invite you to go explore the notions of distance and the converse of its similarity. Often the secret sauce of companies is their notions of similarity and distance for certain functionalities. So go learn about it. Learn about Lagrange multiplier. Learn about this argument. And some of this are perhaps not in your textual. So read a little bit more widely outside and see where you can find references to study it. Perhaps, Albert, you could look around and post some references that cover the sort of territory I covered. Andrew, sorry, not Elbert, Andrew. I think this is a pretty good overview. Okay, yes. All right, guys, so any questions?