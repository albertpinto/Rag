 We move now to the new topic of momentum in gradient descent. So let us recap the equation of gradient descent. We said that the new value of the parameter, and we have used multiple conventions. We have used the convention beta, beta tilde I used to say for next, next value of the parameter, parameters, beta being a vector, is equal to beta, current value of beta, minus alpha times gradient of the loss with respect to beta, right, implicit that is with respect to beta. This is the original equation that we did in ML100 we are familiar with. Excuse me. When we deal with deep learning literature, you find that there is a, people use a whole variety of convention. So most of the time we are talking of a weight, right? So you can say that in our notation, I've been using this notation that if you look at the weight vector, its next value I'm using today for next is equal to the value of the weight vector minus alpha times the gradient, obviously, with respect to weight of the loss. Right. the loss right written out in specific terms i j let's look at the specific address of that of the layer l so suppose i have in the layer l l layer just the the ith node and then in the L minus 1 layer there is a jth node jth node so the weight associated with this edge is W I J L this is equal to a w i j l minus alpha and then the gradient when you when you do it it becomes a partial derivative of course because you are being very specific w i j l right this is laying it out in specific pieces so far so good guys okay so this is laying it out in a specific pieces so far so good guys okay so this is the convention that i've been following everywhere uh in deep learning course fundamentals course i've replaced beta with uh beta with this and here also beta or any particular beta i is equal to tilde is a beta i minus alpha dL d beta i in specific component wise right that is what it is so far so good guys this is i hope a recap furthermore now for the purpose of to align with the convention given in this book so i'll say to align with the the mentioned given in this book. So I'll say to align with the which we are using for review. Review. What are we doing? We are replacing W with theta. We are saying that this W vector, it goes to theta as a vector assumed it's a vector and my tilde goes to t plus one you're seeing that in the t plus one iteration right the theta t theta now does he use subscript or superscript Iigate? Maybe T plus one is equal to, this is essentially in my language, W tilde, right? Is the previous value of theta in the previous step minus eta, right? So this in my language would be, this part would be W. And this would be, this part would be W. And this would be, what is this eta and gradient of, gradient, so let me just write it. This book writes it as, gradient writes it as a G vector, G, and for some reason makes it into a superscript. So the convention is slightly uh inconsistent but for whatever it is this is what the book follows so now what is the eta in our language guys what is the eta in our language what we what what are we calling it alpha learning rate yeah this used to be the learning rate alpha and what is this guy this beast this. And what is this guy, this beast, this gt? What is this? This is basically our gradient with respect to theta t of the loss, isn't it? Like in our language, this is what it is, the gradient of this part. If you replace your W with theta. So, so far, are we together, guys? Is this anything looking mysterious? It's a little bit of jargon, you know. Nothing has changed. The intuition is the same. It's just that different people will use different notations. Why am I confusing you with new notation? I don't want to, but I'm just making you familiar with the notations of a book that we'll use for review. So far so good? Are we together guys? So we will start with this as a gradient descent equation. So here the t refers to the iteration. tth iteration. It's sort of like time. Yeah, t. And that's how people think. That's a very good word you use think of it as time time moving in discrete jumps first step second step third step fourth step of gradient descent what is each step one mini batch right when when the data one update to the to the parameter value is one step right so the tth t-th hydration, and think of t as time, if that helps you, right? So the gradient descent equation is, now guys, if all of this is getting too theoretical, I promise you, this is our last pure theory thing. After that, we'll use all of this knowledge for practical architectures from next week. So according to this, it is theta t plus one is equal to, I'll just repeat it here, theta t minus eta gt, right? Obviously, this vector is assumed and I will just deliberately write it in our notation just for whatever it's worth. This is equal to the previous iteration of this minus alpha gradient of the loss, right? These two are equivalent. This is the way to think about it. Equivalent. But different symbolic conventions. Now, we realize that this is the basic gradient descent equation. When we do, we said that the learning rate shouldn't stay the same as you go from epoch to epoch. So suppose you decide to train the data for 100 epochs, what should you do? We saw in the loss function visualization, which I can bring back again. Let me do that. It should be sitting somewhere here. Let's go. Do you notice that? We'll again keep referring back to this picture. In the beginning, let us say that you are up here. What do you want to do? You want your learning rate to be small or big? Can you guys see my mouse? Yes. In the beginning. It can be bigger. You want it to be big. You want to take rather large leaps because you want to fall off these little crevices quickly. Local minima. Closer and quickly. Local minima. Closer and closer to the minima. As the epochs evolve, you want to slow down. So at the beginning of each epoch, you need to decide what your learning rate should be for this entire epoch, right? And that is what we did when we did the learning rate, learning rate you will take, you will consider for each epoch. And this is very important. You change the learning rate after going through an entire epoch of data. So it is course grade you don't keep changing it in the middle of the epoch itself you do that and so this equation becomes the your fundamental equation t plus one got modified to t minus this part got replaced with the learning rate scheduler, right? Which is a function of, right? I will just mark it. It's a function of t and the current value of theta, theta t, and which epoch you are in and so on and so forth. That times the gradient t. So we did not touch the gradient itself. We had all sorts of ways to do this. We had the cosine annealing. We had the exponential decay. We had the stepwise decay. And stepwise, yeah, well, let me just call it stepwise decay for better word and then there is also one that is sensitive to the plateau plateau of loss function now this i didn't get time to cover and i don't think i'll get time to cover today so how about this we we make this a reading assignment for you guys and we do it at some point in the lab kyle did we create a lab using the plateau also no just with the stepwise and exponential just the stepwise and how about cosine yeah uh it is working um so there okay but plateau is not there see if we can add so guys your assignment is the plateau rate scheduler learning rate scheduler on your own and if it looks confusing then I'll cover it in one of the Sunday sessions alright guys so now one thing is there at a learning rate became this but we did not play around with the gradient now we are going to play around with the gradient we are saying that this equation the next step is we are going to play around with theta t minus and whatever your learning rate is i don't care maybe maybe you leave it as a constant eta so for now for the sake of this argument to keep both of these things separate i'll just keep it as eta and this becomes some function of t, right? And theta t, of course, of theta and t, the current value and t. So we have replaced the gradient with a much more sophisticated version of the gradient. We are saying we won't literally use the gradient, but we'll use some function of the gradient, right? Now, why in the world would you do that the reasoning is very much like what we talked about before see if you don't have a momentum a velocity what happens is did you remember that we used to get if you come down and you hit this point any one of these local plateaus or local crevices local minimas you will get stuck there right because if you try to move away from there the gradient will point back at it will take you back are you seeing that so in other words suppose you are in a situation like this In other words, suppose you are in a situation like this. You are here. Right? You are here. What is the gradient here? Zero. Isn't it? Gradient is zero. Would you agree? Or let me just say gt to use the convention notation of this. gt is zero at this point. Right? And so you have been coming down here. is zero at this point right and so you have been coming down here you're at this point your gradient is zero so what will happen learning will stop no matter what your learning rate is your learning will stop here because whatever you multiply your whatever your eta is you're multiplying it by zero right so the next the next value of the parameters would be the same as the previous value of the parameters would you agree guys are you seeing this it's an obvious statement i'm making yes bounce out of local minima yeah one of the downsides of remote classes i can't see your faces so i can't see whether you're understanding or not but yeah let us assume that you have some speak up guys if you don't understand speak up i'll assume that silence means you're understanding it now what you want to do is you want to somehow have a modified gradient g which will make you go past it. Something that will make you go past it, right? And go past it enough to jump beyond that, jump beyond the local, make you take a step to here, something like this. Are we together guys? So you realize that the local gradient is not enough. What do you need? We can take the velocity or the momentum we have developed coming down. And what we do is we say G is in some logical sense, momentum We say G is in some logical sense, momentum plus the gradient, plus GT. How about that? Logically. If we use it, that it is the sum of momentum and GT, then what will happen? We will move past this minima, local minima. Am I making sense guys right the same is true for the plateau suppose i have you would just stop here gradient is zero but if you have if you are on the roll you will roll past this but when gt like gt equals zero you multiply by zero you always get zero so how's momentum going to help you no because your total g is the past momentum plus gt you're saying when i do the gradient descent i will use g capital g right so even though this goes to zero but because this is greater than zero i will still move i will still update my parameters are we agreeing on each so let's say that by the time you reach this place your momentum was 10 right so what is your g it will be 10 plus and the actual gradient would be zero and so your update step would be what t plus one would be your previous value of t minus eta times g which is equal to 10 right so you will still end up updating your parameters manish you got that okay yeah that is the whole point what you don't want to do is stop at every time the gradient vanishes. You don't want to stop. You want to have from somewhere borrow from your history, from your recollection. You have to borrow a direction and say I'll continue to go in that direction. And what is the preferred direction here? The preferred direction here is follow this curve right continue moving this or or follow this don't stop here keep moving here right so you don't just bring this g you need to have an intelligent way of having some g some momentum some velocity so that is why these approaches are called momentum based methods. So where is the momentum going to come from? The momentum can be like, we can take the simplest form of momentum. The couple of methods we'll talk about. One is the stochastic gradient design, SGD momentum. For historic reasons, it's called SG momentum, because this is stochastic remember stochastic still. Vipul Khosla, M.D.: cars stick gradient. Vipul Khosla, M.D.: The same stochastic gradient descent used to be that you learn your what is your batch size and stochastic gradient descent. Vipul Khosla, M.D.: One right. Vipul Khosla, M.D.: yeah many batches so size in stochastic gradient descent? One. One, right? Yeah. Mini batch size. So what would happen? It would zigzag its way to the minima. And then even when it reached the minima, it will keep zigzagging its way around it. We saw that. So this momentum term was initially created, and it's really useful to fix stochastic gradient descent, but it doesn't apply only to stochastic gradient descent. It applies to many batch gradient descent also. So keep that in mind that the name is a bit of a misnomer. So what do we do? What we do is we say that we create a definition. We say that we create a term called velocity or the momentum, velocity, momentum. So remember if the mass of a particle is one, what is the momentum is the same as velocity, right? So that's the way people write it. So Vt plus one is whatever, it is whatever your previous velocity was multiplied by a fractional amount, this is anything between zero and one, right? Plus your learning rate times gt. So now look at this term. This is effectively your new g, t plus one, right? effectively your new g, t plus one, right? What you're doing is you're saying take the g, take the learning rate, but add a little bit from the momentum from the past also to it. This is exactly what we did here, right? So you can take mu is equal to, for example, zero point, depends upon how much you want to consider momentum, how strongly you want to consider momentum. Now, let me show you in very practical terms remember that uh so look at this thing we will do it visually um i will take this we will do gradient descent but as we do it uh no uh close um this one uh settings this is the this is the momentum very high momentum see this is your degree of momentum right think of this as your the mu here we want it close to zero so when you when your momentum is very low like 0.01 what happens let's see I will start here. You'll get stuck. Did you notice that we got stuck here, guys? Well, then it gets stuck at the next place. You never know whether it will ever reach the minima or not. It sort of is bouncing around and going somewhere else. So what is happening? If you look at it, if you look very carefully, it is got caught up in this local minima here. Do you see these guys? And it's a significant minima. And the next thing you know is it will get caught up in this minima. And this is a pretty deep minima, hard to get out of, right? So now I will start from the same place here yeah and what i will do is i will take a bigger momentum let's say momentum is uh how much should we take let's say i don't know point 50 half the momentum let's say now what happens i'll start at the same place is it generally between zero and one yeah zero and one that mu so now you notice this is a pretty big valley here do you see that this was pretty deep half so you're taking the consideration of momentum 50 is coming from the gradient 50 is coming from the momentum what happens doesn't help remember i told you that you'll get stuck in this deeper valley you're stuck in this big valley That much momentum is not enough to get you out. Do you see this guys? Just looking at this, you know that you really need a strong amount of momentum, right? So let's go and do that. Let's go and do that again. And this is what we are doing. We are learning by playing with it. Let's do this. Let's take and do that again. And this is what we are doing. We are learning by playing with it. Let's do this. Let's take a bigger momentum. We take a really big momentum, right? Right. And then what do we do? Let's see what we can do. We again go here and try clicking. This is really actually a hard direction because there are so many traps along the way right and you realize something happened what happened let's go and see what happened the momentum caused it to bounce around quite a bit you see that it did get caught up in the local minima, but after a little while, the sheer momentum took it across. Where did the momentum come from? From falling down this very, very steep mountain, isn't it? And so that momentum carried it forward. It made it knock off very fast here. And then finally it came here. Look at this. So do you see the value of adding a momentum term to the gradient descent? Grace? Oh, definitely. This is it. This is the basic intuition. So the rest is just algebraic equation. So what we are saying is this is our new GT. So when we take the step T plus one, it is equal to whatever the D value was there minus, well, this funny thing, which is a mu times the previous T minus eta G T. So you're saying that add a little contribution, a little contribution of past velocity, right? And this past velocity, so we do. You can keep doing this. Now you will realize that what it means is that the past gradients that it has encountered. If you look at VT plus two, you realize that this will become mu VT plus one plus eta gt plus one and now what was uh if you look at this you substitute the value here it is mu times mu vt plus eta gt right plus eta g t plus one so one of the effects you see that the prior two steps before momentum is getting mu squared and this is getting the learning rate is being again reduced by mu eta so this is just algebraic ways to say that the momentum that you encountered many steps ago the gradients that you encountered many steps ago their influence is gradually fading you are much more impacted by the gradients that you have been encountering in recent steps right that's that's yeah go ahead albert so here you can see visualization of that problem yeah so if you have a low momentum and you just let it go and you don't reach them yeah so how do you solve that problem your accuracy won't be good and so you will see see this is it in deep learning the the one of the biggest problem is given the non-convex loss surface of the complicated loss surface you should never trust your first run of the data you should always try out different values of the hyperparameter. So you have to do quite a few runs. So you have to go all the way from 0 to 1 in the moment of the data. You have to play. So now that brings up a next question. Well, it's computationally very expensive. Some of these learning things run for days. So you don't get the choice to keep on running with different hyper parameter values. So that brings us a very interesting question. How do you find the best value of momentum? The best way you know, the all of these are hyper parameters of the model? Right? How do you select those? And that brings us to the whole world of automated machine learning. And that automated machine learning is a big topic, big topic. Today is the most important topic because there are so many hyperparameters in this deep learning world, right? So suppose there are 10 hyperparameters and for each hyperparameter you take 10 possible values, let's say momentum, the mu mu you take 10 possible values 0.1 0.2 0.3 0.4 0.5 but then you realize that you're multiplying by nine other factors which have 10 values so 10 to the power 10 is 10 trillion and each time you train the model suppose it takes you one hour to train the model you don't have 10 trillion hours. Even if you were infinitely rich with infinitely huge amount of hardware and patience, your lifetime is not 10 trillion hours, right? So the fundamental question is that these models that take so long to train and have such high number of hyperparameters, how would you find the optimal hyperparameters, how would you find the optimal hyperparameters? And the search for that hyperparameters is variously called optimal. So that is a problem. And you can't do it by gradient descent because there is no gradient there for hyperparameters. For parameters there is, but not for hyperparameters. So you get into this very fascinating did i talk about biation today yes right so there's a it is one of the biation reasoning of biation mathematics that comes in and the process is biation inference and biation optimization those things kick in and they give you an elegant way a very elegant way to quickly find the zone in and find the best combination of hyperparameters, right, in very few steps. And so it's just marvelous to do that. Now, it is worth mentioning, do you remember, guys, that when we did the basic machine learning course, ML100, I talked about, people often talked about grid search. But I told you that grid search is actually a terrible idea, at least in a very bare minimum. Use randomized search, which is better for hyperparameters. But now I'm saying that even that ultimately has a combinatorial explosion of possibilities when you have a very high number of hyperparameters. So it doesn't work actually in practice for deep learning situations, deep neural networks. practice for deep learning situations, deep neural networks. What works is this Bayesian optimization. Bayesian optimization is a core area of research. Somebody once told me that if you look at the total amount of money that the different companies are investing in this topic, the sum total is more than a billion dollars just to make progress in this area of hyperparameter tuning and bias and optimization and so forth at this moment. So there's a huge, huge investment. I don't know how true that statement is, but I do know qualitatively that there's a massive, massive investment in figuring out the best hyperparameters and the bias and optimization problem. And this whole thing of learning hyperparameter. See, hyperparameter is not just the gradient, your learning rate, or momentum and so forth, or the learning rate, the schedule. There's more than that. In a neural network, how many layers should you have in the neural network? That's a hyperparameter, isn't it, to solve a problem. How many nodes should be there in each layer? That is a hyper parameter which activation function you should use is a hyper parameter isn't it and now i just introduced one more i'm saying that well you know the mu is your hyper parameter and so on and so forth even which momentum methods we should use because now i'm going to introduce you to two more momentum methods so that is a hyper parameter and we can go on and so how do you find the best hyper parameters that gets interesting and we'll do that in the as we continue the journey over the months and into the various courses uh this topic of automated machine learning is so important and so absolutely hot today it is not something we can do as an aside in one lecture it is something we should talk about in depth properly in the course of two three weeks in a subsequent course so we won't do that so this is it now this is the SGD momentum method. It has a hyperparameter mu. What is its effect? The effect of this is quite interesting. Let us say that you take some one particular parameter, right? Theta j, the jth parameter, right? Which in my language is w i j ofJ. of layer L. Right. Suppose the value of this is continuously moving in this direction. Right. It's pulling you forward, forward. It's gradient, the gradient of loss with respect to theta. Sorry, let me put it this way. Like every time you take a theta j step theta j t plus one right is equal to theta j in this language of t minus this term the step term the gradient term uh gradient plus momentum term what happens is that if all the time let's say that it is increasing in value. Right? What do you learn from that? During the learning process, it is moving every step. It's moving in the right direction, isn't it? If, on the other hand, this thing was continuously going up, going down, going up, going down, going up, going down, going up, going down. What do you conclude from this? Your learning rate is too large. You could, you have many things, but generally, basically, right? You don't want to take this zigzag too seriously because averaged out, it is going nowhere. Isn't it? This theta J is basically wiggling around and staying the same right and there are various causes like like you said learning rate and this or that but suppose other things learning rate is fixed in a epoch and as you do the steps the hundreds of steps that you do remember that the two for loops are epochs and mini batch step. Mini batch step. And remember, T refers to this step. For, for, mini batch step, you do the learning step. The gradient descent step is the last thing here. Remember the four points and gradient descent is, right, the last part. You take the step. You compute the forward pass. You compute the loss. You compute back, the last part you take the step you compute the forward pass you compute the laws you compute back propagate and then you take the step. R. Vijay Mohanaraman, Ph.D.: So when you take the step and you notice that something like is something is behaving like this, what do you want to do. R. Vijay Mohanaraman, Ph.D.: Suppose you learn within this up your learning rate is fixed whatever the learning rate is even if you're using a rate scheduler but what you want to do is for this theta i in which all the changes are in the one direction you probably want to take bigger steps but for this one where it is just wiggling back and forth you want you don't want to take much you don't want to do this uh like a roller coaster ride you want to more or less take small steps you just maybe a little bit wiggle is okay keep it more or less the same for the jth parameter does that make sense guys right so one parameter continuously shifts forward you you you want to pick up momentum in that direction and go faster another parameter is just wiggling around well you don't need to pay too much attention to the wiggle you want to just dampen it down isn't it and that is it because uh is it because we want to make it converge then it has already reached the minima and we just wanted to convert is that so for that one yes quite likely for the jth parameter if it is just wiggling around it probably is just oscillating or around or is unstable or has reached the minima or many one of the many reasons but you realize that something that just keeps doing that what can you conclude you might as well take the average value and sit there right you don't want to listen to the um wiggles the gradients too much for that thing so in other words how much you listen to the gradient if you if it is proportional to the momentum for the ith one uh let's look at this for the ith one you every time you're moving in the same direction so what happens all the gradients are nudging you to have higher and higher momentum isn't it picking up velocity when you add the term on the other hand here what happens all the gradients are cancelling each other out one gradient cancels the previous gradient cancels the previous that that one cancels the previous one that one cancels the previous one so what is that what does the momentum term be momentum will be approximately zero isn't it what do you agree guys that the momentum term well let me use the v term velocity term will be basically zero in this yeah right and the only thing you're doing is you're looking, if your learning rate is sufficient as well, you're just listening to the gradient up, down a little bit, up, down a little bit. You're wiggling around. But in this case, where this particular parameter, which is benefiting and going steadily in the same direction, you're taking bigger and bigger steps because you have momentum there. For the same fixed learning rate, you're taking bigger and bigger steps because you have momentum there for the same fixed learning rate you're taking bigger steps so the lesson to learn from this and i would like to show it to you there's a beautiful diagram in your book which i would like to use now let me bring up this application what is it called cruise see are you see are you why am I not oh see no not because CZUR strange name for the yes CZ ur scanner there it is uh bear with me guys for a moment and i want to show this in a beautiful picture in your book this is on page 173 of your book and i would like to show this a new version blah blah blah we'll do that later scan oh we are in why are we scanning badge color visual visual better okay right so So look at this thing. If all of your changes for one particular parameter, one particular weight, they are all, are you able to see my mouse guys? Look at this. Folks, are you able to see my screen? Yes. And the wiggles. All the are steadily moving in the right direction. What do you want to do? You want to take, you want to amplify the steps. Right? On the other hand, if your gradient is, you notice that continuously oscillating. So what do you do? It is very inconsistent. So what do you do? You just say, ah, forget about it. We want to suppress these oscillations. We don't want to listen to it too much that is the main point of a gradient based method and so what happens is when you uh do let me clear this when you actually apply momentum where am i how do i clear this yeah so look at this here without momentum you notice that in the good situation, in the good situation also we are taking small steps, in the bad situation also we're just bouncing around. We don't want to bounce around. And here we don't want to bounce around and here we want to take bigger steps. That is what momentum helps you do. What happens now is, now look at this. I'll zoom in a little bit more. Do you see that here you are each step, I don't know if you can make out, each of the arrows are becoming longer. Each of the steps are becoming bigger, little bit bigger. And here it is, let it keep oscillating, but at the end of all of those oscillations, it just keeps moving back and forth and getting there. It's oscillations they do you no harm basically right and this part i wanted to show you it has a limitation though when you use this gradient descent method what happens is see you added this term and this term you remember these two terms we were adding. Mu times the previous momentum plus eta, the learning rate times the gradient. Right? This is the equation that we wrote. Is it making sense, guys? We just now wrote this equation. Where is it? Yeah. Yeah. This is- In the book. Here it is. So this is what we wrote. So now it has a problem problem one problem that it has is that imagine that you're moving very fast here here momentum tends to take over because you have a strong momentum the gradient doesn't have that much influence. So here, where is the minima? If you're here, the minima is in this direction, isn't it? See, look at this, this blue arrow from here, it is pointing in this direction, isn't it? Let me, I hope I can draw it here. Let me use a color that is, will be visible over that. Maybe this is will be visible over that maybe this color will be visible right so at this point right where is the minima pointing to this is the big island of minima so the gradient is pointing in this direction right which is this guy gradient is pointing in this direction which direction has momentum momentum is in uh let me take yellow right yellow yellow or maybe i'll take blue the momentum is in momentum is in this direction. Isn't it, guys? You see, you have been coming along this line. So momentum is still along this direction. Right? Your gradient is pointing in this direction. Momentum is along the blue direction. So the additive effect of these two will be in which direction, momentum is along the blue direction. So the additive effect of these two will be in which direction will you go? You will end up going in the direction marked here, namely you'll end up going here because that is the end result of these two momentums. Well, this software is not terribly good, it's smudgy, but bear with me. Does that make sense guys? if you look at the effect the momentum vector overwhelms the gradient vector are you seeing that guys right it reminds me of the circus uh riders who ride in a cage and they keep they keep oh yeah yeah but driving in circles Oh, yeah. Yeah. Driving in circles. Their momentum overrides the... Yes. They don't go crash on their heads. Yes. So now, the problem with this is when you let the momentum take over and you have a very shallow or very steep, like, valley, right? If you think about it like this, let's go back to this picture. See when it is, look at this, this is a fairly steep valley, right? Minima is here. So, and some of these other visualizations, they have even more steep. This is a pretty good, nice, big minima, but in some cases, the valleys are very steep. So for example, let me find in the surface a rather steep valley. Oh yes, here it is. Look at this. Suppose this was the global minima. Suppose this is what it was. You realize that you could easily miss it on your journey home, right? You could go past it in some other direction right without veering towards this which is the point that this book is making that if you don't pay attention you'll go here and the next time you'll go here so you have literally blown past that very steep little valley there right so there is a trick you can use, which is actually a very popular form of momentum. What it does is, and the argument is pretty elegant, it says that don't take a step like this, take the step in two forms, take a two-step process. First, what you do is you let the momentum take you somewhere and then compute the gradient from there see here at this point let me say right at this point itself we are computing both the momentum and the gradient isn't it guys yes right but suppose we did this suppose we did this. Suppose we did the opposite. What we did is, now pay attention to what I'm saying. Suppose, here's a minima. So, right, or suppose at this, you are here. And let me mark it in color, bigger, fatter. No, this is fattener. Why is this not very bright white uh completely white okay let's see suppose your momentum is carrying you suppose your momentum suppose your momentum suppose your momentum has been carrying you here so what will happen is if you listen to the momentum term it will take you here right now imagine this imagine the surface so you listen to the momentum let's say that the momentum is carrying you along this direction. Along this direction. It takes you here and you let this little intermediate step take place. You reach from X to X prime, this location. Let me just say theta because that's the convention your book uses, theta to theta t prime actually, this really makes up the t's and I'm trying to be consistent with it, which is a little bit of inconsistent notation, but that's okay. You go here and here you compute the gradient and the gradient will now point which way? This way, isn't it? And then you take the gradient learning step here so this becomes your t plus one so you go from theta t to theta a t prime to the momentum you're following the momentum for part one and from that location wherever you end up with now you take a gradient descent step at the gradient, gradient t step. So this becomes your t plus 1. So what happens is that you will get a much stronger gradient if you wear off in the wrong direction. That is what this thing is saying. Now let me bring this part forward. Yeah, I want to see both of them side by side. Okay, now look at this. Here, let me remove all of the markings and let me mark this to line with this line. I am trying, this software is not the best. I am trying, this software is not the best. How do I line with, make it very small? Okay, let's see. I'll move those diagrams. So here, at this point, momentum was carrying it in this direction. Momentum. Gradient is taking you this and you eventually end up going this way. As this picture shows. But in this, what you do is, let's say that the momentum, it is still taking you this way. But what happens happens look here carefully you are in a much steeper gradient situation at this point do you notice that the gradient is much steeper right i don't know if you can make out it's a much steeper gradient so what will happen is the gradient will push you much more strongly inwards and so the end result of this two thing will be this final motion which we should give in some color some lovely color which color should we choose let's say i'm going to be exhausting the colors let's try this and so you would agree that these two lead to a movement in this direction. And this is nice. So you first took a step, followed the momentum, you went far off. Then you computed the gradient, which takes you steeply back in. And so your net motion is in this direction, VT plus one. But you actually took two steps. And so you don't fly off the handle you're doing here in the left one you're just sort of flying off but here you have a much stronger pullback and so you converge to your solution much faster and that is why this form of finding the gradients is this is a very popular and for the longest time it was the standard and this is the nesterov uh nesterov momentum method right and uh let me say let me so this is right this this page is around 170 you must study nesterov is written like this n-e-S-T-E-R-O-V. This used to be the most popular method for a very, very long time and its variance. Well, state of the art keeps moving and now we have one more, many more methods. So the new family or the king of the hill is something called ADAM, adaptive momentum. And adaptive momentum has more little detail. We will go into that. What I will do is, because we need to cover more territory, you got the main conceptual idea. Now think, people are doing optimization after optimization. So this book author says that they use ADAM as the default because it's the state of the art. Well, the state of the art has moved even further. Now people use Adam W. Keep making improvements but conceptually is the same, right? I will discuss about Adam in perhaps an extra segment. I would like to use the next hour to talk about normalization, a new topic called normalization. Are we done with momentum? This is the main idea of momentum. That when you do gradient descent, not only you go along the gradient, but you also consider the momentum from the past. That is the core lesson to learn. And when you do that, you don't get easily trapped into local minimas. You don't shallow minimas you move forward in the right direction so far so good guys that's the main lesson you're referring to the same yellow jacket book or you're referring to another book this book is for those of you who missed it this is this book it has just come out guys this is a it got published a couple of weeks ago it got published after we started the workshop right and so highly encouraging folks to get a copy of this book oh by the way kate did you look at the bulk discount thing for the whole class oh i need to make that call it's the poll worker has been uh taking my time i'll see if i can call during break tomorrow sure yeah no no don't do that you're busy with the poll right we can do it we can certainly do it okay it's on my list yeah no hurry so so that is it guys uh if you are saying are saying this whole course seems to be about gradient descent, in some sense, we have spent a lot of time on gradient descent for a reason. See, neural network surfaces, large surfaces, as you saw, are highly complicated non-convex surfaces, isn't it? They don't look anything like my nice, nice, beautiful convex bowl, right? With one single minima. Instead, they look like what they look like. Namely, they look something like this. Oh boy. It is both fascinating, beautiful, extremely frustrating. So for many years, people said that neural networks are a hopeless cause. You cannot do learning using a non-convex learning, non-convex loss surface. And there was a whole winter, neural networks, deep neural networks have gone through many winters. There is interest, then it dies off, then again it surges some fundamental problem gets solved it comes back with the surgeons people start using it and then again they get stuck and the whole field dies off and then again there's a resurgence so we are literally in the next resurgence of neural network theory the deep learning is most of the breakthroughs have started since representation learning work of Jeffrey Hinton. So Jeffrey Hinton is one of those people in this field who's very, very influential. He's one person who stuck through all the winters and believed in neural networks when people were abandoning hope in neural networks. So one of the reasons people, there are many reasons, but some of them were legitimate. We didn't know how to solve problems. Neural networks weren't getting us anywhere because some of the core pieces we had not discovered, people had not discovered. So it wasn't quite working in as many situations that people would have, but it was never not working. It was always working very well for some situations. So one of the things people theorized is was that because the loss landscape is so complicated and non-convex, therefore you can't really do neural networks. It's a hopeless cause. Well, people learn to all of these techniques that I taught you, the momentum, the simulated annealing, the learning rate, so the cosines annealing, the learning rate scheduler, you realize, and the next one, I'm going to teach you one word, the normalization and the regularization and all of those things, each one of them, small as they are, they have made a profound difference. They have made neural networks work, even though these ideas that you have been introduced to these concepts, they look small or they look straightforward. Historically, they are of tremendous relevance. This is what has made this very complicated, made it possible for us to learn in this very complicated last surface. So people now, the Jeffrey Hinton and people, they joke, they say that machine learning experts were suffering from something they call convexivitis. You know, the illness called conjunctivitis of the eyes. So they joke that people say that you must have a convex loss surface. They suffer from convexivitis. They only can work with convex surfaces but deep learning does just fine with highly non-convex surfaces so guys if you have been here with me trust me you understand more about neural networks than most people practicing in this field there is a joke that andrew ing used to make long ago at Stanford. He would give a course in machine learning and when people submitted the first homework and did it successfully, he would tell the grads, I mean he would tell their Stanford students that congratulations, now you're machine learning experts. So they would all get surprised because they have just picked up basics, linear regression and so forth. And then he would give the second part of his statement, he would say it is because most people who claim to be machine learning experts in silicon valley actually know less than that you know right and uh in the same spirit i can genuinely tell you if you have followed me up to here without exaggeration i can tell you that your understanding is already deeper than most people in this field who have not bothered to understand this field deeply they are all you know code junkies they'll pick up code copy code from here from their pickup libraries and try to get the work done and sometimes it works sometimes it doesn't but when you really understand the subject deeply you have a mastery you have much more confidence that you can solve any problem that this neural networks can solve, you'll get there in a short time. So that is a very important level of confidence to have. So we are going to end with the gradient descent thing now. We will talk now of a much simpler topic. And once again, I'll give you no more than a five minute break. We will now talk about normalizing.