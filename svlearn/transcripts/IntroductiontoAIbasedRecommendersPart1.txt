 We'll give about an hour, hour and a half to the topic of AI recommenders. These are all AI models and we will cover these recommenders from the beginning, from part one, part one is the original recommenders, as they used to be to the state of the art today. So this started out as quite an interesting problem. It has to do with the whole nature of the so-called web tool. When companies like amazon.com, et cetera, came about, the main value was they were selling things online. You could order it online and you could receive it without going to a shopping mall and so forth. Now, when you look at the history of bookstores, for example, there was the Borders, there was the Barnes and Noble, many of us have fond memories of those and taking children to those. They used to dominate the market. You would ask, why would a bookstore, why would an online bookstore, which is what Amazon started out as, What possible chance did it have against this behemoths, the borders, the Barnes and Nobles and so forth? Why would they succeed? When you ask that question, you realize that Amazon started with quite a few disadvantages. For example, you could not see the book, you could not preview it, you could not, in those days at least now we have a preview functionality people books are very tangible things people like to touch it feel it smell it and so forth and then purchase books there's a pleasure in browsing bookstores and so you you deprive yourself of all that pleasure. Furthermore, if you purchase a book, it would be a week or three, four days before the book actually arrived in mail. So with all those disadvantages, it is worth asking from a very practical perspective, why did Amazon take off so well as a bookstore? And the answer, there are many, many answers given to it. Today, of course, those answers, they look sort of no-brainer. The answers that are given, and this is where we will start our technical discussion is, see, first of all, a brick and mortar store can have only n number of books. N can be based on the sort of floor area that you have or the shelf area that you have. For small bookstores, you will take the world's bestsellers in each of the subjects. So suppose you're keeping books across 20 different subjects. In each of the subjects, you will look at the top-end bestsellers. Common sense strategy says that you should populate your bookstores, your little bookstore, with the top-end items from the bestsellers list. And the best best sellers used to be quite a thing. Everybody wanted to know whether a book has made it to the best seller. Because once it makes it to the best seller, there was a rich gets richer effect, right? Beyond a critical threshold, your book would make it to the best sellers list or would get reviewed highly and had a higher chance of getting to the bestsellers list. And so people kept hoping and praying that the very few critical reviewers would review your book well, and it would have enough sale consequent to that to make it to the bestsellers list. Once it made it to the bestsellers list, it would actually show up in a lot more bookstores. And once it showed up in bookstores, it actually had a fair chance of being purchased. So that was the world before these recommender systems came in. Now, when you move to the online store, technically you have a sort of an infinite warehouse. You can do, you can acquire a book or ask a publisher to directly send it to a user, to a purchaser. Now, how many books can there be? Suddenly you have a whole world of books open. And these books, the first thing we notice about books and in consequence later on, we realize that the same thing is true for a lot of other things. So if you look at the number of items sold, number of items sold, and think of this as popularity top, or people often call it in the language of BI, business intelligence, most of you must be familiar with it as top end or as popularity. The way is the distribution follows what is often called a power law or zip in distribution. What it means is that if you look at the popularity, a number sold sold let me just call it popularity of an item i based on its ranking let's say that the ranking in terms of popularity is r so you would see that the popularity the measure of popularity how much you sold was approximately proportional to r to the power some n right It could be this sort of a graph is power law graph or Zipfian distribution, power law graph. When you have a power law graph and the other observation in this is, in this power law and then you could have various different shapes of it of the power law and so forth. But one thing that was definitely true which this picture doesn't convey, is that if you look at how long the tail is, you will realize that it actually dips very fast and it stays, well, I am sort of increasing and decreasing it. That is an error. It should be flat, but I'm not able to draw a flat line. This is called the long tail. You have a long tail distribution. What it means is that while very few books are hugely popular, there's a tremendous amount of books whose sale is not zero, whose sale is greater than zero, or you can even say greater than, let's say, 10 or some very minimal threshold, let's say 10 or some very minimal threshold, let's say greater than some epsilon value, enough that it should be, it is profitable to sell it. So long as you don't have to store it in your store. I mean, it's not occupying and displacing some high, more likely to sell book from your bookshelf. But they are of value. And this still is heavy. In other words, if you look at that it is also a long heavy tail heavy tail what does heavy tail means if you look at the area under this curve it is actually greater than half the area under the total curve it means what it means is if you could sell these items which other stores cannot sell which a brick and mortar stores cannot sell this is your internet opportunity here right and this is your let me just call it the zone of opportunity zone of opportunity in the sense that you literally have no competition from or very little competition from the brick and mortar stores because they simply don't carry books from this long tail. Now the question is how valuable are these books are now and from books let's generalize to items to movies and to things like that right. So one example that you could give in the US is you know that in the Indian diaspora, Hindi movies are very popular. Why do mainstream movie halls not show theaters, not show Indian movies? The answer is very simple. The clientele is very small. The clientele belongs to this long tail. And yet you imagine that there is quite a bit of money to be made by exhibiting movies or just showing movies from the Indian, the Bollywood, because you have the market of the Indian diaspora, which is significant. It's just not cost effective to do it in a brick and mortar store. So this becomes the zone of opportunity, for example, for Netflix, right, and for Spotify, the songs of various countries and so forth. And even within the US, there are many genres. For example, there is a science fiction writer, it was actually, she's passed away recently, a writer that I like very much, Ursula Le Guin. She has written absolutely wonderful books, like for for like and some of you may have read it like The Left Hand of Darkness or A Rose for Lucy and things like that. Hauntingly beautiful science fiction stories. She is not very well known. She is not like the author of Lord of the Rings or something. But at the same time, her reader base has never died. She has had a loyal amount of readership continually over the years. So never very popular, but never out of fashion either. And this long tale is made up of such interesting writers and such interesting content, that is, for which there exists an audience, it's just that that audience is minuscule. But the sum total of all this minuscule audience actually makes for a massive, massive opportunity. Now, with that as a business background, let us now ask, how do you sell to this long tail, for example? You realize that if you try to sell to the top end, the other side of it, the most popular, this has a rich is richer, rich gets richer mentality. If you only stock and recommend the best sellers list, you will keep on perpetuating and exaggerating the sale of the best sellers. Isn't it? and exaggerating the sale of the bestsellers, isn't it? Disproportionately. And things that just about don't make it to the bestseller but still are very good, they will be altogether ignored, right? So it is a feast of famine economy. It used to be for authors and things like that. So now we live in a different world. The world is full of authors and lots of millions of authors and they all have a little bit of readership and they all are in many ways we live in a much better world as far as producing content is concerned. So the crucial question is how is it, what were the factors that went into making this zone of opportunity come about. So one of the things is if I could tell that suppose you have a user, user X, I have to use them and there are many items. So let's say that this is a fundamental question. Suppose you have item zero, item one and all the way to item. M right, and suppose you have user one user. and all the way to item M, right? And suppose you have user one, user N, right? You're here and you ask this very basic question. We are not even asking about rating. What you're asking is given user I and given item J at the intersection of user I and item J, what happens here, right? This is the question that we will answer. Now, when you look at today, a website like Walmart or Amazon or Netflix or even YouTube and so on and so forth, any of these content providers in some sense that you purchase, what happens is they give you an ability to purchase things which is the first thing engage with it engage with items when you engage with items engagement can take one of many forms a very concrete form would be the user actually purchases that item right or in youtube it would be the user watches that YouTube video. Right? A less concrete form but still suggestive of a signal would be the user went and read the description of it. Right? Searched for it and then drilled down into it and actually read the description of that item. Right? On Amazon, it could be a book, it could be one of the items on YouTube, it could be a video, and just read through some of the comments, and so forth. Another signal could be that the user searched for something and it showed up. It means that it is within the zone of relevance for this particular user, that particular item, or that particular book, and so forth, right? And some even stronger signals could be the user not only consumed or purchased an item, the user actually left behind a rating and a comment for it. And now what happens is if you look at the data, you have a lot of data of items showing up in searches. You have obviously a lesser amount of data of user drilling down into items, you know, of all the results that show up in the search result when you visit Amazon or YouTube. You drill down into only a few of those. And then once you do that, even smaller amount of data, you actually go and watch, right? And even smaller amount of that you actually go and watch right and even smaller amount of that after that you probably share or you share and perhaps write a review on so what happens is right give a rating on and even fewer you bother to take the time to write a write a review for but to the extent that we are i mean the fortunate thing is human beings are a very social very gregarious community we like to express and say things fortunately the even though the comments are the smallest set of data that you will get from the user interaction user item interaction it is still substantive the initial question that that that was posed is, can we predict? So the basic question that we tell is, given a user i, given an item j, so you have a fundamental thing. Can we have a function such that this function maps a user and an item to something. Either it goes to a Boolean field, it says will be interested, thumbs up or thumbs down, will be interested or not. It could be 1, 0. Will or will not buy. So yes, no. Will buy or not? I'm taking an example. Suppose your goal is to sell this item to the user. You can do this. So this is one question. The other way you can do that is would the user give a thumbs up? Or would the user give a thumbs down? Let's say a thumbs down, thumbs up. let's say a thumbs down, thumbs up, thumbs down, right? This could be the case where a user visits a YouTube video or something like that and gives a thumbs up, thumbs down. Or you could give a rating on a Leichhardt scale. You could say you often find this rating on a five scale right rating now when you give a rating on a like art scale the like art is a formal word uh like card like i believe is it e or a i forget okay like art scale then you take three as neutral anything below two is below three is negative three as neutral. Anything below two is below three is negative and anything above three is positive. Right and so these are the things and lastly of course you can have a detail a comment as a rating. So if you have a comment which is a text what you can do is in effect comments can be translated using natural language processing a sentiment analysis so and all of you have taken the deep learning course so you remember we used but the one of the earliest examples in natural language processing that we did is we use a transformer model a bird model remember hugging faces we used a hugging phrase bird or transformer model and there are many lesser models many many models, many different models, not just that, to do a sentiment analysis of the text. And you again come up with, you can convert this either to rating, to some form of rating, right? Either a like heart rating or thumbs up, down rating positive or negative sentiment right positive get there or scaled you can do a scaled rating it's very easy sentiment analysis today as you know natural language processing has advanced enough sufficiently that we don't catch sarcasm it has limitations it doesn't get all the things right, but as far as business is concerned, we have good enough signal, right? We have reached the level that we have good enough signal for sentiment analysis, right? In a statistically aggregated works, though in the individual cases it might not be so perfect. For example, it doesn't catch the nuances of humor, of sarcasm, of irony, and so forth. But with all those limitations there, the state of the art is today useful, right? So this is it. And now the question is, this is what we want to do. This is the fundamental problem that we are trying to say. I will just leave it as a buy or not buy, right? Now now the history of this field is before ai started proposing solutions to this you can imagine that this solution is older than than our web duo it's older than amazon or anything it's a fundamental question that marketers ask how who should i sell this item to and when you try to figure out who to sell the item to there are many many techniques the the the broad class of techniques is that you do which you don't the traditionally you don't uh you were not using ai for and let me use a different color to do this before ai what you would do is you would look at, for example, item characteristics. Item characteristics. So for example, if I notice that you're searching for computers, right, you happen to read some computer which belongs to, let's say, Dell, a Dell machine. which belongs to let's say Dell, a Dell machine, what is the next thing that you do? The user hasn't yet purchased it. It makes common sense at that moment to recognize that this particular item is a computer, is a desktop computer with a certain characteristics. So for example, it's a mid-range computer and you have some demographic information this sort of computer is popular amongst you know certain income groups or students or something like that what do you do you immediately start showing more computers more computers to the user and you see this behavior very much in effect if you walk into a store. You go and talk to a salesman and you say, I came looking for this particular computer. He'll take you to the computer. But very soon he'll start showing you all the other computers and all the computers are kept in proximity of each other. So that so that the user can potentially comparison shop, compare this model to this model to this model and their features. And obviously, the stores will very helpfully put the characteristics, the features of those items in some little display that this is this has this much memory, this has more memory, this has even more memory, this has a bigger processor, this processor and so on and so forth. So there is a way to show a gradual gradation of computer abilities along various dimensions and the user can select. So the same idea when taken to the web, it's literally lift and shift to the web environment. Could we do that? Yes, we can do that. The user is searching for shoes. So let's show all sorts of shoes. It's common sense. We can do that. So that is item characteristic based recommendations. You can recommend things or show based on that. And by the way, there is a continuum between search and recommend. If you look at Amazon, the lines between searching and recommendation are blurred. In fact, today we consider search to a large extent to be AI driven. It's not everywhere true. So for example, we, and again, those of you who did, none of you here in the audience did that project with me, but if you did it, we made a distinction between the search results coming from Elasticsearch and the search results in which you have further augmentation from AI, right? Deep learning based search and the search results in which you have further augmentation from AI, right? Deep learning based search and so forth. So the latter is highly sort of has a huge overlap or is interleaved with recommendation systems. So it will keep that aside. The second way is user characteristics. And again, I'm talking of the world before AI. This is trying to segment the user into buckets. So for example, you want to, the joke is, the last thing you want to do is sell a lawnmower to somebody living on the 10th floor, right? Because you know that that would not be relevant. So you bring the user up into certain segments, market segmentation, and for each segmentation, you know what items to show, right? So for college students, the last thing that you want to do is show potentially minivans, they're not interested in minivans, whereas a different group of people, you do show minivans have a different market, isn't it? And so for likewise for luxury cars, it is pointless to show it to a young, you know, people with a starter job, freshers with starter jobs and so forth. At the same time, you don't want to, and so there's the car industry, you know, there's a huge amount of segmentation and people have niche market brands of niche markets that they go after so that is user-based recommendation you try to find the you where does this user fit which of their segments market segments the user fits into based on the user characteristics and now you can sell items to the user based on that segmentation right so that sort of thinking has been there and you pick up any book on marketing in an MBA course and you will see these things talked about quite a bit you know in a more broad sense now at the turn of the century so now this is the history of it and we will come now so market segmentation I'll just put the word so now I'll come to the turn of the century, the 21st century, and now I'll start getting a bit more mathematical. See, there was a big question. The whole value proposition of this so-called e-commerce companies that were emerging was from the long tail, long heavy tail. And somehow we needed to find for a given user, what were those unique things sitting in the long heavy tail that this user would find useful, right? So once again, so the function that we talked about, let me use a different color, we're changing theme here. Let's say, I'll try this. so the basic function is given a user i given an item j what is the value what is it is equal to what so initially people were looking for explicit rating they were looking for a number. Amazon started the tradition as a rating from one to five. Then Netflix followed. And if you look at this, did it succeed? It did succeed quite a bit. If you ask this question, why did Netflix, for example, completely break down Blockbuster, which used to be a video brand. We used to all, if you remember, some of you, we used to rent videos from Blockbuster, forget to pay back, or return the video in time. And then of course pay a tremendous amount of penalty. And in fact, they used to monetize a lot of, large part of their revenue from the penalties and so forth. So that's the old history of this, of Blockbusters and so forth. Today, most the young people have never heard of blockbuster and similar brands. Why could they do it? Along with it is the fact that today e-commerce companies, sorry for the sort of place. Give me a moment. Today, e-commerce companies, you go to Amazon, can you guess what proportion of the sale happens because a user came to the site knowing exactly what they want versus purchases made by what Amazon recommender system recommended. So some of you of course have, no, I've spoken of the answer in previous workshops. So please don't speak up, but those of you who haven't, can you make a guess what proportion of the money or the revenue of Amazon actually comes from the recommender systems, things that it has recommended to you that you eventually end up buying as opposed to i know that i need this particular device and so i've come to the site to buy this particular device anybody would like to venture a guess I think about 70%. 70% comes from? The recommender system. That is true. Most of it, a large proportion, much more than you would have thought comes actually from the recommender systems. So recommender systems have come to dominate our world. And in fact, if you ask this question, what is the singular value proposition of this e-commerce site first of all there are multiple things one is the wisdom of crowds whenever you go to an item you see the ratings on that item or the thumbs up thumbs down the item contributed by community now if you remember the ensemble methods with thing that we learned the wisdom of crowds is generally better than the statement of one or two experts. It's more trustworthy for sure. So when you get critical reviews of a movie, you may or may not like the movie, but quite often you'll notice that your own review is much more aligned to some users, right? And some ordinary people who have reviewed the movie and given things, comments on it. So that is the wisdom of crowd kicking in. That's a huge factor, the fact that you can actually observe to study what others thought of the product of the item. And the second factor that is much more that is equally relevant is that it uncovers for you things that the moment you look at it, you know that it was in the back of your mind, you were thinking of purchasing it, and it is actually useful for you. So for example, going back to that situation of a Dell computer and recommending more, see, till you haven't bought a computer, seeing recommendations of a computer is very more. See, till you haven't bought a computer, seeing recommendations of a computer is very useful. But the moment you have bought a computer, you will realize that that old way of showing you more and more computer is absolutely useless because computer is the last thing you want to buy. It is probably more pertinent to show you a printer, a scanner, a a monitor anything but a computer isn't it so what is more vital is to know what else what is it that you should be next looking to so that brings up multiple dimensions from an ai perspective multiple ways the old way which was a did in data mining we had something called market basket analysis so you would do associations you would notice the fact that computers and printers and monitors tend to be bought together and mice and keyboards they tend to get bought together so you would create these associations and there were all these algorithms like the air priority algorithm and so forth that you would apply on the other hand you uh you could look at it also, and this gets into the sort of AI we have been doing just a few months ago, you can look at it as a sequence model, a time series. What is the next thing a person is likely to buy after buying a computer? So you realize that if you don't have evidence for this person buying, let's say, a printer, and you can put it in a buy-in perspective. Let us say that the prior probability, and this is how you would say that, suppose the prior probability of buying a particular printer, printer i, is something, right? This is your prior probability. Now, you don't, this is in looking globally at the data and knowing nothing, anything about the user. Let's say that probably a printer tends to sell one for every 10,000 users. That, so the probability, the prior probability of the printer being sold is one in 10,000. Now look at the evidence. The user has just bought, given the fact that the user has bought, you multiply it with the likelihood of now function of, you have just bought a computer, right? And so now you have the posterior probability, the posterior probability, probably probability of that printer. probability of that printer. Would you say that the posterior p is greater than prior probability now that the user has bought a computer? Would common sense dictate that you're much more likely to buy a printer. Are we together guys? Does it make sense? Anyone? Everybody is on mute. Yeah. That makes sense, right? So what happens is that there is a certain amount of sequential, you can build sequential model that given the behavior over time of your purchases and your interactions, what are you next likely to do? So you can build a model on user interaction, predict user interaction. interaction, predict user interaction over time. And actually, from an AI perspective, we are doing that with great success. See, you can do a static model, a model that is invariant of the time dimension. Or you can build an AI model, which is a sequential model, very similar. And when I take sequential, I mean it in the sense that we were talking about in the deep learning workshop series. For example, LSTMs and RNNs and GRUs and stuff like that. So we can't be using a LSTM model or any recurrent neural network model, or even one of the classical time series AI model like ARIMA or something like that, you could use. Though it's much more traditional, it's much more common to use today in 2021 to use a deep learning model for these things. So you could do that, right? So there's a sequential nature for that. So now we will ask this fundamental question. How do we solve this question in today with modern ai so when people started looking at this question the history of it is that netflix had an algorithm in-house the sign mark or something they used to call it in which they would predict with what movies a user is nest like next likely to watch or would find interesting. And they would recommend it on their website. Remember, Netflix in those days was not streamed. It was still going out in envelopes by postal mail, and people were returning those envelopes after watching the movies, those DVDs after watching the movies. And yet it's a testament to the recommender systems that Netflixflix even then completely demolished blockbuster and all the other brick and mortar video stores completely wiped them out the streaming service and then of course came the streaming service which added a new dimension to it which instant in time you like something you start watching it right now i expect of it but even before streaming came blockbuster was practically for all purposes destroyed and gone the old world was no more you know we were in a new world so that's the value of the recommended systems now they threw a price a million dollar price a million and one uh 1.1 million i believe or one let's say roughly one million dollar price and in that price, they asked this question, can somebody beat a model by 10%? And many teams tried, and then the teams collaborated with each other and so on and so forth. And at the end of it, the team that won, which was a collaboration between two, three teams, finally beat the 10% boundary by a significant amount. And I believe they first went to 11%, then they went to 12 or 13% or so. And they soundly beat Netflix's own model. That algorithm, we will use that as a beginning foundation today. How did they do it? How did they do it? And having done that, it sort of started this whole world of personalized recommendation. The solution that came out of the Netflix prize is the context for today. I would like to give the whole time to walking through it. I have walked through it with some of you perhaps in a more brief way, but now I'll go into the technical depths. So that started amongst other things, and I'll just end the historical preview here or the prelude here. What it did is it started an explosion of possibilities. E-commerce really took off. At the same time, you could see that it was affecting many adjacent areas. For example, online advertising, this whole business of targeted advertising, is nothing but the recommender systems coming and taking over. And they have been very, I mean, today it has become uncannily effective. If you own one of the affinity cards from a grocery store, it is today well established that when you enter a grocery store, you might have a list of things to buy, like most of us are responsible, and we'll have some things on what we need. And we think we'll buy this, the things on our list. But when you look at your shopping card, quite often, for most people not not for some highly disciplined people but for most people what you come back with is somewhat resembling your shopping list but it has a lot of other items now it turns out that the recommended systems are much better able to predict and tell what will be in your shopping cart when you walk out than you yourself were. And just ponder over it. You think we know, we all think we know ourselves and when we enter a shopping store or we enter a grocery we feel we know what we are going to buy and yet the shopping cart is much more reflective of what the mathematics tells you will come out with. It is that uncanny accurate and there has been many, many cases. I'll end the prelude with one particular case which was to do with Target. So Target started doing this targeted advertising, I suppose. It's a pun. So they got hold of a Silicon Valley company and say, can you do targeted advertising for us? This was a much talked about case a few years ago. So they said, why not? Let's do targeted advertising. They started sending targeted advertising. What it meant is that the brochure that you got was different from the brochure that you're and the coupons that your neighbors got in the Cooper so there was subtly different then one fine day dad walks into a department store target very angry and says there's something wrong what do you mean what are you guys trying to insinuate we notice that in our brochure there are are coupons for pregnancy items for pregnant girls and for childbirth and things like that, you know, young babies and things like that. And we don't see it in our neighbors' items. These coupons are not there in the brochure for our neighbor. And obviously, the departmental manager was very apologetic and was, he expressed a lot of, I don't know, concern and apology. And then if I remember the matter correctly, I only vaguely remember the whole story. What happened is a few months later, the father comes back and now he was apparently perhaps angrier. He's like, how did you know? How did you know something that we didn't know? So it turns out that in that house there was a girl, a young girl, a teenage girl or something who was pregnant. Right. And when you are pregnant and you purchase things in the department store, your behavior changes slightly. Your taste in things change and what you pick up changes. And the AI had caught on to that. Because it had caught on to the signal, it was recommending different things to that household, knowing that there was, this was the user characteristic there now. in physical terms pregnancy takes a little while to manifest itself and so the family became aware much later about it right and it just goes to show the uncanny power of the recommender so that is the end of the historic prelude and that was a few years ago so you can imagine these recommender algorithms today there is a there's a very interesting thing to it. On the one hand, they're really worth established. You can almost say that recommender is old news. It's boring. I've heard these phrases come. People are using recommenders and they say it's totally boring. All you have to do is take up some libraries, plug it in and it works. That is true by and large. Today the recommenders are a very mature domain and there are excellent libraries that do it. But there are two other sides also to this story. While you get a generic, a very pretty good performance from a recommender and they are in quotes boring, we have really mastered them from our artificial intelligence or deep learning perspective and uncannily they are they they do far better than we do as human beings and they guess things that we couldn't have anticipated they're doing all of those things and they're very mature libraries to do it yet there there is a huge scope for improvement because every bit of improvement that you make to a recommender system translates typically in e-commerce to an untold amount of millions for every company every large e-commerce company it means a lot for example if you are recommending books to users or in the company that i work for we which is we are as you know under my day job is in the learning company but we recommend learn personalized learning the fact that you we recommend personalized learning. The fact that you can recommend personalized learning, you can understand what a user is trying to learn, what the user should learn, and so on and so forth, is tremendously powerful. At one point, I remember there was one client, a British client, a large British client, and they wanted to use it. They said that we'll purchase your product if and only if you can show us that it works. Now, obviously, real life is more complicated. You know, in the application, there may be all sorts of constraints and whatnot. So we deployed the application absolutely without constraint. And we got a team, obviously, because it was a high powered really big company like as in global conglomerate. So they got their best AI experts there in that place. These were all PhDs in machine learning and so on and so forth. And the whole test was that use it for three months and then we'll make your recommendation because it takes time for it to understand user behavior. They used it for three months at moments they came back and then we asked compare your recommendation and each person in the meeting said i asked them what did this system recommend to you and what did it recommend to you and what did it and they all talked about their recommendation and i remember one of them saying what she said still resonates with me she said that these recommenders are so uncannily uh specific that it's it's practically it is actually a privacy violation to reveal what the ai has recommended to me just revealing it just speaking it out is a violation of privacy right so this is the extent to which the AI recommenders have become good. And what we had done actually, I would not by any stretch of imagination say we had done any rocket science actually for that particular system. What we had done was, yes, we had done some bit of original work in that, but broadly it was based on well-established theory for the last 10, 15 years. And that is how good it can be. Well, there is a other side also to this story. A recommender system, so the recommendations of AI, they don't exist in a vacuum. The recommendations, by the time they bubble to the UI layer, there are many application layers that come up. And that is where the cautionary tale is. I have seen here in my place, and I have seen in many places, because obviously in our trust, you see many people come from different companies and talk about the experience and so forth. Quite often, two things happen. When recommendations don't work, people always look at the recommender system. When they do work, then no one obviously questions that. You know, it is like astrology. I like to joke or whatever. If you make one wrong prediction, it will be noticed. search results produces something or recommend something hilarious, everybody will know about it. I think in India there was a meme that became quite, quite famous, right? Somebody was ordering a fan and apparently, I don't know how, whether it was a fake image or it was real, Amazon India recommended a stool and a rope. India recommended a stool and a rope. You know what it means, right? Did anyone catch the implication? Yeah. Exactly. And so that instantly became a meme. Everyone in the world knew that Amazon is recommending a stool and a rope with fans, right? So it's dangerous. That's a danger of recommendation. You have to be cautious with the recommendation and be very sensitive to the application domain. You have to put enough safeguards around it because if you don't, you get into trouble. So that's that. So now with all of that big introduction, let's go a little bit into the technical details. I will start with the approach. So there are many approaches to it. The dominant approach that I will start with. So let me break it up into two areas. There is a content-based approach. Content-based approach. Content-based approach. Content based approach by content, the word content is used in a more generic sense, user based, item based, or both of them together or together. What it means is you actually see what are the characteristics of the user. It's sort of like market segmentation, the old marketing, market segmentation thinking, user characteristics. Or you look at item characteristics. You see that this item is this and so on and so forth. Like for example, if you're looking at user, you look at the age, the income, the gender, such demographic aspects. If you're looking at item, you look at now all the income the gender the such demographic aspects if you're looking at item you look at now all sorts of characteristics of the item right that it's a computer part and this is a tractor part it's a tractor and so on and so forth so those are the item base and you put them together and you can do it and you can still apply ai techniques right so one very basic ai technique that you could do is you could just ask this question that suppose there's a user features a feature one of a user feature one let me call it the user feature and feature up to feature n and let's say and this could be the height i mean i don't know the age the income the etc etc the user. And then you could have item characteristic. Let's say item n plus 1 of the some feature of the item, let us say, of the item up to feature m. This is of the item. So item feature could be, for example, its price, its category that this item belongs to, subcategories, the product weight and whatnot. Right. So you can give all of that. So this forms your X vector. And so you're asking this function, will this user buy it or not? So if you say buy or not, now this is your classification problem. Or you could say rating. What would be the rating the user would give? And that makes it into a regression problem. Are we together, guys? Or what you could do is you could take this entire feature vector, and you could look for clusters within this and do all sorts of dimensionality reduction. Also do unsupervised learning, dimensionality reduction, or clustering, etc. So that is one approach to doing things. We will come to that. Today, I want to focus on the so-called collaborative filtering, which is the dominant methods, collaborative filtering. Within collaborative filtering, again, there are two fundamental sort of, there's a dichotomy. One is model-based methods. You actually build a model. You say the user buys this item because, and here is a reason, here is a model that you have that tries to capture the dynamics capture the because of this right so what does the data show you what is a manifest data the manifest data is user some users have bought some items right so uh so but you you build a hidden a latent model that explains why this user bought this item but not this item and not this item. Are we together? So when you try to infer a causative model underneath it, or not, I wouldn't say cause it's a generative model. When you try to infer a generative model that can tell you whether this user will buy or not buy any arbitrary item here. That is a generative model in the AI sense. And you build a generative model. There is another way of doing it is to not build a model at all. It's an unparametric sort of a prediction. And those are memory based models. Well, I wouldn't call it a model is sort of not not people use the word unparametric model, but I will just call it unparametric memory based approaches or unparametric approaches. So how would this approach work? What you do a little so these are these again come under the category of item based or user based so what are item based and user based i and i always in my case i always tend to get muddled between the two and then i realize i've said it the reverse way but i hope i'll do it the right way So what you do is in the item-based approach, you say given an item, so you know that given an item you would have records of many users who would have bought it, isn't it? So let's first go back to this big matrix, the interaction matrix. It's called the technically in all literature we call it the interaction matrix. We call it the interaction matrix. And most of the mathematical literature in this space, they represent this with a giant M. It's your interaction matrix. By convention, the y-axis is the user. The rows are the users and the columns are the items. Items, users. Are we together in this matrix? and the columns are the items. Items, users. Are we together in this matrix? So given a user I, and given an item J, what you're trying to do item J, you're trying to find what is the value here in the interaction matrix. The value could be just the user bought the item or it could be a rating. When it is a rating, thumbs up, thumbs down, or a rating on a Likert scale, a number, let's say one to five or whatever, one to ten. Generally, one to five is already considered too much. Suppose it is that, then it's, and you build a recommendation system in which you are trying to predict a number, a rating, then you call it the explicit recommender. So let's get a terminology right in the basics. And today we are just setting up our mathematical terminology. We'll go into details from the next one. So explicit recommender predictor rating. So in some sense it's a regression problem, right? You're predicting a number, you're predicting a rating which belongs to the set of real numbers, right? That is what sort of like regression, like regression. On the other hand, these days, increasingly, we realize that it's far better just to answer the question, will this user buy it or not? To buy is a vote for that item. Most people don't, as I said, the data set for just people buying is bigger than the set of data sets for leaving explicit recommendation. So you can consider a person buying an item, watching a movie, taking a course, right, listening to a song as a positive word for that item. And you don't have negative word, you don't know what the user is. So implicit recommendation just goes to and which which is what we try to do these days. What we try to do increasingly is we have leaned towards implicit recommenders more and more. Which doesn't mean that we don't do explicit recommender, but we give, let's say more weightage to implicit recommenders, because we know that owing to more data, they just tend to perform more. See, it's a first order. First question is whether you'll buy it or not. And then may or may not be important what you will rate that item, how much you will like that item, because the fact that you'll buy it is an overwhelming your preference for the item to begin with. So sort of like that so implicit recommender just answers a boolean question will you or will you not buy this item great and of course it's a it's more like classification like classification it's a classification problem so for for the rest of the discussion, I will assume if I mean explicit recommenders, I'll say so. But when I don't mean explicit recommender, then assume that I put the largest discussion. It may be either of the two recommenders. And in practice, the examples that we will take will lean towards the implicit recommender in keeping with contemporary practice. 10 years ago it was different. 10 years ago we paid a lot of emphasis on the explicit recommender. I remember the recommenders, the initial recommenders that I was building more than, you know, when it was just hot about 10 years ago. These things were absolutely hot. If you could just do a basic recommender system, you got a job in one of the... You not just got a job, you could do a startup in Silicon Valley. At one time, I remember the joke was that every single startup was nothing but a... I mean, it was just a cottage industry of recommender systems, or what they call targeted advertising. There were so many, many companies doing targeted advertising and targeted and this recommendations and so on and so forth. It wasn't even funny. It was odd to see some of the brightest minds of the universities graduate. And all they were doing is they were all trying to make the recommender systems and each of them essentially looked alike because ultimately each of them was fueled by the state of the art in theory and the theory is well published, the research papers are well published. Dr. G R Narsimha Rao, LACs, And it is just how you implement the product development around those recommendations systems for different use cases and customer needs is what created a whole cottage industry of lots and lots of startups and lots and lots of companies there but today we have moved forward so where are we today there are many approaches from our now let's look at it from a from an ai perspective we will use it we will use a family of techniques starting from um the original paper which won the netflix prize where is that? I wanted to show you guys the cover of that iconic paper. Yes, there it is. And those of you who took a workshop with me many years ago, I would remember that I covered this paper in great detail. Does anybody remember me covering this paper? Well, it's not a paper, it's an article actually, but it sort of is a famous one in which this matrix factorization technique that won the Netflix million dollar prize was brought up. And I will explain that to you. So that's a linear algebra, it's a linear technique, extremely effective, very effective actually for a whole variety of problems. The virtue of this technique is it doesn't use, for example, deep learning. So as all things considered, it is a track table problem. You can actually build a model quickly, relatively quickly. I would say relatively quickly. Building a recommender system is brutally, brutally expensive to give you a sense of how expensive it is. Where I work, I lead the AI team. We build recommender, again, for personalized learning. And we have about 75 to 100, now we have close to 100 million users, but it used to be 75, 70 million users and even less actually. Before that, the models used to have 50 million users and across about 100 to 200 million items to be sold, which is nothing compared to Amazon. Amazon has 300 million users and maybe even more now and about 100, 200 million items. So the items in our case are still pretty huge with the number of user bases bounded to about 75 to 100 million users. One training, just one AI model when we would train on a cluster, even today as I train, as we do, this weekend is being trained. We are using close to 100 machines, each of these machines is a super server. And the computation finishes over seven, eight days, the whole AI recommendation generation takes a lot of time. So AI, this this recommend is enormously powerful. But behind that power is also a tremendous amount of computational cost. So, which is far more than what you would get for typical classification or regression. We are in, especially in the deep learning territory, today it's a given that you burn a tremendous amount of electricity to train a model. And you have to keep training a model because user behavior changes, new items keep popping up, new users keep popping up. So we retrain our model every two weeks. And now in the next generation architecture that we are just putting in place, we are going real time. People are apparently not happy that a model gets trained every two weeks. It is a psychological thing. If you say, oh, you know, a model gets rebuilt every two weeks, they have an unhappy face. They would rather that we built a model in real time. New user comes, new item comes, it gets new user interactions happen. You factor that in real time and do that. Does it help? Yes and no. Yes and no. Yes in the sense that locality of effect. If we just found out that you have developed interest in, let's say, thrillers or science fiction books, you don't want to wait for two weeks to recommend. Just when we see you getting interested in science fictionality of reference quickly to catch on to the trends. The other side also is true. Human behavior or human likes and dislikes, they fundamentally don't change, especially, let's say that in my field of learning. I mean, obviously, leaving aside the whole educational theories and cognitive styles, each of us have a well-defined mental makeup of cognition. And so you can pretty much tell when very early on you catch on to a person's cognitive style and how or what they would like. After that, it doesn't change much. So there is some value in having real-time recommender systems and there is some value in not bothering because when you're doing computations at scale once in two weeks is enough this is by far invite i believe the social graph somebody told me i don't know how true it is that um facebook does it's a full model update relatively infrequently but they keep putting incremental layers on top you know incremental updates like i'm saying and then every once in a while only do the full social graph model updates based learning updates and so forth so your mileage may vary but anyway these computations are brutal but the matrix factorization as all things considered, today we consider it somewhat simple. So what is the basic idea behind a model-based approach? The model-based approach, actually, let me just first take the memory-based approach because they are simpler. So you look at this. Let me draw the matrix again here. Actually, I'll stop today. And I notice that we are running out of time. Let me take this matrix. So let's say that you have a user x high. It is a row, right? What ratings the user has given or bought or not bought the item. Now you realize that this matrix M is sparse. M, what is this quality? It's a very sparse matrix. How do we know that it's a very sparse matrix? Simple, suppose there are 100 million items in a e-commerce store. Any one user, you would have bought up to 100, 200 items. 100, 200 items are nothing as a data point compared to the 100 million items that exist. It's a one in million ratio, right? So in other words, only occasionally you will find that it has been bought. Most of the time, there is no data. There is missing data. In other words, there's question mark. There's not even that you won't buy. It is just missing vacuum. Every once in a while, there is data that you bought some item, right? So for a user, it is like that. Are we seeing that, guys? Likewise for an item. Given an item, and suppose there are 100 million users available, or 300 million users available, so it's a rectangular matrix. This is important when we go into the linear algebra of it. So given this item column, it's a column matrix, you would realize that this too is a very sparse, there would be only a limited set of things, only some extra, extra popular items would be something that maybe even a million people have bought. Most items are bought by 1000 people or 2000 or 10,000 people. At 10,000 people when you compare across 100 million people people it's still a sparse purchasing history right so m is very sparse in a way in a way you can think that you you don't know what these values are right and for each what you're trying to do is you can look at it as a journey from a sparse matrix to an m hat remember we put a hat for prediction predicted matrix prediction matrix a prediction matrix of interaction of interaction very sparse interaction matrix you're looking at a predicted interaction matrix in which all the gaps have been predicted. So for example, this would say, will not buy this, you will buy, right, and so on and so forth. Will or will not buy. Now, remember that in our classification theory, we say we don't like to predict absolute numbers because it's a probability, it's a posterior probability. Probability, It's a posterior probability. Probability. What you're trying to find is the posterior probability in view of the evidence that this particular user, probability that a user i will buy an item j. And that belongs to the, we tend to put it in the interval from zero to one, right? It's a real number. So it's a real number between 0 and 1. So it's a certain probability. So what you predict are not 0s and 1s. What you predict is the probability that this person will buy that item. Closer to 0 means it seems unlikely, and closer to 1 means there's a high probability of buying this item. So now let's look at this the item based right what suppose you do an item based what you can do is pick an item let's say let's say pick a printer you want to determine who should you sell this particular printer to. So what can you do? You could actually look at, or actually, let me do it the other way around. Let me do user-based. User-based. So what happens is that given this user would have bought, or I don't know, it doesn't matter. So you can do it both ways. So you can find items, one item two given this user what items the user has bought right uh suppose they have bought key items some key items now what can you do the items that the user liked most the top and favorite items of the user the top and favorite items of the user, you can take those top end and then you can go to this for those item. So let me just write item column vector, you know the column vector here as yj. So you can find the similarity, some sort of a similarity measure. So this could be from your cosine similarity dot products or something like that. But generally, you have to be a little bit smarter than that. So some similarity and a lot of the secret source of recommenders is in how you design your similarity function. You can do a similarity measure between this one of the light items let's say that i y i is the reference liked item and it's uh it's column vector and you do a similarity measure with all other items so you really the scale of the problem you have to compare against all 100 million items find the k nearest neighbors when you find the k nearest neighbors, you have to now you realize that that's a linear problem. It is order number of items, order in the number of items and then only you'll be for each user, you'll have to do this. And suppose you take and use n favorite items, you know, good items that user actually like good items that user actually liked, the items that the user liked very much. And then you do the similar exercise. Then it becomes order n times order n, which means that for a given user, you're doing a n times m computation for each user. So this is again quadratic already computationally brutal. And then when you realize that you are building such a model for each of the users, and now it becomes even more intractable problem. It becomes an order. Basically, it's a cubic order. So suppose there are N users, N favorite items per user, and M total number of items. You're looking at this particular order of magnitude computation, which is quite brutal. So people optimize, as you know, wherever there is a problem, there has to be some sort of a solution worked out. So people don't do the nearest neighbors. They do nearest neighbors they do approximate nearest neighbors approx nearest neighbors and as we go into the technical aspect we'll go through a couple of lab exercises at some point for example we'll use dim sum and so on and so forth you can you can use those techniques to do that then there is similarly, you could go the other way around, take an item. Let's say that some item, tilde, you can take. And for this item, what you can do is you'll get a lot of users, right, who have bought those items. User one, user two, user n. And now what can you do? Well, there are many, many things you can do but amongst the things that you can do is you can find users you can find those users which rated your item highly right which are exactly the kind of users you are searching for and again do a knn search right you can do a knn search so by the way here what do you do once you find in the item space the nearest neighbor items you can pick those those are your recommendations those are your item recommendations to the user right and now for items it gets a little bit more interesting you find this users who are most like likely to you know who are most similar to your well users who liked your item once you find those items now you have to ask the reverse question what between those use sorry once you find those users you find what other items is popular amongst those users you know amongst your uh favorite users what are your what other items for this item are popular there and then you again can recommend those items so it's a more indirect way now all of these ideas are based on fundamentally two things three three basic principles similarity measure you need a similarity measure which you have to be really smart about you need an approximate approach because it's computationally brutal you need approximate knn and the last thing after that game is simple you have to pick top end for recommendation these are again just to refresh it, these are model-based collaborative filtering. And today, collaborative, we are doing filtering methods. And then finally, it is 2.36 now, should we do that? Then I'll just give you a brief idea, guys, of what we are going to talk about the next time. Then comes this famous algorithm for the matrix factorization it is think of it as a granddaddy of our the more recent methods that we use which are much more deep neural networks and based methods for collaborative filtering but it's a grand idea of it what it basically says is that suppose you have this matrix M, right, you do one of the things which in linear algebra is sort of the crown jewel. It is called a singular value decomposition. Those of you who took my the math behind data science workshop of course are familiar with it singular value decomposition what it basically says is that this user item matrix with the the dots that you have imagine these dots are purchase dots in this very very sparse matrix the dots are what they are there is a causation you can you can sort of hypothesize a generative model or a sort of a reason why it is like this and not like something else right why the dots are not somewhere else right in other words to hypothesize that these are not random dots but there is a systematic there's a reason or a dynamics behind it what you do is you hypothesize a lower dimensional and that's why you find the word low rank matrix there's a lower dimensional let me call it a latent square matrix right there is a lower dimensional matrix or such that it is and so I'll explain what it means by that it means this that let's take a take L of dimension I'll explain what it means by that. It means this, that let's take L of dimension. I'll just take an example. Typically these are 200 dimension for Netflix, two to 300 dimension and so forth. But just for the sake of argument and the way this original paper beautifully explains, it explains it in very simple terms. It says that imagine that this is your two factors, latent factors not 200 just two factors and I believe they use things like let me just say originally I read this paper I'm looking at this paper after almost 10 years actually what particular thing they use I don't have my reading glasses before me. Serious versus escapist and geared towards male versus geared towards female. Okay, let's go with this. This is a male light and this is female light. I don't know, so many years later in this world of diversity and inclusiveness, whether this example would be liked anymore, but it's the original example. And this is serious and this is escapist or hilarious. So let's try to cook up some examples. Actually, this guy, this paper makes some examples. Let me see what examples it has. Again, I don't have my reading glasses, so let's just stretch it out. Ah, it says a movie called Braveheart. Braveheart is both serious and male-dominated. And Escapist, somewhat Escapist is a movie like The Princess Diaries. Has anybody seen Princess Diaries? I'll ask my daughter what they think of it. I hope this is correct. Then here would be Sense and Sensibility, is jane austen say ability ability and then a dumb and there's a movie here a dumb and dumber which i remember seeing which was pretty good and um number and independence day so so forth so in other words guys when you look at a scale of male versus female and a serious versus just escapist, I suppose a documentary would be somewhere here like that documentary. Right. And I don't know. In recently, I noticed that some some kids from my neighborhood gathered together and they were watching an Indian movie called Dhoom. Would you agree that it is somewhere here? It is sort of a unrealistic, fun-loving movie. Well, anyway, so this is it, or maybe it is somewhere here, whatever, somewhere it is. So you can place, you would agree, all the movies at some place in this page, in this coordinate system. Now, there are only two coordinates we put, but they are actually, imagine that there are lots more coordinates. So in this space, this latent space, or more practically on this writing board at this moment, two-dimensional writing board, every movie can be projected down because it will have some degree of bias towards male or female liking and a certain degree of seriousness versus escapism on that spectrum it will fall somewhere so you can put a point for a movie now what can you do for a user so when you put a point for a movie you see that the movies have those traits this movie uh d Dhoom, has a trait of, let's say, 70% maleness. I'm just picking it up. I have no idea how much it has. The Princess Diary has a very high female likeness factor. And then it is like, I don't know, the brave heart is supposed to be a very serious movie, a high seriousness factor and high male-like factor. So you can give these two values to these factors and therefore coordinates, you get the coordinates. So items have traits, but what do human beings have? What do users have? They have certain affinity to the trait. By the way, I tend to use a different example, which to me I suppose is more intuitive. I like to cook, I like to enjoy cooking. So when I look into the kitchen, especially the Indian kitchen, you find a lot of spices. You find these ingredients and the spices. So let me just take spices. How much of chili you put into the food and how much of whether it is trending towards sugar or towards salt or how much sugar you put. Think of these two axes. Now, if you were to imagine that suppose you have 10 spices, right? Then, or 10 ingredients, like white spices, take all the ingredients, also rice and lentils and so forth, 10 ingredients. And you make lots and lots of dishes. There's an infinite combination of dishes you can make from the 10 ingredients by doing different combinations of these ingredients, right? And so you cook up a cuisine out of that. Now, every user will have a certain specific affinity for that cuisine. And that whether the user will like that cuisine or not, at the end of the day, you can sort of hypothesize that you can decompose that like, based on their liking for certain traits, their liking for sweet versus spicy versus things like that. You can do that, you can think in terms of that. do that you can think in terms of that and because you can think in terms of that you can say that cuisines have traits and users have certain liking for this underlying invisible traits like how sweet it is or how you know spice it is and you know spice it is and so on and so forth so things like that that is the basic idea so in the case of movies you can project that each movie has a certain a user let us say that this user is there the user likes silly movies like you know when you want to have a relaxation you want to just get a few laps um cool down and go off to bed, right? Let's say. So you are here. And so you would gravitate, you would agree to movies in your vicinity, right? And that's the fundamental idea of the recommender of the singular value decomposition is the mathematical technique from linear algebra that basically helps you do this you have a latent space of l dimensions and what it does and the typical dimensions is 200 and so on and so forth and what you what you do is you represent each user each user as a row matrix of l dimensions of l dimension each of those values here how much so suppose this is a dimension I a dimension kth dimension it basically says how much does the user I have a liking for the kth trait are we together kth trait here how we together? The kth trait here. How much does the user like this trait? Likewise, every item, j, can also be represented by exactly l-dimensional. How much of traits does it have? So suppose you have the kth. How much of this, for example, if it stands for spiciness, how spicy is this cuisine will be measured by this value here this number the scalar here right and therefore what you can say is that roughly that it is the inner product of this when i multiply this with this or in more mathematical languages when I do xi yj in a product dot product when I do this dot product I will then know how much it should give me the rating now let's think about rating if it is explicit rating of course you get it on a scale of one to five but when it is implicit you know that the user either has bought it or not bought it. But we don't do that. We give it a probability from zero to one, right? Probability from zero to one, which is again a number. So it is okay. Rating is just the probability, implicit implicit probability of buying, buying, streaming, reading, et cetera. So we do this. And now, having done this, how do you so now comes the machine learning part. We know that we are going to do this. This is a linear algebra part. How would we reduce it to machine learning? It's actually very straightforward. We are taking the dot product. We know that if I do transpose, because this is a row vector, this is a row vector, you have to take the transpose to make it a row versus column, so you can multiply the two properly in matrix notation. So this gives you the dot product of these two. Now, if you're talking of explicit rating, this is okay. What are you comparing it to? You're comparing it to the data, to the actual visible or the evidence. The evidence is you have some, for some item I and J, user I has bought item J and put a rating or just bought it. If it is just explicit, then it is okay. You compare these two, this will be number, this side will be number, and this is of course, this is number and this will of course produce a number. On the other hand, if what you're looking for is between zero and one, like you want to make it into either the user buys or not, so the values are zero or one, these two. So then of course, what do you do? Now we remember in classification theory how do we convert this into a classification problem how do we do it for example in larger so this is your logistic regression expression right uh minus z so we we just apply that or let me just put a sigmoid function there because this is literally what it is you do a sigmoid and you're comparing these two and what are you doing this therefore square of this and obviously with the appropriate summation over i j this is your loss function right this is your sums mse the sum squared loss are we together this is your sum squared loss part to this loss you must add the standard regularization term and i will just mention it to you without further ado this is your sigma i j x i square plus this is actually minus typically you do minus because you want this to compete with this regularization yj i hope i'm getting the math right i'm just doing it from memory at this moment but in case i've made a mistake i'll tell you if a correction is required okay so this is it you just go and add the standard regular this regularization guys you must be remembering this is your standard what is this this is your lasso this is not your right so sorry this is your ridge regularization if you remember the regularization theory this is your rich regularization so this is your basic loss function. Now, when you solve this loss function, you can solve it by whatever means necessary, right? So long as you get. So in other words, your algorithm is to do, so because this is your total loss, you want to find that value of xi argmin, xi yj, that will do the arg argument of this complete expression, isn't it? The sum squared loss with regularization. Now this from ridge regression should be extremely familiar to you. The only difference that we have is we have derived it out of the interaction matrix. So the moment you have this insight, and it's amazing that you know this insight took a long time to develop and the people who developed this insight literally were given the million dollar Netflix prize and this moment was considered a seminal movement actually in the recommender system. Properly we say that it really took off from this mathematical expression and from this particular breakthrough. So the way you do that is it's a tough problem to solve. So what you do is you do it in two phases. First, what you do is you keep the y's value constant. So when you do the gradient descent, and I hope you guys remember the gradient descent learning learn the gradient descent learning step what is the gradient descent learning step the value next right the so theta next is the current value of theta the better minus alpha gradient of the loss right do you remember this expression guys this is your fundamental loss function so obviously you don't do loss just like that. You will of course do mini batch loss, mini batch, right? You may do a full gradient descent. You may do stochastic gradient descent. So for example, if you look at Spark on a big data scale, it takes the approach basically of doing stochastic gradient descent. So I won't go into the trade-offs of mini-batch stochastic and full-batch gradient descent. Those are things that we have covered in extensive detail in the past. But you do the gradient descent. So what you do with the gradient descent is you do it in alternating cycles. First, keep yi's fixed. First, keep yi's fixed. And update only the update yj's fixed. Update xi to better value. Do a gradient descent on the xi. The next step, first, next. keep xi fixed, update yj to better values, right? And then what happens is that if you just look at this expression, it pretty much degenerates to the regularized regression that you're familiar with. And you keep alternating between these two, alternate. Once one cycle, you update the XIs, keeping Y fixed. The second cycle, you update the Ys, keeping XI fixed. So gradually the taste, you know, the values begin to get better and better. The traits begin to fall in place of an item correctly. And the user's taste begins to be more and more realistic that is why this algorithm famously was called alternating least squares and for example one of the popular frameworks uh for doing in big data space was is spark right so spark's first implementation, the moment they got their stochastic gradient descent going in place, they implemented this Netflix prize in the alternating least square methods. And even today, that is the dominant approach for matrix factorization based recommendation system in Spark and very effective, it works, it gives you a pretty good result and that's it. So today I'll end with that guys. Next time we go into, we'll keep going deeper. I'll give you a preview of what we'll do. We'll do various approaches using deep neural network. The deep neural network work is rich and vast. You have the traditional neural networks, feed forwards, you have then we'll do it with autoencoders, with generative adversarial networks, then we'll do graph neural networks, you know, how to create a graph of this, because users and items, you know, you have edges between them and you can do graph neural networks and then you can do sequence models, time evolution models, and then you can do some of the more cutting edge research geometric, for example, you can embed them into some sort of interesting manifolds, interesting geometries, and then try to predict things. It's a pretty long journey. Let's see how many sessions we feel like doing or how many sessions you folks are interested in. But anyway, thank you for coming to this session and I hope you all had fun. I'll open it up to questions.   