 We stopped before the session break at this equation, at this quantity that we need to maximize. It is the likelihood, We need to maximize the likelihood. That is why it is called maximum likelihood estimation. The process that we'll follow at that model will favor which maximizes the likelihood of this hypothesis, MLE. That's where the word MLE comes from. And what I alluded to you when we were talking about the prior probabilities, et cetera, is that there is a, I wouldn't talk about this in this course, but there is such a thing called maximum. A post to your. or stay real free probably. map it's a it's what thees think about quite a bit, but we won't cover it in this course because we are at this moment focusing on the classifier. So we'll gradually come to it. The Bayesian you're looking at the the maximizing this let us now have a probability right or think about it if you look at this you notice that these probabilities go between zero to one. All the probabilities go like that. We are trying to maximize likelihood. Likelihood is what? Now, if I write it in scientific notation, or not scientific, mathematical, in a more succinct notation, is a probability of all the points that is probability where i are belonging to all the points that are positive, positive, like a cherries, probability of xi times the product of, if you really look at it, j, all the negative test cases, where it was, that point happened to be a blueberry, right? Cherry and a blueberry, blueberry, right? Probability. And there we were looking at it not being a, if you're right, then it is not a cherry, right? Because it's a blueberry, XJ. So it is this because it's a blueberry xj so it is this it's the product of this right if you really let's go back and see if it makes sense see what are we doing we are multiplying the probabilities of cherries these are cherries positive cases and we are multiplying it with the negative cases. Does this make sense, guys? Negative cases where we don't want it to be cherry, where the ground truth says it's not a cherry, it's a blueberry, we look at one minus the probability of cherry, right? In other words, we want the probability of it not being a cherry. So this is a likelihood function, likelihood beta function. We want to maximize this we want to maximize l beta oh all the the different data data points i is equal to one two let's say all positive cases all positive all negative all negative cases are we together that is all it is so we want to maximize this now it turns out that most of the optimizers things that do gradient descent now Now from here, let's connect the dot. Gradient descent like to go to the minima, isn't it? But here we are trying to maximize something. So how can we go from maximizing something to minimizing something? Just take the negative of it, right? So suppose you take minus log beta so that is one step one one so suppose you take this as your likelihood function and now i will write my likelihood loss function loss is this minus log let's say if loss could be so this is negative and now i have a product i could uh now i have a negative so i can minimize it isn't it so that is one step forward there is one more trick that people use see dealing with products what happens is that it's a computational thing. When you have very small numbers and you have a lot of data set, you have any kind of number between 0.1, zero and one, if you keep on multiplying, what will happen ultimately is you'll have underflow. Basically, you'll have a numerical problem, computational problem. So this is not mathematical. This is the nature of machines problem. Even if you take 0.9 and you multiply it to the power 30, let's say that you have 30 data sets, you will soon realize that you're pretty much screwed right this will be approximately zero and your computers they can't deal with numbers which become too small right so there is a trick so now just to solve the numerical issue and, it turns out for other reasons, there are some other aspects also. You realize one thing that the log, if you take the log, log converts products into additions. Do you remember that if you multiply some things a times b if if y is equal to a times b log y will be log a plus log b make sense guys guys am i am i talking simple things isn't it so well this is that if we do that so actually so you take the loss function as could be minus log of l the likelihood function so what is it? It is the negative log likelihood. And that is the technical term you'll often see referred to that the last function for classifier says negative log likelihood, right? Because what negative log likelihood will do, let's say in our cases, it will do this. It will say for positive cases, minus log probability of X1 plus, sorry, this minus is outside here. So I'll keep the minus outside here. Plus log probability of X2, right? Plus log. Suppose these are positive cases, X3. And say there is a negative case. For negative case, what do we do? So positive cases. And negatives, zero. not a not a cherry negative cases you do log p x let's say i will put a i will put a tilde here, tilde 1 plus log. Oh, sorry, 1 minus log. 1 minus. Exactly, 1 minus. Remember, for negative cases, we take 1 minus, oh, sorry, 1 minus probability of tilde x2 and so forth. So far, so good, right? So far. Now, you notice that, so this is it. This is our loss function. The loss function, broken it up, loss function, loss, which is a function of beta, the total loss function given the data is this, negative log of probabilities added. Now there is a mathematical trick, a mathematical notation. So remember, why did we take the log? Okay, is it justified? So you may ask, is it justified to take the log? Are we justified in doing that? And the answer to that turns out, the answer of this lies in one interesting fact. See, what does the log function look like? How does log go log of 1 is 0 it rises up like this isn't it and it goes to what is log of 0 minus infinity right it keeps going down. But so obviously I'm not drawing it correct. But notice that it's a monotonically increasing function. As x increases, it also increases. So taking the log is okay because it is not a weird function. If probability is high, log of probability will also be higher. Isn't it? When you're comparing two probabilities, instead of comparing them, you can compare the logs. That is it. Log is increasing function so it is okay or okay or justified in other words if if given If given Px1, Px2, if Px1 is greater than Px2, then log Px1 will also be greater than log p x2 right so all your comparisons still hold so now we come to the conclusion that loss once again to write it out i will write it out here loss we realized is negative of now can i write it in a slightly more sustained notation instead of saying log x1 x2 x3 i will write it and tell me if i'm right okay maybe i'll write it this way again x1 plus log x2 plus log x3 all the positive cases plus log 1 minus these are the positives. One, right? One minus probability X tilde one plus log one minus probability X tilde two, all the negative cases. That's what we wrote a little while ago. Now I will write it, this is a rather tedious ago. Now I will write it. This is a rather tedious notation, so we'll write it in a more succinct notation minus sum over a positive cases. plus summation over j sum over negative cases log 1 minus p x j. Would you agree that these two sentences, these two lines say exactly the same thing? It's just a notation for it, guys. I hope it looks simple. Yeah. So now comes one little mathematical trick, right? Do you realize that would this equation be affected if see here this is where the ground truth y is equal to one right the ground truth is it's a cherry y is equal to one and here the ground truth is y is equal to zero isn't it and so we can write this equation with a and this is a mathematical trick that people use it has no I mean there's nothing magical about it we can write this equation with a, and this is a mathematical trick that people use. It has no, I mean, there's nothing magical about it. You can write it as i for positive cases as yi, because yi is 1, log p xi for positive cases, positives, plus 1 minus yi. Because if y is 0, 1 minus 0 is 1, log px, let me say yj, pxj. Of course, this is your y hat of course this is your y hat probability is your y hat right it's your prediction isn't it are we together guys y hat is your prediction of prediction at the point x and so this is also written as minus sum over i, yi log yi hat plus j, 1 minus yj for negative cases, log 1 minus yj. Great. This is your loss. There's a word for it. It is called cross entropy loss. Now, loss was fine where did this big big mathematical and scary sounding word cross entropy come from it turns out that suppose you have see here we have two probabilities right yi is one it is the probability that it is a cherry guaranteed it's the ground truth it is one whenever you have two probabilities so cross entropy whenever you compare two probabilities let's say a probability p and probability q then one easy way to compare these two probabilities is, a mathematical way, is to say p log q. It will tell you how much the two differ, sort of. So, for example, if p is 1, it is a cherry, and q, which is the prediction, is also 1. It says, it is surely a cherry log of one is zero so the cross entropy will be zero means there is no difference between the two they are in total agreement cross entropy is a measure of disagreement or differences between two probabilities yes between two probabilities. Yes. So in the last in line variation of the loss question, why is the yi and yi ? Oh, I'm just saying that I'm keeping the i notation for the positive cases and the j notation for the negative cases. So why were the yi ? Oh, it's just a mathematical convenience because people realize that if you write it here, see, first of all, if you write it, your your whole explanation doesn't change. But the advantage of writing it this way is now you can give it an interpretation. The interpretation is it's a cross entropy. You can impose that interpretation so you can connect the dot to another area of mathematics. And that's the beauty of it. And that is why this is called the famous cross entropy loss. It is by default, the most, the default loss function that people use in classifiers is cross entropy. So guys, this is the real explanation of why in classifiers, Raja Ayyanar?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati? will start going from minus infinity to plus infinity, which is true. But that also is true. That's sort of a motivating example. But the more strict example is this. Now you may ask, well, if you're looking at likelihood, why didn't we use the likelihood argument for regression also? Right. So I would like to take a very small digression to show you that sum squared error, actually, when you apply it to regression, the likelihood function, the loss function, automatically becomes the sum squared error. In fact, that's magical, isn't it? So, in fact, Gauss, who, of course, created the Gaussian distribution, one of his, Gauss, who of course created the Gaussian distribution, one of his, you know, one of the theorems I said that the best unbiased, you know, blue theorem, best linear unbiased estimator, pretty much the main argument comes from this fact. So I'll show you. Suppose you look at the loss, right? Let us revisit, well we must revisit regression in another colour. Let's try which colour should we take. How about this colour? Let's draw a line, changing topics. Applying MLE to regression. If you apply MLU to regression, so how does it do that? There, remember that suppose you have a point, you have a line. And so suppose you make a hypothesis. Your hypothesis line goes like this. Right? You know that you will have your residuals. What do you want your residuals to do? For a good hypothesis, your residual epsilon. So what happens is y minus y hat is the residual, right? I. For a good model, residuals should be what? What should be the distribution? They should be close to zero, isn't it? For a good answer, probability of the residuals, right? Or let me just use epsilon because I think I've been using epsilon for the error. So you would want to believe that your errors, they go like this, is proportional to one. So there are some proportionality constants and we won't go over them, two pi, right? See, there is a sigma squared. Forget about this. Epsilon I squared over two sigma squared. This is called the bell curve. Now we have been through a lot of labs. you realize that right? We look at the residual pattern and we keep hoping to see a bell curve of our residuals, isn't it? Of our errors, residual errors. Do you remember that guys? We have been doing that. Yes. Right, we have been doing that. So the thing is now you can argue that i want that hypothesis so the probability or the amount of error the probability of an error of a hypothesis if it is given by this right where epsilon is defined as y i minus y hat then i can ask what is the probability of seeing for a given hypothesis, let's say for this hypothesis, which is quantified by a beta vector, what is the, we are seeing so many residuals. Now the question is, given the data, which is here in this color, what is the likelihood function? The likelihood here is, once again, remember the likelihood function? The likelihood here is once again, remember the likelihood function is given this beta is, well, there are no negative cases. It's just the probability of the residuals, isn't it? So given the probability of the residuals, all we are asking is once again, this is probability of residual for epsilon 1, probability of residual for the second data point, probability of residual for the third data point. And suppose we have n data points. This is it, isn't it? We want to maximize, we want to find that hypothesis, that line says that the likelihood of this joint probability is maximum would you agree the same argument applies right give me that hypothesis such that and you know that that hypothesis will be good because a bell curve has a nature such that it is zero peaks at zero we want right we want all the errors to be as small as possible namely their probabilities to be maximum isn't it is all of us getting this when each of the points is having small residuals their errors will be small because small residues is small zero means higher probabilities right and we are looking at this maximizing this probability now what is this probability let's look at this this is equal to in other words this is the product over all the data points of probability of epsilon i very simple actually for regression it's much simpler you just can i write it in this notation, guys, so that I don't have to write P all the time, this? And what is this? Let's break it up into individual cases. It is the product of I, 1 over 2 pi sigma squared, for each of the time, and E to the minus epsilon I squared over 2 sigma squared right now can i can i just for simplicity erase the sigma out of the equation for the time being okay maybe i can leave it but you can take sigma is equal to one or something like that but okay we'll leave it here so what is this equal to this is to the n it is nothing but 1 over 2 pi sigma square to the power n, which we can ignore, right? It's just a proportionality constant, times e to the minus, oh, what happens when you multiply e to the minus e1 square over 2 sigma square e to the minus epsilon 2 squared 2 sigma squared n squared 2 sigma squared. And I wish I could just set sigmas to half. So, you know, like I told you, like my professor used to say, forget constants. The argument can be made neatly, erasing the constant. But let me keep it there. So some proportionality constant here. Now, what happens when you multiply exponentials of something? They add up, right? So this is the same as e to the minus epsilon 1 2 sigma squared plus epsilon 2 squared by 2 sigma squared epsilon n squared 2 sigma squared. Isn't it? And what in the world is that? Now let's look at this. It is equal to the same thing e to the minus. And would you agree? 1 over 2 sigma squared, sum over, ah, now the whole thing is beginning to sink, isn't it? Do you see? Somehow in the exponential, our sum squared residuals came about. Do you see that? In the exponential, now we see the sum squared residuals come about. So we seem to be heading somewhere. Isn't it? Now let's take the negative log likelihood of that. Again, remember negative log likelihood. Negative log likelihood would be minus so if this is l beta log of l beta log of l beta which would be minus log of this quantity minus now what e to the minus all of this is what the same thing right e to the minus 1 over 2 sigma squared now this is what we are trying to minimize isn't it and minus of minus remember minus of minus where is this minus coming from from here and where is this minus coming from from the minus of the log likelihood isn't it and so now do you notice what this looks like it is just some constant minus log of minus log of something is such a thing as a plus log of 2 pi sigma squared to the n, right, which is n, actually, n times number of data points times this. This is a constant plus up to a proportionality constant, 1 over 2 sigma squared, nothing but epsilon i, which is ignoring this term, it is n log 2 pi sigma squared plus 1 over 2 sigma squared y i minus y i hat squared. Do you see the beauty of it. And what is this? Some squared error. Isn't it? You see the beauty, right? And now you know why. Remember one of the questions was, why should we not consider fourth power, fifth power and so forth, right? So if you believe that the error should be normally distributed, the best way to do it is square it. And that's the explanation for it. It comes from the negative log likelihood function. So this is it. I thought I'll give this as a sort of a point for you. I still don't have a negative. Which negative? I get the negative log likelihood depending on the right side of the equation yeah so so the negative the minus on the right side would be one over two sigma squared times summation of x1 i squared you mean are we stuck at this line or this line this line yeah see okay see this is exponential right log of exponent just erases out the e so but there was always a minus remember you have this minus here yeah so minus of the minus will make it plus that all. And then on the proportionality constant, why is that not a... Oh, because if you notice, it was 1 over 2 pi sigma squared to the power n. So log of 1 over x is minus log x. So again, a minus and a minus will cancel out. That's it. So basically, you realize that if you look at the loss function, this is a constant, you can ignore it, there is no way you can gradient descent on this, right nothing but this term. Now there is actually more to this argument. There is an interesting way in which we can actually bring about regularization into the whole framework. If you look at it from a Bayesian perspective, it turns out that spills out. But we won't get there. Let's limit it to this. But see, I will summarize it with this, guys. This ends a big topic. We learned about a powerful concept called negative log likelihood. Negative log likelihood is the source of this loss functions, both for regression and classification. For regression, it leads to some squared error. If you assume a Gaussian error, if you take any other error function, for example, a Laplace error function, et cetera, you'll see something else. And in the case of classifiers, it naturally leads us to the cross entropy loss, isn't it? Both of them spill out from a very simple argument and it also explains why for classification your textbooks tend to use that cross entropy loss. The reasoning, i hope you found it very simple if you can understand that between ram and sham arguing over rainfall in seattle is the same argument that that you're carrying forward and you come up to the cross entropy loss right are we drawing a parallel between residual and distance from the decision boundary? Is this comparable? See, distance from the decision boundary is very specific to linear classifiers, but this negative log likelihood concept is true for all sorts of very, very deep and highly nonlinear neural networks. It's still true. It's a more universal concept. Whereas distance from decision boundary is globally, it makes sense when you have a linear classifier, but it also makes sense when you take local maps, local maps of the situation. Like locally, any decision boundary, however curved, will look linear. So in the neighborhood, you can apply the distance argument, but it doesn't generalize to nonlinear situations. So it will apply to logistic regression, et cetera. It will apply, for example, to support vector machines and maximal margin hyperplanes and so on and so forth. But it doesn't generalize. But cross-entropy loss is much more general for classifiers. And it is the mother of other loss functions if you use something else. You adapt from here. How does this loss function look geometrically? How does it look in the case of classifier? Yes, it's quite interesting actually. This loss function is steeper. Let's put it this way. The learning rates are faster. I think we'll have to visualize it by the way here i'm just speaking from memory huh we can draw it out actually why don't we just guess and draw it out why guess so when let's let's look at the situation where y is equal to 1 right y is equal to 1 1 and we will take this with which color? Let's take it with big blue color. If y is equal to 1, what is log minus log p, right? So this is, let's say y. What is the log of y hat? Because we are looking at the log of, I mean, y hat is the log, right? Y hat, log y hat. When y is equal to one, the last function becomes just this. And let's take one data point and see what it is, just for one data point. And we can reconstruct the intuition. So we realize that when it is we agree let's take some points that we sure of when y hat is equal to 1 we agree that this is equal to 0 loss is equal to one then how much will the loss be log of one is zero kyle yes yes zero right on the other hand now let's look at this at y y hat being zero, what is log? Log of 0 is asymptotically minus infinity, isn't it? And minus of minus infinity is plus infinity. Plus infinity. So it means plus infinity. Means it has to come from here. Also, we know that log is a monotonic function. It's a monotonically increasing function. So minus log will be a monotonically decreasing function. And we can check that at value half. So let's make a few half-way stations. And I'll let you make that 0.25, 0.5, 0.75. Would one of you try to tell me what is the negative log of 0.25? Any one of you on your calculator? Just tell me what is log of my negative 0.25. Let's do it. what is it minus 2 into no no 2 log 2 means i mean yeah 0.25 is a physical 100. Yeah. Log of... No, guys, just do it on the Python. Log of 0.75, the negative is going to be 0.125. 0.125. Okay, so this is here, very small. What is log of half? Best, base E. is here very small what is log of half best base e approximately one little less than little more than one minus log minus log of 0.5 0.69 0.69 0.69 so let's just take 0.69 to be here i will take this to be one unit one unit because this is it actually geometrically let me uh do if this distance is one unit this distance is one unit so did how much did you say 0.69 huh 0.9 is approximately here and how much is uh approximately here. And how much is a... And- 1.3. 1.386. 1.39, double of this, right? So what do you notice guys? This function goes like this, isn't it? In fact, if you plot it out, because I knew it, the logs look like this. In fact, if I have to do it a little bit more, exaggerate the shape so that it doesn't look like a straight line, I would say that the shape is rising up like this to infinity. This is the log function. infinity right this is the log function if you did now one one question that people often ask is is this the reason that see this is very steep what does that mean the steeper the loss the faster the gradient descent isn't it you learn quickly from bad mistakes. Because big mistakes contribute a lot to the loss function. It's very steep. You notice that it's very steep. Now, let's compare to, on the other hand, if it was. But shouldn't we draw it with respect to the parameters? No, I'm just saying, forget the, I'm saying whatever the value of the parameters no i'm just saying i'm just forget the i'm saying whatever the value of the parameters is is going to predict a y hat right so i'm predicting only with respect to y hat y hat can take values between zero and one given the parameters i'll make some level of mistake what i'm seeing is when you make big mistake the loss is much bigger from big mistakes very big suppose i took y hat then what would happen what is the when this is y is y hat suppose Suppose y is equal to 1. And you predict 1. What is the 1 minus y minus y hat squared? Let's look at this. It is the same as 1 minus y hat squared. Let's try to work it out. At this point, this also is 0. Now, what about y is equal to zero y hat is equal to zero when the prediction is it is zero then what is this one so it will be from here and it is a quadratic equation right so this quadratic equation will be quadratic equation will be like this, isn't it? And the point here is that which is steeper? The log function. The log function, which is why it's one of the motivating ways. See, there are many, many ways to look at it. But generally, this works in your favor because a very steep loss function means that you will learn quicker. Right. So yet another way to look at it. Does that answer your question, Kaya? Yeah, I need to go through this again. You see that, whatever your y hat, if your y hat is 0, then remember I'm making an axis of y hat versus loss. Now y hat is a consequence of the parameters changing, that is true. right but irrespective of what whatever like you notice that losses losses are much higher for big mistakes right and the same is true when y is equal to zero when y is equal to zero y hat what will happen is it will go the other way around the same loss function let me put it another color pink color it will go like this so in this case we are assuming y is one and yeah yeah okay okay i get it for y is equal remember this entire analysis was for y is equal to one maybe i should write it down in this y is equal to one ground truth is equal to one and in the pink ground truth is y is equal to zero then you realize that the pink is the loss function for y hat yeah yeah right so that is that yeah now i understand yeah Yeah, that's how it makes sense. Most of these things, like these questions when you have, you can literally derive it on the spot and see what is going on. All right, guys. So this is 12.15 and we are finished with loss functions for classifiers, a big topic. Now I would like to move to the various metrics about it. So I would like to first say that review the confusion matrix. Remember, suppose you have cows and ducks. So we're changing topic, guys. Suppose you have cows, cow, cow and duck. This is your ground truth. And let's say your predictions are here. In other words, column has the ground truth, rows are the predictions. So this is cow hat duck hat right and suppose there are 100 cows and 40 of the cows you mark correctly 10 and there are 50 cows sorry and your 10 of the cows you got it wrong and your 10 of the cows, you got it wrong. Ducks, you got 20 ducks, you got confused with cows and 30 ducks, you got it as duck, right? What is this matrix called? This is a confusion matrix. In this confusion matrix, these are the correct answers. Cow is cow and duck is duck. Let us consider, let cow be the positive case. For whatever reason. Then what is the true positives in this prediction engine what is our true positives 40 and 30 yeah let's write it down true positive 40 40 yeah true negative true negative is 10 or what is it 30. not kyle true negative is 10 10 yeah would anybody like to debate that? No. No, 30. Negative case. What is the negative? Negative of a cow is a duck. So truly, a duck is truly marked as a duck. 30. 30. How many false positives are there? So something that's not a cow how but you predict it as a cow false 10. it's 10. why is it 10 it is this uh you know you you you predicted you predicted uh false positive false positive is not 10. it's 20. remember false positive is when it is not a cow you mark it you mark it as a cow that is 20. 20. and how many are false negative 20. And how many are false negative? 10. Means it is a cow, but you mark it as not a cow. Here, isn't it? A true negative is here and true positive is here. False positive is 10. This is it. So from this, we will derive a few uh facts first thing and I'll just review two measures accuracy and error rate they just converses of each other and before lunch we'll just realize its limitations accuracy is what how many did we get right proportion of right How many did we get right? Proportion of right? 20 percent. 40 plus 30 over 100. Error rate is defined as by definition is one minus accuracy. That is how many mistakes did we make? 30 over 100, 30%. 30%, right? So 20 plus 10, the off diagonals. So anything not in the diagonal is mistake of some kind, right? 20 plus 10, it's a mistake. So this is our error rate. Now you would say accuracy. People often, people talk a lot about accuracy. So I'll take a little bit of time talking about this whole business of accuracy, right? So many, many times you will say this device has accuracy of this much. Or this algorithm has accuracy of this much. If you go and look on the web, so many people will do a data analysis and the metric they'll quote is accuracy. So accurate. The question is, is accuracy the correct measure? I'd like to bring it out with a story. Not a story. Okay, let's say a story. Let us say that one fine morning, I start fancying myself as a doctor. I go to some town or some village and I fancy that I'm the doctor. A quack of course, but nobody knows that I'm a quack. So the moment somebody comes to me for any illness, I always give them some ash or some liquid, nice syrupy thing, and I say, go drink this, you'll be cured. And let us say it's something very basic. It's fever. I only treat basic things like fever, simplified. And the fever goes away, right, after a little while. right after a little while now people the thing is my diagnosis they ask me this question am i going to die i have only one answer no no you're not going to die right so i pump him pump the person up with great optimism and confidence very confidently i say say, my dear boy, you'll be just fine. I'll take care of you. And sure enough, most people get cured because simple illnesses like cough and cold and fever, they are self-limiting. They go away on their own. People don't die of these illnesses easily. So my prediction that this person will be okay, suppose only one out of thousand has a serious disease that is not self-curated, that is not self-limiting and will fix itself. How accurate is my prediction? How accurately? how many people have i cured out of a thousand people 999 999 my my cure rate is 99.9 percent amazingly high say that kid amazingly high you're a genius yes right I'm a genius. I don't have to take medical degrees or anything. I might begin to doubt why people throw money at medicine, learning medicine. 99.9% positive rate. But do you think I'm an effective doctor? If you knew that's what I'm doing, would you come to me? You should not come to me, isn't it? Because where it really matters, in the one case who is really seriously ill, it can be catastrophic. I will keep hoping for the person to self-cure and the person can go downhill very rapidly and die. So that brings out the danger of accuracy. Accuracy is that predictions are often right when the data is highly imbalanced. So if the data is very imbalanced, if one in 1,000 actually have the kind of illness that is scary. So one easy way to tell is, will I die? The answer is simply no. Tell everybody no. And you would have extremely high accuracy in your prediction. That is the problem. In fact, we look back now at the history of medicine and see most of modern medicine started roughly, very broadly, after the invention of penicillin. Before penicillin, we didn't have really this whole large gamut of modern medicines. So what exactly were those people doing in the 10th century, 11th century, etc? They were doctors. They were pretty famous doctors. Everybody trusted them. They went to them for cure and those doctors were actually curing people. They believed that they are curing people. There were medical schools in which they were going and learning from other great doctors and professors. So what was really happening? They had some theories and some medicines. And when they would give the medicines, the guy would be cured. Right. And so they would write in their notebook that yes, yes, it's cured. Do you see that multiple things that I've taught, we have been talking in this class was in effect? Can you tell some of the problem? One of them was a confirmation bias. You already have a preconceived notion that this syrup will cure you of this problem. You give that syrup, the guy gets cured, you say, okay, that confirms it. The other problem is this whole sequence thing. Just because the guy became cured after you give the medicine does not imply that there is a causal relationship. Sequence does not imply causality. Just because I came to Silicon Valley and then the dot-com boom happened, I didn't cause it. It is as absurd as that. Just because you gave that medicine and the guy got cured is not evidence that the medicine cured the person. The only way to find it out is to do a double-blind study in which you give some people, don't give that people that medicine, you give a placebo. And to other people, you give the medicine and see what happens right you have to create evidence in favor of it and more practically you have to go to the scientific foundations and find the causal reasoning the set of chemical reactions that explains whether it is curing or not curing what it is doing in the biomolecular level you have to figure it out. So that is what. But at the same time, why did the patients believe that this doctor is good? Well, most of the time they would go to the doctor, they would have self-problems that would anyway cure themselves. You get a nice painkiller, you get some good vitalizers, and then you begin to feel better and you feel it's all the doctor's medicine. Today we know that most of those so-called patented medicines and secret mixtures and whatnot, potions and whatnot of the medieval ages, they were all junk. None of them actually work. But they work because the human body is miraculous. It cures itself of most problems sort of goes to the heart of it anyway i thought i would end and break out for a lunch break on this anecdote okay something to think over that accuracy doesn't help after lunch we will look at other measures if accuracy helps it also depends on from whose point of view you are looking at. From doctor's point of view, it is good. Oh yes, of course.