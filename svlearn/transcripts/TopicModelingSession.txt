 Give me a moment guys. So, we start, so let me give you a general idea of the code. In the first part, I'll just walk through the basic structure. This should be the familiar set of inputs. The only thing you have to change is wherever you install the code base, you know, download the starter code, change this one line so that it refers to that. Then, rest of these are imports. We are importing gen sim, we are importing spaCy, pandas, sql, actually umap, I didn't come around to doing it, but I'll show you beautiful visualizations next time with this. Matplotlib and so on and so forth warnings so now let's look at this we load the data it turns the reason i used sk learn scikit-learn is because scikit-learn contains this 20 news groups data uh built in right in its data sets listing so it's very easy to just go get it. Now, what are the 20 news groups? As I said, it is the emails, about 11,000 odd emails, spanning about 20 different news groups with 20 different topics. So which topics are as follows? You see those topics here. Now, here is sort of a homework I'll give you guys. I am spanning 20 topics. What I would suggest is pick three topics that you look fairly orthogonal. For example, you can take sports, one of the sports or two of the sports topics, and then take science, let's say space or something like that, and take one from, I don't know, politics or something like that, and just take three of the topics. You can do that by putting filter, remove, and then there's a filter clause also that it takes, if you look at the documentation of this. And then you can then do the topic modeling. Here, what happens is we already know the topics, right? So you might wonder, if you already know the topics, we know the 20 things it is coming from, why in the world are we doing topic modeling? The idea is in real life, you won't know the topics. You will try to figure out some good topics in the data. In this particular case, it happens to know that. And so what we'll do as we do the topic modeling, we'll try to see how well the topic modeling algorithms are able to sort of surface or figure out what the real topics were. That's our exercise. So take that. As you can see, it's more or less uniform distribution. Some topics are less. The talk religion mask seems to be rather less and atheism seems to be less, but not by much. We will do the first approximation, consider it uniformly distributed. So what I did is you take this data and you just put it in, and again, I'll go a little bit fast because then we'll come back and do it slowly. I just put it in a Pandas data frame to see what it looks like. When you look at this data here, let me sort of increase the font a little bit so you guys can read this. If you look at this, do you notice that it contains a lot of strange characters, I mean, a lot of things which are not really English, but sort of are other things. Their article from a certain article by a certain person and so on and so forth. So there is a lot of end line, there's that characters here. So one of the first things you do is, and then also you notice that if I look at the topics, there's a lot of overlap, like politics, maybe you don't want to distinguish between all the different categories of politics and religion, all the types of religious discussions, talk and so forth. So what I'm doing is to simplify, I'm breaking it up into just six groups. And I would suggest you can do it with smaller number of groups but even before we do that uh the so here's the thing what you do is you have 20 news groups here i'm reducing it to politics religion sports computer science and for sale and here's a hint i'll give you actually one simple exercise you can do is you can merge computer and science. They are both STEM field. So you could merge the two. And perhaps you could merge religion and politics. They, despite all the effort to keep them separate, they all tend to overlap with each other. So you could do that. So you will get fewer number of topics. And for sale, you can add into uh i think in this case it's uh equipment or cars or so forth you can merge it with sports so that so that you have three good topics and you can try to do topic modeling right so it's just a basic idea so what i'm doing is for each of the row i'm adding a new column called topic. And when I do that, you notice that if you look at the columns here, the rightmost columns, the target is what came with the data. And I'm taking another topic, simplified topic, which is now sports, computers and whatnot. So I'm just augmenting the data with info, so some topics that. So now when you have a data and you try to do any linguistic analysis or so forth or any kind of a semantic understanding, you know that some things don't contribute to a semantic understanding, the semantic structure or the semantics of the data or the meaning of the data. For example, the basic stop words, the, has, this, that, the very common words, you don't want to keep them. So every single library that you can think of in NLTK, sorry, in natural language processing, whether it is spaCy or NLTK or whatever it is, they all give you a list of stop words. And quite often the stop words are either derived from or directly the famous Stanford WordNet's list of stop words. So we will remove the stop words. Likewise, we'll remove punctuations. Now, whether we should remove punctuation or not, it is a little bit of a thing. I decided that in this exercise we could remove the punctuations. Does email contribute to semantic meaning? Like, for example, when you look at this, is it going to contribute in any way to that? It is not going to. so we are going to remove emails remove prop actually i removed that i kept the proper nouns because then i thought it's cozy and things like that and quite often they are proper nouns that are worth keeping so i should change this remove this from your mind limitization of the word which would one of you remember what lemmatization is? Getting to the root word. Getting to the root word. Now, somewhat conspicuous by its absence is stemming. I am not using stemming. I tend to use it less often. I prefer lemmatization for many more situations. But you know, actually, it's more laziness. You should try both and see which helps you more. Now, so another thing is, let us go and clean the text. Before we clean the text, we will use the language model, NLP's's english language model and you remember which model are we talking about we are talking about the english core web large if you remember i'd suggested always use the large one i'll continue on so once you have done that i'll explain to you what a clean text looks like i was wondering if anyone out there could enlighten me on this car, blah, blah, blah. Do you notice this? This thing, right? Now, what does that do if I try to clean this text? Now, by the way, I've given a clean text, little utility, very simple function to clean the text. Let's go and see what the clean text does. It is extremely simple, actually. It is, again, where would you find the NLP libraries? In the NLP SV, NLP directory. So go into utils, and you'll notice that I am doing, I'm basically, and this is something you can play with based on your context. Like spaces don't mean anything. Pronouns don't mean anything. Punctuations don't help much. Symbols don't help much. You know, left and right brackets you may, so you can decide which you want to keep and which you want to remove by commenting or uncommenting and consider it an exercise that you want to do that. Now let's look at what we do. What we do is you're giving a token and you're trying to decide whether to keep it or not and the code is just one line. If the token is in the ignore part of speech list, you know, punctuation and symbol and so forth. It is not worth keeping. It has not much semantic value. Likewise, if the token is like a URL, or it is like an email, or the lemma, when it's lemmatized, the length is less than two. Now be careful with this. Sometimes, in rare cases, it might have meaning. But here, I just said that, OK, if it is just one or two letter word, drop it. And if it is a stop word, again, drop it. That is all. So you see how it is. I drop the not so useful parts of speech, drop common words, drop URLs and emails, and so forth. And you can have more conditions, but to keep it simple, I stayed with these so far. When you do that in that one line of code actually, and this is a power of NLP libraries or NLP itself. It's pretty mature for all the pre-processing tasks. You notice that this thing has become this, right? We could make it even worse. Let us say that i make it this i say are you very sorry and appearance instructor at support vectors.com let's see now what happens i run this do you notice that it has managed to remove all of those things? You could play games, sometimes the email addresses come wrapped up in angular brackets and you could do that and it would still be able to figure it out. It's pretty good at that. And if you read the words that gets left behind, I hope you would agree that these are likely to contribute to the meaning of a text. So now I just illustrated this function, clean text. Now what do we do? In this data frame, we are going to clean up all this text and create a new field called cleaned text. And in the data frame, the newsgroups data frame, I'm just applying clean text to each row. Go ahead. The topics that you assigned earlier to those rows, those are random. If you go up there, you assign those topics. Yes, yes, yes. They're just random assignment. Yeah, it is my choice. I wouldn't call it random, but it is subjective. You could do your own assignment. Am I clear? So I'm not talking about the group here that you picked, you know, the seven topics. I'm talking about the output seven, where you assign those. So that assignment I said, was it random or? Output seven, you mean the last one? The output of the seventh. Output of the seventh. Yeah, topics here. No, no, no, it's not, right? You see, the topics are the ones that we assign, but the target comes from this list in the data itself. Yeah, correct. The target is part of the data. The topic is something that you assign. That I am assigning, yes, very much true. And remember, this is not topic modeling, because here, what we are doing is we already are setting up an answer. And then we will see how good the topic modeling algorithms are in coming close to our answer. So you know what that line, the text is then. You know what you're assigning. Yes, yes. This is my choice. Very subjective assignment. So, so far we haven't gotten to a topic modeling at all. What I'm doing are standard exercises of NLP, which I hope by now you are all very familiar with, right? So, do guys, again, to recapitulate, please do make sure you're reading the green book and the green book the one green book being the spaCy and the other green book being the Manning natural language processing in python right so make sure that you're reading it and then there's the O'Reilly book pick your choice it doesn't matter where you read it from or whether you're reading it straight from the spaCy website and so forth the important thing is to become fluent with at least one natural language processing library for all the classical stuff. And do it. And these are small libraries. I chose spaCy partly because it's high performance and very small library. It's not like vast like NLTK and so forth, where there's so many, many things. You get the learning curve for spaCy, you'll realize it's very short. So what I'm going to do is we have... One quick question before we move on. Line 10, you're importing something called EN. Line 10. No, no, the line is different. Sorry, line 8 in yours. Yes, yes. remember what what is this this is the english language okay but about that it'll be equal to spacey dot load n is that how we do it or was it a longer string no yeah okay for some reason i get an error with that okay i'll figure out. Yeah. You can give the full name if you prefer. You can give it the exact full name. Okay. So let's try that. I think that's what is needed perhaps. Yeah. You can do that and that will also work, right? Yeah, it will also work. So yeah, it's sort of these are variations. Now, let's go in here and see what I'm going to do. I'm again not reached topic modeling. I'm just setting up the system so that you can compare the topic modeling results with this. So if I take the data in which I have clean text as the input and the topic as the output, this becomes a case of supervised learning, classification, right? I can train a basic classifier and see that. Now, to have a basic classifier, all is well. The trouble is the input must be a vector. And so the only vector space representation of document that we have learned so far are the count vector or the tf id of vector right so we'll use the tf id of vectorizer we'll convert each document each document being one row of text into a vector again simple once we do that this is just a illustration of the kind of vectors uh each of this becomes the words become and so forth. So I wouldn't go into that, this is just illustrative. I added it here. Now what do I do? This is basic by now, you must be familiar with. If you want to write a basic classifier, what do you do? You take the data set, split it into test strain, then you take a classifier, you fit it to the data, you make predictions, and then you come up with some prediction metric. So I wanted to do a very quick and dirty result to see what it comes up with. And if you notice, I use a very basic algorithm so that it runs fast. It literally executes in six millisecond, I mean, yeah, here, executes in 37 milliseconds, very fast. It literally executes in six milliseconds. I mean, yeah, here, executes in 37 milliseconds, very fast. To get a really rough and ready idea of how good a classifier, how good the classification is, how well are the topics separated. 82% F1 score, that is a balance of precision and recall is, I would consider it pretty good. Like 83% is a pretty good separation between the topics. So now one of the things I taught you is mentioned, the Zipf's law for distribution of word frequencies. What the law says to recap is that the rank, the frequency of a word is inversely proportional to the rank of the word in the corpus, right, with respect to, like how, with respect to other words, the frequency rank, right. And so we can try and see whether it is true now it was more than that f was uh one over the rank to the power n to the power s where s could be something for simplicity we assume one but all right that is f right let's just see if that holds in some form what i do is you take all the words that are there in the corpus. You know, this is your clean text. You can extract all of the words. And it turns out that they are unique 114,000 words. 289, but let's go with 114,000 words. That's a pretty large vocabulary of words there. So let us see if the ziff's law holds what i'm doing is i'm just creating a frequency table you know the word word frequency very basic then you see what happens is i happen to know that you'll end up with a very long tail you will get gazillions of words with, you know, that have occurred only once or twice in the corpus. So just to keep your, just for the purposes of visualization and no other reason, I deleted any words, I removed any words at least for the visualization purpose whose frequency is two or less, right? So then I end up with a smaller number, 28,000 unique words. And do you notice something very interesting? We started with 114 and 114 has already reduced itself by a factor of almost, you know, one fourth or one fifth that has become by the time we go to words of length three or more. I mean, words of frequency three or more. That already is suggestive of the sort of Zipfian distribution to some extent, but let's continue that. And then I make the frequency plot. Do you see this one over rank kind of distribution? The X axis is the rank, Y axis is the frequency. I hope you can see the classic signature of a long tail distribution. I've taken only the first thousand Rangs because otherwise the graph looks, it doesn't have, it doesn't look nice. So I just made it like this. And then if you take the log of it, the log of it should have been more or less a straight line. It isn't exactly a straight line, but let's just say that, okay, we'll call it a rough approximation to a straight line. Would you agree guys on the right hand side? Right, sort of like a straight line, but not quite a straight line. So it is a very rough approximation to the Zipiffian law distribution so that is that i'm illustrating the fact that ziff's law of frequency distribution does hold and that is the reason why remember in the tf idf definition for the idea if we take the log right just to linearize things a little bit okay now we come to the topic modeling with GenSim. Now, guys, we know enough about the data. So we will start the main thing that we want to do, which is topic modeling. We use a library called GenSim. It's a fairly popular library in the Python world for topic modeling. And there's a pretty good job. It's written in in c underneath it so it's a fairly high performance implementation what does it need it basically needs an array a list of a list of documents each document being itself a list of tokens list of words so it is a list of list of tokens, list of words. So it is a list of list of words, right? In some sense, a two dimensional array, a two dimensional matrix of words. So what I'm doing is I'm just creating it like that for every text. First thing, do you notice that I'm using NLP? Where is this NLP coming from? Spacey, right? This thing, you must be familiar with the previous week's lab. Right? This thing, we must be familiar with the previous week's lab. We get to the words, and one of the things I'm doing is, for token in dark, if token.text in keep words, I'm creating a list of words which are frequent enough, at least twice they have shown up in the corpus, and keeping those words. And then when i have that words i'm looking at the very first sentence and what it first email and it decomposes into this wonder wonder enlighten car see two doors sports blah blah blah uh now now what you do you realize that the words that we found in the corpus are a limited subset of the words in the entire English dictionary. We have about 20, 50, what is it, 28,000 words. So we will create a limited dictionary of only those 28,000 words, right? So that our id of that the vectorization is not irrationally big okay so we do that now when you do that you can do you do it using a dictionary you give your text and you create a dictionary this is a gensim thing and now you have a dictionary and it says that there are 28,000 unique words. It starts with these kinds of words. Now, this dictionary, by the way, has a way. Remember that I removed for the process of visualization, I removed the infrequent words, once or twice words. Now, it's because I did visualization before. So I removed it. But usually, if you haven't done that, then Gensim gives you a way to remove these uncommon words, filter the extremes, the very common and the very uncommon is there. It contains a feature called filter extremes. We are not going to use it because we know that we have already sort of removed the stop words, which are the common ones, and remove the rare words. ones and remove the rare words. So I skipped this step. But as a homework, you can consider implementing it and seeing how it affects. So now comes this. Very simple. We have a list of words. Now we are going to create a Gensim model. So when we create a Gensim model, first, it's a two-step process. We create a bag of words. Remember bag of words was what? That long representation in which one hot encoded sort of a representation. Given a word, whether it's present or not and so forth. word in the dictionary and then you feed it into the tf idf model now remember one thing guys that you can do the tf idf vectorizer every nltk sorry every natural language processing toolkit will give you that this is the syntax for genseng there is if you just look at scikit-learn, there the class is literally called TF-IDF vectorizer. Then once again, you would give it the bag of words and it will do that job for you. So these are just minor differences in syntax and I won't go over it. It's something that you become familiar as you do it. Then I talked about the singular value decomposition, that you could do that linear decomposition into latent topics. Here, I've taken the number of topics to be six, because we happen to know the answer, which is a good number of topics. But in reality, you'll have to play around and see what gives you good topics. So we take these topics and then we look at the topic. I let the LSI or LSA do the modeling. The modeling is very simple. Do you notice that this is just one line? You give it the corpus, TF-IDF corpus. Now, when you give it the corpus,f idf is based on a words index in the dictionary which not very readable it is much better to contain the actual words so the id back to dictionary you can look for that in the dictionary and the number of topics is six then you go and run it and then what happens you ask it to print the topics and the important words found there so let's see if it makes sense let's look at this one topic zero no people think like good use problem thanks work well i don't know can you guys infer what it is know can you guys infer what it is probably hard the second topic is file card drive tags there's god and then there is windows people driver window desk can you guess what this could be amongst the six amongst the topics here topic one could be what anyone's guess? Religion? Religion would probably not have windows in it. No windows. It looks like something with operating systems or... Yeah, probably could be computer. Yeah, computer. Yeah. But it's not very clean because it has the word God in there and so forth. So topic three, topic two. It's negative right on god yeah negative what's negative doesn't it's the size that matters the magnitude it's still pretty okay now the topic is key game team that also seems it has game and play maybe sports perhaps on the year it says chip so could be gambling too chips could be gambling yes but it has encryption which uh maybe science so it is so the topics don't seem very easy to see. Topic three has game, God, Jesus, chips, encryption. So you can see, and government also thrown into it. Never a good idea to mix God with government, isn't it? So, well. Did you have a constraint in terms of how many topics it will use? Yes, literally that. If you look at it, you can change the number of topics and this is one thing you should do guys I would strongly encourage you to play with the number of topics try three topics and see what happens hint that's a hint actually change this code make it three topics then see what you get all right and. And you will see something worth it. Then, so it's hard. This one seems to be religion, quite definitely religion, right? Chastity, skepticism, shameful intellect. May or may not be. Surrender, banks, God. but there seems to be God everywhere. It's a bit hard to tell. So that was the latent semantic analysis based on linear, you know, this singular value decomposition, which is sort of a linear factorization process. which is sort of a linear factorization process. Now, perhaps that didn't give us as good a result. Let's try the latent Dirichlet allocation, the one that we are talking about. So here, not much difference. The code is more or less the same. When you do this, this is what you get. All right, the topics have, first topic is drive, game, card, problem, tanks, disk, port, controller, work. The second has card, video, driver, surrender, skepticism, chastity shamefulness so i'll let you decide the third is israel food delete prize jewish so this clearly seems to be moon it clearly seems to be about geopolitical situation so that's that and this seems to be about sports bike ride motorcycle rear car but there's a bit of morality thrown in auto rider captain would you all agree that this looks like uh sports isn't a car in sports uh we don't have a category for sports we have put it in the car by right bike rider so topic three seems pretty clean. Yeah. And we will see in a visualization if that is true. Topic four is no people think like God want and we'll see. Topic five is core-ish, armin-ish, plain, antenna, liar, and so forth. So let's see. There is a measure there are two measures actually one is perplexity measure you can measure how well the topics are separated out and that measure is called the perplexity measure you can see a value of minus 10 source of value but i want you to read up on this the other thing is there's another measure which I leave as homework for you. If you guys can take it down, it's called the coherence score. So just like I did the perplexity measure, I want you to, as a homework, add right in this analysis, in this notebook, add coherence score. Right. Coherence score model to this. So now let's visualize this topics, how well it has separated. Remember six topics. So this is a visualization. So I'll just make the thing a little bit smaller. Okay. Yes. So when you look at this, you see, what do you see? Topic one seems to be really separated out. You know, the more the separation between the topics, the better. So clearly topic one is dominant and it seems to be well separated out. Can you explain the axis here? Yes. So this is the principal components, principal component, first two principal components of the topics itself. You further take the topics and reduce it to two dimensions so that you can represent it in a plane. In those two dimensions, clearly the first topic and these are called principal components um so let's take it as a uh frame pca takes a bit of time to explain uh so i won't go into full detail but here's a simplified view if you were to take data and you would to project it down in the most meaningful way possible to two dimensions so that you can visualize it. Right, in a linear two- and it's some sense a linear projection downwards. PCA is the classic technique to do that. But in terms of those when we saw those coefficients right now they were totally about what six or seven variables right those were the word variables yes about the words so in this visualization it's actually condensing that from i think there were about eight variables there no no no you had six topics right so you have a six yeah you you're taking that yeah so all of those variables what you're doing is you're projecting all those topics down to a two-dimensional plane all right so all the documents that are in the out there and basically the topics here putting it down in a two-dimensional plane here you're saying how do i put if i were to project all the topics down to two dimensions yes okay take your word space but project it down to two dimensions what would it look like okay right but so the more the separation the better so we clearly see that this is far this is far but these guys are all pretty close to each other. So let's look at it a little bit in detail. Topic one, know people like God. See, God is an expression that people use in computers and science everywhere. People will keep saying, oh, God. Right. Oh, my God. And so on and so forth. So it's a I wouldn't pay too much attention to that but look at the rest of it believe come system work here point car do you see a clear topic here i'll leave that for you to sit and judge the next is this one drive this one this one I would say is very clear. It's about computers. Look at the words drive, game, card, problem, driver, controller, modem, motherboard. Would you say that it has separated out this one well? Right? And then let's look at this guy. This also seems to be, interestingly, it seems to have mixed computers. The lower half of the words are computer words, and somehow it has thrown in skepticism, chastity, are religion related topics intellect into this so not good this is clearly about sports topic three seems pretty clear it's about sports bike ride cycle, car, auto, rider, captain, wheel. What about number six? It seems to have some degree of overlap. But to me, it looked like a fairly separated out topic, though I couldn't make out what topic it is. It seems to be about geopolitics, location-based or something like that. But it has science in it mars venus space basically in it and then the fifth one is much more a mixture of space and politics you know the countries israel palestine turkey and so forth jewish and so forth it seems to have mixed up politics with some amount of science to it. So this is how Gensim worked. There is another library, Mallet, which is a Java-based library, which is very effective actually. In my view, quite often it gives you superior results. By the way, when I'm doing all this here, guys, I have not done any kind of tuning or optimization. In all exercises of NLP, you gain a lot by properly tuning your models, playing around, doing things, and you spend a lot of time doing that. And so what I would encourage you to do is take as you work with the lab, try to improve it. So here is a hint. I've deliberately left a lot of basic optimizations I have not done because it would have complicated the code and made it hard to read. So I've not done those things. I play with those. For the very basic, you can play with just three topics and see how well separated they are. Then you can go around and play with whether lemmatization was a good idea or not. The other most important thing that I forgot, or not forgot, I deliberately didn't do it, is something called bigrams and trigrams. I hadn't taught you that. It is basically putting two adjacent words together into a pair as a single unit. It's called a bigram. So happy cow, right, the two words. I saw a happy cow. So I saw would become a bigram. saw a would become a bigram, and happy, a happy would become a bigram, and happy cow would become a bigram. If you remove the stop words, then it would become saw, right, happy cow. So, the bigrams would be saw happy, right, Happy cow and so forth. So, and as you can imagine that those add meaning. So please take this as a homework. I know, I happen to know that for this particular analysis, when you add bigrams and trigrams, those are expensive to add by the way, to explore your vocabulary. So start with just bigrams see how much your results improve if you add bigrams to this analysis what is the intuition for n-grams helping in topic modeling yeah so think of it when you use words some sometimes the words don't convey the meaning. For example, the word machine learning together, machine will convey a sense, it will contribute a sort of topics or contribute a lot to mechanical engineering. Learning will contribute a lot to education. Machine learning together, you realize that as a pair, they contribute a lot to education. Okay. When you put machine learning together, you realize that as a pair, they contribute a lot to artificial intelligence. Okay. The word artificial intelligence itself. Do you see why bigrams are useful and worth adding to the vocabulary? Okay. So please do add bigrams to the vocabulary. The hint is it's just a small trick, a few lines of code, enhance this this and see how well your topic modeling works when you add bigrams to this exercise. All right, guys. So now I'm using mallet. Using mallet. Now, good thing is that, you know, do you notice that you didn't have to write java code you just literally stayed within gensim use mallet as a wrapper there's a it gives you a wrapper gensim wrapper for mallet so you can use mallet for again a latent dhla allocation it's a twist on that uh and then let's see how well it performs do you feel that the topics are separated out guys this time the bubbles are separated out yes yeah it is better it's just a general experience by the way i in general i find a mallet in general, I find a mallet superior. And then, but then, whether or not it has done a good job, actually, I'll give you a hint guys, add the bigrams, then your results will significantly improve. But for what it is worth, as a quick indent here, I brought it here. You can decide what it is about these topics are what they seem to be fairly clean. Right. This seems to be about mechanical, about cars, sports, so forth. But this is about. If you remove two door and bumper and body. Well, it seems to be rather mixed. This one seems to be pretty pure. Anaheim, Brent, Satan, problem system, I can't make out. But anyway, that is that. But at this moment, without adding bigrams, this is how it looks. So add bigrams, guys. Now there is one thing i wanted to bring in at some point it has not anything to do with topic modeling actually it is independent of that see python code when you run it runs in a single thread in the sense that it is a sequential run all that code when you run when you work with large data frames things can get pretty slow because suppose your machine these days has many cores for example my workstation has 64 cores you know if you have a thread Ripper the you have uh threadripper the it will have 64 32 physical course means 30 to 64 uh sort of hyper trading or sort of like that uh virtual course or something let's say that it has 32 or 64 what will happen is normally when you write run your code only one core is an effect. So your performance is 1 64th, or if you have more cores, 100th or something. So how do you speed things up? So Python has this very nice library called job lib. If you can make your problem in some sense, a parallel, parallelizable. So for example, if you're doing an operation to each element in a list and that it doesn't have to deal with there is no interdependence between them every word let's say that you're capitalizing every word or breaking every word into nlp tokens right tokenizing the words you realize that you can deal with every text separately every email separately no reason to do them sequentially so the point that and so to illustrate that point i have i put this code here and hopefully this is your starting point for um doing parallel computing in python doing parallel computing in Python. So you notice the code is straightforward. You create a parallel object and you then give it a function. You give it a argument. What will be the argument to the function? And then from each text from a list of texts, which is news groups. Remember the text column had 11,000 such records. So it will execute faster, right? And you'll get the same result. So this is about job parallelization, not necessarily about topic modeling, but you can and should use it for this purpose. Like you should use it everywhere that you can. So, all right guys so one of you requested that in one hour i should give an overview of what i'm going to teach in the remaining time and so we are done with the overview any questions before i go through this a bit more slowly. So what are some examples that this could be valuable? So one thing is let's say, I'm trying to figure out a topic to look at, as example, I look at the app called Flip, which gives me the news. I could choose data science, politics, anything, right? And on those articles from newspaper, and then it gets summarized for me, okay, that will show. So I'm trying to figure out like when the when i do do this modeling in the practical corporate world what are the use case you have in mind that could be benefit that we could use it maybe you see that uh see first of all before i answer your question if you put one card here see guys do you notice that if you just run through the analysis in a simple way direct way your topics are not terribly good right they're good enough that's all you can say but they're not terribly good there is a huge scope of improvement of tuning this and so people who do topic modeling they often spend a lot of time tuning with this, playing with the number of topics, seeing how it matches, adding bigrams and trigrams, checking, playing with different parameters, the hyperparameter tuning. Machine learning, the difference is quite high in performance between straightforward code and optimized code. Obviously optimized code means now the code looks more complex, or there is more going on in the code. When you're just learning, you don't do that. Learn to do the straightforward way as here, and then do a better job of tuning it and making it better. Having said that to your question, so there are many examples. Like for example, you're getting a lot of Jira tickets, Shani, from your customers. A lot of issues or complaints and suggestions and feedback are coming to you. You don't know what those things are about. They're just plain running texts. Like suppose you sit on a division, let's say that you sit on the WebEx division. Right. Besides the fact that they're all about WebEx, it's the feedback that you're getting or the reports that you're getting could take all the text and do an answer. Topic modeling is unsupervised. You can see what topics they seem to be about by doing topic modeling, by looking at the coherence score or the public city metric and trying to come out with clean set of topics. First, you have to find how many topics you, it sort of decomposes into, you visualize, you do all sorts of things. And then finally you gravitate to a stable point, you converge. Once you have converged, now you look at the topics and you try to give it an interpretation. For example, one topic I hope is that WebEx does a terrible job in allowing you to add a lot of virtual backgrounds. Now, for example, they have given that feature, but if I were to guess with Zoom giving that feature, a lot of people must have given that feedback. We want it here too. Yeah, great. Or basic direct integration to youtube it doesn't have it so i'm sure they would be i mean i'm just giving the points that uh that helped made me move after so many years of using webex made me move to zoom so if you go and look at the feedback, you'll find something. But how do you find those? How do you find the core themes in the feedback? That is one. Look at the tickets, ticketing system. When you do a topic modeling on the ticketing system, you may realize that a lot of people are talking about certain themes or topics, which don't fall into your traditional buckets of this product, this component. You see that, right? And so on and so forth. You can use it. Suppose you see a lot of news articles emerging. You can quickly do a topic analysis and find the most representative document or sentence around that topic and see what is it. It could give you an idea of the emerging news. So it has a wide application. Topic modeling is a big deal. Now these are the examples that I just thought at the top of my head, but if you spend a bit more time then more and more examples will come to you i have a design question here so right now when we did this exercise what it's essentially done is for each of those topics they were essentially a combination of words right that's how we basically define topics we have six, each of them were a combination of eight or nine words. Oh, no, no, no, no. You don't think of it like that. See, looking at the lab, you may think it is like that. What the topic is showing you is that how much weightage it is giving to some of the more important words for that topic. Yeah, I was building on that. So yeah, so right now it's done it on words. But I was wondering, is there a way to define these in such a way that it becomes a two-level process? What I'm looking for is, let's say these words, some of these words were essentially things that, let's say, describe technology. Some of these words are things that talk about emotions. So I want a two-level. Yes, do that. You could do that. See your problem, for example, right? You could do something like that. See, this is where the, you know, okay. See, there's supervised learning and unsupervised learning if you have some label data for this system to learn that this is about emotions and this is about technology then in the first stage you could just run a classifier and in the second phase then having classified the data within technology you can find topics and within emotional stuff you can find topics or you could try your luck by just doing topic modeling into two topics and hope that a good topic modeling result would be that everything to do with technology falls in one topic and everything to do with emotions and experiences falls into other topics so you have to play with it and see. Okay. Right? A lot of this is experimentation. Another way that I would see is, do you notice that when I did a classifier, the classifier had a very high degree of accuracy, 83% accuracy. Supervised learning. But this topic modeling is unsupervised learning and then the results are more mixed. You have to work a lot harder for it to give you good clarity. This is a general experience you have. See, the reason supervised learning, you tend to get higher accuracy or quicker results, is because the hard work has already been done in giving you the data. Creating label data is extremely hard and extremely expensive, right? Except in such situations like this, where you already know, you know, you're going to news groups and harvesting the text, in which case you know what the news group is, right? Or let us say that you're looking, suppose you want four subjects and you want to gather you know from the web some content around the four subjects if you specifically know this subject uh you're searching for the web and putting it in a directory all those content locally in a directory marked signs right you already know you have label data. That is a gold standard. But now, remember that used human effort. Right. Increasingly, the big push in machine learning is how can we do things either in a semi supervised or unsupervised manner because supervised doesn't scale very well. supervised doesn't scale very well right in many situations in some situations it's cheap to get labeled data in most situations it's very expensive to get labeled data most of the data comes unlabeled in real life so that is why the importance of techniques like topic modeling because they help you in that or clustering and dimensionality reduction and so on and so forth. They have tremendous importance because if you can just sit here and keep on refining your mathematics and you can therefore discover very good themes, what you have just saved yourself is a massive spending on people sitting and labeling data. Okay. So there's a practical aspect of it. Any other questions guys? Asif. Yes. It's just some feedback. I like this format better than the old one. I'm not sure how other people feel. Okay, a quick walk through the whole thing in an hour. Excellent. So I'll follow this process, guys. Any other feedback, guys? Otherwise, we'll declare it a break. Let's meet after dinner. And then we'll go over this code a little bit more in detail. This code is actually quite simple. There isn't much more detail to go after, but I'll still go over it line by line in the next. And so those of you who understand this code, you all have downloaded it. If you understand this code, then you don't need to come for the second half. But if you do need some help with that, then do please come back and we'll go over the code in detail. It is 8.23 on my clock. I will stop the recording. So this should be easy for you. The first, you just listen to the recording, the video. Now, every session, I'll break it up into two videos, part one and part two. Part one would cover the whole thing and part two will go into details and clarifications. All right, guys. So let's take a break. It's 8.23. Should we give ourselves 25, 40, 50, 8.50? Should we regroup at 8 at 8 50 is this reasonable in 25 minutes or maybe we can group earlier 8 45 20 minutes is 20 minutes okay guys or should we should we take a little longer this is fine 20 minutes is fine it's. Yeah, we can get back at 8.45. Yeah, let's see. I'll see you guys at 8.45. By the way, Shankar, do you have any further suggestions on topic modeling since you're doing it on a day-to-day basis? What else should I include? I think I used it in a previous project right now at least at my present job people are using topic modeling but then at least these things at least you have a good understanding of what the topics are generally in data that are not clean the topic modeling is not that useful it's not that it's pretty hard to get it right yeah you have to do a lot of hard work on it and then the only way you can adjust is the coherence score and coherence score. Exactly. And use that to. And then sometimes what happens is it is not just the data. Your data becomes too domain specific. So that topic topics do not match your expectations. Are all the things come in one topic yeah most of the things so that is the limitations we faced that's right no so my question is you know you see i walked through the notebook yeah this is a good approach yeah yeah i i skipped the bigram which is the homework you guys by gram those make a huge difference in my experience. So bigrams make a huge experience. Then anything else big that I missed? I think you covered a mallet. I think probably this is whatever you can do with topic modeling. You've covered most of it. PANKAJ UDHASANANI- Pretty much at the limit. I think this is the high end. like the maximum you can do with this. Maximum you can do with it, yeah. There is one particular algorithm I didn't do. It is a HDA, hierarchical LDA. The reason I didn't do is because I've never found it to outperform. Actually Mallet also has an implementation for it people say it's supposed to outperform lda it's supposed to be a better version of it i don't know is here in my experience i've never found it better have you had a different experience with it uh no no i have never used used xda mostly i used lda for most of my work nice i think we are together but yeah so that is one thing i haven't covered maybe for next time all right guys see you at 8 45. okay thank you Obrigado. Gracias. you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you E a√≠ Are we all back? Ah, 19. There are some people have dropped off. Let's give them a couple of minutes for them to show up. Did you guys finish your dinner? Yes. Just wait a minute or two for the attendance to reach 20 and then we'll start. Thank you. By the way, I forgot whether Kate managed to join the group or not. Did she make her way into the Zoom? Praji, would you know? Sorry, Asif, what? Did Kate manage to join the Zoom? Yes, she liked the one where it was told to her that the the youtube channel is live so i suppose she is watching on from youtube you want me to confirm once with her yeah she she's watching from youtube from youtube okay all right guys i'm about to. I don't see more people coming. So this is the second part of our lab on topic modeling. In the first part, we did a quick scan through the solution, and we talked about the high level issues now what I want to do is be very clear about the homework guys one important homework is play with changing the number of topics right play with changing the number of yeah the bigrams trigrams and so forth these two are essential make sure we do that and those are two i didn't do a particular form of lda latent a location called hda i will release code samples for that there's nothing unusual about it it's just a couple of lines but you can play with that now if you look at the way i have changed the code there there is a opportunity to play with this in slightly different ways i would like to show you my code and explain what i mean uh see you have space pronunciation pronouns punctuation symbols unknown i have deliberately marked it with a hash, right, so that you can uncomment it and see what effect it will have on your overall results. Things that you remove and keep. Then I change this. away words or limit after limitization, whose size is just one, a single letter and so on and so forth. So it is a two, what if you change it to like four or you do something? What if you throw away numbers, right, things like that. So play with this file, clean text, see what happens and you'll find that you'll see some subtle small differences in what you are doing right and here also you can sort of so that is that that is one thing worth doing the so now going through this lab what i would like to do is just go over each of these and see if there are any questions and explain it a little bit more in detail. If we can get through that part quicker, then I would like to give the last 40 minutes or 45 minutes to explaining or making progress with our project and giving you guys some suggestion for the projects because i believe at this moment you guys haven't made any big any large progress with it so far so let's go with that so the imports by now you must have is this looking anything unusual in the imports guys anybody notices anything unusual what is this multinomial a little bit louder please the import of of multinomial. Where is multinomial? Line 25. Multinomial naive bias. It is a very simple, it is the naive bias classifier for multi distinct data. Where are we using that? for multi distinct data. Where are we using that? Oh, so remember this, I don't use for topic modeling, but I said that just to see how topic modeling is and whether the ground truth of the data is that the topics are well separated. Remember we made a classifier here, a basic classifier. Let's go here, see, I created a very basic classifier. What does a classifier do? See, I could have used something else. I could have used logistic regression or random forest or decision tree or whatever. The reason I use multinomial live bias is because it's very fast. Do you notice that it is a cheap model? The model finished building in just 37 milliseconds. So I wanted to just use it for basic classification to get a sense of the topics are well separated out in reality, whether the ground truth topics are well separated out. And it seems to be you know with the 83% f1 score which is quite good topics do seem well separated out yes you got that right okay so let's get back to imports by the way no particular use now if you don't like multinomial just use logistic regression, use to say, use whatever that you feel comfortable with. It doesn't matter. Okay, anything else guys? So what is bunch in line 23? Ah, very good question. So what happens is when you use the scikit learns data set, you know, the data sets library are coming from somewhere here I have scikit learn data set. I got 922 line 22. Exactly. So this function, what it returns is it returns a object which has whose member variables are actually why don't i why am i just saying that let me just go to that sk learn bunch let us do that sk learn 11 bunch. So you look at this bunch, what it contains is other version details, but it just contains is basically a hash map. More realistically speaking, actually this documentation is terrible, so let me explain it here. It contains, it's a hash map which contains the data, you know the text, it contains the target variable. What is the target? This. Do you see that? Right? It also contains some explanations and basic things, some text and metadata. That's it. So think of it as just a hash map of where the data and the labels are kept as key value. Text and labels are kept as key value. What are the labels? One, two, three, four. Is that the label? No, no, no. That is something i just created 20 news groups right labels are the name of the news group oh and then what's the right what is the target variable what's the difference between label and target it is the target variable. The words are interchangeable. When people use the word label target variable output. These are all interchangeable words. Think of it as target actually, they use the word target. This in the bunch, all of these values are kept there. So do you see bunch dot target names will tell you all the target names, the unique values of the target. So bunch is a hash map of what? It is a hash map which contains the text. There is one list of text, which is the data. The key is the data and the value is a list of text, which is the data. The key is the data and the value is a list of all the English content. And then there is another key, which is called target, whose value is for each of the email which news group it came from. Are we getting it? So guys, do you know what a news group is? Or maybe you guys are too young at this. You don't know the world of news groups. News groups are like Google groups or Yahoo groups. The word news group, it harks back to 20-25 years ago, where most of us used to subscribe to news groupss and we used to keep getting messages and posting messages on newsgroups. Alt.news. Exactly. So the thing is, these are all newsgroups. And the equivalent today is the Google group or Yahoo group or whatever it is, or maybe your WhatsApp channels. They are like that. So what happens people post messages there right and messages are on certain themes or think of it as a slack channel you have a channel for ml 400 you have a channel for different topics so 20 years ago the equivalent of these things used to be on the linux machines on the unix machines they used to be this news group a new server and you could have you could subscribe to this chant these news groups the protocol right NNTP yes exactly you remember that training yeah you're from that generation and uh it's i mean even today you can install and i hope it's free uh you don't have to go and use all this fancy stuff so that is it does that explain to you and it's like for you yeah so it is it has target target label and what's the other target has target target label and what's the other target so one is like the document itself let us do this let us go here and look at the documentation and i'll show it to you what the member variables are and this is just a directory structure in which people organize documents for. Yeah. So when you look at the returns and okay, a bunch, it is a dictionary. So look at this. A bunch is a dictionary like object with the following attributes. One key is called data. What is it? It's a list of texts what looks easy yeah and each text you know comes from a news group now where is the news group stored target you see target it is a list of labels no target labels and the news names right then file names is not very relevant it just tells you when when you not target labels, the news names. Right? Then file names is not very relevant. It just tells you when you download this data, where on your machine it sits, right? And it will sit in your home directory under something called sklearn data. You can go and check it out. There is another field called description. It will give you the description of the dataset. Now there is another field called description it will give you the description of the data set now there is another field called target names right it is the list of the type now target names is nothing but the unique values distinct values of the target list i see it is nothing but calling distinct or unique on this list if you call unique on it you'll still get the same thing is it enough yeah that is it so that's how we understand that so i'm just printing out the target names that is that and then just do a count plot basically do i hear a question from somebody hi as if uh this would i just wanted to check with you about that uh uh you know we have the 20 some one second uh that fetch 20 news group right so is that a function or how I I didn't quite get that this comes built in with the scikit-learn if you call this function see they give you it's like it learn comes with some standard data sets because the 20 news group is a very it's like you know do you remember the breast cancer data set of the California housing data sets yes right so where what so what happens is all the famous data sets that we use for learning psychic actually makes it very convenient to it has the psychic data set okay which has a list of all the famous data sets that's all got it got it okay fine sounds good yeah thank you that is it there's nothing uh else to that data sets here we go yeah data sets and the psychic data sets loading utilities so do you see low diabetes data set load iris we remember doing the ir dataset, the wine dataset, the California housing dataset, and the 20 news group datasets. So all of these are just famous datasets and you just use it. Yeah, sure. Thank you, Asif. Asif, one question. In line seven, you also have targets and then you mentioned dictionary so what what kind of uh um what is this code type how have you written this i've never seen that before like targets and then then you have line number line number seven and then you mentioned dictionaries separately what happens is most people they would okay are you familiar with the empty dictionary like this somebody creating i understand empty dictionary but why do you like what is this way of is that a way of naming the dictionary targets is the name right so here's the deal you would you. You would see people in Python doing just like this. Exactly. I tend to be more clear. I tend to tell what type it is. So that later on when you're reading the code, people never get confused that it's a dictionary or what is it. I'm just being explicit. Do you notice that I'm being explicit here also? There is no reason to do this. Specify the type. Right? But I could have just said bunch is equal to fetch this. But I wanted you guys to know what kind of object bunch is, that it's an instance of the bunch class. That's why I did that. See, it's called type hinting in Python. When you do type hinting in Python, there are two things, two benefits. First, you make it much more easy for a reader to know what your code does, right? What those variables are. And secondly, if you're using a tool like PyCharm or any ID, then when you do type hinting, in return, the code completions and code assists becomes much more powerful. It begins to resemble a little bit like the power that you get with Java or any of the typed languages. When you write it in a code editor, you will see how much power comes. Type hinting, I would strongly suggest that people do it. In Python, it is optional, but your code will be a lot cleaner if you do type hinting. Is anyone of you familiar with type hinting or doing type hinting in your workplace? Anybody here? Nobody. Interesting. You guys write Python code without type hinting. Asif, actually I learned type hint you know when you were uh helping us with the python class so which i found it really helpful you know sometimes uh you know when even in 100 data set right when we were working on uh we used to get confused where uh you know the uh explicit typecasting we should have done but we but we would have missed all those things. So this really is helpful. Yes. Yeah. Thanks for saying that. So all right, guys. So this is a simple count plot, isn't it? No particular reason I used Seaborn. Seaborn just makes it prettier. We could have used matplotlib.bar. So what is it counting here, Asif? Counts how many records belong to each of the target types, to each of the new scripts. I see. So it just takes a length of the target group? No, no, no. So how many I see in this row data, no, in the data that you have, how many rows belong to how many items of data belong? How many emails belong to alt atheism? So it seems to be about 490 emails seems to belong to alt atheism so it seems to be about 490 emails seems to belong to alt atheism you see that right other ones seem to have a little less than 600 target will be the news groups right it won't be that target news yes yes targets are the news groups so these emails belong to those news groups right it is showing you the distribution of emails across the news groups right but in this case like one won't be a teaser right it will be some news look it up look at the table here one is atheism no but that isn't that target name so i thought target name was the label and target was the news group so I thought target name was the label and target was the news no no no oh goodness see target they call the variable all I'm doing it is all the target values in the group the which news group they belong to. So I'm grouping all the emails by the by the news groups they belong to as simple as that. Are we getting that? Yeah, because you use target names up there. And you're using target variable no target names is is the distinct value of targets is the target in the bunch in the bunch look here well okay basically right target names will have it is just a unique list of all the possible target values the variable name is target these are all the values that the target could take. The Y variable, if you think in terms of X and Y, Y is target. These are all the distinct values that target can take. Now the question is how many rows of data has this particular value of the target right it turns out that they about 490 such rows right and so forth that is it so 490 email seems to belong to that group now is it more clear yeah is it more clear yeah so see if i have one quick question so this entire thing target dot not like if you scroll up a little bit so this entire thing talk dot politics dot medis this entire name is the target name, right? This is the name. It is the name of one news group. Okay. So let me make it very real for you. Let me take one of these, uh, comp dot math dot software, hardware. It's course, somebody may have archive of these, uh, almost surely. Let me. it somebody may have archive of these uh almost surely let me let me go here and see hardware yes com do you notice that here is a web archive do you notice that here is a web archive somebody has that archive of this do you see what do you see this is a message board no where people have messages do you notice that their emails and conversations back and forth and people are discussing things yes as simple as that. So the name of this mailing, what in modern world you call discussion groups or discussion boards or whatever it is the word or Yahoo groups or whatever you call it. The name of this is com.cis.mat.hardware, right? Which is what you see here. You can take any other one of them. For example, let's take SOC.religion.christian. See if there is such a thing in the world. Name that. the other somebody would have archived this soft dot religion yeah here it is once again people are talking about conscious intent you see that they're talking about this religious concepts i think i don't know why physics has come in here but okay so are you guys understanding what a news group is So are you guys understanding what a newsgroup is? Aritya, is it clear now? Yes, yes. So it is as simple as that. These are the names of the newsgroups and each record is an email or a message in that newsgroup. Somebody posted in the newsgroup. That's all. Look at the data. When you look at the data when you look at the data you'll see it right do you see this data no one data says I was wondering if you're anyone out there could enlighten me on this car I saw the other day it was a two-door sports car etc right and it belongs to the wreck wrecked auto And it belongs to the rec.auto target group, right? Then there is something called a fair number of brave souls, blah. It belongs to this. Mac hardware. Do you see this? Is it now becoming a bit more clear what the concept of this news group data is? Yes. That's what you need, just a message in a news group. And how many such messages are there? And so what I'm doing is I'm merging some of the records and you can create your own grouping. You can play the games and group it together. Then lastly, what I do is I create a new column called topic. I leave the target column there. I create lesser number of topics. So how many topics do you see? Politics, religion, sports, computer, science for sale. Six topics, right? You create your own number of topics. If you want, you can keep all 20 topics. See how well it works. Right? Then what do I do here? Is this code clear, guys? What am I doing? I'm adding one more column to this Pandas data frame called topic. Where I'm creating these topics to be based on our own choices. We are saying that don't give it this name, com, sys, mac says Mac like this palette computer. Right on graphics is also about computer. Am I making sense guys or is it all looking very obscure. Anyone. Yeah, it makes sense. Makes sense. Okay, this is it. Then it's like Craigslist also right in your way. Exactly. It is like Craigslist except that Craigslist of course, it is about advertisements, but something similar. It's very similar to that. Yes. Yes. I guess you guys are the young generation. You're not. You haven't seen the old news scripts. We used to live off that. But most of our things if we wanted to get information before all these powerful search engines came about news groups for the way. So. stop words. Let's look at this element. What are stop words? Do we all understand? Anyone who wants me to explain that? Stop words. Sorry. Let's say, do you see what the stop words are? I just ran this code. What are these stop words? Just looking at it, can somebody explain to me what these top words are? What are they? Common words. Yeah, common words that don't add much meaning. They are the glue words, you know, that help make sentences, that help explain. The structure of the sentence conveys meaning, but it doesn't convey the theme of the sentence in any way. So those are the stop words. Then I'll remove this from my thing. So we need to clean this text. So this is a thing about how do we clean this text? And I showed you the utility. What I'm doing is for every token, what is a token? We talked about token. So this is a couple of sentences, isn't it? So in this sentence, when we have a text, what does the word token mean? Tokens are things that you create out of by understanding the text word by word. Roughly speaking, one token could be one word or one punctuation. get that right so for example this full stop is a token this word is a token all of these are tokens so think of them as words and punctuations are tokens right so it will have a lot of tokens now these tokens some you want to keep some you don't want to keep you want to not keep the urls you don't want to keep email addresses and so forth are we together guys does that make sense right if i want to clean out this text would you agree that this is much better when it comes to the actual semantic content of this text? This is a better, it is better to just take these words out. So far so good, right? So that is the end. One little comment here. I mean, I don't know if we can solve it here. So I've been following the lesson together by running the lab. But every time I go through this clean text, my Python notebook seems to get corrupted. So I have to figure that out but i was wondering if do you have any insight in terms of what's happening inside cleantext which could be peculiar based on implementation i have never experienced that cleantext is just a very tiny function just this okay word keeping and and then you use it. So if you are having a problem, let me know if anybody else is having a problem. See guys, I do understand that there are problems. For example, yesterday we are struggling with another person, Shini, his laptop and so forth. So I have a bit of a handicap. I only work on Linux and I use Windows only when I'm teaching, like for example now. I'm not terribly familiar with Windows, but familiar enough, I don't know Mac. I'm guessing Premjeet, you have a Mac. Yeah, I'm on a Mac, yeah. Yeah, so Mac is my handicap, never used it. In my life, I never moved away from Unix slash Linux. I worked on all my life on either VAX clusters, which most of you don't know about, or Unix machines. So my day to day is on Unix. And when you do much of this machine learning on a Linux machine, the Ubuntu, for example, is a defector standard. Almost everything works flawlessly on Ubuntu. So when I hear all of these issues happening, sometimes I'm able to debug and find what's happening on other people's machines. Sometimes I'm not able to, right? So I just help say that, this docker container here everything is properly set up and just fire this up yeah i'll figure this out as if and i'll post it on our group or if i don't prachi might figure it out so all of you mac people the mac fans or apple fans you guys need to club together and help each other out and take Prachi's hand. But I can't imagine why something simple like this should bail out on you. So, all right. So we clean this text. And as you notice that this text here looks a lot cleaner than what was there before, isn't it? Look at the text here from article blah blah blah and look at the content here i hope you would agree that this looks a lot cleaner guys yes would um would the list of lists ever create any issues later on or do like do we need to keep that in mind always uh while processing the data down downstream no no see I created into a panda's data frame now you see this lovely pandas data frame right uh do you notice that I converted the news groups, news apply, this news group. So as a data frame, where did I create the data frame? Yeah, a little bit up. Do you see me create our data frame? Yeah. So once it is data frames, now we are not still having it like that, but we will create list of lists where needed. But list of lists is very straightforward in pandas i mean it is commonly used but you use list of list up there right uh just one line above where in news groups then you use basically it's clean text and topic in line 11 no that is the list of this no no it is a panda syntax you are saying that from the news group pandas create another data frame keeping only these two columns it's not a list of lists but then why wouldn't you just keep clean text and topic instead of keeping it under it into two parts so that the meaning is clear columns columns to keep is equal to let me do that and see if this makes you if it makes it easier for you to understand columns you to understand columns no i understand that but what i'm saying is why why keep clean text and topic in the list why not just keep them within say you have news groups pandas yeah no particular reason i just wanted to keep a smaller data frame in which I have only the things I need no but if you if you just had clean text comma topic without the two brackets it will go right oh it won't work it is just a bad idea yeah so panda says that when you project when you create another data frame with the limited columns you must give the columns as a list and the symbol of list is that square bracket i see i thought the reason of reason you're using list is clean text generates a list no no no it's not a column name name. So this is basic pandas syntax. So that is that. So now you have a simple data frame with only two columns. Right? Now we go through and how do I know how many rows there are, you can just call the length on it, then it has 11, 314 rows in the clean text again we haven't lost any rows or something next is so if we write a simple so first thing we do is you can't do anything with text remember we need to convert text into a vector space representation convert text into a vector space representation. So in this line, we are just vectorizing the text, which column? This column, right? A clean text column. It needs to be vectorized. Do we remember that guys from last time I said that words needs, I mean, sentences and documents, they need a vector space representation, because machine learning only works with vector spaces. It is a mathematics on vector spaces. It's basically linear algebra, right? A lot of linear algebra. You need to represent it in a vector space. Everything. You might have a sentence, but the sentence must look like a vector. everything you might have a sentence but the sentence must look like a vector some vector in a Euclidean space so guys tell me if I need to explain that concept over again that is clear isn't it what is data.assign oh it is just a way of creating a new column okay and that's all right right I said I was just looking at the tfid of the numbers they don't look like it's like one vector there's zero and then some number the notation is strange ignore the notation huh yeah so the notation the way it is done is straight don't worry about it but in your mind think of it as the vectors the way i taught you right so basically what it means is this particular word whose name is this peculiar thing, it has this much value. So remember, each word's TF-IDF value is this. And when you do the TFID, it's a document. The same word is there for all of them, maybe because it's sorted, is it? Yeah, yeah, it is just sorted, that's all. So ignore this part. At this moment, this thing that is here, don't pay too much attention to that. So this is it. By the way, this is just a little syntax to put so many columns, so many rows. So one of the things is, it is 150 here, and this is it, right? So you represent it. So this part is clear now, right? This, by the way, is a standard. So besides the TF-ID by the way is a standard so besides the tf idf which is a vectorizer to create vectors out of what the clean text column each of these convert it into a vector tf idf vector so now you have a lovely space you have the vectors and therefore you can do machine learning remember our standard machine learning, the X is the input, little y is the output. Output, we know what it is. It is the topic. And the input now are the vectors, dot vectors. A document, like a text document being one email, has been converted into a vector. Now what I'm doing is I'm doing a split between test train. There are 11, rows so i approximately give or take and so i'm taking the first 7 000 to train and the remaining 4 000 odd to test this is a test train split is this simple guys this is basic array the list notation the indexing notation of python array the list notation the indexing notation of python what is data.topic remember look at the data this is your data frame right this is really your y right this is your target variable isn't it in a classifier if you're doing a classifier, this is your limits. And this is your input, clean data, except that it can't be the input, the TF-IDF has to be the input, it's a vectorized form. Once you have the input as vectors, doc vectors column is here, the TF-IDF, you use as a, like you can basically, here's the doc vectors you created now for displaying i put it in the pandas you don't have to even put it in pandas here it is right directly i used it here first 7 000 rows are used for training second thousand rows then i create a basic classifier by the way why multinomial nb go use whatever classifier you feel comfortable with no reason but then the rest of the lines does it look very familiar guys you could do matrix dot accuracy matrix dot whatever i just use f1 score f1 is what can just can somebody remind me what is an f1 score guys Is it the harmonic mean? Yes, it is a harmonic mean of precision and recall. Just think of it, you think of a standard mean, right? It's just a different kind of mean. You may have learned in school the arithmetic mean, the geometric mean, the harmonic mean. So it is the harmonic mean so it is the harmonic of the two numbers that is that so 83% which means that the topics are well separated out any questions here guys in this section is there are there any questions i said can you please summarize christian and record i kind of get the um it's tough but i don't know the exact formula oh the exact so i'll tell you guys there's a way there's an excellent reference that you should use for this which where a picture speaks a thousand like a thousand words kind of a thing uh let's go here uh wikipedia precision because they have a lovely page for this do you see how it is given here now please pay attention to this picture here suppose you have data this side is the this side let's say that the left-hand side is made up of positive values the actual ground truth is is positive and the gray area is negatives right true negatives this is false negatives means it's positive so now look at this you have true positives false positives green is positives true positives this is false positive for a classifier so what is precision is this is false positive for a classifier so what is precision precision is green over all the positives so let's be very particular suppose you you you have a test and you need to determine how many cars uh since you like cars uh you're sitting you're standing on a bridge and you have a little device that looks at the picture of a car going by and tells you whether it's a sports car or not. So your device will get, now let's say that a sports car actually does go by. Your device says it's a car. It's a sports car. True. That is an example of true positive, isn't it? On the other hand, let us say a minivan goes by and you mark it as a sports car. So that is an example of false positive, isn't it? It is not really a sports car. So precision is how many of the things that you marked as sports car are actually sports car. Do you see that? Think that you marked a sports car. What proportion of them were correct? Were genuinely sports car? Are we together? Right. And the recall is of the ones that you marked as sports car, right? How many, like how many sports cards went by and you couldn't detect them so this area represents a true sports car so let us say a nice convertible a v8 engine went by and your instrument couldn't detect that it's a sports car it missed it so it is how many it recall is how many you caught versus how many you how many there were right so of all the sports cars that went by how many did you catch that is your recall is the meaning now clear yeah right so that is precision and recall keep this picture in your mind guys it's a very simple picture and it will help you and it's really nice that uh actually of all the books that i have seen i found this wikipedia picture to be the simplest and nicest okay so now guys do this four lines of code, do they look easy? Are they straightforward? Anybody needs clarification on this? This is straightforward. Zipf's law says when applied to NLP and word frequencies, it says that the rank of a, so suppose you rank all the words by their frequency, the most frequent word being ranked one, right? Which would be a word like a, the, or something like that. And the least frequent word that happens only once being ranked at the tail end right having a much lower rank then it says that the frequency of a word is is proportional to some power of the inverse of rank forget the power part, roughly speaking. The frequency is proportional to the inverse of the rank. If you ignore the power business, inverse of the rank. Means the bigger the rank, the smaller the frequency. And there's a one over R relationship. Now those relationships, they look a certain way. They look typically one over x relationships look like this this is a Zephyr distribution kind of a look hey so is this we're just checking whether this is true and so all I'm doing is I'm making frequency plots and then plotting it out so I'll let you stare on this for some time and tell me if anything looks odd here or needs explanation as of us if this is not true then what does it signify to me it would signify that the words actually don't come from natural language in other words genuine genuine language. They have been manufactured by a random generator. So suppose you take a random generator to pop out words. What will happen? It will produce words with equal probability, right? Yeah. Right, or you take any other distribution. So when you look at genuine text that people use, it tends to have, this is our observation, that law is not a law. In other words, it's just an observation based on human languages. It is so. So you think about it. How does a child learn language? The child in the beginning just picks up the important words. Mama hungry food whatever it picks up some words and then as the child grows the vocabulary expands but a natural expansion of the vocabulary is the child will pick up the most commonly used words first and generally a very limited set of words are sufficient for you to become capable of speaking a language if you really think about it if you can pick up the a few hundred words in a language, any language, you can pretty much become conversant, you know, listen to that language, be immersed in it, and have a vocabulary of a couple of hundred words, you will survive. Right? In other words, most things can be done with very few words. You don't need to learn the 5 million English words to survive in the US. As a person, I mean, we come from English speaking countries but if you look at people, I mean, I wouldn't call it Indian English. Indian English is a word, Hinglish. So we come from English speaking world which has an overlap with English. So we don't struggle, but if you look at people who come from non-native English speakers, I mean different words like China or Korea or Japan and you see how they learn. Their vocabulary in the beginning remains small and over time grows and so they pick up vocabulary if only those words they need in the beginning and gradually they expand out so anyway this is just verifying that it is roughly true in this corpus topic modeling with gensim so now what we do is we start modeling with gensim the topic first of all we want to figure out how many words are there and which ones to keep so we define the words to keep as those words which are common enough right we don't remember we threw away words which don't occur at least twice in the vocabulary rare words you throw it away because you know it's too rare to have meaning and then you have those words you create a dictionary a limited dictionary of those only those words that you care about and that you find in your corpus and once you have your own special dictionary it turns out and as you can clearly see your own dictionary is a very small dictionary of 20 well not very small but 28 000 unique words there is a facility called filter extremes which you could have used to remove common and rare words very common and very rare was the two extremes i didn't use it because we had already as part of the data visualization here already removed, we have already removed the stop words and we have already removed the most, the rare words. So finally, now actually down here, finally we come to topic modeling, right? So how do we do that? It's very simple, two, three lines of code, that's all it takes. And so here, first what we do is we take the words and we create a tf idea vectorization of the word so till now the tf idea vectorization we did we you did it with spacey right but now we can't uh gensim again this is a problem that you have to realize that every library will have its own TF idea vectorizer. Scikit-learn has its own TF idea vectorizer and Gensim has its own and then Spacey has its own, NLTK has its own. So you have to watch out. You can't feed one's vectorizer into the other. They are not expecting it. They are not compatible with each other so here we are doing the very basic things we create a bag of words and then we vectorize it gen sim happens to need these two steps approach to do it so here we go we we generate the tf idf do we do we understand the guys thing guys right tf idf yeah the guys thing guys right tf idf yeah simple so we take the tf idea vectorizer applied to the corpus bag of words and we get the tf id here so any any doubts in this code guys guys i'm only keeping those texts for text in the dictionary for every line of text we are creating a bag of words and so you end up with a list of bag of words and that would be our bag of words so remember once again to recapitulate if you remember what a document is, document is any body of text. That's why you have doc to bow. Bag of words was a simple vectorization that we talked on Monday. Every word, the only question you ask is, is it present or not? One hot encoding kind of thinking or then the frequency of the word and so on and so forth you do that and TF IDF is basically saying so bag of words is in that document how often did a word appear and so forth now TF IDF is we talked about TF idea last time so we wouldn't do that term frequency inverse document frequency it gives you the the sort of importance of a word based on how often it happens in a document and weighing in the uncommonness of the word so that is it so once you have that you have tf idea vectorizing you get the tf idea for the whole corpus then comes different ways of doing it. Last time I taught you latent semantic analysis, or LSI, another name for it is, and LDA, latent Dirichlet allocation, which is something, the inner details I haven't explained. I'll do that next time. Do that, and just as a reminder, these were the topics. So what do I do? I apply the model, very simple, do you notice that I apply the LSI model to this, it is almost like dot fit. If it was scikit learn, it would have said LSI model dot fit the corpus and that is that. Asif, can you explain the dictionary part from which which you started from jensen the dictionary dot doc to bag of words yeah see what is the dictionary it is your vocabulary of actual words isn't it so what it does internally is for all the words that are found it attaches a index right a location because if you create a vector let's say that there are ten words in your vocabulary so you will have a vector you have love array right with the indexing 0 to 9 isn't it and each location each index location is for one word one of your ten words are you understanding that okay let me let me just put it and draw it out. And because I think you guys are forgetting the let me do that. One moment. I'm bringing up the writing pad. If you can explain that from the line where you said corpora.dictionary and then you included text in it. So what does that mean? Yes, I'm doing that. Give me a second. I will. Then we have this shared. this shirt all right are you guys seeing my writing pad yeah okay so let us say that you are here and you want to so suppose you have a word the words the only words in your vocabulary are cat okay let's make it actually this is the wrong thing i opened up. Give me a second guys and something odd happened. But no. I didn't know what happened. Okay, something strange has happened in my machine. Yes. MLP. I have no idea why it is misbehaving. Okay, it's picking up the text very slowly. I don't know why. But so bear with me today. This tool seems to not always be perfect. Actually, I could use another one. All right. Whiteboard is... There has to be a way to create a new document here, somewhere here. Okay. Create a new whiteboard. All right some luck this will work suppose your words are cat dog horse giraffe zebra right these are the and cow let's say somewhere there cow these are the only words in your dictionary these are the your it's a zoo and the only words you ever talk about are this right kids do that right when they grow up quite often their vocabulary is filled with the names of all these cute animals so then suppose there's a sentence cow dog cow dog horse a child says cow dog horse how will you write? You could write it like this. You know that you give to it the index 1, 2, 2, 3, 4, 5. You could say cow, 1. Dog, 1. Horse, 1. No cat, no giraffe, no zebra. zebra do you see this this is your dog this dog gets represented this sentence gets represented as this particular thing this vector am i making sense Yes. Is that human? It is. I just created a vector in which for each of the location, each of the index location belongs to one of the words. Right. So every word has an index location. Cow has zero, horse has three. horses three and then given a document all you need to know is how often any one of the words have occurred and you just go and update this vector so so if the word is not in the dictionary it won't be captured yeah the height that is the point so that is why we created a dictionary so so the important thing is important create a dictionary create your oh yeah we actually created the dictionary it's not some pre-made no we created a dictionary out of the corpus remember that dictionary it actually creates a list yes this sorted list by whatever the you know whatever the code is in gensim it creates a sorted list that is right essentially right it gives the location basically what it when you give it a corporate corpus of uh corpus of words or documents all it takes is that it finds the words and then all it does is for each word it first gives it an index location right do you see this it gives it an index location it says cow will be zero so it will internally maintain something like cow one cat do you get the idea this is your dictionary but i i'm guessing the the actual code for the actual uh mechanics of the condition is pretty complicated right so yeah internally you don't want to because they're it's as far yeah there's a little bit more going on you can read the quiz. It's not very complicated, but there's a little bit more happening than at the conceptual level that I explained. And this doc that you saw is essentially a bag of words. Like if you look at this doc, this is a doc, right? But it is not vectorizing, right? It is not vectorizing. It is not vectorizing it is a vector look at this what is this but don't we vectorize it afterwards so there are many different representations remember i said that the vector space representation last time by the way were you there on monday yeah yeah i was i i'm just trying to understand so right now i think it just gives a index right but doesn't doesn't create the tf idf or one hot encoding vector space representation here's a summary again vector space representation what it means is you take a string of dark uh cow jumped over the moon Cow jumped over the moon. Well, here, since those words are not there in our vocabulary, let me just use cow, dog, horse. This is a sentence. The purpose is to take the sentence and uniquely map it to a vector. What is a vector? In simple terms for computer scientists, it's an array of fixed length, isn't it? List of fixed length. So there are many ways you can do it. The question is, first of all, the indexing is, this is the 0th index, one index in an array. You know, this is how you index the array, isn't it? That is simple to you, right? You realize that arrays are indexed by this notation. If I give you a list and you say three, what does it mean? It is the fourth element in that list. Right? So indexing is given. And now the question is, how do I map this sentence to this to make it into a vector? Because this now this now for example belongs to a five dimensional space right this is a vector in five dimensional space so long as i put numbers in these locations floating point numbers in these locations i can do that one easy one easy way is just count the frequency of each of the words so cow was there uh dog was also there uh cat was not there uh then let's say horse was not horse is there in the sentence so here we go one here and then let's say a giraffe was not there and zebra was not dancing and say suppose i extend this sentence to dog horse dog then what happens where is dog two or two location so i convert well okay that is confusing let me not have two dogs because it will look uh let me just make it a cow again cow so what happens the cow location now becomes two. Right? Do you see that this is a vector? Guys, do you all see that? It's a vector. It's an array. It's a list. A fixed size. It is a point in a five dimensional space here. Word space. words your word space now you can do a vector which is which can be used by the tfidf yeah exactly and now so this is a halfway station this vector is your bag of words bow bag of words words in a way and in a way you know what you can do what people do is they don't associate too much like there's no order associated so internally you don't want to store all these zeros because that leads to very inefficient storage so you might just keep it as a hash map you can just say only the locations that do exist for example zero maps to two right one location has nothing a two location has one and that three location has one so when you represent this with this the same thing as this do you realize that a map is a much shorter way of representing it. You're not keeping all the zeros there. You're only keeping the non-zero values. You can keep it as a. So that is called a sparse representation of a vector. The way you think of a vector as an array is a dense representation of a vector. And a sparse is like a dictionary like a hash map are we together and now from this another representation that you can get is you can get a tf idf representation right tf idf vector from the bag of words if i if i have a bag of words representation for every document in the corpus then then I can, for each of these documents, now for cow, dog, horse, cow, I can produce the same five-dimensional vector. are the tf idf values term frequency inverse document frequency let's say three one whatever it is uh those uh you'll have to compute that but okay that's that am i making sense now yeah so uh as if that tf idf you have two lines you have one with models.tfidf where you're using the corpus. Oh, the tfidf models, the tfidf just creates the tfidf vectorizer. Actually, the API is much cleaner for scikit-learn. Can I show you the same code for scikit-learn for a moment? Because in my view, the notation it uses is a bit confusing. Let me show this thing. It'll be more, and this is one of the things, every guy uses his own notation. Their library becomes popular. Now you have to get used to their stuff. So let me show you the same thing, how it is done, for example. I think scikit-learn is always excellent in clarity of its notation. is excellent in clarity of its notation. SK, SK learn, a T F I D F vectorizer. See, let's look at an example of a vectorizer, user guide. Let's go to the user guide. Do you notice that given a corpus, so here it is. This is a corpus of text documents. I don't know, I'll increase the fonts a little bit. Just pay attention to this paragraph of code. We'll pay attention to this. Correct. Now, what are we doing? This is a corpus. We agree this array of text, each text is a document. Nisarg, are you getting this? Yeah. Then we use the simple count vectorizer, your bag of words kind of vectorizer, and you fit it to this. So you will get a representation the way I explained it to you. Then you can do more. You can go to the TF-IDF, you can even do bigrams and so forth using stop words. Now here is the TF-IDF, it explains it, but forget that you already know the explanation. The bigrams and stuff would happen before, before vector no we haven't done that forget biograms for the time being they just add to the vocabulary just focus on the words now think about the tf id of vectorizer do you know there's a tf idf word is here and forget all this long explanation and what it does is it helps you create like actually hang on at some we need to look at some using sparse features um some code example code yeah vectorizer print vectorizer count transform. Okay, the example, this is not a very illustrative page. Let us go to another page that shows you a good succinct example. We don't want to read so much here. Okay. So look at this. You have a TF vectorizer. You can create a TF vectorizer here, found vectorizer and so on and so forth. Then how do you create a TF ID of vectorizer? You can again give the thing. You create the vectorizer instance. You haven't used it you have to fit it to the training data when you fit it to the data it becomes what you want it to be right so you instantiate it so now coming back to it this line where am I this line therefore what is it doing it's just instantiating your TF IDF vectorizer they call it the TF IDF model right and then you apply it you get the TF IDF on the bag of words it's just a strange syntax that's all it is a strange syntax that's all it is so i hope by now clarified so read the documentation in this it will clarify it it's just gen sim's own language the latent semantic analysis is the model the svd based model you create the model and again it has this syntax which is odd it's unlike You create the model and again it has the syntax which is odd. It's unlike scikit-learn would have written a slightly cleaner syntax, but for what it is worth, it is the creators of this library made it like this. So that's that. Now does it make sense? All you're doing is you're giving it the corpus to a model, to a machine learning model, to do unsupervised learning, to discover topics in it. Right? And then you ask it to show you the topics and it shows you the topics. Right? And it shows how the topics are made up of the words, the important words they are made up of. So far so good guys. So it's 10 o'clock. Let me wind this down. Nisarg, are you getting this? Yeah. Straight forward. And the same thing you could have done LDA from a coding perspective is just a change of word instead of LSI, you change it to LDA word. You change one word and you repeat the same thing except that these two algorithms are far different lda is a very powerful algorithm right then there's a measure how do you know how good your model is you can do the model for complexity now i invite you to read the documentation on this and on coherent score but coherent score is often used now this visualization tool is very good it helps you see how good a model you build it is okay okay i wouldn't call it terribly good you can improve upon it one and four seem to be well separated out two three five six are pretty close to each other then I suggested that use mallet mallet generally outperforms ginseng's built-in when you do mallet you get a separation like that which looks cleaner much more separated out and that is that guys that is it that's sort of your detailed walk through the lab. So guys, here's the thing. At this moment, I would strongly suggest if you have not given time to reading the Spacey book, you must. We are two weeks into the NLP workshop. I'm going to go into deeper topics now. I'm going to go into deeper topics now. I'm going to go into, next time would be the word embeddings in detail and then the week after that, the next three weeks will be all transformers because the world has changed. It's all about transformers now. And we'll be introducing more and more libraries and bring in hugging faces and so forth so transformers and we'll do a lot of code this this part two is all about hands-on coding so we didn't get time to get to the project maybe next time we'll talk about it on saturday we'll talk about it guys if you want to get most out of this workshop do the labs and do the project and read that book spicy book you can actually finish in two three days do it right or you don't like the book go to the website and just read the documentation it's very well documented likewise go to the ginsim website and read the documentation any questions guys by the way i get the i got the feeling that you all are struggling a lot today was it really hard understanding this lab any any feedback guys are not is the code looking straightforward I it looking very strange Rabin thanks for that feedback anybody else Shiva how did you find it it's not not too difficult. The only thing is lack of practice. That's what. Yeah, you need to practice it. it's not too difficult to get it. Yeah. Thank you. See guys, here's the thing. You guys are working hard, I'm sure. It takes, for me too, it takes a lot of effort. At the end of it, my real reward is, as you know, at this moment, this is a break-even business. It's sort of not really a for-profit business, right? It's because I like teaching and I've been teaching for 30 years. I'm just continuing that. But what is my reward? My reward is to see you guys succeed. And no matter how much I teach, at the end of it, it comes to practice. If you don't practice, you won't get there. So I don't get the satisfaction unless you guys succeed. And I can clearly see, I mean, through 30 years of teaching, many things I've taught, calculus, I've taught physics, and I've taught graduate courses in theoretical physics and nuclear physics and so forth, and over the years, and I've taught computer science, and I've taught graduate computer science and all these years of teaching the one constant that remains is ours generally people in engineering are very very bright their IQ is above that of the common population there is no doubt about that especially the sort of audience we have here by definition these you guys are self-motivated motivated enough to give your evenings away from your family and be here in the sessions and motivated enough to you know want to learn these topics that itself puts you in a a pretty top tier category so intelligence and motivation is not in doubt so the only factor that remains is the number of hours you put in and success consistently has been almost a direct function of only one variable number of hours you put into this so please put in the hours and practice then you'll get a lot out of this course and these workshops. And I would love to see you guys come January, be achieving your dreams, whatever brought you here to support Vectors, going out there and achieving it. Either moving on to interesting projects inside your company, looking for a job change, getting into graduate school, whatever your goals are. There are people who came here to start companies, and they have started companies, you know, one guy has his revenue apparently has already crossed 30 million. He started with me about three, four years ago, and so forth. So those are the successes you cherish, you you look forward to and that comes when you you put in the effort so please if i can say you guys we all live professional busy lives as parents as professionals so time is a rare commodity but see if you guys can devote more time to the labs it's very important do the book do the labs you'll get a then you'll feel very confident with all of this these are not hard it's just that we are moving fast and every week we introduce new things so it is new and by the time you catch up or try to catch up we already moved on to the next topic isn't it and the nature of this workshop is it keeps moving on relentlessly from topic to topic because we have the whole of ai to cover the neural network and the nlp and image processing and anomaly detection and time series we have a pretty large landscape to cover and it will keep moving on at a brisk pace and it's very interesting people who come to support vectors quite often say, why does it take three months or four months or five months to learn this stuff? Why can't we just learn it? And sometimes it's ridiculous over the weekend or maybe in one week or two week. And so there are people who want to learn it all in a hurry but at the same time it is a vast subject we are moving as fast as we can right i can see that this space itself is hard to keep up but you have to try to keep up otherwise we'll miss the topic all right guys that's all i have any questions any feedback otherwise i'll close for today we didn't go over the project right yeah we didn't and let's keep it it's 10 10 10 11. actually i need to get into my india meetings in a bit but generally i'd like to guys get started let's do it over saturday uh let's make progress on the project. And this Saturday, I was going to do the Java version of the microservice. You know, you often take code to production in Java or Scala or Kotlin or C or C++. These are the production level languages quite often. So how do you take a model, you have built it in in Python and then run it in a high performance environment? That itself is an interesting topic. I think somebody requested I don't do it this Saturday. You guys are busy. Do it next Saturday. But I'd like to do it next Saturday. This Saturday we'll do the quiz review. I'll release the topic modeling quiz and do that and we'll review that and then in the afternoon we'll give time to project reviews and let me help you guys with your projects. Does that sound fair? And then on Sunday we will do our research paper reading, the one that got postponed. All right guys, that's all I have. So unless there are more questions, I'll stop the session. Thank you, Asif. Go ahead. No, it's just thank you. Oh, thank you. Good night, Asif. Good night. Thank you, Asif. Good night, Asif. Good night. Thank you, Asif. Good night. Good night. Thank you, Asif. Good night. Asif, take care of your health. Get some more sleep. Yeah, get some more sleep and fluids. Yes, I should. You look actually sleepy, Proud. Yeah. Yeah. I'm sleepy. I'm sleepy. I'm sleepy. I'm sleepy. I'm sleepy some more sleep and get some more sleep and fluids yes I should look actually sleep very proud yeah how are you doing Patrick is your project moving along fine we were supposed to meet Tuesday as if but because of the it was Election Day we we were gonna meet on Friday so we'll meet tomorrow so we'll probably have we'll see what we can get out by Saturday I guess so I didn't want to ask the question ask but uh so Mike I had something just a simple question what if what if your word actually had more than one meaning but that is the problem that's why we'll go to the the problem with TF idea phase it doesn't deal with the context and the the different meanings of a word it's a very primitive tool right I mean 400 the first part we learned about word embeddings and and contextual word embeddings the transformer based word embeddings word based so we are coming to that in detail now see everything that we learn you know small steps first you have to learn the old way then you learn a more powerful way than a more powerful way so now i'm taking you slowly through that journey but if you go back and look at your notes of the one week that we did in nlp go back and look at your notes of the one week that we did in NLP. We went fast, but I actually took you through the whole journey in one week that we are now doing carefully in six weeks. I really appreciate that Asif. And your explanations are really, they're very good enough, they're profound enough and easy. I wouldn't say simple, but it can carry me through coming from a not so math heavy background. So I'm really thankful that you're able to teach it that way. Yeah, so, yeah. And for me to ask that question about the multiple meanings, cause I wanted to reference the word jump in our last quiz, right? You had jump as a noun and then jump also as a verb. Yeah. That's what I was gonna ask in the bag of words, because if it's context aware, it'll be able to pick up both the noun and the verb, but it didn't sound like the bag of words or the TFIDF was at that level where you can discriminate. Yes. Yeah, now thanks for clarifying, Asif. And yeah, I'll finish the book over the weekend so I can catch up up to speed. But I've been looking through the code and it's been very helpful, Asif. Thank you. Sure, you're welcome. And please take care of your health, Asif. Please do. Okay, thank you so much. I won't bother you now. Anyway, good night. Night. Yeah, please do. Okay. Thank you so much. I won't bother you now. Anyway, good night. Night. Good night.