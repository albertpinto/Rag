 What a guy so we have gathered today. It's Monday evening. Today we have a theory session. To recap what we did last week. We focus much of our attention on topic modeling. The basic premise was that if you have a document which contains a lot of words, a corpus of documents, there is a reason why certain documents contain a certain distribution of words. And the reason or the sort of latent factor responsible for that particular distribution of words and documents has to do with topics. There's latent factors we call, I mean, we identify as topics. And therefore we learned a series of methods to find these topics. When we discovered the topic, if you remember the topic space was a much lower dimensional space. A document is a fairly high dimensional object. If you use TF-IDF as we did in the lab, we were still looking at 20, 30 dimensional TF-IDF vector. When we do the things we are going to do today, which is a new topic, also along those lines, still it's a big vector, but the topics are like 5, 6, 10, 15. So you reduce a document to a very small number of topics. The gist of what we said, what we found last time was that a document can be considered a mixture of topics in different proportions. And any one document, so for example, one particular document, one particular blog may talk both about sports and politics in equal measures, but not about STEM fields. Another blog on STEM fields, on neural networks may be completely dedicated to, let's say STEM and so on and so forth. So documents have different distributions of the topics. At the same time, a topic is characterized by a certain unique distribution of words. For example, and the intuition is that if you look at a topic which is a stem field, then the word integration, differentiation, probability, vector spaces, they're pretty common at least in the mathematical fields and in physics and in computer science and electrical and all of the engineering fields. we use these words quite commonly, but we don't find that these words are all that common in politics isn't it so we they are used occasionally but rather infrequently and sometimes they have different meanings. So that is the idea that a topic is characterized by a sort of a specific distribution, probability distribution of words. And so we now have an interesting thought. You can represent a word with respect to topic as again, how much it contributes to each topic and a topic has a distribution, a unique probability distribution over words. And a document is very simple. You can reduce it to a very low dimensional vector over the topic space, right? So the topic space is a latent space of a small number of dimensions and the documents are vectors in it. So I will illustrate this by just summarizing example. The basic idea is a document. Initially, you take a document which is made up of many, like, for example, if you look at a tf idf vector it's a long vector right it's a it's a very long vector of length whatever length it was and number of words in the vocabulary but when you represent the same document in terms of topics anyone this thing gets reduced to just a certain probability distribution. It's 90%, let us say stem, right? 7% politics, maybe 3% sports. You get an idea. So for example, a document I can be represented as a document i vector, but it could be represented in terms of the topic space. So let me just use the word x. When you represent it in the x space, this is the x is the tf idf space, then it is a pretty long vector. But when you represent it, let's say D is Z, Z being the latent topic space, and I'll just write it here, in terms of a topic vector, the same DI, Z being a topic space, then you realize that the same document can be written as just some small numbers. It becomes 0.9, 0.07, 0.03. This one standing for STEM, this one standing for politics, and this one standing for sports. Are we together guys? And likewise, there is a similar telescoping effect from the space of words. So you have a large number of words. All of these words are there. So any particular word, if you're looking at any of these words, you can map it to a certain distribution. So for example, a topic, if you take a topic, topic X, topic A, it will have a unique probability distribution of the words, isn't it? So it may be like the word, a topic is stem. So the word here, gradient, let's say a gradient word, it will have a value, gradient descent tends to happen a lot in neural networks. So maybe it will have a probability of 0.01. Whereas the word president would have a frequency of 0.00001, right? Something like that. It is far less frequent. A gradient would have a one-person probability of showing up in a stem field, what a president word might have or something like that. Or let me just make it even more specific. If you take a word like protest, President of the United States, that would have an extremely low probability of showing up in STEM field documents. So, in other words, a topic itself is a distribution over, is a specific distribution of probability distribution over the words. Does this make sense, guys? It's a very simple and intuitive idea of all that we have learned. Now, while this idea is there, then the question comes that this is a good way to conceptualize. But what does it mean? By the way, I just wanted to mention something. You will notice that I often use x vector to what is manifest and z vector for what is latent. You'll see me use that quite often. I'll use x vector to the observable. Or input. And z vector, I'll use. z variable variable I'll use for latent space representation. Here the latent space is of course the space of topics. So that was it. Now we learned a few ways to do topic modeling, two basic ways we covered in a lab and you know stuff is one is to use the LSI right latent semantic indexing it's also called latent semantic analysis what does that use that uses the notion that you can look and look at this it directly does a matrix factorization. All right, so we learned of a few methods. One method that we learned about was LSI or LSA. What this does is it looks at a direct matrix, a document matrix, document one to document N, and then it looks at word one to word n. And if you really think about it, each of these, basically you have a TFIDF vector. So the entire corpus can be written as this thing, D vector, D1 vector, D2 vector2 vector d3 vector and each of these d vectors is of course a tf idf row vector right so your corpus is basically corpus is basically a rectangle a rectangular matrix of n cross m. Let's say that you have m dimensional or sorry this is a m words you have in the vocabulary. m is equal to the size of your vocabulary and n is the number of documents, number of docs in the corpus. So if I write this matrix as the C matrix, then the basic idea is that this matrix C, corpus matrix, can be decomposed using singular value. Composition, singular value decomposition and what it does is you can write it as u d v transpose and we won't go into all the details all the details of this suffice is to say that the basic intuition a simpler intuition is if you look at eigenvalue for square matrices, you have eigenvalue decomposition and what it basically does is that it's sort of an any matrix m will take a isotropic noise and it will shape it to a particular distribution, a particular way. And when it does that, there's a little bit, we don't need to worry about it. These become your principal axis, U1 and U2. The axis of maximum variance become your eigenvectors. So singular vectors are also like that. They are the equivalent for rectangular matrices. And so what happens is that they give you the inner content, this D, and the singular vectors of which a U matrix would be made out of singular vectors U1, U. Let's say the number of topics is UK if you just keep K components. So these are all singular. This matrix U is made up of singular vectors. And these vectors are essentially your topics. They represent your topic space. And so in this topic space, you can now put any document and say, look at this. This document, which is at this level, um this document which is at at this level this document diva di is now existing in the topic space of the latent factor space so guys do you remember recall that we talked about this last time is it looking familiar to you guys? Please give me some feedback. Yes. Yes. This is all looking like a review of what we did. Then there is another thing which we didn't quite get time to cover. And today I'm sort of still wondering whether I should do that or not. Time permitting, I'll do that or I'll do it in a separate session. The other thing we did is latent dirichlet allocation. We did that in the lab but I didn't quite explain how it works. It doesn't do a straightforward linear matrix decomposition of the corpus matrix. It takes a slightly more sort of a generative programming approach or a Bayesian approach. It basically says that, see, there is a reason, there's a particular distribution of topics, the words in the topic, a probability distribution. So there's a certain probability distribution for a topic, let us say. But then this distribution itself, we need to take from a sort of, we need to take this distribution from a distribution of these possible distributions we need to sample off distribution from a distribution of these possible distributions, we need to sample off. So I won't go into too much, that gets rather complicated. A distribution of a distribution is rather funny. So I'll give you a very simple intuition. Suppose you have a fair dice and you keep throwing the dice and you keep looking at the distribution or the frequencies of one, two, three, four, five, six, you'll get a certain frequency distribution which would be uniform for a fair dice. And now you make it slightly unfair in one of these directions so that the probability of one is a bit more than the probability of the other and so on and so forth so old dice for example they tend to roll in such a way that some things become more likely than others so that now the distribution has changed and so if you think of a a box of many dice of different degrees of wear, each dice having its own distribution, then you can ask yourself a probability distribution of producing one, two, three, four, five, six. And now you ask yourself, what is all of these dices put together? What is the distribution of the distributions that you see? What is the distribution of the behavior they produce? Do most of the dices tend to be fair dices or do most of the dices tend to drift towards that distribution which favors six? Things like that. You see that, right? So you're asking a second level question, a distribution of a distribution. That is what Dirichlet tends to get into. But at the end of it, it also finds the same things, the latent factors, which are the topics. So we won't go into that, but I would like to generalize this idea of latent factors with a certain diagram. And we'll talk a lot about it from a deep neural network perspective, because obviously this is a deep neural network workshop. Later on, you know, the topic modeling was a good segue, a good introduction to what we are going to talk about today, which is all about bottlenecks. Bottleneck-based algorithms, I call them. So let me talk a little bit about it. So that's the topic for today. Think of it like that. So let me set the agenda. What are we going to talk about today? We're going to talk about bottlenecks and their value. In certain things we'll create a bottle, when the information flows, we'll create bottlenecks and we'll see what is the value. We'll talk about something called auto, a principal component analysis. We'll talk something about auto encoders. Remember, I promised that this is a topic that I will cover. In particular, I will talk about variational autoencoder. Variational encoders tend to be a little bit math heavy. It's not very hard math, but I will see how far we want to go today into the mathematics, but I'll take you a little bit far. You should have some idea about the mathematics of the variation encoded. Then I'll do the word embeddings. And we'll do a few word embeddings. We'll do the C bar, the skip gram, the skip grams, and maybe the fast text. These all come under word to vec. Then we'll do glove. So we have a relatively long session today. So I guess we need to budget our time and move forward. We're going to do all of this. So these are all still context free word embeddings. Now, as we make progress, I will talk about certain other things like a transformer based word embeddings, are quite important and we'll also talk about universal sentence encoders and so forth so those are coming down downstream but today agenda for today is this that's what we're going to do so uh with that agenda and so i'll break it up into small sections because we are covering a lot of territory. I might break it up into maybe one hour sections so that we can move fast. I mean, and keep those things separated out. I'll also break the video into three videos so that we can review them separately. So imagine that you have, so the basic intuition is what happens if you pinch, imagine that water is flowing here, water flows, right? And you somehow create a bottleneck, a really tight bottleneck here. And then the water goes out. So here water goes in, water goes out. Of course, you know that if you're a bit familiar with physics, all sorts of magical things begin to happen. So for example, the velocity will increase here. Things will have to go faster, and then it will again slow down into all of this. But when you create a bottleneck, so for example, suppose the water is dirty and you have a very strong bottleneck and this water contains big pebbles if you were to think about it. You would agree that those pebbles will probably not make its way forward. They will get stuck in the bottleneck. Well, maybe i should make the bottleneck even more pinched like this so they will sort of hold the pebbles back are we together and then let some things go forward so this captures one of the information or one of the intuitions that we are going to use in a little bit as information flows. Information is like water that flows from here to there. And we will try to capture that. So the basic intuition I'm seeing is you can actually get more at the other end by putting a bottleneck. You take normal water and you get filtered water here in some sense. Are we together guys? So this is one thing you can actually get more, more than the input by putting a bottleneck. Are we together? Another way to think of it is quite often the water that we drink and so forth, I don't know if they're still doing it, they used to typically go through some sort of a radiation, electromagnetic radiation. I think it used to be UV light, very strong UV light goes through water filters. And so the water that you get is now better in some sense. So that intuition is the sort of the physical intuition that we use by creating something interesting. So imagine that you have a vector. A high dimensional vector. It doesn't matter what it is. Let's say that you got data which has features x1, x2, xp. So you have actually, let me just say n because I'm using n above, n dimensional vector, right, and you just have this data, right, and so you have a x vector which belongs to n dimensional Euclidean space. This is your data vector. Somebody has given you a data. It may be label, it may not be label, it doesn't matter. given you a data. It may be label, it may not be label, it doesn't matter. So maybe it is label. Maybe you're looking at, for example, the breast cancer data that you guys have encountered in previous homework. Maybe it is that, and this is why is whether or not the patient has cancer. And these are features. So it may have y or it may not even have y, it doesn't matter. The important thing is that there's a feature vector here. So you take a vector and in a classic neural network sense, the way you feed a vector into data is each of this input, the input layer is x1, x2, X3, X4, all the way to Xn, right? And then what you do, you create a second layer, a neural network layer, but let us say that this layer, I deliberately create, let me just for the sake of argument, take only two layers. So what will happen is all of these will feed into this and all of this will also feed into this. Are we together? And now what you do is you created this, but this is not your output layer. This is something very interesting. Now you create an output layer of the same size for whatever reason. And you say, and we'll see why. X1, X2, Xn. And you do the same game. You span it out, you fan it out back to this. And we'll see why we want to do this. So you see that you have created a big bottleneck in between. You say that I want to get the x vector in and now it will go through some transformation. There will be, so for example, what will happen here? There would be a sigmoid, some sort of activation of the, whatever the weight matrix is, w dot x plus bias, right? The sigma 1 and sigma 2 will come out here. Are we together, guys? This is standard neural network. I'll call this A and this B, or one or two, one, two, you would agree that the Z1, that the input to A neural network is just W associated with all the Wi of the input, one, right, x plus, what is it, v1, right, xi. So, so far, so good, guys. You agree that this is what it would be? The output, and then the output of a a node is sigma of z1, the activation function of z1. This is the activation function. Now you may wonder where am I going with all this, but imagine that you have a simple dense network like that, a very simple dense network, and we are looking at the first hidden layer would you agree that this is true and the output of b would be likewise the output of b would be would be likewise c sigma 2 where sigma sigma z2 where z2 is equal to wi2 xi plus b2. Right? So this is the weights multiplied across them. Are we together guys? So far so good. Anybody has a question here? So in vector notation, if you write it in this notation of matrices, which is not really necessary, and you write the x vector as x, so then this becomes w dot x. you write the x vector as x so then this becomes w dot x z is w dot x z vector is w dot x and then the output is the sigmoid of z output output vector coming out of these two nodes is the sigmoid on the w transpose x which being the z in terms of matrix notation but let's just take the we have just put two nodes so let's just stay with the indices so the meaning is clear someone had a question so far so good guy so guys uh please stop me because this is all a review of the dense nets we did long, long ago. Is this clear? Are we all together here? All we have done is we have created a hidden layer with just two nodes and we are just looking at the output of those two nodes. Now comes a crucial thought. It says that, let's say that there is another matrix here, W prime, right? And what it is doing is it is applying itself to Z, another matrix. Remember that I put a prime, or maybe I should use some other word instead of W. We use W for weight. So W prime, what it is doing is it is producing a vector of the same dimensionality as the input. So let me just call this y1. I mean, see, this is essentially your... Let me actually not use x1s here. Let me call them y1, the output of the y1, y2, yn. And you know, these are your predictions, right? Remember, these are your y hats. So let's think through what will be the output here. These nodes, they are, if you think of it as your standard softmax, right, if you want to, or you can just think of it as your, just lean, straightforward without any activation, it gets multiplied by w. So it becomes z. It becomes z. Look at the information flow. Right? w.x. I'm just ignoring the indices here. Right? Let me just write it in terms of a dot product. So far, so good, guys. And z gets activated by the first by the activation functions in the first layer so you get this you get something actually um that the trouble is in the world of auto encoders you don't use you use z for the hidden representation itself. Okay, we'll stay here, z. And then this goes through another matrix multiplication, w prime sigma z, right? And of course, you add the bias term, sigma z plus a bias prime. And that goes through yet another activation, w of sigma z plus b prime do you see that and this is your this is your y vector guys i hope this flow is easy because this is a flow that is at the heart of what we are going to talk about today. X vector goes in, it just gets multiplied by some weights and biases, you know, weights and biases act upon it to produce this, which gets activated by this and then it moves forward and forward and so forth. forth. So the only thing I'll do here is I'll remove this. I will just say that the z I'll remove because in the literature of autoencoders, they use z for something slightly different, wx plus b. And in fact, the activated version in this world, this is your hidden representation or latent representation. And I'll tell you why we call it the latent representation. And then this becomes, once again, if this is z, it goes through it goes through WZ, the hidden value plus this, and then it becomes, therefore, it becomes this weird thing that WZ plus B. It gets further. I think a quick question here. So this intuition is relevant only when there is a single hidden layer, correct? A single hidden layer. In the beginning, assume a single hidden layer. Okay. We're not generalizing at this point. We can generalize from that, and we will in a little bit. But to understand it in the beginning, just look at this very peculiar neural network I created in which instead of putting lots of nodes and lots of layers, we have put actually a hidden layer is dramatically simpler than even the input layer. It has just two nodes or three nodes or some small number of nodes. Are we together? Okay. Right. So if you think about it geometrically, what we have done is, if you look at this representation, this space, this is your, this Z is basically the made up of Sigma one, Sigma two activations from these nodes A and B. So geometrically, what you're saying is a point, a X vector, which belong to a very high dimensional space let's say let me take a number for you let's say that it was in 100 dimension space and you have suddenly and z belongs to r2 in this particular example because we just took two nodes and what we are doing is we are projected so this is your the two components of z actually let me just write it as z vector is made up of z1 z2 right so this is your z1 axis this is your z2 axis so every point has gotten projected axis. So every point has gotten projected. This has become some point z, right, belonging to R2. Now nothing special about 2, by 2 I want to convey a small number. So what do you notice happening? This network, if you look at the hidden layer, the output of the hidden layer, it represents a lower dimensional space. Isn't it? It is taking a vector in a very high dimensional space, input space, and it is somehow squishing it down to a very low dimensional space. Now, why we are doing it or what is the best way to do that, we'll come to it in a moment. But assume that we are doing it, and now we are doing something else now. This part people often call the encoder part, E. And this people often call the encoder, E and this people often call the encoder decoder for reasons you'll see in a moment. This is sort of the vocabulary that people use sometimes, but it is not true for all situations. That's why I was avoiding to use this. But all right, let's keep it here. Now you realize that you're producing a y, which is you're re-inflating the y, the z, you're going to y hat. Y hat again belongs to the n dimensional space, right? The original big space, the output. Now you can say, what uses it, how can this be useful to us? So let's see how can it be useful to us. Suppose let's look at this situation in which we say that the activation function, so you know one of the things is we are using activation function on w transpose x plus b. You realize that right? The Z, the internal representation is the activation of this. What happens if we set the activation to be one? Activation one means sigma of X is like sigma of W, identity actually, not identity, not one. W transpose X plus B is equal to just W transpose X plus B. Means this multiplying by sigma has no meaning. It's an identity operation, right? So then what happens is, think about what we just did. And I'll now write it as this. We took a vector, X vector, which was pretty big. And we took a small two dimensional vector, which is Z. And we sort of came here and then we re-inflate it back to Y. And now comes the interesting thing. If we say that y should be, y should tend to x, right? In other words, the output should be. So you say, well, let's think what in the world is going on here. You're taking x, you're multiplying it by transpose x, you're getting z. you're multiplying it by transpose x, you're getting z. And then you're multiplying by w prime, w, w prime, w prime z. And you're saying that this should be, this thing that you get, y, is equal to this. And therefore, it is equal to y prime w transpose x, right? And you're saying that the y should be the same as x, right? The same output. So one way to do that is, one easy way to do that is, you make the loss function, basically your sum squared loss. You basically say that if x goes in and y comes out right this is my loss loss function your standard sum squared loss you say that i was expecting x to come out and whatever comes out the the deviation from x is the error. Do you see that guys? So you try to minimize this loss function. You ask for arg min w, w prime, that will make the loss function minimum. So remember this loss function is a function of w and w prime i've just subsumed the bias terms also right so what happens when the loss is zero for example guys or close to zero what what you have done is you have created a lower dimensional representation of x is very small actually it won't be zero but very small isn't it now now you say well there is something fishy going on how can you compress a n-dimensional vector into just a two or three dimensional vector won't you lose information right you're compressing it into a lower dimension information. You're compressing it into a lower dimension. So that compression has value. It's a lossy compression in many more situations, but the loss is not necessarily actually of information. Sometimes you actually get more structure, more value from it. get more structure, more value from it. Because what it does is, X has some inherent structure that you don't see. And pushing it through this bottleneck forces the neural network to discover the essence of X. See, if you think of it that you have, you know, think of milk and powder, milk powder. If this were to be milk, one analogy, very rough analogy of it that you have, you know, think of milk and powder, milk powder. If this were to be milk, one analogy, a very rough analogy I'll give you, if this were milk and this was milk powder, and then this is a dehydrate, the powder, small amount, and then this is rehydrate. Do you see the analogy here so that you get back your milk so what will the process of dehydration do it will evaporate and take the water out and keep the rest of the nutrients do you get that intuition guys and in many ways it's a compressed it is just the milk compressed a vast quantity of fluid compressed to a small can of powdered milk. Am I making sense, guys? Yes. So that's the intuition. Now, it turns out that if you use a, and I won't go into the full mathematics of it but what you what you just did is principle component analysis please go ahead the dimensions you mentioned of x and y they are same right they are same okay in fact you expect them to be this otherwise you won't be able to subtract one vector from the other. So they are the same. And so what you say is that if I try to minimize this loss, what will I have to do? I'll have to shake this w and w prime, isn't it? My gradient descent, it will have to keep playing around with w and w prime in a nicer way until this loss becomes minimum. I can pass a lot of x vectors into it, and I keep expecting the same thing to come out. But what will come out is something different. Because in the beginning, your weights will be initialized to some random values. random values, but gradually as you back propagate and back propagate, go through epoch after epoch and then step after step of back propagation, gradually these weights will align themselves in such a way that this will approximate zero, right? So this will start approximating zero. In the ideal case of very clean data, and let's say that you somehow could do that, In the ideal case of very clean data, and let's say that you somehow could do that, what does it mean? What it means is that these W prime and W, they are basically inverses of each other. And what you just did is rediscover principal component analysis. Now, PCA, if you remember, that's what we used to do, right? We would take data and then we would try to represent it in a lower dimensional manifold, lower dimensional space. In the process, we were willing to lose some information. Guys, do you remember this part? this is it so what we did is that so now that is with the unit identity activation function but now suppose i change the activation function to something else i still keep this i have this part input x and so so guys i hope you get the message that what you're doing is you're compressing you're getting the essence of the vector into a lower dimensional representation x goes in there's a hidden latent representation latent representation and this blows out again to this if I expect that y is approximately equal to x then I have initially I have created a very compressed representation of x because now what what can I do suppose I have to ship x I need not ship x because x will eat up a lot of space. I can instead ship the Z. So long as I remember my weight matrix, I can just ship Z and then apply the inverse of W. You know, if Z is equal to W dot X, you agree that X is nothing but W inverse of Z. Right? So I can just do the inverse operation to always recover back my initial X. You get a lot of compression here. Now, what is the value of this? Try to relate it back to, in some sense, your topic modeling. And now, well, OK, so one more thing I will do now. So the same is true, even if I activate it, apply an activation function, right? So then what will I have to do? I will have to do something like W inverse of activation inverse of Z, right? If you look at this equation, hang on, let me write it in big bold so that it's clear. Remember guys, this is our activation symbol, right? w dot x. And I've forgotten b here, the bias term, I've sort of ignored it for the time being. So if you reverse it, you can get back your x, so long as you have an invertible activation or some way to invert it and so forth. See, logically thinking, you may say, well, ReLU is not invertible, but actually there is some function that approximates the ReLU that's invertible and so on and so forth. So you can compute this. And so you can get back to your stuff. Am I making sense? So you ended up with a latent representation of it. Now this looks like Wyler. This is great. Isn't it one of the things we were trying to do? Take a document which is very long, somehow compress it down, and so on and so forth. So that brings us to this very interesting intuition now. So anyway, before I go there, I'll summarize it. So w is sitting here and this is w, x, w dot x. Let me just use the dot notation because I don't want to keep putting transpose. And let me bring up the z. Now you notice that z here, the way I'm using it is a little bit uncommon. Typically you say z is the before the activation part, it's just w dot x. But in the autoencoder literature we tend to call the activated w.x as your z. So now if we are doing it, what does an activation function do? Activation function introduces a lot of non-linearity in the data, isn't it? Because your z1, suppose you have z vector is made up of z1, z2. Let's say we are looking at only two nodes. And by the way, now from there we can generalize it to more nodes. And this is equal to activation of W1i Xi plus B, the X vector plus B, dot B. And this one is sigma W1 again i dot x i plus b this is w2 xi plus b2 and b1 right so this is again your two-dimensional representation now we can generalize it to higher dimensional representation but this output is a non-linear this nonlinear distorted form, nonlinear transformation of x. And you know why we need nonlinearity. Remember all of our statements about universal approximations, and so on and so forth. So you just introduce a nonlinearity. and now you can go back and do further steps and try to recover back your initial thing. Now what is the intuition behind it? The intuition is unlike principal component analysis or SVD which are linear transformations you know matrix factorization is at the end of the day a linear transformation. The fact that you are putting an activation function, you are now opening up the interesting world of possibilities. You're saying that there may be a nonlinear transformation going from a mapping, going from high high dimension x dimension to the latent space right a very very non-linear function but we can discover that which is a much more general statement are we together it is just taking your dimensionality reduction like pca etc or yes and generalizing it to it so let me me, we can just consider this. And so this picture is sort of now from two, you can go to many dimensions. Let me go to P dimensions, P dimensions, right? So you can say is Z belongs to R to the P where P is a small number. X belongs to R to the n, and then you expand out, and you get something, y again belongs to rn. And this thing is called an autoencoder. Are we together? Now, we'll deal with different variations of it. The most simple variation is y output is the same as input. The most simple variation is why output is the same as input. When you put this constraint, then this whole picture, this bottleneck picture becomes the autoencoder. Are we together, guys? Now you can say, what can I use it for? You can use it for many things. Let me give you some idea. One, of course, is that you can compress an image if you so want to do something like that a very specific domain you could do some any any form of data that you have you can come up with a lower dimensional representation of it with the auto encoder isn isn't it? Am I making sense, guys? You're doing a nonlinear dimensionality reduction. That's all you're doing with the autoencoder. Next, what else can we do? You can actually use it for very interesting purposes besides dimensionality reduction. So dimensionality reduction is one, which is no-brainer. Or compression. Data compression. But the other interesting thing is you can use it as a denoiser. What about the denoiser? What do we mean? Suppose what you do is when you feed in X, you feed in X a picture. Let's say that you feed in a nice picture of a house or something like that. Or maybe let's take a digit. Let's say that you feed it eight but what you deliberately do is you add some kind of a noise some kind of a noise it could be uniform noise it could be normal noise noise and then you train this machine to this auto enc, but you put a very interesting requirement. Even though you added noise to the actual input, y, you keep your loss function still the same, which is x minus y square. And so what will it do? If you do argmin, argmin, and let me now write it as z. You're trying to find the best representation of the w. OK, actually, let me, I'm tired of writing w, w everywhere. Can I just write theta is equal to the w, w prime. And if you want to explicitly put your b's also b, all the parameters as one theater vector so it is just a way of representing it nothing fancy so. If you do this now notice that you're you're doing a very peculiar game you're taking an input adding noise to it. And then feeding it, but what you expect is clean output now if you really look at it this is a standard loss function you can apply gradient descent to it and so what you will end up doing is you will end up with something very interesting. You'll end up with the noise filter. This auto encoder will act as a noise filter because once you have trained it, what can you do? You can give it a noisy data, deliberately, and you expected clean outputs. So that's exactly what it will try, that's what it will start doing, it will start cleaning up your data. In very literal terms, the way it works is, for example, you know, when you do handwriting recognition or a page of text, when you scan a page of text, let's say that the page has folds to it, and it even has a coffee stain, or it has something. And you're taking that image with substantial amount of noise in it, stains and all of the creases and so forth. One easy thing is it will just take the page and create another image for you, which has none of the creases and coffee stains. It will be a very clean sheet of image, page. Guys, I'm not getting any feedback, but I hope you are understanding what I'm saying. I just made a very simple statement that you can use an autoencoder as a denoiser. So Asif, let's say for healthcare, when we have a condition and we want to look for like what are the most pertinent like laboratories or like x-rays or like what specific procedures we want to do to be the most minimal in finding a diagnosis we can use this right like because sometimes yes you could see the latent representations that you get it contains the essence of the information so by definition, it will throw out from the X vector. It will ignore or weigh down or not consider too much the those factors that don't matter right because, in many ways, is just a fancy evolution of PCA. You know dimensionality version of PCA, dimensionality reduction using PCA. I see. Because in the US, they tend to order a little more aggressive. But in third world countries, they have to be a little conservative with the costs. So maybe having this on top of your EHR or your healthcare records, you can find out what's the minimal cost to treat the disease or to find what to diagnose the disease. See the trouble is, Patrick, that see it's a lower dimensional representation, but it has been mashed out of potentially all the dimensions of the X vector. Sometimes you're lucky some of those x vector, it's called the, by the way, in PC of world people call it the loadings. How loaded is each of the initial x vector in representing the PCA, you know, the z. And you throw away those pieces which don't contribute much, those dimensions. People do that sometimes, but see, I know where you're getting at. What you're trying to do is you're saying, can we just find out what really matters? It's a slightly different topic. That's a topic of, it is a topic of feature importance. So suppose you're prescribing 50 blood tests. Dr. G R Narsimha Rao, And you can't afford to, you can afford to only, and let's say that all the blood tests cost exactly the same amount, a lab work costs exactly the same amount. Dr. G R Narsimha Rao, And these days, I suppose the amount is $1,000 for anything, any blood tests you do, you get a thousand dollar bill and this insurance and you hope your insurance covers it. So let's take that. So then you're saying that suppose a patient has a budget of only three, which three to pick? You can find that because now you're doing, see you can't use PCA for that but you can just use feature importance. that but you can just use feature importance right and feature importance is something you have not learned with me but you'll learn in this workshop how to find feature importance are we together yes you can therefore find out what's the relevance in making good correct diagnosis how much does each test really matter. It's a slightly different. The trouble with this kind of PCA is it mixes up the axes mixes of the features. Vaidhyanathan Ramamurthy, And by the time you get to auto encoder with its nonlinear transformation, it becomes hopeless because now it really is a mashup. Vaidhyanathan Ramamurthy, Of the input space information in the input space of the input space, information in the input space or the feature space. So this one asks if you have different X inputs, but they all carry, let's say, a certain weight that helps impact your lower latent space, then it doesn't automatically get thrown out by the autoencoder. No, autoencoder won't do it. It will instead create an alternate representation of three dimension or four dimension or two dimension that you are searching for that best represents the input. OK, I get it now. That is a .. I also have one question. Go ahead, Swayed. So continuing on Patrick's question, so is autoencoder a viable alternative to domain intelligence where we, what domain intelligence entails is obviously the feature importance and prioritizing few features over the other. So does it in the industry people work with that or no? Saurabh, not really. See you're not doing feature importance of the autoencoder. What you are doing is though I mean so okay let me hold it back. We'll have more to talk about it in a few weeks but in its simplest case, the way I've presented it, it's not doing feature importance. It is mashing up the entire X vector and squeezing the lemon and then basically saying, okay, here's a condensed solution. Okay. That's what it is doing. And the beauty of that is see, ultimately in all of neural networks and machine learning boils down to what is your loss function. If your loss function, if you can state your loss function, you already have a winner. If you look at the denoise, for example, you're being very clever. You're taking a real input, adding noise, and then asking it to produce a vector without noise. noise and then asking it to produce a vector without noise. Yeah. As if maybe one way of saying it is, feature importance is supervised, whereas autoencoders are unsupervised. Absolutely, that is. Or yeah, autoencoders are self-supervised. Right? The feature importance, when you when you say, as Jenna pointed out, it is in then it is supervised learning right you're asking each of these blood tests how much does it matter in predicting the output right in making the diagram so then you can talk of feature importance but you cannot talk of feature importance in the abstract it has to have a target a purpose which makes it uh something that you do with supervised learning. Okay. Asif, how did we eliminate the error or the noise in this one? That's the thing, Rafiq. What happens is, you deliberately, the kind of noise that you expect in real life to be there, you take clean data and you deliberately inject that kind of noise and what you do is you expect y to look like x so in other words you expect this entire thing to behave like a noise filter and if it if it is not able to remove that noise you you have a large penalty you have a large error term and you then back propagate the error and as you keep tuning this network basically right in some sense like if you think of signal processing it begins to act like a low pass filter right so in this case you have to know what the uh what is the x or the pure x you're expecting oh yeah because you know that you have a data set right you have a data set that you're training on so let's take an example suppose you have lots of you take image net or some other lots of animal data animal pictures right and what you do is you feed a lot of noise to it and then you feed it in but what you expect is those feed a lot of noise to it and then you feed it in. But what you expect is those clean image animal pictures to come out. Then what happens? Now you trained it on very clean images. Right. But when real life you deploy it in the field and you get these very grainy pictures taken from cell phones, right? It will clean those pictures. It will denoise those pictures. And that's why this autoencoders are actually being used. As you probably are aware these days, the photography, which used to be the art of painting with light, there is a huge mathematical aspect now, computational photography. So the pictures that you take from your smartphones, they are no more the reality. They are augmented reality in many ways. They have gone through many of these mathematical operations. Have you noticed that nowadays if somebody takes your picture it looks better than you standing in front of your mirror? So that's sort of the world we live in. Now there's another use that you can do and I'm coming gradually towards NLP but let's take a detour here. You take a picture and you have a picture of a cat right and since I can draw cats very well here it is. I'm kidding. So suppose you have a picture of a cat. One of the things you can do is you can take and you can flag out certain sections you can say i won't show you you deliberately mask mask random samples of the picture you pass it through the whole thing this auto encoder thing and what you expect it to have done is you expect it to have discovered the picture without the mask. So this is picture with the random mask and this is literally fill in the gaps, fill in the gaps. It needs to be able to produce, these are the whiskers, here's the body, four legs, right? I should have four legs and one tail. Okay, there you go. You got back your cat fully, and there is no mask there. So you could continuously train, feed a lot of pictures in which you deliberately apply a mask and you tell this autoencoder to learn a way to fill in the gaps. Right? And it will learn to do that, except that the autoencoder that we'll talk about is a little bit more sophisticated than the basic autoencoder that I drew. So now let me make it more sophisticated. See, I drew the picture, the basic autoencoder is this input. It goes in somehow it becomes Z, right? X goes in, becomes Z and then Y comes out, right? And you expect Y to approximate X and z is the latent representation representation so now comes the interesting generalization it need not be just direct one hidden layer you could have actually many hidden layers all helping you all helping you do that. Typically you make it symmetric, right? So what happens is x goes in and it goes through all these layers, it reaches here, and then it re-inflates itself through all of those layers. So what happens is you have a deep deep net and you typically mirror it here. Are we together guys? So no reason why you should have only one layer. The important thing is the pick the middlemost layer and whatever representation, whatever output your middlemost layer shoots out, you consider that your latent variable, the latent representation. Well, I hope this looks easy to understand. Like once you understand autoencoders, now you'll realize that this entire mathematics, nothing is affected. If this is the function f of x, right, then and this is a function g of z and you get y is equal to some function g of z and z is some function f of x. I mean ultimately all of these layers need to represent the at the end of the day they are just some very complicated mathematical function transfer function isn't it? X gets transferred into Z, Z gets transferred and mapped in back to Y, right? And there are certain functions that do that. So it doesn't matter, you can have one layer or many layers. And then whenever we think of these layers, what kinds of layers? The question is what kinds of layers? So they can be dense layers, but nothing prevents you from putting convolation layers. And pulling layers, etc. All of that also, nothing, nothing stops you. Pulling layers. So you could have an input. And then you remember our image processing, the things that we did. We had all of this conv layer followed by the max, some sort of a pooling layer, followed by the activation. So see, just as a recap, we had filter, conv. So one conv layer traditionally we thought of it as applying the con filter followed by pooling followed by activation right of the output this is how information flowed and we called it one column con player. Right, so nothing prevents you from between the Z layer the central layer and the input you can put dense layers. You can put convolution layers. I haven't seen anyone put actually, you know, there must be somebody wants to put transformers attention layers also in there. I don't know. actually you know there must be somebody must have put transformers attention layers also in there i don't know um so you can do you can do that so you have convolational auto encoders right denoising auto encoders and you could have even sparse auto encoders so one of the one of the more interesting auto encoder architecture is suppose you have x here and you want to get back your X. You want to create a sparse representation, you can actually do this and do this. And if you are clever enough, you will end up with counter-intuitively a very sparse representation of x that also works even though you're putting a layer with many many things in between now there's one catch to this kind of architecture that i want to mention suppose you have x and you put more than that let's say that you put two suppose this has um 10 dimension and you make it 20 dimension let's take an example and this is somehow okay why why you're producing it suppose you want x to come out and you're training this training this neural net you're not careful, guess what will happen? You know your network will learn to memorize this x value and just translate it, just identity forward it from input to output. Do you see that guys? One easy way to reproduce why, if the number of nodes in the hidden layer is more than X, right, to get the same vector as X that went in is just to do no transformations, just to pass it forward through the hidden layer. Do you see that guys? A very simple statement I'm making, right? So you can have these many, so these many layers are just doing identity transformations and these many layers are just flagged off, switched off. So your network effectively becomes like this. Data goes, x goes to z, goes to x, where z is nothing but x itself. Right, so you have to be, so this is one of the reasons you find mostly that the central layer is a bottleneck, so that it actually learns something. You know, it learns a representation. You say that it learns a latent representation of input. Now, what does it have to do? Our NLP, you'll see in a moment. So this is a 30. There's one more more I will talk about one more kind of auto encoder. It turns out that these auto encoders, these basic auto encoders, they have a bunch of problems. What happens is that, so suppose let me just take the latent space made up of two dimensions. This is Z1, Z2. What will happen is, suppose you are doing, let's take the, you know, the MNES digits or something like that. What will happen is the area, the points where you have one and the points that you will have nine, the points that you will have three. You see that there's a lot of overlap. It's hard to tell in these overlap regions whether it represents a 1, 3, 9 and whatnot. That is one problem. The other problem that you have is, if you look at this space, there is a lot of empty space here. Where it is not able to make any prediction of what that point is. Right. So, for example, this point represents nothing. Right. Represents. Oh, this is multicolored. Nothing. I should take a more sensible pen. Let me take, what should I take? Okay, I'll take something like this. It represents nothing. So you won't be able to tell if you're given a data that looks like this, what is it? some issues with that you have holes in the latent space you have overlaps and so forth so the original auto encoders are used but there is a more powerful version that removes some of its limitations and that more powerful version has an additional quality it is a generative model now we encountered a generative model remember GANs encountered a generative model remember GANs? Do we remember generative adversarial networks that we did? Yes sir. What was it? It was able to now start producing all sorts of fake images and the same thing is that we are going to make a generative model. Let me take this a generative model generative autoencoder these are called variational autoencoders and what they do is something pretty interesting. So it's a little bit of a mathematical theory, guys, what I'm going to talk about in the next 10 minutes. I'll simplify it down, but if you don't get it, just ignore it. Just think of them as a fancier version of the autoencoder. So suppose you have this X going in. What happens is that it produces a z. And so then obviously, there's a restoring part of it and so on and so forth. So the way you look at it is you say that suppose x went in what is the what should be the z signature so what is the probability of z given a certain input so suppose the input was the letter eight right what should this z value look like ultimately that is your pursuit right that is what you want to know that is what you want to find out by training your neural network. What is the best representation of z, right? And so, what is the probability of z looking a certain way given x when it is fully trained, right? Now, this is a little bit of biased theorem, etc. I'll just mention it to you what it is. It is the probability of x given Z times probability of independent probability of Z times probability of X. What are these strange things? This is just the, if you look at all possible values of Z, like for example, do you notice that in this space there is a lot of concentration here, a lot of concentration here, a lot of concentration here, a lot of concentration here, but almost nothing here. So in a way the probability of z gives you the shape of data in the latent space. Let me use the intuition. Shape of data in the latent space, in the latent space, latent representation space. So that is that. Now, if you pick a point, this part is also asking a simple question. Given a point, a point z belonging to your latent space what was x right and so you can figure that out ultimately that is the whole point of this game because uh from here you're projecting back to uh back into uh x outside so this is this part of the journey right you're trying to do so this also is fine you can train it the problem is what about this guy what is the probability distribution of the input itself that you can't find okay because you can have more and more and more input and this quantity actually is more complicated it is actually a probability of x given Z times probability of Z for all possible inputs of Xi. And in the more general sense, this becomes an integral. It becomes an integral and this integral is intractable. Intractable means you cannot solve it in a closed form. So just take it as a fact that this denominator in all of Bayesian statistics, this denominator is very problematic. It's called the evidence, the probability distribution of the evidence itself. It's very prob- it's sort of very hard to ever calculate. So you can calculate, you can train the first one, you can see the second one, you can do all sorts of things, but you get stuck here when you think about it. So there are two ways that people deal with it. One is something called a Markov chain monte carlo remember monte carlo do you guys remember gibbs sampling we did it in the engineering math gibbs sampling hist histing and all that testing algorithms and so forth so so we did all that MC MC right you try to just randomly sample from a posterior. Raja Ayyanar?nilavakal, and figure it out what it is by doing a particular remember the random the walk the drunk person's work that we used to talk about in engine, that is one approach, the other approach you do is something called variational inference we. Raja Ayyanar?nilavakalational inference. Variational inference. And variational inference is where, is the word, is the magic thing that gives this kind of encoder its name. Variational inference. Now what is variational inference theory? What it does is it's a very clever thing and it will be very reminiscent of the GAN that you said. You basically say that that suppose you can't find something. You can't find probability of X given Z. This is something you can't find, calculate. It's hard. So what you do is, you say that I'm going to just take another distribution, some completely different distribution, which I know how to calculate. Right. And let's say that I'll make it out of bell curves, Gaussians, which is the typical thing you do. Gaussians. Bell curves, you know, lots of bell curves and just decide where the bell curve is. So what it basically means in simple terms is I'm going to do this game. You remember that I will just put some bell curves centered around this location. You know, centered around this location. So this will be my digit eight. This will be my digit three. This will be my digit two. Bell curves. You remember how bell curves look like? The bell hills, right? They have a location, a center, a mu vector, and a sigma vector, the variance vector, a standard deviation. If I can give the location of the bell curve where it is and the sigma actually the right term is big sigma because in higher dimension you call it the big sigma it's the same thing standard deviation there's actually a bit more to it but okay we'll we'll just leave it as that right so this is two and this is let's say seven right So what you do is you take a Q distribution that is centered around some random points. And then what you do is you train this neural net and this is what I'll say is that suppose Q could be a high fidelity proxy for PX, then the problem is solved right then you have solved the problem of computing this posterior distribution and i won't go more into it but just say that how do you make q look closer and closer to p so that brings us to a little bit of a bit of theory that anyway is overdue, I should teach. See, I have used the word entropy. I've used the word information. And I've used the word KL divergence. So what we will do is after the break, I'll explain these three terms or review these three terms. We have sort of done it in the past for some of you. We will cover this and then we'll use it to explain this concept of variational autoencoders, at least in brief. And then we'll move on and see how all of this is applicable to our space of NLP. So I'm going to pause the recording for a moment. Let's see if Prachi did get to manage to come in. Yeah, I'm fine. That's fine. No problem.