 I'll get started. This is our first lab walkthrough and the data set that you got, the river data set, in the beginning it must have looked rather simple. There are two predictors, two features, x1 and x2, and there is a target variable, t, which is binary, it's 0 or one likewise the flag-like data set is very similar except that it's tri valued zero one two or the three values that you see so this data set if you visualize this data set and you do exploratory analysis which i hope that far you all reached did we reach that far just load the data do a summarization of the data visualization of the data you know the exploratory data analysis a check for missing values right so when you do that then when you visualize the data the first thing that surprises you is this this data has a true decision boundaries it seems not one isn't it there is a to draw it out it is the data looks like this suppose this is your data you have sort of if I remember right it is something like this like this and this is a little bit bigger I suppose and there's blue points at this blue river sort of close through that and then you have the other points which are here does this look like a data date this is a data and so how do we solve this you know what are the problems with this data the first thing we notice problematic is the decision boundary is nonlinear. What's wrong with this decision boundary today? Boundary is nonlinear. You also notice that there seems to be two decision boundaries, isn't it? Where are the two decision boundaries? Let's draw those decision boundaries out. So one decision boundary is here. Isn't it? And the other decision boundary seems to be here. So now comes the problem. How do you solve for this? Once you visualize the data and you think about it for some time, you know that you need something more powerful than just a linear classifier, a classifier whose decision boundary is straight. Even if you could somehow straighten the data, there is also the fact that there are two decision boundaries so how do you conquer this so we are going to take a methodical approach through the data i hope all of you have tried and if you just try so the approaches that you can try is a simply you can try is a simply apply logistic regression to the input data later as it is. Hang on. So we will see whether, let's see if the model is good. We have a good model. Good. Have a good predictive model. What do you expect? Do you think that you'll get a good model here? If you directly apply logistic regression. Remember the logistic regression is something that looks for a linear decision boundary, right? It expects something like this. And this is the distance from the dx is a beta naught plus a beta dot x. This is the distance from the decision boundary. Just going back to your basic theory of logistic regression that we did. And the basic idea is that the further away you are from the decision boundary, the more sure you are that it is a positive case versus a negative case. So the probability Vaidhyanathan Ramamurthy, That the given point x Vaidhyanathan Ramamurthy, X. Vaidhyanathan Ramamurthy, Is that a given point X is log 1 minus PX, right? This is equal to the distance from the decision boundary of that point. So how far it is from the decision boundary, that's an intuition. If it is close to the decision boundary, clearly we that it is we have a sure answer for example we used to take the example of I believe blueberries some the sorry Blueberries, yeah. So suppose this site is all blueberries. And what should I say? Some sort of a red berries, whatever those may be. Raspberries or something like that. Suppose those are sitting here. We do know that occasionally there are mistakes. Obviously decision boundaries are never perfect. Data is never perfect. There will be things that will creep across and so forth, things like that. But the further you are from the decision boundary, the more sure you are that it is like this. just to recap see if you are here if you are at this point hey you have it are you pretty sure that it's a blueberry you're pretty sure right on the other hand if you are at this point B you are not so sure because you know you can see that there are a few this red berries also here on this side of it. So B you're not so sure and if you are at a point like C here then once again you're pretty sure that it is a red berry isn't it. So we use that to motivate ourselves. We said how far you are from the decision boundary determines whether Sajeevan G. It's a blue. It's a red berry or not like the probability of a very is that so far so good, guys. This is just a quick recap. But the point was that the decision boundary was linear in the case of In the case of this, it was, sorry, I tend to mix the colors. It's a linear decision boundary. But when we look at, in this problem, do we see a linear decision boundary? We don't see a linear decision boundary. We have multiple problems. We seem to have a nonlinear decision boundary. At the same time, there seems to be too many decision boundaries one too many there are two decision boundaries so when you apply logistic regression so now I'll ask this question that directly when you apply and that is what I ask you to apply do you think that directly applying it to the data would give good results no sir no it wouldn't we wouldn't expect a good result results because a logistic regressor directly will try to regression classifier directly would try to find a decision boundary and what it will eventually do is the best decision boundary it will find is way beyond the whole data set so it will declare all the data set probably as the majority data right so suppose it draws a decision boundary beyond the scope of this data then everything by default gets colored or marked or predicted as a majority now it turns out that the yellow yellow points are the are twice as many as the blue points so you will get about a 66 percent accuracy now 66 percent accuracy may make you jump into life like if you're not careful but you have to and this is one of the things we'll do will we will ask this question when do we have a good model are we together and do we have a good model or not so that is it so anyway moving forward the second approach we will take is we will take an intuition so let me give you the intuition suppose you take the intuition that see hey I have a way to do this I can treat this beyond the data I can pretend that this thing turns around right and this thing turns around what about now now we have a single decision boundary isn't it if you pretend as that or hypothesize I I imagine that beyond this data, the decision boundary curves back and meets itself. You would agree that you have one decision boundary, which is a deformed circle, isn't it? Are we together? Yes. So then we are only left with the problem that it is a nonlinear decision boundary. Given the fact that it's a nonlinear decision boundary, we can say we have one decision boundary, but it is nonlinear. How do we solve that problem? We can remember, how do you linearize a problem? One easy way is to go to a higher dimensional space. Go to polynomial degrees, right? Go to polynomial degrees right go to polynomial space go to if you remember i keep saying that for every data there which can be sold there is the hope is if it can be classified there must exist a higher dimensional space some space in which the data is linearly separable right and the decision boundary is a hyperplane is a simple plane so that now the easiest thing you can do or the easiest way to go to higher dimensions is just to go to higher polynomials. Are we together? Higher degree polynomials of the features. So if the features are x1, x2, you can also start looking at x1 square, x2 square, x1, x2, and so on and so forth. Once you go to the higher degree polynomial, we may end up linearizing the problem. Assume a bend, assume that we have a single decision boundary, boundary by imagining a bend in the data, imagining the two boundaries meeting beyond the data set, beyond the feature space. Feature space, data, data features of space where the data exists, right? I don't know beyond the bounds of the data beyond the bounds of current data So this part of wrapping around comes from the fact that you notice that we we are claiming that These things are bending right just outside they're bending over right and so if we take this that will be our second approach we can go to a few high degrees of polynomial and we'll see what sort of accuracy it gives and then we can do a third approach which is that and which is the main approach i would like to which is that and which is the main approach I would like to sort of bring to bear. And the lesson here is that when you get data sets like this, let's go back to the data. When you get a data set like this, your first reaction should be, can I visualize something? Can I impose a narrative from my own experience? Just imagine something. So there are many many things you can imagine. One easy thing that I take in this particular case is you could imagine for example that this is a river flowing through. The blue is the water and the yellow is the sand. So if you think of this as a river then you would agree that you can draw a decision boundary the decision boundary that you can draw well let me take a color that is visible what would be that color on this background color that's visible could be green is good green okay let's take green so we'll take three yes imagine the center of this river you realize that if you think of this blue as water, then it makes sense that how far you are from the river. Let's say that if you are at this point versus this point. This point A is blue, B and C, let's take C, they are yellow. What can you tell about about the relationship of a B and C with respect to the green line with respect to the center of the river is a is nearer to it yes you can see that points that are very close to the center of the river they tend to be blue and at some point the water finishes on both sides the water finishes and then you can have all the other points which are sufficiently far from the center of the river there is no water anymore it is just sand right so let us use this intuition to build another model and why do we do that we We are doing it just to learn that sometimes you can feature engineer from the data by thinking about it. This is deliberately a toy data. It's a didactic data to bring home a point that by playing with the data, you can impose a narrative. You can do some feature engineering and that feature engineering, and that feature engineering often can lead you to success. So the third approach would be feature engineering. In the feature engineering, our steps will be first draw the center line. Centerline of the ripple. How can we do that? Find fitting a line to the data usually is regression. And so it's a paradoxical situation. We are going to first use regression instead of classification, even though the problem is classification do that and be feature extract actually we have used ABC so let me use the step says I feature extract the distance from the center from the center line and then the third part is after that build a simple classifier simple logistic classifier in and let me call this the distance as a D, right? In just one variable, one feature, right? From D going to T. Do you see that? Based on how far you are from the decision boundary, you can, in a sense, tell whether it is a river or sand, or sand so this is a basic intuition I would like to walk through now this in the code and so I'll switch over to the other machine any questions before we start guys does anybody have any questions? I have a question. Yes, please go ahead. So it's a, I mean, from looking at the data when you plot it in SMS plot with a hue, the ones are all falling in a sine curve, but you have chosen a center line instead of a sine curve to extract the distance from it. Is it the first step towards going close to the sine curve or the idea is when we have this curve this thing see there are two ways of doing with it you can think you can say i don't recognize a sine wave a sinusoidal function so then you go to polynomial space right or you can be smarter you can say hey you know what i can see the sine wave. You remember that in ML 100, we did that. So if you can guess what function it is in the lucky situation, then you can directly model it. And in fact, not in the Python, but in the R version, I will show you how to, and we have done this before if you remember this river is nothing but your data set 2 from ml100 isn't it if i could extract the river and just the blue part it just becomes like a data set data set 2 which is the same way kind of thing. So you can model it with the same way, or you can model it with a polynomial. Either is good. So sir, I can do log transform and feature engineering, but what is the best approach? Like you said polynomial is okay, right? Do we have like auto model where it can recognize that this is a, you know, it will automatically take it to higher transfer right see this entire course is about a ways to algorithms that do the feature extraction for you right today is the in a way we i'm saying that uh before you get too comfortable with those algorithms, I wanted to teach you the lost art of feature engineering. It's something that is very essential. See, algorithms at the end of the day, we will pick up somebody, some researchers will do deep research to discover an algorithm. It becomes after that implemented software library. Then at the end of it for us to apply as we were looking at it on Sunday, it's just a couple of lines of code. And those code now will often do a lot of things. But what happens is that you lose something along the way. Sometimes you lose interpretability, as we were talking about, and you lose something along the way. Sometimes you lose interpretability, as we were talking about, and you lose something. So the gold standard is, before you hastily go and use one of those black box models, it is perhaps useful to see if you can think through the data and do feature engineering. Thinking through the data and doing free changing is the gold standard and if you look at the people who do excellent work Or win competitions and so forth They often invest most of their time in feature engineering and data cleanup data preparation and feature engineering It doesn't sound very exciting. People get very excited. Oh, I used the transformer to solve some problem in natural language processing or something like that. Those are hot topics. Yes, there is a place for that. But my philosophy is you should progressively go to more complex algorithms because more complex algorithms have embedded complexity like biases that it may not be informing you and also you lose sometimes the legal footing so for example in many situations you are in US any kind of prediction that you make that affects the protected classes you know the gender the age race and so and so on and so forth immediately you're in violation of the law and companies have gotten sued with this in this respect so So, we should always try for interpretability and feature engineering is your path to it. To keep modeling simple, use simple tools and do the model. I'll tell you a story. I suppose this is a side. So guys, forgive me for going on a tangent. I'm into photography. So, in Silicon Valley photography you know this we are all engineers most of us a large proportion of people are engineers and engineers tend to be gadget sort of gadget fields they have a love affair with gadgets camera is something that most engineers tend to splurge on so if you go to the beach will not these days in a COVID but normally when you go to the beach you can always spot this newly minted photographers you know people with a big camera fancy cameras walking around. So they'll buy an expensive camera, the latest and greatest and god knows 50 megapixel, whatever it is, and they'll start taking pictures. Quite often they'll just leave it on the automatic mode or something like that. So somehow the instrument is supposed to itself produce good pictures. After a little while you get disappointed the pictures don't look good, right? In fact sometimes embarrassingly your cell phone pictures look better because in cell phone there's a lot of computational photography a lot of software trying to make your pictures look better whereas a professional camera will give you exactly as it sees so you get frustrated then you ask around okay what happened why why do my pictures suck? After a little while you get an idea and you say, oh, it must be the lenses. After all, it's the lenses that bring in the light and the image. So you can see people splurging on expensive lenses, Canon L-series lens and so forth. And you can often spot them long big white colored lenses or cream colored lenses often on the beach and what you can always tell that somebody is still learning because in the middle of the day he'll be asking his sweetheart or children to go pose in the water or something like that on the beach now how do you know that these people are novices? There's actually a very good tell, a very good state. A good photographer will never take pictures, rarely ever take pictures in broad daylight. If he is forced to take it, he'll do all sorts of extra lighting support like flashes and so on and so forth to get rid of glare. Because in broad daylight daylight we all develop stark shadows on our faces. So good photographer I mean especially if you do landscape you want to take pictures of people in landscape and so forth but you want to take it either on a cloudy day or you want to take it when and the sun at sunrise or sunset but not during the day so anyway the joke goes that the photographer the nervous photographer is still puzzled and wondering what happened to this camera you have a great camera very expensive lens and by now you're close to six seven thousand dollars shot and still no good pictures and so the joke is supposed to be that finally it dawns on the person that it is not the camera and not the lens but it is the tripod enlightenment is he will now go in by the biggest tripod his spouse was willing to carry because of course he is carrying all this heavy equipment himself. So anyway, so why do I mention that? Tools. There's a parallel to this in machine learning, a strong parallel. These days there's a tremendous amount of research and there's an explosion in newer algorithms, more and more algorithms. They all have a place under the sun, they all have a use. But you need to use them judiciously, just because you pick a powerful tool doesn't mean that you will get very good results. The way to get good results is thinking about the data, feature engineering as much as you can, cleaning the data, preparing the data, and going about it. Some people say in view of this way complex algorithms future engineering is dead. Right? Be that as it may, you know, if you use any one of those algorithms, let us say you use a big deep neural network, you lose all interpretability. People are doing a lot of research to regain approximate interpretability by making linear models that approximate those networks. But it's a problem. The gold standard is see if you can simplify the problem. Look at it in such a way that the entire problem looks simple. So for example, the photographers who go and do National Geographic you know those masterpieces quite often they will go into culture into a country with just a simple camera a good camera professional camera pretty old like it might be a few years old and their favorite lens which would be many years old a couple of lenses at most and they'll walk into the wilderness or they'll walk into some country or wherever it is and they will come back in a few months with absolutely stunning pictures that make their way into National Geographic and our pictures we are the blokes who go in buy fancy equipment obviously none of our pictures ever make it to National Ge. So in the similar way, I try to use simpler equipment and put in your expertise, deepen your expertise so that you come up with good results. So that was the main lesson that I wanted to bring about today. Anyway, just a parallel to this story. So all right, after this digression, let us come back to the data. What I'll do now is I'll stop sharing on this screen, I'll go to the Linux and I will talk about it there. So remember, we are going to take three approaches, direct logistic regression approach, which we don't expect to do too well. We can do a polynomial approach and to linearize the data and a feature engineering approach. So let's do that that's the school and here I leave it as a exercise for you to go repeat yourself because I haven't seen anyone do his submission for the other data sets you can try it out of the other data sets there's a question. Please go ahead. Yeah, this is Aritra. So my question is like for the center line, right, that the green line that you had drawn. So for using regression, you assume only the blue water data, right? Yes, yes. So you will basically? Do a extraction you will take a subset of the data that has only absolutely you hit it on the head So let's go and see that Okay. Okay. Thank you. Let's do that. So I'm going to now share So, how do we submit homework? You just said information. Dr. S. Mthuvudana S. S. Yes. I would suggest is till your homework becomes polished, don't export it to Kaggle. Especially don't make it public because you don't want to do that. So do it in Colab and share it with me that you can share your notebooks with people okay so you okay so you want me to share my uh exercise with you on google not not necessarily on google there are ways of sharing kegel directly yes i think in hang on let me verify that i'm i'm just wondering you you mentioned two three times so i was thinking how do I send you the homework like okay? Why don't we do it together? We'll figure it out together. Okay, girl. It seems like we have to paste all the lines of the code At least that's what it seems like to me. Yeah, that is the thing but you can select with whom you can share that Yeah, that is right. So let us go there into kegel to colab and suppose i'm in colab i'll just take a random uh example uh intro to pandas let's say this one and do you see the link at the top here saying share right you can share warning share will contain you so if you do your own thing let's say that we create your own new notebook here we go i created a new notebook and now if i do a share right you see it gives you means to share it you can get a link or you can add people gives you means to share it. You can get a link or you can add people directly by email to share it. So just share your collab with me. Yeah, that's fine. But I'm doing an R so just I will send you the R markdown file. Yes, perfect. That's perfect. By the way, you can do R also in collab and it's one of the things that playing to you and Julia too and Julia too yes we'll do it in Julia Julia I don't know if collab supports yet see what happens is I tend to spend a lot of time on a local Jupiter or in a notebooks on the Google notebook which is more high-powered up is something i like but you don't end up spending a lot of time there uh because it tends not to be very powerful for you have linux server so i just put the docker anaconda docker and you know we can run jupyter lab on that i'm running it jupiter on docker running it so I have that so my environment is completely set up for deep learning deep neural nets and that's where you say you're seeing I don't know if you're seeing my entire Linux I can share my entire screen but at this moment here seeing my notebook screen let's go through that so is the text looking big enough or should I increase the font size? Increase it please. Yeah, a little bit of height. Let me. For the private notebooks, how do we share with you? In the collaborator, I couldn't find the name. In Colab, you're talking about Colab, isn't it? No, in Kaggle. In Kaggle. No, in Kaggle. I think a little harder to share. Maybe there is a way to share. You'll have to add me as a collaborator. Yeah, what's the name in the drop down there's an option like number of people it shows like which um you have to select that particular person you want to share it with and you can do that okay so yeah work with price no what's the name i search for okay let's take this offline why don't we do this at the end of this class? Let's take care of the cable and the collab issues. Right, let us give some time at the end and we'll do both of that. Prachi, would you like to take that session at the end? Uh, yeah, okay, please do that. So here we go. Uh, hang on let me share it again. So, uh, is the text bigger now? I'm trying to make it much bigger. Is it big now? It's good. Yes. So, all right, if maybe too big I might be too big isn't it okay I'll make it slightly smaller so I can myself see some code all right so when we look at this what we'll do is we'll take the first is the imports. So let me go over the imports carefully. NumPy and pandas. What do you think NumPy does by now we all know it gives us the linear algebra, the matrices and matrix operations. Then what does pandas do? It gives us the data frame in Python. In R, of course, it comes built in. And then what are the stages that we take the data to? We need to pre-process the data and we'll talk about it. One of them is you standardize the data. It is always a good idea to standardize the data. You don't need to standardize the data for logistic regression, but for all situations, you should standardize the data. So let me talk a little bit about standard scale. See, suppose you're taking features of an elephant, right? So the age of an elephant may be like 20 years or 16 years, but the weight of the elephant, be like 20 years or 16 years but the weight of the elephant if you're especially measuring it in ounces it will be like what is it tens of thousands of ounces isn't it are we getting that if you go and weigh an elephant and the unit of measurement is ounces it would be a massive amount so the unit of measurement of each of the observations like you know the features in the observation affects the data now also there's a wide uh sort of skew some things are in millimeters for example if you look at the temperature of an elephant or the temperature of either a human being, it is within a very short range. Most people have temperatures close to 98. They may go down to 96, 97, which is getting bad, or up to 100 and 405, and that's also getting bad. The numbers are within a very narrow range. On the other hand, the weight of an elephant could have a very wide distribution. And the unit of measurement may be huge. I mean, the number of pounds, the values may be huge. So one of the things you do is you standardize the data. So if you subtract the mean of the data, and I'm just reviewing what we did in ML100, if you subtract the mean of the data, and I'm just reviewing what we did in ML100, if you subtract the mean from the data, the data becomes center aligned, isn't it? So all the values would be around center. Means, for example, for temperature, 98 will be recentered to zero. Any positive value would mean temperature above normal, and any value below 98 would mean temperature above normal and any value below 98 would be temperature below normal, below average. So that is the first thing you do, you center the data. After you have centered the data you still have a problem. Your data is too wide in some areas and it contains the units, you know, your ounces and this and that. And a lot of these algorithms, they don't like data to have so much variation from feature to feature. One is in millimeters, one is in decimals, and the other is in tens of thousands and so forth. It's cues of the learning process. So the next thing you do is you scale it down to a standard to a unit area there are many scalars one scale scaling that is very popular it is called Z value you can not only take a data subtract the mean from it but you divide it by the standard deviation when you do that your quantity now your number becomes dimensionless. There are no dimensions. The ounces disappeared. And when you Z value data, even if you had measured your data in kilograms, you would still end up with the same Z value. That's the beauty of it. It becomes dimensionless. The second good thing that happens is it obviously brings it down to much closer to zero so that the values are typically between minus three and three, but beyond three are the outliers. Beyond absolute minus three or three are the outliers that you have. So that is the standard scalar. Subtract mean and divide by standard deviation. It's called z value some of you may recall you may might have done it in your high school or college textbooks but that is not the only scaling you can do min max scaling what it means is that you can subtract the minimum from any data and divide it by the range min minus max is the range so then your data will all fall between 0 and 1 Right if you are at min Your value will be 0 if you are at the max point your value will be 1 and all the data will find it Will fall in the unit interval and so there are other scalars a scaling scaling means to scale or normalize the data. Interestingly, once you have done the modeling, sometimes you can squeeze the last bit of accuracy, or last bit of juice by, for example, playing around with different scalars, different ways to normalize the data or scale the data into a standard representation. data or scale the data into a standard representation. So this is if you're not used to scale is that is what it is. So the standard scale is as the value Z value things. Now remember one of the approaches we will try is the polynomial features, right, but we will expand into a polynomial space. Where higher degree polynomials can do that. So these are pre-processings we need. Or we need classifiers. So we'll only use one classifier, the logistic regression. Now, you remember that in one of the approaches, like the feature extraction approach, we are also going to use regression to find the midline in the river. So I have included the regression library also. Now, when you have logistics, when you have a classifier, the measures of a classifier, as you know, are confusion matrix. In other words, if something between a cow and a duck, how often did your algorithm confuse a cow for a duck and a duck for a cow right so that will be the off diagonals in the confusion matrix and the principal diagonals are the values that it got right and so that's the confusion matrix based on that there's a classification report based on prediction and what the value was So, the classification report based on prediction and what the value was, that report has now in classification, there are multiple measures. There is accuracy, there is precision, and there is recall. And then in different literatures, you use different words. There is specificity, there is sensitivity, there's type one error, type 2 error, and so on and so forth. There's the F-score. There are many, many metrics. They all come by comparing what was predicted to what was the reality. Here, we'll take a few of those and look at it. And the next thing is something called receiver operator characteristic curve, ROC curve curve and the area under the ROC curve that's what these two lines refer to now if I trust you you know from prior ML 100 what these are but those of you who haven't taken at some point will take a remedial session walk through this project would you please take us explain it to people yes would you like to explain these things if people need some explanation of the ROC curve in area and I would you rather I did it I would rather you did it like over the weekend some some telecom in which we will cover these things next F1 score is just the of the precision and recall. Yes, I've got it. One second. So coming back now, going forward, linear regression, what do we look for linear regression? We look for the coefficient of determination, the R square, and we look for the coefficient of determination the r square and we look for in regression in general if it is not linear in general we look at the mean squared error how much is the mean squared error residual error that is left behind and the average of the residual errors residual squared errors or you can take the square root of this then it becomes a root mean squared error RMSE so that those are the libraries now going down little bit matplotlib inline it just this is a statement saying you don't need well okay all it means is that the show the graph, any plot without needing to have the word dot show explicitly stated. Any plot you shouldn't have to call show. Just assume that it is there and show it in line. Then the And just show it in line in this notebook itself. This is the normal plotting library in Python. Matplotlib is the default library. Good thing is that it's quite powerful actually. Bad thing is that a lot of people complain that it's not very aesthetically pleasing and there are better libraries out there and so forth. So Python has now almost a zoo of visualization libraries. And people who love to do data visualization, they get carried away and they do absolutely stunning data visualizations. They become very familiar with libraries. They write their own libraries and contribute back to the open source. It's a wonderful world out there in the visualization space. Visualization brings out the best of creativity in people in many ways. So let's start, if you're not familiar with this, I would say start with matplotlib. It's the default library. When you use a matplotlib, then import this library called Seaborn. Seaborn is a much aesthetically more pleasing library. Now, even if you don't use Seaborn, include Seaborn, because what it will do is it will go import Seaborn, because it will go and override some of the matplotlib defaults. And so matplotlib itself will start looking nicer. So it is worth doing. The next is colors. If you want to give the colors, most of us do web pages. We get used to giving colors in hex form, you know, hash something, something, something, something, or hex form, and different forms, ASCII forms and hex forms and so forth. So if you are in that mood to give it an ASCII or RGB values and so forth. This library color helps you. I don't think I use it in this notebook, but I tend to use it. The other thing is nowadays, and this is something most people don't know. I think it's fairly recent. Pandas has Plotik built in. And when you plot with Pandas, one of the things that happens is you can choose a back end. You may choose Matplotlib or Bokeh. Bokeh is another exciting library, interactive library for data visualization. Or you can choose Plotly. Plotly is a commercial thing, but it has an open source counterpart. I think I like the lightweight version of it. So that, I don't know whether this lightweight or complete, I am not too familiar with Plotly, but it looks beautiful, is based on D3 visualization. So you can specify those backends and therefore your visualizations will then automatically start using those backends. So it's an interesting thing. Now, this is very basic. I like to have my pictures in landscape format. Why? Because ultimately I have to, you know, I give you guys notes, written notes, but there's a few who have been looking at ML 100 tabular data, the PDF. You realize that all of these things make their way into a chapter in the textbook. So having it in landscape format means I don't use up a lot of vertical space that's that and then this is basic form setting now plotting style there are many styles of plotting some people like dark background some like white background some like slight gray background and so forth and so there are many many styles if you go and look at the matplotlib styles it's a zoo and people are writing their own styles so by all means pick your style I like ggplot because it gives me a uniformity with the with R and in R ggplot is the reigning champion of sorts. A very, very good visualization based on the grammar of graphics searches. The rest of it is just setting some attributes. Now, what- Asif? Yes? You mentioned about using like yellow brick as like one of the libraries. So you're not using that for this one? Yes, I will be. But there is a delicate reason i have not included it at the top the reason is unfortunately but their function names are exactly the same as a scikit-learn function names so if you if you okay whichever you import last takes over okay right it says anyway that's the reason you'll see that when I use it at that moment now latex I mentioned this fact is that at the end of the day once you have done your analysis you need to tell a story because that is the story behind the data so to tell the story you need visualizations you need plots and you need good writing. You can fix your writing but you also need good visualization. Having professional quality visualization makes a world of difference. When you take your show on the road and try to convince people. It's very jarring. It's like if somebody were talking to you in English and the English had huge amounts of grammatical mistakes. It would be annoying. Or like, for example, I'm an immigrant. So I assume that the native speakers in US, when they hear my English, they must be getting annoyed that I put accents or stress on all the wrong places. So it's like that and bad graphics is somewhat in the same genre, you know, people tolerate it. It's not that they're lenient and they'll acclimatize to that and so on and so forth. But generally it doesn't look very professional. It's certainly curable. Like for example, I cannot possibly cure my accent, but I certainly can make better graphics and make sure I don't have grammatical mistakes. So in that same way, spend some time becoming good with this. So now I'll move a little bit faster. This line just suppresses some unnecessary warnings when you run. Otherwise your Jupyter notebook gets littered with all sorts of output and that's not pleasing to look at. So what do we do? We first load the data and we drop any rows that have missing values. This is not obviously the best idea. I did it because I already knew that they're not likely to be too many missing values. Generally, you should load load the data you should do something called missing value analysis see how many missing values are there in more complex situations then you should do imputation you should impute try to fill in those holes in the data by imputing some values sensibly but that is a whole thing in itself when you do boot camps with me you go through a lot of rigorous sort of practice with those sort of things, but not for today. Here we are first learning to walk before we run. So I'll just not go into that. I've been a bit sloppy. Just forget about the missing values kind of thing. Describe the data. So this describes the data and what does it see it has three columns these three columns now I already told you that the the data has T is the categorical variable right even though it looks like a number it's actually a categorical when you see that the mean is 68% it means that 68% of the values if it is 0 and 1 68% of the values are 1. So suppose I build a classifier in which I declare all the values to be 1. I would still be right 68% of the time, isn't it? So my baseline classifier just picks the majority and declares everything to that. It's also called the zero R classifier. So it forms a benchmark, a baseline. Any classifier you write must beat the baseline classifier. So to illustrate the value of this with a little analogy, see, why should i not be impressed if a classifier has 68 percent oh what number is good number people ask in the accuracy of a classifier first is accuracy in itself is not a good measure you have to take it in tandem with other things recall and precision they mean a lot for example if you're diagnosing somebody if you have a diagnostic tool and you're looking for let's say indicators of breast cancer or prostate cancer you want to not miss any positive case so then the recall as a metric becomes more important if you are finding that you're lost with these words please go and review chapter 4 of your text book. I wouldn't have time today to review all those concepts but that's that. So recall becomes more important than accuracy. Now speaking of accuracy let me give you a narrative. Suppose you have a tool and you you're screening people for let's say something pretty malicious let's say some form of cancer breast cancer whatever it is and most people will be healthy because if you're random if you're doing a random sample of the population then you wouldn't have that right so they wouldn't have that maybe one percent or less will come out positive. 0.1% will come out positive, let us say. So let us say that you have your very well researched, carefully crafted tool that can classify a person into healthy or unhealthy. But there is a quack next to you, who obviously has no tools, but whosoever comes to him, he says, my dear friend, you're perfectly fine. Go home. I tested you and he'll make a big show of testing, maybe draw the blood and then just throw it away. And then he'll declare you to be healthy. Now what proportion of the time would he be wrong? If you think about it, he would be wrong only 0.1% of the time. He would be right 99.9% of the time. And yet you would rather not go with the quack. You would go with the proper diagnostic tool which catches the problem when it is there so the other metric recall is more suitable to that things like that so there are many metrics of uh classifier to judge a classifier based on context's precision and so forth as we move forward uh make sure you review it and again I'll give some session so that's that so that's about accuracy in fact the history of before modern medicines came about I would say that the history of most of those ancient forms of medicine were often oftentimes not always they were very effective in quite a few chronic cases but for acute. I don't think they were very effective and a lot of the time a quack would just sit down and declare everybody to be healthy. Vaidhyanathan Ramamurthy, He would be he would come out like a genius, because the human body has an amazing ability to heal itself. amazing ability to heal itself and so the outcome used to always be aligned with his prediction so anyway that's a denigration now coming back to this data so one tool that I would like you guys to use is called pandas profiling it does in an automated way it does your exploratory data analysis you can build your you know the histograms etc etc But just with one line of code, you can generate the entire profile report. So let me show you what I mean. Look at this report. And I wish this was this could be opened in a new page. No, I don't think it can be opened in a new frame. So you notice that it gives you a statistics of the data. There are three variables. These are the number of observations and the Boolean variable. There are two numerical value and one Boolean type. So it was smart enough to detect that T is actually a Boolean type, 0, 1. It's a binary type, which is good. The variables, it shows you the histogram of the variables. It shows you the mean and all the statistics associated, mean, minimum, maximum, and so forth. And you can even look for details, more details. If you look for more details it will give you all the quantiles do you see how uh how uh detailed it is it will give you the media and and those of you if you want to brush up your statistics this would be a good thing see if you can explain the meaning of each of these variables we don't have time so i wouldn't go into basic statistics but here it is and the same thing for x2. We can toggle the details and see it. A t do you notice the lovely thing it immediately identified in exploratory analysis that t is a is a boolean data type and that there are 4 300300 instances of one and 2,000 instances of zero. Right. And it gives you the ratio of the two 68.3%. It means that 68.3% any classifier needs to be better than that. Right. Can you make a classifier worse than that. Believe it or not, you can actually You can always do worse than that the interactions like how interrelated are the these x one and x two. And when you look at it, you still see the pattern of the data you see this here. And you'll see this more. You can look at the pattern of the data you see this here and you'll see this more you can look at the correlation between the data and you notice that the data is not if you just let me I'll just zoom out a little bit so we can see a bit more yeah you'll realize that most of these correlations are below the off diagonals are low so X x1 and x2 are not correlated. T seems to have some degree of correlation to x2, but with x1 it seems to have no correlation. So this is it. And so, by the way, I'll leave it as an exercise for you to read about all of these correlations that these are the different correlations in statistics. It could be a good review of your statistical concept. Are there any missing values? There are none. But then when I loaded the data, I deleted this. This gives you a preview of your data, some sample rows of the data. First rows, last rows and so forth. So this is the value of this descriptive Statistics, right? Variables, Interactions, so you can just go straight to this. You can look at the samples of the data. So in other words, Pandas profiling is a pretty powerful tool. There are a couple of other tools. R also has Data Explorer and so on and so forth. couple of other tools are also has data Explorer and so on and so forth use those but know that you should know how to do histograms and correlations and correlation plots yourself then it is good to know but you can save a lot of time after your master that by simply using the tool so if you remember in the ml-100 we use the basic tools we did the histograms by hand. We did the correlation plots by hand and so forth. But now we'll just use Pandas profiling for Python and Data Explorer for R. So what do we do? If you want to train a machine learning model, we can We can, okay, standardize the data. This part I can just remove it. What am I doing? The first thing scikit-learn needs is a separation between the feature space and the target space. So our target variable is t and our feature variables are X1 and X2. So we are separating them out into X, which is the feature space and Y that is the packet space. Now, there's a lot going on in that line if you're not familiar with the syntax. So I'll talk a little bit about it. This is very Pythonic syntax. This X takes the first value here and Y takes the second value. Now the first thing you notice is I'm converting t to a categorical. I could have made it Boolean, but categorical is good enough. Categorical means it's a category because it's zero one. We don't want to treat it as a number. Then the other thing we notice is that I use a specific notation, capital X and little y. There's a reason for that. You could have used any variable in programming. What names you give to variables doesn't matter. In data science, it tends to matter because of expectation. It is a convention in the field that in the textbooks the feature space is represented by bold capital X. The data representing a data of the features is represented by bold capital X and the data for the target variable whether it is numerical or it is categorical is traditionally represented with little y. Y is for vectors see generally small letters are for vectors and capital letters are for matrices the feature space with all the rows feature space will have like the data features will have a lot of features and in the feature part of input data lot of features and it will have many rows of data so obviously X will be a matrix capital Y on the other hand is a single scalar value or or it is a type coward but there for every data instance it will have a value so it is a a vector. It is a column vector. So a small letter for it. It's a convention in the field. The next thing we do is we take this data and we split it into training and test parts. This code is essentially boilerplate code. You will see in innumerable notebooks. And you will get very used to doing this are you splitting the data into training and test this random number state is 42 why it is and leave it as a mystery for you there's an interesting story behind it with and let you explore it has to do with the hitchhiker's guide to the universe so what do we do with the data now that we explored the data we did descriptive statistics on the data let's try to visualize the data and see what it is trying to tell us when you visualize the data you notice that it is it looks like this by the way this is data visualization code let me talk about it here the only thing that is important is uh this uh creating the figure and uh doing this actually why did i do this i need not okay i'll just leave it at this we're going through so the size of the figure it's more twice as wide as it is vertical this line is purely optional I just add some little bit of padding here in there next is I give a title plot then I do a scatter plot these kinds of plots as you know are called scatter plots what do I need to do I need to give the X and the Y value and I'm coloring it by the label, the target value. I use a color map. Color maps are color schemes. See what happens is that we shouldn't randomly pick colors for plotting because of a lot of aesthetic issues as well as disability issues quite a proportion of men and sometimes women are colorblind I do not desk colorblindness exists in women I don't know anybody who is but maybe it does or doesn't one of you a doctor here can confirm but in men it certainly is their colorblindness or people who have partial colorblindness they can only see some colors. So people have created after a lot of thinking, certain color palettes, color maps, which are optimized for disability and at the same time are good looking, I mean pleasing looking. So that is why you can look up the dictionary on the web. There are many, many color maps. Matt has the documentation. You can pick a color map and give it. You may have a different choice. I just gave this. Alpha is the transparency. Do you notice that this yellow is not deep yellow and And this purple or violet is not deep violet. Right, it just faded out. That is the alpha of the transparency of it. And S is the size of the dots, how big the dots are. So if you don't give anything else, if these things were not there, your code would still work. But this is just adding a bit of aesthetics to the plot. Likewise, you don't have to give the title, labels, etc. It's always good to give it. Now, the title is this, Cataplot. One of the things that you can do, again, optional, is if you're familiar with LaTeX, you can then apply some latex formatting to it so that would you feel that this thing looks a little bit more publication ready let's open it up in a new tab and say compared to default it's not perfect but now i'll let you judge. Would you agree that compared to default, it looks a little bit more publication ready? Yes. So that's the point of it. So when you do the plotting, plot it out and then spend some time brushing it up, cleaning it up a little bit. So from this data, we make a few observations. The observations are, first thing you notice that there seems to be two decision boundaries. One at the top, where yellow meets violet, and one at the bottom, where violet meets yellow. And the decision boundaries are nonlinear. We talked about it a little while ago. Now, I explicitly said in the statement of the homework that use logistic regression. And that is rather, you may say that that's rather painful because this does not look like a linear problem. A logistic regression, as we mentioned, makes a linear decision boundary. And we are quite literally staring at a problem that is not linear so how do we solve it let us say and just to remind you what is a logistic regression this part is the distance from the decision boundary and this is the odds P over the probability of success versus probability of failure in other words probability that it is a red berry versus that it is not if you take the log of it the log of odds is the distance from the decision boundary that is the essential statement of logistic integration classifier so let's say the code when we do the code it's quite straightforward. You have this, PLF fit and so forth. You realize that to build a logistic regression model on the data, it's just two lines of code. Maybe I'll now increase the font size, the two lines of code. Once you build a model, if you remember the methodology that I sort of trained you folks into was that you build a model, then you check the model diagnostics. You check all sorts of metrics to see, is it a good model or not? And even if you do that, you plot it, you make all sorts of visualizations of that afterwards. And then finally you plot the predictions on the data. And if everything still holds out, you say the model may be a good model. Generally something along the way will fall. And so you abandon the effort so let's look here we will do the confusion metric and the classification report i wouldn't when we do that right away we see a problem here do you notice that none of the zeros are being predicted as zeros right so it's a disaster everything is being predicted as well. So here, this is the rows, the columns here are the real values, 0 and 1, and the rows are the predictions. So as you can see, clearly we are looking at a problem. Not only that that when you look at the accuracy what do you see 66% so should you be impressed with this accuracy definitely not why not all once we're 68% right that's right so the thing is even the baseline classifier had 68 accuracy and it got that wrong secondly its accuracy for zero is like for zero the precision recall etc is just hopeless uh right and f1 score which is the harmonic mean of these two is again, zero. It's a pretty bad model. All right. And for this also, it should have been, at least for one, it should have been a completely accurate statement. If you just use a baseline classifier, your precision recall in F1 score for one would be exactly 100% right but this one seems to not even get that so at this moment you abandon the effort you say we're lost so then you take the next effort which is polynomial regression what do you do you take the data I don't know why am I doing this here I must be thinking of something I did this anyway oh where did the polynomial regression disappear I think I accidentally may have dropped something at the bottom hang on give me a chance oh here it is the polynomial regression is here. Let me bring it to the top. Yeah, let me move it to the top. Okay. And we'll go one more step. Yes. So the next thing you do is you do it as a polynomial regression. So when we do polynomial regression, just the steps. remember the first we need to scale the data. That is something you should always think about doing. Unless there is a reason not to scale it, you should scale the data. The question is what scaling would you use? The second is we want to use polynomial features. So that is the next transformation we'll do pick a degree I just generally picked a degree 5 or 5 degree polynomial is a pretty complex model isn't it it will have a lot of interaction terms and so forth so we have that then what you can do is data input data will go in it will be scaled it will be it will become polynomially expanded so you can use this feature called make pipeline that will do you don't have to tediously do it in your Jay Shah, got a nicely scaled and polynomially expanded feature space feature data that is good then we apply a logistic regression model on this polynomial expanded data let's see if it works yes why did pick degree 5 here because all the cars there very good question yes I should so here's a mathematical trick I'll tell you one two three regression count the number of bends here how many bends do you see three two three and then four five actually six so at the bottom but because two of these bench are aligned you may just take six five whatever it is but you need to take a fairly high degree polynomial so see this is this is the fundamental theorem of algebra it says that a polynomial of degree n has n roots. In other words, it has n places where it meets zero. And therefore, if you're trying to solve a problem, and it means that the curve will meet the x-axis n times. It means it has to bend that many number of times. It's one of those little things of mathematics. If you you know it you can apply it when the situation arises it's a pleasure to do that. Anyway so we come here we will print the model fitness model coefficients and we'll make predictions with the model and once again we'll see the classification report and see how good it is. When we do that you realize that this seems to have been a good idea. Your F1 score goes up, this goes up, general F1 score for 0 and 1 also goes up. Accuracy is close to 79%. And if you were to print out the coefficients of the model, you would realize that, do you see how complex the model is? There are so many coefficients, one, two, three, four, five, six, 18, 19, 20, 21. So imagine an equation in 21 terms and such a complex equation is able to get a reasonably good prediction here. Right? So now we're going to do a little bit more careful analysis of the data. So how do we do that? First what we do is let us extract the river from the data. Remember we talked about if we could get the midline of the river, we can compare every data point to the midline of the river. Isn't it? So let's do that. You have the data. You hardly any need to describe it. What did I do here? I just kept the river points, which are the t is equal to zero points. When I do that, I have a much smaller number of data points, only 2000 data points. And out of those data points, if you visualize that data point, this is how it looks. You see that this shape of a river stands out. And of course, this is a toy example that I created. But most rivers are not so perfect. They would meander here and there. And so you would need some sort of a polynomial or something to describe it. But it will do for us because here we are trying to learn about feature engineering. So then what can we do? We can take this data and we can model it as a polynomial. And which is what I'm doing. I just took a fifth degree polynomial. But here I am not fitting a classifier. I'm fitting a regression line. I make a regression line. And there are only five terms. Do you notice that? It's six terms, of course, because they're beta zero. Then on those, and the first coefficient is actually zero, so five terms. You notice that the R square is close to 90%. So I have gotten a pretty good model. Let us check. Whenever we build a model, we check the goodness of fit. Now, the way, if you remember in ML100, we got trained. First thing we do is we look at the residuals plot to see if there is any heteroscedasticity in the data when you look at this and as you scan your eyes from right to left there is no distinguishable pattern there is no funneling or anything or there's almost no pattern anything worth noting so the data does seem to have homoscedasticity. This also has, you look at the residuals, they tend to have a Gaussian distribution here, guys. That is another criteria for residuals. They should have a Gaussian distribution, bell curve distribution. And in the margins, you can see that they do have a bell curve distribution so we seem to have a model the river correctly so then we go down a little bit we also do the error analysis we compare what is the prediction error like what is the value y versus y hat right and you see a strong correlation they seem to be along the principal diagonal right so a good thing next we go and so this is it uh best fit and identity line so our best fit line is pretty close you know r squared is 90 90 which is very good the other thing remember i mentioned and so it is good to be careful one of the things that I mentioned is you should look if your linear models are being hijacked by points of high influence, points of high leverage. There is a way to do that a very simple way and here's the code for that and Anil do you notice the library that you mentioned is being used ah yes so you notice that now there is in Cook's distance or plots what you look for is a dotted red line and you say that anything beyond that red line begins to stand out as points of high influence. Here there are actually none. We don't have those points. All the points look normal and fairly good. Some minor here and there. So then let's visualize the model. So the next thing is so far, no reason to reject the model. One final test, let's visualize the model over the data. So this is a data visualization code, obviously, for those of you who are just getting started, I would say, be patient with yourself, you wouldn't be able to do all of it right away. But do that book, Finder's for everyone and the scikit-learn book. do that book finders for everyone and the psychic and the psychic learn book it has example codes the web has example code while this code looks scary you know oh goodness so much but actually it's a very trivial code once you get used to plotting so i'll just walk through what the code does this is just some beautifying parameters i'm using LaTeX. Why? Because you notice that the title has a publication quality text to it. If you look at it, all the text is nice, good quality text, good fonts, and mathematical expressions look like mathematical expressions. So you can ignore that. I just create a lot of synthetic data in that space to draw the line and then I just go ahead and draw the line but in the background of the line I put the original data points all the river data points so do you think that this line more or less represents the center of the river occasionally it seems to miss miss it but broadly speaking it seems to get it right. Would you agree guys? Definitely. Yeah. It does agree. Now what we will do is now let's do feature extraction. Let's look at the distance from the center of the river, which is very easy. We just define the distance to be the absolute distance of a point from the center of the river. It doesn't matter whether you're above or below, what matters is how far you are from the center of the river. It doesn't matter whether you're above or below, what matters is how far you are from the center of the river for that point. So I can take the X2 value of a point and compare it to what the center of the river is. River middle is this, and I just subtract and get the distance. And so you notice that I have one feature, D, the center of the river. Go ahead. Probably I'll make a comment once you explain it. Sure. So then with this data, now what you do is you create a much simpler data set. You see that my feature space is now just the D variable and target space is T variable. So now I'm talking about a classifier in just one variable right a line i'm just trying to divide a line chop a line and at some place saying on this side of the line is river on that side of the line is sand sort of and so if you build a classifier with this, you expect it to do well. And here it is. Right away, when you build this classifier, you notice that first thing you notice is you get a good confusion matrix. Remember, the initial one was a disaster. We got zeros. But here, you notice that most of the points seem to be correctly classified. There are 92 times when one is misinterpreted as zero and zero is in here. So into 7% of the time and zero is interpreted as one. So I don't know why I divided it by 10. Okay, so that's the thing. And you can see the classification report. What is our accuracy 89, 92, close to 90% give or take. Now, this is the accuracy that you can have. You usually can't exceed that, give or take one or two points here or there. You can't exceed that because the data inherently has noise. If you go back and look at the visualization of the data, do you notice that there are areas in which the yellows and the purples mix up? The zeros and ones interleave. So there is inherent noise in the data. So you realize at this particular moment, you seem to be pretty close to the best possible situation. This is a way of visualizing the same data. This data is nicely visualized here, right? Precision, recall, F1 and support, which is here. which is here. So this gives it a bit of color to it. The next thing I mentioned in classification is the receiver operator characteristic curve. The receiver operator characteristic curve is this curve that tells us how good the classifier is, another way of looking at it. The idea if you remember is that your classifier, if you are this diagonal line or below your curve is below this diagonal line, then you're doing a pretty bad job. But if your curve is like pretty much hugging the top left hand corner and most of the area of this page of this graph is below the line. So this graph is one unit by one unit, even though I've made it into a landscape format. If most of the area is below the line, it is a good, it means that you have a good model. So here it says that the ROC curves look good. First of all, they look pretty good for zero and for one. Both of them are good. And at the same time, the area under ROC is 96%. 96% is a very good number for that. So by ROC plot also we realize we have a good model. Now, this is a simple plot that tells you how often did you confuse 0 for 1 and how often you confuse 1 for 0. So, the level of mistakes. You're making marginal mistakes. And that finishes this particular analysis. So, guys, now that you have this, you can use this to do the flag analysis. In flag, the only difference is it matters whether you're above the line or below the line when you visualize it you'll know what I mean you should be able to do the flag analysis in 20-25 minutes now I take questions and then I will perhaps do it in our the same exercise in power so what do you think guys? Do you feel that this is helpful in trying to solve the problems? Very. Yes, thank you. Good, good. So understand it. I have a quick question. So here we looked at the data. It's a 2d data and we saw a pattern of where it's all zeros and ones and we saw that river is where it's all zero tells where it's one and since we knew about the data we modeled the sign uh sign i mean the whatever the fit was and then subtracted the distance uh so it's like a cheating i mean it not cheating, we know about the data and so we, so how does this generalize to, if it's not a river? No definitely, see a polynomial can capture many things and your point is valid. See this is a toy example, deliberately crafted to introduce you to the concept that feature engineering is a powerful concept, a powerful to use in every situation, you'll have to think hard and you'll have to decide how to extract features from the data, but the universal truth that remains is that Sajeevan G. Effort at feature engineering almost always space. Sajeevan G. How you can S can put in quite a bit of effort at feature engineering. Because I was thinking like, okay, it was a jackpot. I saw the sine curve and then I thought there would be a formal way to, you know, I mean, to approach this problem. But I think the scope of the problem. Yeah, as we move through this course, right, we will learn a lot of methods. See, okay, I'll make a statement, opinionated statement. See, everything that you will learn from now onwards will be methods, powerful algorithms that will somehow linearize the problem and do feature extraction for you we live in the world where deep neural networks deep learning is very hot if you look at what deep learning does actually is that here we thought through and did feature extraction ourselves when you can do it you have a simple interpretable model that you can explain Jay Shah, Dr. trees random forest gradient I mean boosting and support vector machines deep neural networks they all can solve this this problem is too simple they will solve it in the blink of an eye but what will they lose decision tree still has some interpretability but beyond that the rest of them become black box, or black boxes, right? So that is it. But if you like what a decision tree does, for example, or what a support vector machine does, let me give these two as an example. When we do the theory of deep learning in that bootcamp that is coming up, deep learning bootcamp, you will realize that layer by layer, you are systematically extracting representations or in some sense features from the data and higher level features and higher level features or representations in a systematic way so a deep neural network layer by layer is a method to do the same thing feature engine the feature extraction, except that you do it using machines because the problem domains are very hard to do. The same is true when we talk about, you know, whether it's true for like photographs and congulation networks, the vision networks, it's true also for transformers and so forth, attention. And for example, when so forth, attention. And for example, when you apply an attention model to a picture, you ask this attention model that where is the, we are looking for a building, where are the buildings, right? So it will tell you that it found a building in the picture, right? This is a building, but then you look at the whole photo and you're not sure that this algorithm knows where the building is. So you can use an attention model to say, or attention mechanism and say, show me where you suspect there is a building in this picture. And it will start highlighting those areas where it says you should be searching for a building for. Because let's say that the building is just a way small, not quite visible or whatever it is. And the neural network is starting to pay attention to certain regions and it will then go produce a heat map for you showing that that's where the building is. And if you think about it, what happened? You went through the neural network somehow went through a systematic process of feature extraction and learning. What we are doing is we are learning the feature engineering ourselves. When you have tabular data and you can do feature engineering and solve the problem with simpler algorithms, you have a gold standard because it's very interpretable. You can explain it to customers and walk away with it. So this is what we do, the complex models will do it automatically for us, but now the scope is for us to learn how we do it by ourselves. Yeah, how we do it by ourselves. And also it has value. There are many, many situations, all these shiny algorithms. We will learn all of those. They may not be used for legal reasons and for a whole host of reasons. You're forbidden or it is too dangerous to use them. You are seeing what is happening with facial recognition, for example. It is a disaster disaster it's become a tool for discrimination okay you should always search for interpretable simpler models to get the job done a while complex models can do it for you you know you you lose a lot you you have to sacrifice a lot the point within being able to predict and being able to interpret you want to have both and you won't have both if you use very complex models right that's the agree all right guys so uh i said just a question, maybe maybe if I'm thinking it right If you go to the river again they were This one or the one here the one below that right this one just every way itself, okay The other one very midline I'm sorry the other one with the lightning yeah by the way as a best practice guys whenever you make your notebooks add a table of content to it you see this icon here it will help add a table of content to it. You see this icon here, it will help add a table of content and have a table of content on the left-hand side so it is easy to navigate. Quite often you come across people's notebooks, which are a mess. You just keep scrolling up and down. So that's one here. I just want to validate if I'm thinking it right. Do you think if I normalize those blue point, I will get a better middle line, like a center line, right? I'm not sure if it's evenly distributed or not. Remember that when we were visual, when we were doing the analysis, we did normalize the point. In the visualization we have not. You can do that. You'll get exactly the same graph. You'll get the same. But if you went back, here it is a poly data transform. There's scales and there's polynomial expansion. This poly data is a pipeline. Scale into polynomial expansion. this quality data is a pipeline so basic good hygiene guys make sure you give a good descriptive title don't just leave the title accent give legends like for example this makes it clear what we're talking about model here do you see where my mouse is we have a legend give legends you can label your axes and so on and so forth it's uh it is worth doing there is a school of thought like for example Edward Tefft likes principle of leasting these days you don't make a lot of grid lines and so forth so I have made this with some places having grid lines and some not having grid lines all over the place I think when I visualize the data no here also okay so these days people are trying to remove clutter from diagrams or visualization so that you can see the information much more. It's talking. All right guys, so what I will do now is let me bring up the R version. Sorry, one last question. So when you made this middle like this midline right through the river, so you separated out that river data using that T as the classifier? Yes, T is the target variable. Remember, T is the target variable. It's always a target variable. It's always a target. In the next time, if we can have more visualization, if we look at this picture below, you know, we can interpret that X2 variable all over the place. So it doesn't explain anything. Only X2 variable has inherited information. So if we can visualize those things that way we can understand better. Yes, that's true. I want to show you something. This question that you can do better with more sophisticated algorithms. I wanted to deal with that. So, I'll give you a little bit of a walkthrough of what we are going to do in the coming weeks. We have just talked about trees. It may be worth asking how well do the trees do? So let me share that and how do I share it? Let me share the entire desk. Are you folks able to see my screen? I see one. Sure. Please be a bit louder, please. Be able to share the notebook that you put today. Today is a little, I haven't finished it yet. The Python. The Python one itself. The R is actually finished long ago, but the Python one, I still have to do it. I'm still adding a couple of things. And the way I share it is that, see, if I shared the notebook itself in raw, then you'll just copy paste the code. So I'll make it harder for you. I'll actually give you the chapter of my book that contains all of this. So at least you'll have to key it in. Sure, sure, not a new. But you'll get the idea. So guys, we are going to talk about trees. Let's look at the same data set. Are we seeing this data set guys? Oh, by the way, this is how it will look. These are the chapters of my book. So if you're looking, this is the way you'll get it. And I hope you agree that it is better to get it as a well written text rather than just a rough notebook. Nice looking. Yeah. So this is the river data set. We use a decision tree that we learned about. This is just taking the data and bring basic statistics on it and visualization. Turns out there are many libraries in R that you can use to do it. We'll use one of them please. And when you use trees you come up with this kind of a let me zoom into this do you guys see this regions do you see how complex the decision bound boundaries are you know how complicated the regions are each of these rectangles is either zero or one would you agree that this looks very complicated extremely complicated extremely complicated then if you prune the tree and it becomes a little bit simpler, but not by much, compared to our interpretation using feature engineering, I mean, this cannot even hold a candle to it. Yes, it's a prune tree, it's accurate, but see, it's still, it is not very interpretable what it is trying to do, more or less. And if you look at the tree you know the decision tree I deliberately didn't put the labels here because it would be too hard to put it on a page this is the original decision tree on the left on the right and this is the pruned decision tree on the left still to me both of them look much more complex than what we are trying to do yes and you could do that and then obviously these are the metrics. In R, by the way, this is once again, there are tools in R that help you get your precision, your recall, your accuracy and so on and so forth. The area under ROC curve with decision tree also is good. But if you look at our area under ROC curve or we are actually look at this what is the area under ROC curve with the decision tree eighty eight point five percent isn't it guys yeah which is supposed to be hang on I'll zoom out eighty eight point five what is the accuracy or what is the area under ROC curve that we were getting? Let's go back and check. 90 plus. Roughly 95. Exactly, right? So when we go and do that, we are getting an ROC area under ROC curve of 96%. And the amazing thing is we used a very simple model you can't go simpler than logistic regression yeah right using a simple model we have just beaten what a much more complex model right and that is the beauty of feature engineering guys so let me go back to the tag and you can zoom out yeah so this is it you can do the same thing using different libraries then you can use even more complex algorithms we will talk about random forests very popular in ensembles next Monday we'll do that when you use that right now random forest is almost the state-of-the-art for table data tabular data right and we would I wouldn't go into this by the way these are literally the notes that you'll get. Still, even the state-of-the-art algorithm just about comes close to our simple linear model, isn't it? You see the area under ROC? 95%. We are at 96%. Well, close. But it took a state-of-the-art black box model to achieve a comparable accuracy. So I hope I've convinced you of the point that feature engineering is a good thing, guys. Would you agree now? Yes, of course. So that is that. I have a Python. So this chapter is ready actually but now that i think about it the decision tree chapter is ready but uh and even this chapter was ready uh kate you took this uh last time you took it i did hand out both r and python isn't it i have one in r i'm not sure if i have the one in python have to look through my uh okay lab notes yeah okay so yeah yeah yeah of my book chapter is it like a meep type concept meep no it is not a book at this moment and now it's in the under publication against the leaves the chapter. Exactly. As I'm writing this book, I'm making it available. 11.9 is Python implementation of a decision tree in random forest for the river data. Yeah. But no, but this feature engineering part, that chapter is probably, if you look at it, maybe I did in Python, maybe I didn't. I'll certainly add both of those. So any chapter will have implementations in both, guys. If you have ever written a book, you realize that writing a book is tedious. Good thing with writing a book is that things are clearly explained. You have a reference. It is, you'll notice that I use much more words than they are in the notebook. Notebooks are just, I put descriptions, but not as much in this as in the book. So you'll get the chapter in due time. Anything else guys? So I'm done.