 Guys, we are on record now. So first of all, welcome to this workshop, Practical Methods in Machine Learning. This workshop is something that traditionally followed the introduction to machine learning, or introduction to data science. In the introduction to data science, we introduce people to basic methods of statistics. We learned about correlation, causation, covariance. We learned a little bit about regression, a little bit about classification, and clustering, some dimensionality reduction. Some of you are coming from that workshop almost i noticed that most people here in the audience have taken ml100 or the introduction to data science with me some of you have not now the way the audience for this workshop is divided is a lot of people they either do machine learning um with me in the ML100, Introduction to Machine Learning, or they have some background because of some prior experience at work, some data analysis or some web courses. But they have some understanding of basic statistical concepts. So long as you have that you can pull through I do also give a few helper or remedial classes for students or for participants who may be lacking in some background so for example if you don't have Python background but there will be a session this weekend which will teach you the Python necessary to do data science. For data science, you don't need to know the vast language to get started or to offer the purposes of this workshop. You need to know a very specific toolkit as Kiketle and Pandas and so forth, NumPy, three specific libraries, and they carry you forward after that. And you need to know the basics of the Python language. So we will have a remedial class for that. It's free, of course, to all of you on Sunday. So look forward to an announcement from me. It'll be either Saturday or Sunday, actually more likely Sunday evening, we'll have that workshop those of you who would also like to learn R R is another language that you may use in this workshop to do your assignments again there will be a remedial session at some point in this week where I'll get you started with the R language both Both of these languages are very simple. To a large extent, if you're used to any form of programming in any language, you'll find that you can pick up these languages, R and Python, simply by seeing and understanding the code. As we explain the way to solve the data science problems, you will sort of very easily and automatically pick up that portion of the language that is necessary to do our stuff. Now, so that's the background. This particular workshop, the way I used to teach in the last few years, it would be a continuation of introductory and we would do more algorithms. We would go from linear to nonlinear methods. We would do things like no free lunch theorem. It's a very important Consideration in machine learning. We would do feature engineering, we would learn about ensemble methods, random forest, extremely randomized trees, gradient boosting, XGBoost, CADBoost, and so forth. We would learn about kernel methods, which would be, and these are again words that you don't know at this moment, so don't worry about it. I'm just giving you a scope of it. Kernel methods would include a kernel PCA, principle component analysis, support vector machines, maximum margin hyperplanes and so forth. And we would do stuff like that. Then we would do recommender systems. And we would do regularization. In other words, how do you prevent overfitting in data in your model when it tries to learn from the data? Those used to be the topics and I would do regularization. In other words, how do you prevent overfitting in data, in your model, when it tries to learn from the data. Those used to be the topics and I would go linearly through it. However, this time I decided not to pursue that for a reason. Many of you, and I realized that at the end of all of that, people get a very good grounding in the fundamentals. But one thing that I felt we could do better is immerse you in practical work right from the beginning. So this workshop will proceed on two sort of steps. There will be one, the foundational steps, building your foundations on these topics. But the other one will be one the foundational steps building your foundations on these topics but the other one will be starting with what methods you actually use in day-to-day work as a data scientist for example in my day job i lead i lead a data science team of approximately it's a pretty intense team well the whole data science team is about 100 people, but the people who are doing machine learning, they're about 50-55 people with about 60% of them with PhDs and about 40% with bachelor's or master's degree. And we have a whole variety of people in this field there are people with background backgrounds and of all things music with backgrounds and completely different subjects we have a biologist we have people actually we have a doctor who transitioned over to machine learning so in this field a field of machine learning or data science, you find people coming from various backgrounds. And they all are very successful for a reason. Each person comes from some domain. Somebody comes from VLSI and semiconductors and electrical background. Some comes from mechanical, some comes from civil engineering and ocean engineering and medical and so on and so forth. We have had people from legal and psychology and such backgrounds. The amazing thing is they all actually after a little while, they all catch up and they all make a remarkable progress in this field so it's a very democratic field the reason obviously has to do with the fact that this field is all about data every data whenever you have data there is a generative force behind the data something caused that data to happen, to appear. Data is measuring something, some phenomenon in the external world. When you look at that phenomenon and you quantify it, you measure it, you realize that those measurements, they are what you are left or what you see, the observations from some underlying force that is causing the phenomena, some dynamics. For example, a river flows, you may compute the volume of water flowing, the velocity, the turbulence, and all sorts of factors, the viscosity, and so forth. But behind it, you know that there are physical laws and physical forces. So behind all data, there are generative forces. And there are many, many subjects that try to understand those generative forces, whether it is the physical sciences, the biological sciences, psychology, law, finance, financial markets, whatever field you can think of people are trying all of science is a pursuit of understanding or modeling the phenomenal world and so all people who bring that domain expertise that domain knowledge are ready to do data science data science is an enabling tool machine learning is an enabling tool. Machine learning is an enabling tool. And the one example that I would give to frame the perspective that I have, when a fire came about, you wouldn't say that I am a farmer, I don't need fire. I'm not a cook. We all needed fire for basic things like cooking, food and things like that. Likewise, when electricity came, we didn't say that I'm not an electrical engineer, so I don't need electricity. We all needed electricity, every whether you're a biologist or you're an anthropologist or a musician, we all use electricity, we all need to be familiar with using the electrical tools. Likewise, when cars came out, it wouldn't be appropriate to say, in the beginning when all these motors, automob cars came out, it wouldn't be appropriate to say in the beginning when these all these motors, automobiles came out, the people who used to use them would be very technically savvy. They would be very interested in machines and they would drive. But gradually everybody needed to use cars. If you don't know how to use a car or automobile, you're quite handicapped, especially in the US. AI and machine learning, data science as you will, this brought something, it's the new electricity. You need to have an understanding of this field, not because necessarily you want to become an AI researcher. You need to master this field because you will need these techniques broadly in whatever field you're pursuing. It is an enabler. It will help you do whatever subject you do much better. Of course, if you do the subject very well, data science today, you're in a unique position because you can collaborate with any domain expert, a domain expert of any field. And in a collaborative effort, you can make remarkable discoveries. The coming together of domain knowledge and machine learning techniques, that is the crucible in which a large number of breakthroughs are happening today in the practical world. So today, the techniques that I'll focus on here practical methods, the mental frame of you should be that you are a person with whom a domain expert can collaborate and collectively together you will make breakthroughs. So with that particular frame of mind, the way I'll start is, not first with theory, but with tools first. And I will talk about the tools that you need in this in this field some of you know these tools maybe many of you know these tools but since it's a introductory and first talk I will talk about these tools first and in this particular workshop we will use tools so let me start with the thing the tools of the trade are you guys able to see what I'm writing yes yes use of the field and these will be the tools that we will use I mean there are many many tools but in the interest of making progress we need to be focused and we will use only these few tools that we will use the tools that we will use will be first is Google Cloud and a platform. In particular, we will use Jupyter Notebooks in GCP and Colab. In the beginning, start with Colab. It's a lot easier. Colab is not, well obviously the power is limited unless you take some more computing power from them. We'll start with Colab. It generally I advise people to do their work in at least two programming languages. The primary or the greatest mindshare is divided between Python and a programming language which is, Python is a general-purpose programming language. is, Python is a general purpose programming language. Most of you may be familiar with at least a little bit of Python. There is a programming language which is dedicated to data science. It is called R. And generally, if you, for most of the things, the R code will be fewer lines of code than the Python code. Python code tends to be more verbose. And the third programming language will be Julia. Now, here's the way I would suggest. Pick a programming language that you're comfortable with. First, do your labs in that programming language. But then try to do it also in one more programming language for example if you do python then also try to pick apart because there are many many things that are done in one language but doesn't exist as well in the other these languages are very very similar if you pick up one picking up the other is child's play it is really worth picking up now there is a third language a new kid on the block there is a programming language called julia which is a scientific programming language it is designed for scientific computation in this particular workshop, because this language is going to be fairly influential, I believe, in the coming years, I will also be giving you lab works in Julia. It's up to you whether you do it or not, but I'll certainly give you solutions in Python, R, and Julia, all three of it. Now, in R, there are quite a few good libraries and but R itself is on very solid foundation. In Python, the traditional things that people use are skeekit learn, which is the machine learning library. You use matplotlib. We use something and I'll explain what these things are. We use numpy And we use ski pi Let me just put these are the seaborn also yeah ski pi Okay. And here we'll have a set of libraries, ggplot, plot and tidydata, tidyverse. These two will be our primary libraries, Skiplot, and we will also use with all of this something called Yellowbrick. So obviously in Python I've mentioned a lot because these days there's a growing set of libraries that people are creating in data visualization. So for Python it is this. So let me explain what these are. The machine learning work that we do, regressions and your correlations, your well, okay, classifiers, your dimensionality reduction and clustering and so forth, and these terms, by the way, if you're not familiar with, don't worry, you'll pick it up in due course of time. All of these algorithms are implemented in the scikit-learn library. So that's a machine learning library. In machine learning, the primary thing that you work with is a vector. It's a vector, let me call this vector x, that represents a point in some dimensional space. Suppose I have a vector space whose dimensions are coordinate system, Cartesian coordinate system. This is three dimensions. Now, traditionally in school, when you do three dimensions, you call them X, Y, Z. But in machine learning community, you tend not to. You tend to just use X1, X2. The reason is the number of dimensions can be 100 and you may run out of letters in the alphabet. So this is your X vector point in this. Let's say in this particular space in this example would be a point is a vector because you can connect the origin to that point with an arrow. Machine learning always works with vectors and this space that you call it, it is called the feature space. Or the data space. You can call it data space or the feature space. So when you talk about vectors, suppose you have a vector. Now vectors you can do certain vectors you represent as components x1 x2 component and let's say that it's a p-dimensional space and let's say that vector belongs to a p-dimension and by the way don't worry about this mathematical notation this is just notation for notation's sake at this particular moment. I'm not saying anything very hard I'm just saying if you have a vector in p dimensions, what does it mean? In simple terms, it means if your data comes with a p columns approximately in a spreadsheet, one column being, let's say you're talking about animals, one column is the age, another is the weight, another is the height, another is the width, right? Another is the which which the animal can run and the another is whether or not it can fly another could be how much food and that animal eats every day right another could be the temperature of the animal and so forth so these are just values of certain characteristics of the animal those features of the animal of certain characteristics of the animal, those features of the animal. So that's all it means. Now, suppose you have X, what can you do with X? To a vector, you can do certain transformations. For example, a vector can be rotated. You can go from X to X prime, or you can take a vector and you can stretch it. X goes to X prime, things like that. So these are changes that you can take a vector and you can stretch it X goes to X prime things like that so these are changes that you can make to a vector rotate it stretch it or invert it X vector can become this sort of and reflect it and so forth so these are small operations to do these operations. What you do is you apply A transformation which is traditionally written as a matrix. Now, by the way, don't worry, we will not use matrix in this workshop. That is for a more advanced workshop we will deal without matrices, to a large extent, but I'm just mentioning it to you a matrix when, when it applies to a vector, when it hits a vector will, matrix M will, let's say, will transform it to some other vector x prime. So when you talk about vectors and matrices, how do you represent them? If you try to represent them in Python, for example, as a list, there's a few who happen to know Python, you know, you would be tempted to use a list, right? You could say x, right, is equal to an empty list, or let's say 123, something like that, a list of numbers. It turns out that these are not very efficient. Python is an interpreted language. Doing computations with X list won't be fast. In machine learning, you need massive power, computational power. It is all number crunching. So to number crunch, you need a language that is blazingly fast at number crunching. And those languages are Fortran, C, C++. These are practically the de facto standards. And the speed, this is the fastest, this is the second fastest, and C++ is a little bit behind. And it turns out that most of the work in this field for number crunching, for numerical computation is the technical word, has been going on for the last 30, 40 years. People have been working very hard at it, writing those Fortran and C libraries. So they exist, those libraries exist. So what Python does, and to an extent what R and other languages do, except Julia, Julia implements everything from scratch. What these things do is they say that it is much better to create an array, vector, an array in C or Fortran, it will be far more efficient and in Python, just refer to it as Python. So the best world would be you create an array. So you can say np dot array right array and then you can give one to whatever it is and then any operation that you do to x now will be happening at the underlying language in c and so forth and there the computations are blazingly fast. The same is true for matrices. You want to do matrix multiplication. All of this matrix, we'll say multiplication, but let's say matrix manipulation. You want it to happen in a fast language like C. You don't want to do it in Python, raw Python, but you want to code in Python, but let the system do it in the underlying language. So the library that makes it possible is NumPy. NumPy sits upon the underlying languages. Those of you who are familiar with Fortran, I mean, some gray hairs in this audience I do see like me, if you have blasts and so forth those basically linear algebra systems so numpy sits upon all of that then there is key pi scientific library this is things like and this may refer bring up some memories Lin pack la pack etc. etc. as far as I know, it sits upon those very, very legendary libraries that have matured over these years. Maturing scientific computing libraries is extremely hard. You have to have very high performance and near zero bugs. So it has taken 30, 40 years to mature those libraries and Python then sits upon all of that. So that explains the NumPy and the SkiPy here. Then this is the, obviously the machine learning library. Now let me talk about data visualization. See, whatever algorithms you apply to data and results you come up with, you may build the most predictive model. But when you sit with the domain expert or when you sit with the business person, if you cannot explain it to the person in a way that the person can understand, you pretty much will have trouble being taken seriously or being for you it is your burden to show them the value of what you have done in order for you to do that to show them nothing is so good as data visualization you can use visualization tools to very easily convey your message. You need to tell a story. Behind the data are generative forces. There are things happening, the insights in the data. When you discover those insights, you need to tell that story, and visualization is a large part of telling that story. So there are a lot of visualization libraries. There's in Python, there's Matplotlib, Seaborn, PyWiz, Bouquet. And one that people don't usually know about and I'll talk about is the yellow brick. We'll use the yellow brick because it helps you visualize not the data itself, but your model, your machine learning algorithm that has, or the model that has learned. And we'll talk in due course about that. In R, such a wonderful library that serves the purpose is ggplot. It's based on the grammar of graphics and we'll use that. In R, there is also a concept called tidyverse. Those of you who took ML 100 with me know what I mean by that. The data is best analyzed if you put it in a standard form. That form is looking like a rectangular data. So it should approximately look like a rectangle and one observation should be one row. but one row. When you put it like that, it is called tidy data. And tidy data, then you can apply a simple set of operations, what is called the grammar of data manipulation. With a very small vocabulary, you can do whatever you want with that language. Those of you who are familiar with SQL know that the moment you can represent a data as a table or a bunch of tables with foreign key relationships, SQL becomes the language that elegantly can help you get whatever you want from the data and manipulate the data. So tidy words or tidy form after that, whether you're doing it in Python or you're doing it in R, you can manipulate the data very easily. Now, R, the basic construct of data, the way you think of it, is as a data frame, which is a rectangular data, you know, a grid of data, data frame. And data frame is foundational to all of machine learning and data analysis. and DataFrame is foundational to all of machine learning and data analysis. You're probably familiar with it. In Python it shows up as a library that is very much like called Pandas, that is for Python and in R it comes built-in. Right, R and Julia gives its own built-in representation of it and we'll deal with that in due course of time so data frame is something now those of you uh send me an email if you are not familiar with these concepts we'll do a remedial class sometime over the weekend so i can bring you folks up to it so this has been the traditional those of you who have done for example ml100 or even people who have done this same workshop that you that we are getting into ML200 with me in the past or would remember that these are the tools that we've worked with but this time I'm not going to start like that I'm going to start in a slightly different way so let me give you a background of how we are going to start this time. And the reason I'm doing it is the frontier of machine learning is changing. We are in a transitional time where machines are trying to discover what is the best means to solve a problem. Now, that's a very interesting statement. Those of you who have been doing data analysis know that if you try to model the data, what you do is you try different algorithms, whether it is a random forest or support vector machines or neural networks or linear regression. You do it. If you do it with, let's say, neural networks, you sit down and design a neural network. If you do it with logistic or let's say that you do it with decision trees or things like that, all of those concepts you learn, obviously I'm using terms that actually we're going to learn, but take me on the word at this moment. Whenever you build a model, a model has something called hyperparameters. Things that you don't know, that the model doesn't tell you what are good values for doing a data analysis. And the reason is that every data is unique. And there is no way an algorithm can guess what those, how much you have to turn those knobs, right? And how to tune those knobs and parameters, those hyperparameters. So there have been traditional techniques, grid search, randomized search in the hyperparameter space to find the best values. While that has been going on, it is a very tedious process. You keep on iterating again and again. And even even today 99.99% of the data scientists are doing that as a part of their daily life but there has been a different approach that is emerging these days what you can do is you can use something called automated machine learning you can create systems whose job is to first discover the best hyper parameters needed and in the case of things like random forest or decision trees or let's say neural networks, their job is to find the best architecture, let's say the neural architecture, to solve a particular problem. Are we together? Now I'll give you an example which is slightly out of the scope of this workshop but it is there in the deep learning. You all know that we live in the times of COVID. This particular workshop that you are attending used to be an in-person workshop here in Support Vectors office in the venue in Fremont but we are doing it remotely but the reason we are doing it remotely is because of the COVID-19 or the coronavirus pandemic so this pandemic is getting a lot of scientific attention. Not only are we in pursuit of the best vaccine or successful vaccine and treatments, we are also in pursuit of a whole lot of research into the virus, into its genomic structure, and into diagnostics. So one of the things people ask is, how do you test a person for having coronavirus? Well, there are some primitive tests. You can just take the temperature. There are a lot of false positives and false negatives. You might have a temperature because, well, you know, you are just not feeling well. You have a normal flu or cold. On the other other hand a lot of people who do have coronavirus the new COVID-19 may be completely asymptomatic they may not have a temperature so obviously that test from a machine learning perspective you would say that it has very poor accuracy it has poor precision and very poor specificity and very poor recall. Generally it's a bad test. Now, not bad, I mean somewhat useful, but not as effective a test. But then the gold standard in this is a PCR test, polymerase chain reaction chip test I believe that's the full form of that word it's very specific it can tell whether you have covered or not the trouble with that is it's a elaborate test it takes manual effort and few hours before a result comes out as expensive people have tried for example as you know about it is if I'm right, and please correct me if my facts are not accurate, created a machine that could give you rapid results for the PCR, the Corona test. And those devices everybody felt was a great idea, except that the reliability of that, the accuracy of that people later realized is barely close to 70% or so. Not very reliable. So one of the things people did, and this happened almost as soon as the corona outbreak happened in China, some companies in Japan and South Korea and Taiwan, they started looking at X-ray images of people. Now, X-ray is something that is very inexpensive to do and very rapid. You can actually do x-ray while people are just walking by in some situations and just pause them for a moment, take x-ray and they are on their way. And could you just look at the x-ray and find whether a person has COVID? Then they realized that you could actually they trained they created a machine learning system that can predict or detect covet the last i checked it was with 95 percent accuracy now you realize that 95 accuracy for a test that is much much cheaper and very rapid literally you have results let say in 10, 15 seconds, is at least a huge improvement of a device that has barely 70% accuracy. It is not as good as the PCR test done properly, but it is something. And so a lot of countries are seriously looking at that. We will investigate that paper, the architecture, towards the end of this workshop. That neural network architecture is called the COVIDNET. The remarkable thing about that architecture is you could not have handcrafted it. So it needs, it is where human ingenuity of those researchers who were crafting the COVIDNET architecture, machine learning architecture, met automated machine learning, you literally used techniques in which you make a neural network search for a perfect architecture to solve that problem. And that is the new world we are in, we are in a world, if I may put it a bit more sort of somewhat inaccurately, poetically, it is that you create a machine learning system whose job is not to solve or model your data, but its job is to find the best algorithmic parameters, hyperparameters, and architecture that will model the data best. So it is like it gives birth to the best model for the data rather than itself modeling the data. That is quite a shift and those of you who have been doing machine learning would realize that that's a huge shift that's happening quite literally here in 2020 now. So in view of all these changes, I decided that this workshop will focus on these techniques. We will start very early on with, with our engagement with automated machine learning. Even when we are doing fairly non-neural network stuff, this workshop is, neural networks is at the very end, but most of it is about other methods, support vector machines, ensembles, and so on and so forth. So can we apply automated machine learning to all of this? And the answer to that is yes. In fact, we must now, and that's the new frontier. We will do that. The other thing that's emerging is something called graph machine learning or network machine learning, network science and so forth. Getting a lot of significance these days and we will do some practical examples of that. Of course that meets the neural networks and what the subject becomes something called geometrical neural networks and the study of geometrical neural networks some people call it graph neural networks is the context of our bootcamp neural network bootcamp that we do in next month but not not of this but this one will build the foundations by doing networks networks are everywhere your LinkedIn is a network Facebook is a network your highways are a network right the circulatory system in your body is a network. Your highways are a network. The circulatory system in your body is a network. So networks are everywhere. And it turns out that networks have very interesting properties which you can exploit in your machine learning, modeling, and so forth. And we will do that. That is the second aspect. The third aspect that I'm going to emphasize in this workshop is cloud more and more it is not feasible for people to be able to afford a good good laptop that can do these large AI algorithms on them for example I have a laptop some of you have seen and I think some of you have purchased, I helped you purchase. The most powerful laptop that you can purchase comes in at $4,000 to $5,000. At that moment, it has 64 gigs of RAM and a lot of very fast GPUs and so forth. And it's big and heavy and noisy, and not to mention terribly expensive. big and heavy and noisy and not to mention terribly expensive. So we are reaching a limit that you the algorithms the power the demand is not what is easily accessible because even with this laptop I can't do most of the important or interesting problems. I can do the regular stuff but not heavy stuff. To do heavy stuff, Support Vectors has a $32,000 machine with four GPUs. And when we do a lot of text processing, a lot of convolation neural nets, even then it runs out of memory. It runs out of GPU space or computational power. GPU space or computational power. It is a fact of life today. So what can you do? Well, it is not common for people to go buy themselves a $32,000, $40,000 workstation. A company can afford it, individuals cannot. And so how do you do machine learning if you have to do those algorithms? And in this workshop, towards the end, we will do heavy algorithms but from the beginning we will get ready for them so the answer today is obviously cloud in cloud you can go and reserve a lot of AI hardware you know hardware needed for AI and but reserve it only for an hour or only for half an hour or only for two hours. Get your job done, run your algorithm, train it and then pay maybe $6 for that. So from a price perspective, it's very, very effective, especially when you are learning. When you go to production, the dynamics change, of course. When you go to production, you're far better of having good hardware under your table rather than use the cloud cloud becomes too expensive at that moment okay so uh that is that so from a hardware perspective let me give you some things we will we will use the google cloud platform why google cloud why not amazon no particular reason Why not Amazon? No particular reason. GCP tends to be slightly superior and easier for all the AI stuff. So it is more appropriate. However, if you insist on using AWS, by all means you can do that, but you might not be with the rest of your colleagues. So it may be a bit difficult to get help from each other. So we will use the GCP. Then if you want to use on premise, Sajeevan GK So we will use the GCP. Then if you want to use on premise like Sajeevan GK You want to buy hardware. I would suggest that go to. I mean, there are many websites. Obviously, I'm not in no way sponsoring or patron or Sajeevan GK advertising for them, but one that I have, Lambda Labs. If you go to Lambda Labs, they make excellent hardware dedicated to machine learning. They have their laptops. They have workstations. They have servers. So you can pick. Usually, a laptop is probably what you will go with. You can also go with a workstation. You can pick, if you're really very serious about machine learning and you're going to spend a lot of hours doing it, then it makes sense to have Lambda. So for example, I have a couple of Lambda support vectors. We have a few Lambda laptops. We have a few, we now have Lambda workstations and servers. The servers, by the the way tend to get terribly expensive they start out at 70 80 000 and go up to about 100 000 so you have to be a bit careful if you can make your company get it for you that would be much better for your project at work right so then everybody everybody in your team can share it they share such resources. So GCP will be what we will use for that reason. Are we together? Now, today is not the lab session. Lab session will be on Wednesday. So we will go over all of these tools. What can you do today? Today, what you can do after the class, because today we'll continue on, after the class, I would suggest the following activities. Create a GCP account if you don't have one are we together number one number two go create if you and if you want to wait till Wednesday that's fine otherwise go a collab notebook. I'm saying and what collab is, is when you get into GCP, you'll see what it is. We'll very carefully walk through this the next time, right? So I'll walk you through, we'll do it together. So you might want to wait if you're not familiar with this. If you're familiar with this, go create it and then create a jupiter notebook in gcp this is it and give it and give it it some gpu that is a graphic computing power because that's what you'll need for the ai stuff do Do it, but then make sure to shut it down. Any resource that you create for yourself in GCP, remember to shut it down. Otherwise, you'll start having those. So once you work in Google in the cloud, remember your worst nightmare is to go to sleep while leaving your things still running. It is as bad as leaving food on the stove and going off to sleep. Probably worse, you'll incur a bigger bill. So be careful. When you use the free tier of Collab, it will automatically, it has a 24-hour cycle. And it's free, so you don't worry about it. I would say start with the free tier of Collab. Are we together? So this is what we are going to do. What is our textbook? Here, the textbooks are, and I will send you the list of this, one of them is the book ISLR. It is Introduction to Statistical Learning. It is a classic. However, in this course, we will need more than that. This is your theory. You'll pick up your theory from that. We will also do some things beyond that. We will do recommender systems. But for that, I'll give you my own notes. The same is true for neural nets. Basics, I'll give you the notes and especially for automated ML. At this moment, I didn't find any good introductory book. There are a lot of articles and books that are very formal, but there isn't a thing. So these things are introduced to you by notes. Those of you who are doing your stuff in Pandas, if you're not familiar, one of the books that you should do is Pandas for Everyone. And obviously I'll formalize it and send you pictures and links for this. Get yourself the book Pandas for Everyone. And the other one is for R you should do modern data science. So I'll send you pictures and ISBN numbers and direct Amazon links to these textbooks. So you must, these are your textbooks. Now this, to round up the introductory aspect, this is a fast-paced course, guys. This is a fast-paced workshop. How much time do you need to give? First, you need to give Mondays and Wednesday evenings to this. We'll go from 7 to 9 30 it is quite possible so do talk to your family it is quite possible that sometimes it may go from from 7 to 10 or 10 30 also based on what we are doing if you're doing it the lab can go and this is it the second thing how much of self-study do you need beyond this say that you need anywhere between every week between six to ten hours of self-study it so I would suggest that give give half a weekend or a whole day based upon how how much your background is if you have no background you'll have to give ten hours to it if you do have background maybe 66 hours may be enough so you'll have to pace yourself keep pace it's will move pretty fast remember in six weeks will be done now besides this formal sessions Mondays and Wednesdays I'll also be giving a lot of extra sessions help sessions like for example the Python session I talked about or the our session I talked about or Background session I talked about so I'll be announcing a lot of these extra sections unfortunately, I tend to be very busy with my day job and So when I get time for these extra sessions is a bit unpredictable So these extra sessions will happen with maybe one day warning. And so do check your emails. If you see a notification for the extra session, do join it. And you feel it's interesting to you, do come and join it. This is a high engagement class, guys. And there is a lot of learning you can do as a social learning this course will have the lab work besides the lectures it will have lab work and there will be substantial amount of lab work it will have project now why should you do the project those projects would be interesting and these are group projects so one of the first things you should do is in this class, form your group guides. You'll benefit from that. The projects are worth doing. If you do the projects, you can put it on your resume. I'll give you a very practical, but not hard, simple but practical projects that you can actually mention in your LinkedIn or in your resumes or whatever. The past history is that. But they become conversation points for your job interviews. Now, one more thing I'll give on the extra sessions, I will give you what are called quizzes and interview preparation quizzes and data science interview preparation. That is, come to those sessions and I will ask you questions. Well, I wouldn't single you out, but as a group I'll hand out. And it will give you a reality check. Are you ready to interview? Are we together? And I think you will find that beneficial most of you it's also a reality check if you take a quiz you on a topic that you thought you had mastered and you realize that you bombed most of the question well that's a reality check the other thing that I will do is there will be polls. See, this class is remote, which is an unusual thing for me. I am very much a person who likes to look into people's eyes to see if they understand it or not. Those of you who have been with me know that my classrooms are very interactive. I don't like being on a monologue just speaking while people are listening because I have no idea whether you understood something or not. So after every class, I'll be sending you a poll, guys. Can I request you, and I strongly mean it, guys, poll will take you about one to two minutes to fill. Its purpose is it will ask you about each of the topics that I taught, whether you understood the topic and how well you understood those polls are anonymous. Right. What it means or I'll make I'll try to make sure it's anonymous. If you want to put your name, you're welcome to The idea is I want to understand, did I teach the topic well? If I realize I didn't teach a topic well enough and a lot of you didn't understand it, I will repeat that topic. If not, in an extra session, some way, one way or the other, I'll repeat that topic and make sure that you guys understand it. This is the only substitute I have of looking into your face to see whether you understand it or not. So do please participate in the poll. There is a discussion group. It is currently on Slack. Please do participate in the discussion group it's important and one more thing every Sunday this is optional for you every Sunday at noon PST Pacific Standard Time I we give a webinar or support vectors gives a webinar think of it as a meetup on some topic of contemporary interest this goes on are we together if you want to listen to it if you want to join it you are most welcome I think when I look at the names in this audience, some of you have come to my past talks. Anybody would like to give an opinion? Anybody who has attended? Did you find it useful? Asif, this is Balaji. It's very useful. It's currently used. I mean, you were talking about the Transformers. That's right. We had the last year. Very useful. Very useful. I don't know how it happened, actually. This is exactly what I was looking for. Thank you. Really? Very good. So, guys, you all are busy. And I'm obviously itemizing all the ways that you can stay engaged. And these are all the ways. Come to the Sunday noon PST Support vectors webinar now what is the purpose of the webinar the purpose of the webinar is beyond this workshop I'll be talking of things that are of contemporary relevance what people need today like biology said trans I talked about something called transformers transformers is not something I would have covered in this workshop because I mean actually I will in this it so happens that I will but not in great depth just give you a glancing understanding but in these webinars in I mean, actually I will, and it so happens that I will, but not in great depth. Just to give you a glancing understanding. But in these webinars, in these sessions, I go into it in great detail. I'll take a research paper in the field and cover it carefully, the original research paper, and I'll cover it carefully. The value of that is it will get you trained in reading original research so instead of reading from textbooks if you want to see how one gets a habit of every sunday taking one research paper and understanding it in simple language right do come to that the way this field is galloping ahead, by the time something comes and shows up in textbooks, usually, usually a long time has passed, right? And people don't wait that long. People want to make progress rapidly. And in a fast moving field, the way to learn is from the latest preprints that get posted to archive, for example. So that is the context of what we do here. Lastly, if you have not, I would strongly suggest, so this is the social media aspect of it. Social media. This is partly forming a community. See, learning is a social activity. You learn by three means, by listening to something and understanding it, by doing it with your own hands and by engaging with people, like-minded people and discussing it. So the social media aspect, we have a YouTube live channel. And in fact, you may be, I don't know if you noticed, as we are speaking here in Zoom, this talk is being live broadcast on the Support Vectors channel. So I would strongly advise you to subscribe to Support Vectors. I have sent you an email about it, Support Vectors. If you just search for the Support Vectors channel and subscribe to it, subscribe to the one that has the logo that says Support Vectors in white, that channel it has. Go and subscribe to that because I will be posting a lot of YouTube live this webinars and sometimes when I do it will be on very short notice so only if you subscribe to it will you know that I'm about to give a talk I would strongly advise that you do that again the next thing is then comes the rest of it support vectors has a LinkedIn LinkedIn has a support like this page maybe in the break if you guys can please take a couple of minutes and do that please go and in LinkedIn I believe the social term people use is follow is that correct you follow a particular company and likewise there is a Facebook page again by the name of support vectors remember support vectors does not have a space in between it's a single word support vectors you can look at that and if you could please whatever the social term is like or followers I don't know what the right term is do that likewise for Twitter so whenever I am about to give a talk I will post it on Twitter that in the next few hours I'm going to go live on YouTube with a certain topic so one of the best ways to stay informed is through Twitter, Facebook and Facebook. Are we together? I'll post a message in Facebook and I'll paste a message in Twitter. These would be the two primary, I mean, let's follow our illustrious president of the United States. He tutors a lot, so it should be okay so we will do that Facebook and Twitter and I suppose that summarizes our social media aspect YouTube live I would strongly suggest you do because many of these YouTube live videos they will stay public only when it is live so for example today's video anybody could be listening to it whether they registered for this workshop or not while it is live. So for example, today's video, anybody could be listening to it, whether they registered for this workshop or not, while it is happening. But the moment the workshop is over, that video will become private. So only students or participants of this workshop will have access to that video. So there may be other workshops going on that you have not registered. And if you keep an eye onto the YouTube channel, you might, for example, find a topic that is of interest to you. You might not want to attend the entire workshop, but on a given day, there may be a topic that you might want to attend in that particular workshop. And that's a free attendance. You can just come and watch the youtube video for the duration so that's the social media aspect of it guys so let me summarize what i said and then we'll take a little bit of a break we haven't gotten into the subject at all we just talked about the tools of the trade we will use the google cloud platform if you want to use amazon azure they are also fairly capable platforms or IBM Cloud or Oracle Cloud, and by all means do that. But you might have difficulty getting help from your peers. There are three programming languages, pick one as your primary and one as your secondary. I would strongly advise you not to stick to only one language. These are very easy languages to learn. Python, R, Julia. Between Python and R, I would suggest that these are good languages to pick up. If you want to stretch yourself, you can do Julia. By the way, I'm about to give this again a YouTube live session on Julia. So for example, I'll explicitly take one of the data sets that we will do in this workshop and give you and walk through the solution of that using Julia. And you will understand that solution much better if you have done it in Python or R after you have done that lab. And I'll time it in such a way that once you guys have done it in python or r then i will give the free session on julia right we will use kick it learn and uh not only skik so skikit learn and these are the traditional libraries matlab visualization libraries numpy skipy but we will also use the automated auto ML versions of these and auto ML will be a big thing for us so the one action item for you these are the action items guys go create a GCP account in GCP see what a collab notebook is co la V collab notebook is we will walk through all of it on Wednesday and so if you feel intimidated then wait till Wednesday we'll do it slowly but create a Jupyter then if you want to get a head start or you know what we are talking about go create a Jupyter notebook in GCP see how you can allocate a different kinds of hardware powerful hardware, not so powerful hardware and so forth. Then go and if you want an on-premise thing, the advice I'll give you is if you're sitting on a puny little laptop with 8 gigs of RAM and you know old machine, I would strongly suggest you upgrade your hardware unless you're willing to do it in the cloud. If you're willing to do it in the cloud then of course you're totally fine. You don't need to upgrade your laptop. But if you want to do it on your laptop, sitting on the beach, then get yourself a good laptop or workstation or server. The textbooks for this class are ISLR. This book, by the way, is completely free. If you just Google ISLR, the word, you will see that the book will come up and is free legally free so it is a PDF those of you who like to read it on your iPads and Kindles or you know e-readers and so forth I don't know if there's a Kindle version of ISLR but certainly there's a iPad you know you can as a PDF you can put it on your iPad and read it, or your tablets and read it, or your obviously your computer monitors and read it. Then for many things, you'll have to refer to my lab notes, I'll give a lot of lab notes. Some of you are familiar with my lab notes, you will also get also two books worth getting are for Python. There is a very good book called Pandas for Everyone. Strongly recommend that you get it. And another is Modern Data Science with R. Again, a great, great book. This workshop will entail the following things. We'll have lecture sessions, of course, not to mention, for example example we are having a lecture session now besides lecture sessions there will be lab work that you have to do you'll have to do a project so guys form a team a team cannot be more than four people are we together and don't i mean unless you're really confident don't form a team of just yourself it's a bad idea you'll have to struggle because the projects that i'll give you will be tiring enough and they will be based on data on Kaggle in fact it is quite possible that i will i may create a closed competition in Kaggle and you guys can form teams and submit your solutions to Kaggle. It will be open only to students of this class, to this workshop and that will introduce you to the concept of competing in Kaggle. Most of you know what Kaggle is? I don't know does anybody know what Kaggle is? Yes. Just Kaggle and offline data. For those of you who don't know, Kaggle is a wonderful site in which they sort of, it's a community structure. You go and find lots of data sets and you find a lot of analysis that other data scientists have done. And they also have competitions in which teams of data scientists get together and they try to get better and better results. Right. And one of the teams that Kegel is running these days is obviously COVID data. We will actually be analyzing the COVID data as part of this workshop. So your project work will be on Kegel, okay? And let me go create a project for you there if I can. Otherwise we'll do it on GCP itself and just share it. There will be quizzes and data science interview prep. Those will be extra sessions. I mean, in other words, those are not out of the 12 sessions here. I invite you to come to those quizzes and data prep sessions. People in the past have found it very useful, especially for job preparation. So come to those. And it's also a good way to get reality check. How do you know that you have understood the theory? Just because you can solve the problem in code doesn't mean you have understood the theory. Theory and practice are entirely different things with a lot of overlap, but they are different things. Then after every class, I'll be sending out a poll. Please do fill out the poll. It will take you at most a minute. But if you do that, if you're not lazy about that, think of it as giving back to your teacher, to your instructor, because that's how I know whether the class was effective or not there's a discussion group slack I'll send you guys all invites to the slack group please do join the slack channel if you are not part of it then every Sunday I give a talk which is a free open talk think of it as a meetup and in fact there is a meetup group for support vectors. So it's a free webinar, Pacific Time Noon. Please come to it, bring your friends, bring anybody you want, invite. It happens every noon. Lastly, on the social media side, we have the YouTube Live. So we have a YouTube channel. Please do subscribe to the YouTube channel. Over time, I'm going to populate it with a lot of videos, many of them public, some of them private, that only you guys can watch. LinkedIn has a support vectors page. Please do follow or like or whatever it is. Likewise for Facebook support vectors, Twitter support vectors. If you guys can subscribe to these in some form, I will post a lot of notification directly through Twitter and Facebook. The reason for that is I have realized that many things are useful to people beyond just the participants of this workshop. We now have a large list of participants who have come to support vectors it is in hundreds many hundreds so quite often they too are interested and so instead of sending out mass email which is usually not a very good idea and Google tends to frown upon such mass emails and for me it's a pain actually because most people don't check emails these days, the young crowd, they just Twitter and Facebook and Instagram and so forth. So in keeping with the modern times, I will be posting notifications on Facebook, Support Vector's Facebook page and on Twitter, the same post I'll put on these two places. So do please keep up with those two things. And I really mean it guys, otherwise you'll miss out on things. Right? There will be a discussion, the Slack will be our discussion group where you guys can in a way, in a private way, discuss with each other your problems. I'll monitor the Slack group and there are a few TAs in the class who will also be monitoring it they'll be helping you just help each other out with solutions if I notice that you guys are not able to solve a problem I'll step in and solve it for you lastly you guys can if all else fails you can by the way the website is vectors.com and the phone number is fairly easy to remember. Learn AI. I hope this is easy to remember. So and you might be surprised that there are a few cars roaming around in Bay Area whose license plate decal has this. If you want a decal, if any one of you are willing to use the decal on your car instead of advertising a car dealership, if you're willing to advertise support vectors, let me know and I'll ship you support vectors, let me know and I'll ship you a license plate, you know, decal or the border for that, whatever the word is with this. Anyway, that's that guys. And we'll take a break. We have given the better part of an hour discussing the processes of this class, but now let's processes of this class, but now let's jump into the theory, jump into the subject itself. So I will pause the recording for just a few minutes. It is 8.20. Should we gather at 8.30 guys? Is that reasonable? Yeah. Okay. Yeah. Let's meet at 8.30 and I'm pausing the recording. So any con, any conversation that you guys do now is off the record. All right. We are on record. See guys, with this machine learning, I'll just set the framework of what we'll do. Broadly machine learning has two traditional branches as this actually three traditional branches. One that we won't cover in this workshop unless time permits. One is people what people call supervised learning. One is unsupervised learning. Reinforcement learning we won't get time for in this workshop, but we'll definitely do it in the deep learning, the next one. What is supervised learning? These names, these words are not something I'm very fond of actually. And these boundaries are not airtight. There is such a thing as semi-supervised learning and so forth. So I would think of them as more like Venn diagrams. There is a supervised and unsupervised, and there's an intersection zone in between. So the simpler way to remember this is it is predictive modeling. Predictive modeling. And this is pattern recognition. A very good primer. In predictive modeling, what you do is you basically are given a lot of data x vector xi an example is you are you are trying to sell ice cream on a beach the example that i keep giving is if you're an entrepreneur trying to open a shop on the beach a little shack an ice cream shack You have to get ice cream. It's a perishable good. You go to a wholesaler and every morning get some amount of ice cream. If you get too much ice cream, it will go waste. If you get too little ice cream, you lose business. So every single day, what you want is you want to feed in the parameters of what the temperature is likely to be, what the wind speed is likely to be, whether it is a workday or a weekend or holiday, weekend day, and whatever other parameters that you think is of relevance, you know that somehow these things affect how much ice cream you'll sell on the beach. Right, what you want to predict is, let's say here, you want to predict that for this input, for this day, let's say on the on a particular day where the parameters are these, temperature, wind speed, work, whether it's a work day or weekend, and you can add some more parameters as you feel appropriate, how much ice cream will you sell like the amount of ice cream that you predict you need a black box that can predict and what you want is that this prediction should be as close as the actual reality that turns out to be in other words on that particular day, how much ice cream did you really sell? So the way we do this is a two phase approach. First, you need some historic data. This box needs to learn. And the way it needs to learn is you give it some data, let's say a row of data on a particular date, one, two, three, four, five, certain certain number of days you have all of these things let's say this is temporary this is pressure this is wind this is rain whether there's a rainfall or not and at the end of it on that particular day how much ice cream did you actually sell sale of ice cream. And so you have certain amount of value and you have this now as you build a model. You take this data and you do something called training you train the mark train the model, the algorithm. What it means is, what it basically means is that it will learn to generalize from this. Let's say you have here, I've just written five examples. In reality, you'll have a lot more examples, but what it should do, let's make it 500, 500 examples. Now out of this 500 examples, you have certain data. It is observing what the y here is. And this part you'll call the x vector, x for a given day i, xi, and you have a certain yi, the sale of ice cream. Now, what you want to do is something that can generalize from this so that if you give it a new XI, let me just say X alpha, a data that the machine has not seen. For example, you get a temperature that isn't exactly there in this column or a combination of temperature, pressure, wind speed and rain that is not exactly there in your historical data that you have. Yet your machine should be able to come out with a prediction y hat alpha that hopefully is as close as to what happens in reality on that day should you wait and see what happened on that day that is when you achieve this then you say that your box your algorithm or this box whatever it is you fed it an X I and it produces a yi a prediction and that prediction you say is good suppose you say it's a good prediction it means that you have learned to generalize or generally you say your machine learning exercises to some extent successful and learning is successful to certain degree and there is no end to it. You know, so how do you know whether this is the right model? You never know. And right doesn't have a meaning in this space. What you want to know is, is this a useful model? Can it solve the problem? So this is one form of supervised learning or predictive modeling. Another is, suppose I give you different kinds of pictures, right? One is of a cat, right? One is of a car, right? And things like that. And every time I show you a picture, this is able to emit the word cat or cow or car, right, or truck, sort of like that. It can tell which of these it is. Let's say four classes. I show you pictures of either a cow, cat, car, or truck. And I give you a lot of examples, labeled examples. In other words, I tell you what the answer is. This picture is a cat. This picture is is a car and so on and so forth then you ask you give this machine a picture that it hasn't seen is it able to correctly identify whether it's a cat or a cow or a car or a truck if it can do that you say that the machine has learned to classify. This too is a modeling exercise. The previous one you say is regression. This exercise is regression. Obviously, this is for those of you who have some background in statistics, this may look very basic. So the difference between classification and regression is that the label, the target space is a real number for regression. Regression, the target space is real number. For classification, it is a categorical categorical means a finite set of values let's say cat cow car truck etc a finite you have to predict from within one of this or one way to look at it is it is an identification task this is a great value for, you are sitting there and a friend comes and says, can I borrow $200,000 from you? You would say, now, wait a minute. $20 is one thing, but $200,000 is a lot of money. Well, he's a very close friend. What is a determining factor? How would you decide whether to give him the money or not? You need a very high degree of assurance that the money will come back because you might have saved the money for your children's education or things like that. In which case, you don't want to lose it. You want to have a good assurance your friend will give it back while you want to help your friend things like that so those are classification tasks to tell whether somebody does or does not have COVID right that's a classification task in this task again you can make mistakes they can be errors in your prediction just as they can be errors in how much ice cream you'll actually sell but you want to be close you want to be mostly right you'll almost never actually sell, but you want to be close, you want to be mostly right. You'll almost never be perfectly right, but you want to be mostly right. These are exercises in predictive modeling. Now there is another set of activities that people do, which is not about modeling, not about learning from data to predict, but it is about a pattern recognition. So in this there are things like clustering. What is clustering? Suppose I give you data and you notice the data is like this. The moment you see data like this, what can you tell? The human eye can immediately spot that there are these clusters. These are this cluster one, cluster two, cluster three. Cluster three. Cluster one, two, three. Cluster two and cluster three. It turns out that there is no deterministic algorithm. It's not like sorting and you can find clusters. Clustering is actually a machine learning problem. It's quite interesting and hard to discover clusters in data. problem. It's quite interesting and hard to discover clusters in data. So there are algorithms, very, very smart algorithms that find clustering in data. And those algorithms are many. For any one of these typical algorithms we talked about, regression, classification, clustering, almost every week one new algorithm is discovered that can do the job. Then another example of clustering or pattern recognition would be outliers, anomalies. In the data you may have outliers and anomalies, novelties and so forth. There are various words that people use. Now what uses this finding anomalies and novelties in data? For example, fraud detection, right? Or you have a lot of measurements. If you find an anomalous measurement, you have to decide whether it's valid or not. For example, you have the vibration of a machine. Most of the vibration is showing a certain frequency pattern, but all of a sudden you start seeing outlier data, anomalous data. It is of value because it may be predicting or it may be pointing to the fact that your machine is about to break down or may have broken down. Some part in the machine may have broken down. Fraud detection, credit card fraud, fraud is very common these days. Intrusion detection, when people have intruded into your intranet and they're trying to hack your machines the those hackers they have a unusual pattern of behavior so if you can observe that and from that infer that this is an anomalous activity it is hacking going on and things like that so anomaly detection is a big thing in pattern recognition. The other is dimensionality reduction. What do we mean by that? Let us say that your data is, suppose you get data in the XYZ plane. But so you can imagine a three-dimensional surface. But you happen to notice that Z is like most of the data is actually along a plane which is tilted. So in other words, data is actually along a plane. A plane has only two dimensions. So let me call those dimensions, those new dimensions, x prime and y prime. And so any point which is given by x, y, z can be mapped to a point on this plane, a new coordinate system in this plane, x prime, y prime. With just two coordinates, you can describe the data point. To do so is called dimensionality reduction. You have found that most of the data and all the data can be represented exactly or approximately by a lower dimensional representation. So that is dimensionality reduction. That is also something we'll do in this particular workshop. Then the next thing we do, we will learn in this particular workshop is something called recommender systems. Recommended systems is based on which author you talk to, they'll tell you that it is semi-supervised learning, unsupervised learning, or supervised learning, depends upon it. The reasons are when we deal with this, you'll see why people differ in thinking about it. But recommended systems answer the question, what, for example, when you go to Netflix or any of these movie or what is it spot Spotify is that it or Pandora and things like that music stores music sites what music would you like to hear what books would you like to buy what items would you like to purchase, what movies, right? And things like that, movies you want to see. So what it does is it somehow finds a pattern. It recognizes something about you and something about the item, which is not obvious. And based on that, it is able to recommend things that you would find interesting, that is unique to you. It's very personalized. The foundation of this is personalized. So when you see an advertisement on television recommending that you buy that fancy car, that is not personalized. They are advertising to pretty much everybody who is watching the television channel. Personalized is when a machine has figured out that you are the target market, you're the person who would really like to drive, let us take a Tesla, a particular model of Tesla. And it is not recommending the Tesla to other users, but it is recommending it to you, because it figures out that you are the right person to buy it and you're likely to buy it, or that it's recommending a book because it knows you'll find it interesting. So those are recommender systems. That is the scope of our current workshop series. Now in this workshop series, one of the first things that comes is, and the way to look at it is, this is a lot of algorithms. For example, for regression, being able to predict a number, the sale of ice cream on a beach based on data. So I take this as a quintessential example. Real life examples are of course much more complicated. Your first homework will actually be on the California housing. You'll have to make a regression model to predict the house price, the median house price based on certain data. So this is real data. It's a bit old from the 1990s, but you'll do that another example would be breast cancer to predict whether or not somebody has breast cancer based on certain features those are relevant things these are two things but then we'll also deal with so these would be your labs when you take any of these problems you ask yourself which algorithm should we apply. In regression family, there are many, many algorithms, countless almost. There's a huge number of algorithms. Every time I feel I've learned or I know about most of them, I find yet some list of algorithms I didn't know about. And after so many years. I'm still pleasantly surprised by new techniques that I don't know about Sajeevan G. And some of these techniques are not new. They have existed for a long time. It's just that I was not aware of it. Sajeevan G. The same is true for classification. There are countless ways to classify and more and more new ways are coming all the time. more new ways are coming all the time. Then likewise for clustering and for in every task in machine learning there are many many algorithms. So the question that I want to ask is how would you know which algorithm is best? That is the thing and you find in the community a lot of people who are in a competition my algorithm is better than yours and so forth and people get rather hung up on it for example a few years ago many many years ago and this is the history of the field but the algorithm called the decision tree was discovered You will learn about that in this workshop. When decision trees came about people got really carried away with it. It could do regression, it could do classification, it could do all sorts of things. So the people who were strong patrons of decision tree, they began to make statements which in hindsight doesn't look particularly smart but in those days look plausible. They said, all of machine learning is decision trees. All you need to do is learn decision trees. All others are either classic methods or obsolete methods or unnecessary methods. Decision tree is all you need to know. Well, a few years later, when people had better understood the limitations of decision trees and the train had moved forward, then came other things. For example, support vector machines came and a lot of people, they're swayed by this kernel machines. They thought this can solve most problems and much better. So this is the state of the art. Then came ensemble methods in parallel and then the landscape was split. There were things that support vectors did better, there were things that random forest and XGBoost etc did better. Today if you go and look at the Kaggle competition you will observe that a lot of these competitions are won by people who actually use these methods who use random forest or xg boost and so on and so forth and what happens is it takes a long time to become really good at any one particular algorithm so when you invest a lot of time becoming really good at it one algorithm you tend to use that algorithm to get better results on quite a large number of data sets than with other algorithms, because in other algorithms, you're not so invested. You haven't given so much time to really become power users of it. Then you begin to believe that your algorithm is the best. then these days for example the new fashionable thing we're in the world of deep neural networks so there's a general belief that deep neural networks is the thing people gleefully go about saying that everything else is uh obsolete right and you don't need to do that and so there is some well for each of these statements, it's not that these statements are false, there is some truth to it, quite often, quite a bit of truth to it. For example, for neural networks, there is a wonderful theorem that says, is the universal approximator theorem. It says that, well, I'll just state it now, but later on we'll see it, that a neural network can approximate any function. So, any mapping from input to output space, it can essentially learn. So therefore, people say, well, therefore neural networks are most powerful. The trouble is, it may learn, but it may learn in the most inefficient way. There may be more efficient ways, simpler ways, or more explainable ways of learning the relationship between input and output, you know learning to generalize for example if you take a deep neural net it can do predictions but it can't explain why it made those predictions it is not as good as a simpler model which can make equally good predictions but you can clearly see why it is making those predictions. There's an explanation apparent from the model. So in that case, the latter is superior. And that is something that is becoming a dominant theme these days. We have models that are easy to explain. And so let me make a little graph that is true of our world. How understandable a model is, what is it trying to say or what has it understood, explainable and accuracy of power. Power of an algorithm. Usually what happens is, different algorithms fall at different points in this, but generally the most powerful algorithms are black boxes. They're very poorly explainable and then a lot of the very simple models are, they don't have as much accuracy on complicated data. Occasionally you may end up in a situation, a lucky situation like this. You'll get a very explainable model which also has high accuracy. So the question therefore is, how do we know which model to pick which algorithm to pick so in that context something to remember is a landmark result in human knowledge it is called the it has a very frivolous name actually no free lunch no free lunch theorem now here actually it is theorems because it's a group of theorems this No free lunch theorem. Now, actually it is theorems because it's a group of theorems. This was discovered towards the end of the last century. And it is considered not only a fundamental result in machine learning, it is considered a fundamental result in the theory of human knowledge itself. What it says is that no one algorithm is more powerful than other algorithms in the general case. There will always be some data sets for which algorithm X will be better, and there will be some data sets for which algorithm Y will be better. And in general, if you look across the whole space of all possible datasets, X will not outperform Y or vice versa ever on the average. That's a profound result if you think about it. What it means is that your most complicated neural network will not in general outperform the simplest algorithm of sorts. So it's something to keep in mind that it is a result, it's a mathematical result that holds. And so never get caught up in this useless discussions about which algorithm is more powerful. Every algorithm has a place. Now why is this no free-loans theorem true? The reason there's no free lines theorem is true is because every algorithm, it has a worldview. It thinks that the reality looks a certain way. For example, one algorithm may insist that the reality, let's say, let's take one dimensional situation. The relationship between X and Y is a linear relationship. Another algorithm may believe it's like this, right? The relationship between X and Y is a linear relationship. Another algorithm I believe it's like this, right? The relationship between X and Y is like this. Now, which of these two algorithms will do better? Well, it depends on your data. If your data looks like this, and this algorithm will do better. If your data looks like this, then your straight line algorithm will not look good. In fact, in one of your previous labs, those of you who did ML100, we took this as a lab exercise to prove the point. Because if you try to do it using a straight line algorithm, it will draw a straight line like this, which is not a good straight line. It's not the right thing to do. This line. Now you can see that this line is not as good as the white line model. So it's sort of I'm giving you the intuition of why it is true. Different models make different assumptions about the data. And those assumptions will sometimes be true and sometimes not be true. The ground truth data you say is the ground truth. The ground truth may be entirely different. The forces that produce the data may be entirely different from your assumptions. And so your algorithms will work sometimes and sometimes not work. Now in one of the labs, you'll see this. As you go and do many, many labs, you will see that we are seeing different algorithms succeed at different times now what we will do at this moment is or in this thing is we will take this we'll take regression as a problem. Regression will be a topic, right? What does regression do? Given the input Xi, it will produce a Y, a prediction, Y hat. By the way, predictions, we always traditionally represent it with a hat, predictions wear a hat. And reality is just Y, what is the real value that you observe, right observe so this Y will belong to a real number so in other words it's a the amount of ice cream that you sell on a beach or the stock price of some company that will happen to be for tomorrow and things like that those are predicting a number so if you say I believe that Google will be at some number, let's say, I don't know what Google stock is at 2000 or whatever. Let's say it's 2000. I don't know what the value is. I don't do stock markets. So then that's a prediction. It's a number. It's a regression model implicitly that you're talking about in your head. So the way I want to start this exercise is we will, before we get into all the different algorithms and we sort of unpack them, we look inside the box. What do those algorithms do? We will take a whole bunch of algorithms that your libraries come built with and without understanding necessarily what they are. One of them is linear regression. Linear regression makes a straight line kind of a model. We will take that. It's a generalization is polynomial regression and you don't have to understand these things to use it. Next time when we do the lab, I'll walk you through and I'll show you how you can actually be effective with algorithms by knowing just enough to get started. We will do decision trees, decision trees, we'll do random forest, We'll do boosting like XGBoost, CatBoost, etc. We will do, besides this, we will do, while we are at it, we will do support vector machines. And as you may have guessed, this company, this workshop that you're taking is under support vectors. The name comes from the landmark algorithm actually called support vector machines. So we will use support vector machines as one of the algorithms. And even within this we will have a lot of variations what we will do is we will apply all of these algorithms to about three data sets and see how they work especially for those of you who are taking a machine learning class for the first time, this will be a way to set some ground rules. So for example, how do you determine if our algorithm is good? It's making good prediction. So we will learn something about called loss function. What is a loss function? We'll learn that. What is and how do you determine if a model is good? We learn these things as we view and in this we will look at some, we'll look very carefully at things like mean squared error, MSE. We'll look at residual analysis. And we'll look at various plots that give you a hint whether your model is right or wrong. This will be a context. Now that is part one of it. The second part of it is that we'll learn ahead of the theory is if every model, if you apply a battery of models, you know, a collection of models, some will succeed, some algorithms will do better than others. As the algorithms do better or worse than others, should we just pick the best model or can we essentially use all the models together to get an answer which is better than the best model best individual algorithm and yes you can do that how you can use a benefit from not one but many algorithms to get a better result so there we will learn about something called ensemble of ensemble of algorithms, models and you learn in particular about things like the voting, boosting, we'll start with that and stacking and stacking is very important these days. So this is one of the least appreciated aspects of machine learning, I would say. Most people get very fixated on finding the best algorithm for doing a data analysis. And rarely do they realize that even the best algorithm can be beaten, easily beaten by taking an ensemble of many algorithms, each of them contributing some value in making the prediction. And when you do that, you get results that exceed the, almost always exceed the performance of a single model. I've often heard people say that, hey, we live in the world of deep neural networks and computational, plentiful computer hardware. Then we can just train one single big neural network and that will solve a problem. That is not true, actually, for multiple reasons. First, if you have a lot of hardware, if your problem is simple, you already achieve the meet or exceed the expected performance levels, the accuracy levels, or predictive power, and then you're done. Many problems are simple. So a single algorithm may be all you need to tune. But in many hard situations, it is much better to use an ensemble of algorithms because if you have more hardware, all the more reason you should use many, many algorithms put together to get even better results. The second reason is that all of these algorithms, they have most of the complex algorithms, they have dials that you can turn just like in a radio, you have knobs or dials. These are called hyper parameters. hyperparameters. Hyperparameters. These hyper, actually if you write it in capital letters, you can read it. Hyperparameters. These hyperparameters are ways in which you can essentially reshape the algorithm, the model. You can sort of mold it in a slightly different way. And when you do that, then your results change, your predictive power changes. Now these hyperparameters are hard to discover. The learning process doesn't learn the hyperparameter. It says, tell me what hyperparameters to use use and then I will build a model. And so there is a game of discovering the best hyperparameters. Now, hyperparameter tuning is quite an interesting game. We will learn it and we'll use the hyperparameter tuning as our doorway to understand the concept of automated ML. to understand the concept of automated ML. And I want to introduce automated machine learning early on so that it becomes a habit with us to use this as much as possible for situations like this. So that would be the scope of what we will do going forward. So let me recap what I have been saying since the break. I said that the algorithms that we will deal with broadly, well, machine learning is supervised learning, unsupervised learning, and reinforcement learning. Reinforcement learning is very interesting, but a little bit beyond the scope of this workshop. We'll do it in the next workshop. Very interesting. You want a person to beat a human being, I mean a machine to beat human beings at chess? Reinforcement learning is the way to go or any of these games is good. is what you use to teach a machine to do a really complicated task by giving it punishment and reward but we won't cover it here we'll focus ourselves with predictive modeling and pattern recognition predictive modeling is broadly two particular sets of activities regression and classification regression is when the input given an an input, you predict a number. How much ice cream would you sell? What would be the temperature tomorrow at noon? What would be the wind speed? You're predicting the weather. What would be your stock value tomorrow of any stock that you're observing. Now what would be the price of a house five years from now? So typical in US people spend a median of three to four years, three to five years in a house and then they sell and move somewhere else, right? So obviously it is a worthwhile question to ask when you buy a house. Would the house price go up or not? Right not right that is it or take the question of Zillow can you predict the value of a house just by knowing the parameters of the house those are examples of regression examples of classification is given for example a picture can you tell whether it's a cat or dog or car or whatever it is, right? Identify. Classification is to identify from within a finite set of possibilities given an input. That is classification and to learn to classify you have to do an internal representation for that, somehow build a model to generalize. In pattern recognition the things that we'll cover in this workshop are obviously we'll do clustering. We will do very novel ways of clustering. And in particular, we will do something surprising. We will cluster in such a way to see whether we can do some startling things. Like for example, suppose I give you a lot of images. Those images belong to cats, dogs cats dogs trucks I don't know let's take cats and trucks but the data is not labeled you don't have enough level data in other words you don't you don't have enough data to train your algorithm on. Maybe you don't have any training data at all, right? But you just have cats and trucks, pictures of cats and trucks. Can you use clustering? And this is very remarkable. It's a hot topic these days. Can you use clustering to essentially do the same thing that classification does? Find clusters, divide the data into clusters, such that each cluster of data, each cluster of images, is either a cat or a truck, with very little pollution or impurity in the clusters. Very few mix-up in the clusters. And if you can do that, then actually you can use clustering for supervised learning for classification that is actually a very hot topic and in fact one of the growing areas of active research we will do that the lab one of the projects that I'll make you guys do based upon which of your teams pick it up is that and the value of that is huge. Getting label data is a very manual task. The dirty little secret in machine learning is it takes a lot of human beings who quietly sit and label data. Quite often you have to sit and label data. You take a lot of pictures, for example, ImageNet and so forth. A lot of people have sat down and essentially they have labeled the data. This image is a picture. This picture is a cat. This picture is a house. This picture is a truck. But if you can use techniques like clustering to automatically do that for you, that's a tremendous saving and has a lot of value. We'll do outliers and anomalies and novelties. Outliers, a simple example of outliers is, you know, suppose you have a classroom and most children they're 10 year olds and so most children are between three and a half four feet let's say four feet to i don't know five and a half feet and all of a sudden in the mess comes a seven foot kid right so that is an outlier in terms of height stands out in the height dimension. So now generalize it to higher dimensional spaces. If you have a data point that sort of stands away from other data points, while other data points are near each other, but this one sort of stands out, it is an outlier. Anomalies is a related concept. It is the discovery of something that doesn't look good, that doesn't, that looks odd in the data. So suppose you're measuring the temperature of people and all of a sudden in a medical chart, you find a temperature that says, instead of saying a hundred degrees, it says a thousand degrees. You may wonder whether that was a mistake or whether the person or the instrument accidentally took the temperature of something entirely different, for example, a fire. It couldn't be the temperature of a person. So those are anomalies and novelties in data. What unique or interesting things you observe. Novelties and outliers are sort of very related so i'll drop them together we'll do that we will talk we will do exercises in dimensionality reduction like how can you find lower dimensional spaces to represent the same data we'll do recommender systems now one thing i talked about is that algorithms are partly Now, one thing I talked about is that algorithms are partly sometimes algorithms that are explainable may not be very powerful and very powerful algorithms are not explainable. Today actually, as you know, we are in the midst of Black Lives Matter. So this may be a good topic to bring about. Many of you know that machines can now do facial recognition, right? And the facial recognition technology is notorious. Its errors are notorious and they're all biased against minorities. So one powerful sort of a story that evolved is at one time, I believe it was Amazon, which in its infinite wisdom decided to release or commercialize a facial recognition system to the law enforcement. So you could show it a face and it will tell you whoo, Ph.D.: They happen to notice that it actually makes mistakes for racial minorities. So what they did is they took the pictures of the entire Congress and the black caucus. Dr. G R Narsimha Rao, Ph.D.: All the congressmen who are black or senators were black it took their pictures and it showed it to this facial recognition system. it to this facial recognition system, very, very often, in fact, almost quite often, it would identify the picture not to belong to the senator or the congressman, but actually belong to some serial killer or murderer or, you know, rapist or something like that, pretty criminal entities. And it just shows it's terrible, that it failed so miserably and obviously the Black Caucus was not amused and they wrote quite strongly about it. The biases in these algorithms are so severe that IBM recently, I believe just this week, announced that they will abandon their research in facial recognition because of the huge potential for racial inequality and bias that it introduces. So now let's think about it. The people who create these systems are not deliberately creating biased systems, but people who do all this work in artificial intelligence, machine learning, they are all well-meaning engineers. They are you and I right so how is it that we got it so wrong so completely wrong it has to do with two factors these algorithms that do facial recognition these are deep neural networks they are notoriously black boxes so if you ask them what is something they'll say it's a cat or a dog but if you ask it why do you think it's a cat or a dog but if you ask it why do you think it's a cat or a dog it won't be able to explain the explanation is either too complicated or it is not something that is for a normal human beings to understand so they lack explainability and because they lack explainability. And because they lack explainability, they cause a whole world of problems. For example, whenever you gather data, you may not know that the data has bias. If the data has hidden bias or subtle biases, which you may not notice because nobody wants to gather data deliberately with bias, bias creeps in. You don't know it is there. Then your prediction model will pick it up. Facial recognition system is a classic example. You look at most software companies or groups, you can imagine what the demographics in most software companies is. And if you look at these product managers and all of these, certainly the Blacks are a minority or they're there many minorities are underrepresented in these companies so if they are just looking at the employee image database to train these algorithms you can easily imagine that it will work well for the kind of demographics represented there, but it will work miserably on a different demographics. So these days there is a huge trust. There's a story actually that is told in this context. I don't know how true it is, but it certainly makes a good point and it makes for a good story. So I'll tell you. So the story goes as follows. One fine day, the military heard that there are all this Silicon Valley eggheads who are doing amazing things in recognizing with AI. So they came to apparently a startup and they said, well, can you tell us the distinction from an image between a civilian vehicle and a military vehicle? the AI group said oh yeah very easy just give us a lot of pictures of civilian vehicles and give us a lot of pictures of military vehicles and he said absolutely no problem then you can imagine that they went back gave orders to their soldiers to take lots of pictures of civilian and military vehicles and those huge amounts of pictures came back people started the startup those data scientists began to train those models then in the lab they were getting extraordinary accuracy they could tell just from a picture you would feed it in an image file it would tell whether it's civilian or military with very good accuracy, very low error rate. Military was impressed. They said, we want to bring our own pictures now, pictures we haven't shown you, a test data set. They brought the data set in the lab. It again worked perfectly, even for images it hadn't seen. So they knew that the system is not cheating. It's working. They took the system into the field. cheating it's working they took the system into the field when they took it into the field the story goes that the algorithm failed miserably now why did it fail they were completely puzzled the same algorithm was working just fine in the lab and now the same algorithm is not working in the field and after a lot of head scratching up they realize apparently the subtle bias in the data what happened is that you ask a bunch of soldiers to take pictures of military vehicles well when do military vehicles gather in the early morning before they leave for their you know trips and in the evening when they return from their trips, from the patrolling. So in the camps you find all these military vehicles abundantly in the early morning and the evening. At that time the light conditions are dim. When do you find civilian vehicles? When does a soldier see civilian vehicles? Well, when he's sitting in a military vehicle and patrolling the city and when he's patrolling the city in a war zone let's say he sees civilian vehicle all around in the daytime he takes pictures and the light conditions are bright right all of those things have a lot of ambient light so what the machine learning algorithm apparently caught on to is that you don't have to do anything you just have to look at the amount of ambient light in the pictures and use that to classify whether something is military or civilian obviously but that is also the reason that it fails but as the story illustrates and i don't know how true the story is as the story illustrates the biases are subtle we often don't catch up and therefore one of the hottest topics these days is something called explainable AI and as we in this workshop will talk a lot about this concept explainable AI so how do you take a black box model and open it up somehow create interpretability around it? There are many ways of doing it. One of the ways is you take a complicated model that makes very accurate predictions and you approximate it with a simpler model that makes almost as good prediction. But the simpler model is explainable right and there are other techniques it's a very hot area of research in this workshop as a project you will actually be doing it on explainable ai as we make progress are we together so all right guys today we just took a grand tour of all that we will do in this workshop I'll be sending you guys email right after with the textbooks and resources and so forth and please do attention to that get started with the picture at this particular moment what should you do between now and Wednesday if you have to do something I would say go and pick up the book ISLR so right now what I would suggest is pick up the book ISLR read chapters 1 to right definitely go review chapters 1 & 2 are we together and try to have read chapters 1 & 2 one is very simple it just says this is a book that will talk about this chapter 2 is the real chapter it's not and it is really worth reading see if you guys can read that. If you have a PDF, read from the PDF, but come back with reading ISLR. So I hope you all know what I mean by ISLR, Introduction to Statistical Learning Using R. It's a freely available book. You can buy it. I would highly recommend you buy it. I belong to the old school of thought. I like my books on paper rather than ebooks. I like to highlight and underline and do all sorts of things, put notes inside books and do all sorts of things. If you are more the younger generation that never likes to read printed books, then on PDF in your iPad or tablet, do it deeply, study it deeply. Another thing that you could do is you might want to get started brushing up a little bit of Python, go see, if you don't know Python, watch a few videos on the basics of Python. Remember, we are not going to need a lot of it, we are going to need only a little bit of it. But that little bit you should uh just make yourself there are many many websites that teach you for something like that pay attention to those pick that up uh if you uh would and i would highly recommend if you also get time with our basics r is another data science language if you are an aspirational thing would be to Python. Aspirational thing would be, there is a language called Julia, if you may not know this, beautiful language. We'll talk a little bit about Julia. Python and R, what they are, they're interpreted languages on top of the language. So you have a choice, either you write your hyper code and then you put a Python layer on it, or if you write it in R in Python, it'll be much slower. Julia takes a different stand. It takes that let's have a language designed for scientific computing very much like R is dedicated for science computing but has the performance of C language in fact C language tries to achieve the performance of Fortran and so forth almost I think at the moment they're almost as good as C I don't think they did see I haven't seen the latest benchmark, you guys can see it. But that's the beauty of Jupyter, it's a young language. Should it take off, I would say that it is very promising because you write your code once, not in two languages, not in C and then put a Python there on it. As you know, most modern frameworks, let's say TensorFlow, they are written in C, C++. But when you and I code, we code it in Python because we are using the libraries and those libraries underneath are in CC++. So that's the situation with Julia. So that's the reading material guys. I will send out these handwritten notes to all of you by email. This thing, I'll just export it out to PDF, so you'll get these notes. Now, there's some action items that you should not miss. One is, these action items are the, make sure that you have a GCP account, guys. Decide, do you want to do the labs on your laptop? Then have a good laptop, or you want to do the labs in the google cloud let's make a decision if you want to if you insist you're very familiar with amazon cloud or azure cloud or ibm cloud or oracle cloud by all means you can do all of that but you might be a little handicapped because your project mates might not like it right So make sure that all your project mates agree that they will do it in the same language that you're talking about. If you are adventurous, you can go and try to do the installation of a conda, et cetera, but let's keep that for the future. Textbooks, get yourself this textbook. Must get yourself ISLR, start reading it. Pandas for Everyone and Modern Data Science with R. Jay Shah, Dr. Vinesh Pramlalli, he, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his, his We'll do projects. Projects are good projects, guys. And let me just say, projects is where you will do most of the learning. So take your projects very seriously. We will have quizzes and data science interview prep. Those of you who are looking forward to a life in data science right after this workshop or trying to interview, by all means, come to those quizzes and data science right after this workshop or trying to interview, by all means come to those quizzes and data science interviews. Also right after this class, I'll send a poll. For heaven's sake, respond to the poll because if you don't, I will have no idea whether I did a good job in explaining things to you. Today was just an overview of course, we didn't go very deep, but still it's worth it. Discussion group, I'll send you an invitation to the Slack. Please do join the Slack. Next is every Sunday we have a free YouTube live webcast. It's a webinar and by the way you can ask questions. It's done using Zoom so if you join the Zoom session you can also ask questions then comes the things that are important i have decided that i'll put out announcements on twitter and facebook under support victor's channels so please do by the way how many of you uh did anybody here go and do those things? Subscribe to the YouTube channel and follow on LinkedIn, Facebook, Twitter, subscribe to all of these? Yeah, done. Yeah, thank you guys. So please do that. And then you will see that I will use this primarily as a communication vehicle. will use this primarily as a communication vehicle. Anything that is not to be just for our class this batch, something that others too will benefit and we have hundreds of students who are sort of waiting to get announcements in the room. So and we live in the world of Twitter these days, nobody checks emails. So I will be using that quite a bit. Anytime you get stuck, you can always reach out to me using this number. Now, I'll try to respond or somebody will try to respond. We talked about various forms of machine learning, supervised and unsupervised, predictive versus pattern recognition. We talked about anomaly detections and pattern, you know, dimensionalityality reduction cluster and so forth we talked about the value of explainable models right so what you ideally want are very explainable very powerful models but there's usually a trade-off it's hard to get both and your goal is the holy grail is to have both are we together we talk about the new freelance theorem which basically says that no one algorithm is necessarily superior to another algorithm however simple or so forth it depends on the data data the ground truth sometimes prefers one algorithm sometimes prefers the other and and on average they tend not to have a particular preference. We talked about regression and we will take regression as a first set of data exercises. We'll start with practical work next time, even before we understand what support vector machines are, what random forests are or XG boosts are. We are in a world in which a lot of these algorithms are there and we can with basic understanding start using them. We will start using them even before we go deep into the theory. So we'll alternate between theory and lab and theory and lab. Mondays we'll keep for theory, Wednesday we'll keep for lab. And you will have projects, do the projects, and on the weekend obviously it will be for cutting edge topics. Next time we'll learn a little bit about loss functions etc. in the next theory, most likely Monday. But on Wednesday we will see it in action i talked about ensemble of algorithms of how you should not just use one algorithm and optimize it you should instead try to use ensemble of algorithms we also talked about hyperparameter tuning we talked about automated machine learning and lastly i said, we give you a couple of stories of why interpretable AI is a good thing. Then lastly, I gave you guys a reading assignment, read chapter one and two of ISLR. Are we together guys? Right? And if you want to be adventurous, go and pick up the basics of Python or R whichever, pick one and then you can later on pick the other if you don't know it. And with that guys, I'm done. So I'll take questions. I will stop recording so that you guys can freely ask questions. One second.