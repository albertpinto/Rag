 So the topic for today, it's a new topic. It is clustering. And we will learn about a class of algorithms that are called clusterers. about a class of algorithms that are called clusterers. This is a large family of algorithms, very much like regression and classification. They are large families of algorithms and every time newer and newer methods are being found to do classification and regression. classification and regression. In the same way, the world of clustering is large and vast, but it is a shift away from making predictions. So far, our machine learning exercise has been to predict something. From today, we will focus not on prediction, but on pattern recognition. We are looking for certain patterns in the data so there would be nothing to predict there would be no target variable we would just look at the data the entire feature space of the data and ask what is it that we are seeing in it is there a pattern to this right are there something interesting things is there something interesting going on in the data that we can discern? And that is the topic. Usually this pattern recognition is called, I mean, unsuper supervised learning methods so the machine learning community has been pretty uh divided into two i mean not community sorry the subject has often been split into two sub areas that of supervised learning so-called and unsupervised learning these words are old words and in some sense i'm not very fond of these words i prefer using the word a predictive modeling and pattern recognition part of the reason is we now have classes of algorithms which are hard to in a very clear-cut way say.D.: supervised learning or unsupervised learning, we have the world of semi supervised learning, we have the world of. Vipul Khosla, Ph.D.: Self supervised learning, and so this word of supervised and unsupervised learning has gotten a bit complicated there's more and more things to it, and sometimes there's disagreement on what to call it. or how to think about that. So we will use more explicit terms like predictive models, recommenders, dimensionality reductions, clustering, pattern recognition topics, and so on and so forth. So today we are going to talk about that. What are the topics that we will do? We'll, as usual, give half an hour to reviewing. What is it that we learned last week? Now, in this case, it wouldn't be last week. It would be what we learned this Tuesday. In case you missed Tuesday, I would encourage you to go watch the video. We covered an important topic, discriminant analysis. So that, we'll review that, then we'll start with the world of clustering. I'll explain what that means. And we'll start with the simplest of all clustering algorithms, perhaps the k-means clustering, which we will do in considerable detail today. Time permitting, we will also do agglomerative clustering and some aspects of it, like linkage and so forth. Then finally, we will look at what are some of the limitations. As we do these algorithms, we see they are very effective. They do work quite often. And in fact, these are the workhorses of the industry. Whenever people think of clustering, very, very often they think of k-means clustering or hierarchical clustering, agglomerative clustering. But these things do have limitations, which is why there are other algorithms that we'll cover next week. Those are density-based algorithms. And if time permits, a few more. Now, in the lab, we are going to do three things. First is we will do the Wisconsin breast cancer data set. This would be a practical walkthrough, a data set that has been of significance for many years. It goes to the heart of this endeavor within machine learning community, within AI, to be of value to the medical community, to be of value in the diagnostics. It is, at this moment, we are talking 2021, end of 2021, we have mixed success in many places it is a good assist tool for diagnostics in many other cases it could do better we are not there yet for example for covet 19 detection from x-rays and for cover 19 detection the AI, there is a report that says that the many, many approaches were tried in AI, but actually it did not succeed. It did not, they all had enough success, but not good enough success to replace or to be deployed in the field at large scale. That just shows that the state of the art leaves scope for improvement. And here also, we will talk about the breast cancer dataset. I'll give you a little bit of a background. It's a simpler dataset than the COVID dataset, COVID X-ray dataset. The COVID X-ray dataset will do, or one of the datasets, we will investigate it in the next workshop, but because that needs a processing of large image files and building models over them, it needs deep neural network and things like that. So we won't be having that here. So today we'll start with something simple, the Wisconsin breast cancer dataset. the Wisconsin Breast Cancer Research. While we are doing it, I will also take the time to introduce something that Holt, one of the people involved in the WECA project, I believe, not Holt, okay. So one of the people made an interesting breakthrough. They pointed out that sometimes very simple algorithms yield results very close to the state of the art. So we are often fond of new tools. Remember I talked about the photographer. We all get into photography and the first thing we do is buy big lenses and big, the state of the art, you know, DSLR bodies. But as you know, great photography is done by people who are not necessarily using the shiniest toys, but who are skilled in the art. In a similar strain, there's an observation in the field that quite often, in many, many, many situations, actually you come very close to the state of the art by using very simple models, surprisingly simple models. You wouldn't have expected them to work. And therefore there's a word of caution or an encouragement. Start simple. Don't ignore the simple because there are shinier toys around. So we'll learn that lesson. Finally, I'll take a detour. You all have been seeing some data visualizations. They're all the standard bar plots and box plots and so on and so forth. Now, visualization, as I said, is where art and computations meet. Science and art meets, engineering and art meets, or whichever way way you say the stem field and creative arts meet and people have created hauntingly beautiful visualizations. we will start with a very simple visualization which i would consider which has been very important and we'll go through it and see how step by step you can build a pretty what looks like a complicated uh visualization and you can sort of build it step by step we will take in fact nightingale's florence nightingale's classic Two Roses visualization, which had such a profound effect in the world historically. In fact, it is a visualization that practically started the modern practice of nursing. It helped convince England, the Queen of England and the people that we do need modern nursing practices that we have today. Now, that is sort of the holy grail in our world of data science, to create a notebook, to create a visualization, to tell a story with data that can change the world. Would you not like to do that? And this is often overlooked. If you look at engineers, they tend to create very complex results. They say a lot, but they often end up saying it in very arcane terms that only other engineers can understand. A lot of technical jargon visualization in a way cuts through all of that and makes what you find accessible as a story to to a broader audience right it helps you communicate with a lot not only with other data scientists effectively but with a larger audience it helps you tell a story with data to a much larger audience so visualization i would say is very very important today will take us a pause from just doing algorithms and notebooks data analysis around that and take a very simple data set i believe just 24 24 rows or so and create a visualization around it so that is the scope for today so with that out of the way i would like to now um get on to the topic of um clustering I will pause for a moment. If anybody wants to get tea or coffee, please go ahead and get it, and then we will start immediately. Any questions before I start from what we have learned so far? So is it possible to briefly explain the LDA concept again? We are going to review. Oh, yes, we are going to review that today. So we will do that. Any other questions? Guys, again, may I urge you to take your quizzes and do your projects in your labs or homeworks. Do the homeworks otherwise you know without practice nothing will sink in without repetition without practice nothing will say quizzes helps you repeat what you learned it makes you go back to the videos back to the textbooks to go find answers right how many of you go back to the videos in the textbook to find answers most of you do right it is the nature of learning you can't understand everything the first time you hear it the first time you understand but you know you forget very quickly but when you go back and review it that's when it sinks in when it settles down quizzes are important likewise practice is very important obviously you guys are very mature people i'm stating the obvious it's almost pointless to do that um it's hard to teach a whole doctor that you guys are not by any means old hogs by any means, old hogs. Sachin is filled with dog metaphors. He has a new dog. Dogs are not old. All right. So where are we? We talked about dimensionality reduction. We talked about dimensionality reduction. The discriminant analysis, quadratic and linear discriminant analysis. What we talked about last time is, if you go back to the example that I keep using of cows and ducks, you project them into the feature space. If you look at the data, what you see is if the x axis, the horizontal axis is the weight and the vertical axis is the volume. So these are the x1, x2 of your data set. Most of the ducts are small and light, so they will be clustered around the bottom left hand corner where you see the ducks and most of the cows would be clustered around the top those blue dots at the top right hand corner are the cows they're big and heavy creatures and so the center of the center of gravity, the center of mass, if you want to think of it, of the data points, which is the mean of that two-dimensional mean vector of the ducts and the two-dimensional mean vector of the curves, or more intuitively, the center of gravity points of the two, they would be separated. points of the two, they would be separated, right? And what you find is that if you think of the weight, the data points, the concentration, their density plots, their probability density plots, it would, the marginal distribution would look like the yellow lines that I have drawn along the horizontal and vertical axis, right? Sort of, they're marginal. Along those axes, they would look like bell curves. Right? So this is it. And the two-dimensional distribution is, of course, like a bell hill. These are called normal distributions or Gaussian distributions. Or just to be more descriptive, I call them bell-shaped hills. Except that the bell need not be round, it can be elliptical. There's no reason why variance in one direction and the variance in the other direction would be the same. Now, with that being there, it almost invites us to connect the two centers, the center of the cow and the center of gravity of the ducks. And then once you have joined it with the line, and that is the dashed line here, then draw a perpendicular bisector of the two. When you do a perpendicular bisector, it becomes a very natural decision boundary. When you do this, and this sort of approach is called discriminant analysis. Here we have built a linear decision boundary. Now I'll show you there's a variant of it where the decision boundary can be quadratic or nonlinear. But to get the intuition right, this is it. But when you do that, we also realize that, see, this axis, the line connecting these two center of gravities, if you consider it as one axis, the first axis, new axis, and perpendicular to it along the decision boundary as the second axis, you end up with a new coordinate system, isn't it? We have a new coordinate system. And in this new coordinate system, any point can be represented either in the original coordinate system of x1, x2, or in the new coordinate system of which are marked in red, and've written as x1 tilde x2 tilde. Isn't it? This is basic geometry. A point can be written in two different reference frames or two different coordinate systems. It is the same point and one is the transformation of the other. One is some form of rotation of the other or rotation and translation of the other, right? In other words, it's a simple translation. Exactly. Rotation plus translation. So as such, we are not doing anything magical. But then we observe something interesting. We may argue that if we want to distinguish between cows and ducks, perhaps the only thing that matters is, or the majority of what matters, is where the point is on the X1 tilde axis, along the axis of the line joining the two centers of gravity. It just stands to reason. And we may, with the first approximation, we may choose to ignore the x2 axis altogether and that is the second beautiful insight from that so what we say is that by by looking at data like this we get two things out of it we get uh a classifier because by joining this line and doing a perpendicular bisector we have a classifier now we can classify cows and ducks in a very intuitive manner geometrically the obvious manner. Do we agree, and at the same time, or we get dimensionality reduction, we can say hey you know what in the new coordinate system let's forget about X two We can write a classifier in only one dimension, just x1 tilde, right? So for example, here it would be whether x1 tilde is negative or positive. If it is positive, it's a cow. If it is negative, it's a duck. Does that make sense? In other words, if it is any point here, any point here is its x1 tilde value would be negative. Any point here, its x1 tilde value would be positive. And so you can say, you know what, how much easier can it get? We have a lovely dimensionality reduction and a classifier that can actually work in lower dimensions with much less features right so that is the beauty of it which is why discriminant analysis is often quoted both as a classifier and as a dimensionality reduction technique go ahead frame sheet keeping the current context right in this current context the variance is the same for both the distributions that's an implicit assumption we made and we look at two dimensional categorization which is or characterization we had an exponent and we used just one dimension to be able to differentiate because after the transformation there's a positive and a negative that is right. That becomes the one qualifying dimension on which we know the value, we can classify. That is true. Let's say it's a higher dimensionality problem. Let's say there were three or four dimensions, then how do you generalize that? So what happens is, and we'll come to that, dimensionality is a big topic, but so I will deal with it separately. We have a whole day for it. But just to give you a preview, this generalizes well. So for example, let's take the ducks. At three features. Some third feature. Yeah, they look big and they look size. Right, so let me take this. How high can you can you fly right so the cows can't fly much to the as far as we know so are there on the z-axis they'll be sitting down there right and ducks the maximum they can fly will be pretty far up so now we have introduced a third dimension but nonetheless you realize that your data has just become ellipsoidal. It is an egg now. It is not an ellipse, but it's an egg-shaped, right? Ellipsoidal. Nonetheless, they do have centers of gravity, center of mass of the data points. And so you can connect these two and still split, right? And you can get your classifier out of that now the second question therefore that comes is what about the dimensionality reduction part see when the data is so well separated right dimensionality reduction will reduce it down to very low dimensions maybe just the x1 axis will contain most of the information. Trouble comes because reality often is not so well, like Gaussian distributions. Like for example, when we go back and look at, what was it, blueberries and cherries, they were not Gaussian, they were skewed distributions, if you remember from your Jupyter Notebooks, data science notebooks. And so is dimensionality reduction valid or do you still achieve it? What happens is that you would say that your dimensionality reduction is, I mean, always lossy, but the loss of information is significant. It is not as effective in that situation as it would be in this situation. So it is a case by case basis. basis and as always there is a data rules right data will guide you on how far your hypothesis works remember that when you have data anything any anytime you try any technique you are building a hypothesis that this technique will work because I believe data is like this. Before you try out LDA, in your mind, you have to have a hypothesis that let us assume that the data is separated out. Therefore, I can do and separate it out, hopefully in nice Gaussian shapes. Right now, that's an idealized expectation that may or may not hold. But the question is, what is the level of degradation from that hypothesis, from the idealized hypothesis? And that will be the measure of success of your model, right? Whether LDA works or doesn't work, right, will be measured by how much the data, how much fidelity data has to your hypothesis in that way. And that's the right way to look at it. Remember that given a data, in effect, all reasonable hypotheses are equally valid. You haven't done anything with the data yet, right? You haven't seen it a priori. So you could try any, each hypothesis represents a different seen it a priori. So you could try any each hypothesis represents a different approach. One thinks in terms of logistic, one thinks in terms of LDA, one will do this. Many different hypotheses are every algorithm has a worldview that the data looks like this. And if it looks like this, then it will be very successful. But if it doesn't look like this, there's a graceful degradation, then there'll be partial success. So you don't know. And in fact, this is one of the things we will talk about in a coming lecture. One of the big themes is, I'll give you a preview of it. It is actually called the no free lunch theorem or theorems because there are many of them. Now the no free lunch theorems are, it seems, it goes back to, there's a bit of a history to it. The no free lunch sounds almost like a joke or a frivolous name for a landmark or an important theorem in machine learning. But okay, let me take five minutes since this question was raised to introduce the no free lunch i'll talk about it just briefly so you jump in and disqualify that thing that i asked a little when you chose the third dimension with the ability to fly that still gave a boolean kind of a situation because the cow can't fly the duck can't fly well let's add some wings to the cow the cow can't fly the duck can't fly well let's add some wings to the cow pigasus i was kind of doing it now just to keep the example going stretch it let's keep our shades of gray how gray is the thing let me address that i got your point let me i'll address it which means cows can be dark cows can be light that's also can be light that's also dark so that particular variable does not have any qualifying useful information that is true then yeah so that dimension yeah that yeah that is right so that dimension essentially is rendered useless and in fact one of the one of the great virtues of dimensionality reduction and machine learning is your ability to discern what matters and what doesn't matter, right? And we will learn about that. We will learn about that. Also, the other thing that you alluded to is the fact that we are talking of ducks, but what if you were talking not cows and ducks, but ducks and geese, and now there'll be overlap in weight and size, right? So these two gaussians overlap, but their centers still exist and you have still the perpendicular bisector. But what will happen is there would be error rates. In this situation that you look is idealized. You practically think there is no error here. It's very, very hard to confuse a duck with a cow, but it is far easier to confuse a duck with a cow, but it is far easier to confuse a duck with a goose, because there is overlap. And so there's inherent error, because if you do the perpendicular bisector, some of the geese will become classified as ducks, and some of the heavy ducks will get classified as geese right so that that brings in the error aspect of it because the two gaussians are sort of overlapped right why oh because nature loves gaussians and we love gaussians gaussians are lovely mathematical functions easy to differentiate do many things with them. Okay, simply because in nature, Gaussian's are all over the place. No, but what you're saying applies to other disciplines. Yes, yes. See, to your point, the point that he's raising is that, why just, why Gaussian's? It turns out that Gaussian's appear rather ubiquitously in nature. And not only Gaussian's, there is actually a deep paper. There's a paper actually, which title is provocative. It says, are normal distributions normal? So it asks this question, anytime you see a bell curve, do you just assume that it is a Gaussian? And it questions that, because there's a whole class of distributions in the exponential family that look visually without doing any mathematical analysis that look like a bell curve. For example, a log-normal distribution can mimic a Gaussian. You can't tell. Then you can make beta functions and other things and Laplace and there are many distributions that you can make to sort of mimic or be close to the Gaussian distributions, the student t function and so on and so forth. So this is student t distribution. The point is that, see in machine learning you make a hypothesis that is approximately correct. So you just make the Gaussian sort of stand in for itself, and many of the things that look like it. Because ultimately, the goal is not to be correct, which you can't be, you don't know where the data come from, but to build effective models. So because of that, now the question comes what if the data is not gaussian like for example we looked at our blueberry and our cherries there truly the data is not gaussian it has a strong skew one has a right skew one has a left skew right then what happens why are we using gaussians what happens is that using a gaussian approach as we saw in the lab you saw that even for that situation classifier one lda worked you ask yourself why did it work lda makes a gaussian assumption the point is that the reason it work is machine learning it's one of these interesting things I've thought a lot about. See, Gaussian is not it, but it isn't too far from it, right? And because the gradient descent optimization takes place, or any optimization that it takes place, will ultimately say, which is the best Gaussian I can fit to this data? And the best Gaussian that will fit in will be a good stand-in proxies for whatever the risk whatever the skew in spite of the skew in the distribution and the non Gaussian nature still it works right and it is this is one of the amazing miracles of mathematics that I find that sometimes you don't have to shoot straight at the target just shoot somewhere near and still works, right? And that is why, and Gaussian are, the tooling is there, everything is there. More than that, Gaussian see, there are things like the center limit theorem and so forth in statistics, which basically say, when the causes are additive, you tend to produce Gaussian distributions distributions like for example when you uh we did that galton's thing right with the rods there and marbles or little sand falling down it naturally formed a gaussian because at each place in the rod it could go this way or this way so there are additive effects from this this this, this, this, this, right? Additive effects, lots of additive effects lead to bell curve distribution. And we see that all over the place, right? So what would be your height based on your genetic height? The deviations would be based on so many additive factors, right? How well you listen to your mom and eat food as opposed to eat junk factors, right? How well you listen to your mom and ate food as opposed to eat junk food, right? How well your mom was taken care of during her gestation period. I mean, God knows how many factors, how the weather was, who knows? Who knows what are all the factors that lead to the deviation from your genomic or genetic height that it should be. Those factors are additive, they're independent of each other. Now sometimes what happens is factors are multiplicative, right? The effects are produced when you multiply factors. For example, the force of gravitation is a product of the mass of the two two masses, right? So there are many situations with forces where the factors that you haven't accounted, they're multiplicative, you just don't know them. Generally, when you have multiplicative factors in nature, you tend to see log-normal distribution. In other words, remember, log converts a multiplication to an addition, and those additive would be a bell curve. And so the original distribution, if you see a log-normal distribution, often you should, it should, and this is, you know, I'm now retaking a detour, and I'm telling you some of the things you learn from the underlying mathematics, and I don't know how much you'll remember it. But when you see log-normal, look for multiplicative factors. When you see bell curve, think of additive factors. It's just a basic common intuition. In nature, there are many additive factors that keep happening all the time. So bell curves are very common. Errors are often additive. You can say that there is a canonical weight that a duck should be and a volume size that a duck should be and a volume size that a duck should be. And so every other, a duck being something else is essentially nature's error. Right. And so it has a, that's one way to think about it, not actually correct way, but biologists will absolutely frown upon it, but you can, you can say, well, there you go, right, that's the, build intuition guys. See in this field, right, getting intuition is far more important than for that intuition to be absolutely rigorously true. So go with that. So anyway, I was going to talk about the no free lunch theorem. I'll just stay brief on it. We'll talk about it in great detail in a whole session. No free lunch is exactly what I said. See, whenever you have a data and you know nothing about the data you can make all sorts of hypotheses you can hypothesize that it is made up of bell curves that are separated out you can hypothesize it is made up of a mixture of bell curves actually i'm using bell curve because bell curves are one dimensional it's easy to get intuition but generalize it to higher dimensions. When you do that, those are normal distributions of Gaussian distributions, the bell hills. And it's pretty hard to imagine higher dimensional bell hills, but imagine that, right? Like I told you, the mathematical trick is, imagine in lower dimensions, but when you speak, say as it is obvious in n dimensions, right? And you'll be pretty much right hey asif to fit the gaussians to the data um would you use something like a kl divergent or something like that you are getting to an interesting point yes uh we could uh so uh Good. So I think our question is, can we, am I, this is you, isn't it? Yeah. So the question is, how do you know, how do you know that a distribution is, we made the assumption is Gaussian. How do you know that underlying reality is that? So one way you can do it is there are these measures, statistical measures, very beautiful measures. One of them is a KL divergence. KL divergence says the data has a certain distribution, you have a certain, but your hypothesis is the Gaussian. How surprised are you to find that it is not a Gaussian? Or in other words, how much does the actual underlying distribution differ from the Gaussian distribution? And so KL divergence is one measure and the topic is broad. We talk now, I mean, we get into even more interesting areas of transport theory and so forth, but let's keep that for deep learning. So, what about the Js divergence? It's the same thing. Js is a symmetric version of KL. Oh, okay. it's the same thing js is a symmetric version of kelp oh yeah but let's not bring those words see one reason i'm hesitant to bring those words is because we have people obviously the people who are learning this for the first time rank it that includes you and um no no because he is also doing a project with me with that so um at this moment, in the interest of people who are learning for the first time, let's just not bring in that machinery. But since you brought it up, that's your answer. Does that answer your question? Yeah, yeah, that suffices. Yeah, thank you. Yes. And the earth mover is the transportation problem or? What's that? The earth mover distance, is that the transportation problem are you mentioning yeah oh yeah essentially um yeah okay see here's the thing now we are getting into the territory that is the engineering math the math for math of data science let's keep it today i have a lot to cover guys i have a lot to cover by the way uh the interest list for how many of you are enrolling for the math of data science, that list contains just one or two people. Remember that my minimum quorum is 15. See, here's the thing guys, there is a for this business. Okay, so, Kyle, please stop the recording. So the no free lunch theorem states this, a very obvious fact, theorems actually, they are theorems. So they say things in a technical and very precise language. And they represent one of the most profound discoveries in the theory of human knowledge actually from the field of computer science uh we have not made too many advancements to the fundamental theory of knowledge itself right like i'm talking about big milestones like for example godel's incompleteness theorem which says that no body of knowledge will ever be complete, right? And things like that, profound statements, proven mathematical statements. So things like that we haven't contributed. But there is one contribution that came out, and it is very surprising people in computer science, not many of them are fully aware of it, is the no free lunch theorems. Very profound. It is a revolution beyond just computer science. It states that given many, many theories, many, many hypotheses, in general, no one algorithm, no one hypothesis or algorithms, no one algorithms will always outdo another algorithm. always outdo another algorithm. So these days you must be hearing fancy statements that, oh my God, you know, we have, see every generation, and I've been in this journey for a long time. It used to not be called, what we call it all of these words now. When I was in grad school, we used to call it, well, we did a lot of numerical analysis and we did a lot of numerical analysis and we did a lot of modeling and we just called it modeling and models model building and numerical analysis and so on and so forth and obviously the subject has been evolving since then and we did machine learning gradually so over the years many generations of algorithms have come that have become the fad and people have said that if you have that you don't need to learn anything else. It was true about decision trees. There was an algorithm called decision trees, which you learn shortly in a couple of weeks. And people said, once you know decision trees, it can do regression, it can do classification and so on and so forth. You don't need to learn anything else. Machine learning ultimately boils down to gossip plus decision trees right there are people who seriously said that and so decision trees people came up with faster and faster implementations of decision trees it was an industry today we know that's not strictly true nobody in his right mind in 2021 would say that that all of machine learning comes down to decision trees then came uh for example the kernel methods that support vector machines and things like that. The completely radical revolution in machine learning. And they were very, very effective for a very long time. Along the way came the ensemble methods, random forest, XGBoost. Even today, now the new statement is if you're dealing with tabular data, all you need to know is xg boost it solves all problems does very well always out does everything else then came deep learning deep neural networks in many class of problems they just say all right deep neural networks wipes out classical machine learning i've heard many many such statements from people it says why bother like people have asked me why bother teaching this course at all the only thing you should teach is deep neural networks the other course right because this is all irrelevant right so don't get this hard it is not irrelevant all the effort that you're putting in is not in fact it is a foundational theorem it is the no free lunch theorem right and remember theorems are forever your software goes through v1 v2 v3 versions and there are bugs in it but in mathematics a theorem once proven is true for eternity there is no version two of the theorem ever there is no version two of the pythagoras theorem it is a theorem right so in the same way no free lunch theorem says that there is no algorithm that is always superior to another algorithm. For some domains, for some data sets, certain algorithms outperform other algorithms. But in different domains, other algorithms will outperform this. And we will go through this exercise. At one point, I'll have you compare the performance of many algorithms on a dataset, a hard dataset, and you will see what happens. So anyway, that's a digression. We'll leave that. So remember that every algorithm is a hypothesis on what the data is. But only when you examine the data, you will know what the data is. The data will speak. It will have its story to tell. And the way the difference between your hypothesis and what the underlying ground truth is will show up in the prediction accuracies and other indicators, empirical indicators that will show up. Anyway, so that is the answer to that question. Now, anything else about discriminant analysis? Yeah, one more thing. In the discriminant analysis, there's a technical point. If we assume, you know, the center of gravities are separated out. But if we assume that the shape of the hills is identical, they are identically distributed, the cows and ducks in the feature space, then you end up with a linear decision boundary. I gave this very intuitive picture, just a poor sand from two bottles near each other, right? And when you see, you will see two hills of sand, but forgetting the z-axis, the height axis, if you look at the line, it will be a straight line cutting through them. The perpendicular, you know, the perpendicular connecting the hills. The line that is the decision boundary is a straight one. Now do another experiment. Take sand and take something a little bit slipperier so that when you pour sand, it falls like this. But when you pour that, it sort of flows and it has a much harder time forming a big hill it flattens out right so it spreads out so now the variance of the two hills are different in such a case just observe and you will see that the boundary between them is curved it is curved towards the it is curved towards the peak here the the hill with the smaller variance, right? Towards the sand hill. That's how it will look. Just do this experiment on your table and this point will become pretty obvious. I wish I could do that. There's a bigger table forming. But then also I'll create a mess and we'll have to clean it up. But anyway, do it at home. Right? With your kids. You'll have fun. So anyway, that was all about discriminant analysis and its review. Right? What does discriminant analysis do? Because one advantage of discriminant analysis is it very naturally extends to multi-class classification. Right? analysis is it very naturally extends to multi-class classification, right? Because if you connect the, suppose you have ducks and cows and tiger, you connect the center of gravities of the three things, you get a triangle. The perpendicular bisector of each of the sides of the triangle will still meet at a point and it will divide the feature space into three regions very naturally. And so discriminant analysis very naturally extends to multi-class classification. With logistic regression, for example, that was heard because it would always make a binary classifier. So how do you extend it to multi-class? You have to build many, many models, one versus rest. And see, is the probability of a duck versus not being a duck higher than the probability of being a horse versus not being a duck higher than the probability of being a horse versus not being a horse then it's a duck you know things like that you have to reason through but you have to build many models in linear discriminant analysis you have to build a single model and you're done so this is it guys and that is all i have to say about discriminant analysis any questions this is a review of last time. Any questions? Yeah, Asif, I had a quick question. So when you're transforming the, like if you go up to the cow and duck example again. Cow? Okay. Yeah. So when you're transforming it onto the X1 tilde coordinate system, what would the coordinates now be? Like how are you quantifying those coordinates? Think about just rotation and take in did you do if you do coordinate geometry in high school you remember that rotation and translation yeah just cosine there is a theta cosine theta sine you know that matrix rotation matrix that's it so x x1 tilde is x1 cosine theta minus x2 sine theta. And likewise, x2 tilde is x1 sine theta plus x2 cosine theta. Remember that equation. So it's pretty much a rotation equation. Plus there is a translation. It is a rotation and a translation. That's all. OK. Thank you. plus there is a translation it is a rotation in a translation that's all okay all right thank you that's that one or do we multiply uh one i mean x1 or x2 with a discriminant so that you do a matrix transformation is that what we do yeah yeah see it's the same thing uh see this matrix right can be decomposed into a translation and a rotation okay okay it is visually obvious if you let me let me know how do you see what happens is let me use a color that i haven't used and i hope this thing cooperates you see this point right this is the origin let me just call it origin o and this is the new origin o tilde right i'll see the pointer now oh i don't know it is just that uh you can see the pointer moving the big point okay okay? So what is happening is this goes here, isn't it? So just think of it, the coordinate system, which was like this here, has gone to like this here. How would you do that? First step you would do, step one is, you would take this coordinate axis to this point right you would do a translation from here to here would you agree you just check the y-axis to this once you have shifted the y-axis you rotate the x-axis rotate it to be parallel to them now rotate it over you're getting okay that is it and you also not only do that you translate the the the y axis this thing also up here so first what you do is you go here so your your axis will look like this and now all you need to do is a theta rotation And now all you need to do is a theta rotation. Okay. That is why you say that it's a translation and a rotation, basically. And then when you put the two matrices, when you take the two together, it will become your transformation matrix. And effectively, you're in search of that. So all of these things are one advice that I'll give to all of you. See this simple intuition, I don't know how it is. Sometimes when I read the books on this topic, it gets into very complicated mathematics. It does a lot of matrix this and that. They're trying to say the same thing, but they say it in a much more formal language. Whenever you see complicated mathematics, don't be intimidated by it. Just break it down into pieces because ultimately what is being said will be something very simple geometrically. Think geometrically, ask what is it, and it will be something absolute common sense. And that's all it is. All right. So with that, if that is done, any more questions in the review? If no more questions, we'll take it is we have been speaking for an hour. It is end of review. One part of our theory is over. We'll go into clustering. So let's take how many minutes break? 51010. Let's take a 10 minute break. We'll start by talking about pattern recognition. This is the new territory we are entering, what we call unsupervised learning and I'll treat these as synonymous. See, whenever there is data, if there are causative forces behind the data, then are you guys able to see me clearly? Remote people? Yes. Okay. I don't know how well I'm basically, but okay. It's not that my head is being chopped off or something like that. I'm clearing. Okay. So maybe a little bit. Yeah. All right. So whenever you have data and the data has some reason, some cause that produced it, generative forces. One of the things you may observe very often is that there is a pattern in the data. Are we together? Data has pattern. You expect patterns to be there. Let's say, so this is illustrated actually in a very common example. Suppose you take lots and lots of data, right? Let's imagine that those data sets are all two-dimensional. Like for whatever reason, all sorts of data, wind and pressure, this and that, whatever it is. And you gather the data. And if I were to ask you, how often are you likely to see any pattern in the data how would you answer that like there are lots of data that have no pattern for example a white noise has no pattern right if you have sand completely smooth it out then if you take a picture of it in the location of every grain of sand, you project it as a location that may not have a pattern. There are many, many situations where you don't have a pattern, but there are situations that you do have pattern in data. So if you were to ask this question, how likely are you to find pattern in data? How would you answer it? What proportion of the time? Now this is for the new students to answer only how what proportion of the time would you say that there are patterns in the data 80 percent 80 percent that is a fairly good guess anyone else would like to venture what about you to venture what about you more than 50 percent and you how much would you say dhwani 60 percent of the time you will see data anyone of you who are attending the class for the first time would you like to give a different answer sanjeev here make a guess it's all a guess. It's all a guess. You don't know. What's the question? Are you attending it for the first time? Yeah. Okay. Make a guess. My opinion is that data always should have 100% of the answers. Some pattern? No. We just took an example when it wouldn't write. Smooth sand. smooth it so may not but we need to get out the noise with data minus white ones yeah that is that is one answer yes let's see what turns out yeah but it's a good guess and you have justified it well uh premjit you're also attending it for the first time how often do you see uh just noise or how often do you see pattern in data generally if you were given random sets of data you take the universe of all data sets all from all sorts of domains and measurements maybe about say 30 40 30 40 percent yes what's your guess to 40 30 40 percent yeah sanjeev deshpande what's your guess 20 percent sanjeev kumar what's your guess uh i'm just maybe about uh 60 percent absolutely it's uh all right so is there anyone else who would like to throw in a guess? A new student? So we have guesses. See, it is all over the place and it doesn't occur. Now I'll give you an intuition that might help in this. See, data is empirical data. It's a measurement of something that happened. Behind something happening there are causes quite often whenever there are causes those forces they leave their signatures and those signatures almost invariably leads to pattern in fact the most remarkable thing is when there is no pattern in the data then something is at work wiping out the patterns. I'll give you an example. People often used to say that when you turn on the TV and it is not receiving any signal, you are just, your antenna is just picking up random noise, molecular agitation noise of the air or something like that. Little bits of electromagnetic agitation is picking up. R. Vijay Mohanaraman, Ph.D.: And when you turn on the TV do you remember that it used to be noise. R. Vijay Mohanaraman, Ph.D.: Do you remember those days right it used to be just noise that would come up if if you're not tuned to a channel and it turned out that that too was not noise. What did it turn out to be? Come again? Test pattern? Universal big bang noise? Yes, Abhishek says universal big bang noise. Yes, indeed. Buried in there was a pattern. It was actually we were hearing, metaphorically speaking, we were literally seeing the reverberations or the echoes of the Big Bang still reverberating through the universe in the cosmic microwave background. And that was a tremendous discovery and of vital importance oh is it turned off or maybe it has run out of battery it is on but it is out of battery give me a second Let's try now. Once again, bad battery. So I need to throw this. Give me a second guys. I'll pause the recording for a moment, please. Generally it's very hard to produce pure noise. We know in computer science it's extremely hard to produce truly random numbers. We keep trying it, we create all sorts of pseudo random number generators and we always have a sneaky feeling that there is a pattern still hiding in there. Very hard to produce true white noise. You can come close to it but there may be a pattern still hiding in there. So pattern is sort of the norm so like abhishek said um it's almost always there in one of the tv shows i used to like and old tv shows which is about mathematics it is literally called numbers there are two brothers one is a mathematical genius a younger brother and the elder elder brother is a FBI agent solving murders. In one of the first episodes, there was a demonstration that went like this. Well, there was always a guy, a bad guy, who was murdering or doing terrible things. And the thing is, where is he? And FBI, they could not find a pattern in what he was doing, literally. And all the crime theories are based on the fact that the bad guys always have a pattern. He can find some way that they can do it. And the elder brother said, the FBI fellow said that there is no pattern here. And the younger brother, his first reaction was that it cannot be. If there is no pattern, somebody is deliberately trying to wipe their trace. They deliberately randomize them by doing something like that. And then he comes up with a model of how nonetheless you can find some underlying pattern, some way of separating them out. And along the way, he does a very interesting demonstration, which I invite you to do in your mind. which I invite you to do in your mind. He says that for every person enter into the room, I mean, I will change it. I won't do the exactly the same demonstration with something equivalent. If you just send people and ask them to stand just about wherever they want, right? One person at a time. And wherever you stand, just take an invisible marker and put a mark on the floor and then walk out. Then when everybody has done and you shine the light into the room, you will actually see, this is different from the one in the TV show, but you'll actually see that those locations are clustered. Similarly, if you just ask people to enter a room in which they are standing already standing and say go stand somewhere either they will deliberately try to stand apart right because they are consciously trying to spread out right or that is a conscious effort not to have a pattern or if you just leave them alone in a social gathering what happens people form groups they form clusters so clusters patterns they are everywhere right and they're there unless you put in an effort for it not to be there most often there's some pattern so now the question comes in machine learning what is the value of pattern recognition right what is the learning why should we care yes just want to add a i missed a little bit to that support for golf so although there's pattern in the data i mean the writer i wanted to add is by the time the data gets to a person who wants to look at it writer i want to add there is by the time the data gets to a person who wants to look at it multiple things have overlapped and after that it's hard to discern the pattern that could be true that is one possibility what premjit is saying is there may be pattern in the data but by the time you get it reality has changed and other things have happened those could happen they are always confounding factors. They always things that complicate things. But nonetheless, the pattern is there. Right. In anything you look at, there will be pattern. It may not be the same pattern that you initially it had. Now it may have a different pattern, but it will still have a pattern to it. And so there will be layers of meaning, layers of patterns sitting in something or changing patterns sitting in something. So for example, if you live in the desert, you literally can see the sand dunes every time there's a storm. There is a wind. The sand dunes change shape. The pattern changes. With time, the patterns change change so it is there that's true all right so with that I'll ask this question what is the value basically of pattern recognition why should we do that so again we're talking a very high level generality so let's actually go up in a balloon go up to a high level to a height see suppose you were take this example and do this thought experiment my professor used to i think use the word gedanken experiment that's a german for thought experiment so let's do that geranken experiment what you do is you are i hope i pronounce that word right you go into the city of san francisco you don't have a map forget that you have google maps go into the city of San Francisco. You don't have a map. Forget that you have Google Maps, right? You are trying to get your bearings. You are on the Market Street in San Francisco, and then you are trying to find the location of some restaurant or something, and you're wandering around. And what happens is that as you wander around, gradually you begin to get a sense of your local bearings, that this is perpendicular to that, and for food i need to go here and so forth you have some local understanding of what the lay of the land is locally but what happens if you suddenly got into a hot air balloon over the and rose above the city what will it do to your understanding in one glance i hope you would agree that you would literally see the peninsula you would see the financial district you would see the the the park san francisco golden gate park you would see where the beach is you would see where the residential houses are and you would get a much better intuition so that next time you're back into the city even if you don't have directions you would have a general sense with a compass which direction to go right if you're looking for shopping areas which direction to go would you agree with that right so pattern finding patterns is tremendously useful and it is a learning in itself the discovery of patterns is of vital importance. So the techniques that we are going to deal with today, we'll deal, there are two, I mean, there are many techniques that we'll talk about. Two of them, one I just mentioned a little while ago, which was dimensionality reduction. You notice that data can actually be, to the first approximation, considered to be living in a lower dimensional space. That's a big topic. We'll do it much more thoroughly later on. Today, we won't be able to do justice. But the second thing is we notice that in data, there are clusters. For example, if you rise up above San Francisco. So let me try to make an example of San Francisco. Let's say that this is the bay, right? And this is the, well, this is a terrible picture of the bay, but I will try to do a better job. And the sea, why am I not able to do that? Okay. So let's say that this is, and this here is, with this big point is getting hard for me to erase things okay and this is the and this is the golden gate bridge and for those of you who haven't been to san francisco area bay area i apologize but just see that this is land let me just mark the land with some very land like color okay let me instead mark the water this is water and everything else is land this is the bay this is the pacific ocean the pacific ocean this is san francisco in the San Francisco, you notice that there is this region which is, I'll call it like this. This is the financial district, financial and commercial district. Market Street. Yes, Market Street. Chris Dornan, Across the bridge is Marin County. Yes, and this is Marin County let's say here Marin County. Marin County. But yeah so this is San Francisco, well, this is not whole San Francisco that would be inaccurate, but this is a close to. Let's say it is, this is SF area. Now in this, there is a green belt, which again, I'll exaggerate and put it like this. The green belt, and then there is the housing belt. The housing approximately looks like this. It spreads out. What is a daily city and all the housing areas, and so on and so, and the other places. Is that a reasonable approximation? Yeah, the South San Francisco and all of that industrial area. They end in the industrial area again start. So then there is another industrial area that starts here. Yellow is the industrial area. So what do you see? Do you see that, do you see a clustering of similar things, a cluster for the trees, which is the Golden Gate Park, a cluster for houses which have tilted roofs, big buildings with tilted residential houses, and a clustering of this flat-roofed commercial district. You can, and high-rise buildings, you see they exist in clusters. In fact, most of the iconic pictures of big cities like Chicago, like New York, San Francisco, and so forth, all you see is clusters of high rise, isn't it? The skyline is clusters of high rise. So clusters are everywhere. So we will use, learn techniques of clustering, because to know this is in a way you can say, you learned something about San Francisco, the city of San Francisco, right? That is the purpose of clustering. Now, clustering of data is also inherent. We were talking of cows and ducks. Now, when we go back and look at it, what do you see here? In this data, you see clusters, isn't it? All the ducks are clustered at one place in the one region of the feature space, all the cows are clustered in another region of the feature space. Are we together, right? And when you generalize it to three, you cows, ducks, and horses, once again, or tigers, once again, there are three clusters. So clusters are there. They may be approximate, or they may be very clearly discernible clusters right so we will search for clustering now how do we search for clusters in data so one of the things is see if i so we will go with clustering today clusters