 One second guys. So we start. All right guys, let us review what we did so far last time we did two things we did if you remember we did boosting and we did a little bit of the recommender system or at least we got started with the recommender system the main point with boosting was and we in particular took gradient boosting as an example boosting is a process of creating a strong learner using weak learners in a way that you grow the you learn slowly you take a weak learner you fit it to data you see what mistakes remain when it makes the prediction in the case of a regression problem, for example, you take the errors from a model so far, and then you use it to build another model, another weak learner, whose job is to be able to improve upon the model by just fitting to the errors. So if you look at the screen, we said that we have the overall model is built out of pieces. It is built out of, let's say, that any given level, you take the model that you have built so far, and then you add one more model to it, one more weak learner to it. That is fitting to the residuals, for example, in regression, residuals left behind by the previous model. And you attach a damping factor so that it doesn't go and overfit, but broadly speaking, that's what you do. So the first model that you build, like the zeroth model that you build is as simple as it can be. It is simply it is basically a baseline class, a baseline regressor. It says that it knows all the predictors. It says that the predicted value is the average of the learning data target value, right? All the labels, all the target value values that you get in the training data, you just average it and that's your prediction. Well, it is a very weak predictor, but at least it makes sense. If you have no inputs, the best that you can do is predict the average. You do. Then what do you do? Next, what you do is, so that is the prediction from the zeroth model you look at the residuals that you're left with y i minus y bar that becomes the residuals and thereafter you try to fit a function to the residuals when you fit a function to the residuals a learner to the residuals that's what to the residuals. That's what your learner is. Now your learner, quite often people in boosting, they tend to use decision trees, but it may not be. You could use whatever you want. If you use that and fit it, notice that you're not fitting it to the y, the actual labels. You're instead fitting it to the residuals from the zeroth model. You will get another model now, f1, a decision tree let us say, for F1, a weak learner for F1. Now you have F0 and F1. You would put it together, F0 and F1, to get the overall big model, which is F0 plus F1 up to a damping factor. What will that model do? That model will now make predictions. Once it makes predictions, it will have its own errors, own predictions. Those predictions will have its own errors and those errors now you'll fit to F2 and you will keep on going up to a certain extent. You can choose m is equal to 20 or 30, whatever it is, 100. You can keep on going sequentially building a building ensemble with weak learners are we together guys so far this is the basic idea that we said we are pursuing now with this basic idea when we apply it in practice as you will will see today, it becomes quite simple. This is the gradient boosting, it's accelerated version, massively parallelized and optimized version is XGBoost. These things work very, very well. Now, that was the process of boosting, which is different from bootstrapping or bagging. Sorry, bagging is bootstrapping and aggregation, simple aggregation. In bagging, the advantage is you can build models in a massively parallel way. So for example, when you're doing parallel computing, if you look at big data frameworks like Spark, for example, you look at the history of Spark's machine learning library, MLlib, and you realize that they had random forest ever since the early days of machine learning, early days of Sparks, sorry. In one of its earlier implementations they already had random forest. Why? Random forest is very, in a way, in many ways straightforward to parallelize. You grow a lot of trees in parallel on bootstrap samples of the data and then you can do SGD, stochastic gradient descent of some form, and you do have the implementation essentially in place. Boosting is a little bit more tricky. How do you do sequential on a massively, like how do you get a parallelized implementation of it. And then now people have tried to do more of it. They have put it on GPU. GPUs are extremely good with parallelized computing or distributed computing, this sort of divide and conquer approach. And for example, CatBoost, another variant of it, and very, very good variant. It can run on your GPU, your video cards GPU. There's GPUs and R. If you can push a computation onto GPU, a machine learning computation onto a GPU, you clearly have a winner because GPUs do matrix manipulation orders of magnitude faster, sometimes hundreds of times faster than the most powerful workstation-grade general-purpose processor, like for example, Intel Xeon versus GPU, whichever GPU. So let's say commonly in the data science, machine learning, AI world, we tend to use Nvidia. I don't know why others haven't caught up yet, but Nvidia is very, very popular. So you take a typical GPU like RTX 6000. It's pricey. RTX 6000 or RTX 8000 will easily set you behind 4000 or 6000 or $8000. So these are not small change. But the computational benefit it will give you is almost 100x acceleration over your typical Xeon processor. Now the Xeon processor is a general purpose processor. It can do very complicated instructions, whereas the computations on the GPU are very very simple in many ways. They are multiplication of matrices and so on and so forth. But for the things that the GPUs do, they do very very well. But for the things that the GPUs do, they do very, very well. So now there are implementations of these things on the GPUs. Of course, our whole next boot camp, the TensorFlow, is all about GPU computing. We'll see. Sorry, deep learning. We'll do it using PyTorch principally, actually, TensorFlow. Also, we will do, but not to as an extent. PyTorch is more like beginning to feel more natural. If you have been doing NumPy, which we all have been doing in the first and the second workshop, PyTorch is almost a no brainer with very little learning curve. You become familiar with it. And that is its great power. It's very expressive, very simple framework. TensorFlow is quite powerful and has an equal mind share, but it certainly feels like a quite different metaphor for programming compared to NumPy and so forth. So anyway, we'll come to all of that in the next. So besides bagging and boosting, what else do we have? We have stacking. Stacking is taking some really well, you have tuned algorithms, you go and train your independent learners. If you so want, you can even take some strong learners though. And then what you can do, you can stack them. You can create a last level learner here. What does the last level learner do? It treats the predictions of each of the learners as its input and says, can I see a pattern in the way they're predicting and the strengths and weaknesses in the prediction? Maybe some learner is predicting well in some region and not in other regions of the feature space. So can I do a further learning on the learners themselves? And when you do that and then make a final prediction, the most obvious example would be just a linear combination, just some weighted combination of the predictions of the other learners. Giving more weightage to a learner, you trust more and less weightage to another learner and so forth. You could do that for regression, for classification, you can do some sort of a sigmoid of that. But more and more, what is common is not to do something linear, but to use one of the more powerful, like really powerful algorithm at the tail end of it. you what is common is not to do something linear but to use one of the more powerful like a really powerful algorithm at the tail end of it and what you put there typically is a svm or a neural network right you put a few layers of a neural network or sometimes just one layer of a neural network and we find that you can sometimes always, sometimes get a pretty good prediction from the sum total of all of it. So this is called stacking. Now stacking is of course much more sophisticated. A lesser version is just a voting classifier. Voting classifier just says you create an ensemble but you make it out of all sorts of different algorithms. You still do a committee, a jury, or a voting, but one may be a decision tree, one may be a support vector machine, one may be something, HDBoost, one may be logistic regression, and one may be a LDA or a QDA, and so on and so forth. And you take a whole bunch of classifiers and you just take a direct vote. So that would be the equivalent of saying, a voting classifier is the equivalent of saying that all of these votes, these rates are essentially one. Right? And you're just taking the average of them or rather one over n, n being the number of classifiers you have in your model. And so you're just taking a direct average of these. So that's a voting classifier, essentially bagging the response, the predictions from each of them. Now, so we learned bagging one example of this is random forest and then the extremely randomized trees and so forth etc I won't write all of them and boosting gradient boosting as I said otherost, I didn't emphasize too much. What AdaBoost does is it takes a slightly different approach. It grows a tree and then the mistakes are amplified to build the next tree and so forth. So you keep on composing, but by amplifying the mistakes. I didn't go too much into the detail. It's usually other people would perhaps start by teaching AdaBoost first and then doing it. But I thought because gradient boosting is so very dominant these days, let me stay with the main one, but okay, there it is AdaBoost, AdaBoost. And XG, XGBoost is essentially just a very fast implementation, fast gradient boosting, right? And then there are other variants of it, like for example, cat boost. I believe Dennis, you used cat boost and got pretty good result and your HIX was on isn't it? It is a very strong classifier. A very strong classifier. So just to give you a perspective the project that Dennis took up was the HIX was on and he's already getting a pretty good good accuracy of 83% isn't it? 83, 84? Looking at 84. 84, excellent. Which is the state of the art, isn't it? The leaderboard is at 84. The measurement is actually called C approximate median significance that has its own formula, but you know, 84 is pretty good, pretty good what is the highest there uh for ems uh it's like 3.8 or something okay yeah like I'm not sure about accuracy though. Okay. All right. So here we are. Then after this, we did another topic. So today our lab is on boosting. We won't get time to do the recommended system lab. That is actually part of the math course. I will do it there. The recommended system was based on the idea that you can do user-user similarity or item-item similarity, but sometimes just by observing users engaging with items, either listening to music or watching movies or buying things on an e-commerce site, by just looking at the interaction matrix here. Guys, give me one second my room has gotten Yes. So you can look at the interaction that is transactional data, maybe in some database, it says that this user bought this thing. Out of that data, you can create a matrix, the matrix of the user interaction with items. This user bought this and this item, and maybe gave this rating for a movie or a song or something like that. Based on those things, you can ask a question, you can make a hypothesis that the items have certain traits and the user has certain taste for those traits. And can we find that low dimensional representation? Here, obviously, we took an absurdly low dimension, a two dimension. We said a movie can be a thriller or not, and it could be humorous or not. And so we put all the movies somewhere on the spectrum. Two dimensions are obviously absurdly low. They don't capture all the movies somewhere on the spectrum. Two dimensions are obviously absolutely low. They don't capture all the sort of traits of a movie, but it sort of illustrates the point. So every item can be projected into the space and then every user can be projected into the space based on their taste. So items have traits and people have taste. And so you can see that there's a person who will like Sleepless in Seattle quite a bit because of their choice of a certain degree of humor and a certain action thriller, like the level of action they want to see in that. So this is the main concept. Now, the hypothesis that you make is that if you represent a user, the taste of a user, the degree of taste for each of the traits, you can represent it as a vector. Those are the attributes of each user, how much taste a user has for humor, for action thriller, let's say this is action trailer, this is a human and so forth. And likewise, every item can be represented as a column vector saying how much of humor action thriller it has and how much of humor it has. And therefore the inner product could be part of as the cosine could be part of as a good indicator, a pretty good indicator of how much a user is likely to resonate with a certain item. So this space, this hidden space of taste or traits that you say, that the more formal word mathematicians uses latent space. Latent means hidden. See, your matrix is the obvious data. This is transactional data. If you look at it, this is transactional data. But you're hypothesizing that the reason people buy things or engage with certain items or what certain is, is because items have certain traits and users have certain degree of taste for each of those traits. And so you have a latent space, not just two dimension, but typically ten or of certain traits and uses of certain degree of taste for each of those traits. And so you have a latent space, not just two dimension, but typically 10 or 20 dimensions or 100 or 200 dimensions, which are true representative of the item. It is that only when you look at a lot of interaction data. And how do you go find it? There are methods like alternating least squares and we do singular value decomposition and so forth. And when you do that, you're able to pinpoint a user in the latent feature space and you are able to pinpoint an item in the latent feature space then you can look at similar items in that space you can look at similar users in that space you can recommend items to a user by looking at what items surround the user you can look at users to whom you would like to sell the item by seeing what users surround an item. So you can run a campaign from a marketing perspective to sort of advertise those items to those users, recommend those users to buy this item with perhaps greater degree of success than just brute force broadcast advertising. So that is the world of personalized advertising. That's where it comes from. That's where the personalized ads come from. So we live in a world of personalization. This says e-commerce would not be possible today without that because e-commerce inherently implies that these big giants, the Walmarts, the Amazons, they have an irrationally large number of items in their inventory. We are talking about hundred million items or something like that. When you have so many items you can't really have a web page displaying them, even by categories. In any category there would be a very very large number of items and there'll be way too many categories. So recommender systems are actually pretty crucial to the e-commerce world. A lot of people purchase things that they have been recommended and that drives a lot of revenue and traffic. So that's that for Netflix, of course. But one of the big selling points was that they could they could tell users that you like this movie they would watch it and they would actually find it useful to their liking right so that's that though that's a summary of what we did now i would like to move on to the lab part any questions before we move to the lab guys to move on to the lab part any questions before we move to the lab guys if anybody has a question please ask me because so asif uh so for the loss function for the boosting so is there a function called like the boosting loss function the loss function is not usually specific to The method? Yes and no. See, think about it. Regression has a well-defined loss function, right? Yeah. Which doesn't terribly depend on whether you're using a decision tree or you're using a linear regressor or polynomial regressor. It is y minus y hat square. It is the error term. To it, you can add further regularization terms, etc. For decision trees is the number of nodes. In gradient boosting, for example, the extra term that you add is the damping factor lambda. That's your hyperparameter to regularize it. So that is the loss function there. The loss function, likewise, you can write for classifier. Now, what happens is that the answer to that roughly is yes. In the theory, when you do gradient boosting, once you write the loss function and you look at the residuals and you train your model on the residuals, there is the gradient of this sum loss over residuals and so forth. There are a few bits of formula and a bit of mathematics unique to gradient boosting. I did not cover that, because that will get us into all this business of gradients and so forth. There is a little bit of that. But if you have to look in broad strokes, your approach should be that, yes, every algorithm has associated with it its specific loss functions. But one simple way to go around, at least at this level, is to know that for classifiers, the loss function would be the cross entropy plus some regularization term. And for regression, it would be some squared error plus some regularization term. And for regression, it would be some squared error plus some regularization term. And invariant of what algorithm we use, ultimately in broad strokes, this is it. Are we together? Now, when you do other boosts and when you do, for example, gradient boosting, there are nuances and they affect the mathematics of it. Like for example, when you do, for example, gradient boosting, there are nuances and they affect the mathematics of it. Like for example, when you do decision tree, you go through a process. You don't directly, I mean, you do have a loss function. Yes, that's true that you're trying to minimize, but you also go about it using entropy, the entropy and the information gain, or the Gini, the index value is changing, right? Where does it have the best points and so forth. So those are the mechanics of it, the internal mechanics by which you bring down the loss. You do have a loss function, and then you bring down those loss values. So for each algorithm there'll be its own special bit of mathematics like for example for support vectors we saw the we saw how we wrote the constraints and how we wrote um stuff in in search for the maximal margin hyperplanes. So those things do remain so yes gradient boosting also has its own bit of mathematics. So those things do remain. So yes, gradient boosting also has its own bit of alternatives. Okay, yeah, that makes sense. Thank you, also. Sure. Sif, because boosting is computationally intensive, is this a classifier that we use secondary to optimization from others or can we just go straight to boosting right away? Actually, XGBoost and this CADboost, they're very fast actually. We'll see this in the lab now. Let's look at it in the lab. You'll realize that they are very, very fast. gradient boosting, any boosting, it's a black box model. So the question remains, is it justified to use a black box model? Your approach should always be to dial up from simple interpretable models. If nothing else, they form a baseline. So let's take an example. Suppose you have a model, Let's take an example. Suppose you have a model, a simple interpretable model, and it gives you 94% accuracy, or precision, or recall, or F-point score, whichever metric you care about. Now, let us say that the gradient boosting gives you 94.7% accuracy, or the metric value, whichever metric you are pursuing. The question needs to be asked should I use this black box model and even if you don't does it make sense to use the simpler model at least for explainability for interpretability. To cross check if in more all the regions of the feature space, the predictions of the two models match. If they do match, you might still use, let's say, a black box model, boosting, backing, random, all of this. But you may choose to use the more interpretable model when you're having a conversation with business to explain why certain things are happening, isn't it? So for example, if you're an entrepreneur trying to sell ice cream on the beach, if it can predict what the amount of ice cream you'll sell on the beach would be, the entrepreneur may be pleased that you made an accurate prediction. He has to buy or she has to buy a finite amount of ice cream and not have it go waste or lose business both ways. On the other hand, the entrepreneur will have this nagging question in her mind that what if the wind changes? Because weather predictions are not exact so what if the temperature is a little bit higher what if the weather changes what factors really affected it another basic question is that if you have an interpretable model you can ask could what should i do to increase my business do you see that isn't it so into interpretable models go a long way, much further than black box models. You use black box models by compromising on interpretability to a large extent. And when you do that, you lose a lot of value. Your approach should always be to start simply. And that's what I've been trying to do. If you notice that in this workshop, I have taken you and emphasized feature engineering and showed you that at least for the river data set, none of the black boxes beat feature engineering. Do you remember seeing that, guys? For the river and the flat, none of the black box models random forest support vector machines boosting we'll do boosting today and see they don't beat it they at most come close to it so feature engineering thoughtful feature engineering still remains the gold standard right so keep that in mind never forget that there is a no free lunch theorem. All right guys, so we are now, I'm going to switch over to the other computer and we are going to do the lab, but I would like to take, I need to, I can hear my dog barking today I am the dog in charge so give me a moment I'll take care of the dog and I'll come back I will just pause the recording for a moment Спасибо. Takk for ating med. Undertexter av Nicolai Winther Thank you. I apologize for the other machine. And let's share the screen sharing. I see the screen shared. You do. Good. So, let us have it. So we will go back to our river dataset. Now in the river dataset, in the flag dataset, just to recap, and by the way, I have posted this to your Slack channel. I don't know if I have posted the most recent version. One of you please remind me that I should post the most recent version today is please increase the font is the second last year I should increase the font for sure yes I do what if you do please remind me and make sure I do post all of this today Are you guys comfortably able to see the screen now? Just to remind you, the RIVER dataset was, what was it? Yes. The RIVER dataset looked like, hang on. The river dataset is, I must start, oh my goodness, I forgot to start the recording. The recording is in progress, isn't it? Okay, I don't have to restart. Okay, now what is happening here? progress is okay. I don't have to restart. Okay. Now what is happening here? Visualize the data set. Yes. So if you look at this visualization, just as a reminder, this was our data set. And with this data set, we have, we call this inner region the river and then this the sand. That's the intuition we use to make progress. Now, as we went through this workshop, as we learned new algorithms, we tried to apply each of these algorithms to this data set. And it sort of was a reference point to see how these algorithms work. I hope by now you must have seen that with scikit-learn for example going from one algorithm to another from a pure lab perspective, from a pure code perspective often was as simple as just changing the name of the classifier. Often was as simple as just changing the name of the classifier. You did not have to do much. Import the classifier and just use it. And we could do, because it's a nonlinear, we could do it using feature engineering and logistic regression itself, which is just as a recap, this is it. You saw us do the, the, and you saw us do polynomial regression, classify logistic with polynomial terms. We did a more feature engineering approach. We did all of this. We did a plot of the river. We extracted the river. If you remember, we looked at the distance from the river. So this was the best, this was a gold standard because if you remember the goodness of this model was, if you looked at how well this model was, the residuals are perfect if you get this approach. And with this, when we got a perfect map of the river, and from there when we did the feature extraction, this was our goal. We said if we extract our features well, then we can build a very simple, in fact one-dimensional logistic regression model. Nothing can be simpler. We transformed the problem to a very simple problem which was a classifier in one dimension the dimension being with the future being distance from the central the river and when we did that we got a confusion matrix that was pretty good this is a good matrix as you can see of diagnose are rather small and the classification report said we have our f1 score on our f1 score of 84 and 92 percent and all things taken together the 89 percent is what we came out with as the weighted average of all of these things. So weighted average of accuracy is 89%. Now for one, we got for sand, we got 92% accuracy. For river, water, we got a pretty impressive AUROC curve, area under ROC curve. Just as a recapitulation, what is the area under ROC curve? Should it be more or less, guys? Do we remember that? Should be a lot of area. We want this thing to write for a very small false positive rate. Raja Ayyanar?nk?a?l positive rate by giving up by having a very small false positive so that is that and so you want the area and RFC curve to be high this is a report on how much misclassification then we started trying all the algorithms we learned in this course with their decision trees. Decision trees also gave us pretty good result but not as good. A simpler decision tree for example gave us a weighted accuracy of only 77% or 12% less accuracy. Not as good. Decision trees tend to fit hard. The area under ROC curve doesn't look so good. By the way you guys can play with it. If you make your decision tree more complex, this results will improve. You can try that out. Play with the parameters of the model, the hyperparameters of the model. Then when you visualize, then we went to random forest and we said if we use a random forest how would it look when we tried with random forest it turns out that random forest pretty much achieved almost as good a result as feature engineering this one is not as good a little bit worse but give or take it's pretty much gets close to it it's a pretty good number the area under rc curve is pretty good so between random forest and feature engineering what which one would you prefer feature engineering yes because it's a interpretable model so given two models which which are good at prediction, you always pick the simpler model. Then you do the feature importance. Obviously, X1 is more important because the whole waviness is along the X1 direction. Then they are both important. Then you tried support vector machines with the RBF kernel. When you do that, once again, you look at the accuracy report. It turns out that the SVM by default parameters didn't do very well. But I left this as an exercise for you that tune the SVM and see if you get better results. The hint was that it can do very well, it can do state-of-the-art results like other, but you will have to tune this and so that was an exercise for you guys to learn to tune the SVM. Then you have the reports. By now you must be noticing that you're falling into a pattern irrespective of the classifier your model diagnostics remain the same the methodology remains the same and the code for changing the classifier usually is not a big deal this is the precision recall curve by the way this is the feature importance the Shapley value that I talked about so today I had to be up since 6 in the morning after sleeping it too so okay here we go XG boost what what does this stand for XGB extreme gradient boost classifier. So this is a pretty state of the art these days, this and CatBoost. Notice something very interesting. It fit into the model with just 190 milliseconds, which is very, very fast. Not only fit to the model, it even did the prediction in that same amount of time. So obviously boosting this xg boost it doesn't eat up too much of your time it is actually pretty fast it got the job done what is its accuracy here oh look at this this is also comparable to random forest you know it got this with same good accuracy this here it missed a point but pretty close to state of the art you get good results with this the roc curve also looks very good very nice but guys do you notice something very interesting after learning the whole theory of xg boost and gradient boosting and so forth, when it comes to the doing of it, this was the constructor. You're creating a XGB classifier, extreme gradient boost classifier, rather than SVM or a random voice or so forth. Everything else remains the same. This is something very nice about scikit-learn and modern libraries. They help you interchange classifiers very transparently. Very sort of easily, you can plug in different classifiers very transparently we very sort of easily you can plug in different classifiers into your model and do that now obviously i haven't gone into all the complications here namely the hyper parameter tuning and so on and so forth that's where a lot of the grunt work remains you know once you get good results then the business wants you to make the best possible model which means that you sit down and you start doing the hyperparameters or it used to be what you would do um in a re so one of the things i was leading to is we wouldn't have time in this workshop but i'll be giving a session on something called automated machine learning or maybe a day to it three four hour session if you guys I maybe I'll have a sign-up sheet for that it would be it would be for our session if you guys are interested just register for it. If enough people register, I'll make it some very nominal fee, very trivial amount like 30, 40 dollars or something. We'll have four, five hours of it or $20, $10, whatever, depending upon how many people register for it. And then we will learn about automated ML if anybody is interested in that. What it means these days is, see now you have many algorithms right by now if you just count on your fingers how many algorithms you have learned since ml 100 and ml 200 just for classification you would have counted about many logistic regression directly logistic regression with polynomial terms, linear discriminant analysis, quadratic discriminant analysis, decision trees, random forest, boosting, where you have XG boost and we'll do gradient boosting and CAD boost, support vector machines. How many did we count? Roughly about 10. Each of these things further has a need for regularization, isn't it? So you need to regularize the data, L1 regularization, L2 regularization, decision tree pruning, how well you prune in a decision tree, whether you use GINI, you use entropy, right? And so forth. There are many many hyper parameters in all of these models. So not only do you have to pick the best model, you have to pick the best hyper parameter in each of these models. A k nearest neighbors, kernel k nearest neighbors, then what is the kernel, which kernel to use, what is the value of the k so this process used to be very tedious it is very tedious and actually when you do that session with me i'll make you do one data set one new problem not this river but some problem more complex problem and i will ask you guys to find problem, more complex problem. And I will ask you guys to find for each of the 10 algorithms to find the best hyperparameter. By the time you are done with that, all 10, you would be pretty tired. It could be a long day just for one data set. And then the whole point is that you will learn such things like grid search, randomized search, and so on and so forth, for the best hyper parameters of the model, model tuning path, and you will realize what the day to day life of a data scientist looks like. Even basic things help, like for example, the data imputation that you do when missing values are there, how do you impute data, what data you do, how do you take care of class imbalance, let's say you use mode or something to take care of that anomalies, then how do you deal with scaling of the data, do you use standard scaling, min-max scaling, robust scaling, what do you do? So there are many choices that you have to deal with. And it has been sort of a bane that you, if you are in this space, you try a lot of things. We use our new people to do that. So there has been a body of research that sort of asked this question, that can we not do it in a more intelligent way? And people have been trying to do that. Let me see if I can give you guys a reference in case you don't have to come to my workshop you can read the book on your own let me at least give you the idea of the book where is it machine learning automated there's a murphy's law the books are usually under my nose till i try to find it in the moment i try to find it it is nowhere to be seen i swear it is here i just can't see it okay so let me give you guys a idea of that book Okay, so let me give you guys an idea of that book. By going to Amazon, you can try reading it on your own. Automated, there are very few books on this topic. Automated machine learning. Yes, to my knowledge, yeah. So Practical Automated Machine Learning, but this is focused on Microsoft. This is the book. This is a very good book actually. Wow, this book is available free on Kindle by all means guys go and get it I didn't know that well I learned something let me go get the free version I think the PDF of this book is also available this automated machine learning by Hutter somebody mentioned that the PDF is also available online. They have very graciously donated this or made it available for everyone. LinkedIn book, automatedml.org. Yes, this is the book. I would strongly suggest you get this book. This is the whole field of neural architecture search and hyper, do you see the chapter this hyper parameter optimization? It's a lovely field, it's a fast emerging world because you know this is insanity to be sitting there and tuning every model and then finding which model is the best for a problem. So there is a quite a bit, the future is actually somewhere in here. So I will be giving a workshop, maybe one day, a workshop on this. I don't think it will be within four hours, but it's really a lovely field. It's something that most people are not aware of, unfortunately. Most people don't know that you can actually do all this and that there are libraries available that can help you do that so we'll learn about that the lab part of it will do in that particular workshop but these are covet days these things it's very hard to say anything that we plan to do will actually happen anyway so this is, you realize that you can do gradient boosting. Again, the accuracy is a pretty state of the art. Notice that all of these complicated models, nonetheless, they don't beat feature engineering. At most, they come close to it, very close to it. But that's about it. This is the ROC curve for the XGBoost classifier. Code is exactly identical right by now guys are you feeling that as i scroll down the page it's beginning to look rather repetitive yes yeah this is it and in a way it's a beauty of modern algorithms that a lot of the code you can recycle right a lot of you can sort of templatize it and then just keep changing the algorithms and keep printing it out so what i do for example in this is one of the things that some people do is they will create an array of classifiers and then just run through them in a loop to see which one gives you good results and then they'll pick the best. I wouldn't exactly advise you that. Why would I not advise that to you guys? Could you give a reason for it? Why is it not a good idea to just run through all the classifiers in a loop and check the accuracy? Is it like a difference between feature engineering and using AutoML or like? No, the reason is deep learning. The default values of a classifier without any hyperparameter tuning is really not an indicator of how well that classifier will actually perform when you have really tuned it. It's like apples and oranges. That is right. Before they're all ripe. Exactly. So this is a classic game people play. If you want to show that your product is much better than the competition, you will do a so-called performance benchmark. You will bring a highly tuned version of your product to the benchmark. And you will make sure that you will use a highly tuned version of your product to the benchmark. And you will make sure that you will use the competitor's version with absolutely out of the box without tuning. And then you'll do a bake-off. And then you'll see my product is so much better. Obviously, this is quite a game people play in the software industry. And there's something of that here. And then some people, of course course get unscrupulous. What they will do is they'll go to the competitor's product and turn on logging, tracing, and turn on all the things that will slow it down. And then in their own, they will have a highly optimized version and then they'll do a bake off. So while that is unscrupulous, I hear people end up doing essentially that inadvertently without realizing that they're doing it. Out of the box, none of these classifiers will give you state of the art performance. There is a lot of lemon to be squeezed, I mean juice to be squeezed from the lemon by tuning it, by hyperparameters. So what you have to do is you have to do the most tuned version of each of the algorithms and then compare it. Though you might say that you know by looking at just zipping through all the classifiers with default you'll get some sense of what are good candidates. Some people believe that. I'm actually skeptical of that. In general, I know that so much is gained by tuning the algorithms. So much is gained that it is not worth just blindly applying. So for example, look at the dataset two, which was a regression problem. Linear regression didn't work but all we had to do is add polynomial terms and it worked, isn't it? And it worked like a charm after that. So it goes to show that you shouldn't ever try to automate unless you do it very intelligently. And this whole field of automated ML is about how to do it more intelligently, how to more intelligently find the best models and the best parameters of the model. Are you saying that does require somewhere human intelligence as well? No, go ahead. Does it require human intelligence? No, actually. Intuning at least those parameters. It wasn actually true for so far see people did see we take one classifier right and it has certain hyper parameters you can do either a grid search or a randomized hyper parameter search and so by the way did I ever explain grid search and grid search and the randomized search? No. No, then maybe remind me that I must cover today. I must explain these things to you. So these are the classical approaches. The trouble is that if you have 10 parameters, you're looking into a 10 dimensions hyperparameters, you're looking into a 10 dimensional space to search for. Computationally, it is brutal. And this brute force searches are not the intelligent way to go about it. So there are two things you can do. People have said that use your human gut feeling, your understanding of the problem, and then let that guide your search. But recently there has been more to it. The approach that people take, there's a whole bit of mathematics and nice lovely body of work that is emerging and rapidly expanding it says you know there is a more intelligent way to do two things just hyper parameter tuning you can say but more than that it is called neural architecture search let us find the best architectures the best architecture search. Let us find the best architectures, the best classifiers, let's build it up from the ground up in some sense to solve the problem. So for example, one example that we'll use is after deep learning, if you look at the state-of-the-art COVID net, see as of this moment, AI is still not reached the level that it could be useful in a practical level for COVID detection because it lags behind in specificity compared to the biological methods, the testing, the RT-PCR. RT-PCR is more or less the gold standard. The hope was that, and the AI community hoped that they would catch up. The problem is not that the mathematics is not there. The problem is that there isn't enough data to train the neural network. So that is at this moment from a practical perspective, we are not there yet. But if you look at COVID net, for example, one of the things you will see is this thing. We have used automated machine learning to discover, use that along with human judgment, bringing together both to create a really good neural architecture. And when you look at that architecture, the first thing you see when you look at it is, oh goodness, nobody could have thought of this. Are we together? Nobody would have been able to think of the architecture just as it is. So that's the world we are entering. It's a very exciting topic. And in fact, when I do the Deep Neural Network, I feel now that it is almost pointless to do it the old way that people have been doing the Deep New Look. So it's an interesting experiment. I'll be doing it from the latest cutting edge research backwards. I'll first introduce cutting edge research where we are in the state of the art. And from there, go back and build the foundations rather than slowly build the foundations and then do some more and do some more. And we'll never reach the cutting edge, which is the traditional way you teach this field. But I want to as an experiment do the other way around. Let's see if it works. Anyway, so this is by now you do, so this is for classification. Let us now try to do it for regression. Let's go back to our California housing data. Once again you remember we are trying to predict the value of the house. We did a lot of pre-processing and so forth. So here I've used actually not just XGBoost but also gradient boosting. Both of them come in Python and in R. And CatBoost I didn't get time.'ll let you guys entertain yourself with it. You take boosting, here's the boosting implementation. Let's see how much does it take to do boosting. By the way, boosting XGBoost has one limitation. When you do one hot encoding, you notice that your feature names have developed this less than sign and so forth. XGBoost will fail actually if you have any of the features with the special symbols less than greater than comma, etc. It doesn't work. It expects alphanumeric and underscore. So one of the things you'll have to do, especially for it, is you notice me renaming ocean proximity less than one hour from ocean to just rename it to something else that doesn't have the special symbols. And once you do that, you do the normal train test split. You notice that there are no special characters in the names. You notice that there are no special characters in the names. Once you are there, then you do the same thing. So what did I do? It could have been linear regressor. Except for this one line change, there are no changes. You do the model fit, but note guys, how many hyper parameters there are that you could have tuned? Do you see that? The more complex the algorithms, the more number of knobs that you could have tuned. Do you see that? The more complex the algorithms, the more number of knobs that you can turn, the hyper parameters there are that you can play with. By the way, learning rate, right? This is the lambda that we talked about in advance. The dancing of the learning rate, how slowly you want to learn. And remember one of the lessons we learned is that the slower you learn, sometimes better and how many trees are you growing one after the other sequentially these many trees here so there are a lot of things to tune in all of this but when it comes to predictions and everything good oh you notice that it makes 84 percent 84.68 percent is the coefficient of determination the r squared how does that compare let us say to random forest let's go and see what happened in the random forest random voice was 83.66 and mean squared error was 0.05 and what do we have here? Same but it's a little bit better. XGBoost is a little bit better. Why it is because XGBoost tends to overfit the data even less than random forest. So if I had to guess what was the reason for that very tiny improvement it could be this. So how good is this model? Let's look at the residual plot. Residual plot looks pretty good. This is a normal distribution of the residuals. You don't see any pronounced heteroscedasticity. It looks pretty good. Your prediction versus error, they seem to have a good relationship, 45 degree more or less relationship. Feature importance, you can do the feature importance here. Again, these values are more or less the same as other ones. What other models want. There is another version of boosting that we can invoke which is gradient boosting. So one thing you notice that when I build this model, XGBoost, it built itself in how much time? 404 milliseconds. On the other, it's a very simple problem the California data set whereas if you do it on the with gradient boosting it takes 50% more time not by much by far but a little bit more time this is not really a clue that this is XG boost is faster than gradient boosting but it's not really a definitive clue it's sort of there when you deal with more complex problem i suppose it's a bit more pronounced right again you have a lot of hyper parameters in the model that you have to play with the prediction oh what do you notice remember that was 0.84 was 0.84 84 right with xg boost 84.68 percent was there and here what do you get it compared to that 84.68 becomes 78.8 so gradient boosting the traditional gradient boosting is actually at least without hyper parameter tuning it is noticeably worse right and it takes a bit longer which is why i mean most people use xg boost cat boost and so on however it does build a pretty good model it is uh you don't see any heteroscedasticity you'll see normal distribution of the residuals. More or less 45 degrees between the two. Feature importance using this yellow brick is here. And then remember the Shapley thing that I taught you about the more sophisticated way of finding the interdependence and the feature importance. So this is it. And so that is it guys, today's lab is this. Now, the lesson here to learn is, do you notice that in a way the theory gets more interesting, but the labs hopefully once you have practiced this enough, I hope much of the machine learning labs now are beginning to feel simple. Are you guys feeling that it is simple guys? More so. Yes, like more repetitive now. Repetitive, you know. So this is it. Repetition of practice brings speed. Once you have practiced these things enough, for you, it is just a marginal cost to pick up the new algorithm from a practice perspective and very quickly you can become productive at it what you need is a methodology guys more than the algorithms i've also taught you a systematic way to approach a problem when you approach a problem in a more systematic way you don't make mistakes right You are careful. Now I invite you to go and look at the various notebooks that people have posted on these datasets and see whether we have done on average a much more careful analysis than most of those posted solutions of notebooks on Kaggle or whatever. I don't know if any one of you bothered to compare. You would realize that just being careful, you come out pretty far ahead of the average. It might not be the best, but you come out pretty far ahead of the average. Isn't it guys? Yes. Yes. Let us do that. Let's go and verify. This is our notebook, right? With all these things there and this. Actually, I haven't done that. Let's do this exercise together. California dataset, I'm sure there must be Kaggle notebooks for that. The dataset is built in Colab also, Asif. Oh, okay. It's built in. Yeah, that's right. It must be. It's a classic data set. House prices, house prices. This is it. Two years. How many notebooks are there? 282. OK. There we go. Let's look at a typical notebook. 157 notebooks have been posted. This seems to... Okay, I'll just go to a Python one, hotness language, because here we have a Python solution. This seems to be the best voted one. This is shaping. Complete tutorial for beginners. Let's go and look at this, the best one, and see how we are doing with respect to the best one. So by the way, guys, something's noticeably, the first thing that strikes you is that this fellow has actually given an introduction, which is a positive sign. Most people just jump into the code. You should explain what you're doing. He's doing that. He's taking the city coordinates. By the way, remember we did the log transforms and things like that. Let us say if he does the log transforms, he doesn't have a table of content or titles or break it up into sections. Two functions, Cleveland, Ohio, whatever. Is it California dataset or which Ohio is it California data set or which data set is it geospatial future engine on the California we'll come to know when we visualize the data yeah so he's visualizing the data let's see how did we do for visualization did we do visualization Do you guys remember guys? Were we careful enough to do visualization? Here it is. I will take it. So we have a visualization. In fact, we have an interactive visualization. So we do have visualization. What else? We projected it onto the map as a scatterplot. This fellow seems to have done also a scatterplot. So let's see what we missed. And that is a great way, actually, to learn by seeing what you have missed. Below is a graph showing the size and location of the cities. So this is the city location. So more of this. Feature engineering, length vector between districts. So this is the city location so more of this feature engineering length vector between districts and so this is actually very good yeah i remember this person doing that very impressive he did an interesting feature engine he said what matters is distance to the nearest metropolis length of vectors between districts and nearest towns so somebody has contributed this but do you notice this graph? It's pretty nice actually. He's telling you how far is a house from the downtown effectively or distance to the biggest city. What are the biggest cities in the Bay Area? It is, I suppose, San Francisco, San Jose, Los Angeles and san diego or those four cities they probably are taking so this is a nice thing we haven't done that so it is something to learn from we can do that then uh actually i remember seeing this one back here in the past at some point so he has taken this data which is very good actually then he has done a stratified shuffling and then he has split the data now the question is or why that can anybody answer this question i leave that as an exercise that before you split the data why has he done a stratified split of the data split of the data. Can anybody- I guess so that your test will not have more of one city and less of the others. Yeah, because data has imbalances, right? There may be more points from one than the other. So you don't want to take a split in which, let's say that all the data points that belong to a small town, they are only in the training data and not in the test data or all the some particular feature let us say all the points that belong to an island Catalina Island belong only to the training data but not the test data or to the test data but not the training data that would be bad so it's certified sampling is good so read about it guys that is it then the rest of it is labeled by we did it actually straightforwardly we also have this we have this oh and then he stops that's about he uses XG boost and so you notice that when he does as it was do you notice that his root means squared numbers are huge so one of the things he did not do while he did feature engineering on the which is very good because he did not use the simpler algorithms the standard linear recreations other variants of it there is no feature feature engineering. So for example, he did not do the log transforms. This dataset sort of calls for log transforms. This particular guy who's done a really good job seems to not have done that. Let's try the next guy. I will limit myself to Python here. Who is the next best? Explaining your model predicting, by the way, this fellow also has not done feature importances or shapely values. This person has done shapely values, it seems. Interpretable machine learning, obviously. And he's put in quite, ah, where am I? I'm not able to scroll. Yeah. So this is it. View the data. You see, well, this graph certainly needs aesthetic improvement with still pretty good training. So he has not done stratification, stratified sampling. This is done in a straightforward. He's using random voice regression because his point is to show shapingley. Do we have these in our models guys? In our notebook, do we have these? Yes, we have this. Shapley is important. Those of you who took my bootcamp remember that we gave a whole day to interpretable M-align and Shapley. So this is good. A short notebook. It is good. Which is the next one? Let's go back and try the next best. Complete tutorial for beginners. Let's try a luck here. Hello, Kaggler's. There's that. Oh, we have this, obviously. The other notebooks didn't have this. Scatterplot leaves something to be said. Then, yeah, more of the scatterplots. We do have this, but we have a better version of this, I suppose. Let's go and see, do we have it or not? Yeah, you see that? We have these, we have these, much more sort of deep, informative versions of these. But let's go and see what else is good. He's looking at this, and from this what does he conclude? Well, I hope he concludes that we need a transformation, data cleaning. So this is the imputer he does a null value imputation i haven't taught you imputation so uh we didn't do it in this particular data set but this is a new dial up the size a little bit it's very small please okay let's see so this is the imputer you know missing value imputation uh that i talked about it but we didn't do it in the data we can do the imputation these things we have the scaling of the date oh that's it so he hasn't done the modeling let's go back and try somebody who has done the modeling optimized, 24 volts. Import data, missing value. He notices that there is a skew. Hopefully he's, see sometimes you can just look at the pictures and tell how the person is thinking. Hopefully he notices a right skew and does something about it. Notices a lot of outliers and he computes the medians and so forth. Unique, unique for median total number. He's finding the median of all of these. Checking for null values. EDA, exploratory data analysis. Let's see what he does. Figures. He notices that there's this big pillar. Probably he'll remove that. Population versus housing, not terribly informative. Removing outliers, good. So he has actually decided to drop outliers, which is very good. Population is good. So in every dimension he's removing outliers. And then this looks very similar to what we have. This is just the correlation matrix. This is the correlation matrix. We have, do we have that guys? We have that. This is just the count plot, the box plots. Do we have the count plots box plots etc guys here it is so this is what we have do you see it guys we have this so he also has this pre-processing he does that normalizing the data do we normalize the data he is using min max scalar we use the standard scalar cumulative variance curve so he is doing a principal component analysis to figure out how many things matter we haven't done it in our notebook because pca did we do pca this time we did it in ml100 right so we haven't done it so he's looking at the pca values which is good So he's looking at the PCA values, which is good. Then modeling. So standard, he's doing a simple linear regression. Then he's doing the residual plot. This is not really the way we do it is a little bit better. How do we do it guys? Residual plots. Let's look at it. Yellow brick. Yellow bricks. We use that which I would consider a little bit better. Linear regression. Here we go. We do that. Also we do all of these log transforms etc. etc. It's a good notebook. It's a really good notebook. Though we seem to have done better in some areas. It's really good. So now he's doing stepwise regression. He's trying to eliminate features. By the way guys, let me just warn you. Don't ever use the stepwise, backward and forward feature subset selection remember that's the one chapter of the book i never taught you because i believe those methods are fundamentally flawed you should never use it so he seems to be using it he is regularizing seeing if that makes a difference and then what does he conclude his elastic net is the regularization with l1 and l2 isn't it we'll do regularization next time our next lab is that ridge then he is bringing in decision trees and in the decision tree he's coming to the conclusion he is doing some hyper parameter tuning parameters yeah do you notice this guys this is what i was talking about it is not parameter tuning the right word should be hyper parameter tuning these are the how deep can you make the tree and so forth. This is a topic we'll cover in a moment after the break. With all of this is coming up with feature importances. Do we have feature importances? We have that random forest, et cetera, actually. And then again for grid search. Yeah, so now he's using the right word hyperparameters. The best feature random forest and so on. So it's doing that. And that's about it. That's that. So guys, what do you feel? Have I trained you to a level at which you can write a very good notebook that can stand on its own and be respectable? Do you feel that you have been trained to that level? Yes. Yeah. Yes, Asif. You do that guys. Machine learning or this field of machine learning, you know, people get very excited about the latest and greatest research papers. What is the latest breakthrough in transformers? What's the latest breakthrough in a CNNs and so on and so forth. But when you actually deal with a business problem, when you deal with real scientific data, you will realize that you go slowly. You know, you will observe that quite often the best researchers, they speak slowly, they think slowly, they're very careful and methodical. That is where the gem is. Great things are done when you do it carefully, methodically and deeply. You don't rush through it. So don't rush. Magic is not in the fanciest algorithm. It is good to use fancy algorithm, but fancy algorithm in the hands of a disciplined scientist is far more effective. Do it like that. Any other notebook worth looking before we take a break? Maybe one more. Let's take end-to-end machine learning, 18. Let's try a look with this. Introduction. Yes, we have these histograms you got right. The ETA, the maps, the scatter plots, the pair plots, the correlation matrix, data pipeline, right, and model training, model tuning. No one seems to have been the law transforms and sector of the data. I wonder why. Okay. So by now we are getting rather low in the list. Maybe we can try a look with most words, this in terms of language. I think we have already done that. All right, guys. So I will, that calls for a break. After the break, I'll explain what hyperparameter tuning is and how do you do that. Would that be OK, guys? Or should I just finish it quickly and that would be the end of it no I think we can take a break all right let's take a 15 20 minutes break which is 831 by my clock here let us let us take a break, pause recording. Let's take a 20 minutes break and let's regroup at 8.50 or 19 minutes break, regroup at 8.50. You're covering grid search also, right? So we're going to search, randomize search, I'll teach you. It's a small topic, but it's really worth knowing about. All right guys, see you then. E aí Thank you for watching! E aí I'm going to make a Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Lysetil, Thank you for watching! Eriksson Takk for ating med. Eriksson I'm going to make a Eriksson Rekordverk. Takk for ating med. Fyra! Eriksson I'm sorry. Takk for ating med. Takk for ating med. Takk for ating med. Takk for ating med. Takk for ating med. Undertexter av Nicolai Winther Takk for ating med. Teksting av Nicolai Winther Takk for ating med. Undertexter av Nicolai Winther Takk for ating med. Teksting av Nicolai Winther Takk for ating med. Thank you for watching! Takk for ating med. Takk for ating med. Teksting av Nicolai Winther Takk for ating med. Thank you. Hello guys, are you able to hear me? Yes. Okay, so I'm going to now back to my writing pad. I'll start the recording again. And the topic now because it's a new topic, let meparameter search. Let me start the recording guys. One second. We are on record. So suppose you have an algorithm. Think of an algorithm, let's say, whether it is a kernel, a KNN, right? right or you are talking of SVM or you're talking about random forest decision tree boosting the gradient boosting boosting and its variance, XGBoost, even logistic regression. Let's say, and I'm deliberately taking classifiers with the same argument applies to regressors, right? So with all of these, there are many, many hyper parameters that you can choose, not to mention that how much regularization to bring in, right? use. Not to mention that how much regularization to bring in. Whether to use L1, L2, or whatever it is for trees, how many nodes there are in the trees and so forth. The learning rate for boosting. These are all hyperparameters in a random forest. How many estimators to take? How many predictors or features do you want to give as a consideration, like a split point in the tree while building the tree? So how do you find the best value? So there are many ways that people think about. This has become practically a stable diet of interviews. So people usually suggest the approach grid search. The only reason I'm mentioning it is it is what people expect you to answer. So if you ever ask this question, you should say that, see, we can do grid search and explain what grid search is but do emphasize that they're actually better ways to do things so what does grid search do so suppose you have two hyper parameters let me just call it HP one and hyper parameter two this can take on values arbitrary many and I'll just take a continuous variable let us say okay so for example let's take regularization in regularization you have the lambda you know so suppose you do beta j let's say square a ridge regression you have sse plus this so what is the lambda value you should take in svm it is the c is equal to the how much of a budget do you want to take right and in an rbf like when we take our RBF kernel, what is the gamma? All of these. So they can take any arbitrary real value. So how do you go search about it? So there are a few basic things that you can do. You first take hyperparameter one, let me just call it H1, and you choose an array of possible values. So for example, if it is kernel, K in the kernel, you could take one, two, three, four, all the way to whatever you think reasonable, let us say 50 neighbors, you can take. So we can't hear you. Let's say that you're using... Oh, you can't hear me. Can you hear me? Anybody else who can hear me? I'm not audible. Yeah, we can hear you now. We can hear you. We can, okay. Like last five, ten seconds. Okay. So let us say that you have another hyper parameter which can take values from zero to infinity so how do you go about it so one thing you can do sometimes is you can go with 10 to the power for example the cost in a svm you could take 10 to the power minus 4 10 to the power minus take 10 to the power minus 4, 10 to the power minus 3, 10 to the power minus 2, 10 to the power minus 1, 1, and then maybe you can go to a few positive powers also, 10 to the power 1, 10 to the power 2. These are all possible values you to take. I'm just taking just two examples of a hyperparameter. Now you realize that if you do H1, Cartesian product H2, how many possible combinations are there? Let's say that these are 50, and these are one, two, three, four, five, six, seven. So you're looking at effectively 50 cross seven combinations. Isn't it, guys? So quite often, the grid search approach essentially suggests that what you do is you take for H1, value h1 in this value 1 to 50 for h2 in now this value you know 10 to the minus 4 10 to the minus 3 10 to the minus 2 10 to the minus 1 1 10 100 us say, you lay it out like this, then the TB are sort of consistent, 10 to the minus two, 10 to the minus one, this is 10 to the zero. For each of these two combinations, you can build a model, you know, you can do a model with H1 and H2 values, isn't it? model with h1 and h2 values isn't it you can build your classifier or your regressor with these two particular values and then you you look at the metric let us say that you are doing classification and what you care for is the accuracy yes you see so you can maintain a table which would say H1, H2, and the metric accuracy. You can maintain a table and then what can you do at the end of running through the two for loops? You're essentially going through every point of the grid because this Cartesian product is this grid, right? These are the H1 values, these are the h1 values these are the h2 values so at every single combination a point combination of these two you will have a certain value there and from this value you will then pick the minima oh sorry maximum the the most accurate let me just say maxima sorry maximum the the most accurate let me just say maxima maximum value maximum accuracy so on cross validation on validation set so that would be your best combination of h1 and h2 does that make sense guys it's a simple idea isn't it take every possible combination of the parameter values take some discrete values that's why the word great great implies discrete set of values and when you take a discrete set of values and you go about searching for it, you'll find some good combination that is good. Now, one further thing that you could do is, you could zoom in, zoom in. So suppose you come to know that a K is equal to, so suppose you went in the beginning, K is equal to one, 10, 20, something like that. And so suppose you zoom into 10. In the second round, zoom in means next what can you do? You can take values like 5, 6, or 6, 7, 8, 9, 10. You can just look in the vicinity of this, 12, 13 maybe, plus minus 3, to see if instead instead of 10 any of the neighbors give you better value in the same way suppose you find out that the cost or whatever it is the other H 2 the best value was 1 so what can you do now you can search in the vicinity of 1 I try out whether 0.7 is good 0 yeah, 1.2, like, you know, five, something like that. You can zoom in and try out local values till gradually you find the best value of H1 and H2, right? H1, H2, right? So you're basically, or in simple terms, you're trying to do argmax, terms, you're trying to do argmax for the metric accuracy of the model. This is what you're trying to do. So it could be accuracy, recall, whatever. Whatever it is, whatever metric that you want. Area under ROC curve. You can pick your metric and then you pick those values that are the best. This is called grid search. If you look at textbooks, they'll often mention about grid search. It is the answer that you're supposed to give in interviews, etc. But actually it happens to be amongst the ways of doing it, perhaps one of the less efficient ways of finding the best hyperparameters. There's actually a better way, which surprisingly is, when you have H1, H2, what you do is, you don't do grid search actually, you take random combinations of this. So we can't hear you. Oh my, did we still again lose the voice? I could hear you. Oh, it was continuous. It was continuous. I could hear you. Okay, so what you do is you take random values of H1, H2. Are we together? So suppose you want to do 100, for 100 search events, what you do is you randomly pick H1, H2 values. When you randomly pick, what happens is you actually have an advantage let us say that your h1 h2 form a rectangle right this is h1 this is h2 in a grid search what happens is you end up picking values only along this isn't't it? This. And so how many grid points did we pick up? Let's say, let's take this count and we'll see how many we are talking about. So 1, 2, 3, 4, 5, 6. What? 3, 4, 5, 6, 36. You have 36 values. Now see what I'm going to do. If you go, so this is grid. Now from this, if I go to randomized, we'll take the same thing, but we do something different. We say that randomly pick, let me use something else, some other color. Randomly pick 36 values. One, two, let me randomly go sprinkle these values here. One, two, three, four, five, six, seven, eight, nine, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36. Do you notice some difference between these two graphs guys? These two plots? If you look at how many, if you do randomized search and you ask how many H2 values did you sample? How many H2 values did you sample? You will realize that you sampled 36 values. And how many H1 values did you sample? Also 36, isn't it? Because when you project it down, when you project along each of the axes, you will have lots of values. All 36 values will be here because it's very unlikely that any of those values would overlap if you take random numbers so you get a more uniform search a richer search of the feature space let us say that your best value turned out to be around here what can you do now in the next round you can just draw a circle and you can test for more values in the vicinity of this. So today we believe that randomized search, this is called randomized search for hyperparameter optimization. Asif? Yes. This assumes there's no local minimum, right? That is true. See, unfortunately there are, and that's why I call these methods not to be the very best methods, but it certainly is better. See, unfortunately most people, A, they don't know hyperparameter tuning at all. They just go with vanilla values or random guesses. If they know hyperparameter tuning, the only thing they know is grid search because that's what they have been fed in the textbooks and so on. And it used to be true, but now we know that randomized searches are better, better than that. So you can do this and you can zoom in and in the randomized search also. Zoom in and repeat. In this circle, again, do randomized search next to the best possible value. And ultimately you'll find the best hyperparameter models. So these two are very popular. If you look at the code on the web in the secret literature and so forth, you'll often find grid search and randomized search often being used. Just now we saw Kaggle notebook. If you, I don't know if you noticed it was a double for loop for two hyper parameters it was basically this this this sort of a thing different values of the hyper parameter you were building a model it was a regression model looking at mean squared you were building a model. It was a regression model though, looking at mean squared image. But the idea is the same. And what I'm telling you is that if you must do that, instead do randomized search, it's just a superior way of doing it. But now comes the state of the art, these days, right? And this is where automated ML and these things come in. We have a way of systematically We have a way session for it. In which I talk about how these things are done. These are non trivial methods. These are not just, you know, the above to let me just say that the above two are the poor man's Methods. They are very popular. Most people don't know anything beyond that. But there is actually a systematic way to optimize your hyperparameters. And we will do that at some point. Today is not the day because a much longer discussion. I just planned that idea that there was a systematic way to do hyperparameter search. And there's this whole world of, you know, neural search and so forth. We won't go there at this moment. But today what I would like to learned in this course? I would say the biggest thing, the number one thing is do your analysis thoughtfully. The more you think what you are doing, the better you'll do. This is the, it looks almost like a cliche or a trite statement to say that, but I must take this phone call, Thank you. Thank you. I apologize for the interruption guys. So this is it, you know, if there is one message I wanted to give in ML 200, the overarching message is be thoughtful in your analysis. If you are, you will always outshine. You'll create an outstanding notebook. Go slowly, go methodically, and more or less, you will get somewhere. Very, very definitively, you'll have a good analysis of the data. And it's quite a pleasure when you can start with any data. And you know that you'll find every data has some story to tell, something interesting to tell you about. And it's a journey. And you enjoy that journey. It's not a haphazard or a quick rush to build a model and then say, we have done it. See, in coding, in normal software, there is a lot of ego or one-upmanship. One guy will say, oh, you took one week to do this problem. I did it in one day or half a day or two hours and so forth. And at some phase, I suppose we all have indulged in this sort of a game, right? Because we think it is a measure of intellectual superiority to be able to finish a problem quicker. Maybe true for normal software development. Actually it's not true even then. My basic rule to my team at least is, write code as slowly as you can. Because code takes not much time to write, but 10 years people will maintain the code. So every single bug will come and haunt people later on or you later on. So it is much better to write code slowly correctly as far as possible write the first time buffer it up with unit tests lots of testing and things like that documentation etc now in machine learning it couldn't be truer people who do machine learning are not here the the game is not to be fast the game here is to have the most predictive model the most high-performing model the most interpretable model a most useful model usefulness and speed have nothing to do with each other you create good useful models when you go slowly. When you go slowly, almost surely you'll hit it. Whereas if you rush ahead, it's a luck of the draw. You might hit luckily once and you may feel pretty good about it. Most of the time you won't hit it. If you look at the brightest people in the field, they're very thoughtful, slow people. They are not people who rush or have this ego thing that I did it in 10 minutes. Anyway, most of these data sets, large data sets and algorithms when you apply, they will take hours and hours and days to run on the machine anyway. So where is the rush? You might write code in five minutes, but it will still take hours to run on the machine. So iterations are slow. You might as well, instead of doing a lot of useless iterations, you might as well be very thoughtful and do careful iteration. So that's the message. The second point is that no free lunch theorem, NFL. It's again something that most people miss. There are no perfect algorithms. The data decides which algorithm to favor because each algorithm has a certain perspective on the ground truth, on the world. And if the ground truth resonates with that, in that particular data sets case, that algorithm will shine. In some other data sets case that algorithm will shine in some other data sets case another algorithm will sign so nfl theorem is there importance of feature engineering is there then we learned about ensemble methods but before ensemble we did decision trees cart classification and regression trees and so on and so forth. Then we did, and by the way cart is general term for decision tree, but it is also often, and the trouble is the word has dual use, it is also a specific implementation of decision trees that works with GINI. ID3 works with information gain and entropy. So there are subtle nuances, but let's gloss over it. So then there is random forest. Then we did ensemble methods. That is the crowds, the wisdom of crowds kind of a thing. I hope I drilled the whole idea pretty well into you with all the quizzes and so forth that we did. I hope I drilled the whole idea pretty well into you with all the quizzes and so forth that we did. We did bagging and an illustration of bagging that we did was random forest. We did a boosting and an illustration of that we did XT boost boosting in particular we did gradient boosting I talked about other boosting but I didn't quite cover it did not quite do other boosts too but so I'll just leave it there xg boost gradient boosting classifier regressor then what did we do we also learned about support vector machines kernel methods methods we did about KNN K and then neighborhood approach but methods we did kernel, KNN. Then we did the support vector machines. Oh goodness, what's the count though? I forgot the count. Seven, eight, nine, 10, 11, 12, and 13 is the SVM. Support vector machines. Then we did regularization. We talked about distance metrics and Minkowski distance. We looked at it as a constrained optimization problem. both for SVM and for regularization. And then we did rich and lesser. And lastly, we did the recommender systems, little bit recommender systems. Recommenders intro, let me just leave it as that. I introduced the idea of recommenders collaborative filtering. And where is the count going? 14, 15, 16, 15 16 17 18 and lastly we did red search great search then we did 20 we did about feature importance right and the Shapley values, etc. So we seem to have, this is what we have done. I hope you remember we did a pretty large number of topics in this particular workshop in the last six weeks. This is what we have done. I'm itemizing it. I'll share these notes. Do, if you get chance, go back and review, because if you don't review, you'll lose it. We all suffer from the forgetting curve, so-called forgetting curve, which means that anything that you don't go back and cycle through or review through again will begin to fade from your memory, however well you may have understood it. So the only way to remember things is repetition and practice. So keep practicing these things, guys. Keep doing the labs, picking up new problems, solve it in the perspective of all that you have learned, and go back and review the books, read new books, read new things. At this particular moment, if you have to ask me, where do you guys stand, I would assure you that you are ready to work in the industry. You are ready to interview. The only thing you don't know is deep neural networks, deep learning. Other than deep learning, the rest of the machine learning, I wouldn't say all of it, but all the significant areas we have covered in ml 100 and 200 you should feel quite comfortable going and interviewing i don't know if anybody has been interviewing i think one of you said you have been interviewing and doing pretty good pretty well who was that was that you or shiva Sorry, I was on mute. I already have, I joined the company as data scientist last year. So that was Walmart labs. So, but I'm taking this course after, I mean, that's how important this course is. So I already have a job, but I know where this course will help me. Yeah, thank you. I'm planning to implement this in the current job that I'm in. Excellent. Excellent guys. So guys, I've been very busy. Some of you brought some problems so I asked for advice. Some people I've been able to give advice, some people I haven't been able to because I've been very busy. Considering that I'm not teaching the math workshop now, it doesn't seem likely. So I'll have a little bit more bandwidth, come discuss. But remember the workshop, our relationship doesn't end with the workshop. You are my students, have been with me. So we can, you always have access to me. Reach we'll talk any help that i can give you i'll be happy to give you stay in touch and even if you don't intend to take the rest of the workshops do stay in touch if you ever need help reach out and i wish you all the very best with your careers any questions guys this is the official last day it is the official last day. It is the official last day. I still want to take out time to do one more lab that's an optional lab. I was Saturday or at some point, but we'll certainly have a quiz. By the way, those quizzes will continue on. Just because the workshop is over doesn't mean the quizzes will be over. I'll continue doing quiz on some topic or the other every Saturday. Asif, does it make sense to have like a kind of a prelim session for Python with like PyTorch? So that like we're better prepared for... Yes, I would love to give a starter session. Actually, it's such a pleasure after doing TensorFlow. TensorFlow is a very powerful framework, but it's sort of, I don't know, it's a little bit more non-intuitive. You have to prepare the execution graph and then execute it. And now there's the TensorFlow eager, which with Keras API, making it more intuitive. So we can do both. I can introduce a bit of TensorFlow Keras and so forth, and I can introduce PyTorch. And we can do something like, for example, you use PyTorch to do a distinction between cows and ducks. Our usual quintessential example we have been carrying through thousands of images. We can do that to do word classification. We can actually do something useful in a small session three four hour session yeah I would love to do that look forward to it as if this pie torch well mature enough already for edge computing or inferencing see pie torch is written all of these things have a C++ code so are they ready for edge computing by all means inference absolutely you can do inference on very weak hardware it is the model building that is uh the problem where you need gorilla hardware see think about it you have a neural network it's pretty complex neural network at the end of the training what what do you get? You just get some weights, isn't it? You just get some weights of the graph and you're doing matrix multiplication. The forward part, you have the forward pass and the backward pass. And we'll talk about this. Think about a simple dense network. The forward pass has, as the data comes, what are you really doing? You're just doing matrix multiplication and applying some activation function reloop, which is straightforward multiplication, then again matrix multiplication, and activation and a little bit, and then occasionally the regularization and batch normalization. All of these are matrix operations. Matrix operation you can do today on very flimsy hardware. So you can definitely do inference on the edge. That's the whole idea, right? These days, even your iWatch can tell you whether you're having anomalies in your heart rhythm. Isn't it? Yes, Asim. Yeah, I don't know whether, by the way, anybody knows whether iWatch does it locally on the edge or does it send the data back to the cloud and has the inference done there and gets the results? I don't know. Somebody should know that. But yes, edge computing is a reality, whether with PyTorch or with TensorFlow. There's TensorFlow Lite. Inference was easy. Let's just say that the TensorFlow tensorflow got there sooner there's a js api uh pytorch is in a bit of catch-up mode in everything but inference is not a big deal i mean you can do an so So any other questions, guys? So we'll take out time. I want to do a lab on regularization. It would be really worth it. I may not be able to do it immediately, because we have a whole month. I will keep announcing it whenever I get time. And please come and join in those months. And remember the quizzes also the project will happen and the sunday seminars every other sunday we do a seminar sometimes more often come to the seminars and whenever you guys need help with your projects reach out to me do your project i would really love to see you finish your project you finish your project yeah i mean like our notebooks and everything can we show once yes yes you don't even have to wait to finish it whenever you want to do a show and tell and get uh get feedback from me and just drop me a word and we'll find out sometime okay great thanks and just drop me a word and we'll find out sometime okay great thanks as if are you going to start a deep learning uh boot camp on the time which you specified or is it like a changing date it is the first of september or what is the first week of september as of this moment i'm holding on to this line the number of registrations i've received is about eight. I don't know, it's either because a lot of people have not registered the plan to, or because they don't intend to register. If it is eight, I'll postpone it. If we reach a quorum, a reasonable quorum that makes it worth starting, we'll definitely start. We need at least three, four teams to be able to present. Four teams of let's's say, five people. Mix 20. Otherwise, it's no fun. I mean, you won't enjoy it if you find that you're in a... See, small classes are good for teaching lessons. Small groups are not good for boot camp mentality. Boot camp is... Excitement is when a lot of people are having fun and doing a lot of show and tells and ideas are flying around and everybody is helping everybody else so the dynamics in a bootcamp is different I'll see if you reach the core so that like I said it would be phased I'm hoping there is one month for registration guys if you haven't registered and you intend to register go and register now otherwise if if you're just waiting till the last moment you might be disappointed I may either postpone the bootcamp for a little bit till I get registration or worst-case situation I don't get enough registration it won't happen I'll just refund you your money so that's how the dynamics works and are you doing that uh one week or two weeks math uh math course or math like a kind of a session yeah yeah so i'll do some extra sessions just to prepare you guys for the deep learning that you know we have a lot a whole month and i see these extra sessions that i keep doing i'll keep on doing that along the way. We'll do a little bit like you guys are taught. So I'll introduce my taught math. I'll introduce a bit of math. We'll keep on doing this little bit of exercise. I'll use Sunday afternoon example to do that. Okay, thanks. Just monitor the Slack channel then post such announcements. Yes, stay in tune with the Slack channel guys. I'll announce it here by the way did you guys like the um that this thing what do i say this sunday's session yes yes one second let me stop there we can talk now