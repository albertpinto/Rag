 On Saturday, we started with pattern recognition. For pattern recognition, two themes that we will discuss in this workshop are clustering and dimensionality reduction. We realize that pattern recognition often can give us a lot of insight into data. We took the example of San Francisco city. If you were to rise above the city, you would see its structure. You would see where the financial district is, where the Golden Gate Park is, where the residential areas are, and they are clustered. They come in clusters. And we talked about the fact that clusters are very common in nature. Wherever you look, you see clustering of data. In fact, uniformity in data is much harder to find. Clustering is quite, quite common common you look up at the sky and you see clusters of stars. Kallush Koshykar- So you look anywhere you look at the look at people at a social gathering you find them clustered so clustering is everywhere, how do we detect clusters then. clusters are called clusterers. Now there are many many many types of clusters out there and obviously in one workshop you cannot cover all of them. We'll cover three core areas of clustering. K-means clustering, hierarchical or agglomerative clustering, and density-based clustering. K-means and hierarchical are both there in chapter 10 of your textbook. Density-based is not there. So I'll give some other material to study that. Now, when it came to K-means clustering, K-means clustering sort of has, in some sense, half a problem. It says, tell me how many clusters you want to find. And then the algorithm goes and finds how many clusters you want to find and then the algorithm goes and finds those many clusters in the data now whether that number is right or wrong it will nonetheless find those many clusters in the data that is the risk of that in the absence of uh like you can't visualize very high dimensional data so it becomes you don't know whether that clustering is correct or not so in order to know whether that clustering is right or not you have to try you have to take a mathematical measure there are many measures but one measure which is quite common and in fact mentioned in the textbook is wss so to to set up k-means clustering, you need two things. You need, of course, the data and the number of clusters predetermined that you are searching for. And you need a distance metric. You need to be able to define what is the distance between two points. Quite often, we assume Euclidean distance. It need not be Euclidean, it could be any other form of distance. So long as you can define a distance between two points, you could go about doing clustering. So this is one little bit of a technicality that I don't think I covered. What is a distance? How do you define distance between two points? Distance between two points is, in a more general sense, it is, and let me start with that today before I review the rest. Distance between two points, let me make it a little bolder. Yeah, it is bold. Distance between two points, two points xi and xj. Obviously, assume that they are vectors. just it is a function you realize that distance is a function that maps this point xi the pair of the points xj and i'm deliberately starting from a more abstract place it maps it to distance is a function that maps it to a real number Are we together? And real plus, actually. In other words, distance can be zero to infinity. Distance can never be negative. So that is the first definition, first aspect of a distance. Distance between two points, xi and xj, must be greater than and equal to zero. The second part, obviously, and you would imagine geometrically, you can imagine that this xi, xj, distance between two points is equal to zero if and only if, so this word is if so this word is a mathematical notation of if and only if if and only if xi is equal to xj means in the in the space in whichever space you're representing these points these two are essentially sitting on top of each other then only can the distance between two points be zero right so if you geometrically think of a space and this is xi and this is xj the the distance is this now comes an interesting third part part of the the third condition that you need to define distance there are many such functions that you can do. First also, this is true, xj, xi is equal to a distance xi, xj. So it's symmetric. It's a symmetric positive number, zero or positive number. But there's one extra thing that you need for distance. Suppose you take a point xk, then you would agree that this path is longer than the path from xi distance, like let me just call it a, b, c, right? a plus b will always be greater than equal to c. Would you agree? the asymptotic limit your your this point could be here literally the k can be here in which case it will become an equality but in general a plus b is a longer path than c right and any function that fulfills these requirements namely we can say that if xi xk plus dxk plus xj is always greater than equal to distance from xi to xj. In fact, the shortest distance, and here I'll bring in a new word today, the shortest distance, once you define a notion of distance, these are your conditions. This is how distance is mathematically defined. And if you go back and look at Minkowski distance, you'll see that it fulfills these identities, these relationships, distance function. This is distances sort of a function which fulfills all of these conditions, it is positive zero or positive. It is two points have zero distance if they are the same point and the symmetric at the same time the distance between two points this distance c there's a name for it it is called um let me use a bolder font it is called uh now let me use something else um it is called a geodesic geodesic the it is defined as the shortest distance between two points. Now you would say, well, why is it not a straight line? And the example that we took last time is, if you think of the earth, for example, and even if you are looking at sort of locally this thing so then the distance between this point here and some other point here is not the distance through the center of the earth because that distance you can't actually travel you're living on the surface of the earth so what will you have to do your distance will in reality be uh what is called the great arc right it is you have to walk on the surface of the earth and find the straight distance from here to there and that distance is a curve it's not a straight line right so uh so this is something a bit of a learning see as you go into machine learning inevitably it uses mathematics so then one has to become gradually familiar with how mathematicians think. Mathematicians actually think in terms of generalities the moment they see something the first question they have quite often is how can I generalize this into a broader theory, right? So we think of distance as Euclidean distance. So now we got introduced to Manhattan distance and other distances. But then you can ask a much more generic question, general question. What in a sense is distance? And the moment you realize that a distance is a function that takes a tuple and maps it like this, then it opens the door to asking the question that what is the shortest path between two points, right? So is it always a straight line? It is not. In non-Euclidean geometry, it is not. And so that brings us to the notion that the right term is not a straight line, you shouldn't use the word straight line, but you should use the word geodesic. And as you do, as we go around and make a progress in this workshop, and we get into for a couple of sessions, not always, but we'll have to deal with topology and things like that. So remember this point that I mentioned, that geodesic is the right appropriate term for the shortest distance between two points, and it need not necessarily be a straight line. So it's a review. Now, going back to what we were talking about last time. Once you have framed the problem as, I have a distance measure, I have a number of clusters that we need to find we have fixed then it is straightforward we use this process of iteratively of finding it randomly pick those let's say let's say k is equal to two randomly pick two points and declare them to be centroids and then what you do each then you go through this iterative process assign all other points to its nearest centroid and then recompute the centroid of each of the clusters so you'll end up with clusters and then recompute the centroid and you keep on doing it till you achieve till you notice that in a very intuitive sense the centroid stop drifting right now drifting is an intuitive term what does it mean in programming? It just means that the centroids are not moving around too much. They are within an epsilon distance of the previous iteration centroid. That's one way of looking at it. Another way, which is equivalent, is to say that not many points are switching loyalties between the two centroids. So that's what it is. Now, we went through this process iteratively and we did notice that it does converge pretty reasonably. But there are two problems. Their pathology is what if you end up sitting at the just by absolute, absolute bad luck. The data is such that there's a trap in it and you just fall straight into the trap. You end up picking points A and B in such a way that the centroid now, the first centroids you find or the clusters that you find are stable clusters and you know they are not good clusters, right? They don't have cohesion. You can actually see that you have taken half of one cluster and half of another cluster and called it a cluster. And likewise for B, you have taken half of one and half of the other and real clusters and call them a cluster. So how would you know that you shouldn't do it? There's a measure of cohesion which is within cluster some squared distance. So for each cluster, find all pairwise distances squared. Don't bother taking an overall square root later. Just take the square root and average it out. And then see what did it come to. And then repeat it for the next cluster and add up all of these measures. The smaller the WSS, the more tight it, the more sort of thick the cluster is. The more bigger the WSS, the more it is a sense that there are too many distances, big gaps between points in the cluster. And that's why you know that if the top half is a cluster, for example, you see points from the one real cluster to the other real cluster, there will be large distances. points from the one real cluster to the other real cluster, there will be large distances. Because there will be large distances, your WSS will explode. And that is a clue. So now you ask this question, well, how do I avoid this situation? So there are multiple heuristics that people do. One of them they will do is they will pick a point. And then the second point they'll pick as far from the first point as possible so that you don't end up in the situation where you end up creating bad clusters that is one way none of these things to my knowledge guarantee that you won't have bad situations the gold standard of course is in practice it works most of the people they understand their domain and in most of the domain you know where to start. For example, in realistic terms, you get data, you get a new refresh of data. What is a good starting point for centroids? Well, pick the centroids of last week's data. Because the data would have shown some drift. But essentially, last week's true clusters are not a bad choice to start with for this week's clustering you'll quickly converge because you have started pretty close to the real answer okay or if you get data fresh you can try these heuristics most likely once it works if you have the luxury of not big data not terabytes of data then the the gold standard is run this algorithm many, many times and pick that particular runs cluster discovery, which gives you the lowest WSS for the same value of K. That is that. It still leaves. And so there are other methods people use. In practice, as I said, a lot of it becomes moot because in real life, the experienced teachers, whatever you do, the first time it is exploratory, but soon you figure it out. And by the time you're productionizing it, it gets a lot easier. You have a pretty good intuition into the data and you have a good sense of where to start from. So you don't randomly initialize, you initialize well, you know that you'll quickly converge. That's an important fact, actually, that choosing your initial point often can decide how quickly you can converge in k-means clustering. Now comes the second point. What it leaves all of this is the hyperparameter k. We don't know what k is. How would you know what is the best k? Oh, excuse me, guys. There's a small meeting. Can I please take a two-minute break? So we still have to determine what is the best W, what is the best number of clusters that you have. And the answer is not perfect. K means clustering. Its great virtue is its simplicity. Its weakness is that it gets harder to determine what the real clusters are, especially if the clusters are not globular. In other words, they are not convex shaped clusters. So with all those limitations, nonetheless, there is a way. What you do is you start plotting the k for various values of k from 1 to n. You start plotting the WSS. Now, you don't go to the complete limit of n being the number of points, because then you know WSS will be 0. You stop quite early. But what you do is you plot the WSS for a given value of k, then the next, then the next, and at some point you'll discover an elbow. And then you may increase it by a couple of more points just to be sure that the elbow that you saw was genuine. If you see an elbow, it is a very good sign. It means that you have found what is the optimal number of clusters generally. Now you may not see this. The plot can be worst case situation. It could be like this. When it is like that, it generally means data has very faint or non-existent clusters, it could mean. Or it could mean that the clusters that exist are not roundish clusters. They're not convex shaped clusters. So either of the two may be true. So you have to go and try other approaches. But this, we're looking for the ELBO, which is the ELBO method. And this kind of a plot is called the scree plot. What we have just built is the scree plot. And with that, I believe we have essentially a review of last time. Any questions before i start with today hey yes so you mentioned that you start with the prior week centroid right and you kind of use that as your starting point what if there were new clusters that were formed so how does that help I mean yeah there is always a risk because see if new clusters have been born and the K itself has changed so what you do is whenever you are in a hurry like for example incrementally you do it but then uh once a month or once in a two three months what you will do is you will watch out depends on the situation how how adverse it is not to discover complete radical change in data. See, changes in data emerge slowly. They don't just happen in most real situations. So then what you do is you run the K-means clustering from scratch and you do that whole elbow plot method and you cross validate. That is one thing. But having said that, see, in practical terms what happens is k-means is only one algorithm. It's the simplest. What we do is, and I'm going to tell you another algorithm which I use much more, where a k is automatic. You discover what the real number of clusters are. So K-means is something that is good to do, it's cheap, but whenever it gets important or serious, well there are other methods that are far more in many ways reliable. And we will learn about those today. Having said that, K-means is still very popular, given its extreme simplicity. It's a very simple method. It is often used in production in the industry. Any other questions, folks, before we go into the next clustering, which is the hierarchical clustering which is the hierarchical clustering. In your book it's called the agglomerative clustering. So I'll stick to the term the hierarchical clustering is more commonly used in the industry. You'll see why the word agglomerative comes in here. Now this is a different clustering method that is often in many ways people feel is more intuitive and gives you more freedom but it is computationally much more intensive than k-means clustering. So the you do that is uh suppose you have data points um you take all the data points suppose imagine let's start with some real clusters and you'll see this happen so suppose you have clusters like well well, this color is terrible. Where's my white color gone? No. Well, this better. Okay. And this better. Okay. So I hope this white color shows through properly now. So suppose you have a cluster like this. And you have another cluster, which I'll call like this. I've deliberately given two different colors. It doesn't mean that there is any distinction in reality. You wouldn't know, you'll just get data in the feature space. What you do is you take a point, you take an arbitrary point. And so suppose you take a point here. Let me take a point using this color, this point. Now, what you do is you arrange all other points based on nearness to this point. You find given this point, you find its closest neighbor. So you sort this list of points. Where am I going back to this? You take the list of points and imagine that you're making them in a sorted order. They're lined up in a queue. What you do is you just sort and find the nearest neighbor for this point and suppose that turns out to be this you put it here you connect these two you say these two points are now connected then you take the next arbitrary point whatever it is maybe you'll pick some point somewhere and that also will have a nearest neighbor maybe it has two nearest neighbors at the same distance to each other happen chance it can happen so you put these two together are we together and then what you are doing is these are consumed so you keep removing those points from the list then the points that remain you again pick some point and then you join them you you have another let's say that four points are all at the same distance to each other or five points and you sort of join them right and so you keep doing this process till you have exhausted this list so you notice what we are doing you just picked a point and then found his closest neighbor then we represented it in this visual way imagine this visual representation by the way is called a dendrogram so this is your first if you're being introduced to this concept of dendrogram or this visualization then welcome to one of the most effective visualizations in machine learning. Very, very effective, heavily used in research communities, especially in medical research communities and so forth. So this is it. And now, suppose you have exhausted all the points. Now comes the question that and remember so suppose i were to ask this question um if you look at this point it's what is its nearest neighbor let me name these points a b c d e f g h i j k l m n o p q r so if i were to ask this question J-K-L-M-N-O-P-Q-R. So if I were to ask this question, that what is the closest neighbor to D? What would you answer? What would you answer as the closest neighbor to D? C. You would say C, isn't it? However, if I were to ask, what is the next closest neighbor to d say and you would say e actually no you don't any of the fg hr no any one of these things that are here on the on this list could be. It is just an accident that I started putting EFGHI cluster next to CD because I was randomly picking up points. So this is a trap. I'd never- But didn't we order them, Asif? What was the sorting order? Okay, let me repeat the sorting order. You first put an arbitrary point A, right? So you have this list. So let us say that you have this list a b c d let me put this list in some ridiculous order so uh all the way to r a d c maybe an operator color oh like a color okay let's see so let me use okay you have the number says r a d q k m c e j a little what did i miss n Q is already done, A, B is missing, B, C, D, C is here, D is here, E, F, E is here, F, L, M, maybe M, and M, N is already here, I believe. O is here. Oh, P, there is P somewhere here. No Ps. Maybe I'll stick a P here. P, Q, R, all of them are there. So suppose you have this list of points in your data. What you do is you arbitrarily pick A. So is this visible, or I should redraw it in brighter colors? It's visible. It's visible, yeah. Yeah, we can go on with that. Let's say that we pick A. So A is where? A is here. We take A out. Now the question we ask is, what is the closest neighbor to A? And then it turns out that you compare the A's distance to every point. And then you realize that it is B. So you take B out and now B is here. You form this group are we together now arbitrarily you take the next point you say okay let me pick a point for whatever reason you pick m let's say you picked m here and then you ask m what is the closest neighbor to m ah it is l let's remove l right so you form this group now how many points remain you go and say all right let me pick some random point pick up pick a point guys what would you like to pick e p e e because it have four neighbors uh as per the dendrogram oh e okay so let's say that you picked e right so we'll take e out here e it turns out that it has four neighbors all of them equally spaced apart so f g h i so where is f there we go f g h and i do you see how this list is getting shorter and shorter and you create this this thing now arbitrarily pick another point so as if one thing that's happening here is like when we basically e f g h i are a cluster or we kind of put them into a group uh it's not necessary that they were the closest they were the closest of what is left of ungrouped points. No, no, they are the closest. The reason is that if suppose they were, suppose the distance of E to B was shorter than the distance of, to F and G, you realize what would happen. A and B would catch it. It would have, yeah, yeah, okay. A, B cluster. Yeah, yeah, good point. Yes, missed it. Right, so you see that what we are doing is we are arbitrarily picking points and we are creating the subgroupings. This subgroupings, and so the way you say it is that, step one, each point is its own cluster. So how many clusters are there? Total number of points, isn't it? Would you agree? If each point is a cluster, then the total number of clusters is the number of points available. Step two, do the first agglomeration operation. You did that and you formed this. But now comes something interesting. Let's say that A and B are EFGHI. Let's go back to this picture and let us say that for whatever reason, let me find some points which are very, very close to each other. Okay. Let's say that this was the EFGHI subgrouping. Now what you do is you have a cluster. You find its center. And I will leave the word at this moment open. Find its center. And so for each of these points, each of these little clusters you find their center right and so now what you have is a second order and treat it as a point is as a new point proxy for its cluster. What you do is you go and replace all of these points and you say, hey, these points are represented by this point, which this pink point, which happens to be the center of the EFGHI, right? And so you can repeat this process and what you will end up with 1, 2, 3, 4, 5, 6, 7. You will end up with just seven points, right? In this particular case, I started with very few data points, but you will end up with points. Let's say that you end up with something like this, this sort of situation. I just simplified it down to seven points, seven ping points. Those seven ping points that you have, which represent obviously these clusters, seven clusters, what you do is now you do the same exercise over again. You take any arbitrary one of them, you say A, B, and you ask, what is the closest neighbor, right? And if needed, you can sort of move it around a little bit. But let's say that in your visualization, you're lucky. These two come and meet, right? They are at the same distance. It turns out, and it may so turn out, that these three may all come and meet. They are all at the same distance from each other. And it may so be that these two are at the same distance from each other. So it might represent groupings like this. Are we together? And once again, you replace these pink ones with their centers. So let's mark their centers with a yellow color. The center of this will be somewhere here. The center of this will be somewhere here and the center of this will be somewhere here. So now you ended up with three points. And now, once again, you do this game. You say, you take this point and you ask, what is the nearest neighbor to this? It is this. So you end up joining these two. And this guy, it turns out that it has no, all the points are exhausted. There is no neighbor you can associate with it anymore. Then what you do is you say, all right, so now these are gone and I'll replace this with some other color. Let's say this color. So now I have these points and then this point. This yellow remains and now, oh, well, actually I shouldn't use pink. We have used pink I apologize one second control Z control yeah so which color should I use let me use blue I haven't used blue so yeah so suppose this and this point now becomes blue right and so you have just two points and so the top level cluster would be so hang on i did not join the yellow ones let me finish joining the yellow ones yeah and so the top level cluster would be saying one cluster that includes them all and this isn't right do you see how i went from n points we go from let's say we went from endpoints, we go from, let's say, we went from n clusters to one cluster in a process. I hope you get this process. It'll take you a couple of times to review it. It will settle down, but it is important that we understand. All we are doing is we are creating more and more coarse grain clusters by taking small clusters which are near each other and merging them, agglomerating them. SRIKANT DATARANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANAN At the end of it, you'll have just one big cluster. And so in some sense, it's the other way around. In K-means, remember we started with K is equal to one, one giant cluster, looked at its WSS, etc. We kept on going. Now here and there, you could pick any arbitrary K and start with that. Start with K is equal to seven if you wish here. But here you don't have a K. You go through the entire exercise of finding one cluster, two cluster, three cluster, and so on and so forth. So then comes an interesting point. The question still remains, what is the right number of cluster? And here is where the answer gets interesting. You have computed clusters at each level of granularity. Now, what people do is that they really appreciate the dendrogram. And quite often, they'll print the dendrogram and stick it to the wall, right? So that people can see what things go together, especially in the medical data, what things go together and form, you know, sub clusters in the data. So you can look at data at a very fine level, or you can look at it at the cross level. So for example, if I were to, let me draw a cut line. If I draw a cut line, I just need one more color to draw the cut line. Let me use what just happened. This thing seems to have disappeared. Okay, suppose I pick this point. Hopefully this is bright enough. We'll see. If I put my cut line here, you see this cut line? Then below that, you notice that each point is a cluster. That is one level of granularity. On the other hand, if my cut line is here, how many clusters do I see? One, two, three, four, five, six, seven. Those seven pink points, they are clusters. If my cut line is here, how many clusters do I see? I see three pink clusters, which of course you see that. And if my cut line is here, I see two clusters, which basically says that this cluster and this is the two clusters. And of course, so you can actually pick a cut line and decide how many clusters do you want to look at and you have a full representation of the cluster yes a question yes is this layout does this layout depend on the type of points you choose or it's always going to look the layout will always be unique for a given set of points with minor see variations are ultimately right your system will adapt when you are doing the computation so suppose you start with cd here what happens is that the computational engine realizing that ab and cd clusters are close to each other it will move it in the diagram close to it. So generally the diagrams are pretty stable, but there is a possibility of, for example, if I laterally invert it, it starts with QR first and AB is on the other side. It's possible. So there's some minor variations that you can have, but in general, what you look for is the nearness of points not necessarily in what order they have been represented here but does it mean that the lower levels which are there the bottom ones there's kind of a little bit noise where there might be variations but as you start going higher and higher they kind of become stable is that correct that's right so in other words don't be focused on what what is the shape of this what your matter is look at the cut lines and see what clusters they are producing the clusters they will be producing will be stable repeatedly produce the same clusters clusters i see okay thanks yeah but this now let's think okay what are the implicit assumptions we made there are two implicit assumptions we made one is what is implicit assumptions we made. One is, what is the distance measure? Well, we already encountered that with k-means. We need to be clear what is the distance between two points. And the distance must respect the definition of distance constraints. And remember, guys, that definition is invariant. That is true for all mathematics. That's how distances are defined in mathematics. So if you can, please review that. We'll make sure that we'll give you some questions on the quiz to help you review that. But get familiar with that. All of machine learning uses that definition. Now, there is a second assumption that I made, which I blast over. When I said that you have a cluster, let's say that you make a point. You have a point here, some points here. They became one cluster. There is some other point here. They became another cluster. And then let's say that you have another point, another cluster. We have three clusters. You're trying to determine A, B, C. Right? So actually, C, I will mark it as even little bit. Sorry, this is not working out very well. Let us say that this is C, right? And A, let me make it a bit closer. Let me cheat so that I can make my argument correctly. So I asked you this question, what is the nearest cluster to A? What is the distance from A to B? Now you can say, well, you know what? Common sense says I go from the, let's say that I go from the center of gravity from here to here. That could be one notion of distance, isn't it, guys? Are we together? One intuitive notion of distance between two clusters is distance from the centers. But what other notions of distance can we cook up? We can say the distance between the longest points or the closest points. Yes, we can ask for the shortest distance between these two. You know, the distance between the closest points. Or we can look at the distance between the farthest points. Right? So this distance to here. This distance. Right? And based on how you define your distance, see, if you define your distance like this you will realize that actually a may be closer to c than to b right so uh distances can be and this definition of a distance how do you define the distance it is often called the link function or linkage or something. I think link or linkage, something in your book. So there are a few definitions of this. And based on each choice of how you define distance between clusters, your clustering will change. It will produce different results. And that is something to be aware of. In practice, you have to be careful. And therefore, your definition of distance between clusters becomes a hyperparameter of the model. You can try out different clusterings and draw out the dendrograms and then ask which of them looks more intuitive to you from the domain knowledge perspective, which of them resonates with your domain, maybe medical data or whatever it is that you're looking at, which of these clustering most resonates with the experts in the field. And then you pick that, that sort of a, that is a small technicality that you have to remember. So now why would you not use hierarchical or this kind of a clustering? It used to be, K-means is much faster, very fast. It converges. This is more computationally intensive, but these days in modern computers, these classical methods, K-means and agglomerative, are still considered fairly inexpensive methods but having said that it's not completely inexpensive actually the computational cost is in higher dimensional space is very high like because remember a huge number of dimensions and there are all sorts of optimizations like for kd uh k-means clustering i told you that we can use kd trees or in more broadly metric trees to use geometric arguments to accelerate the clustering. I told you that we can use KD trees or in more broadly metric trees to use geometric arguments to accelerate the clustering process and so forth. But in any case, this is hierarchical clustering. This is all that there is to hierarchical clustering. So the gotcha is don't believe that just because B and C are for example, near each other, it necessarily means that B is closer to C compared to, let's say, Q, right? That may just be an accident of projecting it on a two-dimensional page, two-dimensional representation. Your notions of distance are based on where do, so then the question is, how do I know what is the notion of distance? You would say that B happens to be closer to C because it meets C sooner than it meets Q. Where does it meet Q? It meets Q all the way to the top, right? B meets Q here. B meets C where? B meets C, no, not here. B meets C. No, not here. B meets C here. Meets C here. So therefore, you would say B is actually closer to C. Now, that's how you would look at it. But if you were to ask, is B closer to E or F or G? What would be the answer? Where the level is, where it meets in the branches as it goes up the dendrogram. That's right. So if you look at it, what would your answer be? According to this figure, is B closer to E or G? to this figure is B closer to E or G? Can't say, I guess. You can't say. You would consider that as far as this dendrogram is concerned, because they meet at the same point here. They meet here, right? B meets EFGHI all there. So as far as this dendrogram is concerned the information that it has is there at the same distance more or less right now there may be minor variations you have to go back and look at the data and compute the exact values but pretty much from the dendrogram you can't say that because i is is further away in the dendrogram compared to E from B, therefore E is closer to B, that is not true. That is one gotcha you have to remember in the dendrogram. The distances, if you have to infer distance, look at how far up the chain they meet, how far up the tree they meet, not chain, sorry, how far up the tree they meet, this dendrogram tree. And so that is it. It's a simple algorithm, hierarchical clustering. Now, all of those considerations remain. For example, you may ask, okay, where should I put my cut line? How many clusters should I choose? It's lovely to know that I can pick wherever I want. All the clustering is done. And I can find k is equal to 1 all the way to k is equal to n lovely but how do i if i have to just say give me some good number of clusters the same argument applies you will do the ws for for all of these values one cluster this is one cluster two cluster let's say two cluster three cluster and one two three four five six seven seven cluster and of course endpoint of course will be zero and you will have to plot out the this is the cluster okay cluster and this is your wss and you'll again have to draw a screen plot and hope that there is an elbow. You'll have to draw the scree plot and figure that out. And hopefully, if the data looks like what we just do, you'll find that this happens to be around k is equal to 2 for this particular example that we have. So that's how you do that. And that is all there is to it. This is agglomerative clustering. So after k-means clustering, which took a little while to understand, the agglomerative or the hierarchical clustering is a straightforward exercise. Now, I'll give you a homework, guys. Go understand the link functions. Or the linkages. Go understand whatever... Why am I mixing the two up? Anyway, go understand those linkages go understand whatever why am i mixing the two up anyway uh go go understand those linkages it's there in chapter 10 of your book there's a small table there are three or four now the book the last i checked and maybe in the new edition they have fixed it in the in the previous edition they mix they missed a couple of the newer new more recently adopted uh linkage adopted linkage functions in the industry. And so you can read about that, what those are. So take this as an exercise. It's worth knowing what are the available linkage functions and what are their strengths and weaknesses. They have strengths and weaknesses. So read about their strengths and weaknesses. You mentioned that there were two implicit assumptions one was the definition of distance um i wasn't sure what the second one was is this linkage when i was saying i didn't what is the distance between two cluster i sort of hand waved in the beginning when i was explaining to you but then we got more precise and we started asking uh what is it we got more precise and we started asking what is it. Okay. Yeah. And so you realize that you can define it as the distance between the centers, distance between the closest points, distance between the furthest points, average distance between the two clusters, all pairwise distances. You can go on. You can go creating all your definitions of distance. There are quite a few valid definitions of distance so long as they fulfill the distance definition identities they all valid and for each of these you can then ask you will get a different dendrogram okay so the second implicit assumption is that there's a linkage function yeah no the second implicit assumption was that we know how to define the distance between two clusters. Right, and now that is what we came to. That definition is the linkage function. Okay, the definition between the clusters. Yes, of distance between the clusters. Two questions, Asif. How do you decide what's the center of the cluster, right? Just suppose when you bring a point, is there just one way of figuring out How do you decide what's the center of the cluster? Just suppose when you bring a point, is there just one way of figuring out what the center location? The center is again the thing. Once you have a cluster, let's say a, pick the centroid and call it the center. But the non-intuitive thing is even though you can find the centroids, but you don't necessarily compute the distance between the centroids as the distance. You may have another definition of distance between clusters. I see. So that's that part. The other part is that over here, we have these cuts happening at discrete things like one, two, three, seven, like that, which is in k means you have a continuous one, right? You go one, two, three, four, five, six. That is true. Is there some notion that between these two, if we do that, do we have, do we kind of, is there some learning that happens between these two methods if we do that, or these are kind of completely not related to each other? See, primarily they're not not related but what it is telling you the reason that agglomerative is useful it is telling you that it's a we know that the next jump from three is seven right so don't try to build clusters of four and five and six here because those would not be very good clusters see dendrogram in a way because starting bottom up it ends up having more information about the data whereas k-means has no information about the data it just says how many clusters you really want i'll find it no matter what right so that's the distinction between k-means and this this approach this approach. So we looked at, in terms of the concept of linkage function, you basically said there are different distances between the clusters. It could be the distance between the centroid, it could be the distance between the closest points, the furthest points, could be averages and so on. Is there a second concept in addition to that a concept around how do you compute the distance no you always have the distance we already started remember the first assumption is you need a valid distance between any two points okay whatever distance definition there is now you apply it to this okay so that so there are two concepts, one concept, we take it for granted, because that's the part of the domain. Yes. Okay. That's that. So that is it, guys. It's a simple algorithm. I hope after all the mathematics that we were doing a while ago, this feels much simpler, isn't it? These two sessions on clustering hopefully feel simple anybody agrees with that or this is also looking very complex so as if i have quick question a simple question so i was looking to the cdc data for the all the 50 states so i have a excel file that has a all the estates and of commit case so i want to put them into a cluster so i just have a name of estates and the number of covet case or death or something like that so when i'm talking about the points so that i can point find the distance between the points whether the k means are hierarchical. So in this case, what are the points? Point would be the number of... No, see, the thing is, how do I define the point that... See, you must be given data with some coordinates, either the name of the hospital, or the name of the city, or the name of something, something must be given or the most fine grain is that every person who had coveted what's his long longitude latitude usually you don't get that data what you get data is at the level of the either the neighborhood or the city so you must be getting x number of covet cases in this city right but in this case see i just have a number of states. I want to say the X, maybe the Florida and Arkansas or some states, they fall into one cluster, other in a different cluster. So the way I would do that is, you would take the longitude latitude of the centroid of the states as representative. You would say that is the location. And US, even though US is a vast land and it's not a straight line to the first approximation you can just take euclidean distance distance right so given longitude latitude in longitude latitude longitude latitude there is an expression for how to compute distance between two points you can look it up just use that definition of distance between two points two latitude two locations given by longitude latitude that will be good enough for you Is that making sense? Okay, thank you. That is the best you can do.onderance of go with in one location versus another it turns out that when covet started they also started in big cities where their flights coming in and it sort of radiated out from there to the countryside now we have the opposite problem for example the cities are more or less cleaned out, but there is a pretty strong, it's harder to stem COVID in California, at least in the agricultural belt and so forth. So COVID data is a really good data actually, because we are also aware of it. We have a good domain knowledge. So you would like to play with the COVID data. Many, many good notebooks, data science notebooks, you can do with COVID data. And there are lots of data. Asif, when you said that two points are at the same distance from each other, at what granularity are we looking at it? It doesn't matter. If you have three points, let's say that these three points. What is the distance? so suppose this is a b c can you for any reason say that b is closer to a than c so what happens is that it's a rounding error i mean you can decide your rounding but let's say that okay distances and um you don't like if you're looking at distance between two cities and the flight time right or the airplane time you certainly don't want to measure in centimeters yeah yeah good enough so that's where it goes okay all right guys so that's all i have uh if you have any questions you can ask the next uh the next weeks um this saturdays, we will do density-based clustering, which is a little bit more formal way, but it is actually the most, at the end of the day, I find as a physicist, the most intuitive. But we'll come to that.