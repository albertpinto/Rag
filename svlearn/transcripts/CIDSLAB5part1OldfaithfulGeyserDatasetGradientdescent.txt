 So today we are going to do our labs on multilinear regression, regression in many variables. That is one of the topics for today. that is one of the topics for today. Besides that, we will also do univariate regression on a real world data set, the old faithful geyser in the Yellowstone National Park in US. I'll give you guys a background on it. It is the Yellowstone National Park. And for those of you who are not familiar with it, which I think most of you are, but it is a geologically or geothermally active region. It's a volcanic, live volcanic sort of a lot of plate teutonic activities happen there. And there's always the they're supposed to be the molten lava just sort of below the surface or about to erupt or something like that. It's very, very active region geothermally. One of the features of that region is that you have these geysers. Geysers are this water shooting out of the ground to great heights for some time. shooting out of the ground to great heights for some time. Think of it as a pressure cooker valve. When inside the water pressure is high, just shoot salt through these geysers. The geyser runs for some time, let's say about 20 minutes or five minutes or whatever it is. And after that, as the pressure releases itself, the geyser will quieten down and stay quiet for some duration, and then it will again erupt. The old faithful geyser gets its name from the fact that for many, many decades, it has been faithfully erupting every approximately an hour. It is within 40 minutes to two hours. It erupts. Now the waiting time for the next eruption is dependent upon how long the previous eruption was. Sort of makes sense from a physicist perspective, physics perspective. If a geyser has been running for quite some time, a lot of pressure is released. And so it takes longer to build a pressure and the next eruption to happen. So in this data set, we'll have one predictor, which is the duration of the last eruption. And what you have to predict is how long you have to wait till the next one. The waiting time is the predictor for the next one. The waiting time is the predictor for the next one. But the reason I bring it up is sort of univariate. We did a lot of very hard data sets in real life data sets. I was somewhat simpler. And we are going to use this as an example of one of the data sets. So this is the old faithful geyser. If you're looking at my screen is that we can't or not no you can't see your screen and i think did i did i remember to do the youtube thing oh yes i did that youtube thing okay so um let me share that is it now are we sharing the geyser picture on my screen? Yep. Yes. So this is an artistic rendering of other geysers. You can see this water shooting off to great heights. One of the great features, and of course, in the US, everybody knows about the Yellowstone National Park. It's one of the most magnificent national parks. So worth visiting. So I will load this. So this notebook is actually now increased the font size. And go. Sorry. And go. I said can you please one more time? One more time okay there you go and there we go so this data is it readable now to everyone guys on your screens. Considering that we are alive, it could be made a little bit larger. Yeah. Okay, there we go. So this data is very simple. We have eruptions. This is the duration of the eruption, I presume in minutes. And how long you have to wait for the next eruption. Those of you who are in California, you probably know that we have our old faithful guys are here also in Bay Area, in Calistoga, Napa, Calistoga area. It's very, very nice actually. Kids totally enjoy it if you have young kids to go to. So the eruptions of course are not as long. They take three, four, five minutes, and then you have to wait for some time for the next one. You can clearly see that if the eruptions are long, you have a longer time to wait. If eruptions are short, you have less than an hour to wait, right? So this is the nature of the data. Now, some of the best practices that we have been developing along the way, I will assume hereafter that we always do. One of the things we learned is it is a good idea to use a scalar of the data. And standard scalar is quite effective if you don't have outliers in the data. So you can verify that this dataset doesn't have outliers, actually. We'll see it in a moment. We scale the data we look at its descriptions and so on and so forth it's eruption when we scale the data like opportunity yes no no no you should always look at the histograms before deciding on the scalar so here i have not done because yeah so this data set do you notice that I have it here right I have not taken the time to do a lot of cyclic I should have plotted this figure before doing the standardization so what happened is that I knew I'm so familiar with this data. So I've done it sort of backwards. I standardized it, but I should probably have waited to do a data visualization before standardizing. So if you look at the data standardization, even it will not change the shape of the picture, it will just change the X, Y axis. So because I standardized the data, you notice that the mean is around zero. It is not very clear, but the standard deviation is one and it will show up in a little bit. So what do you notice that there is one cluster of values here and one cluster of values here. Right. So you have either short bursts or long bursts of the old faithful guys. It erupts pretty much two groups of eruptions, short and long. If you notice that, and you can see it, once you standardize the data, what do you expect the mean to be? Zero. And so it is 5.15 into 10 to the minus 16. Practically, that's the definition of zero, because in floating points, it's almost never do you get zero. Standard deviation up to three decimal, two decimal place at least is one, right? So the data has been standardized. There are no null values in the data if you plot it it looks like this you can see two groups so later on when you learn about clustering we will do clustering on this data too right so you will get two clusters one for short eruptions and one for long eruptions then today i'd like to introduce this very useful plot. These are called the pair plots. The two variables, eruption and waiting, what happens is this is pretty much the histogram that we were looking at, except that the word for that is called kernel density plots, kernel density estimator plots. What it means is, I'll give you an intuition of it, kernel densities and kernels are things we will cover in great detail when we do the neighborhood methods. The word kernel is very central actually to a lot of things we'll do. But since we haven't done that, it is the same thing. So one year, this is the intuition I'm giving. The intuition is, if it wasn't just this few hundred rows of data, what if the data was asymptotically infinite? Then what would be the distribution that you would see? Density estimation, density distribution. So kernel density estimation is essentially that intuition, it estimates that distribution function, which would have been there, assuming that the data set was very, very large, how the histogram would have looked, histogram would have looked like this, right? So with finite data, it is an estimation of that. It's a continuous curve. That's why it's called kernel density, a probability function that estimates or sort of how much that will be. So how does it extrapolate? Some ways it's trying to extrapolate, right? Yes, it does. And we'll learn about that. What happens is the kernel here, typically these are Gaussian kernels. How do you fit these and put them together? We learn about it in the neighborhood methods will do. But while that topic will come, it's not that hard. See, you notice that this picture here, right? You can see in the two dimensional variable in the two dimensional space, the contour plots of equal density. So this is it. It's a beautiful concept. At this moment, I thought whether to introduce or not and I thought I'll introduce it because it's just such an important visualization. It's good to introduce it early. The top right hand is of course the scatter plot we are all familiar with. This is the wait time, eruption time. As you can see, they are bimodal distributions short long short long so and when you look at the contour plots you can literally see the two clusters form here so it gives you an insight into the data now let's build a regression model where eruptions, like how long did this eruption last, and is the predictor and the target variable is how long do I need to wait? If I've seen an eruption of five minutes or four minutes, now how long should I wait? By the way, it's really very... When you take your children's example here to Calistoga, the first thing your kids will do is they will watch the, with great excitement, they'll watch the eruption. And then the very next question they'll ask is, okay, when do I get to see the next one? Right? So, well, this is going to answer exactly that question. So by now you should be familiar with it. We built a linear regression model yes is the other way on some of the thing holds true like if i wait for this long can i create like oh yes yes yes of course so why an x and x and y how long the next um that is a good thing why don't you do that? But I think people haven't gathered the data this way, that if I see the eruption, if I wait certain time, how long is the next eruption? I don't know whether this data is sequential. If it is, then you can use it. You'll have to look into the original source of the data. If that is true, then that could be another good analysis to do. That's a good idea. It's like a time series kind of thing. Right, yeah. Not just looking over time, but. Yeah, so look into the source of the data, whether it's sequentially given or not. So building a regression model by now, guys, you would all agree that this line of code is pretty easy and straightforward for you to get on this big screen is it coming out legible from the back row uh Premji are you able to see the code clearly I'm looking at my computer oh you guys are looking at your computer okay yes but but on this monitor is it's big enough or do i need a bigger monitor i do need to make it bigger okay what about now this is perfect okay so look at the screen then uh so that i can see see your faces here we go so we this model i hope you would agree straightforward then we build a prediction model by now this is also very so one new thing i wanted to tell you see whenever you're predicting typically we look at mean squared error which is good but sometimes the root mean squared error is more intuitive. So suppose somebody says that the house price was 4000 and your root mean squared error is 100. You know, this pretty accurate estimation of the house price. So that way it is good to know what the root mean squared error is. So and the way you do that root mean squared is set squared is equal to false if you said squared is equal to false in the mean squared error function if you look at that so let's look at the residual analysis what do you say guys in the residuals do you find any choice do you see homoscedasticity or heteroscedasticity Choice, do you see homoscedasticity or heteroscedasticity? Homos. Homoscedasticity, isn't it? You don't see the variance of the residuals change as you move from left to right, right? Move along the x-axis. So there is a rough homoscedasticity. So this looks encouraging. What about the distribution of the errors? Is there any skew in the distribution of the residuals on the side? No pronounced skew, right? Very modest if any skew. And you can do that by computing the skew, but I'll leave you to visually just inspect and say, yes, there isn't much skew. So let us now which what is the final thing we do we visualize the model predictions isn't it especially with one dimensional data it's easy to do so there we go these are your predictions so is this a good model it is i leave a homework for you and what is the r squared by the way that we got a homework for you and what is the r squared by the way that we got uh do we remember 76 percent 76 percent is is it good or bad okay yeah when you look at the data you know you you have to tone your expectations the data is like this so there are things that you don't know right this so there are things that you don't know right the geothermal activity which are actually contributing to it and you have no access to those variables so this is the best you can do with this so it's a pretty good model and you can verify that try building and these things i don't know guys i take this homework seriously because when when you do that, a polynomial regression, you have the code, you can try it out in five minutes. Try it out, see, does it give you a better model? And then ask yourself, should it give you a better model? Right? So I'll leave that as an exercise for you. So this is the old faithful geyser. So I see one quick question here. Yeah. There's also another possibility that happens earlier yeah so i'm projecting a little from this visual to frame my question there's a possibility that there's a structural change in the data right for a moment let's assume the lower cluster had a different gradient compared to the higher cluster if you have to fit a single line through them yes very good question but if you fit two separate single lines that is right that is right we haven't discussed an inference we haven't how to find that what how to treat that yeah so the thing is when you do polynomial because it gives you the ability to flex it can actually come up with different slopes and different regions try doing that but more single polynomial will actually fit it reasonably yeah it will do but there is identifying a split in the data and saying that's the two linear models yes and then they have to join the two somewhere in between somehow so those are called and we will come to you remember i said that the word kernel is an advanced topic we'll do later those are called and we will come to remember i said that the word kernel is an advanced topic we'll do later those are called kernel based regression models that's what they do that's one way of doing it another is to do something called spline make local linear models and keep joining them there is yet another way of doing it which which also we will do, again to do with kernel methods. It is called kernel nearest neighbor, and there are methods like LOIS. What you do is you look at local regions and you make tiny regression models in little patches, and then you stitch those patches together. When the data is highly nonlinear, that is effective. In fact, that old technique is called LOIS. is highly non-linear that is effective in fact that old technique is called lowest in geology that was very very often used the word lowest comes from there okay so there are many many techniques we learn remember this is the beginning of machine learning we have another eight months to go right like two comments on this observation it reminds me of uh like actual sales and things like that which had that characteristic like at different price ranges you would have different factors different factors the few million dollar houses it was cool nothing mattered kind of things mattered and over there there's completely different and they're structurally different they're structurally different yeah between one price range to the other yeah so that was this is a very good point you brought let me further elaborate uh for those of you who are remote sachin brought up the point that the zillow housing data shows pronounced non-linearities and different regions of the feature space there are different kinds of activities or factors involved this actually goes to the heart of machine learning one of the things we will learn is real life is highly non-linear and so most good predictive models they are telling very different stories in different parts of the feature space. So the models are nonlinear. And so when you ask which features matter, like people often say, what determines house price? There is never one answer. The answer depends upon which price range, which geography, which part of the feature space are you looking at? Because there are different factors in play in different. And people know that, people who are in real estate, they will tell you that real estate is a very local market. The forces are very local. That is why local real estate, there's no one global real estate company that has the answer to all problems, right? Or has perfect. They're all based on local real estate intuition and it's the nature of it in diseases everywhere it's true for example when you look at diabetes if you ask in children pediatrics or up to age 15 what is the most common cause of diabetes those factors are completely different for what is most common cause of diabetes in people over 40. Right? So the game changes, even though we build one model, one of the answers to this question, why are most models nonlinear, effective models nonlinear? Because you need nonlinearity to capture the fact that there are different realities in different parts of the feature space right and that's a big topic in fact that's where we are heading uh in another two weeks we like we are still in the linear world in two three weeks four weeks actually we'll be completely in the non-linear world right We have a lot of things to do, kernel methods, support vector machines, ensemble methods, boosting, bagging, forests, right? There is a, and then of course the neighborhood methods. There is a lot that we are going to do, but that will come in due course of time. But the good thing is, the old faithful guys I like, because it's a really neat, clean data data set it has a nice story to tell and very easy to analyze isn't it another data set which i was which reminded me to take a look at is basically the fact the correlation between the fat and the waste size and what happens is that every waste size so there is as the waste size increases the percentage of fat also increases but there is a distribution at each one of them has its own maximum minimum so there is a distribution of people like certain waste sizes how much they spread okay that distribution itself has a nice what you call patterns in the distribution itself okay so as people tend to have bigger waste the fat side of it starts to dominate okay versus the thinner waste the the distribution of fat it basically becomes like pretty normal yes in the sense that it's muscle and it's a very interesting uh data set i can speak from personal experience all right guys so with that so if you look at this the visualization of this data set yes if we you know how this here at the end yeah so if you look at the way the distribution is and if you also look at the rescue plot it kind of even though this will look random yeah but from the distribution perspective yes yes yes that's right i mean you can imagine these little residuals falling off this and you can see that that is what the other plot is showing yes yeah so try this out so guys now that we have the geyser remember that here life was very simple how did we build a model building the model for us was model.fit linear regressor regression model.fit so this is we are using a library scikit-learn library and in real life you often use libraries but then it is also true that you need to learn how to do without libraries. Just from first principles, how would you do the learning? We learned all this theory about gradient descent and so on and so forth. So what does it mean for us to do this by hand? So we are going to do the same data set, simple Geyser data set, but this time around around we will use gradient descent right so i do the basic stuff here things that you are familiar with um here the same data set i use it scale it and so on and so forth the same eruptions mean is zero standard deviation is close to one guys do you see it yeah right and now one of the things that i do is uh so these little tricks you know you'll see me continuously introduce little tricks here yeah what happened this table became do you see that this Pandas table, data frame table, now looks a little bit more interactive. Did you see the difference, guys, compared to this here? I put some color here and so forth. So that you can do by adding a style. I've added a little bit of CSS style to this table. CSS style to this table. How I've done it, you can see, and I'll leave that as an exercise for you to go and look at support vectors common notebook, how you can do styles. Of course, you should have your own styles. My style, I prefer these colors, which are off colors. Typical corporate styles are blue, but I spend my whole day in corporate world, and all I look at is blue, different shades of blue, so I don't feel like looking at that anymore. So i've made all of support vector colors anything but blue. Well, there is the blue, but this blue is midnight blue. It's supposed to be in the color spectrum compatible with it. So we'll have to hear this mostly reddish. I like salmon color. This is the scatter plot of that you're familiar with. This is the eruptions waiting time. Again, you're familiar with. This is just a recap. So now let's go to gradient descent. The beta, next value of the parameter is the beta's current value times this. Right? This is just a recap. We will just take the simple unregularized regression, a simple linear regression, sum squared error is enough. Then if you just work out it is sum squared error, when you take the gradient of the loss function, which is this with each of the parameters, if you work it out, you can see that it works out to this almost literally from here to here, you can see it if you're good this almost literally from here to here you can see it if you're good with your calculus immediately you see it right do you agree that the the the partial derivatives with respect to beta naught and beta 1 come to these two things i'll give you i'll give you a few seconds to absorb it and agree that that's obvious. Do you see that, guys? Yes. That's right. So we move forward. And so when you plug it into these values, into the equation here, this equation, what it leads to is this all we need to do now is to run the gradient descent now gradient descent we will define we will start with the learning rate which is very small actually here i wrote 10 to the minus 5 but that becomes too slow learning too small learning in a single step. I wanted it to be a little bit faster. So I made it 10 to the minus 4 here. Also, typically beta naught beta 1, you start with some random values. So what you should do is remove my 4 minus 4 and start with some random values. When you start with some random values when you start with some random values then it will gradient descent now for reasons of data visualization because you will see a plot that i visualize here i have deliberately started in such a way that the plot that it will draw will bring the intuition out for you so i deliberately forced it to start at this point but for no rhyme or reason just for visualization perspective now we need to discuss a concept called epoch in machine learning during the learning phase we define an epoch is one com one journey through the complete data set are we together when you have visited every point in the data set right you you have completed an epoch. Now, in the gradient descent step, if you look, do you notice that we sum over every single point at each step? If you look at this, sorry, look at this, we sum over every point when we do the next step. So what does it mean? At each step, we complete an epoch. For the training data in the training data yeah for every because in each step we are we are computing the gradient by summing over the the errors of each of the the residuals of each of the data points therefore one step of gradient descent is one epoch now there are many kinds of gradient descent what i have been calling gradient descent in the more formal language would be called batch gradient descent a batch gradient descent is a descent in which the entire data set is one batch the opposite of that is stochastic gradient descent in stochastic gradient descent one step is just one data point you don't have a summation here summation is missing you just learn from one point and its mistakes and then they are in between there is a method in between that doesn't do these two are extremes to take the entire data set or learn from one step learning is from only one point the alternative is to learn from small batches of points those are called mini batch so suppose you have 300 points you will create many batches of let's say 16 points each right and then so then your each step will compute the loss from 16 point next step will compute the loss from 16 points and it will learn from that so there's a Next step will compute the loss from 16 points, and it will learn from that. So there's a lot of learning by the time you run through the whole epoch, right? Approximately 20 steps you would have taken to learn through about 300, maybe 19 steps you would have taken to learn through those 300 odd points, right? If your mini batch size is 16. So there are many different variants to gradient descent the one that we will look at here is the very simple one we sum over all the points right it is the so-called batch gradient descent fullback now this word is a little bit abused the technical distinctions are batch stochastic andatch. But a lot of people, because of colloquialism, they will call mini-batch as batch gradient descent, especially in the deep learning community. And it becomes rather confusing sometimes. But remember, these are the correct terminology. So now, whatever we wrote here, I will rewrite it in code. So I said alpha will be very small. I'm starting alpha as what does this line possibly say? Alpha is equal to power 10 minus 4. What do you think it does? 10 to the power minus 4. Exactly. Thank you. So it is alpha is 10 to the minus 4. Then beta 0, beta 1 are the two parameters of a line, remember? Slope and intercept. I could take initial value to be any random thing, but I've deliberately taken 4. That is purely for visualization purpose. You should change this code. Your homework is to remove it and just actually initialize it randomly. Epoxy is 200. What does 200 say? We are going to take about 200 steps why because simple it is it 200 is already enough right it's a stopping criteria you'll learn for 200 steps and stop now you break the data up into x and y which of course is straightforward you break it up into the arrays and then what are the values that you have to learn? So alpha is this. Beta naught beta one is this. We are going to learn 200, excuse me, epochs. Now what do I do? And this is the heart of it. This is the inner core of what we are going to do. The first one, I just create a data frame in which at each epoch, I will store store that whatever the value of beta not beta when we have learned so far. Raja Ayyanar? i'll store those and i'll store the value of the last function like how what is the last function, so what should happen as I go from Epoch to Epoch. an improvement in the value of beta towards the final answer, beta, and I should see a decrease in the loss, isn't it? You wouldn't expect to see increase of the loss function, blowing it up. You should see a decrease of the loss function, isn't it? Because after all, our loss surface is parabolic, isn't it? So it should see a decrease in the loss function. So let us now, please pay attention to this loop. It is the crucial loop for each epoch and the number of epochs. So for 200 epochs, what do we do? Let's look at this step and tell me if it looks confusing. Compute the gradients. Here are the gradients. So you notice that this is sum over y i minus beta naught minus beta 1 times xi, right? Is this now, compare this to this statement, beta naught, the next step of beta naught is this, beta naught plus alpha, right? So, no, no, first the gradient. This is the gradient this is the gradient gradient is equal to minus minus the sum of y i minus beta i minus beta 1 beta naught minus beta 1 x please pay attention to this equation here is this exactly what i have written in code or here go ahead somebody has a question uh so it's y i minus yi hat the whole square, right? Yes, pretty much. So when you differentiate it, shouldn't there be a 2 in front, multiple of? Yeah, I sort of, the last, yes, that is a very good point. I sort of glossed over the constants, right? The point is too, there should be a 2 here, but I have glossed over the constants. So if you want to be very precise, we can change our loss function. Where is it? Least square is equal to sum over. So let me just make it like this. Frack 1 over 2 is equal to once again this frag okay so let us be precise like people when they're explaining they tend to gloss over it but why gloss over it okay now are you feeling better look at this i added a half here so then when you take the gradient that there won't be a two here okay yeah this uh this constant one over n one over two these things people tend to glass or in the gradient you can apply it to but um it won't affect the results basically okay okay that is why you gloss over these things but it is a good point uh good that you caught it so compute the gradient so we now have the gradient do you notice that this looks exactly what you thought minus sum over y i minus beta naught i minus beta beta 1 x 1 xi does this look intuitive guys anybody does it feel intuitive this statement in view of this statement here look at this minus sum over this quantity yes if you're not understanding it let me know because I have written it in such a way that the code is almost identical to the math likewise the partial derivative with respect to beta 1 d beta here I mean gradient of the loss with respect to beta naught right question yeah this will tell us if x and y the cardinality is different right it basically makes a partition pair of yes yes yeah basically zips is like a zipper you know one one from this side one from this side one from if the personality is different it will go on it will it will give you i don't know whether it will throw an error it i think it will stop at the shorter one but check it out it's a good question i mean the best thing is to do a tiny experiment and see what happens so there we go now the next step is once you have look at this equation once we have the gradients you can take the gradient descent step. This is the gradient descent step, right? So let's go take the gradient descent step. Here it is. Gradient descent step beta naught is beta, next value is beta naught's current value minus alpha times gradient of the loss. Like this, this equation, guys, I hope this is pretty self-evident, right? This is the, this is literally the definition of the loss. Like this equation, guys, I hope, this is pretty self-evident, right? This is literally the definition of the gradient descent. Isn't it? Yeah. Yeah. So this is it. And then after that, what do we do? Now that we have new values for beta naught and beta 1, we can go compute the new loss. What is the loss now that we have improved values of beta naught, beta 1? What is the loss now that we have improved values of beta not beta 1 okay what is the loss here and we store all of it in our table this intermediate stable intermediate values table which we can see what it is by the way whenever you have a table to display with floating points it can be very annoying you can get values till five six seven eight decimal points and it becomes messy so it is a good idea to contain the precision limit the precision to a to a sensible value three digits is more than enough for us to do so i put the precision here for three digits so it's your choice you can make it two, four, whatever it is. Now, do you notice that beta naught starts out with close to four, but then it gravitates towards zero, isn't it? Do you notice that beta naught gravitates to close to zero and beta one gravitates to 0.9 according to our learning. Now, what did scikit-learn have to say? Let's go and see what did scikit-learn have to say when we did that. Do you remember what was the parameter, what was the beta naught beta when we found? Look at this. Does it agree with this? Beta naught, is it pretty much beta naught here is 0.012 and what we found is beta naught is 0.016 if we had run it for another few hundred epochs it would have converged there right and slope is 0.9168 and what are we getting slope is 0.913 means i took i stopped at 200 epoch i'll give you a thing guys run it for 400 epoch see if the two answers begin to match. But do you notice how easy it is that you can do the gradient descent quite literally the inner workings by hand. You don't need to use a library. And as I said, it's good to develop this practice because as you get to more advanced topics, you will be doing it by hand. So let us here plot this. Do you notice that the beta naught values gravitate to its final answer from 4? Beta 1 also gravitates to its final answer. What is the final answer for beta naught? It is, what do you see here, guys, in the graph? Beta naught's final answer is? Zero. And beta 1's final answer is this thing, close to 0.9. Yeah. Right? This is it, as you expect. What about the loss function? Learning is proper only when you see the loss function go down. But what if it starts going back up? You're in trouble, right? Or some strange thing happens, you're in trouble. So here, is the loss function going down steadily? Yes. So here I took alpha is equal to 10 to the minus 4. What do you think would have happened if I took alpha is equal to 1? I will leave that. Big steps. Big steps of learning. I'll leave that as an exercise for you to figure out. I mean, we can run it right now. If you guys have downloaded the code, you can run it right now and see what happens. But I want you to do it yourself and see what happens. Now, I'll plot the contour surfaces. Remember, I told you that in the parameter space, in the hypothesis space, the error bowl, this error bowl is projecting down to the hypothesis beta naught beta one space parameter space right what i keep calling the hypothesis space and there what will be the projection of these iso the equal error curves here there will be curves on this hypothesis space isn't it right so this So those curves are called contour plots. Contour surfaces. In the contour plot, you see these lines. Of course, because we are dealing with two dimensions, so these are lines. More broadly, there will be curves. There will be surfaces. So these are the contour surfaces on this plane. So I've drawn it out. This is real, by the way. This is actually coming from the calculation we just did. Yeah. And what are the colors? Did you link that to some parameter for these colors to happen like that? Yeah. So what you do is you typically pick a good color scheme. Viridis is the default one, I believe. The closer you are in Viridis, the more reddish or violet. Is it violet or what color would you call the center? Violet. Violet, purple, whatever. And the outer you go, it goes through blue, green, finally yellow. So high values are yellow. And i have put the values also do you see that 100 200 400 800 1600 yeah now you notice that it pretty much the error reaches the minimum error now what is the minimum error we reach look at this at 200 epochs we are close to what is the error table showing uh 52 approximately 51.3 right that is where the minima is achieved and you can see that the minima is achieved pretty soon right but there is a little bit more learning to do you know if you really look at it the curve is still going down so you can run another couple of hundred epochs and get the near perfect answer but this is good enough uh this is because i wanted i i wrote the code in such a way that it will run on all your laptops it just it shouldn't be that your laptop hangs and you have to go out for dinner before you can continue so now we keep the alpha constant here we keep the learning rate constant here that is right that is an important point we keep the learning rate constant which is not true when we do much more complex uh regressions or complex classifications and so forth you will see that we what you do with learning is a whole subject in its own right. It is like a specialized bit of research in its own right. There are very interesting and fascinating things people do on how they change the learning rate as you learn, right? And there's a lot going on there. There's all sorts of things like one-shot learning and this and that. We'll talk about it when we get to the deep learning. Now, remember, I keep talking of the bowl. Physically, my error surface bowl. How does it look in code? How does it look? For the same Geyser data set, I visualize the code for you. Right? So what happens is, remember, we started at 4-4. We start here here and the learning happens with this point you see the last point going down down down down down down to the bottom of the bowl and its projection on the hypothesis plane do you see these points and in the beginning do you see that the loss is decreasing by big amounts i deliberately put points to show that it is it is like a rabbit that's hopping forward big strides and then the strides become slow it starts creeping the little rabbit turns into a tortoise right it's creeping forward towards the final answer right and um the code is a little bit more involved i mean it needs understanding there's nothing machine learning about it but usually in this world you'll realize the code is a little bit more involved. I mean, it needs understanding. There's nothing machine learning about it, but usually in this world, you'll realize that once you get the hang of machine learning, you'll start finding that you need to master data visualization, which is a subject in its own right. And believe it or not, like you may dismiss it and say, hey, that's not intellectually challenging. Actually it is, you know why? Because a good data science, typical data scientist make, I don't know, I'm giving you current figures in the Silicon Valley, out of box 250. If you're also a data engineer and a data scientist and you're a full package, well, out of box half a million a year. If you are a data scientist and data engineer and a complete beautiful visualization Guru I happen to know from personal experience from some some people who are doing it can you guess what where do they fall 200 300 400 the outer box of cock a box of coffee easily easily easily three four million so what i'm trying to say is that guys never never underestimate the value of good visualization or beauty in this field like mathematicians they it matters right so one way that i say it jocularly don't take it too seriously see kids when they play with crayon right we consider it normal but as adults we are not supposed to play with colors isn't it the only profession that lets you play with colors is guess what it is mathematics isn't it this is the only one where you see me writing in colored crayons on the on the blackboard if you're physically here writing in color and on the digital blackboard here and in the visualization you can use a lot of colors judiciously so guys there is a the visualization is profound. It can help make huge business decisions. I have seen it in reality. Florence Nightingale essentially founded the profession of nursing with one visualization from the, of fatalities in the Crimean war, right? And that tradition has continued. When we say that a picture is worth a thousand words it's really true in the world of data even more true because when you create models and you come up with insights and if you can visualize that insight it is worth a thousand words right so why don't you use this during your talk when you actually teach this concept the drawing point that you have done no because then i can't do dynamically right i can't draw the surface and draw lines etc you can take this and then yes i could have done that i could have i tend not to use pre-prepared material because i feel that learning should be spontaneous and it should be in the flow of conversation so to preserve, it's a basic educational philosophy for me. So to keep it in the flow of things, see I have seen a lot of material and all that material is dazzling and I can create dazzling material. The trouble with dazzling material is we get dazzled whereas learning takes place best in the flow of conversation with almost no devices right right so when you when your younger brother says explain something to me over a dinner table you may at most have a pencil and paper in hand but that explanation is gold compared to watching elaborate presentations on the internet so that is that that was one of the great insights. For example, Khan, Salman Khan. Before that, online education was all about well-choreographed videos. I mean, it was almost becoming like Hollywood. If you go back and look at some of the Linda Learning or LinkedIn Learning now, they are India Bayou, right? They are very successful, there's no doubt about it. But it's a different educational philosophy. There, each of the videos is, there is a subject matter expert who tells you what to say, right? There is actually a choreography team there. There is an actor who will voiceover the instructor, who will literally master that script, and is not a subject matter expert, but will explain using the words the subject matter is given. And usually the person who is teaching for example if you go to biosector is generally young and pleasing looking making just the right gesticulations right so the entire thing is very choreographed but the trouble is you can't ask a question in between right isn't it but khan took the opposite approach salman khan he was just going to teach his niece right so he just took a little board and scribbled on it and started explaining that and that turned out to be perhaps the most profound revolution in education of this century it completely rewrote how learning uh online learning should be done right and you can see the effects of that everywhere before that before that people used to do a lot of power points and so forth but for example if you look at andrewing today even he does that occasionally he will come up with formulas but then he will doodle a lot all over it in the khan academies in the salman khan style right now i have thought about bringing that one reason i don't bring even like like Andrew in some things written is because I feel that it forces me into a structure and it prevents students from driving the conversation. Whereas when I'm in the classroom, I would rather that the students understanding or lack of it, the interruptions, the questions drive the agenda. In other words, don't come up with what you have to teach in a rigid form. Let the questions in the class create a flow of conversations and go with that, which is why I never really know what I will teach and how much I will teach or where I will put depth. Yes, that freedom I. I remember I kind of bought it just like five, six years back. Some the primitive whatever software was available that time and it took me a hard time I think that's been running upwards of a week trying to. get it running right just but the part was this the amount of detail that you have put in this piece, right, the whole thing so fast. Well, I was planning to do this exactly like my whole, like last week I've been thinking of how to do it and now you have just shown it over here. And I didn't know if during the class if you had done that, if others would have appreciated or they would have got lost in just the beauty of this that is it see one of the things we realize in research is education very active area a lot of opinions and so forth and obviously i'm not an expert in education i my understanding of education is through 30 years of teaching post graduates right and 30 32 years now but um and you learn a thing or two, and I read a lot of books on education and research on education, but I'm not a formal expert in it. I'm an engineer and a scientist, physicist, right? But my general experience has been, and the body of research that I have read says, that two things you should avoid in education. Whenever you're doing online or something, keep your face away from it. Because human beings, the whole brain structure is designed in such a way that the moment it sees a face, especially the eyes, it latches on to that. And it just keeps looking. So remember, there were some videos which was using called lightbox, the guy would stand and write, it would look as though the person is writing on a glass from behind the glass and you see the person. A lot of videos where it was, it is super impressive. But if you actually look at what people are paying attention to, they're paying attention to this guy speaking through the yeah through this because a human mind is deeply deeply conditioned to look at faces. That's how we recognize danger, that's how we recognize emotion, we keep looking at faces. So the best way to teach actually is to take the face away like for example when I'm teaching I'm hoping I don't know what you guys are seeing what you you're looking at is the code. And maybe in the top right-hand corner is my face somewhere, but it's not dominant there, right? The second thing that you learn is that use colors judiciously, principle of least ink. Use color, but use the least ink to express it. When you see this, how much ink there is. So it looks very pretty. It's see, sometimes it's good like for example i believe that once you have seen those drawings then to see this is very impressive but to start with this in my view is not a good idea it's a better way to first do hand hand drawings and then finally come to this and not only come to this so this and by the way, the code, if you want to see is very simple. What I have done is I've plotted the surface, right? Surface plot. And what I have done is because see here, the loss is only 53. So the error surface is practically sitting on the parameter space. The net residual is too small. So just to visualize, I lifted it up I deliberately added 2000 extra error points to it to create the visualization then you play around with Alpha but then there is nothing else now the contour lines these are the hand driven control making sure that my contour line should be at these values and then the rest of it is very simple the contour plot and the main plot. This is it. Two, three lines of code. And then you get it. It takes a little while to become good at plotting and creating visualization, but you do. But then what you can do is I've also given you, because I knew that some of you would want to do this on your own. So I've not only put the code, I gave you the code to make it into a movie. So this code will make it into a movie with one point. Remember, I've said iteration to zero for safety. If you blindly run this notebook, you may accidentally end up running this bit of code. So to prevent it from being done, I've said the iterations to zero. Comment this out and uncomment the next line out because when it runs on your laptop unless you have a powerful laptop it can easily run for a few hours making movies is always longer right so ultimately it produces this movie let's try this right uh i will start this i will just decrease the uh just forgive me so guys pay attention i'm going to start this this gradient descent spread keep your eyes on this point you see where my mouse is keep your eyes there and see what happens do you see the gradient descent taking place yeah right and you notice that as you come closer to the final because at the bottom of the bowl what happens? It's not steep. The gradient is low. So for a fixed learning rate, if the gradient is low, the steps you are taking are slow. Right? So you go like a rabbit and then you start creeping like a turtle, like a tortoise. It was the final answer. And there you are. Was it fun? Yeah. Yeah, so we have to re-download that lab. Oh yes, you have to re-download the lab. It is online though. Yeah, the latest version is there, yeah. That's the difference. Come again? The lab, why should we re-download? Oh no, no, because my teaching faculty, we reviewed this yesterday or day before. So they didn't have the latest version. They didn't have the movie there. Oh, this little movie. I have that. I downloaded it. Yes. So this is it. So guys, do you see that the magic behind the theory that I taught you, right, is very real. You can literally code that theory into code that theory and see it work right and so that is that that is one notebook let's take a five minutes break and then we'll go to the next data set which is about pondering over the mysteries of whether high horsepower engine have low mileage or high mileage