 We are now going to take up the topic of information gain, entropy, cross entropy, and KL divergence. This is relevant in the context of variational encoders. So it's a quick reminder of what this is for you guys. This is relevant in the context of variational encoders. So it's a quick reminder of what this is for you guys. So suppose the way you can look at it is, suppose you have a friend and you go to the friend and very confidently reassure him that the sun will come out of the east in the morning do you think your friend has gained any information or learned anything new no no right why because the probability of that event happening was one. It was guaranteed to happen. But suppose I were to tell you a bit of information, a little bit of information that says tomorrow in the Bay Area, it's a clean air day. We are all supposed to take the public transit. Now, let us say that in the year we have about 30-40 such days, right, in a year out of 365, so that we have one in 10 days, does it come to one in 10 days of clean air, the probability that it's a clean air day is 10%. And you tell your friend that tomorrow is a clean air day. Now, do you think your friend has gained some information if he didn't know that? Yes. Certainly has, right? And now I tell you about a very, very rare event, or not so much a rare event. Let's say that I tell you that the Golden Gate Bridge is blocked and you have to, your friend has to come over the Golden Gate Bridge. The Golden Gate Bridge is blocked because they're doing a major repair. And the probability of that, that sort of thing happens, let's say once in three years years, the probability of it happening is one in 1000. And now what would you say how much information has your friend gained? You would agree that the friend has gained far more information now from being told about a fact whose probability is very low. So do you see a pattern here? The more rare an event is, the more the information gained when you inform that that is going to happen. So far so good guys. So how do you quantify that? So you can say that the information gain, gain, and I just use the word information gain, let me just use the use information content, is somehow, somehow it depends upon how rare the event is, depends upon the inverse of the probability in some sense you can say right the rarer it is right the smaller the probability the bigger the information gain would you say that roughly speaking a relationship like this guys I need some feedback. Does this make sense? And so now what happens is the way you do information gain is because some events are exponentially improbable and some events are fairly common. So as always, whenever you deal with distributions that have some degree of exponential distribution, you tend to take the log of it. Right, so when you take the log of it, this becomes log of 1 minus log of p. And what is log of 1? 0. And so the information content becomes log p. And by log, we mean log ln to the, you know, the log to the base e, which also I can write as ln. And I tend to keep interchanging between ln and log e p. Now, in information theoretic context, they actually use log to the base two, right? Half as likely, one fourth as likely and so forth. So it doesn't matter which base you use, whether you use base two or base E, because let me see, I think I might get it backwards. Log two is equal to log E P divided by log E two, something like that. I think this should be the relationship roughly speaking. So you can work it out, it's very elementary algebra. So it doesn't matter what base it is, but by convention outside information theory, you tend to use, or at least in machine learning, we tend to use information as just a natural log of it. in machine learning we tend to use information as just a natural log of it. So we learned something from this little thought experiment. We came to the conclusion that information gain is a good way of writing it is to say log p. Now comes an interesting thing. thing. Suppose there are two events, there are two possible, there are two values. It is the bridge is okay, and the bridge is broken, is under repair. So let us say that we relax the condition. If there will be a slowdown or there'll be a congestion because some of the lanes in the Golden Gate Bridge are blocked or something like that, more likely the Bay Bridge is blocked or something, it will impact you. Let's say that the probability of bridge being okay is, what should we say, 9 over 10. And the probability that bridge is under repair is one day out of 10, it is under repair. This statistics is I hope somewhat representative of the reality. I don't know how it is, but let's put it like that. The Golden Gate Bridge is like this. So what happens is that every single day, your friend wants, my friend wants me to inform her whether or not the bridge is okay or under repair, whether it's under repair. So you make a phone call. So when you make a phone call here, the amount of the information that you transfer, like okay day, is equal to minus log 9 by 10. And the information that you convey on the days repair is minus log 110. And now what happens is day after day after day you keep doing. Let's say that you do it for a thousand days right so then the question that comes is what was the average information conveyed over a large period of time average information So what happens is the days it is okay, you have very little information conveyed, right? Minus log 9 10th. Oh, sorry, not minus log. Yeah, 10 log p. Yeah. So you have very little information conveyed. So you have very little information conveyed, right? Minus log nine tenth, sorry, not minus log, yeah. 10 log p, yeah, minus log p. And on these days, you have a lot of repair conveyed when it is only one tenth, a lot of information conveyed. But let's say that you do it for a thousand days, how many days do you convey the okay information, approximately? Nine hundred. Yeah, nine hundred. So whatever the total is, 9 tenth of the days, so if you just look in terms of proportion, you would say that 9 tenth of the days you convey minus log 9 tenth of the information, minus log nine-tenth of the information, right? If you were to measure it. And one-tenth of the days, in a large number of days, you convey minus log one-tenth info. So if you were to find the average information, and the word for that is expectation value of information, you can say, but we'll just use the word average. What will you do? You'll multiply this by the, you know, the nine tenths of the days you can maybe conveying a lot, very little information. One tenth of the days you're conveying a lot of information. So you would agree that to take the average, I need to do this. Many days you convey very little information and some days you convey a lot of information. And this would give me sort of the average information conveyed over a long period of time to my friend. Would that be fair that on a given day approximately I convey the weighted average of the information? Does this make sense guys? All I did is a weighted average. Nine tenth times the information conveyed when all is well. this is the okay world this is the world of repair so that's information gain character on the left equals those okay and the repair yeah what we are doing is yeah the this was the so basically what we are saying is let me just write it nine tenth times information gain okay because, because 9 10th of the day, it was 9 10th of the time. It was this. Actually, let me put minus so that this becomes and 1 10th of the time. It was I repair. Would you agree? Right. If the distribution of days is most of the time, it is OK. And 1 10th of the time it is is this so then this would be your average information gain inside the bracket right that would be the average of the two weighted average so another way to put it is that if you take this information okay information gain is a drop of water right let's say that nine days you got one drop of water rained but on one day 10 drop of water water rained so how would you find the average water range you would multiply nine times one you you would want to find the total rain right total information gain the total information gain over the whole time and that would be nine nine days it rained one drop plus ten on the one day it rained one day it rained ten drops right ten drops of water and so uh and divided by 10 the total and so what is that it is equal to essentially 9 10th of the time you got one drop of water plus 1 10th of the time you got 10 drops of water right and so when you work it out it is 9 10th plus 1 right Am I getting it right? And that is equal to 19 over 10. And that is equal to, what is that? 1.9. So what it means is that on average, you've got two drops of water a day. Does that agree with your intuition, guys? Yes. This is that. So this is all we are saying. So the average information gain is given by this. And if you look at what we did is, now if I generalize it, it is just the minus probability, like how often it happens, probability, times the information gain conveyed by that probability, which is log p. And this thing, this quantity, h, has a name to it. And let me just again write it. h is equal to the expectation value. Remember the average, the word for that is expectation value. It is the average of the information gain, right? It is the average of, and therefore it is equal to minus p log p, right? There is a name for that. Anybody knows what this is called? So this is called entropy. In machine learning, we call this entropy. This is the average amount of information that you gain if an event happens, if the probability of an event is p and you tell your friend that p is happening. So far so good, guys? Yes, sir. Yeah, it is straightforward. Now comes a very interesting thing. Let us say that you have the, there is a difference. The probability of the event that the proportion of the time the event is happening is p but the information that it is conveying when it happens is coming not from what you expect but for whatever reason is getting skewed it is coming from q right so one let me try to think of a physical intuition let us say let's look at the bridge repair situation. I told you that the bridge repairs 10 times, so nine times you don't have this problem and one day you have the problem. But then all of a sudden we bring in, so you have a certain ratio of this happening, not happening, and there is a third event, so p i q i. Let me say that there are three events. A bridge, fine, okay. Bridge repair. And then there is like the final one, an earthquake happened and there is a bridge collapsed. Your P distribution says 9 10, 1 tenth and 0. Are we together? But let us say that suddenly, and this is the way the information is conveyed, this is the expectation, belief. But all of a sudden the ratios change. What happens is all of a sudden these okay days become very little. They become three tenth these repair days and the collapse days let's say that the bridge they repair and it collapses and it repairs and collapses by the way it's not in u.s is very unlikely but i come from a town a city an ancient city of varanasi benares where all through my childhood in the 20 years that I lived there or sort of was there, they were trying to make a bridge, a new bridge over the river Ganges, the Ganga River, the Ganges. And I can swear to it that in 20 years, every year the story is they would make a little bit more of the bridge and the floods would come and the bridge would disappear and then they would start all over again. So imagine some situation like that. So let's say that the probability that it has collapsed is like a four out of 10. And then the rest of the time, it's just under repair, or maybe the number of days it is like five. It's under repair most of the time, and only one day of the year, it is actually being fine. So what happens is that you expected this, but this has happened. Q has happened, right? This is P, this was your expectation, but instead Q has happened. So then you can ask that, what is the cross entropy now? How much information am I gaining from the Q given the fact that the initial distribution that I had expected was still P? Now you're comparing literally, you're looking at this, right? So this concept is a little hard to grasp the first time you get it and i'm not sure that i did the best possible example but i took an example that came to my head at this moment right so this thing is called a cross entropy this is well if entropy is cross entropy h cross is minus p log q because q is what is happening given that p was the expectation that p is what you thought it was now what happens what are p's and q's these are probability these are probability ratios in the continuous case these are probability distributions right so i'll just stick in a variable x being the day the random variable These are probability ratios in the continuous case. These are probability distributions, right? So I'll just stick in a variable X being the day, the random variable being the day of the year, any given day, this is a one and QX. These are probability distributions. Right, and I believe this is it. P Q is written like this. So what happens is that the information that will be conveyed, you will realize will be significantly more now because it is breaking away from all your expectations or all your what you thought should should be happening the the the world has changed basically for you right so your cross entropy will be quite a bit high and if you ask how much of a deviation do i have from my normal world what was my normal world my normal world? What was my normal world? My normal world was this. P was the expected. P was my normal world distribution of probabilities of how the bridge behaves. But it's behaving like Q. All is not well with that bridge. So the gap between, so let me just call it H just of P. And this is PQ if you have two different things so the gap between now this one is more it turns out that there's more information conveyed here so if you do this h cross pq minus hp this gap between these two there is a name right and this name the gap will of course be smaller the more the closer q is to p you know if q is a slight deviation from p it doesn't matter but the more q differs from p the bigger will be this gap this gap is literally the divergence of the information that you're gaining from this new situation anomalous situation compared to what you expected it to be p being what you expected it to be right and this divergence was created by two scientists whose names start with k and l so it is the famous kl divergence are we together, guys? So what it means is KL divergence tells you, suppose our distribution is like this, P, and you have another distribution like this, Q. How different is P from Q? This is a KL. So you say P given Q, right? Is equal to, this is it, HPQ, right? And so it comes out to be a P log, I believe it is P over Q, P over Q or with a minus sign or maybe the, I forget, guys, you will have to look up the textbook, whether a minus sign or maybe the other i forget guys you will have to look up the textbook whether the minus sign is there or not it is that and of course when you have a lot of events that you are summing over then of course you have p log p q right so the scale divergence is asymmetric think of one part as the expectation and one is the reality right there are two different distributions one is the reference distribution and you expectation and one is the reality right there are two different distributions one is the reference distribution and you ask how much is the other distribution probability different from this particular probability are we together guys so that is scale division so we suddenly i mean this is it this is the idea now if you look at our situation i talked about let me summarize i talked about information gain i is minus log p i talked about the the average information gain which is called entropy and that is p log p and of course if there's a summation you can do your i and here also i can stick in an i here while we are at it right this is their total information gain average information gain and this is the total info i is the total information gain on the other hand if you your expectation was p but you're seeing q then the cross entropy the q, then the cross entropy is p log q. And then the KL divergence is p. And you use the symbol given q. Let me put this symbol in a special way. Most people will use a double bar in notation if you look at it in textbooks. You'll see this. This is a typical notation for KL divergence. And this is this. I think it is, again, forgive me if I get my minus sign wrong. Log P over Q. Are we together? Right. And if one of you can look it up, please do correct me whether minus is there or not. But the important thing to know is it is a measure of how Q differs from P. Are we together, guys? P and Q are different. So that is that. Now, how is that relevant? Any questions, guys? I hope in a very quick succession, I've taken you through a lot of concepts that we use in machine learning. I'll give you an example of this cross entropy. This is the cross entropy. So remember that this cross entropy is This is the cross entropy. So remember that when you, this cross entropy is the loss. When you have a classifier, let's take a binary classifier. Let's say binary. Like whether it's a blueberry or not. A blueberry, strawberry. Remember the example we have been taking all along, blueberry, strawberry. So we say that, what is the, what is your, we call it the loss function. It is basically your H, cross entropy, H. So what happens? You expect the, when you expect a blueberry, a blueberry, let's say that you mark blueberry as one and strawberry, the probability of blueberry is when it is actually a blueberry. So in other words, when it is really a blueberry, the probability of a blueberry is one, isn't it? The reality and the probability of a strawberry is zero. Would you agree? Yes. It's a given. It's a reality. So what is p? p is equal to one. So you have this probability, which is the reality, y. I'll just write it as y. So y, and what comes out? Q is what comes out. So here it is. Q is what comes out, log. And what is Q here? Y hat, the prediction, isn't it? So what happens is when it is really a blueberry, you wanted to see one, probability of one, and you got Y hat, right? Y hat. So what are you doing? This is a comparison of what? Do you see that this is cross entropy term i hope you guys see this right this is a cross entropy term of course with the minus sign there and then we also do one minus y j one minus y y i log one minus y hat so in other words the probability that it is not a blueberry when you're looking at strawberry the probability of a strawberry is 1 which means that the pro it is because you take 1 and 0 for the blueberry and strawberry because of the convention. So one for strawberry means you'll have to subtract one from zero, one minus zero to get one, and then log of that. Because yi is the probability of it's being a blueberry. So one minus y hat will be the probability that it's a strawberry. And so this is what you get, right? So this is again, in both these situations, you expect a blueberry for sure. What what you get. So this is again in both the situations. You expect a blueberry, for sure. What did you get? Or likewise, you expected a strawberry and what did you get? What is the prediction? The gap between the two is obviously is cross entropy. And as we know from the classifier, that was our loss function. Do you remember that we always use the cross entropy loss? And sort of it puts it in context, putting this whole explanation in context. Now, why is this relevant for us here? Let's come back to our autoencoders. relevant for us here. Let's come back to our autoencoders. And we are a little bit running out of time. So I wanted to do the word embeddings and everything today, but I'll give you guys a flavor of it, right? So suppose this is X and you create a Z. What you see is, you know, remember that I gave you the initial expression that you have, well, long, long ago, that is many hours ago, I gave this expression. So I'm going to now borrow this expression down. If you remember the Biaschens, the probabilistic way of looking at it is, you're basically saying what should Z be given an X? What is this? That is essentially the answer that you're searching for in this autoencoder. So you can say that this is equal to probability of X given Z, probability of z over probability of x. Now this term is intractable. It's most integrals in Bayesian and it's one of the reasons Bayesians had a tough time till computing power increases. Their denominator used to be quite a nuisance, quite a problem to compute and the two approaches as we mentioned Monte Carlo are variational inference. So I'll give you some flavor of the variational inference. How do you do that? I won't tell you all the steps. You realize that you have this to vary this probability of X given Z. So what you do is you instead say I will take another distribution I will take another distribution, right? And then I will look at the KL divergence between this and the P z given x, right? Between these two, I will look at the KL divergence. So what I'll do is I'll make this new distribution, fake this because these distributions are very hard to compute. I'm just going to fake it, right? Or simulate it or imitate it, right? And I can learn to create a good imitation. Remember we did that in GANs. We can learn to create a good imitation by back propagation and by everything. Now, but then what is our loss function? What we do is we take this, it becomes z, then from z we produce x given z. So suppose you have z and you have probability of x given z will come out here. So what you can do is you can find the q and you can try to minimize this. And I'll just, actually, it is not as simple as that, but I'm giving you a hand-waving argument that the loss function is based on something like this, a divergence between. I won't use this because the strictly mathematical expression is different. If I can continuously keep pushing this down, what will happen? I have managed with q to imitate this this distribution which was hard to compute now well you say well how do you imitate how do you do that to imitate another function or another distribution you must have some knobs to turn so for example suppose their distribution is like this and your distribution is like this you do have a knob to turn right one is the center the mu of your Gaussian distribution the other is sigma you can move the mu around a little bit you can decrease the sigma till it begins to look like a so b begins to look like a do you see that guys that if A is B and B is Q, you can play around with B as a function. If I can write it as a bell curve of mu and sigma, I can play with mu and sigma till B sits on top of A or comes as close to A as possible. Another way of saying it is that the KL divergence P, Q becomes minimum. saying it is that the KL divergence P Q becomes minimum. So far so good guys? Do you see the argument that if you have a distribution in which there are knobs to turn like in a bell curve the knobs that you can turn is where is the mean and what is the standard deviation what is the spread of the bell curve therefore you can tune it till it begins to look like a provided distribution assuming that the original distribution has some bell curve like shape you can play around with it and sit on top of that am i making sense guys right because you're using a distribution of normal curves and you can adjust the... Yes, exactly. The parameters. So your theta, the parameters that you play with, let me put it this way, the theta, the parameters you play with are the mu and the sigma, right? And so let us do this. Let's create a very interesting autoencoder we say that given this and i'll redraw it but given this input x right what we will do is we will create let's say 10 you can pick your dimension, 10 dimensions, let's say. So you will create a sigma vector, which is made up of mu 1, mu 2, mu 10, right? So actually, let me do one thing. Let me take two dimensions just to convey the meaning, because I can actually show it in a plane, right? So this represents what? Some center of a bell curve, isn't it? Some center of a bell curve. Yeah. And then I can take a sigma, actually, traditionally you use a big sigma here, a sigma, and that will be made up of sigma one, sigma two. Actually, it's a matrix, but okay, I'll tell you. It's actually a matrix. actually a matrix sigma two sigma one and sigma two square well okay i'm fudging my notation a little bit but assuming that these diagonals are gone i mean the off diagonals are zero then you can just look at it as something that is sigma one and it can be uniquely represented by sigma one and sigma 2. So it becomes a sigma matrix. Assuming that this so-called variance matrix, sigma, of the bell curve is such that all the off-diagonal terms are 0, and this is a theory of bell curve. If you don't remember it, that's fine, the normal distribution. And I'm saying if only diagonal terms survive, then the diagonal terms can be written as a vector, sigma 1, sigma 2, right? A two-dimensional vector. So you have mu as mu 1, mu 2, and then you have sigma as sigma 1, sigma 2, right? Sigma 1, sigma 2. And now what you do, these four parameters are literally here. You have a mu 1, mu 2, that is, in other words, the mu vector, and then you have a mu 1 mu 2 that is in other words the mu vector and then you have sigma 1 sigma 2 and what you do is you make this autoencoder learn these parameters basically right learn these parameters now you say well that is very interesting because by fudging these parameters by turning these knobs you make make this process the recovered y look like x basically and so when you are successfully able to do that you have one interesting thing what you have is not just you can represent x when you transform it you can represent it in terms of bell curve normal distribution xi is some normal distribution i normal distribution in terms of mu one mu vector and the sigma vector are we together? It will be some bell curve in the hidden space, in the latent factor space. And then you can recover it back to your initial vector. Well, now comes an interesting thought. What if we put a sampling layer in between? And there is a reparameterization trick I won't go into. Suppose it goes into this and then it goes out into that. What it basically means is that suppose you feed in X, it will become into this, it will feed into this layer. Then from this layer, what this guy will do, this layer will do is it will sample into the bell curve distribution means it will toss a coin and it will pick somewhere some value here so it will pick a value from here here here somewhere around the bell curve and it will pick more values close to the center of the bell curve than away from it are we together and then it will recover that so now what has happened is the input and the output, once this thing is trained, the output is deliberately a fudged version of the input. Do you see that? Wherever the value went, you went and you added some, let's say, deviation to it, you added some, let's say, deviation to it, and then you decoded it. So the advantage of that would be, so what is the advantage of that? There are two advantages of that. One is that suppose you give it very clean data, 1, 2, 3, all the way to 9, 0. Each one of these, let me just represent it as a box, VA box, zero goes in, it becomes some internal representation, it gets shaken up a little bit, and so another zero comes out that probably looks slightly different. Right? And all of these digits, when they go in, they come out looking slightly different. Right? When you, once you have done this and you're sampling for that, you're playing around with that. What is, why is that useful? I'll tell you why that gets to be useful. Suppose, so imagine that you're creating a, I ask you to write the sentence, the cow jumped over the moon. And I say, write it in different handwritings. How can you do that? You take a T and you let's say that it has been trained on the alphabet also, you pass it through this and a T will come out that will look different from the original T. Isn't it? You pass it again because there's a random sampling. Now it will look different. So each time you ask it to produce a T or any of the letters or digits, they will all start looking different. In some way, it will be different from what it has learned from. And therefore, it has the quality of generating new values, new data. What you can do is you can go back to your inner layer. Remember the internal representation with your mu and sigma. And now you have a sampling layer. Its job is just to sample from these distributions. It will randomly pick. So suppose you ask it to go to two. What it will do, it will go to the latent factor space. It will say, where is two? Let's say two is here, right? This is two, right? With its value of mu sigma. And what it will do is instead of hitting here, it will just add a little bit of noise, make a random noise. And in fact, that noise will be in itself a normal distribution, but okay. It will take a random noise and it will hit here right and assume that the other letters are this is two, three is here, they all far apart, four is here right. little bit you know this is the center of it you randomly move around where the two is and then you decode that you take that and you decode it what you will end up producing is a differently looking tool and so you can go on producing as many new versions of the data as you as you want do you see that guys right now it has some very interesting applications. For example, so you say that why is that useful? I can produce this. In games, when people create games, they use it a lot. You know, when you are doing the scene generation, if you remember a few years ago, all the trees used to look alike, or there were just a few tree types, and they would keep showing up in your scene, in the game scene. Nowadays, all the trees and all the things look very realistic. So guess what is there in the belly of it? You have the variational autoencoders sitting in the belly of it that's making this magic happen. Does that make sense guys right so every time you want to plant a tree into a scene you ask it for a tree it will give you a new kind of tree right you may be asking for the same thing you may be asking for the palm tree but it will give you some new variation of the palm tree you ask it for a face and it will give you some new variation of the palm tree. You ask it for a face and it will produce a completely new variation of a face or a digit or whatever it is that you want. So that is the power of variational autoencoders, very useful. They also remove some of the limitations that were there. They tend to have much better separation in the latent factor space space they tend to have much better separation between the you know between the classes because by construction each of these is a up little bit here the value of mu right those two bars in middle one place mu 1 mu 2 and then Sigma 1 Sigma 2 that that part I didn't understand I think are the those are literally the knobs you will learn by training this neural network you're saying if i pass in x and i want x to come out right in the latent factor space like which is defined by these what should be the mu 1 mu 2 for x let's say that the value is 0 goes in, right? What you're basically seeing is in the latent factor space, right? Where is zero? You're learning where is zero and you say, oh, zero is here, right? So by squishing all the zeros through it and all the other digits through it, what are you forcing this autoencoder to do? Learn the perfect location for zeros in each of the digits. Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal Vatsal are the variants or the spread in the two perpendicular directions isn't it are you getting it so there will be for zero a unique set of value for this for one again there will be a unique set of value for this right so one may look like this much be more ellipsoidal right but Right? But this is it. Pradeep, are you getting that? I think I'm still lost. So mu1 and sigma1 is for one set of bell curves. Yes. See, one bell curve is one bell curve in one dimension. 1D bell curve is uniquely defined by mu. Right. Correct. Mu and sigma, right? Mu is the center and the sigma is the spread, or how flat it is. Sigma is the spread, exactly. Standard deviation. This is your sigma, standard deviation. And mu is this location along the x-axis where it is sitting. And why we are using two bell curves here, sir? Because imagine that it's not a bell curve, but it's a bell hill, a two-dimensional. So imagine two-dimensional. So let me draw it out in three dimensions so that it becomes more clear. So imagine that here at this particular location, there is a bell hill, right? And the center falls at this location. This point is in this hill, right? And the center falls at this location. This point is in this space, right? Let me just call this X1, X2 space. What is this? This is mu1, mu2. The coordinates is mu1, mu2, the center, right? And now you look at this bell curve. Now this bell curve, you know, it may be more spread in one 2, the center. And now you look at this bell curve. Now this bell curve, it may be more spread in one direction than the other. In other words, if I look at the contour lines, it may be like this. It is centered here, this. But if I draw the contour lines, the contour lines may be. So you notice that in one direction, it's spread like this. In another direction, it is spread much more. Got it, sir. So that is why you need mu1, mu2, sigma1, sigma2. And now you can generalize. By the way, I keep on doing two dimensions, but I mean low dimensions. And so what the variational autoencoder is learning are the perfect mus for each of the digits, mu sigmas for each of the digits, isn't it? It is learning to produce the perfect mu sigma representations for each of the digits. In other words, it is learning to place in the latent space where each of the digits should be, right? That sort of thing. It is putting it in the latent space, the centers. So the quality of this is, it has two things it removes from the normal encoders. One is that the things tend to be much more spaced out. You have much less overlap. So if you take any arbitrary point, if I take any arbitrary point, it will be close to, maybe this is a bad one. If I take an arbitrary point here, I always know which bell curve has greater probability at this point, right? It is A, B, C, D, E. Which of these Bell distributions, actually we shouldn't use the word Bell curve, people call normal distributions, let me call it Bell Hills. Which of the Bell Hills has more influence, more value here between A, B, C, D, E? A. Say it's A here, right? So what happens is if you randomly pick this point, you know that what will come out of the other end is an A, something that looks like an A, isn't it? So there are no holes in this representation. And the other thing is, yeah, so that is another good thing. These things tend to be well separated, and you don't have holes, basically, empty spots in the latent space. Every point in the latent space. Every point in the space is well accounted for. That is why variational autoencoders are an amazing thing. We use it a lot for many, many things. And the other most beautiful quality is their generative models, because once you have trained them, they can help you generate a lot of data and that should remind you of the Gantt's and so forth that we did in the past but the data generated would be similar to it will introduce only little variants or yes I need to introduce more also certain way so suppose I take a B I put a point here so hopefully it will look something like this. And now we are lost. Is it A or B? Right. But there is probably no use of doing that extra extra variance. It depends on the situation because suppose you have just for fun, you may have Labrador and golden retriever. And just for fun, you may want to produce a Labrador retriever. So you can mix and play. See, ultimately, it's all a game after a little while. Once you have a latent space representation and you have done it, training, now you can play all sorts of games. You can produce a zero that looks more like eight right so if you think about it zero is this right this is also zero but at some point this begins to look like eight right and so you can do all sorts of things in between that is variational autoencoder. So this is one topic, guys, I wanted to cover. It is an important topic. Now, what is the relationship of all of this to NLP? Well, we needed to cover this topic in deep learning. It's a crucial part of deep learning. So fortunately, we got time to cover it now today.