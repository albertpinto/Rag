 So let's recap a basic fact. In machine learning, I'll write a statement and tell me if this makes sense. is important to standardize the data or normalize the data. So this is a claim which you have to tell me why is this true? Who would like to speak? I'll give an example. So suppose you are looking at elephants, like the mascot of support vectors. And you're measuring, amongst other things, the temperature of the elephant, which is your x1, the weight of the elephant, which I don't know, the, the, what should we say, the diameter of the trunk, of the trunk trunk at the end. Let's say this. And let us say you want to look at all of these things for whatever reason and build a model. You're trying to predict all things considered if your Y is healthy or not. Or maybe let's take even something different i don't know why you would consider temperature let's make it a regression problem how much food food to give this elephant you're trying to predict how much food to give this elephant what temperature might not be but i'm putting, throwing in temperature for that. So now you realize that temperature will be somewhere in the range of what? The variation of temperature will be from 90, I don't know, 95 degrees Fahrenheit. If elephant temperatures are anywhere like human temperatures, I have no idea. Or 97 degrees to what, 103 degrees? Is this the range six delta is about six degrees fahrenheit certainly true for human beings right variation of data is within a very small range what about the weight let's say that a baby elephant i have no idea how much does a baby elephant weigh but let us postulate that the baby elephant weighs 1,000 pounds and a grown-up elephant weighs 6,000 pounds. Any biologists, if you feel like correcting me, please do, right? But give or take a thousand pounds here or there. Let's say that this is right. Diameter of the trunk, let's say that this is right diameter of the trunk let us say if you measure in centimeters it may go from let's say 10 centimeter to 20 centimeters right so what happens is that when you do the prediction of how much food it is you know that there will be weights associated w1 w2 w3 even if you are making linear model right what you will find is that weight is in big numbers right so your data has a wide variation of weights but diameter and temperatures are very small variations so your loss landscape in the the, the curve, it becomes highly irregular. It becomes very ellipsoidal. You notice that it is not like nice round valley. It's not like a nice bowl, even a very sort of almost like a bowl that is more like this pinched right on its edge that is what this surface that you see and that i drew looks like you know this is like in one direction it's very very narrow in one direction, it's very, very narrow. In one direction, it looks very narrow. In another direction, it looks white. So the cross-section here is much less than the cross-section in this direction, given any contour line. Major and minor axes are widely different. In such a situation, a gradient descent becomes problematic. It's a very ill-behaved learning situation. So that's a technical reason. But forget the technical reason. Just looking at it, you can tell that, you know what? One pound change in weight will hardly make a difference. But one degree temperature difference is quite a lot, isn't it? And you can tell about humans when your temperature goes from 98 to 99 98.7 to 99.7 what happens you know you are sick isn't it but if your weight changes by one pound most of the time you don't know it till you stand on the scale and you say oops isn't it so that's how it goes so you you want all things to be on the same scale and so you normalize it typical standardization many ways of doing it the most popular is you replace each variable xi with the zi i minus the average right any data suppose it's one dimensional data point by sigma right so if you this is called z value and we have talked about it considerably in the previous courses when you standardize it most of the values will be between 3 and 3. Non-outliers will be here. Will fall between minus 3 and 3, right, generally. So it is in a much narrower range, and all the data points will get scaled to the same minus 3 to 3 range. This is one way of doing it. Another way is you can scale everything to zero and one interval. So there are many different scalars. In scikit-learn, if you remember, there are many different scalars. You can look it up where I put X, where X could be, there are many different things. There are min-max, min-max scaler, standard scaler, standard scaler being this guy, which is the most popular scalar in machine learning. So what you do is you take a neural net, let's say that you have a neural net with many layers, one layer, two layer, three layer and so forth and final last layer producing the output this produces your logits let's say but this is your x vector so x1 xn goes in here you know that input now in machine learning says input you should normalize to you should normalize right so we realize that this is what you should do you should normalize the input right now what goes in you a batch of data goes in how would you be able to find the mu and sigma you can't find that from one data set but fortunately because you sent a mini batch of data, you can compute its mu and sigma. Right. And then do it. You can pass it on. So that is why it's called batch normalization. You compute your see when you compute your mu and sigma. How did you compute it? You need enough data to compute. Where did the data come from? If you computed it across the batch. If you computed it across the batch, this is called batch normalization. Now comes the interesting part. This data is normalized. That is fine. But this is producing its activations of the layer 1. That is going into the second layer. But the same argument applies to the second layer. If these values are not properly normalized, if they are all over the place, one value has a huge variation, another value has a much smaller variation, what will happen? This will not converge properly, the second layer. Would you agree that the second layer will have the same problem that the first layer would have had? Same machine learning problem problem that if the data is not properly normalized it will be bad folks are you getting the idea it's very simple i'm saying that as far as the second layer is concerned it doesn't care it doesn't know that it is the second layer all it knows is it got an input and it's producing an output it is an independent machine learning unit am i making sense right so as a machine learning unit it would love to have normalized data and you'd be pretty annoyed if the data is not normalized and so it is a good idea to take the output and not just feed it to the next layer but to normalize it and then feed it to the next layer so that brings us to this very simple thing what happens is so far people used to this and let me use the similar picture to this i typically go left to right actually let me first do it my way so x goes in first layer what what happens in the first layer is first you do the z uh let me say z of the layer one z of the first layer and followed by activation of the first layer isn't it and that is produced isn't it guys so what happens is you do the the output of the first layer is equal to the sigma of the z1, which is equal to sigma of x dot w plus bias. I hope I'm making sense, guys. This is what comes out here. Yes. bias i hope i'm making sense guys this is what comes out here yes this is yes it's fine so this is it and now what we said is don't put pass in x remember to normalize it normalize the first layer don't take the input and just shove it in instead just say into this machine, into the hopper, you say, no, first I need to normalize it, and then the machine needs to get it, right? And the hopper will then emit out some output, and then you pass it to the next layer. And once again, remember to normalize N2, Z2, followed by activation 2, sigma 2, which will produce the activation 2, which is basically sigma of this, and so on and so forth. So what happens is the sequence of activities normalize, then linear transform linear linear linear part or people often call it the affine transform that is z is w dot x plus b and here you do the x minus sigma over mu i'm sorry x minus mu over sigma now what is mu by the way guys i forgot to mention describe to you what mu and sigma are anybody knows what those mu's and sigmas are that's like a bell curve it's the for any data set mean and standard deviation a word for mean and sigma is standard sorry i should have mentioned that standard deviation. Yeah. The word for mean and sigma is standard. Sorry, I should have mentioned that standard deviation. Mu is literally xi summation over that, over n, right? And sigma is much more complicated. Sigma squared is each xi minus mu squared summation divided by n, right? Actually, let me just put minus one for reasons that those of you who are into statistics would know. It is the fact that samples can be biased and so to get unbiased estimation of sigma it is always a convention to just cheat a little bit subtract one from it and then you get much better values but anyway we'll forget the n minus one just say averaged average of the squares is a pretty good way of looking at it especially for large numbers because when you're talking of thousand and 1000 minus 999 dividing, they give you more or less the same results. So normalize, linear transform, activate. This is the deformation stage. Non-linear deformation. right so a is equal to sigma of z right these are the three steps step one step two step three and so each layer we do we do the three steps we must remember to normalize now while intuitively it makes sense to normalize linear transform what happens is sometimes quite often people swap these two and it doesn't make a difference. Whether you do the linear transform and then normalize. So in other words, you normalize the z's or you normalize this does not make a difference. But if you do it more conventionally, makes no difference. Did I write the word conventionally correct? Conventionally, sorry. Conventionally, more conventionally. You find people write this, but I find this more intuitive. And so does the author, actually. Very nice. transform followed by normalization batch and then normalization followed by activation. So it is like this. Z goes to then you go to normalize and then you do the activation right so this is each layer right each layer it makes no difference a layer which way you do it so guys i hope once you remember the fact that normalization is a good idea you would agree that this is a no-brainer. We should normalize in each layer before feeding it to the next layer. Would you agree? Yes. It is. But, you know, sometimes obvious ideas take a while to be discovered. And it's one of those ideas that made a huge difference. Now comes a very peculiar situation. Why it makes a difference we don't quite understand. I sort of led you to believe that there is a good reason to do it. Right, and it is good reason, data in each layer should be like it should be normalized. Normalization is a very good idea. For the reasons that we mentioned but the argument is not watertight people have asked why does normalization and this author calls it the pixie dust and actually that's that's the way it was when normalization came out suddenly all the models you normalize, they would give you a better accuracy. You add the normalization step, they give you a better accuracy, better predictive power regression or whatever it is. So actually, mathematically, rigorously, we don't know why it works. Because we can show that normalization plus the linear step is nothing but another linear step right they're both affine transformations the word i once told you about affine transformation right you rotate and shift and when you do this normalization plus the rotate and shift of the w it is just another w w prime on the data so why it works the only non-linearity comes from the activation so why it works in improving all the models is a little bit of a mystery but there are two variations of the normalization one is normalization of how do you compute your mu sigma so it needs a data it needs a data set right remember these are statistic over data so what data when the data is the mini batch itself, which makes logical sense, then it is called batch normalization. works very well with our dense layers or fully connected layers fully connected neural networks or layers layers and also works with a architecture which you will learn in the in the coming weeks is called convnets, convolational networks, which we'll learn about soon. It doesn't work so well with recurrent, not so well with recurrent. Now, what exactly is that recurrent neural net architecture? We'll learn about it in the coming weeks. But suppose when the data A, B, when instead you normalize over in a different way, what you do is when the data is the activations or outputs of a layer, it is called layer normalization. Now, what do we mean by that? See, suppose you have each of this is A1, A, N. These are the activations from the given a layer l, suppose these are the activations coming out of it. So suppose you compute mu mean as a one to a n over n. So what are you averaging over? Not on the mini batch, we are averaging over the features itself the features produced by the this layer and likewise for the sigma from this that is called a layer normalization not so commonly used for most situations batch normalization is very popular but for certain architectures like uh like recurrent neural network, etc., the better choice or the choice that works is the layer normalization. But that is the idea of normalization. Straightforward idea. I hope after the long journey of gradient descent, you would agree that this was common sense almost, in hindsight, except that many people like this author beautifully calls it the pixie dust. Normalization is one of those lovely things. It's like pixie dust. You sprinkle all over your model and suddenly your model glows. It does better. What a common sense idea. A common sense idea whose reason why it works, why this works as pixie dust. We don't quite understand, but it does. Heuristically, the idea is clear. We should always give normalized data to each layer. But harder to prove rigorously that that's the reason. So guys, do you understand normalization? I hope this is a simple idea. Going back to something we know, all we are saying is in neural networks, remember to normalize before each layer. Any questions, guys? Anybody who didn't understand, wants me to repeat something? Guys, there's dead silence from which I surmise everybody is asleep by now. No, made sense. made sense. Good. Anyone else still awake? Yeah, we can count on KL. Kate, is there anyone besides the TAs, is there anyone still awake? Yes. All right guys, so I'll end today, like last 15 minutes we keep for QA, so we are on time. It is 9.45, actually 9.50, Let's give the last 10 minutes to QA. So please ask any question you want on what we learned today. We learned three things. We learned normalization, we learned momentum, and we learned a sort of an intuitive reasoning of behind the loss function for the classifier, the cross-entropy loss for the classifier. So classifier loss is just a measure of unpleasant surprise, right? cross entropy loss for the classifier. So classifier loss is just a measure of unpleasant surprise, right? How unpleasantly did your model surprise when you know the answers?