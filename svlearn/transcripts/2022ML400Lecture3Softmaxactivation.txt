 So for those of you who join later, this is a very good book that has just come out as in like this week it has come out. I'm told as someone just mentioned it is already out of stock. Good books are like that. They become out of stock on day one. But the e-version of the book is still available for sale. You can go to Manning website and purchase it. Now, every book, when it comes out, I like to read it cover to cover. Given how busy I've been with the packed day, I've still managed to read half the book. And it's been a pure joy to read it. It's a lot of new things. to read it. It's a lot of new things. To my knowledge, this is the only book at this moment that treats the topic that I'm going to cover, the topics today, properly, in depth, and yet in a very accessible way. In fact, very similar to the way I would teach, except that with the sort of graphics that it would take me a lot of time to make or draw on a whiteboard. So what I will do today is I'll use a mix of my drawing this blackboard and projecting this book itself. Now, before I continue any further, the virtues of this books are, and this you can see with both Manning and O'Reilly publishers, they have begun to introduce color illustrations in their book, which makes it very, very nice. It is no more gray, you don't have to infer things. It's all in great color and makes for very pleasant reading. Now, for those of you who are willing to invest in this book and i suppose with all sorts of discount look for manning coupons there are many many coupons on the internet with all the coupons and if you just get the ebook i suppose it will put you down by maybe 15 20 dollars it's worth it do please get this book if you can right now the reading from this book that I would recommend you is each. So this go and do the first chapter, the second chapter, and the fifth chapter. Fifth chapter is what we are going to do today. We'll deliberately skip over convolutionals and recurrent neural nets because I do fundamentals before I do neural architectures. We will have time enough for doing the neural architectures. At this moment, we are still building out our fundamentals, we're building our muscle before we start running the marathon. So please do go read chapter one, chapter two and chapter five. That's your reading material. Now, having said that, now let me come back and talk about things. When we ended last time, if you remember, we talked about gradient descent. It is the learning of machine learning. Do we remember that? It is the inner loop. When we say we will iterate over the data and learn from it, what do we do? We take, we are doing, like in particular in when you talk of a neural network, but more broadly when you talk of any machine learning algorithm of the you you have a loss function the loss function quantifies how wrong you are right roughly speaking it quantifies how badly you're doing so by minimizing the loss function you function, you learn. Learning is literally any movement or any reduction in the loss function is the definition of learning, isn't it? And so long as that helps you make better predictions in terms of your test data or real world data, you're doing well. Now, there is an interesting sort of a disconnect. You may say that this loss function is rather abstract and mathematical. For example, for learning, for regression, it is prediction y hat minus y squared over all the data sets and perhaps averaged one over n. You call it the mean squared error. Now, while you say that, in reality, you don't just do that. You add all sorts of regularization terms, L1, L2, and so forth. And you do other regularizations, like, for example, in neural architecture, we'll deal with it, dropouts, early stops and whatnot. That is a whole topic in itself for another day. But let's take in its simplistic form, even though it isn't exactly what you do, but let's say y hat minus y squared. That is abstract. Yes, you can minimize that. But how do you know that minimizing that is actually leading to very good predictions? So for regression, the gap is small. If you can reduce prediction minus label squared, then you're doing pretty good. It is not so when you're talking about classifiers. Because the classifier loss function, if you remember your previous machine learning courses, what is a good classifier loss function? Anyone of you remember? Remember, it comes from the maximum likelihood estimator argument, like log loss of what? Okay, we've forgotten that, which means that next time I come back from my trip, log loss of what? Okay. We've forgotten that, which means that next time I come back from my trip, I would like to give a Sunday session devoted to reviewing all the loss functions because we won't get time in the class. Let's devote the next Sunday. This Sunday, I'll be absent, of course. I'm going to Seattle. But the Sunday after that, I would like to devote to the various kinds of loss functions. Kyle, please make a note and remind me of that. Instead of doing a paper reading, we'll do loss function review. Sure. Sure. The loss function for classification is the most common loss function use. I wouldn't say the loss function, but a commonly used loss function is the cross entropy loss function. Cross entropy is a divergence between what you expect and what you got. So, for example, if you expected, let's say that you're trying to determine whether it's a cow or a duck, of a classic example that we have been carrying through this course. If it is a cow, it's a one. If it is a cow, it's a one. If it is a duck, it's a zero. Let's say that you came up with a logistic regression model or any model that says the probability that this object that you're looking at is a cow is, let us say, 0.7, right? And you happen to know that it was a cow you would say well you know it's doing pretty good because it's more than half the probability it's still a cow but you can measure the error in many ways you can measure the error by saying okay tell me don't tell probabilities tell exactly is it a cow or a duck so you put a threshold above a threshold it's a cow below a threshold it's a duck common by default you would take 50 but then that's not necessarily true for example if you're You put a threshold, above a threshold, it's a cow, below a threshold, it's a duck. By default, you would take 50%, but then that's not necessarily true. For example, if you're detecting cancer, even 10% probability that somebody may have cancer, you will take it very seriously and mark that person positive for further tests, subsequent tests and so forth. So where you put the threshold, it depends upon the context that you're dealing with. So this is all territory that we have covered. But irrespective of where you put the threshold, let us say you put the threshold. So now probabilities above a threshold are one, probabilities below a threshold are zero. So in that case, 0.7 probability makes it a car. Now all you have to do is count the number of errors how often you had false positives means you marked a duck as a cow and how often you had a false negative how often your machine your model marked a cow as a duck right cow being the positive case it's a false negative it's not a duck it's a cow so those are the two types of errors you would get into obviously the non-error the good situation would be how often did you identify a cow as a cow and how often did you identify a duck as a duck are we together so the fundamental data that you deal with is the y hat the probability of one of the two classes or one of the many classes and the label the actual label and if you look at if you want to create a confusion matrix of what it is you also need the threshold at what threshold you're calling it the positive versus the other classes and so forth now the generalization of that is softmax. Today, this is how I will start. Softmax is a topic I've missed. So I'm going to talk about softmax in the beginning. And then I will go back to the fundamentals and talk about the gradient descent all over again. But before I do that, in the gradient descent, there is one thing that we haven't talked. Suppose you just want to make less number of mistakes. So now that is very practical. That is something you can see. That's empirical result of your model, right, of its goodness. On the other hand, a cross-entropy loss feels a lot more abstract. Number of mistakes, false positive and false negative, you can explain to anyone, right, with a modicum of education. But to explain cross entropy, you probably have to drag them to a machine learning class or a statistics class to explain what cross entropy is much harder. So the thing is, what are the machines when the learning takes place when you do the gradient descent, the question is while the gradient descent is over the loss function, what are you trying to optimize? What is your score or metric? And that matters. Now there is a way to specify, even though you're minimizing the loss, there is a what happens is that after a certain time, you sort of start having perturbations around baseline loss level. And there are considerations of how you can sort of focus on optimizing one of the measures, for example, accuracy is a measure. Accuracy is exactly this the converse of this is error rate you can choose to focus on this now those of you who are familiar with the classifiers know that other metrics are the precision the recall the f1 score which is the harmonic mean of precision and recall and so on and so forth so you can choose which metric ultimately you want the learner to learning to focus on and give you good results on fortunately pytorch gives you excellent support for specifying your metric so at each each iteration it will now what are the iterations remember there's an outer for loop of epochs there is a enough enough for loop of epochs there is a enough in a for loop of in a for loop is of many batches excellent many batches and inside that are are the four crucial lines first thing is forward pass to make the prediction you get y hat now that you have y hat and y and the actual labels available to you during training, you can compute the loss. That is step number two. You compute the loss. The third step is you compute the gradients of the loss and you back propagate it in a neural network so that all the gradients get computed throughout the network. And the fourth step is the actual gradient descent step. Those stay in between. So the topic of today, crucial topic of today that I'll do, after doing softmax, I apologize, softmax is something I wanted to cover. It's a small topic, but we have missed it. I'll cover that first. So today, after softmax, we are going to focus on the last step, which is the gradient descent step. And we are going to realize that there are many subtleties involved in gradient descent. If we use it the naive way that we learned so far, it leads to a host of problems. Those problems tend not to happen when the error surface is convex, remember, like a bowl, the lost surface. But neural networks are inherently non-convex surfaces. We saw that in the visualizations, we saw how many local minimas they have. In fact, lost landscape of neural networks are infested with local minima everywhere. Do you guys remember the visualizations and the website, lost landscape? I hope you all took some time to explore that. So that is important to remember that neural networks are special. They have a lot of complications coming from the fact that the main loss landscape, the loss surface is highly, highly non-linear and non-convex. So then comes the question, as we saw, if you remember, do you guys remember that we used to get trapped in local minimas when we started at different points? How many of you remember that? Yes, sir. We used to get trapped into local minimas so today one of the questions we will ask is the central question we'll ask is how not to get trapped into local minimum that is one problem that we'll try to solve so let me start writing those down please give me a moment i should i guess it's time for me to writing those down. Please give me a moment. I guess it's time for me to do that. Guys, if you did not attend the session on Sunday, please go attend that. There is one sentence that I said in absent-mindedness, which I would like to correct. I don't know if I did say it wrong or right, but in any case, I drew it correctly, but I may not have set it correctly because I was looking at Kyle's notes, but I noticed that I did not make it very clear. See, when you draw a plane, this is a plane, isn't it? A plane is flat. The surface of a cylinder is also actually flat locally. Now, what happens is that if you identify the opposite parallel lines in the plane like this edge and this edge you identify means you wrap around it becomes a cylinder and then you you identify this and this and it becomes a torus. So a plane with the edges identified is homeomorphic to a torus, not just a plane in itself. The plane, of course, becomes, but you have to remember that when you take a torus to reproduce the plane, you have to cut the torus twice. One cut of the torus, let's say here, will make it homeomorphic to a cylinder, and another tear along the cylinder will open it up as a plane. So what is missing is the identifications. You have to remember that. That's why I made those arrows there. But I think in my absent-mindedness, I may have implied that plane is homeomorphic to a torus. No, plane with identification is homeomorphic to a torus. So you identify opposite edges, one set of edges, it becomes a cylinder, isn't it? You can do that experiment and see, just take a sheet of paper, roll it, becomes a cylinder. Then if that sheet of paper was flexible, well not paper because it has to be a rubber band. That's why topology is also called rubber band mathematics. You turn it around and it becomes a torus. So that is one little thing I would like to emphasize. But yes, and a torus, a doughnut is completely homeomorphic to a cup. So which is why the joke is, topologists eat the cup and sip from the donut. I said, what was it plain with identified edges? Actually, are you guys able to see me? See. Yes. Take this. Let me do one. Are you guys able to see me? You are, right? See? Yes. Take this. Let me move. Do you guys see this sheet of paper? Yes. Right? It is a sheet of paper. Now, suppose I say that this edge is the same as this edge. Identify the two edges. What it means is bring them together and in practical terms, put a tape along the edge so that these two edges disappear. They become one. If they become one, what is the surface you got? What is the shape you got? You got a cylinder, isn't it? Are you looking at a cylinder? Yes. You at a cylinder yes right you got a cylinder so now if also you identify this edge with this also so what it means is when you fold it once you have a circle at the top and the circle at the bottom right and if this circles you have to identify, now what does this look? Well, if you inflate air into it, it will begin to look like it is a torus. Right? Does it look like a torus now? Sort of. A is a torus. But to go from a torus to a plane, you'll have to sort of de-identify. To cut it here, it will become this, and you have to cut it along the cylinder, and you'll get the plane. Which is why people, the way mathematicians say it, you get used to a language. People just put two arrows to signify you have identified ad and bc together means you have made ad and bc touch each other and then you have made a b and cd touch each other right and when you do that it begins to be but there is something else you can do which is quite funny actually let me show them do you guys know mobius strip right mobius strip is is a very interesting surface so i'll show you what i mean look at this if i identify a b and c d it begins it becomes like this a cylinder isn't it a thin cylinder short cylinder but it is a cylinder does this make sense yeah so the cylinder has an inside surface and an outside surface for example when you buy a canned food you have absolutely no ambiguity which side is outside and which side is inside the inside is touching the food the outside is filled with advertisements glorifying that food right but there is a weird and this is just for fun i'm telling you right so little you know this thing the way mathematicians play this game is they'll take the same plane c d and let me just say homeomorphic above identity yeah see i wrote it correctly but i mean i don't know why uh so kyle i'm just not looking at your notes so fortunately in the lecture or the day before i've written it correctly so maybe okay there's so much so look at this suppose i do this a b c d okay c d c d but suppose i do this weird identification this side i identify to the opposite direction do you see the difference between these two look at b c which way the arrow is pointing suppose i do reverse identification what it means is here a to d is going like this b to c is going like this but i'm turning it other way around what it means is i rotate it and then this i give it one rotation and well This, I give it one rotation and well, then I identify it, I hope. If I identify it, do you see this shape that I get? What is the shape? It's called a Mobius strip and it has a very interesting quality to it. It actually has only one side to it there's no two sides right you could either if you keep on going on it you'll never come you will you'll have to make two circles to come back to the same plate right because that's a very peculiar thing so anyway these are games that you play as a, when you are a topologist. So anyway, I was just going to mention that. Is my screen still shared? Yes. Okay. So yes, I had written it, right? But for some reason it didn't come through in the notes. Those of you who didn't attend the Sunday session, please do attend. The purpose, the summary of the Sunday session was why is it that we need a non-linear activation function? What does it do? And how do you get an intuition of how a neural network is deforming, topologically deforming the input feature space to another feature space where the decision boundary is straight? It's a plane, right? So we covered a lot of very interesting territory. Again, it was optional. But for those of you who are interested in all these beautiful visualizations on why neural networks work at all i invite you to go watch that watch that video and read ola's paper blog which is very very interesting so today the topics for today are quite interesting interesting. We are going to do two things. A short topic is softmax. The second topic is gradient descent, the second incarnation. Let's look at the second incarnation. By the way, take it in a humorous mood. Don't get too into it. All right, what is a softmax? This is a big topic. We'll keep it. See, when you are trying to predict so okay before we get there suppose imagine that you have an array of light bulbs on a street street lamps and all those lamps they are glowing at different intensities And all those lamps, they are glowing at different intensities. And you want to tell somebody, well, those times are gone. But when I was young, well, I claim I'm still young, but when I was much, much younger, right, I obviously coming from third world sometimes i had to study and what would happen is the like when i grew up it wasn't electricity was a scarce commodity so the street lights would be glowing but the electricity available to residential buildings would be not so available. So what we used to do is we would like to chat or sit down, have fun with friends or perchance read a book. It could be a storybook, it could be an actual school book because we had to do homework or something. So what we would do is we would look at all the lamps and try to figure out which is glowing the brightest right and then we would gather around it so for whatever purpose fun or serious purpose now the trouble is suppose each of these is glowing flowing. At number two in lumens, let's say this is 20 lumen, this is 50 lumen, this is 75 lumen, right? And so on and so forth. And this is 10 lumen, 12 lumen and so forth. And let's say 82 lumen, something like this. When you look at this, you may want to say, all right, this is good. How do I make, this is a series of numbers. You can eyeball it and you can ask, which is the biggest one of them? So you may say, this is big, isn't it? It's big so for positive numbers like lumen this is this is nice but suppose you took a quantity that could also accept negative values let's say that i'm not looking at lumens and fog right or some other factor so let's say that so fog right or some other factor so let's say that so how desirable a lamp is it could even have negative values minus 15. i'm obviously concocting an example minus 16. what's that imagine what it means some of the belts like har like Harry Potter's world would actually absorb all the light right so let's say this is minus 33 and you're looking at all of this and you're saying okay which is the lamp how can I quickly quickly figure out which one it is which of them has the highest proportion of desirability if I want to do this. So there is an interesting fact. You could do this. You could say, I can just take the total and divide everything by the total. So then this one will stand out. It will be the biggest ratio. It doesn't quite stand out because it compared to let's say 75 it doesn't glow so much brighter but you really wanted to want to exaggerate the effect a little bit. So here is a trick that people use to exaggerate the effect. First of all if you're comparing to simplify matters instead of negative and positive what you could is, if you want to convert all of these into positive quantities, you could do it by taking the absolute value, but is there anything else that you can do that will make it into positive quantities? Square. You could square it. Yes, that is one thing. You could square it. That is a valid answer. But there is something that people find for a variety of reasons that we'll come to in a moment, a little bit more intuitive, and you'll see why. What people do is, this also works. While squaring works, suppose I do e to the x, so that this becomes e to the 20, e to the minus 16, e to the 50, e to the minus 33, and so forth, e to the 82, so forth. Are we together? Now what will that do? Will it make all the numbers positive? What is e to the minus 16? Or instead of e, think 10 to the minus 16. What would 10 to the minus 16 or instead of we think 10 to the minus 16 what would 10 to the minus 16 be it will be a tiny number right it is what one trillion trillion billion billion trillion or something like that so it would be a vanishingly small number so this would this would disappear all the negative numbers would start looking like close to zero right and all the positive numbers they would start blowing up so this is a way like it's a it's it's in a way you can almost think it think of it as a form of soft filtering because it will so significantly dampen down the values you don't want, the negative values, that you don't, it is easy to ignore them, right? Now, look at this, look at two comparables, e to the 75 and e to the 82. What is the difference between them? The difference is e to the 5, right? e to the 7, it is e to the 70, sorry, e to the 7, is e to the 70 sorry e to the 7 75 oh is that correct 5 plus 2 yeah yeah would you agree e to the 82 is this yes what is e square it's about 10. so e to the 7 is thousand three thousand isn't it so what it means is i i hope i'm getting my math right e to the two is approximately three thousand give or take tell me if this is right or wrong guys because my brain is fried after the whole days of work but okay the point i hope you get the point one of you please quickly do the calculator e to the seven i postulate that it is approximately um about two to three thousand uh could you do that on your dennis could you please do it quickly it's thousand around thousand one thousand one thousand thousand ninety six thousand ninety six okay so i was off by a factor of three. Apologize. So this is it. So e to the 82 is 1000 times 75. Do you realize that even though 75 is not that far from 82, but when you exponentiate it, it really begins to shine far more brightly. Right? You could have made that argument for square also, but you will see why we didn't pick square. Square actually makes the negative ones glow brighter, right? Yes, you got it. Absolutely. The trouble with square is negative will also glow brightly. Right? You don't want that. Wonderful answer. So now it has all the good properties negative ones will disappear small ones will disappear and the really big ones are the dominant you know brightly glowing bulbs now right so you say well that is lovely so we should take e to the xi. And now what happens is these are big numbers. These are terribly big numbers. What can we do now? What you do is you sum up, find the total of e to the xi over all the values of i, right? Sum. And you make that the denominator for each of the values so let me just call this the value let me create a new variable which i will call what should we call this variable z i right so you can say z i is actually this by s e to the x i by s e to the xi by h. Are we together? Now, what is the quality of this? Any value of zi will be, you notice that it has a lovely quality to it. It belongs to the interval. Convince yourself that this is true. Can you argue and convince yourself? Anyone can give me a reason why this is true it will belong in the zero to one interval why will that be true see imagine some quantity is uh like literally zero or very negative. So what is the smallest value it can achieve? Remember, we talked about approximately zero, right? Zeros. So zero over some will still be zero. Now, what is the maximum value you can have? Suppose all the bulbs are zero and there is only one bulb with whatever value. Let's say value T sitting there, and all of them are zeros, more or less, approximately zeros. So what would be the sum? Sum would be approximately equal to t, isn't it? Sum of all of these values would be t. And so this is the e to the t. So this is the T is equal to E to the, well, let me just mark it. Zi is E to the Xi. It's the only one that is glowing. All others are off. All the other ones are off. So would you agree that the sum would be approximately just Zi itself? Well, I shouldn't use the word Zi because I use Zi for this. I'm making an enormous mess. Okay. So let me just use the word z i because I use z i for this I'm making an enormous mess okay so let me just use the word w i w i no z i is this and w i by definition is just e to the x i exponentiator this is our definition so this is not this I hope I've after confusing you enough I I got my syntax right right so you see that when you exponentiate the numbers if all others are small they will look like zeros and what would the sum be like the sum would be like wi approximately is this true or not Right? Now, z i by definition is what? z i by definition is w i over s. Right? So, it will be w i over s, which is equal to w i over approximately w i itself. Right? So, this w i, just to clarify, is not the weights it's no it's the exponentiation of the weight this one exponentiation of the weights okay don't think of it as weight oh by the way forget weights i'm just talking about numbers we're talking remember like okay there are no weights around all right yeah so you could that. So now this looks like a lovely function because what would happen is that you would get all these light bulbs and associated with all of them is a ZI, i-th light bulb. the the bulbs that are not glowing well or negatively glowing they all disappear and if we could have this exponentiation function with the scaling denominator some denominator there's xi if you were looking at and let's say that the intensity of the bulb through some magical glasses would be zi what would happen immediately the brightest lamppost would be glowing bright and all of this would be subdued or they would be completely muted is it making sense to you guys right in this very crazy analogy is it making sense so what it does is that it can it sort of exaggerates the biggest elements right not only that there is another lovely quality to it if it is built if it is between zero and one and the other property it has is summation of all the zis is equal to one again by definition isn't it now what is it that typically what is something common that you know which falls in the zero to one interval and when you look at all possible outcomes adds up to one probabilities probabilities exactly right probabilities they have this property they have additivity property which again these lights will have this they have they're between zero and one and this add up to one, right? So what you have recovered is, or you have the journey that you have done is when you go from some numbers, xi, xn, somewhere in there is xi. From there, you come to the wi's. W1, wi is equal to exponentiation of x i w n equal to exponentiation of x n is equal to exponentiation of x 1 and then continues when you do this and from this you divide it by you get to z 1 right which is equal to e to the x1 over s, continue on, zi is equal to e to the xi over s, s being the total, so forth. Zis are probabilities. They look like probabilities. Have the nature of probabilities. Behave, Z i's behave like probabilities. So it's a fun fact, lovely fun fact. What does it have to do Lovely fun fact. What does it have to do with us? So let's see what it can do with us. Suppose, now we will bring this to neural networks. Suppose you want to determine whether a picture is a cow, a duck, or what else can we have? A platypus, right? Three animals, a platypus, a cow, or a duck. Now they all look fairly differently. So you would do what? You would create a neural network. You would give it an image, which you'll convert somehow into a X vector. You would have layers after layers. You can pick your choice, how many layers you want. But at the end of it, suppose you had three layers, just three neurons in the final layer. So what would happen? This is the Lth layer, the last layer, Lth layer. And you can pick Ls, 5, 10, 100, whatever it is. It will produce its own activation. This is one, two, three. A1, this will produce this activation A2, and this would produce this activation A3 in the last layer. Now comes this question. These things are often called the logits. It's a term that people use very much in the industry, logits. Logits are the activations emerging from the last layer or penultimate layers. So let's think last layer. Right. What do we want to do? If we want to classify this picture as a cow, a duck or a platypus, we would hope that these activations, one of them is way bigger than the other, isn't it? Would you hope for that? Right? You would hope for that. Or especially, and what you would do is, you would want to know which is the highest activation, and somehow you would like to convert them into probabilities. Because suppose I say, I reserve the first one for cow, second one for duck, and the last one for platypus, right? Then I would like, I would say that, okay, whichever, I will train the neural network in such a way that whichever activation is the highest activation, that would be the animal. But there is a catch to it what if the numbers are very closely matched right you may also want to know what is the probability that it was a cow it was a duck it was a platypus so how do you go from those how how brightly is the activation or the light burning from those neurons to probability. And we just did the journey a little while ago. So what you would do is you would feed these into our so-called softmax. You would feed it into a softmax unit what will that do it will convert each of these into a probability of a cow probability this one will become probability of a cow probability of a duck probability of a platypus guys i hope you see this it's very simple think whatever numbers come out you can convert them into probabilities if you can convert them into probabilities. If you can convert them into probabilities, you can do cross entropy. And so you can now train the neural network to minimize the cross entropy loss. You can keep showing it pictures of cow and make sure that this one burns the brightest. You want the first one burns the brightest. You can show pictures of the duck and you train it to want the first one burns the brightest you can show pictures of the duck and you for you train it to make the second one burn brightest and then you feed it the platypus pictures and you train it to make the third light bulb or the third neuron burn brightly is that making sense guys yes so that is what a softmax does. Now, softmax, to summarize, the softmax of a set of values, given an array of numbers, x1, xi, to xn, softmax is. And if I were to treat this as a vector soft max of x let me just use i y b a soft max of i is equal to e to the xi over summation of all possible values j e to the x j means all possible this is the sum of course summation over this right we went through the journey this is called the soft max now you say well that is lovely but i don't want probabilities what do you want at the end of your training suppose your metric is accuracy you need to know which of the probabilities is the highest because you're going to mark that to be the real animal. Let's say that the probability of a duck is like 45%. The probability of a platypus is 25% and the probability of a cow is 20%. What animal would you like to call it? The duck. Yeah. Whichever is the highest, you would like to call it that animal. So there is, so, and here is the abuse of term. What you want to say is the class, the actual target class, whether it is a cow, duck, platypus, that answer is the argmax i, argmax i of the softmax function. It's a very peculiar argmax . What does it mean? Mathematicians often use the symbol. The reason I'm saying I could have easily avoided the symbol, but as you make progress, you will encounter textbooks and research papers that use this notation. It's actually not hard. It is saying, so suppose cow is one, duck duck is two platypus is three it basically saying which is the value of i which maximizes this right so suppose a p1 is equal to a 45 p2 is equal to what did we take? 25 P3 is equal to what did we say? 20. Right? There's something wrong here. 35. That makes more sense, isn't it? It adds up to percent. So does P1, P2, P3 add up to 100%? Yes. If you look at this probabilities it is saying which value of i is the highest probability it gives you the highest softmax which of them is it i is one isn't it look at this i is equal to one is 45 percent does it make sense guys as simple as that you just inspect it and see ah the index i is equal to one gives you the highest probability highest softmax probability isn't it Are we together? Yes. And so when you are making a classification, you say that the class is class I that it belongs to. Is this because it maximizes the, this I maximizes the softmax function value, right? It's corresponding softmax function value. All right, it's corresponding softmax function value. Nothing magical. The only reason I'm mentioning it is, so you become familiar with it. So a lot of people, you know, they get very annoyed with this argmax, softmax, and different books, they use different terminology. And before you know it, you're running around in circles. You can't make out what was this guy saying and what was this guy saying. It's a little bit of a confusion and a mess. So I wanted you to understand what is argmin, argmax. These things keep coming in. Now, just in case you're finding this mysterious, let me tell you what is it that you have learned. You know, you say that suppose your neural network is made up of all these parameters, the weights. So you can have a weight vector, isn't it? Now I claim that your solution is always in this new language. language, the new language, the optimal solution of an optimal model is arc min w for the loss function w, right? See, your loss function is made up of y hat. Y hat is a function of w, hat, y hat is a function of w, isn't it? It's a function of w and x, right? And loss is made up of somehow y hat y, maybe y hat y minus square. But ultimately, what are you doing? You're finding that w star, w star is equal to the arc Min, that minimum point in the parameter space, that optimal point in the parameter space, that minimizes the loss, that is the minima of the loss. That is common sense, isn't it? That is what we have been searching for all this time. So far so good, guys. I hope i'm not saying something mysterious let me repeat what i said all the time we have been searching for that perfect value of the parameters that minimizes the loss right that is our model suppose you have an equation of a line, y is equal to beta naught plus beta 1x. So here, what is the parameter vector? Beta naught, beta 1. This is your beta vector is equal to this. And what is your loss? Loss is y hat. So y hat is beta naught plus beta 1. hat so y hat is beta naught plus beta one so y hat minus y this square right and obviously there's a summation here which i'm glossing over summation over all instances and so forth xi yi and then over n and if you want to normalize it then it becomes mean square this is your loss you're familiar with this and so in the language that we just learned about arg max argument you would say you're looking at the beta star you're searching for those optimal value of beta star that minimizes so our gradient descent that minimizes that value w of the w vector that minimizes what or in this particular case beta because we are using beta for the notation for straight line right so guys this is common sense reformulated into a little bit more formal language because this is so this to mathematicians a little bit more formal language, because this is so this to mathematicians feels very succinct. They will look at it and say, ah, so what we are doing is we are we are somehow trying to find the best beta that will minimize the loss, which is exactly what we have been doing are the best parameters that will minimize the loss. But what we say in so many words, in a mathematical notation, once you get familiar with it, this is a very succinct way. These are very succinct ways of saying that, right? In the same way, when you're classifying and you have produced the logics, so the journey is your neural network will produce the logics for cow, duck, platypus, etc. And these are the logics. You'll feed step is the argmax. Which of these is the maximum? It just picks this. It says this guy. So this is your answer. And so you predict that your class is the picture, the animal that you're looking at is cow. Do you see this intellectual journey, guys? This is how you go. And that is, I introduce you to new jargon, logit, softmax and argmax argument, which are mathematical terms. And softmax is also a function. To recap, softmax function is defined as this thing. So I give you a homework, go read more about softmax, make yourself familiar and make yourself familiar with this language of argmax, argmin. It's a bit of a mathematical thing. They're YouTube videos that sort of slowly explain the meaning of argmax argmin etc makes it more intuitive. Repetition makes it makes for familiar. There's no depth to it. It's all it's just a way of saying it, right? But it's an unusual way of saying it unless you have been familiar with it so familiarize yourself with it because you'll see i will try to avoid it in this course but in the subsequent courses as we go and do papers in various topics computer vision fraud detection etc you will see that they keep talking about art max argument softmax etc these words are pretty pervasive in our machine learning So, these are the two things that we can do. So, these are the two things that we can do. So, these are the two things that we can do. So, these are the two things that we can do. So, these are the two things that we can do. So, these are the two things that we can do. So, these are the two things that we can do. So, these are the two things that we can do. So, these are the two things that we can do. weird things to mathematicians it looks obvious or to mathematically literate people not just mathematicians people once they become familiar with it it looks natural but first time you encounter it at least my own thing is first time i encountered it i was like what exactly is this right and i sniffed around it very carefully for half an hour before i convinced myself that i actually understood it right so if you're having a similar experience speak up is it clear guys yes okay good so then give me two minutes of a break please stop the recording