 Welcome to the Tuesday evening session for the comprehensive introduction to data science. Since this session is only an hour long, we will focus on one topic. The topic that I'm going to talk about is a very non-intuitive idea. Popularly, we say it's Monte Carlo, but I would like to broaden beyond that. It's presented like that see when we see randomness in nature we see complete randomness you don't expect it to have information right you you think that that is the that is randomness is quintessentially the opposite of information so you don't have any anything there. One of the non-intuitive results that came out and very startling discovery is that you could use randomness actually to solve many compute the value of pi to arbitrary precision simply by using randomness. And how could that be possible? We're going to explore that. Then the next thing we'll look at is the problem in calculus. When we do calculus, traditionally in elementary calculus, the sort of calculus we all learned in high school or in early college, you do your limits and the mean value theorems and this and that. And after that, you get into differentiation. You do a lot of differentiations of functions and learn a whole bag of tricks on how to differentiate functions. And geometrically, if you remember, we said to take the derivative is to look at the gradient of a function in a simple one dimension. It's looking at the tangent or the slope. In a simple one dimension, it's looking at the tangent or the slope. In higher dimension, we are looking at, again, the gradient, which is a generalization of the slope to higher dimensions. So you can do that. And we know that any function that can be expressed analytically, we can take its derivative. With all the back tricks we are able to do that, most functions we can take derivatives. But then it turns out the opposite of derivative, the entire derivation, is something called integration. And the integration itself has nuances to it. Traditionally, if you look into real analysis or study it as a mathematician, there is the Leibniz integral and then there's the Riemann integral and so on and so forth. The most common integral that we all use, which we think of as area under the curve. If you have a curve, you look in one dimension, right? Input is just one variable and response of Y is another variable. So then we think of integral as the area under the curve from X value A to B. So that's the way we think. And we'll do this in a little bit in the writing board but that's a basic intuition of integral. So for example if the velocity of a car, you're driving a car at a steady pace on the highway speed you know and your speed keeps changing with time you know that the total distance that you cover in some sense is obviously not just some speed times distance because your speed has been changing. So what you have to do is you have to break your time interval into small bits and in those small bits the speed at which you were driving the car can be considered to the first approximation as constant right and if it is constant the speed times the amount the duration the tiny duration gives you the distance traveled and so you could accumulate all these distances by discretizing the time into small durations or small intervals and then finding out what speed you were driving the car at at that given point in time and then the and this is a very common intuition you can apply and we'll do that in the writing board in a moment and that gives you the integral now when you do integrals in college, in high school, it is traditional, it is quite common for us to have the impression that since we can do differentiation of all functions, integral of all functions being the opposite of all, it is also possible. Actually, that is not so. One of the surprising things about calculus is there is this fundamental asymmetry. You can do the derivation of functions. However, when it comes to doing the integration of functions, it turns out that analytically we know how to do the integral of only a handful of functions. Most of those functions that we can actually integrate, they show up in your elementary calculus textbooks as exercises that do the integral of this into the integral of that. And there are methods like, you know, integration by parts and so on and so forth, and variable substitutions and so forth and most of these tricks I mean they help you do the integrals that we all know but it turns out that the vast class of functions that exist especially that exist in real world you cannot find the integral of very surprising thing to when people hear about it for the first time, they are saying no, what that can't be true. But it is true, actually, we can't do the integral in a closed analytical form. In other words, you can't take a paper and pencil and figure out the integral, even very common things like the bell curve. Right? if i tell you to find the integral from let's say minus infinity to some value x or a or whatever it is if i ask you to find the integral of the bell curve in a certain interval it turns out you can't do that and a bell curve if you think about it is at the foundation of machine learning. Just about everywhere we look, we have a mixture of Gaussian's, if you remember. They're everywhere. In nature, perhaps the normal distribution, multivariate normal distribution, as we have become familiar with it, is by far the most common function or at least the approximation to real life that we take most of the time. In support vector machines, for example, when you take your kernel to be a Gaussian, what you're saying is your lighthouses are, they shine light like a Gaussian, and works for us it gives pretty effective results we all know from experience that generally Gaussian kernels tend to outperform linear and polynomial kernels or Laplacian kernels for most situations simply because they are and the word that people use is bell curves or gaussians, are universal approximators. In other words, any arbitrary function can be expanded in the basis of gaussians. And there are many other things, for example, basis functions, other examples, of course, you can expand any function in terms of trigonometric functions. That is, of course, your Fourier series. You can get a Fourier series expansion. And there are many different polynomials. If you know your chemistry and you look at the atomic orbital clouds, the SPDFs, what are those SPDFs? Those are modes of a spherical Bessel's harmonics. They are essentially the discretized modes of that. And so all of these transcendental functions that you see, they are very, very hard to integrate. We don't know how to do them quite often, right? So people have created numerical techniques to do those integrals. And in machine learning in particular, it is very relevant in a form of machine learning that is called Bayesian machine learning. So I'll give a very quick introduction to Bayesian machine learning, not too much, it's a short talk today. We will do it in greater detail when you take the math of data science course, but I will give you enough of a motivating example for you to realize that we need to take these integrals quite seriously. for you to realize that we need to take these integrals quite seriously. But that we'll do a little bit later. First, we'll assume that, yes, it is at the core of machine learning, especially the Bayesian approach to machine learning, that we should be able to integrate certain functions. Then the question is, what are the many ways that we can do it numerically? There are quite a few. It turns out that when the functions become terribly complex, there is one method which uses randomness. And that is the method we'll focus on. I'll take a very simple example, and I'll show how you can compute, for example, the value of pi just through randomness. And then we'll generalize it to more complicated situations. And that's where we'll stop. So remember today, to the extent that it is a short Tuesday session, whatever I'm going to say would be the tip of the iceberg. It's a foot in the lake. I'm just introducing you to what is actually a pretty fascinating and involved topic and that we are going to do in much greater detail when we do the math of data science. So before I start, are there any questions or any other requests that you guys would like to have? So, Asif, one simple question. So when, if to find the integration of a function is not easy, can we just like loop through it, find the value and multiply it with like a very small number and increment and find the sum that is the that is the reamer integral so let's talk through that let's talk to that i will that is essentially the gist of what we are going to do um before that could we just have a confirmation from somebody that you can see my screen uh could we just have a confirmation from somebody that you can see my screen what are you seeing my writing pad yes yes we are yes okay number of learners and the ensemble oh yes this is the end of the last uh's height a little bit so it's more manageable. Otherwise I'll stand and do it. All right, that should do. The question is, at this moment, are you folks able to see me clearly? Or are you just seeing the top of my head or the bottom of my head or whatever? A little bit toward the left, left we can see all of you yeah we can see you well yeah okay uh let me to the left you mean you mean the right is that better yes okay and is the height better i should increase the height or something like that so it's better chopping off a bit of your head it's chopping off my head so i need to increase the height how about this that's perfect better better okay so let us get started take this as a game guys don't think of it as a very serious topic because it isn't it's math is pure math is always fun and for once I'm taking a digression into math. Math is that mathematicians are the people who have fun all their lives and then expect to be paid for it. And it's a lovely thing that they actually do get paid for it. I've always felt that there's a beauty of mathematics. You never do anything serious and you still get paid. So we are going to play a game. Suppose I give you this. Imagine that you have a unit. What shape is this? A square. A unit square of size one, one, one, one. And let's say at the center of it, you draw a circle. So the circle goes, the biggest circle that is still contained within the square, you would agree, would be, well, my circle hardly looks circle is circle-ish let's say it is this circle what is the radius of this what's the radius of this it is one isn't it guys excuse me i think an alarm has gone off i'll turn it off All right. So this is a circle, the biggest circle we can draw inside the square. So far, so good. And now let's do... Since the side is one, isn't it? You're right. Isn't it 0.5? Yes. So the sides are two two and then. Let's make the sides two. Yeah. That sounds better. That sounds better now. Good. So yeah, be on the watch out. So we have this. And now we'll do an interesting experiment. We'll treat this as a dart board. You're going to shoot darts at this dart board. It will land anywhere within the square. So one thing, the rule is we will throw a large number of darts at this dartboard randomly. Important thing is you're not trying to shoot. This is not target practice. You're not trying to shoot for the center. But there is an assumption. We'll assume no darts fly off the square. So these are the rules of the game. So what do you think will happen? What may happen likely is that in some ways you will have dots all over the place, you know, the dots having made landing all over the place. And of course also assume that dots don't interfere with each other. Would you agree that you'll have darts all over the place? Intuitive so far? Yes. Yeah. This is it. And I want to draw all of them. This should suffice, but extrapolate from this to assume that there are dots all over. Now imagine that this number of dots. And I will use a language that goes back to actually Newton. So in this world, one of the problems with calculus is people use very fancy language. They'll say in the limit and all of that. But I'll use a simpler term, which was introduced by Newton and reintroduced by this lovely book that I recently read, Visual Complex Analysis and so forth. I'll just say, ultimately, so if I say that, let's take a statement like this, limit n tends to infinity, some function becomes, let us say, some function becomes a value y. I'll just take as an arbitrary way of saying this. So then I will traditionally say it not like this, which is a very traditional way of looking at it, if you remember. So I will say it using a simpler notation which is more sort of common to us. I'll say that f becomes this peculiar notation up and down becomes y which is f ultimately is y. is y. So when you hear me say ultimately is y, it means exactly this. It is just as rigorous, but at least to me it sounds more intuitive that f ultimately is y. Are we together? So this is a little side notation that we are going to follow. Let us now go back up and ask this question. And this is the fundamental question to ask. proportion of darts inside the circle be y. Let me just say it to be y. Right. By proportion, I mean y is number of over n. This is inside number inside circle. This is total number of darts thrown. And you assume that a large number of darts are thrown. So when you look at it, what does y ultimately become? What would you say that Y ultimately becomes in your way of looking at it? Let's figure that out. One way that you could do perhaps intuitively is suppose I told you that the area, that the area. Could you guys please mute yourself? I'm getting Prashant. Could you please mute? Yes. So let the area of the circle be pi r squared. Now, what is the radius of the circle here? One. One. And so this is equal to, therefore, this is the radius of the circle here? One. One. And so this is equal to, therefore, this is just equal to pi, isn't it? Let the area of the square be, literally, the side is 2r diameter squared,'t it and that is equal to 4. would you agree if r is equal to 1 2r square is 4. so far so good yes yeah so now wander over it if you go on throwing darts at this dartboard, what will be the proportion Y of darts inside the circle compared to all the darts? What proportion of darts will be inside the circle? Would it be reasonable to say that Y as in the limit would be equal to will ultimately become the area of the circle over area of the board right is that reasonable because if you randomly throw darts, how many darts will land inside the circle? It is proportional to the area of the circle relative to the area of the board itself. Is that reasonable? Yes. Yes. So area of the circle is this much. You would agree that only approximately the ratio of the area of the circle to the area of the board itself would be the proportion, ultimately the number of darts or the proportion y is equal to this. And what is that why ultimately becomes what a pi over four isn't it so far i hope this is all within a bounce of simple intuition any questions Any questions? I would assume no questions, none of you are asking. But what do you think has just happened? Now we will ask ourselves, what exactly just happened? Did we compute, if you look at this, this is, you have been able to and this is literally n over capital n so you can say pi is equal to four times the number that are inside the circle over the total number of dots that you threw so do you see this, guys? Something interesting. So far, so good. Nothing odd, isn't it? I would really appreciate some feedback, guys. Yes. I can understand. Yeah. This is straightforward, isn't it? The math is straightforward. But, Asif, this is like assuming some all unit things right but in reality is it so how what do you mean by unit something what is your we have taken one right like radius one and we assumed one and things like that you don't have to so let's say that you keep radius arbitrary in this so the ratio of this will be pi r squared over 2r squared and that will still be pi r squared over 4r squared this will cancel out and it will be pi by 4 yeah i took unit just for simplicity, but it generalizes to arbitrary sizes. Yeah, this makes sense. Yeah. Yeah. So now, what is remarkable here is, and when you look at this, it looks straightforward. But the most remarkable thing here is that you're shooting random darts. This only works if you genuinely have randomness in the darts that you're throwing. If you have a preference towards the centre, it won't work. You genuinely have to have uniform randomness. And you computed the value of pi using not a mathematical device, but using empirical evidence from experiment. So this looks like more something that a physicist would do rather than a mathematician. You found mathematically, if you want to find the value of pi, you will do all sorts of Taylor expansion, this and all sorts of number theoretic arguments to find the value of pi. But here we have found the value of pi not by doing any fancy number theory but simply by doing an experiment taking a dartboard and shooting darts at it lots of darts and we are basically saying that as in the asymptotic limit of large number of darts, a pi ultimately becomes, let me use this, that pi ultimately becomes the ratio, the four times the ratio of the number of darts that made it into the circle versus didn't. Isn't that a remarkable feat? So you could quite literally write code here. You could in Python, you could do np.random. What is it? Uniform random is just random, isn't it? You can take random values between minus one and one, and you could produce, let's say, 100,000 of these, or maybe a million of these values and then for each value and so you could say that x y is equal to so the location x y is equal to and you could just repeat the same thing again and it checks if this is the right function I'm using, but you get the idea, minus one to one, and another million of these. So you would get lots of x and y's. Now, all you have to do is check if x squared plus y squared is less than equal to one, right? You can have a variable. How would you do that? You could do even better. You could write it actually in one line of code. You could say little n, the number that made it inside, is the sum of the array, is the sum of the array, right? Sum of the array, right? Of the count of the array. And you can say sum of one, right? For x, y in zip. Let's say that, sorry, let me take variables. Let me call this capital X and capital Y because they are arrays. And let me just take this as X, Y in zip capital X, capital Y. If. If what? If X squared plus Y squared is less than equal to one? So if I sum up all of these instances where this condition is fulfilled, I will get the little n in one line of code. So in two lines of code, we have been able to do that. And therefore, we can say pi is equal to four times n over, what is it? A million. what is it a million and so in three lines of code by doing an experiment with randomness you are you are able to find the value of pi i don't know how you think if you just think about it it's pretty astounding you used randomness to do an experiment out of that experiment you looked at the empirical evidence and therefore you computed the value of a mathematical constant. You see that, right? And it takes a while to realize that something quite startling has happened. The first person who made this observation for the longest time doubted that this can't be true and puzzled over what could be the flaw in the reasoning. It isn't, of course, but it's a startling discovery. So we are going to use this discovery to now generalize it, to harness it. The question is, this is pretty good. How can we use it for something else? So I will now, and today, because time is short, I'll do it only in one dimension. This problem or this approach gets more interesting when you do with multiple dimensions. But for today, we'll keep life simple. Let us say that you have, let me use another color. you have, let me use another color. So consider a function fx, consider an arbitrary function, arbitrary function that is well continuous that the only a constraint that we put at least for now to understand things that is continuous therefore we can write it as this function can be written as and you if you plot it out if this is your x-axis and this is your fx axis would you agree that this would be something like some arbitrary function i'm drawing it out so far so good guys so this is fx now you should recall this much from your basic calculus now comes an interesting question suppose i take two limit intervals a and b and i ask you what is the area under the curve the question is what is the What is the, under the curve? Integral of f of xh. Yeah, that is, by definition, almost by definition of the Riemann integral, we say it is fx dx integral from a to b, right? Now, there are ways to do it numerically. What you would do is numerically, you would break this interval into many pieces. Let's say that n pieces. So each of these interval is interval sizes interval size would be the whole range the range is p minus a right let me call it r is equal to this and so interval sizes are divided by n isn't? This is the size of one little interval. So far, so good, guys, right? Let's give this interval by definition. Let's write it as delta x, one interval. So we realize that in each of these little intervals, we can technically consider this, the function to be constant by taking, let's say, the average value here, or the midpoint value. It doesn't matter where you take the beginning or the end value, because if you make your intervals very small, this thing, you can trace, this is a trapezoid kind of a shape. Let's say it is like this but you say that this is approximately equal to or this becomes a rectangle right so let me call this p and this will q so we say that a p ultimately becomes the rectangle q q when becomes the rectangle Q, Q when delta X gets small, gets vanishing. Would you agree with this statement? This is essentially the calculus thing, right? So what you do is you compute the area of each of these rectangles, and we all know what the area of a rectangle is. Area of a rectangle at this particular thing would be fx at that particular point, xi. Let me just call it 1, 2, 3, 4, whatever it is, xi, d delta xi, and you sum over all values of i from let's say one two how many points are there nine n sorry n intervals so this is approximately your area and you would agree that this area ultimately the fair this is approximately the area but in the limit of delta x becoming vanishingly small this thing becomes basically and you represent it as a as integral which is if you write sum and you stretch the two ends out it will begin to look like an integral it becomes and the sigma which is the sum begins to becomes this and this is the uh this is of course the Riemann definition Riemannian definition of integral it becomes this so you could do this numerically by subdividing the range into arbitrary many large intervals right a large number of intervals each interval being very small and at each at each point you can compute the fx and you could then do fx times the interval size and you could what you're doing is you're essentially summing up uh those little values right um as you do that you will end up with a total. So far so good, guys? Yes. This is, I hope, very intuitive. But now I'm going to mention another way that we could do this. It turns out that doing this subdivision into very tiny parts gets quite problematic in high dimensions and data in machine learning often presents itself in extremely high dimensional spaces so in very high demand so suppose you divide this into uh one million as intervals you know let's take thousand intervals a thousand little interval sizes right. But when you're talking about 100 dimension, how many little patches of area in the ground in the in the feature space will you be looking at a volumetric spaces, you'll be looking at, you'll be looking at 100, like 1000 times 1000 times 1000, that so on so forth a hundred times in other words thousand to the power hundred so when you do thousand to the power hundred you realize that you're looking at an intractable problem it doesn't scale at all isn't it you can't use numerical methods in such high dimension spaces is that is that obvious guys so imagine that you're doing it just in two dimension now what will happen you have some some surface and you you want to find the area in this patch the integral over this patch this patch projects itself to something right so this little patch here you want to find the area, which will you have to do? You'll have to discretize it, delta x1, delta x2. And for each little patch, you'll have to multiply it by the fx1, x2 value to get the volume here, right? Because that is what the generalization of the integral would be in high dimension. But now how many subintervals, how many intervals do we have? If along each axis we take 1,000, now we have 1,000 square in two dimensions. Generalizing it, it becomes 1,000 to the power n for an n-dimensional space. And this number very soon becomes computationally hard or computationally intractable. It takes a very long time. So we need a better way to compute, for example, this integral. So here is one way we can do. We can use the same randomness that we used before. And I will just redraw the function that we have. Let me use the same colors. Maybe I should just copy paste paste is there a way to copy paste there must be right uh let me give me a moment let me try this i'm not very no uh well let me try my luck again this much i'll copy So let me try my luck again. This much I'll copy. This much I'll copy. And copy. And let's go here and try to paste it. Let's see how good I am at this activity. Paste. Wonderful. It wasn't bad after all. So now we'll take a different take on it. Let's go about it in an entirely different way. We'll change the game. And we'll say that, okay, forget about this whole business of picking up cuboids or the rectangular regions. And we won't do all of this at all. This is too cumbersome, especially in high dimensional spaces. So what do we do? We go back to the randomness argument and we say, so I'll erase this and I say, okay, let me bring back the curve so that we have the curve. Yeah, and I say that, you know what? Let us shrink wrap this in a box of some arbitrary height H, right? so now you look at this and you you do your darts experiment all over again you start throwing darts all over the place right when you throw darts all over the place and here here here and you ask, how many darts landed below the curve? So if you throw it like this, let's see how many landed below the curve. In other words, how many landed in the region, in this region? Let me call this region, let me call this region, let R be the region below the curve. We agree that R is essentially integral of fx dx from a to b would we agree with that statement i ponder over this and give me some feedback does this look very straightforward so far yeah yes it is the area under the curve now here is the interesting thing let us revisit and re-ask this question what what is the total area the total area of the dartboard of the dartboard. What is the total area of the dartboard? It is this thing which is b minus a times the height, isn't it? The total area is dartboard is dartboard is base which is b minus a times height would you agree that this is it uh how it is b minus a so you might say is the region is the region see value x is equal to b and this is x is equal to a so how what is the length of the base of this rectangle yeah b so yeah it is b minus a and what is the height of this dartboard that we made by construction we gave it a height edge we just made sure that the height edge is more than the maximum of this function just put take a safe enough height so that you know that the function will stay within it. So, so far so good, guys. And now ask this question. If you throw a large number of dots, what does, would you agree that when we throw an arbitrarily large number of dots, say you throw a million dots, which is not bad, a single for loop of a million dots, what proportion y is equal to little n over capital N of dots will be in the region R? If you were to ask this question, now that you know what we have been learning about, how would you answer this question? Why will ultimately become what? It is N over capital N, and this proportion should be the same as R over A. Would you agree? The proportion of darts that made it into the region, under the region, should be the same r as the ratio of the proportion of the region r versus the whole area a. Would you agree? Yes. Right. I hope this is all very simple. So this thing would hold true. And so we can say with the same argument that this is equal to, and these two are related, the proportion that make it into it. Now, what is r? r is equal to area, n times area over capital N, which is n times area was b minus a times height over capital capital A. And so you can say that what you have just done is r, which was fx dx from a to b, which was essentially your area that you were looking for, the y that you were looking for, is, which is literally this, equal to this, ultimately becomes n times B minus a H from A to B over and now you pick your edge as you wish, and then all you have to do is you choose how many dots to throw, and this is your empirical observation. Pretty much the same line of code now will be able to tell you the integral of this function. Are we together? Does this make sense? Yes. Yes. So this is the beauty. This approach is called in its simplest form, this is Monte Carlo. Monte Carlo is the ability to solve complex integrals by using randomness, by throwing random dots into a certain volumetric space where, hey I did it for two dimensions, but now you can generalize it for certain feature space arbitrary feature. feature space, not just a one dimensional feature space, you could therefore compute the integral using this, which is a pretty startling fact, let me recap what we said, we are seeing that the integral of fx dx arbitrarily in this interval A to b is dependent on it ultimately becomes the this is our observed value and you have solved it not by doing some elegant mathematics but actually by doing an experiment and observing what n turn out to be. Just go count the n. And when you do so, you have computed the value of the integral. You can do integration, in other words, by not numerical means, not analytical means, but by a third way, by using randomness and doing an experiment. This third way is called Monte Carlo. Monte Carlo. Monte Carlo. And this whole process of throwing darts is often called Monte Carlo simulation. But let me just call it the Monte Carlo method, which is... So we have come up with a third method, a third way of computing integrals of functions. So let us recap what we learned. We learned that if we take a unit circle, where is our unit circle? If we take a unit circle and wrap it around with the units with a square that just about captures the circle inside it then and throw start throwing darts we can use it to find the value of pi right and we can use that generalize that to that thing to do other experiments with functions and do that. Now, let us go back and revisit how was that circle able to do it in view of an arbitrary function. So what was the function in the case of a circle? If you go back and look at it, let's look at only one fourth of the circle so suppose we make a dartboard like this sorry a dartboard that is just like this this is truly one this is truly one this radius is one so here the area of the circle is pi by four isn't it Because it's 1 for the circle. Right? And the area of the square is square area is equal to 1. Now, what is the function that we found the integral of? If you consider this y and this as x, x is y is equal to, or your function of x, y is fx, is equal to, you would agree, that 1 minus x squared square root, right? And so what you have done by doing this is to show that the area fx dx integral, which is 1 minus x squared, and of course, you can scale it up to any arbitrary radius. This integral from where to where, from zero to one, here to here, dx, is nothing but the proportion of n over n, how many dots made it inside. And of course, we do know that this happens to be pi by four. Pi by four was equal to n over n. And so we found the value of pi. But the interesting thing is, and so I just remapped this thing to the more general situation, that you can find the integral value of any function simply by doing experiments or trials or Monte Carlo simulations. This is the Montelo method for finding integrals why is this so useful to machine learning it it is a it's a lovely mathematical curiosity why do we care about it in you have, let's say that you have a prior probability distribution. Okay, that gets complicated. Let's start simply. Let's go back to our assumptions. Let's say that you landed, remember the example that I keep taking, suppose you land in a city, you and your friend, and your friend believes it must be a desert, it must be someplace like Arizona in Phoenix, but you believe, no, it's someplace like Seattle. You believe about, and you know that Seattle is raining, is raining, and the phoenix is dry for the most part. So you believe this is your, so you say, let's say your hypothesis says Seattle. Your friend's hypothesis says it is Phoenix. Now, which of these is true? Let's do a distinction. So you are asked to tell what is the probability that it will rain on a given day. Let's say that you say it is 9 over 10. And your friend says, well well you know what it is it is one over ten or even less right so but for the sake of argument i will just take it nine over ten and one over ten as argument so we have two hypotheses and i will draw it out and ask this question now. So let's say that you have a hypothesis one that you are in Seattle and hypothesis two that you are you have landed in the dark in Phoenix. So you realize that what is the prior probability? What are your initial guesses? Priorities, initial guesses. For Seattle, of course, it is 9 over 10. And for Phoenix, that number would be this. Now, suppose you have the evidence of this. Let's take this thing. It rained the first day. Rain, dry, rain, rain, rain. Well, now let us see what is it telling you about each of the hypotheses. What is the, let's do the likelihood. Likelihood of Seattle hypothesis being true. If it is Seattle, the probability of rain was 9 10th, was 1 tenth, 9 tenth, 9 tenth, 9 tenth, isn't it? So basically it is equal to 9 to the power 4 over 10 to the power 5, isn't it? This is the likelihood here, 9 to the power 4 over 10 to the power 5. This is the likelihood here, 9 to the power 4 over 10 to the power 5. This is the likelihood. If you believe it is Seattle, the likelihood that you would see this evidence, this particular sequence, right, is this. Would you agree with that, guys? I've just multiplied the daily probabilities of the event. What about the Phoenix person? For Phoenix, the probability of rain was one-tenth, right? So one-tenth, probability of dry was, of course, nine-tenth, right? Because dry is what the person was looking for. And then one-tenth, one-tenth, one-tenth, because rain, rain, rain. According to your friend, it shouldn't have been raining. The chances of raining on each of these days was only 1 10th, whereas the chances of it being dry was 90% 9 10th. And so the likelihood for the phoenix would be 9 over 10 to the power 5. Now, who looks more likely now? So if you look at this, first of all, which evidence is more likely? If you look at the maximum likelihood hypothesis, which hypothesis has the greatest likelihood? You, the first guesser. The first guesser, isn't it? Seattle. This is the likelihood. But the way the Bayesians think is slightly different. They say, hey, you know what? We use evidence. We can never be sure. It is all about uncertainties. So all we can say is that if you start with this, you're unweighted. So there's a norm without a normalized, let me just call it unnormalized probability sort of is, multiply the prior with the likelihood. In other words, you take the prior probability of the evidence, you multiply it by the likelihood of the evidence, which is what I'm doing here. This is here. So this is pi of x. This is the likelihood of x. And so you just multiply these two. And so this will become 9 to the power 5 over 10 to the power 6. And this will become 9 into 10 to the power 6. All right. Now let's do this. Let's sort of add up these two things. You will end up with 9 to the power 5 plus 9 over 10 to the power 6. And what you do is you divide both of these by this total. This total, what you do is you add up all the probabilities and you do that for this game. And what you will see, by the the way what we did is this mathematical device is called the bias box right we'll do it more formally when we do the math of data science so we do this experiment and see let's scale all of these by the total what proportion of probabilities are tending towards seattle now this would be nine to the power five plus nine Seattle. Now this would be 9 to the power 5 plus 9, right, sorry, to the power 5 divided by 9 to the power 5 plus 9, right, and this one would be 9 over 9 to the power 5 plus 9, right. So would one of you like to do this computation and tell me what those probabilities are coming to? do this computation and tell me what those probabilities are coming to. So this is often called the posterior probability. It's saying that irrespective of what your belief was, you have to change your belief. You have to say that my posterior probability, a belief that I am in Seattle is this, and the posterior probability that we are in Phoenix is this, right? So who would like to compute this, please? First probability is 0.998. 0.998. So Seattle went from 0.9, 90% to, or rather 90%, it went to 99.8%, isn't it? And Phoenix went from 10% to maybe 0.001. One second. maybe zero zero point zero zero zero one one second So this, of course, takes you to, let's say, point two. So what do you notice? The evidence has pushed you, convinced the person much more strongly that we are in Seattle and we are not in Phoenix. So you say that your posterior belief is leaning much more strongly towards Seattle based on the evidence that we saw. Now, this is the Bayesian journey. This is all that there is to Bayesian statistics, actually, if you could get the hang of this. But in this, there is a catch. Do you notice that we have to find the total here? Now, in reality, what happens is that you don't have one belief. You have hypotheses that you can have. Could be anything from 0% to 100%, isn't it? So the probability, the prior probability that you could have taken could have been any value by X, right, any particular value, this could be between zero and 100. And so you would have to work out this table for each of them. So in other words, what the total becomes, and at this moment, I'm just giving you a flavor. We'll do it much more rigorously. This total, which is a sum, what happens is as you go from discrete to continuous case, it becomes an integral of all the unnormalized probabilities. normalised probabilities. And basically what it is, is it is Lx dx integral over all possible values of x. Lx times pi x dx, all values of x, if you think about it, that's what we are doing. So the moment you do this integral, and now you realize that here we just looked at the rain and so on and so forth. In real life, you have a feature space that is very high dimensional. So in learning, you have this pesky problem of dealing with an integral, which is rather intractable using analytical methods and hard even using numerical methods. So you have to take recourse to the Monte Carlo approach. You have to throw random dots and compute the integral. So now this biaisian part I rushed through because we didn't have much time. I have another meeting coming up, but its main purpose is to convince you that in machine learning, there are situations where you need to deal with other interactively hard integrals and Monte Carlo proves very useful in solving solving that so let's just summarize what we learned monte carlo is a means to it's yet another way besides analytical and numerical methods yet another way to compute integrals it's very useful when other methods are not possible right or not scalable for example the the the basic numerical method would not work so well then monte carlo comes in very useful there's a few who have background in some fields of disciplines of engineering and in physics probably know that most situations the integrals are intractable so monte car Carlo is often considered the brute force approach. Like it's the big hammer. When all else fails, bring out the big hammer and then you can still solve the problem using a Monte Carlo simulation. It's quite often used in finance. It's quite often used in many, many disciplines it is used. But it is also central to our approaches of Bayesian machine learning. Now, Bayesian machine learning is something we'll get introduced to in the next workshop, which is the mathematics of data science. All right, that's all I have for today. Any questions? any questions uh when there are higher dimensions at that time uh how will be uh how we will be calculating the number of points which are under the curve very easy so suppose so imagine two dimensions so suppose you are looking at this area inside this patch of a function, fx. So what you do is you wrap it up into a volumetric rectangular volumetric space. Oh, sorry. This should be here and this should be here. My rectangular diagram is pretty terrible. But now what you do is randomly throw darts into this volumetric space. Imagine that it is some sort of a jello, a block of jello, and you can shoot little bullets, which will get stuck somewhere in the jello. More practically speaking, just pick arbitrary number of points and ask, are you under the curve, or are you under the surface, that arbitrary surface that it is reflected, or are you above the surface? That's all so the curve generalizes to surfaces in higher dimensions now there are certain complications i've given you the simplest version of monte carlo but you can actually see it gets complicated because what if the function has a floor that is not flat, that we assumed here is flat? And there are situations in which, for example, if I say, what is the area of this? And let's say that I can express this as a function for you. It is some function, let's say, fx. This is gx. And I can say that this is fx minus gx right is or something like that you know something like that is there is this region here and how would you compute it so there are complicated situations you can come to and we won't get into here but i wanted to give the basic intuition that if you think in terms of functions as curves in one dimension, you realize that there is a way to compute the area or the integral of that function using randomness. And then we have to be careful as we generalize it to higher dimensions and more complex situations. dimensions and more complex situations. Asif, I'm remembering another example you would use that I think of like a drunk taking steps kind of. Oh, that was Markov. That's Markov. Markov. Yeah, Markov, Monte Carlo. So, okay. Yeah. So we have to, that is a full, that is of course the metropolis algorithm. When you take the Markov chain, in other words, which is a fancy word of saying, when you take a drunk person meandering all over the feature space and put that idea together with Monte Carlo, then you get Markov chain Monte Carlo. And its most important realization is the metropolis algorithm. If you google the word the most profound or the most important algorithm of the 20th century, if you were to google it today, surprisingly you'll be surprised that the answer to this, surprisingly you'll be surprised that the answer to this which is correct is the metropolis algorithm so it's a it's sort of a tragedy that most people who do computer science are never taught metropolis so we will do that of course in our math of data science much more carefully so it's a combination of Markov and Monte Carlo. Yes, it's putting those two ideas together. It's the metropolis algorithm. Okay, thanks. I just remembered the drunken elegy. Right. And we did an experiment also. Remember the poor guy and the rich guy doing investments? The poor guy always lost. So we'll do those fun experiments. Like the game of Monopoly. Yes, exactly. And there is a critical point beyond which the rich gets richer.