 ðŸŽµ so So I want to do it as an interactive session and it's not per se a quiz. In other words, I don't give you a quiz that you just sit and do. Instead, we'll discuss the questions, we'll create situations and I'll ask you what is it. And we'll do it in a rapid fire way. Let's see how much we can cover today and we'll build on to it week after week after week. We'll go deeper and deeper into the field of data science and machine learning. So today let's start with the very basic. If you and this is a sort of a more philosophical question. Let me call it this one. What is the difference between the difference between a data analysis task, right? For example, fetching some data from the database and finding the top end and something like a regression. regression what is the difference between finding the top end let us say top end book sold sold in an e-commerce site in an e-commerce commerce site and predicting the number of books that will be sold tomorrow so let me pose this question. Which of these things would you consider as a machine learning task? And which of these would you consider a data science, like broadly a data science task? Who would like to answer that? First one is the data science task. The second one will be the machine learning. And why would you say that? Because we're predicting something. In the second one we are predicting something. Yes that is correct. Anybody would like to give another answer? No the predicting is clearly from regression. Right. A number. We are predicting a number. So when you say the top end, traditionally top end, does it require any kind of machine learning? No. You can, for example, run a sequel query or or some Miller similar and you can find what the top end is whereas this one is predicting you give it something and you're predicting number real numbers and it's a regression problem so which would you say that the second one is also data science or not data science i would like to answer the first one you guys are saying is data science you need some sql or some sort of a bean counting something similar for bean counting that's just cpu work if you work what about the second one would you call it data science or no, it is purely machine learning? Well, machine learning is a part of data science. Yeah, I mean, data science is kind of involved in machine learning also. And that's the reason I asked this question because people often are a little bit confused about how we define data science. So broadly the answer for that is that suppose you have, let's see, give me a second. So generally when people say data science, the word is a bit ambiguous. And you ask different people to define data science and you would literally get different answers. If I remember, Berkey at one time had a whole website devoted to the various definitions of big data and data science and things like that. So what happens is when something is hot, everybody believes that they know what it is. But if you ask them what it is, they have an entirely different definition of it. Some of the definitions of big data that Berkey once gathered, I think they still have a website showing that. They're hilarious. They are from consultants who have absolutely no technical background so those are aspirational gushing flowery and so forth but we are technical people so we need to be a little bit more precise the data science today we recognize to be a slightly broad field which which includes everything so for example exploratory data analysis right using sql uh etc etc we traditionally call it as part of the data science toolkit data visualization data visualization is another part that you typically put in data science and then you have the machine learning part ML part you have a lot of the ML and this is also part of your arsenal of data science. Now in ML people often begin to talk about something which they say is beyond ML. So there is this word artificial intelligence. So let me ask this question, a very basic question. What is the relationship of artificial intelligence to data science? Are they the same thing? Are they different? What's the relationship between the two? Anybody would like to say? What is the relationship of artificial intelligence to data science and to machine learning machine learning a very uh basic question i'll write these relationships here data science ai and machine learning would somebody like to venture between in the relationships between these terms so there is an academic relationship and there's a relationship that that people are increasingly sort of in a very casual way throwing around. Would somebody venture some of the words or some of the relationships that you can think of? . I think data science and company encompasses both the artificial intelligence and machine learning. Machine learning is rather a subset of artificial intelligence. We don't have truly intelligent machines yet. They're still at a much lower level than had been envisioned decades ago. Yes. Anyone else would like to say, what's the difference between data science, AI, and machine learning? So for example, if you apply for jobs, you notice that their jobs of AI, the jobs of a machine learning engineer and the job of a data scientist, they are often described differently. But from an academic perspective, so let's address it from an academic perspective, what's the relationship between these? I think data science and machine learning are subsets of AI, you could say. Okay. So one possible answer is that AI is the broad field. And you would like it this way. And you would say data science is here and machine learning is where? Something like this? Oh, yes. Data science and ML. This is one way of drawing out the Venn diagram. Anybody else would like to draw it out differently? I would say machine learning is part of DS. Machine learning is part. So for example, let me put ES here. And let me put the ML here. And what about AI? AI encompasses the whole thing. It's a very broad subject. This is like this. So this is option number two. And anybody else would like to venture a possible solution? AI and ML come under DS. Yeah. Okay. So you would put DS here and you would put AI here and you would put ML here. Yes. OK. So you would put BS here, and you would put AI here, and you would put . Yes. OK. Any other possibilities, guys, that you guys can think of? I think the third version, but the machine learning is just a subset within the artificial intelligence. So how would you draw the box? You would draw ML, then you would draw AI. No, the opposite. The opposite. Machine learning is just a kind of a small subset within artificial intelligence. Okay. The data science encompasses all of that. Encompasses all of that. This is interesting. This itself is a good possibility. Anybody else would like to have another possible Venn diagram Shankar can you already respond yeah sure so the bigger one would be like the data science will be the big scrap and then the machine learning will be yeah machine learning will be within data science within data science ml and artificial intelligence is the one surrounding both ML and data science. Big one, bigger one. Bigger one. All right. So you would put this as a possibility. Yeah, I think it was the second one. Then it begins to look like the second one. So I'll leave. Okay, guys, anything else that you would like to see? And I'm asking for a pure academic definition not from a job perspective the interesting thing that i want to bring out is the way the industry the job defines it is different from the technical definition of this any other point does it live you know there are only three circles in how many ways can we draw three circles anything else three ds and ML, AI and ML as two separate circles under DS. Why not? Let's do that too. DS, AI, ML. But the AI and ML should not be disjoint, right? There should be some intersection between AI and ML. So let me just call it number one, number two, number three, number four, number five. So do you not realize that we are all people who have some degree of exposure to this field? And many of you have done the ML 100, most of you have done it yet, and you guys are practicing it in your job, and some of you, right? And this is a pretty illustrious audience I can come a few PhDs here and most people have they're in college certainly everybody is beyond high school you're all college graduates or in college or post graduates and so forth yet we at the very foundation people don't seem to know what it is we have different vocabularies literally and so conversation often becomes hard because when we give different meanings to the same word it is easy to misunderstand each other so the way traditionally you say is different. The word data science is a very recent invention. The field used to be that you would have, broadly speaking, you would have AI, AI's goal was make machines intelligent. The trouble with that is we don't know what the word intelligent means. And so there are different definitions of intelligence, strong intelligence, weak intelligence. Strong intelligence means you really want them to think. And there was a company called Thinking Machines. It went nowhere. In Silicon Valley, illustrious company in which many Nobel laureates came to contribute and so forth. The goal was way back in the 70s or 80s, it looks almost silly that with that kind of hardware they could dream of creating intelligent machines, but they were dreaming Minsky and so forth. I can't hear you. Then came actually a... Hello. We had lost you. Hello. Okay. You're talking about thinking machines back in the 70s and lack of hardware. Yes, I apologize. I don't know what happened. So yeah, at one time people used to believe, the dream was that with these computers we can create intelligent thinking machines. In fact, the whole pursuit of computers was for that purpose. If you go back and look at Charles Babbage, why he created machines, it was for the purpose of building intelligent machines for computers. He wasn't just thinking of number crunching. Minsky and folks, they were very ambitious. They started creating, those were the days of Perceptron and so forth, and very primitive hardware, and they started creating dreams of intelligent machines. They formed a big prestigious company in Silicon Valley. Many Nobel laureates came and worked. It was hard. They were struggling with it but everybody was hopeful they'll achieve AI in our own lifetime. Movies started coming out with all sorts of scenarios in which the our new overlords would be the machines. It didn't quite succeed and actually one of my professors, Sir Roger Penrose, a great mathematician who is also the professor of Stephen Hawking wrote a book called the Empress new mind basically saying that we are pursuing a full serent because we don't know what thinking is thinking is not something we can quantify mathematically. To the best of our knowledge, thinking is likely a quantum phenomenon. And so these classical machines with the electrical circuitry is not likely to ever create thinking machines to the way we can do. And I'm obviously giving away butchered version of his book. It's a very nuanced and lovely book to read if anybody is interested, An Empress New Mind. So people began to then say, okay, then what is intelligence? There's a weaker definition of intelligence. If a machine can fool you into believing that it's intelligent, that it's human, we consider it to be intelligent, right? So for example, if it can beat you at a game of chess, we say it has intelligence and things like that. So at specialized tasks, machine can simulate the appearance of intelligence, which is what we pursue today. Nobody knows a theory of generalized intelligence. Um, now when we talk of intelligence, we talk of learning machines. Learning is quantifiable. What you do is you give a task, given a task to a machine, you ask it to do the task. You look at the number of errors. So you quantify the errors, right? With the errors, from the errors, you create something called a loss function, and this is something, if folks have been with me, so error function, and what do you do? If you can, learning is the minimization of error. We define that if you can make the computer, the machine, make less errors, that is learning. Learning is to minimize errors. So this whole field of machine learning starts with the premise that they can take a ground state in which you have a machine, you make it build a hypothesis, which is fairly useless, but you can quantify the errors it's making. For example, if it is identifying cats and dogs in the beginning, it's randomly guessing. And by the way, that's true of children too. They randomly guess. How does a mother or a father know that a child is learning about cows and dogs after some time you notice cats and dogs right or no I usually go with cows and ducks so cows and ducks you keep pointing out cows and ducks and after some time the child begins to get the sense of what's a cow and what's a duck and when you ask it what is it it is able to give answers mostly correct then you say that the child has learned about cows and ducks the same translates to machines when machines make less errors you say that the machines are learning and so the subject that makes machine learn is machine learning it is a part of ai it is a crucial part of ai ai has other parts for example robotics is there of AI. AI has other parts. For example, robotics is there, which is extremely hot these days. We talk about, we want our robots to vacuum our floors. We want our robot to drive our cars. We want robots to fly drones. And by the way, they fly drones very well and so forth. So robotics is the utilization of machine learning to do a lot of host of things with its actuators and sensors. Machine learning is the core of AI. It is the theoretical foundation that we use in AI. Now, AI has some other aspects, the search and many other things are there. And for those of you who are interested, the landmark book on AI, artificial intelligence, a modern approach, right, by all this Stanford bunch of props. The new edition just came out this month. I invite you to get it. The lovely thing is it now has color pictures. especially those of you who are in college or just came out of college if you can get a copy of the book it would be lovely it has a lot of new sections especially specifically in natural language processing transformers etc they are all there in the book the book has grown fatter so the pages have become thinner but they are in color I suppose the weight of the book is almost the same it was a heavy big fat book thousands of pages but it is still artificial intelligence a modern It's really a very good book to have. You can't over, I can't sell it too much. It is by these people, Russell and Norvig. It is the great classic in the field guys if for no other reason just to keep yourself motivated have one on your desk or have one in your shelf and every week dip in somewhere it's a huge book you can't unless you're a student you can't read it linearly as a professional you won't get the time to read it linearly end-to-end but I would suggest take up a project you know every month maybe pick a chapter read through a chapter and you can actually hop around the chapters because you all know the subject broadly so you can sort of pick and choose what you learn from that book it's very good so anyway that is artificial intelligence machine learning is a part of it and choose what you learn from that book. It's very good. So anyway, that is artificial intelligence. Machine learning is a part of it. And if you take that book, you realize what a vast subject artificial intelligence is. Now, where does data science come in? Data science people is a very confusing thing because it's a new term. Different people began to throw the word around as meaning different things some people went or gave definitions that were so broad as to encompass all of science for example they say data science is the study of data then what exactly is physics or chemistry or geology or biology or something everything is the study of data science by definition is the process of asking questions, gathering observations, observations are data, drawing inferences from data, building hypothesis, and then continuing the cycle. That's your cycle of ask, measure, learn of science. So the entire scientific method is that. There is no science without data. So that's something worth remembering. But the data science crowd can often get exuberant about all of this. Then there are claims that data science is the fourth science beyond physics, chemistry, and biology. And so there are all of these exaggerated claims I wouldn't go so far I would say that data science is basically a set of techniques which help you analyze data and therefore it has always been evolving with science itself and its successes have been the successes of science for example it's when you think about what data can do, one man observed or wrote in a book that was in the Library of Alexandria that if you put a stick in the ground, then on a certain day at a certain place, at a certain day at a certain place place at a certain date at a certain time of year the stick will cast no shadows now you realize that if you take a stick here and just stick it in the ground, any stick, even at noon, if you look carefully, there will be a shadow. But this gentleman said that at a certain place, you observe that if you put a stick in the ground, it will cast no shadows. What is it? It's a data point about something that happened. It's an observation many years later a person who was studying that book happened to come across the statement and it got him curious what is it and why is that true so he went about putting sticks everywhere and noticing it always cast a shadow so he went to this place at that time of the year and put a stick and sure enough there were no shadows. From that he concluded and I will let you think through why it is that it means that if the Sun is here the earth cannot be and the shadows at noon if the length changes, but at a certain time at a certain day if it casts no shadows, it's because the sun is looking directly at it and so he concluded that the earth must be round and not only is it round because at different places it is casting shadows at the same time not only is it round it tilts on its axis the axis of rotation keeps tilting it rotates on its axis and the rotation tilts it so just think of the profundity of it from one simple observation you can come to conclusions you can come to fairly big hypothesis about the about the nature of the world you suddenly put a sun out there you made the earth spin and you made it sort of have this wobble around its axis and that is science that's the heart of science so I wouldn't say that data science is something that we just came upon the last few years but the terminology has gained a lot of attention and so today when we say data science we are basically saying science but we are focusing on specific techniques and those techniques are basically data access cleaning cleaning and preparation data preparation cleaning and preparation data preparation visualization visualization and then we also put machine learning in the AI in there right the AI is there part of it right so I would say that data science is basically one aspect of science science has many other things that you worry about, but focusing more on the data itself and the manipulation of data, all science is about data. Then you have AI and then you have data science AI. So this is the academic definition of the subject. This is the way I would define it. If you search on the internet, just Google the relationship, you will find all sorts of Venn diagrams. So all sorts of people have different interpretations, but stick with that. Now, in practical terms, what it means, this confusion has led to a lot of things in practice. What has happened is there are jobs for data science. Can I see that diagram again, please? This diagram, yes. So we basically have science, data science is a part of it AI and AI is the broad field and data and sorry not data science machine learning ML ML is in there does this make sense guys any questions make sense to me. So let's stay with that. We'll stay with this vocabulary. Now in practice, however, this is not what is practiced. When we say machine learning scientist, people use the word jobs come as data scientist, machine learning engineer, data scientist, machine learning engineer, and AI engineer. Have you noticed that when you look for jobs they come in these three broad categories? So those of you who have been job hunting from a practical perspective would you like to define what these mean? So data scientist is mostly actually doing all of these things, data visualization, cleanup, prep, et cetera. And he is doing the algorithmics, the machine learning. It's very interesting. Much of the machine learning algos on data are being applied by data scientists. The machine learning engineer is taking it to prediction, is taking these things of data scientists to production, to operationalize it. So when you actually become a machine learning engineer, remarkably, your job is to take the models that people have built and use it for inferencing at runtime, like at production systems. This is a trend I'm observing more and more in job descriptions. There's a little bit of a fluidity in how people define things, but more and more I'm seeing this. When you look at the AI, the people, for reasons, whatever reasons, they're talking about the data scientists who only deal with non-structured data, text, vision, computer vision, images, vision, computer vision, images, etc. The non-structured data sound, the manipulation of non-structured data, or more specifically the use of neural networks or deep learning, is what people are meaning. So those data scientists who are focused on that. So what happens is this data science becomes about tabular data in practice. So if a bank has data it is likely to be tabular data. Data that you can represent in a data frame and that is what most of the data scientists jobs tend to require. On the other hand the AI jobs tend to require working with images, videos, text, sound, etc. This is sort of the practical boundaries, the way people draw it, which is quite different from the academic background. Anybody who agrees with this, having looked for jobs and looked at the job situation, or who has worked in companies and see how this word is defined. Ajay, how is it in Walmart? How is it like, is it somewhat like this or is it different? Ajay Nandigamara Yeah, I mean, it's kind of, yeah, but I haven't seen any AI, anybody's called an AI engineer, but still there are people doing a lot of work on images, image search, like that, but they're all called data scientists. They're just called data scientists. Yeah, but even everybody is called a data scientist, not even machine learning engineers. Everybody is called a data scientist. Yeah, data scientist, yeah. Yeah. So it's sort of fluid. If you go to Facebook, practically any job to do with data, even database experts like SQL people, they are all given the title data scientist. So the word has become whatever people want it to be. Anybody has experience with Facebook? So the way they define it is that just about everybody seems to be a data scientist if you're dealing with data and then they have a group funnily called the core data scientist. So to imply people who are actually dealing with R&D on algorithms. So every company is taking a definition somewhat differently but broadly consider this tabulation. So, all right, now that we have gotten that out of the way, when we say, now let's go back to the original question, it's a long-winded way of doing it but I wanted to set the stage for today. Now, most of the questions we won't spend half an hour, we'll just spend perhaps five minutes or three minutes. So, now let's get back to the question. What is the difference between finding top end books sold in an e-commerce site and predicting that the number of books that will be sold tomorrow? So the first part, finding top end, what is it? Is it machine learning? It's not machine learning. It is just standard data analysis, isn't it? And so it is just data science, but not machine learning. The second part, predicting, it is a machine learning task, isn't it? It's regression. And so regression is machine learning, but it is also, what else is it? It is also therefore a task in AI and a task in data science. Isn't it? According to the academic definition of the words, AI, machine learning, right so in other words if you look at this picture if it is machine learning ask is machine learning it is of course automatically AI and it is automatically a data science ask it's a regression is all of those that's a basic equation now I'll ask a question suppose you have data we'll talk about a concept called uh residuals or understanding errors so suppose i have data points like this and you let us say the data points are like this first of all you try to predict Y is what you have to predict based on X is this a problem predict Y as a function of X this is a problem in regression or classification or classification? What's the task? Y in terms of X. Oh, that's a predicting that's just a um regression. It's regression because you're predicting a number. Because you're predicting a number and you're trying to draw a line sorry a line let me make this line a different color make it even bigger red suppose you do this now could you guys tell me what is a residual? In this story, what is a residual? What's a residual? The residual is the difference between the actual data point and the regression line between the predicted Y. That's right. So in other words, this gap, where should we say, this gap, this gap, this is Y for a given X. Let me just call it XI. This i this is y i hat this is the prediction so the gap residual residual is the gap between the prediction y i minus y i hat this gap is the residual right this is the mistake residual. This is the mistake in prediction. This is the mistake in prediction and this is the residual. The residual stands for residual error. After you have made a best fit model to the line, you have the residual error now given this residual error how do we define the sum of all errors what is the total error that we have the total error we would define as what are I squared this is the sum of all errors isn't? Some add up all the errors and square them and we get it. Now why do we square them? Could somebody tell me? So you can cancel out the negative and the positive differences. That is true but I could have done it something like this, sum of the mod of all the errors. That's mathematically messier. Mathematically messier, mod is for example not a differentiable function and so on and so forth but there is also a theory amplify amplify those yes absolutely great jr a quadratic will amplify bigger errors error right it's like it's very much like you know the way you deal with children a good parenting says that don't shout at your time point out every mistake when they make small mistakes you look the other way around you try to ignore it but you rain down on them only when they make a mistake that can really hurt them right so big mistakes and when you do that you are much more likely to get something like that. I always try to think of that analogy but anyway I don't need to think of an analogy. It turns out that a guy, Gauss, has a theorem called the Gauss-Markov theorem right which says that of all the ways of quantifying errors this is the best so long as certain assumptions are met in those assumptions we weren't going to it today those are assumptions of the most capacity of that of the residuals and things like that it's almost cadasticity and so forth so we have covered that in ml 100 course do you guys remember that we just did it about a month or so ago. Those of you who took the course recently. And we talked about the fact that residual should have a bell curve distribution. If you look at the size of the residual, right? They should be the count of this, the density of the r i cumulative pound it should be like a belka distribution and there are a few such assumptions the five gauss mark so i won't go into that but it captures the gist of it that this is the best way of doing it this is the total error when we reduce the error how do we reduce the error so suppose you have a lot of points here and you try to fit a best fit line i could take this line i could take this line i could do this i could do this i could do this you realize that for each of these lines which is a hypothesis there is an error right so for every hypothesis there is an error so suppose this is a hypothesis h there will be a certain degree of error you're trying to find the line of least error that would be the learning but you want to learn as fast as possible so how do you go let's say from a hypothesis that is this let me say hypothesis one how do you go one to the best hypothesis two what is the journey from here to there what is the shortest journey from one to two what is the process to get there anybody remember the word how would we do that excellent so we do you gradient descent so a silly way would be to search for every randomly pick millions of hypotheses in this plane, millions of lines and compute the error. But there is a better way which is gradient descent, which comes from the fact that associated with every hypothesis we have certain degree of, we can represent in the hypothesis space let us say x1 x2 if you look at a two-dimensional data you can do the error and not not x1 x2 I apologize the beta so suppose you write your hypothesis the equation of a line as beta naught plus beta 1x then what will happen is in your hypothesis space which is the beta space space I always call it the parameter space of the hypothesis space or parameter space hypothesis is more intuitive to me because every point in this plane every point here represents every point represents our value of beta naught beta 1 and associated with it is a certain degree of error what we are trying to find is the point with least error this is a solution this point this best point a best hypothesis corresponds to the best line here. It is corresponding to this line here. And so the way to do that is to follow a process of gradient descent, which means walk down this error bowl and come to this. So we won't recap that, but just to remember that this is the process. So just to summarize with regression or we have the two main parts of machine learning. First is quantify the error. Jay Shah, MA, RNIDHAS GHARANAMANI, Quantity five. Well, my spelling is atrocious quantify the error. Jay Shah, MA, RNIDHAS GHARANAMANI, Quantity five it and then gradient descent error back prop or gradient descent to lowest error error calculus people use the word minima right to the minima and that is machine learning it's a two it's a walk on two legs as far as this sort of regression problem is concerned you first quantify the error of a hypothesis and then you take the gradient descent and this hypothesis in this particular case is based on a beta right whatever the value of your beta is beta naught beta 1 and then you take the gradient of it to find the point of least error so that is that so now comes this another problem you can make a hypothesis that is too simple of hypothesis that is too complicated so suppose i have data and let's take polynomial hypothesis so you can say that y is beta naught plus beta 1 x plus beta 2 x square up to the nth degree polynomial right now the question is what is n what is best and that is a hyper parameter of the model isn't it you can fit you can find the best hypothesis given it if you take a line you can find the best beta node beta one so suppose your data is given like this and you make a linear hypothesis suppose you make a hypothesis this is again i want it to be a fast quiz sorry let me take this suppose you take a hypothesis like this you can convince yourself after some time that this is the best you can do with a straight line you realize that this is not as good as a model that would have made a prediction like another model which went like this let me call this two let me call this, I'm sorry guys. Let me call this one and let us take a third model of a different color. What color should I pick this one? This, let's pick this this so imagine a model that we drew three which of these is the best model for this data which is the best hypothesis for this data would look like the green one, number two. Green is the best. So let us say that this is the best. This is the green one is the best. Now, why is it the best? It will have the least residual error, isn't it? The residual error after fitting it would be the least. Now what do we say about one? It seems to, compared to the best answer, so let me state this in a point that is important actually in practice. Green hypothesis is is closest to ground truth closest what i think i have closer to ground truth ground ground truth being the data you know you can look at the data and tell that really green seems to be better fitting to whatever produced this data. What about red? What sort of errors? How do you describe the red hypothesis, the straight line hypothesis? It is suffering from errors of what type? Bias. Bias errors. So it has high bias. It has high bias it has high bias errors bias errors are errors that come because you have made a hypothesis that's simpler than the ground truth and what about the yellow what about the yellow line guys what sort of mistakes is the yellow line making I want someone else to answer this time okay who would who would tell what is wrong with the yellow line the yellow hypothesis high variance I very very good so the yellow line suffers from errors of the high variance kind yellow high variance errors. If you were to so that that brings us to the next question. If you were to make the bias variance plot. How would it look suppose this is the degree of the polynomial as the complexity increases or you go to higher degree polynomial. How would the, in terms of the validation error, so in terms of bias, how would the bias errors go? The bias errors would increase or decrease as you increase the degree of the polynomial? It'll decrease. Increase. Why? Because it's flexible. Why would it decrease? Would somebody like to explain? The residuals will be smaller. Like a fitting of more data, right? Overfitting. Yes. So bias errors will keep going down. And this is the error. What about a variance? A variance errors. As you increase the degree of complexity, What about a variance? Variance errors as you increase the degree of complexity. Of a model. What happens increases increases increases. Okay, so we say, yeah, compared to the ground truth after certain vials, this error will start going down. Now, do you remember guys, what's the relationship between total error bias errors and variance? This is a little bit mathematical, but I'll just stated the total error total residual error after the learning is based upon what all things there is bias errors there is variance errors and there is the irreducible noise right or the things that you can reduceable reduceable errors what are irreducible errors unavoidable just noise in the data or from variables that you haven't accounted for so for example if you're looking for sale of ice cream on a beach and the only predictor you're using is temperature, then irreducible error in your model is coming from the fact, for example, that you haven't accounted for weekdays versus weekends. Most children are on the beach on weekends than on weekdays. So you will get a huge band of numbers for a same temperature and the ice cream that you sell on the them for that value because we can you would sell more and we can sell less but your data won't reflect that so it comes from what you don't have what you want to learn from because it doesn't know so what's the relationship between these two it is bias square plus variance plus this is a technicality why you're squared it has to do with the definition of bias itself right so what will happen this cannot be gotten rid of this is this and at the end of the day you can minimize your this there will be certain amount of irreducible error and this is your minimum error that you can have are we together guys do you see that you'll reach a point minimum error and so what do you do you need to dial up the complexity of your model till you hit upon the minimum error and that will be closest to the ground truth it's your sweet spot and it shows up in all sorts of models integration you can do this thing about polynomials if you're doing things more complex things and I'm just jumping ahead you'll see in this workshop we'll do support vector machines but there we have a cost factor right how many errors we are willing to tolerate how many points we are letting stay between the maximum margin hyper planes and in the case of neural networks it is there lots of parameters by the way I don't know if you guys are following one of the talks I'm going to give once every Sunday we give a talk so one of the talks I'm going to give is the latest transformer models gigantic transformer model that open AI group has created called the GPT-3 if I am right it has let me think maybe I'm saying off the top of my head but I believe it has one just unbelievable can you guess how many parameters there are those of you who who know Neon Networks, maybe Shankar, can you guess how many parameters are there in GPT-3? I don't know about GPT-3, but BERT has like 110 million parameters. like that it is the most unbelievably gigantic model anybody has ever created and it is uncannily accurate as far as we can tell like we are all impressed by the performance of that model GPT-2 was supposed to be so good as a machine learning model that open AI actually withheld releasing it to the public because of its potential for abuse. They felt that it is so uncannily human, GPT-2, that it could be used to create bots and articles and fake news and so forth. And then ultimately they revealed it and now we have something that's practically a hundred times more powerful than GPT-2. GPT-3 just came out last month. So we are building models that are extraordinarily complex. So how do we tame complexity? So for example, how do we reduce this variance errors, overfitting errors, right? Variance errors are overfitting errors. Another way to think of variance errors is that when it when the data over fits I mean when the model over fits the data it has a tendency to just memorize the data right instead of generalizing from the data so in machine spelling is atrocious. Let me think. Memorize, S-E or Z-E? I don't know. Memorize the data. When the model will memorize the data versus generalize and learn from data, generalize and learn from data. Machine learning's goal is to do this and not do this. This is the worst thing that can happen. When you take very complex models you always have a tendency that the model will overfit the data which is another way a simpler way of saying is to memorize the data. Now we all have been students you remember that if you don't want to learn a subject what can you do? You can find the so-called important questions that tend to come in the exam and just memorize the answers to that and when you go to the exam, hope those questions come and you regurgitate the answers. So then you have just gained the system, you've gotten your A grade or whatever good grade, you passed, but you have certainly not learned. Same is true for machines. Machines don't learn if they just get away by memorizing the data. The more complex a model you make, the more it memorizes the data. That is the meaning of the variance errors or overfitting. It overfits the data. so then you use techniques like dropouts and so forth L2 regularization we will learn those techniques in the part as part of this course so I won't go into that so that is it bias and variance errors now suppose you take a linear model right suppose you have a data which is actually like this. I'll ask you a trick question today. Is it therefore a bad idea to take a high, you don't know the ground truth right so you suppose you take a pretty complex model think of a neural network or something but i'll just keep it simple and you take a model which is a polynomial polynomial of x to the degree i'll take a large value, 10th degree polynomial, right? So it is a beta naught plus beta one X plus beta 10, X to the power 10, right? So this is a fairly complex model. As you know, this model will reflect a 10 bands, right? Curve with 10 bands. So the trick question that I would like to ask is, what should you do? Is it a bad idea to do that? Will it always underperform a model that is just beta naught plus beta one X? Significantly underperform or what will it do? Which is the best model, A or B? So a simple answer to that question would be, if you get a data like this, which model would you use, A or B? A. For that data, I'd prefer A. A, because A would be something like this, right? What about B? What sort of a model will this produce? If the data, and here is the part that is something worth asking, is B bad for this data? Who would like to answer? It risks overfitting. It might overfit, yeah. It might overfit. It might. Where does the might come from? What determines that it will overfit? Complexity. Well, the model is the 10 degree. We already said the complexity. It is a complex model, but when it overfitted, when... suppose I give you a data set of only 50 points, what will happen? What will happen? When it comes to test data, test data, the fitting won't be good. Yeah, that's right. So you have to memorize the data. They have big oscillations in the data. So this is the traditional way that you get, but here's the part to it that's very interesting. interesting how can you have a tenth degree polynomial and still get a good mark what how can you do you can set a lot of the betas to zero minimize them you can get one answer is you could get more data. That's good. So suppose I get instead of 50 points, I get a 10 K points. Then I could use this model and perhaps get away with it. What else can I do? Regularization. You could regularize. Excellent. Regularization is something people obviously will cover in this course. So if you don't know, that's OK. It's a way to tame the data. If you can't go with more data, you can get away with regularization. So today, what we aim for is we try to do both of these. Because we realize that there are many situations where the underlying reality is complex, but you do know it is much more complex than a straight line, but you don't know how complex it is. But if you're talking of a space that is 10,000, I mean, suppose it could be best represented with a polynomial of 10,000 degrees versus a polynomial of 20,000 degrees. Either of the two you could not have guessed. So what you do is you take a model that's fairly complex, but you can tame it down. And so you can make it into a red line. Let me, if I were to give you the effect of regularization. You can tame it down the red line to that line, which is very close to the yellow line by either regularization or by throwing a lot of data at it. And that is the game we play today. The very, very complex models when we deal with, let's say the GPT-3 that just came out what did the GPT-3 have for lunch it ate pretty much ridiculously large amounts of data it ate the entire Wikipedia but that was nothing there is an open crawl project I think some of you know open crawl project is perhaps you know the the largest open source initiative to essentially crawl the entire internet. So basically much of the internet that has been crawled in an open source way is available. And it is like huge amount of data, right? You crawled, it learned and ate that for lunch. It ate the Wikipedia for lunch and it ate a whole lot of other sources for lunch. Right. And it learned from a lot and lot of data because you know when you have the parameter space so large you have to do this. Does it have regularization? Of course it has regularization and all of that built in. And that is the state of the art. When the problems are truly very complex, language problems, things like that, natural language processing is a complex problem human language is actually quite quite complex and unpredictable simple models linear models don't quite work and so you take very complex models but you need to tame it down with a lot of data and a proper regularization technique techniques so we have had an hour any questions though I will stop then I want some basic feedback would you guys like this format of interactive so we'll continue this guy is every week I'll try to continue this every week and we'll go through the whole of data science. Pure perspective. I won't really teach very deeply, but I'll assume you know it, it would be a refresher and we'll cover the whole subject. Is questions are typical questions that you're asked in interviews. They're very simple direct questions for keep popping up in interviews. They're very simple, direct questions for that keep popping up in interviews. Anybody who has been to data science interviews and would like to confirm or deny that these questions come. Nobody has been to data science interviews. Okay. All right guys, so I'll share these notes onto our slack and let's close the session here. Okay, there was one other interesting thing I noticed in the book about the difference between prediction and inference and I thought it was good how it made a distinction that the prediction just you get the number out and you don't care if it's a black box while the inference is that you care about the relationship between predictors and the result that's a very good point yes yes you care about the model you want to be able to interpret the model so people these days well the book does make and statisticians have always pointed it out that it is just not enough to have black boxes we We need to have interpretable models, something that you can decipher, see what is happening, infer out of it, infer a hypothesis out of it. We live in a world in which the world has become very complex. So the thing is, whether we like it or not, for complex problems, we bring very, very complex tools. We bring all this support vector machines and neural networks and so forth to the game in random forests and so forth those become black boxes so the new push these days is the opposite we say that during the learning we don't need to build interpretable model let us learn to make the most accurate model and once we have made the most accurate model, let me just put it this way. I think we talked about this at class, that if you have interpretability, so on one side is how interpretable it is, how interpretable it is, and how accurate the model is, how powerful the model is. So what happens is that the very powerful models are like this very poorly interpretable black boxes apologize guys i'm having a lot of allergies and simpler models tend to be not so accurate. They're highly interpretable. A linear model, for example, or a decision tree, they're highly interpretable, but they don't give you outstanding accuracy for complex problems. So what do you do between this and this open interpretable model? This is a thing, and those of you, some of you came to my boot camp we went into it quite detail the new thinking is you can actually get away by not compromising on accuracy you take the data you build the most accurate model but your accurate model may have all sorts of problems, for example, and very genuine problems. For example, the ethical problems of bias, racial bias, gender bias, and what not, like discrimination. So you can't trust this model as a black box. So what you do then is you create, you use it for making predictions your y hat but you also create a interpretable approximation to it local approximations you create local approximations to it so that and that is the whole field we will get into of interpretability and feature significance and so on and so forth and then you check you check whether there are biases and problems here in this you use another so you create a machine learning model to interpret a black box machine learning model an interpretable machine learning model so this is what we are doing these days and this whole movement is called xai again a topic we'll cover those of you who are coming to the boot camp in ai you will get a full dose of it nice thanks kate for bringing that up. Anything else guys? Otherwise I'll stop the recording now and I'll post the recording, keep it there for a couple of weeks and then I'll take it down because it's an interactive session. I don't want it to be there for too long just to protect your privacy and but if you want to watch it or review it do it within the week guys okay so uh stop sharing stop recording where do i stop recording stop recording