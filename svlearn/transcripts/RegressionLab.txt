 We are ready to go live. It is time now. We are going to start. Kail, is there any improvement in the quiz engagement score? Just a few. There is slight improvement. Okay. Quiz number four is standing at a total of? Right now I think it's around nine attempts, nine or 11. 11 attempts. nine or 11. 11 attempts. That's good. All right. Let's start now. So, folks, we can start the recording. Oh, yeah. Yeah. So, today, we are going to do our labs on multilinear regression, regression in many variables. That is one of the topics for today. Besides that, we will also do univariate regression on a real world dataset, the old faithful geyser in the Yellowstone National Park in the US. I'll give you guys a background on it. It is the Yellowstone National Park. And for those of you who are not familiar with it, which I think most of you are, but it is a geothermally active region. It's a volcanic, live volcanic sort of a lot of plate teutonic activities happen there. And there's always the, they're supposed to be the molten lava just sort of below the surface or about to erupt or something like that. It's very, very active region geothermally. One of the features of that region is that you have these geysers. Geysers are this water shooting out of the ground to great heights for some time. Think of it as a pressure cooker valve. When inside the water pressure is high, just shoot salt through these geysers. The geyser runs for some time let's say about 20 minutes or five minutes or whatever it is and after that as the pressure pressure releases itself the geyser will quieten down and stay quiet for some duration and then it will again erupt the old faithful geys geyser gets its name from the fact that for many many decades it has been faithfully erupting every approximately an hour. It is between 40 minutes to two hours it erupts. Now the waiting time for the next eruption is dependent upon how long the previous eruption was. Sort of makes sense to have from a physicist perspective, physics perspective. If a geyser has been running for quite some time, a lot of pressure is released and so it takes longer to build a pressure and the next eruption to happen. So in this dataset, we'll have one predictor, which is the duration of the last eruption. And what you have to predict is how long you have to wait till the next one. The waiting time is the predictor for the next one. But the reason I bring it up is sort of univariate. We did a lot of very hard datasets. In real life datasets, it are somewhat simpler. And we are going to use this as an example of one of the data sets. So this is the old faithful geyser if you're looking at my screen. We can't see your screen. And I think did I remember to do the YouTube thing? Oh, yes. I did that YouTube thing, okay. So let me share. That is it. Now, are we sharing the geyser picture on my screen? Yep. Yes. So this is it. This is artistic rendering of how the guys you can see this water shooting off to great heights. One of the great features, and of course in the US, everybody knows about the Yellowstone National Park. It's one of the most magnificent national parks. So worth visiting. So I will load this. So this notebook is actually now increased the font size and go. Sorry. And go. One more time. Okay, there you go. And there you go. And there we go. So this data, is it readable now to everyone guys on your screens? Considering that we are a live stream. It could be made a little bit larger. Yeah. Okay, there we go. So this data is very simple. We have eruptions. This is the duration of the eruption, I presume in minutes. And how long you have to wait for the next eruption. Those of you who are in California, you probably know that we have our old faithful guys are here also in Bay Area, in Calistoga, Napa, Calistoga area. It's very, very nice actually. Kids totally enjoy it if you have young kids to go to. So the eruptions of course are not as long. They take three, four, five minutes, and then you have to wait for some time for the next one. You can clearly see that the eruptions are long. You have a longer time to wait. If eruptions are short, you have less than an hour to wait. So this is the nature of the data. Now, some of the best practices that we have been developing along the way, I will assume hereafter that we always do. One of the things we learned is it is a good idea to use a scalar of the data. And standard scalar is quite effective if you don't have outliers in the data. So you can verify that this dataset doesn't have outliers. Actually, we'll see it in a moment. We scale the data. We look at its descriptions and so on and so forth. It's eruption. When do you scale the data? Like, after doing the analysis? Yes, see. Starting with the scale. No, no it's eruption when you scale the data like opportunity used to analysis or just yes see no no no you should always look at the histograms before deciding on the scalar so here i have not done because yeah so this data set do you notice that i have it here right um i have not taken the time to do a lot of uh I have not taken the time to do a lot of cyclic. I should have plotted this figure before doing the standardization. So what happened is that I knew I'm so familiar with this data. So I've done it sort of backwards. I standardized it, but I should probably have waited to do a data visualization before standardizing. So if you look at the data standardization, even it will not change the shape of the picture, it will just change the X, Y axis. So because I standardized the data, you notice that the mean is around zero. It is not very clear, but the standard deviation is one, and it'll show up in a little bit. So what do you notice that there is one cluster of values here and one cluster of values here and one cluster of values here right so you have either short bursts or long bursts of the old fight faithful guys that's pretty much two like groups of eruptions short and long if you notice that and you can see it, once you standardize the data, what do you expect the mean to be? Zero. And so it is 5.15 into 10 to the minus 16. Practically, that's the definition of zero because in floating points, it's almost never do you get zero. Standard deviation up to three decimal, two decimal place at least is one, right? So the data has been standardized. There are no null values in the data. If you plot it, it looks like this. You can see two groups. So later on when you learn about clustering, we will do clustering on this data too right so you will get two clusters one for short eruptions and one for long eruptions then today i'd like to introduce this very useful plot these are called the pair plots the two variables eruption and waiting what it happens is this is this is pretty much the histogram that we were looking at, except that the word for that is called kernel density plots, kernel density estimator plots. What it means is, I'll give you an intuition of it, kernel densities and kernels are things we will cover in great detail when we do the neighborhood methods. The word kernel is very central actually to a lot of things we'll do. But since we haven't done that. Is that a kernel function or is that? It is the same thing. It is, so one year, this is an intuition I'm giving. The intuition is, if it wasn't just this few hundred rows of data, what if the data was asymptotically infinite then what would be the distribution that you would see density estimation or density distribution so kernel density estimation is essentially that intuition it estimates that distribution function which would have been there assuming that the data set was very, very large, how the histogram would have looked, histogram would have looked like this, right. So, with finite data, it is an estimation of that it's a continuous, continuous curve, that's why it's called kernel density, or a probability function that estimates sort of how much that will be. So how does it extrapolate, some way it's trying to extrapolate, right? Yes it does and we'll learn about that. What happens is the kernel here typically these are Gaussian kernels. How do you fit these and put them together? We'll learn about it and the neighborhood methods will do. But while that topic will come, it's not that hard. See, you notice that this picture here, right? You can see in the two dimensional variable in the two dimensional space, the contour plots of equal density. So this is it. It's a beautiful concept. At this moment, I thought whether to introduce or not, and I thought I'll introduce introduce it because it's just such an important visualization. It's good to introduce it early. The top right hand is of course the scatter plot we are all familiar with. This is the wait time, eruption time. As you can see they have bimodal distributions, short, long, short, long. And when you look at the contour plots, you can literally see the two clusters form here. So it gives you an insight into the data. Now let's build a regression model where eruptions, like how long did this eruption last, and is the predictor and the target variable is how long do I need to wait if I've seen an eruption of five minutes or four minutes now how long do you should I wait by the way it's really very when you take your children for example here to Calistoga the first thing your kids will do is they will watch the with great excitement they'll watch the eruption and then the very next question they'll ask is okay when do i get to see the next one right so well this is going to answer exactly that question so by now you should be familiar with it we build a linear regression model a question yes is the other way also the thing holds true like if i wait for this long can i create like oh yes yes yes of course so why an x and x and y right how long the next um that is a good thing why don't you do that but i think people haven't gathered the data this way that if i see the eruption if i wait certain time how long is the next eruption? I don't know whether this data is sequential. If it is, then you can use it. You'll have to look into the original source of the data. If that is true, then that could be another good analysis to do. That's a good idea. Right, yeah. a time series kind of thing right here not just uh looking over time but yeah so look into the source of the data whether it's sequentially given or not so building a regression model by now guys you would all agree that this line of code is pretty easy and straightforward for you to get on this big screen is it coming out legible from the back row uh premji are you able to see the code clearly i'm looking at my computer oh you guys are looking at your computer okay yes good but on this monitor is it's big enough or do i need a bigger monitor i need to make it bigger i do need to make it bigger. Okay. What about now? Now, this is perfect. This is perfect. Okay. So look at the screen then, so that I can see your faces. Here we go. So this model, I hope you would agree, is straightforward. Then we build a prediction model. By now, this is also very... So one new thing I wanted to tell you. See, whenever you're predicting, typically we look at mean squared error, which is good, but sometimes the root mean squared error is more intuitive, right? So suppose somebody says that the house price was 4,000 and your root mean squared error is 100. You know this pretty accurate estimation of the house price right so that way it is good to know what the root mean squared error is so and the way you do that square root mean squared is set squared is equal to false if you set squared is equal to false in the mean squared error function it you look at that so let's look at the residual analysis what do you say guys in the residuals do you find any uh choice do you see homoscedasticity or heteroscedasticity almost almost cadasticity isn't it you don't see the variance of the residuals change as you move from left to right, right? Move along the X axis. So there is a rough homo skidasticity. So this looks encouraging. What about the distribution of the errors? Is there any skew in the distribution of the residuals on the side? No pronounced skew, right? Very modest, modest if any skew and you can do that by computing the skew but i'll leave you to visually just inspect and say yes there isn't much skew so let us now which what is the final thing we do we visualize the model predictions isn't it especially with one-dimensional data it's easy to do. So there we go. These are your predictions. So is this a good model? It is. I leave a homework for you. And what is the R squared, by the way, that we got? Do we remember? 76%. 76% is it good or bad? Okay. Yeah. When you look at the data, you know, you have to tone your expectations. The data is like this. So there are things that you don't know, right? There's geothermal activity, which are actually contributing to it, and you have no access to those variables. So this is the best you can do with this. So it's a pretty good model and you can verify that. Try building, and this things, I don't know guys, take this homework seriously, because when you do that, a polynomial regression, you have the code, you can try it out in five minutes. Try it out, see, does it give you a better model? And then ask yourself, should it give you a better model? Right? So I'll leave that as an exercise for you. So this is the old faithful geyser. So one quick question here. There's also another possibility that happens. So I'm projecting a little from this visual to frame my question there's a possibility that there's a structural change in the data right for a moment let's assume the lower cluster had a different gradient compared to the higher cluster if you had to fit a single line through them yes very good question but if you separate single lines that is right that is right we haven't discussed an inference we haven't how to find that what how to create that yeah so the thing is when you do polynomial because it gives you the ability to flex it can actually come up with different slopes and different regions try doing that but more single polynomial will actually fit it reasonably yeah it will do but there is identifying a split in the data and saying, let's fit two linear models. Yes. And then they have to join the two somewhere in between somehow. So those are called, and we will come to, remember I said that the word kernel is an advanced topic we'll do later. Those are called kernel based regression models. That's what they do. That's one way of doing it. Another is to do something called spline, make local linear models and keep joining them. There is yet another way of doing it, which also we will do, again, to do with kernel methods. It is called kernel nearest neighbor, and there are methods like lowest. What you do is you look at local regions and you make tiny regression models in little patches and then you stitch those patches together. When the data is highly non-linear that is effective. In fact that old technique is called Loess. In geology that was very often used the word Lo comes from there. Okay. So there are many, many techniques we learn. Remember, this is the beginning of machine learning. We have another eight months to go. Right. So the comments on these observations, it reminds me of zero data, like actual sales sales and things like that which had that characteristic like at different price ranges you would have different factors clearly see the distribution distribution the few million dollar houses it was cool nothing mattered how far you were away from the whole neighborhood. Was it the lower ones, you could clearly see that number of bedroom, those kind of things mattered. And over there, it was completely different. They're structurally different. They're structurally different. You could see those gaps between one price range to the other. Yeah. So that was . This is a very good point you brought. Let me further elaborate. For those of you who are remote, Sachin brought up the point that the Zillow housing data shows pronounced nonlinearities. And different regions of the feature space, there are different kinds of activities or factors involved. This actually goes to the heart of machine learning. One of the things we will learn is real life is highly nonlinear. And so most good predictive models, they are telling very different stories in different parts of the feature space. So the models are nonlinear. And so when you ask which features matter, like people often say, what determines house price? There is never one answer. The answer depends upon which price range, which geography, which part of the feature space are you looking at? Because there are different factors in play in different. And people know that. People who are in real estate, they will tell you that real estate is a very local market. The forces are very local. That is why that real estate is a very local market. The forces are very local. That is why local real estate, there's no one global real estate company that has the answer to all problems, right? Or has perfect. They're all based on local real estate intuition. And it's the nature of it. In diseases, everywhere it's true. For example, when you look at diabetes, if you ask in children, pediatrics, or up to age 15, what is the most common cause of diabetes, those factors are completely different for what is most common cause of diabetes in people over 40. Right? So the game changes, even though we build one model, one of the answers to this question, why are most models nonlinear, effective models nonlinear? Because you need nonlinearity to capture the fact that there are different realities in different parts of the feature space. Right? And that's a big topic. In fact, that's where we are heading. In another two weeks, we are still in the linear world. In two, three weeks, four weeks actually, we'll be completely in the nonlinear world. We have a lot of things to do, kernel methods, support vector machines, ensemble methods, boosting, bagging, forests. right? There is a, and then of course the neighborhood methods. There is a lot that we are going to do, but that will come in due course of time. But the good thing is, the old faithful guys I like, because it's a really neat, clean data set. It has a nice story to tell, and very easy to analyze, isn't it? There's another data set, which I was, which reminded me to take a look at is basically the fat the correlation between the fat and the waste size and what happens is that every waste size so there is as the waste size increases the percentage of fat also increases but there is a distribution at each one of them has its own maximum minimum so there is a distribution of people like certain waste sizes how much they spread okay that distribution itself has a nice what you call patterns in the distribution itself okay so as people tend to have bigger, the fat side of it starts to dominate. Okay. Versus the thinner waist, the distribution of fat, it basically becomes like pretty normal. Yes. In the sense that it's muscle and it's a very interesting dataset. I can speak from personal experience. All right, guys. So with that, so if you look at this visualization of this data set, yes. If we have this here, the other one at the end. Yeah, so if you look at the way the distribution is, and if you also look at the end yeah so if you look at the way the distribution is and if you also look at the residual plot it kind of even though residual looks random yeah but from the distribution perspective it is yes yes yes yes yes that's right i mean you can imagine these little residuals falling off this and you can see that that is what the other plot is showing yes yeah so try this out so guys now that we have the geyser remember that here life was very simple how did we build a model building the model for us was model.fit linear regressor regression model.fit so this is we are using a library scikit-learn library and in real life you often use libraries but then it is also true that you need to learn how to do without libraries just from first principles how would you how would you do the learning we learned all this theory about gradient descent and so on and so forth so what does it mean for us to do this by hand so we are going to do the same data set simple geyser data set but this time around we will use gradient descent right so i do the basic stuff here things that you are familiar with. Here, the same data set, I use it, scale it, and so on and so forth. The same eruptions, mean is zero, standard deviation is close to one. Guys, do you see it? Yeah. Right, and now one of the things that I do is, so these little tricks, you know, you'll see me continuously introduce little tricks here. Yeah, what happened? This table became, do you see that this Pandas table, data frame table, now looks a little bit more interactive. Did you see the difference guys compared to this? here? I put some color here and so forth. So that you can do by adding a style. I've added a little bit of CSS style to this table. How I've done it, you can see, and I'll leave that as an exercise for you to go and look at support vectors common notebook, how you can do styles of course you should have your own styles my style i prefer these colors which are off colors typical corporate styles are blue but i spend my whole day in corporate world and all i look at is blue different shades of blue so i don't feel like looking at that anymore so i've made all of support vector colors anything but blue well there is the blue but this blue is midnight blue it's supposed to be in the color spectrum compatible with it so well okay here it is mostly reddish I like salmon color this is the scatter plot of that you're familiar with this is the eruptions waiting time again you're familiar with this is just a recap so now let's go to gradient descent the beta next value of the parameter is the beta's current value times this right this is it this is just a recap we will just take the simple unregularized regression a simple linear regression some squared error is enough then if you just work out it is some squared error, when you take the gradient of the loss function, which is this with each of the parameters, if you work it out, you can see that it works out to this. Almost literally from here to here, you can see it if you're good with your calculus, immediately you see it. Do you agree that the partial derivatives with respect to beta naught and beta 1 come to these two things i'll give you i'll give you a few seconds to absorb it and agree that that's obvious do you see that guys yes that's right so we move forward and so when you plug it into this these values into the equation here this equation what it leads to is this all we need to do now is to run the gradient descent. Now gradient descent, we will define, we will start with the learning rate, which is very small. Actually here I read 10 to the minus 5, but that becomes too slow a learning, too small learning in a single step. I wanted it to be a little bit faster, so I made it 10 to the minus 4 here. Also, typically beta naught, beta 1, you start with some random values. So what you should do is remove my 4 minus 4 and start with some random values. When you start with some random values, then it will gradient descent. Now for reasons of data visualization, because you will see a plot that I visualize here, I have deliberately started in such a way that the plot that it will draw will bring the intuition out for you so i deliberately forced it to start at this point but for no rhyme or reason just for visualization perspective now we need to discuss a concept called epoch in machine learning during the learning phase we define an epoch is one com one journey through the complete data set are we together when you have visited every point in the data set right you you have completed an epoch now in the gradient descent step if you look do you notice that we sum over every single point at each step? If you look at this, sorry, look at this, we sum over every point when we do the next step. So what does it mean? At each step, we complete an epoch. For the training data. In the training data, yeah. Because in each step, we are computing the gradient by summing over the errors of each of the residuals of each of the data points. Therefore, one step of gradient descent is one epoch. Now, there are many kinds of gradient descent. What I've been calling gradient descent in the more formal language would be called batch gradient descent. A batch gradient descent is a descent in which the entire data set is one batch the opposite of that is stochastic gradient descent in stochastic gradient descent one step is just one data point you don't have a summation here summation is missing you just learn from one point and its mistakes and then they are in between there is a method in between that doesn't do these two are extremes to take the entire data set or learn from one step learning is from only one point the alternative is to learn from small batches of points those are called mini batch so suppose you have 300 points you will create many batches of let's say 16 points each right and then so then your each step will compute the loss from 16 point next step will compute the loss from 16 points and it will learn from that so there's a lot of learning by the time you run through the whole epoch right approximately 20 steps you would have taken to learn through about 300 uh maybe 19 steps you would have taken to learn through those 300 odd points right if your mini batch size is 16. so there are many different variants to gradient descent the one that we will look at here is the very simple one. We sum over all the points, right? It is the so-called batch gradient descent, full batch. Now, this word is a little bit abused. The technical distinctions are batch, stochastic, and mini-batch. But a lot of people can, because of colloquialism, they will call mini-batches batch gradient descent, especially in the deep learning community and it becomes rather confusing sometimes but remember this these are the correct terminology so now whatever we wrote here i will write rewrite it in code so i said alpha will be very small i'm starting alpha as what does this line possibly say alpha is equal to power 10 minus 4 what what do you think it does 10 to the power minus 4 exactly thank you so it is alpha is 10 to the minus 4. then beta 0 beta 1 are the two parameters of a line remember slope and intercept i could start i could take initial value to be any random thing but i have deliberately deliberately taken four, that is purely for visualization purpose. You should change this code. Your homework is to remove it and just actually initialize it randomly. Epoxy is 200. What does 200 say? We are going to take about 200 steps. Why? Because simple data set, 200 is already enough, right? It's a stopping criteria. You'll learn for 200 steps and stop. Now you break the data up into X and Y, which of course is straightforward. You break it up into the arrays. And then what are the values that you have to learn? So alpha is this, beta not beta one is this. We are going to run 200, excuse me, epochs. Now what do I do? And this is the heart of it. This is the inner core of what we are going to do with the gradient descent. The first one, I just create a data frame in which at each epoch, I will store that whatever the value of beta not beta one we have learned so far, I'll store those and I'll store the value of the loss function like how what is the loss function so what should happen as i go from epoch to epoch i should see an improvement in the value of beta towards the final answer betas and i should see a decrease in the loss isn't it you wouldn't expect to see increase of the loss function blowing it up you should see a decrease of the loss function, blowing it up. You should see a decrease of the loss function, isn't it? Because after all, our loss surface is parabolic, isn't it? So it should see a decrease in the loss function. So let us now, please pay attention to this loop. It is the crucial loop. For each epoch and the number of epochs, so for 200 epochs, what do we do? Let's look at this step and tell me if it looks confusing. Compute the gradients. Here are the gradients. So you notice that this is sum over yi minus beta naught minus beta 1 times xi, right? Is this now, compare this to this statement. Beta naught, the next step of beta naught is this now compare this to this statement beta naught the next step of beta naught is this beta naught plus alpha right so no no first not first the gradient this is the gradient gradient is equal to minus minus the sum of y i minus beta i minus beta 1 beta naught minus beta 1x please pay attention to this equation here is this exactly what i have written in code uh here go ahead somebody has a question uh so it's y i minus y i hat the whole square right so when you differentiate it shouldn't there be a two in front multiple of oh yeah i i sort of uh the last yes that is a very good point i sort of glossed over the constants right the point is too there should be a two here but i have glossed over the constants right so if you want to be very precise, we can change our loss function. Where is it? Least square is equal to sum over. So let me just make it like this. Frac one over two is equal to once again, this frac. Okay, so let us be precise. Like people, when they're explaining, Okay, so let us be precise. Like people when they explaining they tend to gloss over it, but why gloss over it? Okay, now are you feeling better? Look at this, I added a half here. So then when you take the gradient that there won't be a two here. Okay. Yeah. This this constants one over n, one over two, these things people tend to gloss or in the gradient you can apply a two, but it won't affect the results basically. Okay. Okay. That is why you gloss over these things, but it is a good point that you can quarter so compute the gradient so we now have the gradient do you notice that this looks exactly what you thought minus some over why I minus beta not I when is. Raja Ayyanar? bit of a beta one X one X I. Raja Ayyanar? Does this look intuitive guys anybody does it, this statement, in view of this statement here? Look at this. Minus sum over this quantity. Yes. If you're not understanding it, let me know, because I have written it in such a way that the code is almost identical to the math. Likewise, the partial derivative with respect to beta 1, d beta here I mean, gradient of the loss with respect to beta naught. I have a question. Yeah. Zip will tell us if x and y, the cardinality is different, right? It basically makes a partition pair of yes yes yeah basically zip sees like a zipper you know one one from this side one from this side one from if the cardinality is different it will go it will it will give you i don't know whether it will throw an error it i think it will stop at the shorter one I think it will stop at the shorter one. But check it out. It's a good question. I mean, the best thing is to do a tiny experiment and see what happens. So there we go. Now, the next step is once you have, look at this equation. Once we have the gradients, you can take the gradient descent step. This is the gradient descent step, right? So let's go take the gradient descent step. Here it is. the gradient descent step, right? So let's go take the gradient descent step. Here it is. The gradient descent step beta naught is beta. Next value is beta naught's current value minus alpha times gradient of the loss. Like this equation, guys, I hope this is pretty self-evident, right? This is literally the definition of the gradient descent. Isn't it? Yeah. Yeah. So this is it. And then then after that what do we do now that we have new values for beta naught and beta 1 we can go compute the new loss what is the loss now that we have improved values of beta naught beta 1 okay what is the loss here and we store all of it in our table this intermediate stable intermediate values table which we can see what it is by the way whenever you have a table to display with floating points it can be very annoying you can get values till five six seven eight decimal points and it becomes messy so it is a good idea to contain the precision limit the precision to a sensible value. Three digits is more than enough for us to do. So I put the precision here for three digits. So it's your choice. You can make it two, four, whatever it is. Now, do you notice that beta naught starts out with close to four, but then it gravitates towards 0, isn't it? Do you notice that beta naught gravitates to close to 0 and beta 1 gravitates to 0.9, according to our learning. Now, what did scikit-learn have to say? Let's go and see what did scikit-learn have to say? Let's go and see what did scikit-learn have to say. When we did that, do you remember what was the parameter, what was the beta not beta when we found? Look at this. Does it agree with this? Beta not, is it.012. And what we found is beta naught is 0.016. If we had run it for another few hundred epochs, it would have converged there. And slope is 0.9168. And what are we getting? Slope is 0.913. Means I stopped at 200 epoch i'll give you a thing guys run it for 400 epoch see if the two answers begin to match but do you notice how easy it is that you can do the gradient descent quite literally the inner workings by hand you don't need to use a library and as i, it's good to develop this practice because as you get to more advanced topics, you will be doing it by hand. So let us here plot this. Do you notice that the beta naught values gravitate to its final answer? From four, beta one also gravitates to its final answer. What is the final answer for beta naught? It is, what do you see here guys in the graph? Beta naught his final answer what is the final answer for beta naught it is what do you see here guys in the graph beta naught's final answer is zero around zero and beta one's final answer is this this thing close to 0.9 yeah right this is it as you expect what about the loss function learning is proper only when you see the loss function go down but what if it starts going back up you're in trouble right or some strange thing happens you're in trouble so here is the loss function going down steadily yes no so here i took alpha is equal to 10 to the minus 4. what do you think would have happened if i took alpha is equal to 1. i will leave that big steps big steps of learning I'll leave that as an exercise for you to figure out right I mean we can run it right now you if you guys have downloaded the code you can run it right now and see what happens but I want you to do it yourself and see what happens now now I'll plot the contour surfaces. Remember, I told you that in the parameter space, in the hypothesis space, the error bowl, this error bowl is projecting down to the hypothesis beta naught beta 1 space, parameter space, right, what I keep calling the hypothesis space. And there, what will be the projection of these iso, the equal error curves here? They will be curves on this hypothesis space, isn't it? Right? So those curves are called contour plots, right? Or contour surfaces. In the contour plot, you see these lines, of course course because we are dealing with a two dimension so these are lines more broadly there will be curves there will be surfaces so these are the contour surfaces on the on this plane right so i've drawn it out all those this is real by the way this is actually from coming from the calculation we just did here and see those are the colors did you link that to some parameter for these colors to happen like that yeah so what you do is you typically pick a good color scheme viridis is the default one i believe the the closer you are in viridis right the more reddish or violet or um is it violet or what what color would you call the center? Violet, right? Violet, purple, whatever. And the outer you go, it goes through blue, green, finally yellow, right? So high values are yellow. And I have put the values also. Do you see that 100, 200, 400, 800, 1600? Now, you notice that it pretty much the error reaches the minimum error. Now, what is the minimum error we reach? Look at this. At 200 epochs, we are close to what is the error table showing? 52, approximately 51.3. That is where the minima is achieved. And you can see where the minima is achieved and you can see that the minima is achieved pretty soon right but there is a little bit more learning to do you know if you really look at it the curve is still going down so you can run another couple of hundred epochs and get the near perfect answer but this is good enough for us this is because i wanted i i wrote the code in such a way that it will run on all your laptops it shouldn't be that your laptop hangs and you have to go out for dinner before you can continue so now we keep the alpha constant here we keep the learning rate constant here that is right that is an important point we keep the learning rate constant which is not true when we do much more complex uh regressions or complex classifications and so forth you will see that we what you do with learning is a whole subject in its own rate it is like a specialized bit of research in its own right they're very interesting and fascinating things people do on how they change the learning rate as you learn. There's a lot going on there. There's all sorts of things like one-shot learning and this and that. We'll talk about it when we get to the deep learning. Now, remember, I keep talking of the bowl. Physically, my error surface bowl. How does it look in code code how does it look for the same geyser data set i visualize the code for you right so what happens is remember we started at four four we start here and the learning happens with this point you see the last point going down down down down down down to the bottom of the bowl. And it's projection on the hypothesis plane. Do you see these points? And in the beginning, do you see that the loss is decreasing by big amounts? I deliberately put points to show that it is like a rabbit that's hopping forward, big strides. And then the strides become slow. It starts creeping. The little rabbit turns into a tortoise right it's creeping forward towards the final answer right and the code is a little bit more involved i mean it needs understanding there's nothing machine learning about it but usually in this world you'll realize that once you get the hang of machine learning you'll start finding that you need to master data visualization, which is a subject in its own right. And believe it or not, like you may dismiss it and say, hey, that's not intellectually challenging. Actually, it is. You know why? Because a good data science, typical data scientists make, I don't know, I'm giving you current figures in the Silicon Valley. Out of box 250. If you're also a data engineer and a data scientist and you're a full package, well out of box half a million a year. If you are a data scientist and data engineer and a complete beautiful visualization guru, I happen to know from personal experience from some some people who are doing it can you guess what where do they fall 200 300 400 the outer box of com easily easily easily three four million So what I'm trying to say is that guys never, never underestimate the value of good visualization or beauty. In this field like mathematicians, it matters. So one way that I say it, don't take it too seriously. See kids, when they play with crayon, we consider it normal. But as adults, we are not supposed to play with colors, isn't it? The only profession that lets you play with colors is, guess what? It is mathematics, isn't it? This is the only one where you see me writing in colored crayons on the blackboard. If you're physically here, writing in color and on the digital blackboard here and in the visualization you can use a lot of colors judiciously so guys there is a the visualization is profound it can help make huge business decisions I have seen it in reality Florence Nightingale essentially founded the profession of nursing with one visualization of fatalities in the Crimean war, right? And that tradition has continued. When we say that a picture is worth a thousand words, it's really true in the world of data, even more true, because when you create models and you come up with insights and if you can visualize that insight it is worth a thousand words right so why don't you use this during your uh talk when you actually teach this concept the drawing point that you have done no because then i can't do dynamically right i can't draw the surface and draw lines, et cetera. No, no, but you can take this and then draw on it. Yes, I could have done that. I could have. I tend not to use pre-prepared material because I feel that learning should be spontaneous and it should be in the flow of conversation. So to preserve, it's a basic educational philosophy for me. So to keep it in the flow of things, see, I have seen a lot of material and all that material is dazzling and I can create dazzling material. The trouble with dazzling material is we get dazzled. Whereas learning takes place best in the flow of conversation with almost no devices. So when your younger brother says, explain something to me over a dinner table, you may at most have a pencil and paper in hand, but that explanation is gold compared to watching elaborate presentations on the internet. So that was one of the great insights. For example, Khan, Salman Khan. Before that, online education was all about well choreographed videos. I mean, it was almost becoming like Hollywood. If you go back and look at some of the Linda learning or LinkedIn learning now, they are India Bayou, right? They are very successful. There's no doubt about it, but it's a different educational philosophy. There, each of the videos is, there is a subject matter expert who tells you what to say, right? There is actually a choreography team there, there is an actor who will voice over the instructor who will literally master that script and is not a subject matter expert but will explain using the words the subject matter is given and usually the person who is teaching, for example, if you go to bio sector is generally young and pleasing looking, making just the right gesticulations. So the entire thing is very choreographed. But the trouble is, you can't ask a question in between. But Khan took the opposite approach, Salman Khan, he was just going to teach his niece. So he just took a little board and scribbled on it and started explaining that and that turned out to be perhaps the most profound revolution in education of the century. It completely rewrote how learning online learning should be done right and you can see the effects of that everywhere before that before that people used to do a lot of powerpoints and so forth but for example if you look at Andrew Ng today even he does that occasionally he will come up with formulas but then he will doodle a lot all over it in the Khan Academy, in the Salman Khan style. Now, I have thought about bringing that. One reason I don't bring even like Andrew Ng some things written is because I feel that it forces me into a structure that this and it prevents students from driving the conversation. Whereas when I'm in the classroom, I would rather that the students understanding or lack of it, the interruptions, the questions drive the conversation, the agenda. In other words, don't come up with what you have to teach in a rigid form. Let the questions in the class create a flow of conversations and go with that which is why I never really know what I will teach and how much I will teach or where I will put that freedom I have this curve I remember I kind of taught it just like five six years back using some the primitive whatever software was available that time and it took me a hard time i think i spent probably upwards of a week trying to get it running right just but the part was this the amount of detail that you have put in this piece right the whole so well the thing so fast because i was planning to do this exactly like my whole like last week I've been thinking of how to do it and now you've just shown it over here and I didn't know if during the class if you had done that, if others would have appreciated or they would have got lost in just the beauty of this. That is it. See one of the things we realize in research is education is they're very active area and a lot of opinions and so forth and obviously, I'm not an expert in education. My understanding of education is through 30 years of teaching postgraduates and 30-32 years now but and you learn a thing or two and I read a lot of books on education and research and education but I'm not a formal expert in it. I'm an engineer and a scientist, physicist but my general experience has been and the body of research that I have read says that two things you should avoid in education. Whenever you're doing online or something, keep your face away from it because human beings, the whole brain structure right? Because human beings, the whole brain structure is designed in such a way that the moment it sees a face, especially the eyes, it latches on to that. And it just keeps looking. So remember, there were some videos which was using called lightbox, the guy would stand and write, it would look as though the person is writing on a glass from behind the glass and you see the person a lot of videos where it was it is super impressive but if you actually look at what people are paying attention to they are paying attention to this guy speaking yeah through this because a human mind is deeply deeply conditioned to look at faces that's how we recognize danger that's how we recognize emotion we That's how we recognize emotion. We keep looking at faces. So the best way to teach actually, is to take the face away. Like for example, when I'm teaching, I'm hoping, I don't know what you guys are seeing, what you're looking at is the code. And maybe in the top right hand corner is my face somewhere. But it's not dominant there. Right? The second thing that you learn is that use colors judiciously, principle of least ink. Use color, but use the least ink to express it. When you see this, how much ink there is. So it looks very pretty. See, sometimes it's good. Like for example, I believe that once you have seen those drawings, then to see this is very impressive but to start with this in my view is not a good idea it's a better way to first do hand hand drawings and then finally come to this and not only come to this so this and by the way the code if you want to see is very simple what i have done is i've plotted the surface What I have done is I've plotted the surface, right? Surface plot. And what I have done is, because see, here the loss is only 53. So the error surface is practically sitting on the parameter space. The net residual is too small. So just to visualize, I lifted it up. I deliberately added 2,000 extra error points to it to create the visualization. Then you play around with alpha. But then there is nothing else. Now the contour lines, these are the hand-driven contour, making sure that my contour lines should be at these values. And then the rest of it is very simple. The contour plot and the main plot, this is it. Two, three lines of code, and then you get it. It takes a little while to become good at plotting and creating visualization, but you do. But then what you can do is I've also given you because I knew that some of you would want to do this on your own. So I've not only put the code, I gave you the code to make it into a movie. So this code will make it into a movie with one point. Remember, I've said iteration to zero for safety. If you blindly run this notebook, you may accidentally end up running this bit of code. So to prevent it from being done, I've set the iterations to zero. Change the, comment this out and uncomment the next line out. Because when it runs on your laptop, unless you have a powerful laptop, it can easily run for a few hours. Making movies is always longer. So ultimately it produces this movie. Let's try this. I will start this, I will just decrease the, just forgive me. So guys, pay attention, I'm going to start this gradient descent spread. Keep your eyes on this point. You see where my mouse is? Keep your eyes there and see what happens. Do you see the gradient descent taking place? Yeah. Right. And do you notice that as you come closer to the final, because at the bottom of the bowl, what happens? It's not steep, the gradient is low. So for a fixed learning rate, if the gradient is low, the steps you're taking are slow, right? So you go like a rabbit and then you start creeping like a turtle, like a tortoise. It was the final answer and there you are. Was it fun? Yeah. Yeah, so we have to redownload that lab. Oh yes, you have to redownload the lab. It is online though. Yeah, the latest version is there. Yeah. Is that the difference? Come again? Why should we read it now? Oh no, no, because my teaching faculty, we reviewed this yesterday or day before, so they didn't have the latest version. They didn't have the movie there. Oh, okay. This little movie was in there. I downloaded it. before so they didn't have the latest version they didn't have the movie there oh this little movie yes so this is it so guys do you see that the magic behind the theory that I taught you right is very real you can literally code that theory into code that theory and see it work right and so that is that that is one notebook let's take a five minutes break and then we'll go to the next data set which is about pondering over the mysteries of whether high horsepower engine have low mileage or high mileage uh yeah here is the loss yes right so you know i have been sloppy with like the latex code oh just double click on it and you'll see it frack half. You will get, I'll post this latest notebook also. See guys, whenever I do this, this is an old physicist style. You know what physicists do when they deal with equations? They set all the constants, they erase all the constants. So you can always tell a person is not a theoretical physicist because the normal physicist and everybody else will say e is equal to mc square what will a theoretical physicist say can you guess e is m so all the constants are equal to one right so for us half this that proportionality constants they all all disappear, right? Because it helps you reason if you just get sloppy with your constants. So in fact, most theoretical physicists book, that's literally the way they write it, E is equal to M. You'll never see E is equal to MC squared. My professor researcher, he used to put it very humorously. He says, what is a constant between friends? If you got the minus sign wrong, you would say, what is the minus sign between friends? All right, guys, so quick five minute break, bio break. Let us regroup in not five let's make it ten minutes let's regroup in ten minutes okay How about now? Much better it. This screen was. The next biggest screen is 100 inch. This is 85, 86 inch. The next biggest screen And this is all going to pay. I'm doing. Washington. Washington. Washington. Washington. Washington. Washington. Washington. Washington. Washington. Washington. Washington. Washington. Washington. Washington. yeah yeah Thank you. Gracias. um let's Thank you. No, no, no, no, no, no, no, no, no, no, no, no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no And again, in volume, I didn't get into except in one SEC. The foundation computer vision. What I. which is the TV back in the So, wait, not still getting back to, browsing through the notebook is not, okay. Okay, so. yeah except for proportionality things to make sure it works so that's a parallel thought running on my I was very interested in parallel thought that's running on my daughter she can't write science she's a computer or a computer sometimes she uses now she's a scientist um she cannot express math concepts that are getting comfortable conflicts now she has started using that she's kind of exciting with her then she's all right You can train up. I should. Talk to you. So the way that I can look at it, so we do see the details of the way you like to explain the environment and why is it? So, it's not just the math this is how it looks The mindset is not aligned to see so i just went this week um i don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. sure and then i'm starting to get into the book and seeing how i can it's everything images uh that okay channel. I don't know. Okay. okay you you you you you you you you you you you you you you you all right guys our 10 minutes hopefully are over we will now So we did gradient descent. And now we are going to change on, change topics. We will do, go back to some real things. Before I do that, I would like to take an interlude. Just as last week we did, Harini showed her diamond dataset. She has actually added a lot more to her dataset. If you, Harini, are you here? No. So anyway, I have to change the LinkedIn link to her, but she has done quite a bit of improvement to her diamond dataset, and it is now posted in the course portal. Please do visit it. Do I need to start the YouTube thingy? I suppose so. It's already live. It's already live. Kyle, would you like to walk through your Moringa thing, notebook, or should I do that for you um I'll okay I'll do it yeah you do all right so you share your screen and walk everybody through your leaves project oh I think that would be a problem. Yeah. I have some issues sharing my screen. Oh, yeah, I will you talk and I will share the screen for you one second. Okay. All right. So just let me introduce you. So Kyle, of course, you're all familiar with she is a data scientist here at support vectors. She is also a teaching, part of the teaching staff here, teaching assistant. And so this is a project, this is her submission for the leaves project. It is still partial work. She has not looked at two species of plants. She has looked at one and she's continuing to work in that. But this a work so far moringa is of course the drumstick plant we almost all of us have in our homes right and this is her project this is kyle and if you want to look her up on linkedin this is it most of you have been hearing her voice rather than seeing her. So this is her. She is probably the youngest member of our team here, amongst all of you. So very, very motivated. So now this is the project she has done. So, Kyle, please start. Okay. So for this project, I collected around 20 compound leaves. So if you see the image below, that is one compound leaf. Like a branch, but it has a lot of sub branches as well. So the central, I mean, called the the branch the main branch and then it divides into two on either sides and towards the end you have just i mean just one leaf leaflet at the end so the individual leaves they are called leaflets and these branches they're called petioles these branches they're called petioles, I think that's how you pronounce it. So what we do observe is that, it sort of is in a pattern. We see that it's in the multiple, it's all in odd numbers. We see that it ends with one, and then you have the smaller, I would say smaller compound leaves or they're all uh sort of uh subsets of one another so you have uh you have uh i would say a compound leaf of size five and then you have three and then you have a one and then it's of seven and nine so it's all in odd numbers and it's it seems like there is a pattern here so i wanted to study that and what i did was um i i took the site so yeah scroll down um so we see here that I took the, I studied, I mean I measured the length of one petiole. So, I considered each of these. So, we see that I have five, say, five compound leaves or at least five clusters. And I measured the length of the petiole and I counted the number of leaves that are there in each of these petioles. So I did them for different sizes. So Asif, can you just scroll up to the previous image? Yeah. So yeah, here. uh to the previous image to the previous image yeah so yeah here so i if i in this compound leaf there seems to be i there's like around 10 of these smaller leaves i mean smaller uh compound leaves so uh from one compound leaf i i took around 10 measurements so around 200 i got around 200 measurements from around 20 compound leaves and then i collected collected the data and then i analyzed it so if you could scroll down that's it thank you Thank you. Want me to go down more? No, no. Yeah, this is it. So I did the basic descriptive statistics that we usually do. I okay this is how i collected the data we have ptl and elite redcom head and tail and then a sample um and then i described data.describe and we see so one thing that I did I collected the data for smaller PTO lengths more than larger ones that was sort of by mistake I would say so we have more leaves of smaller size than of larger sizes. So as we can see in this plot, so I plotted my data set and we see that towards the lower end, we have more data than that compared to those of larger video lens. But also we can see some sort of not exactly linear but some sort of exponential growth of the number of leaflets with the PTO length. So at first I tried fitting a linear model to this and seeing how it performs that. So like I said, I collected more data for smaller lengths and we can see that in the histogram that we see for smaller lengths around like, even i i calculated i measured um you know branch length of say 0.5 centimeter you know if if there were any i calculated them too so um they they were more in number compared to those of longer lengths so because of that we have more data for shorter lengths so that's kind of evident from this histogram and and again so this one is just i i tried doing standard scalar but that kind of messed up things later on so i removed it and i left it as it is and then for linear regression model so like we do with the sklearn used a i first split our data into training and test data and then i fit the model to x train and y train like we do and so the coefficients that i got was um uh minus three and three so that's what i get and the mean squared error is around 22 and r2 is around 0.92 so it does seem like a pretty good fit even even though it isn't um so this is how it looks when I fit the linear model. The prediction line is, it sort of fits the data but still it's kind of obvious that it's kind of underfitting the data. So now I do residual uh analysis and we can clearly see there's a lot of pattern here first of all and then there's also heteroscedasticity um yeah it's pretty evident from the residual analysis and yeah and yes the distribution of the residuals is skewed and there seems to be bias error and the model is over simplistic because the model is over simplistic this one is the same histogram so now i since i since it appears to be exponential, I tried to do nonlinear regression and I used the exponential function. Here the function that I'm using is a into e to the power b plus c. So I'm trying to fit this function to our model i mean to our data and so now when i see the coefficient that i mean so i use a curve fit and i use this the function that i created and from that i stored my parameters and the covariance matrix so from here um the covariance matrix shows that it's um it's pretty much zero in all in all cases um so probably is good that the covariance is pretty much zero and And we see that for the coefficients, only A is pretty significant while B and C are pretty much zero. So it's almost like 16e to the power x, 16e to the power the PTO length. That seems to be the function that, the model that seems to fit this data. So now when we plot it, plot the function and overlay it on our data, it seems pretty neat if it fits the data. So for this, even though we do see some heteroscedasticity which is not addressed in this analysis so far uh it the the model still pretty much fits the data for now so after which i think asif i think you you can explain what you think about this no no after that no we can so one of the observations of feedback i'd given was that if you could continue this is a pretty good fit if you look at this a clearly an exponential model has worked very well for this data just looking at it and but you do see some degree of heteroscedasticity in the residual. So you ask, is there a different way to look at it? So one way that I thought we could look at it is, if you go back to the leaf, and that's sort of a physicist's way of thinking it, is if you look at this big leaf, right? And you think about it, you look at this leaf, for example, does it look like a triangle to you with a base? Right? Yes. And so if in a triangle, barring some empty space here and there, you're looking at an area of the triangle, very roughly, you can say that the area goes as the square of any of the sides or the heights, very roughly. It is clear that the width of this triangle is proportional to the height of the length of the length of the petiole and so one is represented by the other so the area of the triangle which is a which is a good measure of the number of leaves right is therefore proportional to the square of the petiole length so if you you make that hypothesis, then I don't know if we, we did this experiment Kyle yesterday, I don't remember fully. Yes, yes, at the bottom. So what you do is you create another data variable called area. And when you do that and you do plot area with the leaflet count, you notice that there is a more, a little bit more, it is still spreading out, it's a little bit more. Did we do the fit here? We did the fit. Why fit? And what was the mean squared error? Now we still get a mean squared error, which is along the same line. Did we do the mean squared error here? Area, leaflet count, determination, et cetera, et cetera. So that's another way to look at it. I don't know whether we did the residual plot or not. But yeah, so when I was collecting the data, I realized that the thickness of the PTO also mattered because if it is pretty thick, then there are more leaves compared to a thinner pituitary um so i guess it is uh that could probably define i mean it uh be the reason for the heteroskedasticity that we're seeing as the and that happens only when the the pituitary length is pretty large. So for smaller PTO lengths, it's very standard, the width is pretty standard. But when the leaf is really large, the thickness also, I guess, matters. That's right. But all of these are still very good models, because if you really look at the correlation between Y leaflet, which is the leaflet count and Y hat, you're getting about 96% correlation, which is about as high as you can go. This is in other words, all of these are fairly very effective models. So a great project, Kaya, very good job. Thank you. How many records in this dataset? She has about 200 something records for this one species. Yeah. Just a quick curious question here. 209. Yeah, go ahead. I read somewhere the leaf patterns follow Fibonacci sequence. They do. They very much do. And how does that factor into analysis? Is there a model that represents that kind of scheme? See, the way I would say that is this. See, first thing, so okay, I'll tell you this. There are multiple ways to look at it. First of all, yes, leaves counts. They tend to have a Fibonacci series pattern. One of the consequences of Fibonacci series is when the numbers are large, like for example, if you look at this compound leaf, and these are very approximate statements, guys. And so when you look at this very compound leaf you notice a lot of leaves here by now these these effects kick in now the ratio of successive terms in tribunaki series it is asymptotically it goes to the so-called golden ratio which is about 1.6 now I I pay attention to this and see when you look at this and you look at the length of this and you look at the width of this, does it agree with your intuition that this is approximately one and a half, right? The width is, the length is one and a half times the width. Does it look like it? Yeah, it does. It is. So nature follows the golden ratio in surprising places, especially when you see compound leaves. See, when you look at nature, you find, and if you're interested in math and these things, as I told you, you see transcendental functions, you see numbers everywhere. So one of the things I find interesting that in the animal kingdom, most things come in even numbers, you know, we have even number of hands, even and legs and so forth. Right. But if you look in the animal kingdom, odd numbers are much more common. You count the number of any one of these and you'll see that they are odd numbers. Here, like counted, you see that they are all odd numbers, 3, 5, etc, etc. They can count it. In nature, in the plant kingdom, odd numbers rule, in the animal kingdoms, it's not that evens are not there, evens are there. But you see odd numbers much more frequently than even numbers in the animal kingdom on the other hand you tend to see even numbers much more right and when you see compound leaves like that right it all these things come so anyway that is an answer to your question of the fibonacci series and all of this golden ratio and so forth does that answer your question yeah somewhat like that my thing was like is there any mathematical models that can be used for fibonacci it's like for sometimes these kinds of oddities like the fibonacci's the factorials and things like that they sometimes trip you uh in some pattern which we don't recognize. Yes, you see, here is the way about the way I think about nature. The nature and the world frankly, is written in the language of mathematics. So if you, one metaphorical way, a poetic way to say it is that if you want to read the mind of God, learn math, because his thoughts are in or her thoughts are in mathematics is the language of mathematics. And in India, we have all the stories of Ramanujan being inspired by a goddess. You remember those stories, isn't it? Yeah. It is a language in which the whole deep mystery of the universe is written. And I would always recommend people that learn math first, everything else becomes easy after that. You notice that when I'm teaching you machine learning is very different from the way most people teach you. Most people teach you code onwards, or here is a code, here is this library, do this, this will happen. Right? Whereas I go to the mathematical foundations, because I feel that if you understand the geometry, if you understand the math deeply, then language, you know, Python, R's, you know, all of this color, this, that they will keep coming and going, but your understanding will remain that's true thank you all right guys so we'll do another data set this is an interesting data set so you have to assume that you don't have real world experience with cars yet right so imagine that you are an alien and you have come to this planet and you observe that people tend to move around in these automobiles right and these automobiles are characterized by their horsepower their weight and they have these engines engines have displacement what is the displacement of an engine does he see rating that you? See what happens is that think of our engine cylinder. The the cylinder of a engine is like this. Yeah. So what happens is that this thing, the this part, it increases and decreases, because inside there is a piston. And what the piston does is it pushes its way in so that the available air gap, the space available or the volume available, decreases. Think of it as an injection, the way you do it in an injection. So you compress, the piston goes into the cylinder, the available space is very little. In that available space, you spray a mixture, atomized mixture of gasoline and air. And once that is saturated, that mixture saturates that volume, the spark plug, you have a spark. And what the spark does, it ignites that atomized mixture of air and gasoline, and it literally explodes. There's a fire, it burns. There's an exothermic reaction. Exothermic reaction creates temperature, temperature creates pressure, pressure pushes the piston back, right? So the piston goes all the way back, but when it goes back, it doesn't just go back. Behind it, at least in the older design, there was a camshaft, right? So what it would do is it had a camshaft design, it was a wheel there, basically, there's a way to convert this backward motion into a rotation. And so it causes a rotation of something, right? And that rotation forces the cylinder back, because here, what has happened is once the exothermic reaction is over and the fuel is consumed now this can push back because there's no nothing creating the pressure and so forth so this can go back because the a port opens which lets the exhaust gases to leave once the combustion is over so now this piston can go back because it will force all the air out of the chamber ignition chamber and this piston goes back and then the cycle repeats as it goes back more atomized fuel mixture comes into the chamber of the cylinder and ignites and this thing again goes back so you have this piston going back and forth the camshaft and it's causing a rotation right and that rotational motion is further propagated and that is basically your internal combustion engine. Or very similarly a diesel engine, et cetera, so that you don't need sparking there. So that's how these fossil fuel engines work, basically. So now what are the criterion? You're trying to determine how much miles per gallon of fuel can you get from an automobile. That's the target variable. Now, what are the features that matter? Common sense says that the sheer weight of the car or the automobile matters. For example, if you bring a Humvee, which is, I believe, five or six tons, then the amount of mileage you expect would be different from the mileage that you would expect from a compact Prius. So that way. And then another factor that matters is horsepower. Horsepower is like how much horsepower does the engine manifest in terms of its pulling weight? How much can it pull? Right? And horsepower literally started with horses. The standardized might of a horse. So today we run cars with massive horsepower that is unimaginable. I mean, we run cars of 500, 600 horsepower. It is, we never really think about it as fun to drive those cars. But if you really think about it, 600 horsepower is 600 horses are pulling just you, right? The army of horses pulling just one person. So obviously these are very powerful engines here in the US. Then the other thing that matters is cylinders. So how many of these chambers or cylinders act in collaboration or collusion to cause those rotations, the primary rotations? So the more the engine, the better. In US, you talk of the so-called V engines are the most popular for the last 100 years. So the V-shaped engines have a body in which the cylinders are along the V, on the tip of the V in a line. There's a line of cylinders. So you can see, open your car and see that. So we have V4, which is the default. V6 is supposed to be for more powerful engines. And then if you really need a muscle car, then you go V8. And if you really have no consideration for money, you can go to V12 or something like that. There are such engines. So number of cylinders matters. The V2, V4, V6, V8 represents the number of cylinders. So the cylinders are there. The weight of the car matters, the horsepower matters, and then the word acceleration matters. Acceleration, when we talk of a car's acceleration, we talk of it like this. In US, you talk of it in an interesting way. You say, how many seconds does it take to reach 60 miles an hour? Right? seconds does it take to reach 60 miles an hour right so if it takes 10 seconds or 15 seconds it must be a very very weak car right if it can do 60 miles an hour in three four seconds it's a mighty car right and so the small so the acceleration is a very peculiar in this data set it's a you have to remember that it is not in the literal sense a physicist would say the stronger the acceleration the more powerful the car but here the way we speak of in acceleration in the car industry is how much time does it take to reach 60 miles an hour right so acceleration therefore big is big is weak car small acceleration is powerful car that is one thing about this data set you should remember so with that all of that there this data set was collected a few years ago by looking at automobiles of three countries that countries is given by the origin there's a feature here called the origin uh you will see do you know notice that origin of the data alone the data set we see origin so one is you zoom in a little bit yeah and also the initial graphic um that describes the issue oh yes yes this is by the way literally question page and question nine of your islr textbook right okay it's in the textbook yeah it is quite literally in the textbook let me uh isla okay yeah if you did okay yeah because that png is missing ah okay i should upload that i apologize so it is literally question eight and question nine of chapter three of your textbook right so so this is how the data looks if you sample the data you notice that miles per gallon you can imagine that this is would you say these are impressive miles per gallon by modern standards probably not most cars don't give uh give much higher mileage people are shooting for a 40 50 60 these days right as miles per gallon especially the four cylinder engines easily give you these days 50 60 miles a gallon especially if they're hybrid and the the six the eight cylinder engines of course it is a 15 miles a gallon do you think that is still reasonable or that's low or high i think it's a good yeah that's what you're expecting yeah actually that's what my that is what my car gives me my car still gives me 15 miles a gallon it's a v8 right so displacement now you see displacement is literally a measure of the size of the cylinder because it is how far back the disc you know the piston goes back and forth what is the displacement of the piston and that of course would be a measure of the size of the size of this can or the cylinder right that's what it is number of cylinders and think of it as sort of the size of the cylinder horsepower how many horsepower is it manifesting the sheer weight of the automobile right how much automobile there is now remember acceleration is how many seconds it takes to reach 60 miles an hour obviously in those days this used to be the amount of seconds it used to take today how long it takes much much less the engines are far more efficient and powerful right and this is a display yeah see the same sample there's some statistics here miles per gallon the mean is 23 and standard deviation is about 8 and maximum mileage used to get is 46.6 and the minimum you get now one more thing here displacement everything year the minimum year is 1970 and the maximum year is 1982 right so it is a study done over 12 years over 400 about 397 data sets. Now, origin. Origin, minimum and maximum values are three. You can see that there are only three possible values, one to three. Now, if you read the documentation behind this data set, go back to the original paper, it mentions that one stands for USA, two stands for Europe, and three stands for Japan. Japanese-made. European-made automobiles, I mean American made automobiles, European made automobiles and Japanese made automobiles. So let us do the basic exploratory data. The first thing you realize, it takes a while to realize that something is wrong with horsepower. Do you notice that all of these mean medians everything is missing why would it be missing you expect name you see the name the name fort pinto you can't have the it is a categorical variable you can't have mean etc for categorical it makes sense but what's wrong with horsepower if you look at horsepower you suspect something is bad and so you can check maybe some values are not numeric and when you do that you find that yes truly apologies it's just my allergies so x for X in unique values if numeric. So what happens is there are about six data points that have question marks. You can check, right? Six or seven, I don't know which exact number. So they are missing values there. What we will do for now is just delete the missing values, right? So we do some data in the beginning. This is real life data now. You have to deal with the fact that some things will be missing, some things will not be okay. So what I've done is I've converted horsepower, wherever it's missing, I've deleted the row altogether. And then after that, I've converted those values to numbers because horsepower should be a number, isn't it? It should be a number. I've converted it to numbers and origin, which be a number isn't it right it should be a number i should i've converted it to numbers and origin which is a number i've done the reverse you know that these origins are these are not numeric two is not greater than one two is just a representation for europe one is representation for u.s made vehicles three is for japanese vehicles so i've converted horsepower to numbers and origins to string this is basic data manipulation to clean it out also i realized that if you're trying to predict the name of the mileage of a car do you think name is a good predictor no because if mileages could be changed by name most manufacturers would go about finding the optimal name to improve mileage right it wouldn't it doesn't make sense so that is that so i just dropped name forget name after that i noticed the data looks clean missing number analysis shows data is clean info everything is as it should be miles per gallon is a float is a numeric number of cylinders is integer as it should be a wage for some reason is mentioned as an integer I could have bothered to convert it to float I didn't bother. R. Vijay Mohanaraman, M.D.: Right here of year the car was made is a integer this is it so now let's go one of the things I didn't do is that this profile. this profile because it takes a while to run you can uncomment and run it on your local environment so now i take this and data visualization you visualize the data right so let's look at it case by case uh we are looking at miles per gallon is this data symmetric no it is skewed isn't it and it is skewed. So we need to worry about that. Number of cylinders, well, you don't get cars with odd number of cylinders. There seems to be some data point which is unusual, but be as it may. Most cars are, what are they? Most cars are V8s or what are they? Four cylinders. Six is in those days was not so common, V8 was very common. Of course, the situation has reversed. Today, we get a lot of 4 cylinder and 6 cylinder cars. V8s are rare, very rare actually. It's hard to get a V8. A bit of personal thing. I actually grew up with V8s right from the beginning i used to love sports cars and of course now i'm in my 50s so it's a bit of an anomaly but because my hand and my reflexes are tuned to a v8 as i grew older i don't drive very fast but because my hand and my complete sense of braking and everything is attached to a V8 engine, I still drive a V8, which looks sort of anomalous because here is a person going in a V8 and driving in a sedate way rather than accelerating like a young man. Right. But so it is what it is. young man right but so it is what it is displacement how much displacement so most cars have small displacement but yes data does have a skew here then we look at horsepower again most cars don't have that much horsepower in those days wow Wow. Most cars didn't even have, most of them were 100 horsepower or low. That's all, that's, world has changed. Weight, again, a right-square distribution. Acceleration seems, roughly speaking, evenly distributed, isn't it? Acceleration is this. And the year of the car, much cars were made in each year then I count the number of cars by origin by country of origin it turns out that US is the biggest maker of cars number three was Japan Japan is makes also a lot of cars and Europe makes the least number of cars just a little bit behind Japan but US is of course that is the land of cars, right? It's always been the land of a lot of cars. Let us plot this data. Now, this plot is interesting, and there is a lot going on here in this plot. I want to show you what it is. So I will just, because it's a blown up the screen, this is a big plot. So let me just for one moment, shrink the size and show all in one, even then it's not okay. Of course you have to squint to see this, but you notice that this is a pretty busy plot about six, seven, one, two, three, four, five, six, seven, eight, eight variables plotted. The way I've done it is, and the way this plot works is called pair plots. The diagonal is the frequency plots histogram, right? Then these ones are scatter top half, The top above the diagonal are the scatter plots between two features. Below the diagonals are this kernel density plots so that we have everything now. So now let me just blow it up and show a few examples to see what it means. So look at this. This is the histogram. The way to look at it is histogram, miles per gallon versus what? Miles per gallon. This is the y, this is the x-axis. So intersection here is miles per gallon. This is the histogram we just saw a little while ago for MPG. This is exactly the same as this MPG, right? Miles per gallon thing, MPG. This is it. Now let's look at this row. This row is miles per gallon is along the Y axis and the X axis is cylinders. Of course, you have to scroll down to see the word cylinders here. So this is the cylinder, right? I'll make it even bigger. Yeah, cylinders. So what does it say? As the number of cylinders increases, does miles per gallon improve or decrease? Decreases. Decreases, right? You notice that with V8 engines or the eight engine, eight cylinder engines, the miles per gallon is pretty low whereas for four cylinder engine the miles per gallon in general the middle is somewhere here pretty high the next feature is displacement here the x-axis is displacement how would i know it i can scroll down and see yes it is displacement other thing the way i use it is I just look at the order here. Third is displacement. So here, miles per gallon goes up or decreases with displacement. What would you say, guys? Decreases. Decreases. But is the decrease linear or what does it look like? It's a nonlinear one, isn't it? Yeah. Yeah. Yeah. Likewise, the relationship of miles per gallon with horsepower is like this, right? It's sort of disappointing. Wouldn't it be a teenager's dream that mileage improved with horsepower? improved with horsepower. It's one of the disappointments. The moment you run big cars, sports cars, the whole thing just guzzles up the gasoline. And it makes you feel guilty as it is disappointed. But it does. That's what it is, actually. So the next one, actually, I have a little thing. My daughters, I gave them pretty much the similar cars to mine because they liked it. So they were very excited in the beginning. Then gradually they went to college and I'd given them fixed pocket money to fill the gas tank. You can imagine the disappointment. They'll curse you. What's that? They'll curse you. Because they say this thing just guzzles up the gas. They can't eat in the pocket money. The car will eat everything up. Yes, exactly. So now they came and negotiated that I'll pay for the gas and they will not pay from their pocket money. This is a lesson that you see in this graph. Next is weight. So what is the relationship of miles per gallon to weight? Shouldn't you have seen a reciprocal relationship? relationship of miles per gallon to weight shouldn't you have seen a reciprocal relationship the more the weight the less the might the miles per gallon why is there no clear relationship it could be different to the materials used plastic sorry no this is weight it is weight it is a reciprocal relationship it is the inertia yeah the fourth is acceleration. Why does acceleration not have anything to do clearly with miles per gallon? Do you notice that the relationship is not so clear? You would imagine that the bigger the acceleration is, the longer it took to reach 60 miles an hour, the more the mileage. Because weak cars, they take a long time to accelerate to 60 miles an hour. And they give you perfectly good mileage. but you don't see such a good mileage why is that i will leave that as an exercise the answer is quite simple but i leave it as a food for thought i should or should you want me to answer that? Maybe the newer cars that got better engines that they are more efficient. So it's 182, right? So 12 years of difference. So that's a mixed number of efficiencies there. Why is acceleration and miles per gallon not having a very pronounced and simple relationship the more the acceleration the longer it takes to reach 60 miles an hour the weaker the car the you would imagine that the more the miles per gallon it's also functions on the size of the car exactly so the size what happens is sometimes acceleration longer acceleration is not just because of a weak engine longer acceleration is also because you're sitting on a big massive engine for example most buses are heavy right and there they have pretty weak acceleration they take a long time to reach 60 miles an hour right but so that is the explanation the year here matters i think one of you said year. Year is also a factor that interplays. The next one after acceleration is literally the year. So this is year versus miles per gallon so i'll just blow it up what do you think guys is the my is the are the cars improving with respect to their mileage fuel efficiency or getting worse with years come again it'll be better as the year goes by because advancements come up well come again it'll be better as the year goes by because advancements come up yes exactly if you draw a line through this you can literally feel that yes slowly steadily slow and steady improvement in mileage with passing years right cars are getting fuel efficient in the 90s of course by 82 and then of course in the 90s people discovered suvs and the whole situation got worse so it's never linear anyway now you look at the country of origin the last one is the origin i believe see one quick question with previous products yeah in some of them they have clusters broadly identified, but in some they are not. It looks like Matelot-Levitt is using some kind of a deterministic... No, no, no, no, no. Here is a discrete integer variable. So it takes integer gems 1, 2, 3. Likewise, the country of origin 1, 2, 3. So you see that mileage of American cars are terrible. Mileage of the Japanese cars are best. Mileage of the German, European cars are slightly worse. Look at the road to first power. But because of the black lines, first power. First power, that's a different. You mean this one? Road to, road to first power. Oh, this is something different. That's a different. You mean this one? Row 2. Row 2. First car. Oh, this is something different. I'll explain that. So I'm still interpreting the first row. Okay, sorry. So in the first row, origin, the number one stands for American cars. They always traditionally had bad MP miles per gallon. Japanese had better. And Europeans were comparable to Japanese. This is, by the way, this fact unfortunately is still true. You probably know that the same car manufacturer will make exactly the same car, but in Europe will give much better mileage than in the US. So some of the fuel efficiency components, because they are expensive, they will not get installed in US. Because the laws here are not as strict as in Europe. So the same car will give much better efficiency there. Can't do much about it. It's a fact of life. You have to get the laws changed. California is doing its part, but let's see. Now let's look at this guy. Look at this second row now. This car is in, this plot is interesting. It's below the diagonal. Do you see that? These are called kernel density plots. It plots, well, we talked about kernel density plots. They plot contours of equal value, right? And what they tell you is, well, okay, they say a lot of things, which I don't want to talk about until we get there. But basically, they show you that how is the data distributed. So clearly, data is distributed around six cylinder, four cylinder and eight cylinder, you see these bands of data, right? And it is a cylinder versus a miles per gallon. And you can clearly see that four cylinders gives you, the center of it is more to the right than the center of the six cylinder than the center of the eight cylinder. What does that mean? The average miles per gallon from a four cylinder engine is far more than from a six cylinder is far more than from an eight-cylinder engine. You see that, right? So that's what you get from this density plot. So this is the way you interpret it, guys. I'll let you interpret. The most important line was miles per gallon and its relationship to each other. But then I'll show you something quite interesting. Look at this plot and look at for example displacement what is this guy let's look at this guy this is displacement versus what one two three four five one two three four five displacement versus weight is this this plot can you guys see where my mouse is what would you say looking at this graph what is the correlation between the two displacement versus weight positive very very strong correlation right you can see a strong positive correlation you also see a strong positive correlation here you see a strong positive correlation here right so while correlation of a predictor to response is a good thing correlation between the predictors for linear models is not a good thing it leads to a pathology which is called collinearity what it means i'll give you the intuition we'll talk more about it the intuition is linearity. What it means, I'll give you the intuition, we'll talk more about it. The intuition is, you know, if two variables, they are giving you the same information, right? So how should I say it? Let me make it a joke. So you guys, we come from the software industry, where we are supposed to work often in pairs or groups, right? Have you noticed that once some work is done, the first person who goes and talks about it pretty much steals the thunder of everyone else, right? If he doesn't give the credit or she doesn't give the credit to the entire team, everybody will think this guy did great work. The next guy who comes and explains it has almost nothing new to add. You know the phenomenon, right? It's literally somebody has stolen your thunder. They're going to say something very impressive and somebody said it already. Right? That is it. So when two things are highly correlated, right? For example, you look at two variables. One is the height of a hill above sea level and the other is the height of the hill from uh from this valley below right from fremont you would agree that the two variables should be very correlated for the regional hills behind support vectors, right? Next to this mission peak that is there and all these hills around you, you would imagine that these two variables will be highly correlated. And if you think about it, you don't need both. You just need one because one is as good as the other. Height above sea level and height above fremont all you the only difference will be the height of fremont above sea level do you get that guys does that make sense guys yeah right yeah so if you have two features one is height above sea level when a height above fremont you would say that they are very, very correlated up to some noise factor. And common sense says we don't need both. So what happens is that it shows you the redundancy of information in your predictors. And for linear models, that sort of redundancy is not good. That highly high correlation between them is not good. there is a word for it it is called collinearity or more formally multi-collinearity what it does it screws up the the machine learning model it causes and there are many things it causes let me not steal the thunder of tuesday's session but it basically causes problems so you have to watch out. When you see that, you should eliminate, you should pick amongst the highly correlated factors one factor and drop the other ones. So that is that. So now looking at this, we make some more plots to see the distribution of the data. These are called violin plots. We see the data distribution of each of the variables and I've separated it out by origin. So you can see miles per gallon, where most American cars used to give about 10-15 miles per gallon. The Europeans used to give 25 miles a gallon, the Japanese used to give, oh God, they already were giving 30s gallon. So these are violent plots I mentioned it to you. They're common, they're pretty, they're useful. And an alternative to them are the box plots. These are called the box, oh, I shouldn't say violent plot. I should change it to box and whisker plot. I apologize. Box and whisker. Box plot of the features this is the box plot so in the box plot this is the middle value median value this is the way the data is distributed the sort of average data is distributed the whiskers represent the i think the quantile limits q right and then these are the outliers so these are the standard box and whiskers that you use from college then you look at the correlation between the data i'll slightly decrease the font to show the correlations so when you look at this correlation table guys the font to show the correlations so when you look at this correlation table guys do you notice that miles per gallon is correlated to number of cylinders negatively the more the number of cylinders the more the less will the miles per gallon be does this agree with common sense yes it does displacement also the bigger cylinders, the worse the miles per gallon. More powerful a machine you have, heavier the machine you have, less the miles per gallon. And the more the acceleration, the more the miles per gallon. But by acceleration, the meaning is inverse. How long it takes to reach 60 miles an hour, right? And also it has a positive correlation to years. So the more recent the year, the better the mileage. Does this all agree with common sense, guys? First row? Yes. But now comes trouble in paradise. Let's look at any one thing. Let's look at the weight. Weight, okay, mpg is fine but you notice that it is highly correlated with cylinders what does it mean heavy cars tend to have big more cylinders which is common sense big heavy cars they tend to be v6 v8 displacement even more v8 displacement even more 93 correlation horsepower again a lot of correlation so what does that mean guys that if i take weight maybe maybe i can forget about a couple of these variables these are highly correlated variables at least these two I can forget about. Maybe keep horse power, may not keep horse power. This is close to anything like 90%. You don't get much value by keeping those variables. So let's do that. First, what we will do, and then this is, by the way, same correlation plot visualized very negatives are black and very positives are light and so forth so the same information visually displayed so now let's do one thing the your book talks about doing analysis what's the time now oh it's five o'clock already so actually we are still an hour or so away from the thing are you guys open to a long session or should we finish at 5 30 and take it up next week I need to head out in a few months so maybe I can drop and say that recording finish at least the auto data set and we'll keep California for next week Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam of horsepower to miles per gallon what do you see guys does it look like a linear relationship to you no it looks like a slide yeah it looks like a children's slide yeah that's a very good observation uh looks like a children's slide so what can we do my intuition and whenever i see this of course this is the habit of years but If you ask any physicist, the very first thing they will tell you is, hey, this looks like 1 over x to the n graph. I actually mentioned this to engineers, and I was surprised that most engineers don't think like this. They don't think immediately a reciprocal power law, a graph. They just say, okay, it doesn't look linear. But it is, a graph. They just say, okay, there's a, it doesn't look linear, but it is, it is. And the only question that remains is what is n? So if this relationship is like that, what is the simplest way you can make it into a linear relationship? How can you linearize it? Old, old trick, just take the log of both sides. If you take a log of both sides, do you see that this relationship now becomes linear? is not linear in x but log y is linear in log x isn't it so if you think of new variables y prime which is the log of y and x prime which is the log of x you see that y prime is proportional to x. Do you see this, guys? It is obvious. Anyway, so what we do is we can verify my intuition. You can take the log of both the sides. And what does it do? Would you call this essentially a linear graph now? Yes. This, by the way, it's a very common thing. I have seen so many people struggle with data analysis. This relationship, inverse relationship will be staring them in the face and they won't spot it. Right. And. I hope you are not one of those. You will learn to recognize the inverse relationships and immediately know that when you see inverse relationships you should take the log right whenever you see what kate nicely said is a slide the shape of a slide immediately take the log of both sides because that will make into a linear relationship and so we can do that. If you don't do that, if you do the regression in the original variables, you will get some intercept, something, you get the prediction on the test data. What is your R-squared? 56.5%, right? 56.6%, let's say, is your R-squared. Your residual analysis, what does it show show does it show homoscedasticity yeah there's a lot of pattern there is a lot of pattern and there's a very clear increase in the variance right so there is a distinct presence of heteroscedasticity here then here is one more plot that is useful a Predict if y and y hat are completely matched, then they would be along a 45 degree axis. But actually, when you plot the y and y hat and you look at the best fit line, you notice that it is tilted. It hasn't gotten their relationship well. In the lower part, y hat is an overestimation and after in the right hand part like after after a little bit it is a underestimation so somewhere systematically you see the bias here it is a overestimating here it's underestimating here right so when you visualize the model's prediction, you can see it distinctly. Do you see this very distinct thing, shape? Would you call this line a good fit to the model? There's still slide shape in the data. Yeah, you can, but all it shows you is maybe you could go with it but there is a scope for improvement right there is a scope for improvement so now let's try the log transform the way i mentioned take the log of these two when you take the log repeat the what is it? Your R square went to 72%, right? 71.8. So from 55, 56, you go to 72. Would you consider that a huge improvement? Definitely. Yeah. In business, in real life, that would be considered a huge, huge improvement in terms of real value, right? So it's a much better thing. How about the residual plots does it look better with respect to the patterns yeah very much yeah patterns have not completely gone there's still a distinct slight noticeable there but much much improved and what about the prediction line and versus the best fit line you notice that these are much closer now, right? It is not so far apart as here, where they have like huge gaps here. The gaps have narrowed, right? So this is, and then when you visualize the model prediction, because now you have taken the log transform, would you say that the model and the data are in good agreement, the prediction and the data are in good agreement the prediction and the data are in good agreement yeah definitely and already you have 72 percent um this r squared so your things have improved so guys this is the this is the power of bringing intuition physical intuition into a problem if there is one thing I can say, learn mathematics and learn to apply mathematics. I can't say that enough. I mean, if I have to quantify what I have learned in life over 50 years, or more actually, much more than 50 years, let's say, without giving my exact age, let's say five decades, what have I learned through education? One deep lesson I learned is know your math really well, not from the point of exams and passing and getting good grades and things like that. Really have a deep feeling for math, just as you can speak your native tongue, speak in the language of math, and wherever you see things, see that the thing, can you interpret it, reality in math and see what is going on. If you do that, you will actually do very well. So one point I was going, and this again is drifting away from objective and going into my subjective experience in this field. See, I did this, I did the log transform, but suppose you had, you said, no, no, wait, what about the fact that we already have a power transform, Waxcox and a Janssen transform? What if we use that? Wouldn't it give you the same results? No, a blind use of a power transform. You knew that a power transform is called for. Why did you know that because both horsepower and mpg had right skew see look at that miles per gallon has a right skew and horsepower has a right skew do you see this guys maybe more evident here miles per gallon has a right skew and where's horsepower horsepower has a right skew therefore you could have said yeah let's apply a power transform let's find the right relationship and all of that what happens if you blindly use the tool which is what people would do so the general reaction of people is yeah data is non-linear so i need to apply a power transform yes you can apply it when you apply the power transform i make a pipeline of the power transform and linear regression. What happens now? Do you notice that your R square has gone up by six, gone up from 55 to 64%. That is a huge improvement, isn't it? But is it the best model so far? No. Just by using our judgment and hand thinking that we need a log transform, we got 72%. 64 is a far cry from 72, which again, I wanted to bring home the point that guys, think about data, make friends with data, you know, know the data so that you can do it. The next question is, what if I tried polynomial regression? Won't polynomial regression give me the bends I need? Once again, if you do it, what happens? It is close to, it goes to 60, 6%. Nothing close to 72%, isn't it? Do you see how hard it is to beat well thought out models? If you can see things in the model and you can recognize it and do your own transformations by hand nothing will beat that no automated blind application of tools will help that right so that ends the the one dimensional or one variable regression now the problem number nine is to do a full analysis using all the predictors so let us use all the predictors now there is a problem one of the predictors is origin which is like us europe and japan one two three how do you how do you deal with that so what you have to do is dummy encode it you have to convert it into numbers categories into numbers one way of doing it is to, it's called one hot encoding. You say, you create three features, origin one, origin two, origin three. Actually I should have named them origin US, origin Europe, origin Japan, but okay, I was lazy. And then if it is a US made car, you say origin one is one, it's true. And for Japan and for Europe, you mark zero. If it is a European car, you mark origin one is one is true and for japan and for europe you mark zero if it is a european car you mark origin one is zero origin two is one and origin three is zero you see that right origin europe is one so that way of doing it is called one hot encoding so funders give you a very simple way to do one hot encoding you can just say getmies. And you notice that it got burst out into three variables now, three features. Now, I want to tell you something about a most useful regression model, Overlook. Before you build complicated regression models, right? You see one dimension, we use this null hypothesis, et cetera. In the equivalent, the closest thing to null hypothesis, in fact, practically the null hypothesis in scikit-learn is the dummy regression. What the dummy regression will do is, if you ask it to predict something, it will tell you, should I predict the mean, median, something, right? Remember, the null hypothesis would just return you the mean, right? The mean of the target variable. Do we remember that, right? Remember the null hypothesis would just return you the mean, right, the mean of the target variable. Do we remember that, guys? The null hypothesis returns the mean of the target variable in the dataset, in the training data. Always as the prediction, whatever the input, it will always predict that mean, because it's the sensible thing, because what does the null hypothesis say? There is no relationship between target and predictors. So the best prediction that you can have with the lowest sum squared error is just to return the mean. And so dummy regression is in that same category. You can return mean, median, mode, whatever it is. But here, just to emphasize the fact that there are multiple choices i took median not for no big reason i noticed there was a small skew in the data so i just took median null hypothesis has a what what is the coefficient of the r square that you should expect from a null hypothesis by definition zero pss minus TSS is zero, right? So this is it. And you get a lot of mean squared error. So you should always compare because, you know, what you should look for is the decrease in mean squared error. Let's keep our eye on that. We take a model one, no transformations, all the predictors in there. Right off the bat, we get just blind utilization, we get 80%. What does it tell us? When we take all the predictors and we apply, what does it tell us? It tells us that horsepower is not the only thing that matters. In the one, only the horsepower based model, there was a lot of irreducible error, isn't it? That error captured the information there in the other predictors which we had not considered in the previous models now that we are doing multivariate regression we are taking all of those right away it has improved to 80 percent but but when we look at the residuals of course what do we notice we notice yeah there's a pattern there's heteroscedasticity we move forward again we notice that there is a gap not so much gap between prediction in this but there is still a pattern so let's take to model 2 this time we remember to standardize the data we apply the power transform and here we go when we do this do you notice that your mean squared error is continuously decreasing is there your r square has gone to 81.6 better let's then what about the residuals ah residual still has pattern isn't it some pattern is there despite the power transform but the errors seem to have a more or less even distribution now right let's move to another model model three model three what did i tell you log transform hand built model right let us go and take the log of weight mpg and horsepower these three also we realize that it has a strong correlation with other things displacement and cylinder let us go and drop those variables right because that would cause multi-collinearity effects when we do that what happens look at this r squared close to 87 percent isn't it do you see this guys once again the same lesson holds when you carefully handcraft your features by looking at the data and bringing your own judgment on it, you always get the best model possible. Now given these things, I mean, you can try to improve upon it, you may do it, but not by much. 87% is about as far as you can go. Residuals of the learning. And do you notice that between the best fit line and the 45 degree y axis, there's almost no difference. You see this guys here? It's pretty good. So clearly, we have a vast improvement over the prior models. So now let me teach you one more thing. It's called the Cook's distance. It talks about points of high leverage. Let me explain what these points of high leverage are before we end today. See what happens with linear models. Linear models are the simplest models. Obviously, we deal with other kinds of models. Linear models suffer from three are the simplest models. Obviously, we'll deal with other kinds of models. Linear models suffer from three, I mean, many problems, three of which we'll talk about. One is the multicollinearity. We'll talk about it next time. One is outliers. So when you do some squared errors, what happens if that one point is way up there, away from the other points? It will exert undue influence because it's an outlier do you see that right so because the sum squared error of that will be huge and so it will have unusual effect that the loss function optimization will keep on trying to optimize for that outlier and it will pretty much pull the line towards itself. So outliers have exaggerated influence, right? So you may say, well, no, you can change the loss function to mean absolute error, do something like that. But anyway, outliers are a problem. But the point with outliers is they're visible. You can see that they are outliers, right? And you can do something about it. But linear regression models suffer from yet another problem, which we don't, which is very hard to see, it's insidious. It's points of high leverage, you know, it's like these quiet people in the sort of like the team who are invisible, but they're the puppet masters they're the puppet puppet masters pulling all the strings sort of like that metaphorically speaking in this data itself there'll be certain points which have undue influence on the law on the training of your model when you look at this there are a few points but not that many usually anything above this line dotted line red line this is called the fence. Points above the fence whose influence is more than the fence, they do have. So how many there are? One, two, three, four, five. Let's leave the small ones actually. One, two, three, four, five, six, seven, eight, nine, 10. About 10 points out of 400. It's okay. You can either weed them out or you can just leave them there. It doesn't matter. But you don't have as many as to get alarmed with. Still, you do get the impression that maybe instead of using linear regression, we can try more sophisticated models which are immune to points of high leverage, right? Then comes the last question. Okay. Before I comes the last question. Before I take the last question, guys, I need a two minutes break. Give me two minutes. uh you you you you you you you Thank you. okay So, suppose, see feature importance is a very important topic. It addresses the fact that suppose I brought predictors. In real life, in business world, it's a day to day. How important really is this predictor in the model? How much influence does it have? So for example, let's take diabetes, the likelihood that you have diabetes, how much more, whether you take sugary drinks, is it going to make it worse? Like what makes diabetes worse? Assume that you have diabetes. What do you need to watch out for? Do you need to watch out for the amount of sugar that you take in food? That matters more or the color of shirt that you wear while eating the food? Which is more important? So would you answer that? How would you answer that? Exactly. The number of the amount of sugar that you take in your food has overwhelming importance relative to a frivolous factor such as what colored shirt you wear while eating that food. But in the absence that you are saying because you have bringing experience to it when a machine does it and you don't know which features are important the most important question that you have to ask the model one of the most important is how important which factors were the most important in making the prediction right what mattered do we get that what mattered is one of the most important questions you can ask the model. The model has learned something. You want to ask the oracle, what did you learn? Right? So that answer is the feature importance question. Now the thing is, if you have standardized the data then in linear regression actually feature importance is very straightforward you can literally see the power of the coefficient why because if beta 1 is twice beta 2 means if I increase factor the x1 factor by one unit, the increase in y will be twice that of the increase if I increase x2, isn't it? So suppose x1 and x2 are two factors. x1's coefficient is two. x2's coefficient is one. I increase both x1 and x2 by one unit, which will have more in more which will have more effect on the target x1 or x2 x1 right because x1's coefficient is bigger whichever factors coefficient is bigger for unit increase of that factor y will respond more common sense isn't it so you can get feature importance assuming you have scaled the data that's a big assumption assuming you have scaled the data all you have to do is compare the coefficients when you compare the coefficients let us look at the regression models coefficients intercept is three which we can ignore slopes are these are the coefficients coefficients of what coefficients of these predictors so what i do here is i just collate this this belongs to horsepower second one belongs to weight third one belongs to this i just collate it and then i create a map i create a hash map and also i standardize like i if i'm looking at percentage relevance i have to add up all of those coefficients and divide each one by that so that everything is on a scale of one to hundred so then what i find this is that the weight matters 35 percent of the feature importance goes to weight. The lesson is, if you want high mileage, don't drive heavy cars, isn't it? Right? So go ahead. When you try to find the relative importance of the feature, what do you do with the constant? The beta not doesn't matter, intercept doesn't matter. importance of feature, what do you do with the constant? Just drop it. The beta not doesn't matter, intercept doesn't matter. We're not giving that any kind of relevance. Any kind of relevance, yeah. So that is it. So this is it. You say a pie is 100 large. How much does each of the factors matter in that? And so weight, of course, don't drive heavy cars. So can I ask you this question? This is telling you that weight matters and weight matters quite negatively. The more the heavier the car, the less the mileage. So therefore, should you start driving very light cars? There's a safety factor there. Exactly. There's a safety factor there. Because if you drive very light cars or in the asymptotic limit you start driving a motorcycle which is very light what happens you become basically spare body parts right so I'm putting it harshly but yeah so in other words a miles per gallon is the only thing you ever optimize on, but in this problem, yes. So remember that everything has to be taken in perspective, of course. Perspective matters. So weight of the car. So for example, in my case, I tend to be environmentally very conscious, or hope well i suppose we all believe we are environmentally very conscious when somebody looks at us objectively and says you're not i try to do my little bit i'm a vegetarian the biggest source of greenhouse gases seems to be the meat and fish industry so i i tend to be, I don't go on expensive vacations, flying aeroplane flights long distance. The only flights I take are business related flights, my work requires me to fly and whatever other means I can. But there is one place where I don't actually I look like an eco terrorist. And that is in the car I drive. As a physicist, I know that when an accident happens and somebody hits you at high speed, weight works in your favor. But the momentum, mass times velocity. So if you are in a big vehicle, heavy vehicle, it will absorb more. And this literally happened. Some of you may remember this if you have been with me. I used to drive this big car, big Infiniti, a five, 6000 pound Infiniti. My life literally got saved in my entire life. I'm a very cautious driver, I'm happy to say. I've never had an accident. And I've been driving since the age of 12. I used to drive my mother's car. Since 12 or 13, I got behind the wheels in India and started driving. I've never had an accident. I was very, very cautious. My colleagues used to say, when we would go out to lunch to a restaurant, people wouldn't want to car ride with me sometimes because they would feel that they would reach last if they went with me so i was very cautious but then i had an accident there was a police chase or something happening there was a guy who was running away with a stolen car and i was crossing over a highway overpass and this guy took an exit from the highway a highway overpass and this guy took an exit from the highway at full speed at 65 70 miles 60 50 60 miles an hour and he tried to at the traffic light right at the exit he tried to make a turn and i was coming literally like this and he completely t-boned me, my car. He hit my car head on at 50, 60 miles an hour. And his car took such a hit, it spun and hit my car and again spun and hit my car twice or thrice, right? And obviously before I could even react because I was happily going in my lane, looking at the traffic ahead, I had the traffic light and going through. And this car just escaped. And he went out of control because it was a police chase. He came and hit my car. And that car got, it was some Lexus. It got completely smashed. And the car spun a few times and stopped in the road. The guy was, it took him a while to regain consciousness. He got, and stopped in the road. The guy was, it took him a while to regain consciousness. He got and he ran away actually ran away. What I found out is not only did he run away, once again consciousness, within five minutes, he car jacked another car and stole another car and escaped. By the time the heist, the police could chase him down and really capture him, I don't know how he did it, but he managed to escape by stealing another car within three, four minutes after that. But it was the only accident I was in. But imagine being hit straight on your driver's side, on your door, and two, three times, the car spinning and hitting you. I would not be alive here today and talking to you guys, if I was not sitting in a heavy car, that's the first thing that came to my mind. It was a big Infiniti, like the biggest one, the QX80. In those days, it used to be called QX56. So big heavy car, I still hold on to that car, even though it's old and it's battered up. And I like to drive it simply because it saved my life. Literally saved my life. And I keep thinking as a physicist that you need to be in a heavy car. It doesn't make sense to optimize on that. And this was one occasion which literally happened and proved the point anyway so be it so weight is most important horsepower don't buy high horsepower cars because that will kill you actually if you really think about it environmentally what you should buy is big heavy car with not a very powerful engine which picks up speed slowly but then you'll be at risk when you merge on the highway. Anyway, there are many considerations. What I tend to do is have heavy cars, which have high horsepower. So in matters of car, I'm an eco-terrorist. So you mean to say Teslas are not worth buying in terms of weight and safety? See, here's the thing i'm on the fence with tesla i'm told those are heavy cars and they obviously give you extremely good mileage the reason i've been on the fence with them is the software glitches my friend went to a restaurant at night and when he came back to return home he found he couldn't get back into his car because there was a software glitch that has locked him out of his car right so i'm waiting for the technology to mature it's an amazing car but it has glitches so as if the model x is pretty heavy so about about two weeks, two months back, Jai was driving the car and somebody came from back and they hit like a 65, 75 miles per hour. the only damage you can see in the back, of course, they're charging a lot of money to fix it because of the software and technology. But if you see the physical damage, the appearance, it looks very little. Yeah, I'm on the fence. My time is over, I'm getting old now. My fascination with cars is more or less fading out. It's my kid's time now. My elder daughter who just got a job in Snap, she has been toying with the idea of getting a Tesla. She always wanted to get a Tesla with her first salary. So who knows? So there will be a Tesla in our family soon. We'll see. It's a lovely car. I mean, I'm very tempted. It's completely software driven. We teach machine learning. The car has tons of machine learning in it, AI in it. I would love to get it. I was just waiting for the glitches to go. So here we go. So this is true for linear regression. But there is a technique called Shapley values that I would say it's the last topic of today, I'll talk about. There was a great mathematician, I think he's still alive or not, I think he may be alive. Shapley. Have you guys seen this movie, A Beautiful Mind? John Nash, John Nash's friend and colleague, Shapley. He got the Nobel Prize for answering one important question. He wrote a very simple equation, which when you see it on paper, you say, well, it's obvious, almost, right. But he wrote that. It was game theory. John Nash, as you know, did game theory of the Nash equilibrium points. Excellent, Premjit. You know your game theory. So did you read the book also or did you just see the movie? I haven't read the book. You haven't? Oh, the book is far superior to the movie. Movie doesn't do justice to the book. And John Nash used to actually live close to the Princeton railway station. And I am pretty sure, I'm not 100% sure, I'm pretty sure I've sat through his actual lecture at a time where I didn't know that he was such a great man. Long, long ago, it was ancient history, but I may be wrong. I don't know. So what Shapely answered the question was, suppose, imagine a team player. When you play a cooperative game, as opposed to a zero sum game, a zero sum game is chess, right? The only way one side can win is if the other side loses. The two don't cooperate and try to win together. You see that, right? Chess inherently is a zero-sum game. One guy will win and one guy will lose. Those are called zero-sum games. But then there are games like, let's say, cricket or team sports. In any team sport, let's say cricket, which is most familiar to this audience here, what happens? All of the players, they need to work together to win. Otherwise, the other side will win. Isn't it? So it's a collaborative sport. It's still a zero-sum game between the teams, but within the team, there has to be a strong collaboration. It is a cooperative gaming in such a situation suppose the outcome is positive you win now the question is suppose i have a pot of gold what is the fair distribution of that money how would i spread it how would I reward people based on their actual contribution to the winning? Now, you cannot just say the captain should get more or the leader should get more. So how do you determine the fair value of it? And there is a way of doing that. And I can go into that. But the simplest idea is that create teams or sub-teams. Okay, one naive way, which is not shaping let's call it baby shape you it's simply take a game take a team in which this guy is there and this guy is not there a team member is there and that team member is not there and then see what is the outcome and see how much improvement in the outcome there is because of because of this team member are we together because of this team member how much of an improvement in outcome was there right but then you can say well you know what uh there's a problem with this this team member didn't make much of an improvement because there was exactly one another person like him right so took over now what happens if you take both of those out and have a team without both of those and with both of them then what happens the difference is substantial so you can say that half of the difference we can assign to each of the two, right? So this is it. Shapely, what it does is, now continuing that idea forward, what it does is it creates all sorts of teams with or without this person there and looks at the average difference between all of them. And that is the contribution of this person. And then you do it for all the other players. So those contribution values are called shapely values and for this particular idea shapely actually got the nobel prize right simple but profound idea because it addressed a key question in economics how much is a person worth being rewarded for contribution now what has that to do with machine learning? It turns out as people are beginning to realize, game theory has a lot to contribute to machine learning. People are only beginning to realize that. They have realized it for some time, but more and more they're realizing that. So here, suppose we do this. You have a model which is effective you got to an effective model right the effective model has all sorts of predictors there how important was this predictor in getting an effective model in getting a good prediction that matters right and you can talk of the relative importance how much contribution did each of the predictors bring because that is literally the feature importance or the predictor importance do you agree with that right and so you can treat it as a game you can say that if you treat the predictors as a team and the game being make an effective prediction how much did each team member namely each predictor feature contribute to the outcome that is its features or predictors importance so that is what is the shapely value in python there is a package called sha which helps you do that it's a it's a lovely package that is very deep and it has many things in it. We will gradually come to that. But for now, we'll start with something simple. We get to the do deeply the presentation of which features computing more one has necessarily made the assumption that the model that is starting with as just the right set of features which gives it the maximum in terms of the hardware not necessarily all it means is that there may be still an unknown feature which is even more important but it is relative importance of the features that you have built if you're happy with this predicted model you consider this is the best i could do given the data that i'm available the question still remains what is the relative feature importance so then if you stretch that argument you would say once i remove for collinearity you will still put them back in yes so what happens is that that collinearity argument actually this is one of the reasons why when you do shapely you have to be careful because if you remove predictor whatever contribution you gave to this suppose this was one amongst three you have to remember that this is one third between those three predictors you have to remember to do that right so in other words if you say for example weight is most important so you you must remember that not only weight acceleration is motion i mean not acceleration but displacement is most important right and what was the other factor that we removed uh for because of weight we removed two other factors um where are my factors that we removed cylinders cylinders so if weight is most important then displacement is also most important and cylinders is also most important so you have to remember to bring back that interpretation that is it and now what happens is this is peculiar to linear regression models when you go to linear models but when you go to non-linear models you never throw away the, collinearity doesn't exist, so you don't throw away correlated features, right? So this is the interpretation of it. So now when you use the SHAP package, you will see that sure enough, weight is number one, horsepower is number two, year is number three, acceleration is four. You see that, right? and you see it here weight horsepower year acceleration you see the same order right so for linear model the advantages and it's one of the things not many people realize that if your data is killed the easiest way to see feature importance is literally look at the coefficient it is obvious from the geometry of it isn't it so again one more question yeah so in the previous one when we were just looking at the coefficients we have to make sure that we have normalized each of those variables yes yes you must have but for shaft is it necessary or would share package take care of all that and share package, take care of all that. See, here is a basic rule. Do not normalize the data, not standardize the data. It's almost like waking up and going to breakfast without brushing your teeth. You should always do it. It's not worth thinking, in this situation, could I have not standardized the data? You don't. You wake up in the morning and you standardize the data. Then you continue on to anything else, right? So just make it a habit, always standardize the data. Linear model is the only model that sort of doesn't fall apart if you forget to standardize. But even then the gradient descent gets adversely affected. So just always standardize the data. Consider it to be good hygiene. Wake up in the morning always standardize the data consider it to be good hygiene wake up in the morning standardize the data and then brush your teeth right sort of like that then there is something called partial dependency plot what it means is that so if you look at the weight and the miles per gallon you're seeing if all other factors were the same. You know, people in economics use this language. How much does, for example, if government issues a lot of money, how much does that affect inflation? All other factors being the same. People are still manufacturing what they are manufacturing. So it is the partial dependence of partial relevance. So that is partial dependence. How much does the miles per gallon depend on weight? When you look at it, quite a bit, by the way, it's not weight, it is log miles per gallon and log weight. Remember, that has a linear relationship. Quite a bit. How much does the log miles per gallon depend on log horsepower? Quite a bit, but not as much. You notice that this is steeper than this or on little bit steeper this is little leftmost is little steeper than the middle one and all of them are far more steep than the acceleration acceleration doesn't matter that much right and you can see it in the feature importances also acceleration doesn't matter that much right and you can see it in the feature importances also weight horsepower they they basically kill compared to acceleration which has only a six percent influence right and so these are partial dependency plots so guys what I'm saying no this is a known data set of course many. Many people have analyzed the Kegel notebooks, this notebook, that notebook, medium notebooks, etc. Generally, they all do a good job. But I'm trying to raise you, I believe, to a higher bar, to make you stronger than that, to make you at a place at which you do a rigorous and careful analysis of the data and build models very very carefully i want you to it's a long analysis of a simple data set but i want you to get used to this analysis right thorough careful methodical analysis of the data and get into the habit of doing such careful analysis right don't be like one of these people who just does a small analysis and promptly writes a media, which is good. You write a medium article to say, yeah, I learned how to do analysis. It will be useful to somebody. They too are contributing value, but you contribute more value to society and to the field if you submit a notebook in which you have done a very thorough and careful analysis of data, are we together? Make it a habit, because when you do very methodical and careful analysis of data, you are much more likely to hit upon a breakthrough in case a breakthrough is sitting, waiting to be discovered in the data. Do that. And remember, all breakthroughs come from the data eventually. right do that and remember all breakthroughs come from the data eventually so with that thing it's 540 it's been a long day guys i will let you go let you folks go i was going to cover two more things i suppose i'm ambitious i'll give you an idea go read this one of them is the california housing data set in californ, everybody seems to be worried about their home values. People who have home keep hoping it goes up. People who don't have homes or an apartment keep hoping it crashes. Right? So, well, this is it. So, the same analysis I've done, more or less along the same lines methodically except that because it's geographical data I plotted it out we'll talk about it next time but if you take some time to study it and my notes are here perhaps we can move faster right and you will see that it is along very similar lines there is something new here and I've used only linear regression There is something new here, and I've used only linear regression. Later on to the same data sets, we'll apply nonlinear techniques. We'll apply gradient boosting. We'll apply random forest. We'll apply support vector machines and whatnot. And we'll see how they respond to more sophisticated models. But this is it. Please do study this on your own. The other thing, the topic that seems to be missed continuously is the topic of tidy data. So I'm thinking because these topics are overflowing and now we are getting into classifiers, we may not get time, I will hijack some of these paper reading sessions instead to talk about each of these topics independently. They are of value to even a general audience. So if other people from outside join, even better. We'll give one session to tidy data, one session to, well, California housing, I'll do, it's important, so I'll do it in the next session, but one session to this collinearity on Tuesday. So we'll take those times to talk about those topics. But with all of this, at this point, I would like to end by saying, guys, if you have been studying the lab, if you have been playing with data, if you have been doing your homework, by now, you should feel reasonably familiar with regression i wouldn't say you're mastered it learning is cyclic the first time you learn something the best you can hope for is you understood and is beginning to feel familiar the second time comes deeper comprehension and fluency comes in a few cycles learning if we have learned anything about learning is learning is cyclical you have to keep revisiting the same thing and every time you revisit it gets deeper it's one reason i have made invited people to repeat this class over and over again free of cost so that now you don't have to necessarily repeat all the way but as you learn this keep seeing if you can get opportunity to keep cycling through what we have learned go back to the videos take the quizzes do the projects do the labs now as you do the homework guys please submit it or at least do it if nothing else do it because unless you do it yourself you will never build confidence right here i notice that not many of you some of you have been doing it some of you also have not been doing your homework do it i know we are all busy we all adults we are professionals we never get time the time is the most precious thing in our life but if you to the extent that you have the aspiration to master all this that will only happen if you give it the commitment of time please do give it i will end with that thank you guys and thank you kyle for staying up so late into the night practically it must be six in the morning by now she's amazing yeah yes thank you nice moringa notebook too one second kate please go ahead oh her moringa notebook was really good too yes her moringa notebook was amazing so i would like somebody else to present uh sanjeev let's let's have you present your project next time yeah i will do that as if yeah definitely yeah go ahead anil is already there on the web page oh to the url is to the local one let me see i'll fix that i'll fix and post it again power transform no oh i have to update the support vectors common okay i'll fix all of those i will i will do that in fact i should do it right now why don't i come and check? One second. By the way, guys, are you finding these libraries useful? I mean, not the libraries, the notebooks. Notebooks are great. Notebooks are useful. Good. So. Notebooks are very helpful. You can read it like a book, at the same time there is a quote to look at. Yeah, yeah. They are literally part of the book I'm writing. I haven't done justice to it, so I need to do justice. um can you please do that again yeah and the file form is the auto oh auto let me do that so let me update that uh auto the auto is pointing to the remote file no yeah it is pointing to the california california california is a problem no no it shouldn't be a problem just get the support vectors library and after that run it i'll come and sit with you and make sure it's all working ah housing is the problem housing housing i have to upload. Housing is the problem. Housing, housing I have to upload. Yeah.