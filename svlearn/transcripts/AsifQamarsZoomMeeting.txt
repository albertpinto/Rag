 Дякую. so okay All right, 10 more minutes and we'll start. Thank you. How many of you got a chance to read the paper? I tried reading, Asif, but it's a little too terse. So I kind of read a couple of pages, then kind of left it. Couldn't wrap my head around the overall concept. So made an attempt. Thanks. Anybody else? Pradeep? Did you read this? No, sir. I was preparing for exams next week as final exams. Oh, final exams, right. Yes. And I can follow the intuition, what they are saying, but how they're building it up is, I would rather listen to you. Okay. All right, we are on time. We have more people's questions So, I would say, a week or two ago. It's a very interesting paper. Usually I don't get papers that come out that are absolutely fresh. We have been covering the classics in some sense, the big papers, the word, the attention is on repeat and variety of other papers which are very important, considered in some sense landmark papers and well established. Today I'm going to cover a different paper which is just come out we do not know how much impact it will have but it certainly makes some very strong points it is as it belongs to a line of investigation that people are pursuing fairly vigorously these days that tries to understand what exactly is a neural network doing. When we take a neural network and it has lots of layers in it, a deep neural network, you have enormous amount of parameters. These days 100 million, 500 million, billion parameters, even 100 billion, 200 billion parameters is considered pretty like normal. We have reached a stage at which neural networks are so big, it is very hard to impose interpretability on it and see what is it doing. So what we do is we apply interpretability by different means, like we make approximate models to mimic its behavior in local regions of the feature space. We do things like that, but we have no direct way to just layer by layer see what it is doing. Our current understanding is that, and it sort of is motivated by congulation neural networks, we see that each of the congulation neural networks, what it does is layer by layer, it seems to do feature extraction. In the early layers, you have the primitives, geometric primitives, the edges and things like that being extracted, and then higher order representations are gradually built out of those primitives, the edges and things like that being extracted, and then higher order representations are gradually built out of those primitives. So the the current understanding is deep neural networks work through representation learning, or some sort of a hierarchical learning as you go deeper and deeper into the network, the next layer builds upon the representations that the previous layer has learned. Which is very good, but we seem to have hit a little bit of a wall. It doesn't further interpretability beyond that. So a lot of people are very seriously looking at the deep neural network and asking, can we look at it from a different perspective can we reorient the way we understand this whole thing perhaps in such a way that it becomes easier for us to get a grasp what is happening inside and if we can grasp what is happening inside or perhaps we can further we can accelerate the pace of research in the genetics unit. As you know, we are making a lot of incremental progress, a wonderful incremental progress, though some people believe that we are not making fundamental breakthroughs, including Jeffrey Hinton himself, who seems to believe that we have hit a wall. There is a need for more work, more theoretical breakthroughs to further or take things further. I consider this paper interesting. At this moment, I suppose that the research community will gradually weigh in on this paper and will see how much impact it has. I thought it was interesting, so I think we need some very good points. We are going to take this paper, dissect it, see what it says. It's unfortunately a set of very simple statements at night, but very insightfully it makes the statement. So let us pursue the reason that is there in these papers. Now, in this particular paper, the title itself is quite provocative. It says that every model learned by gradient descent is approximately a kernel machine. Now, let us limit ourselves to deep neural networks for the time being. In deep neural networks, if you remember, deep network or deep learning, essentially the pillars of our progress have been, one of the basic ideas have been that you do forward compute to you do backward propagation propagation of gradient computes. Basically, gradient of the loss with respect to the weight of each of the layers, if you keep on back propagating it layer by layer and then the third step you do is the gradient descent descent where you essentially say the next value of the weight of any weight vector is essentially the current value of the weight vector minus the learning rate typically we use alpha and then we take the learning step so i hope this part is pretty well established in our minds now that the forward pass is just matrix multiplication simply multiplication, simply matrix maps and activations and activations at each layer. So these are the three steps of a deep neural network training. And of course, evaluation is just, eval is just the first step after the weights have been trained. Now, I will just, so the crucial thing here is to note that of course, we do gradient descent. This is what we do. We do all sorts of gradient descents. We do batch gradient descent, stochastic gradient descent, momentum-based gradient descent methods, add-on this and that and so forth. But ultimately we do gradient descent. Gradient descents are considered first order methods because this is a first order derivative of the loss. So just to recap, L is the loss function. It is some function of y and the gap between y and y hat. So, for example, a loss function could be y minus y hat For example, a loss function could be y minus y hat squared. Suppose I do this. This would be a sort of a sum squared loss. And if you divide by this, it is an MSE, mean squared loss. So there are many loss functions. You can have cross entropy loss, loss and so on and so forth, lossification, regression, and you can cook up a more interesting loss functions. But invariant of which loss function you do, at the end of it, it is back propagation of the gradient computes, and then you do the gradient descent step. You actually take the gradient descent step. Now, coming back to this paper which is not really is it says every model learned by gradient descent. So that's a pretty general statement. It's saying that not just deep neural networks but just about every model that you learn by this process is approximately a kernel machine. Now let us recap what are kernel machines. Kernel machines is a synonym for or also called support vector machines. Let us recap what they are for a moment. But before we recap, just observe what the title is saying. It's saying something quite profound. It is saying in a sense that if you are doing gradient descent in your model, you are basically some form of a support vector machine or a kernel machine, approximately. And I would I would say that that's quite a startling statement to me. Let us see how they go about proving it. It turns out that they prove it in a fairly mathematically rigorous way. And we need to walk through this so what are kernel machines suppose you have a very intuitive picture and this is a extremely quick recap that i will give is a kernel machines trying to find a target a space a kernel space Karnal space where every data is linearized. So for example, if you have points, let's say that you had a data like this. And data here showed up as, let me say, a decision you had points like this. And let's say I'll put green points here. This region is green. Let's throw in a few mistakes. And a few mistakes from a few mistakes together too maybe something like this if this is your decision boundary what what kernel machines try to do is they try to find a space where the separation could be something let's say that it be so it becomes a straight line like say Let's say that it be so it becomes a straight line like say, can I bring your ruler here? Okay. Let me see if I could. All right, let's bring it here. Much better. And so let us say that it becomes something like this. And It becomes a linear decision boundary with its, sorry, with its margin, there's a maximum margin hyperplane and all of that. But here we won't go into that. So in other words, if you remember, the intuition we used is how can we straighten the river and flow the flow the widest river through data in the kernel space in the kernel space. Now for each point x, if you remember, we assume that there is a mapping function, there is a mapping function that takes it to a point, this from here, that is the phi of x. Do we remember this guys? That we need a function, a mapping function to a different kernel space where x goes to phi of x. Then we say that the beauty of this is we can trick says we don't actually have to compute phi we can get away but what we need to do are find support vectors which is that these certain light houses that can guide or define the decision boundary so for example you may pick up some light houses maybe this this this, or this and a few lighthouses, lighthouses are support vectors that help define the decision boundary, they clearly help define the decision boundary, that clearly have defined the decision boundary. And all you need to do is given a sample point, given a query point, point X, an arbitrary query point. If you want to decide whether it is yellow or green all you need to do is find the the similarity of this point with respect to every these lighthouse points these four lighthouse points let me just call it okay lighthouse points. If you can figure it out what is the similarity of this, or how close is this point in the kernel space to a few lighthouses, you will be able to tell. For example, if you point x's here, let's say it is this, you will agree that it shows that if I can relate it to these points and I can basically come up with a way to tell that it should be yellow based on certain things. So obviously I'm using hand-waving arguments. I don't want to go into the whole mathematics that we went into. Now what is the similarity in the feature space, similarity in the kernel space? Now if you remember, this we realize is the dot product of the feature vector, this and the xi. And these are the two vectors. We define the similarity as simply the dot product, which is the most intuitive notion of similarity. So we are looking at the dot product of these two points in the kernel space and that's a measure of similarity. And the whole point of support vector machines or kernel machines is perhaps you don't need to do that for all the points in your training data but you can hand pick a few the so-called support vectors, you make them into lighthouses and quite often they're enough to guide you through. In the worst case, all your training data can become sample or lighthouses or support vectors. So this is your essential theory of support vector machines. So Asif, just so that we can understand that concept clearly, right? The concept of the lighthouses, so I have not done support vectors earlier. So that's the, I'm trying to kind of get an intuitive sense for it right now in this conversation. So that the lighthouse or the support vectors that you mentioned, are they necessarily around the boundary line? They are. And the proximity to one or the other tells you which side of the boundary line you are on is that the intuition somewhat similar yes that is the intuition you can take that to be roughly the intuition okay so that is that so there are two steps to it. You need a kernel mapping function, what you call the mapping function. And you need a kernel function. So these are called kernel functions. Kernel function. And it's to be distinguished between the mapping function itself and using these two things, actually the entire theory of kernel methods is developed and it's a very rich literature that we have here. It's a whole world in itself, just like deep learning is a whole world in itself. If I were teaching the same workshop, let's just say 15 years ago, then I would be given a long four months workshop on just kernel methods. But at this moment, we are going to limit ourselves with just this very basic explanation. Now those of you who have taken support vector machines or things like that in the previous workshop with me, do you guys remember what I'm saying? Anyone of you would like to confirm? Yes, yes. The idea is that when you use this- That's the maximum margin classifier. Exactly, the widest river or the maximum, I think it would be maximum margin. And I've tried as much as possible to make it intuitive in terms of real things that we see, rivers and so forth, but that's what it is. Asif, can you hear me? Yeah. These terms that you use, I had this question when I take the session on support by permissions also the lighthouses are these sort of common terms used to explain i use those terms you use them i feel they're going to put lighthouses in their banks okay all right you won't find this in the literature the literature is more abstract it clearly explains the concept and gives the intuition. I was just wondering. These are my interpretations to explain these things. So you won't find it elsewhere. So now... Kernel space is always a higher dimensional space, right, compared to the space that you started with, correct? Not necessarily. I'll give you an example. So let me pose this question here as part of our notes so that you have it. The question is, are kernel spaces always higher, higher dimensional spaces compared to the input dimension. Yes. This is the question. So I'll give you a counter example that will hopefully, but broadly speaking, yes, it is true, but there is a counter. I can give you a really quickly a counterexample. So let's say that you are looking at data which happens to be in this circle. And from the circle, you can generalize to an ellipse or go to a higher dimension to a sphere or an ellipsoid. But let's keep our discussion simple suppose data points are here okay and the green data points are actually let me not use plus they look like crosses but let me not use plus, they look like crosses. Let me use something like this, right? And we can throw in a few mistakes here and there, error points. Let's throw in some noise here. I'll throw in a couple of noise points here. Okay, noise points and we'll throw in a few yellow points outside also uh sorry crosses outside also for what it is worth so now you look at this problem and uh certainly the decision boundary what is the decision boundary here it is this guy so the decision boundary looks circular, circular, right? In other words, non-linear. Arithya, we got that? Now, we ask this question, that was Arithya speaking, isn't it? Yes, yes. So I guess I recognize you. So now imagine that I do one transformation, x, let me call this axis x1, x2. So suppose x1, x2 goes to this mapping function which converts it to x1 squared and x2 squared as simple as that right and see what this transformation does to the data now my new axes are Now my new axes are, let me call it is equal to the phi x1, phi x2, right? So in this phi 1, phi 2 space, each of these points when you put, you will realize that, see, what is the constraint equation here x1 square plus x2 square is equal to 1 or some value some radius i'll just take maybe why take one let's leave r squared some r squared would you agree that this is the constraint equation for the surface yes right for the circle that is we all know that this is the constraint equation for the surface yes right for the circle rather we all know that this is the equation for the circle now what happens in this space this is phi x1 this is phi the the x2 so this is phi one rather uh let me just apply Phi 1 rather. Let me just phi. Phi 1 affects phi 1. Let me just write phi 1, phi 2. Phi 2. Would you agree that this would become this? Let me actually let me write it here because that will illustrate the point phi 1 plus phi 2 is equal to a constant r squared. This is the this is the this is the constrained surface now. Now, what is this equation about? This equation is actually a line that goes like this. Right. This is the line for phi 1, phi 2 is equal to phi 2 is equal to r squared. You can convince yourself. So when phi 2 is zero, this this much is r squared. So when Phi two is zero, this this much is r squared. And when Phi one is zero, this much is r squared. And now all your points go here. And all your points are outside. So you have gone to a space which is linear, but what's the dimensionality of the space that it's here? This is like a linear. Yeah, but the dimensions, this is, this one is R2 Euclidean space, this one here. And this is also two-dimensional Euclidean space, isn't it? Yes. Just a plane. So in other words, when you go from here to here using the phi mapping, it is not necessarily true that you have to go to a higher dimension. Sometimes you don't need to go to a higher dimension. You can stay in the same dimension, but you just need the right transformation. Okay. But it never go to a lower dimension, right? It can be either same or higher dimension. Yes. Usually, I mean, you don't see it go to lower dimension. See, in practical terms what happens, right? You use the Gaussian kernel and such things if you remember the rbf kernel or the gaussian kernel they go straight away to infinite dimensional inversion space if you remember from the last time we go from any problem we take it to infinite dimensional elevation space elevation space Hilbertian space. Hilbertian space. Hilbert space or Hilbertian space. Yeah, because everything is expanded in terms of infinitely many Gaussians, if you remember. So that much you may be not remembering that that got technical by the end of the simple partition. So in practice what you do, people do is they directly go and launch into a high space and figure it out. But in at least in the theory you don't need to go to a higher space. Okay. So this was our basic thing. So now again, I'll end it with one basic intuition kx. If I have to, given a query point, a probe, an xi, and a data point, so we'll use this word for query or probe, query point, data point, and this is the probe, this is the actual data point in training data. We'll use that. And what we're seeing is it is the dot product of phi X and phi Xi, the data point. So how similar this point is depends upon a kernel function which is nothing but the dot product in some kernel space some kernel space and that is the one important thing to carry forward in this paper with that being there you say and there's a little bit more you see that y remember it's a linear equation right so y can be written as the out can be written as basically in terms of the kernel so you take the kernel functions x and xi all the points in the training example right and what you do is this is the point that i'm going to mention to you you take a linear combination of these linear so this is a linear combination of all the similarities add a bias term also for what it is worth b and then sum over i mean linear combination would imply this so in other words it will be like a1 kx x1 plus a2 kx x2 in the data point all the way to b are we together it will be like this. And there is one more thing that you have is that given this, you may also apply a non-linearity function, an activation function to it. I'll just write it as G because the paper writes it as G. A non-function overall. And you can say that y is equal to this. So it is the linear superposition of the similarities to each of the data points and throw in a bias term if you want, and then if needed, apply a nonlinear activation function. So that is what is being said here in this particular equation. Let me mark this. So this equation now hopefully makes sense to you. So with that background let us go and read this paper from the very beginning. I will read out the abstract. This abstract is... Asif sir, can you increase the resolution? I am doing it. That's exactly what I'm doing. Give me a second. Much better? Yes. Okay. So it goes on, it says, and so first first of all i always like to go straight to the keywords what are the keywords here so yeah you'll be talking about gradient descent we'll be talking about let me make the concepts these are the prints kernel machines are support vector machines the deep learning so it will limit itself to deep learning our current understanding of deep learning is representation learning. And now comes a new concept, which I'll explain, the neural tangent kernel. in research these days. People are taking this quite seriously and paying attention to it to see what all things it is saying. So these are the key words, you know, every one of them except the newly tangent kernel angle. Now let's start reading the paper. Deep learning successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. In a sense, the power of deep learning was that you didn't have to do a lot of feature engineering. The idea was that hierarchically, layer by layer, it would discover the feature on its own. Isn't it guys? So any doubt in the first sentence? sentence. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equal to kernel machines. So that is sort of the important feature, I suppose, that we are talking about here. Let me take this itself. We show This sentence is the most important sentence. It is just one sentence. I'll repeat it again. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines or support vector machines. A learning method that simply memorizes the data and uses it directly for prediction via a similarity function. So do we understand what he's basically saying? He's alluding to the fact that we need to just pick out a few lighthouses and remember them and using. And once we know, once we have a similarity function or the kernel function, then that's all we need. Given that kernel function and given a few support vector machines, we can solve the problem, right? We can do classification regression, whatever, let's take classification. We can do that. that is that easy now intuitive guys uh so i say like what does this memory exactly means like is it um if you memorize like is not is that not like overfitting no the problem is it if memorize it, you can only respond to data points that you have seen. The point of support vector machines is it sort of remembers a few instances, the support vectors of the lighthouses. It memorizes only a small bit and then it uses to generalize from there. In fact, support vector machines are very robust to overfitting because they memorize only a few data points. They discover the real gold, the real ones that matter in the data and light them up as lighthouses in effect, make them into support vectors and use them. That's how the support vector algorithm works so it's sort of okay so is it so is it like a in essence like a generative model right like so based on few points i can generate and get like something else not a generator in a discriminative model but given a few points i am able to do the classification for any point right for any point feature space i'm able to do the classification for any point right for any point in the feature space i'm able to do the classification still discriminative but yes okay so so i see a quick question here uh why wouldn't you use the word centroid for the support vectors because there's nothing no relationship of centroid to support vectors is it is it not the like if it's trying to distinguish between two spaces aren't these support vectors in effect finding the the centroid of the space that's not it they are not i think they're not you have to sort of go through that class, the ML200. OK. Because we developed the support vector theory in great detail. And it's too little time now to cover the ground. But suffice it to say, it is not based on the centroid. The difficulty I'm having is trying to if something is on the boundary, it is closer to the other side as well that is true so me trying to find a cosine similarity with something at the boundary could easily be when the cosine similarity is something on this side of the boundary would be the same as something on the other side of the boundary correct see? See, here's the thing. Suppose I'm looking at a point like this, right? This point. Now, if you look at this point, what you do is you do the kernel with respect to all the points. But what will happen is the similarity between these points will dominate. On this side points will dominate on this side will dominate and what they will do because they dominate the overall sign of your Y the sign of Y which is with a sign as sign sign as in symbol plus or minus minus minus one or plus one, it will tend towards, let's say that this side is positive and this side is negative, right? So I think usually people recommend minus and positive. So what will happen is it will tend, the value of y will tend towards negative. It does not deal with similarity. You are doing the function, you are still doing a i, kernel with respect to all the four, like for example here there are four points, so you are doing with this point and xi, you are doing the similarity with respect to each of the four lighthouses. But the net result would be, and I'm just removing the non-linearity transform, the end result of this would be, and'm just removing the non-linearity transform the end result of this would be and let me throw in a biasing end result of this is this will still come out negative and so you would know that it's a yellow point okay so in essence these sample points or these subset of points that the support vector algorithm is able to bubble up are salient representation of that set. Yes, that is it. The whole point is that it discovers which points are the most essential in this whole story that help explain the rest of the data. Okay. of the data. Okay. So now the next sentence that is here. So guys, I'll review, reread the most crucial statement. We show, however, the deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for predicting via a similarity function, right? So the whole question then is, excuse me, what is the value of this? So the value of this is, this greatly enhances the interpretability of deep network weights. You know, these weights that you have, it gives a lot of interpretability to it or to the parameters it elucidates by elucidating that they are effectively a superposition of training examples so you know if you realize that the weights are nothing but superposition of the training examples what they do therefore is somehow they encode the essence of the data that they have seen in some sense, right? They extract the essence of the data in many ways. So that is that. The network architecture incorporates knowledge of the target function into the kernel. What is a target function? At the end of it, a neural network is nothing but given x goes in it is y of x isn't it it is or another way of writing it is a y is some function of x with respect to the weights the weights of the model right this is at the end of it, if you look at it as a box, this is what a neural network is. Would you agree, guys? Right? Not only a neural network, every single thing, every parametric model is essentially something like that. It has some function of the input. The only question is, we don't know what function. We expect it to be a differentiable function. So on and so forth. We impose good qualities upon it upon it but it is so that's your target function the target and so they go on to say that this improves and understanding this improved understanding should lead to better learning algorithm so the hope is that uh because now you understand or have better intuition into what is going on it will accelerate some of the developments coming should lead to better learning algorithm. So the hope is that because now you understand or have better intuition into what is going on, it will accelerate some of the developments coming. So despite its many successes, deep learning remains poorly understood, which is true. In contrast, kernel machines are based on well-developed mathematical theory, but their empirical performance generally lacks behind that of deep networks. So today what happens is the kernels that we were using in support vector machines, the RBF kernels, the polynomials, the linear kernels, if you use those kernels, the kernel machine performance is below that of using a deep neural network, for example a CNN for image classification. So that has been an established fact. That is why support vector machines at one time were the king of MNIST or digit classification and so forth, but today our deep neural networks have taken over. so forth but today our deep neural networks have taken over right so so we have this dilemma kernel machines are very well understood they're very intuitive the deep neural networks are not really understood at all much at all but they give you superior performance right and that's why people sometimes say that maybe maybe this new genre of algorithms cannot be interpreted in all sorts of non-sensitive statements. So anyway, we'll move forward with that. The standard algorithm for learning deep networks and many other models is gradient descent. Here we show that every model learned by this regardless of architecture is approximately equivalent to a kernel machine with a particular type of kernel. And then you go on to say many more things, but I will now go to the meat of the matter. So this is a kernel machine, the basic equation of a kernel machine, which says that the y can be computed by looking at the similarity of a probe point. So given a probe point, query point x, and you're trying to find the y, all you do is look at its kernel similarity to a bunch of lighthouses, to a bunch of them. In the limit, you can find its similarity to all the points, all the training points, and take a linear superposition of them or linear combination of them, and that is it. If you need to throw in a non-linearity, you can throw it in. But that's about it. Right. Into this argument. So now what happens is that this A.I., the coefficients of the similarity are typically functions of the labels itself, plus, minus, and so on and so forth, you know, one minus one. Kernel machines, also known as support vector machines, are one of the most developed and widely used machine learning methods, right? So the point is that they are very well understood. We know the entire books written about the mathematics of support vector machines. Mathematicians have really gone at it and made it on very solid foundations. So in the last decade, however, they've been eclipsed by neural networks and multilayer perceptrons and so forth. And people used to believe that kernel machines can be viewed as neural networks with just one hidden layer with the kernel as the non-linearity. But now these guys are going to make a different straight line. So we'll watch out for that. This equation guys is what is it? different straight lines. So we'll watch out for that. This equation, guys, is what is it? This is our gradient descent. We would have written it as w next is equal to w minus alpha gradient, the loss of the weight. Is this true? This is implicit, so I won't write this. This is the way we write it, isn't it? So what it means is that this is the same as this. In this paper, they use the word epsilon. For the epsilon for the learning rate. So that is all right. Now comes the big result that they're going to show. The result that they show and in the rest of the paper they prove it is that you're looking for that similarity function between the query point x and each of the let's say some other point x prime and typically the other point is a data point in the training data. It says that that thing is actually nothing but the gradient flows that the word people often use is that these are the tangent gradients or the tangent it means gradients are being used so we will go over it in a more specific way. So I will read it from here specifically. The kernel machines that result from gradient descent use what we term a kernel path. If we take the learning rate to be infinitesimally small, the path integral between two data points is simply the integral of the dot product of the model's gradients at the two points where the path or over the path taken by the parameters during gradient during gradient descent, right. So what happens is that suppose you x you know if you take so when you do gradient descent w you know the weight vector it will go from this to some other point w final right so this is a very non-linear, it will shift to this space in the feature space, in the parameter space, let's say this is your w space, right? If you think of it, this is your 100 billion or 100 million dimension space or whatever, but for simplicity, assume it's a two dimension space. So your weights, guys, excuse me for a moment. so um Gracias. All right. So, you know, you can think of your parameters. It's much easier to think of it in two dimensions. So what happens as the gradient descent happens, your weights will take some path. They'll go along certain values. Any one of the weights will shift a little in small increments at each step of the gradient descent. So what we are saying is, if you look at how the response changes, why changes with respect to the weight, right? That is the most important thing. And it turns out that if you look at this, this is again a dot product. Guys, do you see a dot product inside? that if you look at this this is again a dot product guys do you see a dot product inside that dot project immediately gives you a sense that there is a we're getting close to the notion of similarity here uh specifically the kernel machines that result from gradient descent use what we term as a path kernel so this is the path So this is the part. And what we are saying is that. This point, let's go. Let me let me draw it out here again. I sort of. Let me bring that equation down here. Okay. Okay. This, this is very illustrative. If you look at this, you started out at this point. And W final after a few steps of gradient descent, it has gone from W1 to W2, W0 to W1 to W2, right, to W3 here, W3 here, to finally, W final, right? This is the path that the weights have taken as the gradient descent happens step by step. So at each point, what you can compute is that y is the output. You know that the black box is essentially x goes in and y comes out, right? So what you can do is at each point you can compute and it is a function of the weights. There is some function, y is some function. You realize that it is a function, some function of weights, and of course x. So you can take the gradient of y with respect to the weights. You know, how much does y change? If I just change the weight a little bit, how much does the output change, right? That is, that sensitivity of the output to changes in weight is given by grad y, y of x. So at each of these points, W naught, et cetera, et cetera, you can compute the gradient. This gradient, You can compute the gradient. Gradient, this gradient, it sort of measures the gradient. Let me give you the physical intuition here. Gradient of y. When you write that gradient of y with respect to the weight at the point x i'll write it in words at point x that is the x input x, that is the x input, with respect to the weights, the parameter weights, that is what this is. What is the intuition? Intuition is, it is the sensitivity at input X to output to small changes in the input it's not not small changes in the input i take that back it's small changes in the parameter weights the weights here excellent to the weights how sensitive it is that's what gradients measure derivatives measure sensitivity and now what we are saying that if you go back and look at the definition of that this kxx, I'll just write it here. It is saying that the similarity of the point in the kernel space between X and another point X prime is nothing but you take a path that the weights have taken. Let's say some path that the weights have taken. And this is the gradient of the weights at the of y of x right at this point x in the parameter space what is the gradient right dot product it in my notation i would write it as this dot product it with y of x prime of the weights so this is sort of the notation i'm deliberately picking a notation that we are used to right and you compute this so this is a dot product a dot product is ultimately uh itself a function and you compute it over this over this entire path. Let's say this is W naught, this is W1, this is W2, this is W3. Let's say, let's make it more W final. So now what does it mean? Let us see what it means, right? What does it mean to take the path integral? So the explanation is very simple. Suppose you have two points. Let there be two input points, x1 and x2. These are two input points. Then you compute k here, the K. So let's go here at this point, let's say that the gradient of this, of just an arbitrary point given a query point, query point, think of it as any arbitrary point X, right? Which is not in the data. So remember you have to make prediction for X, any arbitrary point in the feature space, you have to tell whether it's a cow or a duck, for example. And that point is the entire, any random point in the feature space, treat it as a query point. And you have a limited set of the data points on which you have trained your model. So let's take this. What you do is you find y of x with respect to the weight at this point let us say that at that point the with respect to grad y of x1 is this with respect to the weight and let us say that the grad of this but no i shouldn't use this i can use a different color let me say blue goes like this grad w of y x2 at x2 so now look at the grad the red. Which is it closer to the yellow or the blue? Yellow. The yellow one, right? It's closer to the yellow one. And so you would say that here, this is much more similar. Then you can go and do the same thing here and see what values come out. Now what do you do? See the integral can be also thought of as just sum. Sum or average, right? That's the intuition behind integrals. I mean, I'll just take sum. If you take discrete point integrals become sums. You go to this point now and now I'll show you the picture that he has drawn, which is very descriptive. And let me. Look at this at this point. Look at this. At this point, the y of x, the gradient of y, it is closer to x1 or x2, similar to the gradient with respect to x1 or x2? x1. x1. What about at the point w2? Again x1. Again x1. Then what about w3? Same,3 same x1 same it seems to be similar so what you do is i mean obviously he has taken three points an integral is nothing but the sum you know of discrete points in the continuous limit so if you keep on doing this at the end of it, what do you conclude? This guy is more influenced by what? X1 or X2? X1. X1, right? It's more similar to X1. So if you look at the value that is produced, the output that's produced by X1, it will be a closer representation of the output than X2. Right? And that is the main thing. It's a very simple idea, very elegant idea that they represent. So I'll just read out the sentence, the figure one, how the path integral measures similarity between examples in this two-dimensional illustration as the weight follows a path on the plane during training, the model's gradient vectors on the weight plane for x1, x, x1, x2 vary along it. The kernel is the integral of the dot product of the gradients and this y, the x and the probe and the x1 over the path and similarly for x2. one over the path and similarly for x2 because on average over the weight part this quantity you realize that this quantity will be greater because these two vectors are more similar along the path than this vector y1 is more influential or has more influence than why to in predicting why predicting the output for this query point all else being equal and that is the gesture of what they're saying. R. Vijay Mohanaraman, Ph.D.: Now, R. Vijay Mohanaraman, Ph.D.: There is a I don't know what is the time check. How many of you are interested in the mathematical derivation, is 1 pm we are already past an hour uh let me go i have one question about this so about this dot product so it seems that you are assuming only the cosine of the angle right but what about the magnitude of the vector itself dot product is both magnitude sensitive also remember i'm not right i'm not scaling it down by the magnitudes it is a full dot product right so so for some points it can be that even though the vectors are not similar but magnitude wise it can be more similar to the x2 that is right that is right with a little bit of a cover and i'll come to that when you work through the mathematics right uh that sort of gets taken care of but okay uh but yes in theory that's possible so first is we define some terms the the word the tangent kernel is it comes from previous literature people have been researching it for quite some time associated with the function so this is your y y is equal to some function of x which you don't know some unknown function of x and the parameter vector needs some some weight at this point some value of the weight and so it defines it to be the dot product. This is just a matter of definition with the gradients taken at this point. The second thing is now that you have this definition, the integral of this, the path kernel. So given the tangent kernel, the integral of the tangent kernel over a path is the path kernel, right? This is a point. This is it. And the theorem says, suppose that you have a model like this, and it makes, it goes on to say that in the limit of a very, very, very small learning rates. So learning rates going to, tending to zero, it's extremely in the limit of learning rate going to zero, tending to zero. See what happens is that this statement will come true. Y is nothing but a linear superposition of the kernel's similarities with respect to the data. And that's a profound statement. You're saying that in any method that uses gradient descent, you'll always have AI is equal to, well, profoundly enough. To me, it's very interesting. So it is a linear combination of the kernels, right? Or the similarities to known data points. What it means is in some sense, a neural network develops two things, an internal memory of the data, which it encodes in the weight and ability to generalize from that. So that's the way I see it. So now it says that this is true. Now the question is how do we prove that this is true? The first thing is you realize that we wrote, this part is obvious I hope minus W over epsilon is equal to minus grad right so that is the result they are speaking of here this is very easy now what you do and this is the part when you take the limit of epsilon becoming very small this is basic calculus this becomes dw and you treat epsilon as a variable dt let's say epsilon is equal to dt a very small change of the of the learning rate so for very small changes of the learning rate the change in the weight is equal to minus right and so you realize that you can write it like that. Till now you have been thinking of epsilon as just a constant learning rate, but here we are flowing it. You're saying it's a variable. And for, if you just look at how the weights change with respect to the loss, right? The gradient of the loss by changing, small changes to the learning rate, how much does the weight change that is proportional to or equal to the negative of the loss gradient so that's the statement they're making so they're converting this discrete to continuous to a continuous statement so this concept this is called the gradient flow right this is known as the gradient flow. And there has been some literature about it for some time. Now comes the chain rule and the rest of it. Just observe that this statement is of course true. This is nothing but chain rule. Why is a function of the weights? And it is also a function of time so if you want to not time t t is what is mentioned here as time is actually the learning rate flow so dy dt is this equation is obvious guys this is just the chain rule right and so so when you plug it in with this fact here into this equation, it becomes this equation, which again is very straightforward, right? This part, all you're doing is you're substituting the right-hand side from here, and it becomes this, as you can see. Well, that seems interesting enough. What about going a little bit further so now you say that all right i don't want to do it directly like this i want to say dl dwj is equal to dl partial derivative of the loss with respect to the output for each of the data sample the sample points here and y i d weight j right which is again the chain rule and then of course when you do it like that you'll have to sum over all of the i's and so forth so this is the part this becomes this you can write it as this again nothing but your basic calculus chain rule. And surprisingly, that's all you need to do. After that, you're looking at the result quite a bit. So when you rearrange the term, it becomes this. It's the same thing as this, take the minus sign out, just take this guy to here, and then you end up with this sort of things now they say that suppose right you apply some notational stuff l prime is equal to d l d y right this part l prime so you have the sum over all l primes times this part. And they are saying that this part is essentially, if you look very carefully at this part, you will realize that, see this, what is this? dy, dwj, dy, i, dwj. So it is the function of y at x. And this is the function of y at x and this is the function of y at what y i is nothing but y x i right and therefore what is this when you do the dot product over j this is nothing but grad of y with respect to the weight at x dot product with grad of y with respect to the rate at x1 right and so you have already shown that this tangent kernel is involved and so in the limit obviously this is a limit equation now if you remove it like if you if you have d y or d t is equal to something let's say x, would you agree that dy is equal to x dt, which means that y is equal to the y previous value, y naught, right, plus x dt, except that x in our case is this minus blah blah blah, right, so this equation therefore follows. Now what is the value of this equation? It basically says that if you want to see the change of y, right, with small fluctuations, then you just have to look at that a path, sort of a kernel. Now the rest of it is, you know, just going through more vocabulary, creating more terms. You can normalize it by multiplying by this and denominator. So this takes care of the idea of that. Remember you were saying of the normalizing factors and so forth. In some sense it does that. This is it. You continue here. And the rest of it is not terribly interesting. All it says is that if you can create this, this is your kernel. And this part, if you write it as this, L prime, it's a matter of definition. You just define it to be L prime, right? So then your equation therefore becomes this. So long as your definition of AI is this complicated integral, that's all. It's just change of notation. But at the end of it, what you have ended up showing is that in the limit of very small learning rates, and surprisingly, I mean, it's not something that you would have expected suddenly, this result pops out, which says, hey, in the limit of very, very, very small learning rates being used, very small learning rates being used the output is nothing but a linear combination of similarities to input data of training data right which is quite a quite an interesting statement that it is making now there are a few remarks uh it says that remark one so it says the first thing it says that see this differs from the typical kernel machines in that A, I, and B depend on X. Nevertheless, the AIs play a role similar to the example weights in ordinary SVM, support vector machines, and the perceptron example. Examples that the loss is more sensitive to during learning have a higher weight, and so on and so forth. See, guys, this is a mathematical theorem. What it means is you cannot say that this is not correct. It is correct in the limit of epsilon going to zero, this statement is great. So it's a mathematically rigorous proof. We can continue on with that, but let's go to the interpretation of it. What it is saying is that if two points let's say that you want to find out what is the value here why what you do in the simplest term is of all the input data points see that if you change the the you know the weights how do the weights change for this point of the loss with respect to the you know with respect to the loss how does the gradient flow here and how does the gradient flow for that other point are two points whose gradients flow similarly in other words will be very close so their outputs will also be very close In other words, will be very close. So their outputs will also be very close. Right. That's a that's a quite an interesting statement. Now we need to start taking seriously these gradients, tangent gradient, you know, kernels. And that's what it says. It says that that is what it is trying to find, which similarity based on those gradients, a kernel that comes from the gradients. And so the rest of the paper is quite straightforward. You get a given a query, you feed in the K, the similarity to each of the data points, and then you find it out and a linear combination of all of them, what each of them is saying will be the answer, right? So I will reach out the conclusion, we are running out of time. So it says that even when they generalize well, deep networks often appear to memorize and replay whole training instances as you know see what happens is we have known for a fact that when you train a complicated model it does generalize but it tends to often just if you give it the same data as the training data it seems to make perfect predictions quite often it seems to have internally memorized the data. And now we begin to get an intuition of why it does that, because that's what it is trying to do. The fact that deep networks are in fact kernel machines helps explain both of these observations. So what are the two observations it's explaining? Deep network weights are superpositions of training examples, applying the kernel with an okay. Even when we they generalize now we need to go to the previous sentence experimentally deep networks and kernel machines often perform more similarly than would be expected based on their mathematical formulation so so there are a lot of people who are working very actively on this now now the fact that deep networks are in fact kernel machines, basically, it turns out that they're equivalent or they are support vector machines, helps explain both of these observations. They also shed light on the surprising brittleness of deep models whose performance can degrade rapidly as the query point moves away from the nearest training instance. So it also explains that, you know, suppose you have trained data in one by taking data points, a training data which occupies one part of the feature space. When you try, when you ask it to make prediction for points away from the feature space, the deep networks perform very poorly. So it sort of gives you an explanation because none of the training points are similar to it. Now, whose performance degrade rapidly as the quay points move away from the nearest training instance, since this is what is expected of kernel estimators in higher dimensional spaces. It was already known for SVM. Perhaps the most significant implication of our results for deep learning is that it casts doubt on the common view that it works by automatically discovering new representations of the data in contrast with other machine learning methods which rely on predefined features. So that's a big statement, actually. I don't know how much what others will say about it, but to the extent they have made a pretty interesting statement. They say that we have always believed that deep networks could learn representations of the data and so on and so forth. But what they're seeing is if they are nothing but support vector machines, and they look at similarities, they pick those crucial points and look at the similarity of a query point to this, you know, those data points, those lighthouses and some, or all the points if you wish, in some kernel space, then if that's how they work, now, how do we reconcile that with our understanding that if they are representation learning engines, they learn higher and higher order of representation. So I'm sure there'll be more work and more discussion. We'll see what comes out. This paper just came out a week or two ago. As it turns out, deep learning also relies on such features, namely the gradients of predefined functions and uses them. So there's gradients. All of these gradient descends and select features from it. So I won't go into the rest of it. And now it will be fairly easy for you to read this paper. Most significantly, how well learning path kernel machines via gradient descent overcomes the scalability bottlenecks that have long limited the applicability of kernel methods to large dataset. So the interesting part of the breakthrough is not just for deep learning, it is also to the whole world of SVMs kernel methods, because there the problem is the kind of kernel people were using, they did not scale well. There was the sort of the gram matrix which used to become very large and quadratic computations and a terrible performance. SVMs became very, very slow actually for large data sets. You couldn't do. And what these people are saying is that, now they can be cross-fertilization of ideas. While you can apply kernel machines to deep learning, the other way is also true. Deep learning has developed a vast literature of findings. So if we can apply this new path kernel back to support vector machines, we can backport a lot of the results from deep neural networks back to kernel machines, right, and that could lead to a sort of again erroneous of those kernel machines over again. Essentially that's what they are saying, right. So they go on to say the significance of a result extends beyond deep networks and kernel machines. In its light, gradient descent can be viewed as a boosting algorithm with tangent kernel machines as the weak learners and path kernel machines as the strong learners obtained by boosting. Right, so there's more connecting into different pieces of machine learning and boosting and all of that. So we won't get into that. That in a sense is the space for Dyson. So you understood the gist of it. It's just pretty straightforward, isn't it? Any questions? It has, I mean, the understanding, the main core understanding is simple. Intuition is simple, but but it does like they make very important observation multiple important observation right and the derivation is so easy isn't it right right yeah very straightforward but if it does uh i mean if all their observations are true it will you know probably lead to a lot of convergence between a traditional classical and the deep learning that is true actually you know this is all very recent work that i have been observing happen if you keep following the archive when you see what papers are coming up they They're getting a lot of attention. By the way, this paper is getting a lot of attention. The tangent kernels, which is the dot product of this gradients with respect to weighted points, which are called tangent kernels, they're getting a lot of attention these days from a group of researchers. So let's see what comes out of that. We're certainly being taken quite seriously. So guys, this was a theoretical paper. We have been doing a lot of practical papers, the birds and this and that. I thought it may be interesting to take a deeper dive into how people are researching and thinking about the foundations of deep learning. researching and thinking about the foundations of deep learning. Any other questions guys? That's all I have for today. Sir, one more thing. It looks like the debate between the classical method and the deep learning will continue to go on on multiple parameters like you know computation and the the understanding you know interpretability and so on right see to know that something that looked exotic like completely different you can actually see it from familiar perspective itself is a big big thing to see them as kernel machines, which are so well understood. I think it furthers the understanding of Deep Neural Network. And the other way around, the fact that Deep Neural Network has made such wonderful progress in so many areas now means that if we could backport this path kernels, which were not used in standard SVM theory, they were using very simple kernels. But if we can backport this theory back to kernel machines, there would be quite a range. And so they'll be coming together, these two approaches, now that people realize that there's a lot of overlap between these two. OK. Asif, I don't have an understanding of SVM, just the level what you described today. But listening to this, the paper reading, what I'm wondering is, doesn't this pave way for more explainability for neural networks in the future? Certainly, in fact, that is the hottest concern and one of the hottest concerns in deep learning, the fact that we don't even understand what in the world is going on inside that box. And interpretability and explainability is a big deal of it. So this paper actually goes towards the interpretability of deep neural networks that we will talk about a little bit more in the regular sessions. See, most of the efforts to interpretability, they just take an empirical approach. They look at the consequence of the predictions or they make locally approximate models. And in a way, they're trying to, you know, it's almost like blind men, and they're touching the elephants and they're making some approximate conclusion in what the elephant looks like. That is all very useful, and in practical terms, definitely very useful. You need to make locally approximate modules and so forth. But this is more a deeper thing. It says, let's try to understand what's happening inside a new event. And so it brings back certain degree of interpretability. So- The assumption I'm making here is with support vector machines, I'm assuming those were considered to be fully understood. The sense, once those support vectors have been identified, there is a methodical explanation for what the model is doing. Is that the correct understanding? That is absolutely correct, actually. We do understand SVMs far better than the dynamics of neural nets. The theory is very well-developed. One thing is that SVM sort of arose from a purely mathematical line of reasoning. There was this guy Vladimir Vlatnik sitting in Russia. He actually developed some of the core ideas of SVMs in the 1960s and he was completely ignored for 20, 30 years. Still he was rediscovered by, I think think some big company AT&T or some company here in the US and now he's today a professor at MIT. He's quite old now but yeah so it started out from a very strong mathematical foundation and that has always been a problem with SVM. People who come to the SVM theory they know that it's very sort of an interpretable understandable so long as you are you you remember your vector calculus and probabilities and so forth because it does go into the math like you see me right go into the tangents and this and that these not hard, these are elementary stuff if you remember your engineering math, but if you don't then they look hard. I think the math part is one as if I'm what I'm trying to when I say explainable I'm taking it back to the business context right. So the interpretability of the math should eventually translate to being able to explain in a business context what some of the implications are certainly yes so it is it certainly moves the needle forward quite a bit of work like this but but there is this is one there's a whole slew of work going on along this direction and interpretability of deep learning is a huge topic. We are trying to, everybody seems to be trying to look at it very, very seriously. Asif, can you explain that superposition again? Superposition is, this is just this, right? So that is not the memory part, like it's, like based on the few support vectors it does the generalization because you have a1 of x like i'll just take an example a1 of something of p plus a2 of q what is this equation it is a superposition of p and q with a1 and a2 as the weights so superposition is just a term saying you take little some contribution from P, some contribution for Q to produce a final result. And so with that being there, now you look at this equation. It is just a superposition of all the similarities or a linear combination of all the similarities. That's all. Oh, okay. Oh, okay. Yeah. All right guys, any other questions? If not, I wish you all a good afternoon. Enjoy your lunch. I'll see you tomorrow. Thank you, Yosef. Thank you, Yosef.