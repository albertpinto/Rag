 So guys, I'll just quickly review what we did the last time. We looked at the fact that you can take data in a certain feature space, input space, and you could lift it into or transform it into some other space in which the data gets linearized. So we saw this example of a desk. Points on the desk are blue, points outside the desk are yellow. There are two ways of doing it. One was the transformation that I mentioned, x1 prime being the square of x1 and so forth. And what it led to is this triangular situation. So in this little triangle in here, everything is blue. Outside the triangle, it is yellow. And it was one quick way to see that yes, transformation leads to simplifying the problem, linearizing the problem. So that the decision boundary, as you can clearly see, is just a line, is just the diagonal line, right? The off diagonal line rather. So, and the thing to observe there was we did not necessarily live to a higher dimension. Isn't it? Then the other way is, and the same thing, just we can do it in multiple ways. So this other way that we could do it is you could actually live to a higher dimension. What you do is you create a variable x three, which is the as which is the square of the distance from the origin of each point. So you will because it's the square of the distance. It is this thing x one square plus x two square. square plus x2 square, it will have a parabolic shape. And in the parabolic shape, the blue points will be towards the bottom of the parabola, parabolic surface, and the yellow points will be everything above it. And so you can draw a hyperplane that we see, which is a constant hyperplane, x3 is equal to r squared, means at a certain height from the ground, which is defined by x1, x2, you have x3 constant and this ring, this circular disk is the disk that we see in the original data space. That becomes your decision. That is your cutoff point. That's your decision boundary. But how are we defining the decision boundary in a higher dimension space? We have linearized it because for us, it is this X3 is equal to R square plane. It's just a simple flat horizontal plane that cuts through the parabolic surface. And it's an illustration of the fact that you can take data, lift it to higher dimensions and get away with linearizing it. And once you have that, you solve the problem. Oh, there is a, okay, I noticed there's something a little bit off I should do. This is a minus b, a magnitude for a polynomial kernel. I apologize and this is some not just one some arbitrary number. So the kernel if I defined is the simplest way to think of a kernel is the very simple kernel is just a dot product of two vectors but more generally a kernel is the dot product of a dot product in the highest higher dimensional space. So it is actually here also there's a typo. I think I was being absent-minded last time. So what it basically, let me explain this concept again, because I seem to have moved a little bit. The kernel, the basic, the classic definition of a kernel is the kernel of a vector X and Y is first what you do is you take X and Y you take the points X you take the point Y the X vector Y and you take them to some higher dimensional space some space need not necessarily higher dimension. People often say that the space is infinite dimensional because they always think of radial basis functions. Not necessarily, but you just go to some higher dimension space and this is another vector. It's a vector, some vector in a higher dimension space and this becomes some vector in yet another higher dimension space and basically the it is the dot product between these two it is a dot product in that space between these two transformed transformed points, right? And they're much simpler. So that's the classic definition of a kernel. Kernel XY is actually nothing but the dot product in, is basically a dot product in a transformed space between the points. Can you call phi as a transformation to that higher order space? Yes, it's a transformation to the higher dimension space. Examples of phi are quite literally here. Square and all. Square and so forth. These are all the mapping functions so the whole magic of kernels is that the surprising magic is a that you can do the transformation at all and by the way these spaces are called you will often hear the word hilbertian space or hilbert space hilbert spaces are very interesting now what is the most simple intuition i can give you hilbert spaces are very interesting. Now, what is the most simple intuition I can give you? Hilbert spaces can be infinite dimensional. These are functional spaces, right? So potentially infinite dimensions, they don't have to be. Like for example, in this example we saw, we got away with two dimensions, but they could have been higher dimensions sometimes, infinite dimension spaces. So for us to wrap our head around infinite dimensions is a bit hard, but the way to do that is carry your intuition forward from three dimensions. Hilbertian spaces or Hilbert spaces are something that physicists absolutely love because all of quantum mechanics, all of the theories of physics, ultimately more or less, have a deep connection to Hilbert spaces. And so these sort of arguments that you see, let's map it to a higher dimensional space, etc. This is very close to the work mathematical physicists do, which is why you often find that in machine learning, a large part of the literature or the terminology quite often is borrowed from theoretical physics, the concepts, and so on and so forth. The mathematical physics, a lot of mathematics. So as always happens, math and theoretical subjects are a couple of hundred years ahead or 100 years ahead of engineering practices, because reality takes a lot of time to catch up, but the human imagination is way ahead. So a lot of this theory was developed a long time ago and now they are they are proving to be very very useful though a hundred years ago people didn't see all the use practical use for it now it's all proving very useful so simply put as I said the intuition is that think of dot products dot products capture a very rich relationship. Dot product in the initial space or in the target space, they are all wonderful things. We did that. Then dimensionality reduction we talked about. We know, just as a recap, the principal component analysis was wrapping up this data shrink wrapping it in an ellipsoid now you may or may not be able to do it if the data is linear you can do it if the covariance matrix is if there is a positive covariance or negative covariance you can do it but sometimes the covariance is zero like for example in the data set two we notice that you won't get much covariance because sometimes it's positively correlated, sometimes negatively, and so on and so forth. So what can you do? You can go to another space. You can use the kernel mapping, go to another space where the data gets linearized. So you apply a kernel mapping to go to that space. Once it is linearized, then if you're lucky, you can then do PCA in that space and actually come to a lower dimension space. Which is why I mentioned that it is a good idea to do kernel PCA. another concept which is that of kernel distance which is actually different the meaning or the word the kernel here is used in a slightly different manner here the kernels are distance kernels and we applied it to the k and n we realized that when we take k is equal to one one nearest neighbor the decision boundaries are very very, you notice that they have high variance. On the other hand, when you take k equal to infinity, then the decision or k is equal to 20 in this case, the decision boundary is just meaningless. It devolves to the baseline situation, the majority situation. So somewhere between k is equal to one and k is equal to 20 is the perfect answer, is the best value of k. k is the hyperparameter of your k nearest neighbor algorithm. And it has been obviously a way to find that. You can try to find it. As you increase k, you can see that the green line is better. The red is perhaps not so good. It starts getting bad. And by the time you reach this orange line, it becomes the Indian red line, it becomes sort of terrible. So you know that the here the best solution potentially is the green solution and you discover that through obviously cross-validation in the data. Now one of the ways to mitigate this whole business of finding the perfect K is that you can use a distance kernel. You say that the immediate neighbors have more influence than the far-off neighbors and so then the whole question is what is your notion of distance? We talked about the Minkowski distance and the L norm. This is just a review of that. L is equal to half. Looks like this circle. Circle. L is equal to three. And L is equal to infinity. The infinite norm. Minkowski distance is something that you should really review. Oh, by the way, I didn't post those notes, the ones that I showed you guys from my book. I'll post it to Slack. Then the k-nearest neighbor. So obviously what you need is some distance function. Once you have a notion of distance, you need some function that decays with distance monotonic. It may not decay at all, it may just suddenly fall off or it may remain constant, but something, but it certainly shouldn't increase with distance or swing back and forth. It must have a monotonic behavior. So people have come up with all sorts of distance functions to have the best care nearest neighbors. We went through a review of that last time, if you remember, we covered about half a dozen or more distance kernels. Mostly the parabolic kernel tends to do quite well, but almost all of them do equally well. When you're trying to squeeze the last bit of performance out of the model, then the last thing you do is you tweak and you pick your kernel. But broadly speaking the kernels, in the beginning, the last thing you should worry about is which kernel am I using? Focus on cleaning the data, getting your distance measure right, doing all those things right. Then at a later stage when you are beginning to do quite well with your kernel KNN, then also search for the best possible kernel. That brings up this whole thing about hyperparameter search that I would like to talk about today. See guys, do you notice that in ML100 there were not that many knobs to turn. You had linear regression, logistic regression and so forth, LDA, QDA. They did not have too many hyperparameters built in. In fact, there were not any hyperparameters. But in ML 200, we are sort of infested with hyperparameters in all our machine learning models. So the machine learning models say that if you tell me what the value of K is, we can build the best K nearest neighbor model for your data. But you have to find K. So what you have to do, you have to continuously go search for the best value of K Shubham Tulsiani, Right, but then you have another dimension, which is the best kernel. So now you have to search in two dimensions, the best kernel and the best K. Shubham Tulsiani, Now usually what happens is a kernel KNN they sort of mute the importance of the hyperparameter k. It still matters, but it doesn't matter so aggressively. So for example, the best k and the neighboring k values would all give more or less the same accuracy or the same performance. But now you certainly do have a problem. You have to find the best k and you have to find the best kernel to use, distance kernel to use. So you're searching in two dimensions, isn't it? And now suppose I had a few more hyperparameters in some kind of a model. Let's take random forest. What were the hyperparameters there? Well, we are building trees. So how many trees to take? Well, there the simple answer is take a lot of trees. I start with typically 400 in most situations. Then, and go up from there, depending on the complexity of the data set. Then, even for very simple data set, I'll take 100, 200 trees. But then the depth of the tree. How deep should the tree be? Each tree that you're building, what is the max depth that you will allow? That is a hyperparameter. It can go all the way from one to a surprisingly large, you know, you can go whatever depth. Then at what cutoff, like how many points do you want to leave in the leaf node before you stop splitting? You can ask whether I use Gini or cross, or sorry, Gini or entropy as my impurity measure. Isn't it? So there are quite a few. And the other thing of course in random forest is at each split of the tree, how many features are you, what subset of the features or what are more formally, what subspace in the feature space should you consider for splitting at each split in the tree? You don't want to take all the features, but how many features do you want to take? Just one feature, two feature, three feature, all hundred features, square root of the number of features, which would be 10, log of the number of features. These are all choices to be made. And at the end of the day, in the more general situation in practical situations for one data set some of the hyper parameters matter quite a bit, but for Some other hyper parameters may not matter at all, but a priority before you do this experiment. You don't know what the situation is Are we together. You don't know that. So what do you do? You have to search. You have to search this hyperparameter space to find the optimal combination of hyperparameters. But by now you realize that the choices are many. It is a combinatorial explosion, even if you take discrete number of, discrete values for the hyperparameter. So for example, if you take the depth of the tree is discrete, one, two, three, four, five, and so forth. The number of features, max number of features to consider is discrete. The Gini and Gini versus entropy is again a choice, binary choice. Number of estimators is again up to you. These are all discrete. But let us say that in each case you had a choice of about 10 possible values. Jay Shah, Dr. V. S. By the time you have four hyper parameters, you realize that 10 times 10 times 10 times 10 is 1000 so what you're doing is you're building a thousand models to find which model is the best on cross validation. Isn't it? So that's a pretty aggressive computational load. So this way of brute force are just creating a grid of that values and searching for it used to be called the grid search method. The grid search method for hyperparameter tuning has been a classic. It is also a staple diet for interviews. So when you, should you go for interviews and your interviewer says, well, what is grid search? Now, you know, or the other way they can say is, how do you find the best values for the hyper parameter Jay Shah, Dr. So quite often they expecting you to say grid search. And so you should start by saying, well, this is the way you could do it. But remember, this is not the optimal way. Jay Shah, Dr. But then the other thing you could do for reasons that will understand Jay Shah, Dr. Later, or maybe today itself. If get time you can actually do randomized search take random values in each of the dimensions hyper parameter dimensions and then search and then what you do is once you find the best value then you search in the neighborhood of that value to find even better values and so forth you sort of zoom in you can do that that's called randomized search it is from experience we now know it is better than this brute force grid search and lastly these days actually there's quite a bit of theory on doing this in an optimal in a much more efficient manner and that is the domain that's one of the topics that we'll cover in automated ML, automatic ML. One of its main burden is how do you do optimal hyperparameter search? Because you are not just searching along the hyperparameters. Remember, you're also searching along many algorithms. So for example, if it is a classification problem, then given the no-freelance theorem, you don't know which of classification problem, then given the no freelance theorem, you don't know which of the algorithms, which of the classifiers would be best. And each classifier comes with its own bag of hyperparameters. Isn't it guys? So the search for that becomes computationally very, very expensive. You need a systematic way to go about doing it. That is the new frontier. I would like to, we are running out of sessions, of course, and the syllabus of this course, we have two more topics, regularization and gradient boosting, both of which I would like to cover before the course is out. So we'll do automated ML in one of the extra sessions. We'll certainly start the bootcamp with automated ML. So anyway that's the summary guys of the last time. I would like to now move on to the topic for today which is a continuation of what we talked. Now today was supposed to be lab but I thought about it and I sort of changed the decision. I felt that regularization is such an important topic, we cannot miss it. So we should do it today and we should move the lab work to a regular. We'll do it over Saturday. So I swapped it. I said we'll do regularization on the weekend. No, instead we'll do it today and we'll do the lab session over Saturday afternoon. I hope that is all right with you. And the boosting is also an important topic. It needs its own lab. So the next week we'll give to boosting and to its lab instead. The recommend that we are going to anyway cover in the math course. So it will get covered. We don't need to worry about that. This is the way we do. So let's do it properly so that we get to do a lot of labs in each of these. Today, because we have already covered support vector machines and this notion of kernel distance is a bit theoretical. There was no very explicit lab that I could have given so instead I decided I'll do regularization so that it regularization lab is really important to do and we'll do that on Saturday any questions guys before we start can I ask a question yes Yes, please go ahead. So when you go to, when you say, you know, we can go to higher order space, right? But if we do not know that space, like let's say example of size and weight for cow and duck, right? But we want to go to higher order space, but we do not know, let's say color or other thing that what, what do we do then we do then no no you have to know that ultimately the higher dimensional space is manufactured out of the input space so if you don't know the machine learning algorithm won't be able to discover see it doesn't know what it doesn't know right that's the problem with all machine learning algorithms. Remember we talked about the irreducible error in ML100, but that is the irreducible error that you can't get away with. It is true with human beings also. When we believe something is to be true, we base that judgment on what we know, but there remains the fact that we don't know what we don't know. We can just be aware of the fact that there must be things we don't know. I hope one of the things we learn here is that what we don't know may fatally compromise our understanding. It's almost, it is obvious, but it is the irreducible error in machine learning. Correct. And that no algorithm will be able to surmise. You have to bring it to the table. So I think question is that you know when you say for kernel you do not have to know that space all you have to do is dot product in that space right that means to the kernel trick is see what happens is that some of these ways to map are quite powerful like linear and polynomial are good and so for example you know that polynomial will work for the river data set easily. Then linear would have worked easily for the straightforward classifier one problem. Then the radial basis function works for a vast class of problems. When you use RBF, you're actually talking of infinite dimensional spaces. At that particular moment, the infinite dimensional space is so rich, you're pretty much guaranteed a linear decision boundary, more or less, a very high probability that you'll solve the problem. So you don't worry about that. You take these kernels and go. Question that arises is, could you hand construct a kernel, a mapping, just as a feature engineering exercise, think it through and give your own mapping to the higher dimension. If you can do that, all power to you. Actually, it leads to some very efficient solutions because computational runtime with the RBF kernel is quite high. You're taking it to very high dimensions and you're searching for the solution there. If you can do it with your own homegrown kernel, the performance is better, even the computation times are better and so on and so forth. But if you can't, RBFs are very powerful. Generally they do solve your problem. And then the other interesting thing is that obviously it is the dot product that matters. The kernel trick, the heart of the kernel trick, it is not a theoretical breakthrough, but it is a computational sort of a cheat way or a shortcut. You manage to not discover the mapping function itself, but you discover that whatever mapping function there is, you discovered the dot product of it. And if you can reformulate your problem in terms of dot products, in terms of kernels, you have a winner. And that is why kernels became so, people said, wow, that is amazing. Given that that is true, can we then reformulate a large class of problems as kernel problems? That's how the whole kernel metric thing started. And they were very, very surprised actually. You can write many things and if you, with a little bit of thought, you can rewrite it as a kernel function. Amazing, just surprising. It just shows that how powerful dot products are. So a lot of work in that space. Thanks. Any other questions guys so guys remember that we are coming to the close of the workshop we still have two important topics to cover regularization and boosting both of which needs dedicated labs so we have one session short so I use this session for regularization and we will have Saturday so do treat it as a mandatory session guys and we must finish the syllabus which is that we must do the lab in regularization and. What is regularization? See, when you, and I'll take the example of a polynomial regression to sort of motivate the concept of regularization. Suppose I give you data, let's say that's let's take a simple situation of X and Y where we are saying we are in search of X is equal to some function some function and let us say that there is a ground truth you don't know there is a ground truth this is the ground truth of the data. This is ground truth of the data. And let's say that you have these points. Right, let me take these points here and Let me say, now I'll just take these few points. I hope I can illustrate the problem that I have with this. So I will keep this point and let's see what sort of curves we can draw through this. One thing you could do is you could say, hey, this, okay, again, let me be clear with the terminology. G is our ground truth. It is that which you don't know. It is the function that has actually produced the data. What is our task? Our task is to build a machine learning model, which is, let me just make it y hat right you're you're going to pick a hypothesis going back to the basics hypothesis function such that y hat is approximately y for most values isn't it In other words, the real value is Y hat plus some error, and you want this error to be small. A good model is one that makes fairly accurate predictions that are close to reality, close to the real values of Y. y now if you take the case of polynomials polynomial regression you realize that you can write fx as beta naught plus beta 1 x x plus beta 2 x squared plus and you can keep going on to beta n x to the power n do we remember this thing guys this is your polynomial regression equation approximating the data trying to fit the data to this polynomial yeah now the question is what is n here the big problem is what is n? Here the big problem is what is n? What is the best n? Trouble is if you take n essentially becomes the hyper parameter of the model isn't it it has and so you have a headache you have to build models and over and over and over again and see which value leads to better results You can keep trying. So one of the questions that you can ask is, let's see what happens and see if there is a shortcut to this. If I take n is equal to one, what will this look like? n is equal to one would be this, right? In simple terms, what would you call that? Straight line. Straight line, that is like this, isn't it? This is n is equal to 1. For n is equal to 1, it is the straight line. For n is equal to 2, it would be some parabolic curve. I don't know if this is exactly the one, but we'll assume that this is the one n is equal to two but then for n is equal to three let me take another color uh why am i having this color this color so n is equal to three maybe that it is uh is equal to three maybe that it is uh this one right let me make the points big and bold so that we remember those points and then what about n so it is two bands n is equal to three what about n is equal to five what will happen what will happen is as you go to higher and i will pretend because i we should mathematically we should programmatically uh draw this out to see which one fits but just humor me uh this is too fat let me take something like this okay so if you were to take this something like this. Okay. So if you were to take this, it would be, it needs a four bands, right? So it would be something like one, two, three. Let me take it like that. I'm just motivating the example. If we actually did it in a model it might look something different. Maybe I should have done before I came here. So this is n is equal to 5. What do you notice happening? And let me, without actually having done that, let me say that suppose I had done that for n is equal to 10 what would it look like so now I need nine bends and your data will begin to look too fat again your data actually it will be look like this something like this because you need nine bends one two three four five six seven actually I didn't introduce nine bends. We need to make it even more curvy. So who knows, we need to add a couple of bends right here. Make this more like this. So what happens is if you go to n is equal to 10, you have a complete fit to the data. It seems to go through all the points, isn't it? But do you think n is equal to 10 something like this is a correct solution what does your intuition say what is the problem that this model is having it has over fit the data so what happens is the more parameters you have polynomial terms you have the more knobs you have to turn you have to turn in the data it's's a very flexible model. I'll give you an intuition. Imagine that it is a sort of a, what do we call it? Are you familiar? Well think of it as silly putty, something like silly putty, right? If it is very stiff stiff you can't bend it it's like the n is equal to one line now gradually as you make it more and more pliable you can bend it more and more isn't it and so you have bent it actually it's liberty as a bad example but think of something that has some degree of elasticity to it. The more elasticity it has, the more you can bend it. So the higher degree of polynomial, the more pliable it is, the more you can bend it. What happens is if you take a high degree polynomial, you have too much elasticity. You can just fit the polynomial, that curve, to the entire data perfectly. But you know that the data is just a representation. You take another sample of the data and this curve will be all wrong, isn't it? You will end up drawing a completely different curve to this data. So how do you cure this problem? This problem is the problem of overfitting, isn't it? So you do know that flexible models do tend to fit the data, but you don't want it to overfit. So just as a reminder, the total error, the total error of a model, total error of a model, model is equal to the bias error plus the variance error plus what else about sorry the the variance error plus what guys do you remember what is the irreducible error irreducible error right irreducible error thank you kate read you civil error it is the it captures noise as well as that which you don't know that which the model doesn't know so see if you try to reduce bias, variance increases. Are we together, guys? That has been the bias variance trade-off. So let me draw the bias variance trade-off again. And by now, you should be quite familiar, I hope, with this. So as you increase the complexity of the model for example in the case of polynomial as you increase n the two the errors right the the bias error of a model as you build more and more complex models will keep going down like this. Right? What about the variance errors? The variance errors will start the, actually, let me use another color so that I can use white for the final solution. So let's say that your bias errors go like this. They keep on decreasing the more flexible you make the model. Your variance errors, on the other hand, begins to go like this. And let me give names to this errors and this is errors so you know that somewhere in between those two points is the perfect line it is this line and let me use white itself but a thicker version of white hopefully so the total error if you add these two up you notice that it goes like this and whatever the value of K here is this value this is the best model that you can build. So this is the best approximation. So suppose Fn, I will say, of degree n to the ground truth. It is the closest you can come to the ground truth. And this error that you're left with is obviously the part that you can't get rid of. It remains at the end of the day, the residual error for you. Are we together? It's made up of whatever little bias error remains, whatever little variance error remains and the irreducible error. So as a recap, guys, does this look ring a bell? Are we familiar with this so far? We did that just a couple of months ago in ML100. So this is it. So now the question is, here, you have this big search from here to there. Can we, the question to ask is, what happens if you make an overly simple model, a biased model, a step model? Its errors will be hard to deal with. It's a sort of fatal, you can't get rid of that. On the other hand, if you make your model too complex, too flexible, the question that arises is, going back to this data, do you notice that the complex model n is equal to 10 or n is equal to 5, a green and the pink lines, they have too many oscillations. They seem to be oscillating quite a bit unnecessarily. So it is tempting to ask can we apply a damping factor? Can we dampen the oscillations? Is bouncing around too much in the feature space, this curve, can we reduce the bounce? Can we dampen it? Are we together? Because if we could dampen it somehow, then it could be, it could solve a problem. We could start with a flexible model and we could dampen it down. So the basic idea is that can we start with, let's say a green line or the pink line, but somehow dampen it down to the yellow line. Now the yellow line is the perfect line, right? In this case seems to be the best model. That is n is equal to three. Isn't it guys? So the idea is suppose I took a more complex model like n is equal to five. Can I bring in some mechanism that in spite of my taking a more complex model the things the extra degrees the extra flexibility somehow magically disappears and it begins to approximate the yellow line the yellow curve that is the intuition of this whole um this whole problem or this whole bag of tricks called regularization, this technique called regularization. Are we together? It is also called shrinkage, shrinkage methods. In fact, your book calls it shrinkage methods. The statisticians call it shrinkage method, shrinkage. Now, the machine learning crowd tends to use the word regularization more often. Shrinkage means you shrink all of these oscillations and you make it look like that yellow curve. That is what we will learn to do. But for us, so the answer to that is yes, you can do that. It turns out there's a very elegant way of solving this problem. And the solution to that comes from a field, a very simple bit of mathematics called, and it is a problem in constrained optimization. I already, optimization, I already sort of introduced you to the concept of constrained optimization in the support vector machines thing. In other words, find the maximal margin hyperplane constrained by a certain budget of mistakes that you can make. Remember, so we will use same things of constrained optimization today. But before I go into the mathematics of it, I just wanted to say, let's see, this theory is very elegant. It's one of the crown jewels actually of linear methods. The fact that you can take a fairly complicated polynomial, which is still considered linear because it's linear in the polynomial terms, and then regularize it and get to the solution regularization is one way there is another way actually to solve this problem the other way is to just bring the other way let me just write it in a sidebar the other way other option and probably the preferred one I probably preferred probably preferred something is wrong with my preferred spelling preferred way the preferred way would be what get more data get more data so what happens is if you get more data somehow it is possible to get more data suppose you points like this. Lots and lots of data. And I hope I'm not making enough number of dots, but imagine that this is very densely packed with data here. Then what happens is the curve that tries to overfit, it won't get enough flexibility. It may sort of ripple around a little bit, wobble around a little bit, but it will ultimately look like the, even a high degree polynomial will look like this. Do you notice the small wiggles that I have put in the line? There may be small ripples or wiggles in there but broadly speaking because you know a 10 degree polynomial needs 10 9 wiggles it will still be there but it will be unnoticeable right so what has happened is you have given it a lot of data and it is forced to fit with a lot of data so overfitting disappears or gets muted. So here, overfitting or high variance, if you want to put it this way, the high variance gradually is suppressed is suppressed right so so you can and this is by the way the preferred way if you can get more data than do it so what is the catch here why not just use more data why develop a whole new theory of regularization and so forth can somebody guess it can be expensive to get data it is expensive to get data yes that is very true that's one reason see it might not be available like if it's a rare cancer like the data might not be available yes that's the point in a lot of fields data is is very, very hard to get and you have to make do. You want to model with what you have. You can't say that, it's like asking for food on a silver platter. Well, they may not be. So you do want to make do with what you have and more data may not be available. Another way to look at it is, see, remember I told you that the higher dimensional spaces are very sparse. No amount of data will look enough because all the data disappears in that space. So overfitting happens when the data is sparse and you're fitting a very flexible model to it. When you do some of these modern algorithms, let's say a deep neural network, which you will do shortly in the next workshop. Can you guess how many parameters we're talking about here? We are talking about a polynomial of degree 10 in a typical deep neural network. How many parameters do we typically talk about? Few millions to a billion Exactly. So if I am right, yeah, millions of parameters, even the basic ones have millions of parameters. And then I think GPT-3 has what, 500 million parameters or something like that. So the parameter space is just ginormous. So you might come there with 10 million data points, but your 10 million data points are a drop in the bucket as far as those that huge, huge hyper parameter space is concerned. So overfitting is a very serious problem there. This getting to the wrong answer. So we're fitting problem. And so you have a lot of overfitting errors, high variance errors in your problem. And so the theory of regularization, you need to find a way to regularize. So today, hopefully based on how much time we have, I will teach you at least how many techniques, three. I'll try to teach you at least a few techniques of regularization. And on Saturday, we are going to do a lab on those techniques. So how do we do that? Let us go back to the basics. Suppose you have a function, y, let's go back again, write down what we wrote before. We are trying to find a function Y hat is a function of X, right? And you know the ground truth, Y is actually a function of GX, right? So you know that Y is basically Y hat plus some irreducible, some errors that get made in your model. And your pursuit is to find an FX that approximates the ground truth. This is our ground truth. How in the world are we going to do that? Let's go back to basics. We have a loss function, the error function. We know that each time, so let's look at regression as an example. And this will generalize over to classification also. What is the error term for each data point? The error is equal to y minus y hat, isn't it? This is your residual. Now your sum squared error was summed over all I. Do you remember that, guys? This was your sum squared error. Now, given this, and now what you can do, this error, if you just expand it out as Y minus y hat i square this comes to and let's do the little bit of mathematics here this comes to y i minus what was y hat beta naught let me just take the term beta 1 x1 right plus beta 2 x2 etc etc But let me stay with one dimension for the time being, it is this Jay Shah, Dr. G R Narsimha Rao, Ph.D.: We agree and we can sort of add more dimensions to it. It's not going to change the thing. And obviously I shouldn't forget my summation. This is your error term guys. Any mysteries here so far? So when you have this, you know that when you're doing your problem, a data is fixed during learning. Data is fixed, of course, because you're learning on the data. So the values of X and Y are not changing. XI, YI, changing. So this E, what is it really a function of E is therefore Chandra Rangasamyc-huradwimuthi.chandra.dharmakotanayake.com A function of Chandra Rangasamyc-huradwimuthi.chandra.dharmak i can write therefore this equation as e beta naught beta 1 and then i can go to beta 2 etc etc if i so wish is the summation over square of y i minus beta naught minus beta 1 xi right and if I want to add higher or two terms I can do that let's stay with one dimension this is what it is well if you expand this out for each of these terms you'll realize that it expands out as Y I square plus a beta Vaidhyanathan Ramamurthy, Beta one x one x i square. Vaidhyanathan Ramamurthy, A plus minus two beta not why Vaidhyanathan Ramamurthy, Is there a Vaidhyanathan Ramamurthy, Combiner to beta one y i x i minus and obviously we'll have to put a big summation over all of those Vaidhyanathan Ramamurthy, And what else is there we missed one more term we turn on beta 1 and 1 yet to X and X I X I right so if you look at this equation right then it's a bit of a long set of terms but do you know that what is the highest power of beta that you see here in this equation is the square term say beta naught or beta 1 square so you would agree that this is this equation uh this equation is quadratic i. e beta naught beta one is, and this is the important thing, quadratic in beta. Isn't it? It's quadratic in the betas. now quadratic curves look like, how many bends does a quadratic curve have? One. One. And so in higher dimensions, how many, the generalization of a curve is a surface or a hyper surface in this case, just a surface. So what will it look like? Let's look at this axis error. And this is this. This is beta naught, let's say axis. So now remember, we are not in the data space. We are in the hypothesis space. We keep using this word hypothesis space. We are in the hypothesis space. So here, if you remember, we talked about this. This is just a recap guys of what we did. This is for each value of beta naught, beta one, there will be a certain amount of error, right? And this is your curves and they will exist a point, a perfect beta naught. Let me just call it beta naught, beta one with a tilde on it the perfect values the best values the optimal value where the minimum of e minima is achieved is achieved is achieved. Achieved. In other words, what you are searching for in this space is this point, this point, beta naught, beta one, represents in the beta space, right? Let me just write it as a beta, well, tilde vector, but okay, I me just write it as a beta well tilde vector but okay i'll just write it like that this is what it is the point where it is that value of beta naught beta 1 where this achieves the minimum the error achieves the minima. That's just a summary way of putting it. What value, what argument of beta naught beta one will minimize the error that represents our point here. Let me draw this picture a little bit better. We see the parabola actually. I need a bigger screen. That is AVZ or that is ARG? ARGMEN. ARGMEN. ARGMEN. Okay. This word is, let me see, ARGMEN. The reason I introduced this word today is because you'll encounter it in books so gradually you notice that slowly I mean to introducing the mathematical notations little droplets so which value of the argument what are the arguments to this function beta naught beta 1 right so which argument will minimize the function argument means let me write this word down the definition of argument by definition, def. So likewise, you can define the argmax, you know, if you want to maximize something, the counter term or the equivalent term would be argmax. Argmax would be those values of the argument that will maximize the function. So the words like argmin, argmax are pretty common in this literature. Argmax. These words, they keep occurring in the ML literature. And this is just, do you realize that this box that I have here, it is a very succinct way of saying, what are we searching for? Our optimization journey is to find this thing. Beta naught, beta one tilde, the beta tilde. Does that make sense, guys? We're just recapitulating what we did in ML100 here. But if you have not understood, speak up, guys. I'm not getting enough feedback. You want me to repeat? I'll be happy to repeat. Asif, I might have missed it, but is this beta zero and other beta zero tild and beta one tild? This is a specific case for one white one error, right? Yes. So what happens is that, see in your model, your model is this, beta naught plus beta 1 x, right? Based on this, we come into the, we create the error function, beta naught, beta 1, error function. It looks, in the hypothesis space, it looks like this surface, right? This is the error surface. What are you looking for? You're looking for the point of minimum error. Is it for one data point or it is for all the summation of the data point? It is the error surface. What are you looking for? You're looking for the point of minimum error. Is it for one data point or is it for all the summation of the data point? It is the summation of all the data points. E is, yes, you're right. See, E is the summation over the errors from all the data points. You notice this. The definition of E is that it is this. Remember, we started out with this derivation. E by definition is the total error, which is the sum of square of all the residuals. And then we expand it out mathematically. And then we notice that when we expand it out, it's a quadratic function in the beta naught beta one, right? So in the hypothesis is space whose axis is beaten out a parameter space hypothesis space or parameter space hypothesis space it looks like this parabola parabolic surface is this did you get that like should i re-explain like should I re-explain? No, I understand the parabolic part. But when we are trying to minimize, is it getting minimized for only one data point? Or how can we ensure that it is minimized for the summation of the data points? See, E is a function. E is derived from the data points, no? Summation over all the data points. But once we have created the summation over all the data points, no summation over all the data points. But once we have created the summation over all the data points, the only things, the only knobs that you can turn at the beta naught beta one, right? Because the points are there. You have calculated the total error for any given value of beta naught beta one. You know what the total error will be okay and so you're trying to find at that point that Dota right this total error is also called the sum squared error SSE sum squared error sum squared error got. So now comes the theory. So this is a recap of what we did in ML100. Here, now here's the intuition. See what happens is when you add higher degree polynomial terms, so you're expanding your hypothesis space, isn't it? So add polynomial. Go ahead. you're expanding your hypothesis space isn't it so add polynomial go ahead yeah the the definition that you wrote the argument of a function is those argument values that will minimize the function i i didn't get this last part that will minimize the function are you by that do you mean the length of an example? Example, let us take one dimension. Y is equal to x squared plus 10. Right? Now, me at what value this function is y as a fx. Let me write it this Y FX so what what value what will minimize of the function zero x equal to zero right so you say that x is equal to zero is the min of fx. This point, the value x is equal to zero, is the argument of the function. It is the place where function is minimum. But the minima of the function is what? Minima fx is what? That will be equal to 10. Because if you put x is equal to 0 it will be 10 right so the point X is equal to 0 fx is equal to 10 represents your minima represents and you can call it Y and so you can say y is equal to this Does this look simple guys, right? So if you plot it out, it will look like this And this will be 10 So this point x is equal to 0 represents your argument The point at which this curve achieves its minimum so so we will write it so the way you would say that is argument if you were to use this notation argument that is X effects is equal to 0. In other words, the value of X that minimizes the function is zero. Is this getting any clearer, guys? Yeah. Right? So this is the meaning of argument, right? So for example, let me deliberately complicate this problem. I'll make it x is equal to 3. Now what value of x would minimize this problem? Can you get it? 3. So because 0 is special, I don't want to use 0 here. So this is it. And so this would be, argument would be three here and argument here would be three right and on the other hand the min of min fx would be 10. the minima that you would achieve would still be 10. that's the concept of argument, guys. So what we are searching now, let's bring it back to our real life. What we are saying is we are finding that hypothesis in simple terms, those parameter values beta naught, beta one, that will minimize our error term. Is this, think about it for a moment is this clear guys if not and repeat it again anybody who wants me to repeat say so I said can you repeat it again yeah see what happens is let me put it in a slightly different way see when you have a total error that error is associated with some hypothesis some value of beta naught beta 1 some parameter values because your hypothesis is parameterized by beta naught beta 1. so any value of beta naught beta 1 that you take there will be a certain degree of error right in simple terms in this beta naught beta 1 plane this plane that is there right this is your hypothesis plane right let me call it the hypothesis plane or space whatever word you want you for every point here there is a certain degree of error any point you take associated with it is a certain degree of error isn't it and so what you're trying to do is finding that point where the error is the least because you're trying to build a model which makes the least prediction error isn't it and therefore that is your that's your best solution that is the best you can do you can just learn your way towards that and just as a recap in ml100 what we would do is we would start at a random value and then we would do gradient descent here so in other words around this point they used to be this remember this is the projection of these shadows of these circles so these are called contour maps here do you remember this guys I hope you remember this because we'll use this to move forward to its regularization now yeah it's it's shadows of these circles, this contour maps, sort of level surfaces are these. And so you take a random point, suppose you start here, it has a huge error. And what you're looking for is you're finding your way. You're just finding your way through gradient descent towards the center. This is your gradient descent as it is then is the argument sort of either very close to or just the irreducible error then no not necessarily see even when you reach the minima you may have reducible as well as irreducible error. What does that mean? See, for that model, it is the lowest error. But you could build a better model. You can dial in the complexity, take a different value of n and so forth. Isn't it? So, in other words, there is still residual bias and residual variance along with irreducible present in the model in the in the minima. Okay in other words let me go back to the terms where we define the error where is it that we define yes Yes. A bias errors. Yes. Look at this pink equation. See irreducible error will remain. It represents that which you don't know. But if you look here at this optimal point, right? Do you notice that the pink line is not zero? The value of pink is not zero. There is still bias error as well as there is yellow line is not zero there is also variance error isn't it yeah that's the thing so even at your best model points there is still some bias error some variance error and there is irreducible error in fact just we did decision trees what happened even with pruning and so forth, it was not possible to reduce the irradial, I mean the variance errors too much. It's still stuck there. So we went to random forest, which made the thing a little bit better. There are many adaptations of random forest that try to solve that problem of overfitting. So that's that. So far, so good, guys. So let me recap what we just did. We are saying that in the hypothesis space, first of all, okay, recap of what we just reviewed. Recap of recap. you recap of recap. At a term which is a function of beta naught, beta one, et cetera, et cetera, is equal to is beta 1 x i square this is the sum square data and as somebody else over all data points data points now in the hypothesis space so the data exists in real space right whatever the data looks it exists in the hypothesis space, so the data exists in real space, whatever the data looks, it exists in the XY space, the real data space. But in the hypothesis space where the coordinates are, by the way, a bigger drawing board is on the way, so I won't have to scroll up and down so much in a few days. Beta node, beta one. is space, we have a parabolic or a surface or this convex surface which achieves a minima at some best point beta. I'll just represent it as beta tilde. You can assume beta naught beta one is built out of it. Beta naught beta one. It is this. But I'll just represent it as a beta delta a vector. This is it, what exists. And these things, they form contour lines. These are, if you project down this, there are these contour lines in this. So if you start with some random point, you will have to find a path going to this. And this journey is your journey of gradient descent. So we won't recap that. I mean, but if you want to remember the rules of the gradient descent it the it was beta minus alpha grad of E and the derivative of E so this is your ML 100 and so go back and look at the nodes you'll see that this is your gradient descent step gradient descent step. In other words, go opposite to the gradient, right? If you're here, go opposite to the gradient. Actually, you know what, let's go through the derivation of this gradient descent. How many of you would want me to recap the gradient descent step? Can you do it please? Okay. Let's do that. But if you do that, it's 825. Guys, give me about, let's take a break actually. Okay, let's take a 10 minutes break, 10-15 minutes break, and then come back. So to motivate this, I will just take a simple example of what a gradient about gradients to illustrate a point see if you have a function let me take a function y is equal to basically your FX that looks like this right it could for example be something less something x squared or something like this. This is your x-axis and this is your y-axis. I want to explain what gradient descent is, the basic intuition. And again, I'll move a little bit faster because we have done this before. Gradient descent is a way to start from any random point and reach this destination this point what is this point this point is your arc in a new language argmin of FX isn't it it is that special value of X X till day where this function is minimized so far so how do we reach this how do we how do we come to that there's a very interesting thing to observe here and that observation has to do with the slope of the line look at this what is the slope here guys at at X tilde slope is is it flat rising up or for falling but it's flat slope is zero now the definition of slope is increase in height height for unit movement unit increase of X unit increase of X so suppose I suppose I am here at this point if I make a small increase Delta X improve increase Delta X and this is the step forward so I would go from the point this point to this. So have I increased or decreased in height? So when I go from this point A to B, let's take a negative slope X and B is equal to X plus Delta X, right? The height decreases. is equal to x plus delta x right the height decreases so you say that in the vicinity of x the slope is negative right the slope so y decreases in going from going from a to B ie from X to X plus Delta X isn't it guys so why decrease decreases in going to that and so the conclusion we have is slope is therefore at a slope is negative negative now let's look at the Let me call this point. X prime Right or At x prime. If I go a little bit a unit distance forward. Delta x forward. I go from, let's say, see to D. Right. So when going from C to D. From delta x prime C is equal to x prime to D is equal to x prime plus delta x Right. Do we increase or decrease in height. we what did we what did we agree it increased positive slope yes it increased so we increase we went up the height increased So slope is positive. Derivative of function. So we can say it's usually written as a D effects DX or a D Y or dy dx and sometimes for shorthand people often use a prime to signify right pronounced as y prime they often use this I wouldn't because I tend to use x prime y prime just for a point so I will not be using this notation, right, so not be using this notation, will not use this notation. So these notations are common and I'm just recapitulating the calculus that you have learned in high school and this is just a the slope is positive now comes a interesting thought guys at this point suppose you are at a if you want to your goal is to go home to find the optimal point this right so suppose you are at a and therefore this is the error in which direction should you go to decrease the error, decrease the y. Should I go in the positive x direction or negative x direction? At a. Positive x. In the positive x direction, right? So you can say at a, at a, we should go, At a, we should go, go in the positive x direction. Lope is negative. dy dx is less than zero at this point what about b at c where is c let's go back and look at c at c if you are at c guys do you want to go c uh this is the ffc and do you want to go in the positive x direction or negative x direction to decrease by Sajeevan G. Negative Sajeevan G. Negative x direction. Sajeevan G. At sea, we should go Sajeevan G. Go Sajeevan G. In the Sajeevan G. Negative Sajeevan G. X direction. in the negative x direction here slope is positive. So do you notice something very interesting? If the slope is positive, we want to go in the negative direction. If the slope is negative, we want to go in the negative direction. If the slope is negative, we want to go in the positive direction. Isn't it? And if the slope is zero, we want to just stay put. Isn't it? At this point, do we want to go anywhere if you reach this point where the slope is zero? No. If slope is zero, so let's write that condition down at go nowhere we are already home right we are already home observe. So one of the things you can do is you can say that at any given point to make to minimize the function, go the next value of x should be, or let me put it this way, the next value of x, in this case, you want to move in the positive direction, right? But the slope is negative. Suppose I did this, dy dx, and the slope can be a big value. I multiplied by a small number, a small step. I take a small step against the slope the slope is negative I want to move in the positive direction at at a remember once again this is a this is C at a slope is negative dy dx is less than zero dy dx is greater than zero just to remind you so at a what do you want to do if you take a if you multiply the slope by a negative number negative of a negative is a positive number isn't it guys yeah and so you're taking a step in the right direction would you say that the next value of X X next will be somewhere here this will be your B right it will be where you want to be so this is it at now let's look at this at C what do you want to do slope is positive once again your next value of X should be a hop back here you want to hop actually let me make this hops more visible you want to move in this direction here you want to move in this direction isn't it so you want to do at C what do you want to do you slope is positive once again you want to go against the slope the dy DX does this make sense guys if you take a little step like this and alpha you can take as a very small amount like 0.01 you take a tiny step it's a tiny step in the right direction and you can keep making those tiny steps till you reach home right here is your x tilde so to go towards x tilde to go here also towards x today is this obvious i think i'll say for your point c you are missing an equal and x there i believe oh right right right oh i am sorry i i forgot to write that you're right um thank you for pointing that huh so minus the previous value of x minus the negative of the slope because here the slope was positive. Negative and here the slope is positive. And so you realize that irrespective of where you are on the increasing or decreasing slope the general rule rule to take a small step towards home towards X tilde x tilde is that x, the next, your next value of x should be the previous value of x minus some small number, some small fraction of the dy dx, right? This is it. Do you see this, guys? I hope this is this intuition is clear to you from this. I'll recap. For any function, if you go against the slope, you're going towards the minima. Right. So the rule is, I'll write it down. In other words, if you are going against the slope, you are going towards it's just many ways of saying the same thing so now let's generalize it to higher dimensions suppose Y is a function of is a function of x1 x2 xn how do we generalize now there is no notion of dy DX because the question is what is X X is a vector now right and so it turns out that what you can do is you can say that suppose I have a surface like this, I can make a small step like at any given point, I can either move a little bit in delta x one direction or delta x2 direction let's say okay let me make it x1 x2 then i'll give the i should reverse the order delta x2 x1 right are we together guys i can see how this function fx varies as i move in this direction or this direction right so you can say we can look at the change in Y as we move a step long one of the axis variable axis isn't it all you can do is take a small step in this direction or this direction each one of them will produce produces derivatives the partial thus the slope specifically along each axis keeping other not moving not changing along other axis that's important criteria not changing X value along other axis is called it's a partial slope right it's sort of a partial it's just a slope if I just change X 1 right so how much it changes the value changes for partial derivative. Derivative. This is what a partial derivative is and the way we write it is you'll see me use this notation dy d x I write our df x vector along this particular direction you'll see these two notations often used here notation so then comes a very interesting question. Suppose I make a change from the unit vector X to X plus Delta X means if I make small changes along all of these, if you make along each of these values, then what happens? How much change do I get? It is worth asking, what is the value of the function x plus delta x? And that value actually can be represented by saying that it is actually, just as in the case, so let me before I go there let me bring up a concept so if you put all of this let's say that there are two axes x1 x2 right and you are looking at Y as a function of this so you will have D Y a D F D Y let me just write it as D Y1, dy dx2. Both of these you can write, isn't it? So there are two components here. If you treat these two components, now here's a magical thing you can do, and you'll see why. If you can treat these as components of a vector, see x is a vector x is equal to x1 i plus x2 j this is how you write the a vector isn't it guys the the standard notation and it is also we will but in machine learning you don't tend to write i j j, k. You tend to write x1, x2, a column vector. A column vector. Vectors are written in machine learning as column vector. And it is absolutely equivalent to the same thing. It's just a different language in which we write it. So x is a vector. Now, we notice that when you take partial derivatives, you can take partial derivatives with respect to each of the x's. So we can therefore ask, what is this column vector which is the same as dy dx 1 i plus dy dx 2 j what is this it turns out that this is a very useful thing it is called it is very instead of just like you can have a slope it is the generalization of the concept of slope it is this notation nabla or the grad it is also called the grad or the gradient these words are all used nabla is the Greek letter. So is the nabla of a why it's called the gradient of why and more generally, people can just write it as an operator as an as an abstract operator. You say this is just this. It is an operator that is waiting to eat something. 1 d dx2. Do you notice that this is very funny, a partial derivative of what? You haven't specified that. So if you put y here, it will become this. But so if you put why here it will become this This is, and so this is equal to by definition. This is the partial. This is called the gradient of why now. We realize that in ordinary differential equations, the y plus delta y was equal to basically x plus, you know, it was basically delta x. So I wouldn't actually, let me not go into the calculus. Just take it as a fact of life delta y in single dimension in one day variable one variable is d y or d x times small change in x a small change in y is the slope times the small change in X. Which is sort of our intuition is, and it's very obvious here, that you are here. If you make a small step forward, the decrease is proportional to how big a step you made and how steep the slope is. Isn't it? So that's the idea. How much you lost in height or gained in height is proportional to that. So this is it. It's generalization. It's The change in Y in the, in a vector notation as a vector, let us say that, sorry, the change in, sorry, in a, in a Y when X is a vector is actually gradient of Y dot product with small change in X, because now the change in X could be is a vector right you you you go you go from this point to this point. Right. This is the x vector x one x two Sajeevan G. And you made a small change delta x Sajeevan G. And so it is given by this equation. Sajeevan G. Small changes in this. Now, what does it mean? Let us study this gradient and we'll be practically done with the, so this is just the mathematical background to understand regularization. What does, if gradient is a vector, do you realize that gradient is a vector? Then where does it point? Vectors point in some direction, isn't it? So let us study that. Consider a circle. If you have a circle, let's take an arbitrary point this point is a p is x y right from the origin the line that connects to this is the p vector is that obvious so now what is the equation of this point y right at this particular point do you realize that the equation of this function is fx actually let me use the notation of x1 x2 not use x2 I can write it as X1 square plus X2 square. Right. And the circle is circle is Fx1 X2 is equal to some constant R squared. Would you agree? It is just a constant, what you call a contour line of this function. Now let us find the gradient of this function and see what it comes out to be. Gradient of this function is equal to partial of this function with respect to x1, partial of this function with respect to x1, partial of this function with respect to x2. Now, look at this. If I take a partial derivative, means x2 doesn't change. What is the derivative of x1? 2x1. 2x1. And the derivative with respect to x2 is 2x x2 and you observe something very interesting for a circle and this happens to be just pointing in the direction of Why is just the P vector? Isn't it? P is your P vector is given by X1, X2. Are we together? Right? So what does it mean? Where does the gradient point? Gradient is just a vector that points. This is gradient of FXx1 x2 isn't it guys so far so good guys now there is an interesting lesson if I look at lots of circles of different these are all the functions have different constant values so you would agree that along the circles the function has to be constant right those are constant value surfaces or constant value curves for the function what is the gradient point relative to those circles what is the direction of the gradient is it parallel to it is it inclined to it or is it perpendicular to it it's perpendicular so we notice the fact we notice that let me write it in a different color so that this is emphasized or maybe I'll use the same thing observation the grad F let me write it as X vector is perpendicular to Pillar 2, this is the symbol for perpendicular to the constant curves. These are called contours. To the constant curves, i.e., the circles. circles is some constant R squared right for some R isn't it guys this observation is very general fact. Gradient of F is perpendicular to contours of F. f effects for any and every function this is a very general fact I use the example of a circle to illustrate it but actually this is a very general fact it's a it's a thing you need to remember in mind. Well, that's a long detour. So where are we going with all this math? Why all these mathematical preliminaries? So here is where it is coming from. So let's go back to our energy surfaces, our sort of surface surface this was a beta naught beta 1 these were are these were are this thing so here do you notice that you had these contours surfaces. Also observe something. Where does the gradient point the direction of the first of all, it is perpendicular to the constant surfaces, but is it pointing in the direction of increasing F value or decreasing F value? Does the value of F increase as you go in the direction? Does the radius increase as you go outwards in the direction of the gradient or decrease? Increases, isn't it? In fact, it increases the most. Perpendicular to the gradient is of course the flat, the constant surfaces so the second observation is is perpendicular to the and every F and points to the direction of maximum increase of fx at that point it's a very general observation the gradient gradient points to the direction of maximum increase so you want to increase a function follow the gradient if you want to decrease a function and you go against the gradient because that will be the fastest way to decrease a function gradient descent gradient descent and therefore when in our case what do we want to do we want to decrease the error the error. So we go against the gradient, right? In other words, if you are at any given beta value here, let us say that this is some arbitrary P given by beta naught, beta one. What is it, what do we want to do? What, and this is the perfect, a beta naught till day, a beta one till day. You want to go here. What is the way to go there? What do you do? You will say beta naught's next value should be equal to the previous value of beta minus alpha times yeah so in vector notation you can go if you write just beta minus the gradient of the error function right this is the equation the celebrated equation of a gradient descent that is essentially the heart of machine learning. The learning of machine learning often amounts to the gradient descent towards the minima. That is why I took some time to explain this concept because it is not just a concept in machine learning, it is one of the concepts central to this field. It's the gradient. So if you break it out into components. You could say that a beta one or beta zero next Zero next Is equal to previous value of beta zero minus a DED of beta 0 beta 1 next is equal to beta 1 minus alpha DED beta 1 right and so forth you can keep going down to other values of beta n let's say suppose you have betaad- Is equal to beta and minus was the end dimension space parameter space, the beta and right. It's just a general it's just this thing. Jay Shahzad- rewritten in simpler broken it out into components. This is your gradient descent. This is how you solve the problem. Now let's come back to the machine learning part of it right right? This was a detour through calculus, basic calculus, right? We took the detour. Now what happens is, and I'll mention this without giving a reason why, when you take high degree polynomials and when you have overfitting, when you take when you take large N for poly regression, you will, what will you end up with? You will have beta zero, beta one, beta N, isn't it guys? You will have different parameters, beta naught, beta one, this thing. What you observe, they tend to get very loud or loud. So what will suddenly happen is your beta naughts, good values of beta naught will be less than two minus one or something like that. But these values, they will suddenly start looking like minus one, five, seven, nine, six, three. Right. This will look like plus one, 68, two, one, four, two. Do you notice that these values of beta they blow up the values so every factor seems to matter a lot each of the parameter seems to matter a lot it seems to become very loud you know very large values geometrically would and I wouldn't give a proof of this but just take it as a fact that this happens so what does it mean in real terms it means that if you look at the error surface this error surface right it just runs away the minima here is very away from the origin. That's the oscillations we observed we observed for large n. Right? Overfitting is their high variance by terms. So what does it mean? If you go and look at this curve, let's go and look at this. Where are we? Yeah, let's go back at this picture. What you will notice is that for the blue line, the betas are reasonably small, beta naught, beta one. For the, where is is this parabolic curve for the red line you'll have beta naught beta 1 beta 2 they will be also reasonably small for the yellow line 3 bit 4 biters beta naught beta 1 beta 2 beta 3 they'll be a little bit pronounced but still reasonable but by the time you get to green lines your parameter values are big and by the time you go to the pink line your betas are really big are you getting that guys right they become really big and that is what is causing oscillation one is pulling it in positive direction the y value one is pulling it in positive direction, the y value. One is pulling it in a very negative direction. Here, let's do it there. So if you have very large values, you could realize with positive and negative signs, they are all pulling the y value in different directions, positive and negative. So you start seeing a lot of oscillations in the function so what do we do to cure it you basically say that we are not you take of the absolute minima that is here you know of this function let me draw it out again so what do we do how do we solve this you guys i'll continue on for another half an hour i want to finish this topic it's an important topic what happens is that if you look at this bowl if you look at this bowl and our origin is here sorry let me draw it here let's say that our origin is here so you notice that this has run away here very far off right it's a classic imagine that there are lots of meters. This is your error surface. Right? So here are the contour lines. What you can do is you can say regularization says that we don't want the absolute minimum. We are happy so long as we go a limited amount from the origin. Let us say that you set a limit radius r, and you say that around the origin, I will go only radius r, but no more than that. So whatever is the minimum value of error lifted out from here, wherever it meets the error surface, whatever error is there within this disk, within this space is what I will consider. Let me draw it out in two dimensions. In 2D, in the contour view, what you have is this and again I'll just draw it you're saying that I have a certain radius of a budget right and I am not willing to go or find a solution above it. But the real solution is here very far away. And it has its contour lines go here. More contour lines go here. So you notice that inside the desk, let me mark the desk as this. Let me mark it with big fat strokes. Oops, that didn't quite do it. Let me take this. This is where you want to be. There's one intuition I can give. In India we have the story of the Ramayana. intuition I can give. You know in India we have the story of the Ramayana and there is a scene in the Ramayana, the Panchavati, in which, so the story goes that two brothers, one is a king named Ram and he has a brother named Lakshman and they're very very close of course and Ram's wife is Sita. And now they are the good guys. And there is a demon called Ravan, who is always on the watch to grab or cause mischief. So Ram and Lakshman, the two good guys, they have to go out during the day. They're in the forest. They're in exile in the forest and they're supposed to go out and hunt for food, find food, look around for food so while they do it sita is alone at home and there's all sorts of danger so what lakshman does the brother does is he draws around the house a circle right and that is our circle here you draw a circle right and that is our circle here you draw a circle this circle is your is your limit and you just say that so long as you stay within the circle you're safe it's an enchanted circle no demon and no a bad influence can find its way here find its way into this circle, right? Can come into the blue area. Think of it like that. In terms of our regularization, what it means is that you don't go out of the circle because you know that if the values of beta blow up, you will have overfitting so you set a budget you say that no beat us all the beaters must the sum total of all the beaters the magnitude must be within this point right so you could pick any point here but it cannot be more than a radius let me just put this here and the radius this is the origin it cannot be more than a certain distance r from the from that so now the question is you realize that above it are the is the error surface rising you realize that this is the point of minimum error. This is your beta tilde here. This contour line represents, if you think about this, below this are these contour lines, right? They are mapping to these contour lines. So what happens? Every concentric circle, that is, the further you are in this concentric contour lines does the error increase or decrease they represent surfaces of greater or lesser lesser error this is the point of least error this will have this will be this error surface increase increase increase but what you're saying is, so what is the point of the least error inside your circle? So let me call this surface E A, this surface is EB, this surface is EC, this surface is ED. So which of these, if you want to find within your circle, a point which has the least error, which would be, would you agree that EA, all the error, so EA, constant error surfaces, EA is less than EB is less than EC is less than ED? Yeah. Yeah. yes that is care so now i ask you this question what is the point of least error not absolutely least error which is here in beta beta tilde but the least error within this uh enchanted circle just at this point isn't it let me mark this point as some point uh this uh me mark this point is some point this this point isn't it this is my this is within the enchanted desk. We agree with this, right? Now let's think about this. At this point, see, this is a circle. We just realized that in a circle, where does the gradient of the circle point so this circle is given by f a beta beta naught beta 1 is equal to beta naught square plus beta 1 square and we can generalize this is the equation of the circle does it make sense guys we are in the parameter space So here the axes are not x1, y1, but beta naught, beta 1. So this is the equation of the circle, isn't it, guys? This makes sense, isn't it? And the circle is just, is equal to some constant, r squared. I'm sorry, r squared. So far, so good, right? So now, what is the gradient of the circle? Gradient of F is equal to whatever you can just say a 2 beta naught 2 beta 1 so 2 times this it would be at this point which is of the the constraint is this right constraint surface beta is this now what about this error surface? You see that the error surface EA, let me go to the white color now, the EA, this error surface, this is a contour line of the error surface. Yeah. Point. Yeah. Right. Where would the gradient of that point, which is the direction of maximum increase of error. Let me- let me know opposite opposite direction isn't it let me use a color that would be visible in this so I've used maybe green green might serve a purpose let's agree yes let's take this so I'll just make it like this so the this would be the gradient of the error surface would you agree yes the gradient would point like this and so you observe something that you be observe that gradient of F and gradient of E point in opposite directions. Did I spell it right? Opposite directions. Opposite directions. Right? And so we can write gradient of F is equal to minus the gradient of E. But we don't know what the magnitude is. So we'll put a lambda parameter here. Right? So far, so good, guys. The gradient of the constrained surface and the gradient of the error functions, they point opposite to each other. Then there must be, with up to a negative sign, and a proportionality constant. This must be the equation. This is how you write two vectors which point in opposite directions. to a proportionality constant they should look like this yeah that's right so it turns out that what is what i just showed you is not just true of uh just error functions it's true of any function and any constraint here i made the constraint into a circle you could take any constraint surface surface right and you could take any function the point the optimal point where these two things these two surfaces touch just like this you know they glancingly touch tangentially touch and represents a point of the best you can do is always given by this equation and it was discovered by the great mathematician Lagrange I hope I didn't mistake is it Lagrange or Lagrange could one of you please verify I think it's Lagrange oh Oh no, it's not. It is just apostrophe grand. Oh goodness. I should know this. Could one of you please verify and tell me which is the correct one? So it is the Lagrangian. You say this is the Lagrangian multiplier. And what would- L-A-G-R-A-N-G-I-A-N, Lagrangian multiplier. So the name is Lagrange, isn't it? The person Lagrange. Yes. The mathematician Lagrange. So you typically pronounce it as Lagrange, or at least I do, I don't know if that is correct. The mathematician, he was a great mathematician, he discovered not just this, his work is prolific in calculus and analytical geometry and analytical analysis. So this is actually, what I did is using the example of our error surfaces and constraint and regularization actually i took you guys to the discovery process of a fundamental result in calculus this is in fact constrained optimization this is it optimization and so what you can see is this is constrained optimization and when you do it when you apply the disk a budget a unit circle so what so let us summarize My spelling is getting more atrocious. Summarize with unit circle as constraint. As constraint. You get error, we can find the minimum error within the circle. That is the solution we will take. So let's explore this a little bit more. There's a very interesting consequences with this. So we have constrained E, this is equal to minus okay, I think I don't know which way I put the lambda, I put the lambda on the E. Let me do it the other way around, actually, because that is much more conventional. Let me put, it doesn't matter, right? If we are just defining the Lambda. So who cares? Lambda multiplier. Typically you put the Lambda on the proportionality constant here, right? So I'll put it here. This is more traditional. Right? Are we together? You put the proportionality constant on the constraint surface, not on the main function. So it is here. It is like this, lambda. And it is, by the way, just convention, nothing more. This is it. Now, let us rewrite this equation. What we are basically saying is E beta plus lambda F beta gradient of this is zero this is what we are trying to find isn't it I just rewrote this equation there's and from this we conclude that in other words, we are minimizing E beta plus lambda F beta. This, right? What you call it, see error function we were minimizing before. Now we are minimizing a different function. This is called the loss function in machine learning. And now you got introduced to your first loss function, which is not your sum squared error. Actually, we did. There was another one in logistic. Did I do the MLE derivation? Otherwise, in mathematical methods, we'll of course do that. Function, loss function. this is called the loss function the word loss function is pretty very it's pretty pervasive in machine learning and so today you got introduced to the loss function of this problem this is the loss function so let us work it out what does it become this is what you try to minimize. So in other words, now you say solution, the optimal point. Point is where Is where point beta. Let me just say This is The point beta tilde is where By the way, do you notice that I'm using a very scripted L, right? I'm not just writing it like L beta. So it is much more common to use this calligraphic L for the loss function. Mathematicians tend to use the calligraphic L. If you use just ordinary L that is used, but more common is to see this calligraphic L used for the loss function. Loss function is one of the most important constructs in machine learning. You can say that machine learning walks on two legs. First I told you that you quantize the error and then you decrease the error. Now I'm extending it a little bit more and saying it's not just the error, it is a loss function. In any given problem in machine learning, you first write a loss function. Once you have written a loss function function then you can do gradient descent on the loss function to find the minima right so use gradient descent to find beta tilde beta tilde is arc min beta for the loss function. It is that value of the beta that will minimize the loss function. Are we together? And this is a core result. This was where the long journey was coming to. This result is not just true of the topic at hand it is generally true in a wide variety of situations in machine learning as we will realize we just did constrained optimization now let's look at it remember that i told you many minkowski norms different now recall the various distance. Do you remember that from the last discussion that we did, that there are many different ways of defining distance, the unit circle. I will go back to the top the mannequin distance equilibrium yeah that's right i will remind you that we did this is did i do it in this one or i did it yeah this one l1 l2 yeah l1 l2 norms etc so this is it. Yeah, L1, L2 norms. Yeah, we were talking about this distance notation of norms. So we will go back to that and ask this question. I'll just recap what we learned there. So do you guys see that, you see that all these ideas tied together in machine learning, right? You may say, wow, it's such a vast space of theoretical constructs, but you would be pleasantly surprised to know that pretty much if you got here, as far as practical machine learning is concerned, this is all the concepts you really need right a loss function gradient and its gradient descent that's all that's all you really need in machine learning to a large extent i will come to that so what are the minkowski norms if you remember the minkowski norm of uh two points was x prime was equal to x i x 1 minus X 1 prime to the power L plus X 2 minus X 2 prime to the power L this the whole to the power 1 right now now comes the surprising thing look at this little disc that we made in the Lakshman Rekha that we drew here. The yellow line. This is a unit circle, isn't it? What is the definition of a unit circle? What is the function definition? For L is equal to 2, the fx, 1. So now let me write it in the language of beta because we're in the parameter space beta beta naught beta 1 would be beta naught square plus beta 1 square isn't it so the constant values of this function were the circles what is the gradient of this so well for you plus you already did that before so this is it do you notice that and what was error the in this thing in the loss function if you remember the loss function was was equal to the error function of the beta plus lambda F beta and let us replace it with what they really were the error beta was actually equal to going back to the old story of where we started from it is y minus y i minus y i had square. Remember why you had a beta naught plus beta 1 X I yeah square plus lambda now what is fx beta naught square plus beta 1 times square plus this right and well this is the equation that people write in books though I would I would highly recommend that you think in terms of the way I taught you as a constraint surface between error that you want to minimize given a certain constraint. This is the constraint. And now your book essentially writes this equation without it so this is what they're trying to say right i've given you a much more detailed picture of this and you write therefore l beta is equal to sum over all the data terms of y i y i minus y i hat square plus lambda sum over beta i square this is beta j right beta J wings J is equal to 1 J is equal to whatever number of dimension there is n dimensions right sorry whatever degree of polynomial you are taking beta naught beta 1 beta 2 beta 3 whatever does this look like it so when you use n is equal to 2 this is called the ridge regularization irregular right or then you you say all right that is fine but what happens if I take L is equal to 1 right then the L is equal to 1 Minkowski norm means what remember what does it do to your a constrained surface now it became a diamond yes I'm sir diamond right diamond becomes a diamond so let me go back and color this diamond big bold strokes okay I think I'll have to write it so now the in this world it is still remember this is a circle it's a unit circle isn't it it looks like a diamond but because of the L1 norm this is how a circle would look all points on the boundary are unit distance from this origin if your your distance, if you define your distance where the L1 norm, this is where we go and this is your thing. Let's go back and draw our contour lines. Where do they seem to touch? Corner. where do they seem to touch corner on the axis this is something very interesting this this is the x1 axis and this is the x2 axis what happens is more notice how the pointy edge of the diamond Error surface, error contour. Isn't it? This is your point that you're looking at. It's no surprise, actually, that very, very often, because it's sticking out. If you have your, you know, imagine walking into the bar and you have your elbows like this. Have you ever tried doing that? Imagine walking into a crowded bar and you have your elbows like this what will happen you'll make friends not yeah yes exactly you will hit a lot of people and annoy the maximum number of people that you can so that is it so these are the elbows sticking out so it is much more likely to hit the contour surfaces, the contours of the error, the error, contours surfaces of the error in this, in this space, in this plane. And so what it means is now what happens when it hits it? What is the value? So by the way, this is not X1, X2, I apologize. Remember we are in the parameter space. This is beta naught one right beta 1 in this space so what does it mean at this point what is beta 1 it is 1 0 yes exactly so what will happen is at the optimal point beta naught will be will be some value right and beta 1 will be 0 do you see this guys this point is beta naught tilde and 0 so what does it mean in terms of our equation. What it means is that y as a function. Plus a beta one x. What happened this term just disappeared. Right means x doesn't matter. In effect, so now the way it really works is not like this the way it works is let me just call it deliberately I'll cheat let me go to n dimensions it's a polynomial of n dimension right so the n dimensional space so beta naught plus beta 1 let me just say beta nth dimension, right, beta one x plus beta two x squared. Sorry, x squared. X, x to the power n. What will happen is when you hit it along this axis, a lot of the betas will start disappearing. Right. So what will happen is out of the beaters that you have with a one and beta two, beta three, beta and beta I let me call it beta J beta N. What will happen is because the you punctured the contour along this some of these right betas may start disappearing and your problem therefore reduces to a lesser number of terms in the equation do you see that guys and so what will happen is your So what will happen is your model will be simpler. Not only that, you have done smooth and reality reduction as a side effect. You have done dimensionality in what sense? You said that all of these high order terms mattered they don't matter right reduction and now let's generalize it to many variables let us say that you write Y this suppose you write Y as a function of beta naught plus beta 1 X 1 plus beta 2 X 2 you know this is really a different feature this particular regression the L1 norm will start destroying some of these features right so you will get it will say that those features don't matter you can get rid of it what does it mean you, it will say that those features don't matter, you can get rid of it. What does it mean? You have reduced those features, removed those features from your feature space. Therefore you have reduced the dimensionality of the problem. Right? This is actually one of the great benefits of L1 norm. There is a name for this. It is called the LASSO regularization. It's surprising that it is actually, well, rigid regression was discovered a long time ago. Lasso was discovered somewhat more recently. And the authors of your textbook, actually, they have done considerable amount of work with the lasso. They even have a book on lasso regression. It's just L is equal to one norm. And so your beta function is, this is Y minus Y hat square summed over I. But what is your function now? This lambda, remember? What is your function? This is equal to, the unit circle is defined as a beta not absolute value plus beta one absolute value plus beta two absolute value, so on and so forth. So I can write this as L beta hat is equal to sum over I, y I minus y I hat square plus Jay Shah, Dr. that you take it will be this and so when you minimize this is the loss function for lasso this is your loss function see guys this these things are given in books typically it writes the equation and quite often you don't know where the equations are coming from so I went through the derivation now why did I go through this long mathematical derivation? Last functions are at the heart of machine learning and quite often last functions have constrained terms and so forth. So I thought that if today I introduce you to how people create last functions, how they reason about it, how they think and find ways to improving the model or regularizing it or doing things you have gotten to one of the key activities researchers do in machine learning so loss function for last two and This is I promise you the most technical will ever be in our machine learning workshops But do this much level do please try to understand because you need this much if you really want to be good in this field you should know the foundations and this is these are the foundations right so you should know that and by the way all of these things we will review again when we do the math of machine learning right here I'm more focused on the regularization aspect of it so this is the loss function and it has a beautiful effect what happens is that there is this regularization parameter you know this lambda what what lambda does is as you increase lambda the value of the different beaters they they begin to go to the let's say that they start with certain values. They go across, or let's say that you normalize, actually, let me take a more normalized view of it. Let's say that they all started some normalized value at the origin, this beta. So what will happen is some will fall to zero here, some will fall to zero and some will, lot of them will fall here, some will go here. So suppose you take a cutoff of beta here, this point. Suppose you take this as a cutoff. So what will happen at this cutoff, these values, these betas will disappear, right? These features will, this each beta is associated with a feature these features will disappear why will they disappear because their coefficients their parameters have all gone to zero at this value of the lambda so what you do is you choose your lambda wisely so that a few at this moment this particular beta is alive this is alive and this is alive but these ones have all gone to zero so you you do dimensionality reduction by sliding lambda back and forth right sliding back lambda back and forth and you get certain feature reductions go ahead so as if in this in this graph you're assuming that you know all the betas are kind of in the beginning and the lambda is zero you're saying but i ideally they will be different but still the effect will be same no no whatever their values are yeah it will be different one way to normalize is divide everything by oh okay okay normalized just normalize it to one and then see but in reality see i do that more complicated picture but let's go with this so this is what happens you have dimensionality reduction so we learn two methods L2, which is more common is the rich is called the rich regression DG rich regression rich regression. Been practice for many years here the constraint surfaces. Is some over beta square L1 lasso. Regression. sum over beta i square l1 lasso regression you want to use this f beta is equal to sum over the absolute value of i you know remember manhattan distance unit circle of manhattan distance of this so this is it and it's quite interesting that if you understand minkowski norm you understand both of these ways of regularization so easily not only that you can cook up more or more regularization methods of your own for example why not try this why not try l3 i mean you can do whatever you want basically now one of the questions that arises is which is better they both have their advantages and disadvantages lasso has one advantage that certain features just disappear you have dimensionality reduction rich will never make the features disappear it will just make it very small because it's circular so all the feature all the betas will have some more or less non-zero value the ones that really matter will have bigger values the ones that don't matter will have smaller values but they won't disappear so that is one of the downsides of the rich people say but you don't get the dimensionality reduction benefit but the problem with lasso is that it over suppresses right it is too pointy sometimes it gets rid of too many features right and so what people do is between l1 and l2 again you have to try and see which gives you the best cross validation error there is also elastic net what does it do it basically says put get a loss function is get a loss function with both L1 and L2. So then you end up with two lambdas, right? One lambda, one associated with lasso, one lambda, two associated with ridge. And so you try to optimize this. Now you have two hyperparameters to search for. So, elastic net is a very obvious extension. It says in the loss function, go add both the terms. Both the Minkowski L is equal to one term and the L is equal to two term. That becomes elastic net. So, guys, this weekend on Saturday, we will do a lab in which we'll see the effect of all of this regularization so guys today you know i developed or recapitulated a lot of math we took a lot of time recapitulating the math which we have done in the past but we needed it today to understand the regularization but once you understand regularization let me summarize higher degree polynomials without regularization they tend to oscillate right data you can suppress it with more data which is the best way if you can but quite often you can't have enough data data is rather an expensive commodity so you have another technique of constraint optimization. And again, as I said, in very, very complex models like deep neural networks, it is a given that the parameter space is vast, hundreds of millions of parameters. And even if you have hundreds of millions of data, it's still not sufficient. So you need regularization, you need the technology of regularization and constraint optimization. So that is driven by putting a disc around it. So we talked, we went through the whole explanation of what is gradient descent. Some of you asked for it. So I did the recap of that. So at any given point the function achieves a minima, you go in the direction of the minima if you take walk against the gradient. Right, and we saw that and both in one dimension effects and in multiple dimensions. Gradient points opposite to the gradient is the direction of maximum increase so if you want to quickly decrease you go against the gradient if you are in Mount Dabla the last thing you want to do is go against the gradient what will happen if you go against the gradient you'll tumble down the hill isn't it because gradient is the steepest path right up the hill you don't want to go the steepest path downhill because the steepest part downhill probably means you're falling down okay so this is it this is what we are saying that this at in a when you have a constrained region and you say give me the minima here then then at that point, the minimum white optimal point Jay Shahzad- The gradient of the constraint surface and the gradient of the actual error function will be Jay Shahzad- Opposites. So in other words, you can write it in this way error is the up to a proportionality constant and a negative of the other one. is the up to a proportionality constant and a negative of the other one and that is essentially the equation of lagrange multipliers this is called the lagrange multiplier at some point in calculus you must have learned this many many years ago in your calculus anybody remembers learning this in calculus so but if not then well today you learned something very important. It's constrained optimization thing. And so regularization is as simple as saying, what if my constraint surface are the Minkowski norms, you know, unit circles with different Minkowski norms. That's it. All of regularization can be asked in one simple statement. What if I apply a constraint in the unit circle constraint in the parameter space, in the hypothesis space, that the values of the parameter must be within the unit circle? The moment you do that, both of the regularization come together, come like this. So in other words, if you were truly a a very very skilled mathematician who knew all of this theory very well and the way to explain regularization to that person would be oh it's constraint optimization uh in the hypothesis space of the error function the constraint is the unit circle with different norms that's it so that gives you your lasso and your ridge regression. So that's what I meant. That's a summary of what I am saying here. Elastic net is the hybridization of the two. So I would like to stop here.