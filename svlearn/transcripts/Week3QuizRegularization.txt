 So today is Saturday 26th. We are going to review the quiz on regularization. Now, when we talk of regularization, there are certain techniques which are broadly applicable to all of machine learning and then there are specific techniques that that we encounter when we do deep neural networks let us go to the quiz now So this was, I thought that I would make a quiz in which you would have to study a little bit beyond the lectures. You would have to go and review the books, the textbook and so forth to understand all the material. Now I've made this quizzes multiple attempts so if you do a quiz you you don't do well you can go read up come back and try again. The idea is to keep learning what you don't know. The first question is data augmentation helps in regularization of machine learning models during training. This is true the more more data you have, the more regularized your model will be. The less overfitting there will be. Overfitting is a sign of sparse data compared to the number of parameters or the size of the parameter space. So this is certainly true. The next question is, while training a model, early stopping is a technique specific to neural networks and not applicable to other machine learning algorithms? So the answer surprisingly is no actually. This is applicable to other machine learning algorithms too. When you train a neural net to other machine learning algorithms too. When you train on your net to any machine learning algorithm, first it is picking up on the big signal. The big things would be the actual signal, the information. Gradually as that that lemon gets squeezed, at the end of it noise which are small perturbations around zero, they start kicking in and the machine learning algorithm in the training process begins to learn from that. So early stopping can be applied to all of them. A prediction model can exhibit both high bias and high variance errors. Yes, contrary to people's beliefs, these are not opposites. It doesn't mean that if you have high bias you won't have high variance. You could actually suffer from both the problems. Now you may ask what about the bias variance trade-off? See, that has to do with the complexity of the model. If you dial up the complexity of the model, the variance errors begin to come down and the bias errors begin to go up. So then as you dial up the complexity, you'll have less bias errors. Somewhere in there is the complexity of the ground truth. So when your model that you are trying to build is less complex than the ground truth, you'll have high bias errors. Then as you increase the complexity of the model, you may gradually see the bias errors go down, but you may start seeing high variance errors. And because it's beginning to overfit but it's not only the case sometimes what happens is there is still bias errors and you end up you end up increasing the variance error so you can have a model that suffers from both bias and variance errors at some point i'll give you an illustration with a data set to show that this can happen. Asit? Yes. So you know what I thought about is if your model is under feeding then it has high bias and if it's over feeding you have high variance right. So this is true. You cannot have a model that is doing both isn't it? Yes so the operate the crucial word there is local regions of the features space. So what can happen is a model can under fit the data in some regions of the feature space and overfit the data in other regions of the feature space. Sajeevan G. So imagine that your data is along the x y axis right there. imagine that your data is along the xy axis right xy plane and just as a very simplistic notion imagine that below the horizontal axis the model is underfitting the data and above the horizontal axis whatever model you build is overfitting the data so you're saying you can build a model that takes worst of both? Yes, can have, can suffer from problems of both. And it is not as uncommon as one as one would think. So it's surprising that most people don't know, you can actually suffer from both. But in a local region, yes. See any model that is global, it is a complex function. Now locally in certain specific reasons what does it look like? If in a specific reason it is simpler than the ground truth there locally then it is having bias errors. On the other hand if in that local region it just it tends to over fit you'll have variance errors. So when you take across all the regions in the feature space you might end up with both bias and variance errors. You see how that's possible? Okay. I took that as a maximum bias and maximum variance errors. And I didn't think of a model that behaves differently in a different data set. Yes. It is interesting that these things can happen, isn't it? So one thing about machine learning though, this is very interesting. See, when we think of a model, people have the mentality that a model works. When they say it works, it seems to just globally work. But that's not the way you should think of a model. And a model is something that is making predictions all over the feature space. And in general, what will happen is, its performance will vary from region to region of the feature space. That is something to remember. If you say, I'll give you an example, if you make a straight line model and you say that the height of a child is proportional to the age, you would realize that when we are in the developmental phase, that would be something around up to the age of 10, 11, it would be a fairly accurate model, reasonably accurate model. But after that, the growth in height begins to taper off, plateau off, isn't it? Slowly it begins to not grow as fast. So your straight line model won't work beyond a certain age. But it did work in the, in up to the age of let's say 10 or 11. You see how that can be there for. Okay. But is it acceptable though, that the model works for a certain set of data very precisely and it doesn't work that well. In fact, it works worst for the remaining set of data. It is a very interesting word, acceptable. Remember the statement that all models are wrong. At the end of the day, the only thing that matters is from a business perspective, is your model serving the purpose? If it is an effective model, you live with it. Though you should be aware and many people are not, most people in fact are not aware that model has differential performance in different regions of the feature space. You're sensitive to that actually and you go and study it a little bit more carefully. Like where is your model failing? Where is it doing poorly? Quite often you can improve upon that. Thanks. Asif, I have an extended question on that. I'm not sure if it's a valid one, but you had explained that regularization is usually used when your model tends to overfit, which is like a high variance thing, right? but when you say a model exhibits both the behavior then what do you generally take it is it like a high bias or a high variance model and then how do you know like to improve upon that what needs to be done okay why don't i illustrate it with an example yeah so imagine that you have a feature space like this. And let me take an example of data. So let us say that your data is like this. For whatever reason, data happens to show behavior that is like this. And this model, let's say that it goes like this. And then it goes like this. And suppose you built a model and then it turns around like this. You see that the data is very strange, but this sort of data do happen in nature. So suppose you try to approximate this data. And let us say, let me try to imagine what sort of curve you'll make. Let us say that you make a curve that is like this. Okay, let me make a slightly different curve. Let us say that you make a curve that goes like this. And this one is, or maybe, let me do a little bit different to illustrate the point and have to be a little bit more fishy and let's see if I can at a situation and then hi and this data let us say again turns like this the data turns like this and let us say that for whatever reason your model becomes like this sorry let me use a yellow line your model goes like sorry like this so this is your model now observe this region of the feature space what do you see if you are just looking at this region what can you say about your model with respect to the data what is it doing not It's simple, not a good fit, underfitting. It is having, it's underfitting the data in this region. Maybe I should use the same color. In this region, it is underfitting the model, right under fitting so high bias so this area contributes to high bias error what about this area Yeah, here you have overfitting. And so what will this contribute high And so your model. Let me just make it a little bit thicker to illustrate the point. So now I seem to have done it created a really a model that seems to have both the issues let me make it more bold so you said this model your squiggly model Both high variance and high bias. Yeah, both high variance and high bias errors. Isn't it? So what happens is that in different local regions of the feature space, this model is failing or underperforming in different ways. Yeah. That is the point. And this is a point to remember, guys, that whenever you build a model, if you are careful, observe how it does in local regions of the feature space. This is very, what I'm saying is very common sense. Let's say that you tell, you're looking at children and you're predicting whether they have diabetes or not if you're looking for diabetes in children assuming and you're looking for type 2 diabetes not the type 1 diabetes you know that or maybe type 2 or type 1 either of the two i'm trying to concord an example and the doctors here, Patrick and Sokolpios, correct me if this example is wrong. So see in adults, if you look across the adult space, usually weight and level of activity and the food intake, calorie intake, they're they're good proxies for, I mean they're very good predictors of your diabetes, right? So you develop insulin resistance if you don't exercise enough. These are metabolic disorders. Children, by definition, have high metabolic, I mean, not by definition, by construction. It's just nature has built children in such a way. They're bumblebees, right? they keep buzzing around and so forth so they naturally have a high level of activity they eat a lot right but they burn it immediately and so if you're looking for diabetes in children you would look for other markers you know other predictors you would not you would look more at the genetic factor that would be a far stronger predictor of diabetes in children whereas in people over 40 then the lifestyle factors are significant contributors to diabetes the physicians amongst us would you agree with that statement is that broadly correct to us would you agree with that statement is that broadly correct yes sir yes yeah so so i have so this kind of scenario you explained it can happen only if we like uh we are not careful and mix two three different type of data sets into one and then model is really confused right right so And then model is really confused. Right, right. So Sukpal, I wouldn't say that we're not careful. What you have to always that models work well in local regions because their local effects always present. So in this, on the time axis, their local effects, children, the dynamics at younger age is different than the dynamics at younger age is different than the dynamics at middle age and is different for dynamics in late age in geriatrics so because the dynamics are different a model has to adapt to different dynamics in other words it has to literally be different models in different parts of the feature space because when data comes to you so imagine a nervous comes and says hey you know I have this island and I have a data of 35 million people that's a huge island and of this data tell me I want a good prediction model for diabetes one of the columns is age so then you have to you don't know a priori that age is a significant factor you build a model and then you go back and look that if you are not careful that model will work well for one age group but not for the other age group so then you have to go and make connections to it yeah so would ensemble method could possibly eliminate the the bias in this same model where you have variance and bias also? Many ways. So remember almost all the algorithms we learned in machine learning 200, ML 200 workshop are applicable here. Ensembles, support vector machines, non-linear. So in other words, you often have non-linearity of relationships. And whenever you say that there are local effects in place and that globally is not, global doesn't reflect the local, but you are basically saying there are non-linear effects. And so you need models that are non-linear, which is where all of these things, ensemble methods, kernel methods, and neural networks, they shine. They're able to adapt to different parts of the feature space. But the danger is they may adapt well or they may adapt poorly, depending upon how the training went and how the data was, and the differential biases in the training data and so forth. My question is like how can we use multiple models in the same model generation? It is always like that. So you take the example, Hari-Priya, of ensembles. What are you doing? When you do ensembles, you are building lots and lots of models. That's a decision trees for random forest, right? So what is happening is different decision trees will work well in different areas of the feature space. Isn't it? And you're aggregating the result. Yes. And that's where it works. Likewise with gradient boosting and so forth. Yeah. So I use just sets up because you know that in children is a different factors in adults is a different factor. You so yeah, we are trying to be cannot generalize a model that is for sure so uh is it like we know uh as i said like we are mixing two different kind of data sets into one so we can solve with this the domain expertise can we like we know that we should you should and which is why you should always have domain expertise see um there is a hubris amongst engineers that they are smarter than everybody else. There is absolutely no scientific basis for it. I suppose if you ask people in any field, they will believe that they are quite smart. They're smarter than people in other fields. But engineers in particular are infested with this sort of a delusion. So it is a common delusion amongst engineers to believe that you can ignore the domain experts, you can go into a field, you can take their data and you can build because you have access to such powerful models, you can uniformly build models which are far more effective than whatever these domain experts and their theories have been doing for the last 200 years. And what happens is that you know whenever you have a belief like that, a delusion like that, there will always be in real life you can always pick selective evidence. Every once in a while there will be a situation where in some domain researchers were trying something and suddenly you bring in this powerful machine learning algorithms and there is a breakthrough immediately right I'll give you one example we and this is again a medical example see the octodon I believe that's the word you know the the picture of the of your eye the cornea and I that the up that ophthalmologists take. They were used to observe defects in the eye. Then we fed that into deep neural networks. In fact, I'm hoping that we can do a lab similar to that. We feed it into that and suddenly it turned out that it can detect heart disease, it can detect diabetes, it can defect an assortment of diseases. So our eyes are actually in some sense a record of all that is happening, a window into all that is happening in the human body. And it was a spectacular success of the application of machine learning algorithms because none of the physicians had suspected that this would be so or rather i mean just people when they applied it they thought something may come of it and it did and like all scientific breakthroughs it's a surprise right but it doesn't mean therefore that we get rid of physicians and we replace them with just algorithms there are people who are writing books like that. I once read a book by a guy who was gushing about it, that how the machine learning will make medical science obsolete. Obviously, it shows an ignorance of medical science. Human body is so very intricate. Anyone of you who has studied molecular genetics and so forth, or just open the book, The Cell. You know, one of the things that I like to have or read is this lovely, lovely masterpiece book called The Cell, which is in the nth edition at this particular moment. And you read one chapter of that book and it's humbling. You realize how tremendously complex the chemical pathways and the mechanisms of the human body is. So that is one aspect. Listen to the domain experts because a collaboration of the domain experts and machine learning is the perfect combination. They will guide you. They'll take you where to go, what is correct and what is not correct. So that's how I would say. I mean, to get it. Go ahead. I think for this model that you that you drew, if you follow what Sukpa was saying about diabetes, this might look like the first half might be the juvenile population and then the latter half would be the adult. And then you're forcing, let's say, the juvenile population, and then the latter half would be the adult. And then you're forcing, let's say the juvenile population model on the adult. That's why it's behaving badly. So even with the main experts, sometimes we can also realize that they're forcing their knowledge into the model. So it might also be good to have machine learning to break down some conventions. Yes. You make a very good point actually. So diabetes in particular is a case in point. See, people have had so many generations of theories on diabetes and you're probably familiar, I mean obviously I'm not up to the latest, but at one point researchers sat down and said you know what, maybe everything we know about diabetes is wrong. And so they created the so-called minimal model. You may be familiar with that. They said what is the minimal set of things that we are more or less sure is there and let us build a model based on those things. And let us build a model based on those things. So you're right. Every field gets infested with fashionable theories or things that may not be informed by the underlying reality. In my field, I come from high energy theoretical particle physics, so high energy physics theory. There is a very fashionable theory called the super string theory we it's lovely is fascinating it will just absolutely make you fall in love with it the trouble is there is not a not an iota of evidence that there is any truth in the theory it is just a beautiful theory it might for all you care it might be one of the lovely stories from Arabian Nights. Very captivating, but essentially untrue. We don't know. We hope it is true. For years, people have been pursuing data and trying to prove it's true, but so far, no evidence. So at the end of it, now if you look at today, the practical reality is you won't get a faculty position unless you are in favor of superstring theory. And that biases because if in 20 years it turns out that this whole thing was a misadventure, we would have lost 30, 40 years of scientific investigation, wasted scientific investigation. Go ahead. scientific investigation. Go ahead. I'm saying that it must be explaining maybe a little bit, that's why they are behind the superstructure. Yes, you know the trouble is not that it explains a little bit. The trouble is that it explains anything. So you come up with one data, it will say yes, yes, I can, my theory explains this. Now you come up with the opposite data and it will still explain it. So no matter what data the opposite data and it will still explain it. So no matter what data you bring it, it will fit to it. In other words, in our language of machine learning, we would say that the model is so flexible, it has so many degrees of freedom, it just can fit to any data that you bring and claim that it predicts that phenomenon. And so the word that people use is string theories, if you are in this thing, not one theory, but many. So how many? There are actually infinitely many string theories. So any one of them will certainly fit your data. Yeah, go ahead. Sir, I understand the diabetes example. If I can get one more good example like where i'm thinking maybe financial data is can represent this problem as well absolutely so since you brought it up you can pick a domain and i'll give you an example see what happens guys is that they're all of these investment houses right and they will all get this super traders or super investors who somehow know the secret to making money. Isn't it? And what happens is suppose, in their language, there will come a time when these guys will suddenly start doing terribly. Right. And then, you know, all the fraud happens, you know, they will steal money. They'll try to hide the fact that they're underperforming and so forth. And banks have gone under. Some of the most prestigious banks have gone under because of this activity amongst the traders. Excessive faith in the magical powers of some traders. And they will have all sorts of theories about it. They will give you a mathematical reasoning of why they are doing well. And companies, they will believe that and they will invest in that and then make a lot of money in the short term and then suddenly lose the money. If you think about what happened, what happens is in a financial market for certain periods of time, certain economic conditions are true. Macroeconomic conditions happen and certain factors are true. The climate or the milieu supports certain things. So you can build a model, local model in time, which represents growth within those parameters. You don't know that you are building a model. So for example, you can make a hunch that this bunch of companies or this basket is just the right index to invest in and they'll continue to grow no matter what they do continue to grow like these days people have a faith in Fang right the Fang companies if you look at the growth of the Fang companies and compare that to normal Nasdaq one seems to have a far far greater growth than the usual index S&P index or the Nasdaq so people then can start excessively investing in that but till the conditions that cause these things to grow out outsize growth compared to the market those conditions disappear and then suddenly but the fate of the people remains investors remains so their model says that keep on investing the more you invest in these the richer you become and then one fine day it is no more true and that is how fortunes are lost a classic historic example is the investment in the railroad companies in the United States long ago when the railroads were being built. For a while the United States in the heydays when there were no railroad parts, it was a good investment to invest in the railroad companies because they were laying railroads everywhere. And then I don't know what the real forces was because certain dominant players came about and squashed others. So whatever it was, I don't remember. I don't actually not actually not not remember i've never really picked up the history of that but for some reason it was a terrible investment to invest in railroad companies of the of the us and a lot of people a lot of historical examples like this so i have one question uh so how do you know in which case you will need multiple models to fit or in which case you know you can fit it with a single model very good question see the answer to that is see it is again the the old thing that all models are wrong and some are effective when you build a model you let the domain experts speak is it informative is it it useful? If they say it is useful, you might have other things to do. You may go away. On the other hand, if they come and inform you that it works partially here, but not there, you know that you need something more complex than what you have built. So it is always the domain experts who speak. On the other hand, you could take the position that you'll take a very complex model, but then you'll have overfitting because while when we are learning, it seems that data is plentifully available. In real life, in most situations, even in this world of plenty, quite often you don't have enough data right so if you take an excessively complex model you might not you might not end up with a good situation okay thank you so that is the history of this a prediction model can exhibit both high bias and high variance errors we got the background it's true actually Next question is the figure below shows a unit circle of some arbitrary Minkowski norm. Which of these norms is this most likely to represent? So this is directly from our notes, you know, from the session that we gave. In other words, is the Euclidean distance as the bird flies. Isn't it? So it does not need any further discussion unless somebody has a question. Now, this again should be something that I hope all of you answered right. Deep learning architectures are insanely smart. They do not need to benefit from regularization now in fact they need much regularization much more than other things because they are so hugely complex and therefore have the tendency to overfit the Lasser regularization for Pauline for a polynomial regression of degree p. Well, this part, degree p polynomial, was actually sort of relevant because it tells you where the summation will go up to. So it is, which of these? Remember, it's the Manhattan distance. So you have to take the term which has the Manhattan distance, the absolute value, right? As the taxi driver drives. Or nowadays the Uber and the Lyft driver drives. Regularization is the process intended for the suppression of bias errors in a prediction model. So why is this false? Because regularization is Overfitting. false because regularization is over fitting. Next question, dropout leads to regularization by forcing a neural network to identify features that are robust in the presence of random perturbations of the network. That's what it is. So when you drop out some of the nodes, what are you doing? You're basically continuously tilting your loss surface, right? Fluctuating your loss surface. So what will happen is it will prevent, it will sort of mitigate the tendency to fit to noise. So another way to look at dropout is in a dropout you're creating an ensemble of an exponentially large number of neural networks, each created by taking a thinning of the original network by dropping some of the nodes in the original network. And so you have ensemble effect. What does ensemble effects do? The aggregation effect leads to the suppression of overfitting. Next is neural networks with dropout require many more iterations to converge. Why is that true? You need many more because this has introduced noise into the system. The errors that you get because this has introduced noise into the system, the errors that you get now are different. Another way to think of it is that in any one iteration, if you have, let's say the dropout creates 50 different neural networks, as a hypothetical example, any one batch is training only one of them, right? So you need a lot of batches to train all of those 50 neural networks, a lot of sort of iterations to train all the 50 instances and then you are assembling over them. That's sort of a rough way of thinking about it. Very rough way, not strictly true, but like, roughly. Which of the following are valid examples of data augmentation technique? So here, augmentation technique. So here, one has to just think whether you lose information or not. So when you create another image by performing random translations of an image, what does it mean? Let's say that you have a picture of a cat in one corner of the image and you gradually move it to the center or you move it to the side or move it up it's still a cat right the information is preserved and therefore translation is a perfectly legitimate way of data augmentation it's a cheap and effective way of doing data augmentation another is you create Gaussian noise so what happens is you take a pristine picture of a cat and you just introduce enough noise but not too much such that the essence of the cat stays. That helps you get more data. Rotate the cat. That is again a cat. A rotated cat is a cat. You zoom in and out. Zoom out is easy, cat will remain a cat, but don't zoom out so much that it becomes a point and very little information remains. Zoom in also, you have to be careful. You can't zoom in so much that all you see is a little bit of a hair on the cat. So, so long as you preserve the salient features in the image, the information content is the magic word, guys. Think about every situation of data augmentation and ask, does it preserve information or the salient features? Then it is a valid data augmentation technique. technique. Now this one, create another image by making one dimensional, making it one dimensional by averaging the pixels of the original image along one of the axis. So you're marginalizing the image. So suppose you're doing it, squashing it onto the horizontal axis means at any given value of X, all the Y pixels that are there, let's say that you have a 32 by 32 image that is 32 pixel by 32. So along the x axis for every one of the 32 positions, you will have now 32 y values, right? Now what do you do? You just go and average it. Well, doing so is not, you're losing information quite a bit. And it's quite likely that that data augmentation technique will not work for you. Asif? Yes. So here my understanding was what if you have an image that's 32 by 32 by 3 because it's RGB and then you just make it grayscale so I thought squishing it to dimension. That is okay but here I'd use a different example that you can do because that's a gray scaling of it and it's a good point So I thought squishing it to one dimension. That is okay. But here I'd use a different example. That you can do because that's a grayscaling of it. And it's a good point you raised. But see, notice the way I framed the question. By making it one dimensional, by averaging pixels from one of the axes, right? You will end up with a two-dimensional image. When you remove the color axis, RGB axis, and you gray scale it, you get, you average it, you get a two dimensional image, not a one dimensional image. Okay, I get it, thank you. So there was a subtlety. And it's good that you asked this question. I hope that somebody would ask me this. Create another image of the same size as the original by replacing all the pixels with randomly generated values. This, as you would imagine, would be literally removing all information content from the image so it cannot be true. Create another image by making random pairwise pixel exchanges. No, if you make random pairwise pixel exchanges, you will end up with essentially loss of the critical structural information. Like the ear is next to the nose. You don't want to lose that fact. You don't want to plant the ear on the tail right to give a give a rough idea right and and obviously it's not here is many many pixels but when you do random exchanges of the pixels you you basically lose a most of it create another image by this one's a little bit thought-provoking and this you would do if you tried out all the neural style transfer exercises that we had in week one so you know if you take an image and you change the domain so you know this you take a landscape of a forest during fall season and you change it to the you ask the neural network to change it to a picture in the winter season or the summer season today we have neural networks that can do that. And so you have created a rather advanced form of data augmentation. It is a sort of data augmentation people sort of overlook. It's very effective actually. You should use it. Another is you take the image and you do a neural style transfer. Don't be too aggressive, but do a little bit of it. So you take a cat and you make it a cat painted by Monet. Right, so now it's a bit more impressionistic cat. But it's still a cat. And so you it is fine because you have taken the style of new Monet and imposed it upon the cat. So it's a valid one. Creating another image by cropping the image which retains some of the distinguishing features perfectly fine so so you don't need the whole cat as you would agree most of us recognize a cat by its face right just face is enough so if you can preserve the salient features it's enough create another image by playing with the brightness and contrast right and etc so this refers patrick to your gray scaling of it and so forth. But if you flip an image laterally what happens? The cat was looking left now it's looking right it's still a cat or upside down horizontally. Create another image by applying a shear transform. A shear transform is a deformation in which you take a rectangle and you make it a parallelogram, right? You deform it into a parallelogram. As you can imagine, the cat will look funny, right? And but it's still a cat. It will still have its whiskers and eyes and nose and and the tail. So that's it. So this was a thought-provoking one, so I gave a lot of points for that, four points. And again, you could have answered this only if you did all the style transfer, neural style transfer exercises, homework in lab one, the weak one, sorry. Next, when you apply the dropout regularization method, we are in effect aggregating over an exponentially large ensemble of neural networks, each derived by thinning or randomly turning off some proportion of the node. So this is just thought-provoking. See, imagine that for every mini batch as it goes through every iteration, you turn off certain nodes. So what do you get? You get a different neural network. So how many neural networks can you have? Well, any one node could be either turned on or off. So suppose the machine has N nodes, the network has N, capital N nodes, you will have two to the power n possible neural networks, isn't it? Because any one node could be on an on-off state. And that is interesting because that's an exponentially large number of neural networks. And that is the power of dropout actually. Geoffrey Hinton, when he did this work with dropout, he did a lot of experiments to show its effectiveness and it does work it has an ensembling effect when the program runs uh with dropouts enabled does it still run the original one without dropouts does it no what happens is you train it with dropout. Vaidhyanathan Ramamurthy, You train it with an inference. Like, remember that you turn all the notes back on and you just kill the output signal a little bit. So suppose it is 30% dropout. Vaidhyanathan Ramamurthy, Was there. So then before you feed it into the next layer you sort of muted down by 30% otherwise the signal, you'll get amplified because all the notes, it's like all the bulbs are turned on. You need to mute it a little bit. But other than that, at inference time, at prediction time, of course, you keep all the bulbs on, all the notes are activated. Okay, I get it. Thank you. Next question, which of the following optimization techniques leads to regularization by perturbing the last surface at each step and preventing a stop at shallow minima? So the answer to that obviously was, of these, a batch gradient descent won't do it because it's the same data set. The L1 and L2 regularization do it differently. They set up a geometric perimeter around the origin and say the weights cannot explode. But in both these situations, you can get trapped into local minimas. But mini batch and gradient, and stochastic is just the limiting case of mini batch where mini batch size is one. The reason it does is that each mini batch is a different batch of data that you're trying to learn from. And it draws the loss surface differently. It perturbs the loss surface. Perturbation means small changes. The word perturbation means small changes. So it does that. Any questions, guys, on this one next regularization actually I must say one thing thank you for asking questions I often feel that many things that have been in my mind is sort of in a subconsciously there by speaking it aloud it helps me in my own understanding you know just saying it aloud thinking it through it reminds me in my own understanding you know just saying it aloud thinking it through it reminds me about things otherwise you tend to forget so thank you for asking those questions and when you ask questions you folks are my teacher regularization is a powerful tool that we should use aggressively in model building and set the hyperparameter lambda to a high value. So obviously when we talk about hyperparameter lambda here we mean the lasso is one of those regressions the L1, L2 or elastic net the combination of these two. No, if you take the lambda parameters too high you will end up with dimensionality reduction and you'll end up with a underfit model right you will you'll you'll have pretty poor performance very high bias errors will kick in variance errors will disappear but high biases will kick in for bias and variance errors in a model which of the following is true ultimately you want a model with low bias and low variance, isn't it? That's the perfect model. Go ahead. Sir, here if you have a KNN, but a KNN with one set to one neighbor, can't you have a low bias and high variance with good accuracy? No, not really. See, we did this example in ML 200, right? When you said k is equal to one, you risk, let's take a practical example. These days we are looking at elections, isn't it? So if I were to ask you, try to make a predict whether you'll vote for Biden or for Trump and if I take ten of your neighbors it will probably say that you'll vote for Biden if on the other hand I happen to pick your closest neighbor who who happens to be a strong believer in conservative values, but then that person votes for Trump, let's say, then, and I make a prediction that that's how you'll vote, you realize that that would be inaccurate, isn't it? And so, unless of course, I don't know which way you would, and you don't have to tell me, but if I have to, let us presume that you'll vote as most Californians do for the Democratic side so or the Biden then assuming that were true if I take 10 neighbors you know that most of them would be voting for Biden and it would probably be give you the correct answer more more often than if I take the immediate neighbor because the immediate neighbor will introduce local errors you know that singularity that anomalous person sitting there or the guy exceptional person who happens to be going for Trump he will introduce errors local errors in all his neighbors in the prediction with for all his neighbors you see that right in the prediction width for all his neighbors. You see that, right? Okay. So it's more of explaining the actual behavior rather than per instance where you apply. Yes. So remember that in KNN, the K is equal to one represents a situation of extremely high variance bias. K is equal to huge represents a situation of high variance bias. K is equal to huge represents a situation of high bias errors. K is equal to one is high variance errors. That's that. All right. Moving forward, try to capture a nonlinear decision boundary with, with, you know,-linear decision boundary with logistic regression algorithm can it do that so the answer to that is there are two ways that you can do it it is something that people traditionally think no you can't do actually i have a lot of uh you know you end up interviewing a lot of people and in the process you get lots of very interesting characters who come and interview with you. In this field, a lot of people of them, about a third of them will have PhDs. And I have seen them stumble and not give the right answer. They'll say, oh, the moment you see non-linearity in data, you can't use logistic regression. It's not like that. You can use it, but you have to introduce non-linearity yourself by two means either you expand the feature space into polynomials remember we did this lab in ml200 so it should be reminiscent of that or you apply kernel you kernelize the space you take you go to a higher dimensional space through a kernel transformation kernel mapping or where the data becomes linearly separable so you linearize the space through a process of kernelization right so those are two lovely ways that we can still use logistic regression what is the advantage of using logistic regression with polynomials, simple polynomials, the advantage is that you still have a very interpretable model, isn't it? Next question. Imagine that your city streets are laid out on a greater perpendicular lines than Manhattan distance between two arbitrary intersections X and Y. Yes, X and prime is the shortest path that a taxi driver can take in going between the two intersections. That is literally the definition of L1 norm, and that's why it's called the Manhattan distance, because Manhattan is laid on a grid. I would imagine that all of you got this one right. Now, large hyperparameter lambda leads to, yeah, the two fundamental risks are the risk of underfitting the model and the risk of excessive dimensionality reduction. So remember guys, there is no silver bullet in machine learning in AI. There are too many hyper parameters and you can't just make a rule of thumb that I'll keep lambda large and do that. Everything comes with demerits, you know, pros and cons. So now this is about text. And once again, I didn't actually discuss this in the class, but it was for you to ask the question from the perspective of what retains broadly most of the information content of a text. Think of it like that. So you create another text by substituting some words with similar or nearby words in the embedding space. Jai Madhyanathan Ramakrishnan:"So for example, if I say the king ruled over the land." replace it with the queen rule over the land? King and queen are very similar concepts and in the word space they're very near each other. Would you agree that those two, it sort of is a form of data augmentation. So suppose the statement was the king ruled over the land with great administrative acumen or something like that, efficiency, and you replace it with the queen. Well, in a very real way, that might probably be true because most kings rule over the land and the queens rule over the kings, I suppose. Create another text by randomly deleting a few of the words. Again, if you just delete a little bit of information, a few of the words. Again, if you just delete a little bit of information, a few of the words randomly, not strategically go and delete all the important words, but randomly if you delete, the information content would still be there, broadly speaking. So you would say that this is reasonable data augmentation strategy. Create another text by adding next to some words in a sentence, other corresponding words that are similar in the word embedding. So example of this would be, quick, what was it? The quick brown fox jumped over the something, something, or jumped over quickly or over the fence. What is that statement? the quick brown fox jumped over the lazy dog right quick and so suppose you say quick sly fox the information content is there and sly is often associated with Fox so you have created another bit of data to train your model without significantly risking loss of information. So that is what is meant. Create another text by identifying and erasing all the nouns. So suppose, let's take this sentence that the quick brown fox jumped over the lazy dog. So I remove the fox and the dog. Would you agree that the sentence may lose a lot of the value? So that is that. Nouns are rather significant. They are pillars holding the architecture of the sentence together quite often. Create another text by replacing it with a randomly generated text of the same length. Well, the sentence is very poorly worded. It should be create another text by replacing by replacing the original text with a randomly generated text of the same length. Of course, if you take random noise, it is useless, isn't it? It is equivalent to the case of images where we replace every pixel with random pixels. You get garbage. That is literally the definition of removing all information from the data. Next is create another text by injecting noise as additional random characters. So this think of it as typos. You know, when you write sentences on the keyboard, quite often you introduce some extra characters here and there, and then that is all right. Much of it is still understandable the information content is retained that's what you're doing another is so this is a valid one create another text by substituting each word with some random word from the same language that would be nonsense isn't it if if I said the quick brown fox and I replaced the first with omega and the quick truck, the like omega truck, horse, a house or something like that, you realize that the sentence has no information of the original left behind. So that cannot be the right answer. So create another text by randomly replacing some of the characters with other characters at fixed distance away on the QWERT keyboard. So what happens is, suppose you replace a character. And this often happens. When we type on the keyboard, most of us don't look down on the keyboard. We all get used to typing without looking at the screen. But if a finger just happens to not be in the right place there'll be an offset and so some characters will come out wrong and we'll catch it and then we'll fix it isn't it so do you that sort of explains i don't know if you have ever noticed that on your keyboard the letter f and the letter j they have a ridge on them yeah and the reason they have a ridge is when you're not looking at the keyboard you're looking on the screen you need you need a way to or like you know orient your hand your fingers and so you you can now put your index fingers on the two characters and you know that you don't have offset errors but okay so small offset errors are still going to retain the content so it is a valid way of introducing a new data by making those in quotes mistakes so I see there's a little nuance that's happened in the example that he gave right the quick brown fox jumped over the lazy dog they were a collection of those sentences that I recall when I was learning typing, right? The peculiarity of each of those sentences is between the one period to the next period, that sentence essentially has all the letters of the English alphabet. So there's semantics of the sentence. In addition to that, there's a peculiarity which is preserved. So a text that contains multiple sentences where each of those sentences have all the 26 alphabets. Yes. When you introduce a perturbation in it and you break that 26 alphabet requirement, you kind of created a text that is not the same as the original. It is, yes. That is a very good point that this happens to be a unique case where one of the information content is all the character, right? And yes, it is true that if you're doing character level discovery, it would affect you. And yes, that nuance is true. Generally though, in NLP, character based discoveries are relatively rare. Most of the time you do word and word pieces. Semantics matters. Semantics matters. There's one question. So here it has not been specified exactly what text translations are we doing. So I'm just, so my question is, will it work for like you know some different semantics for the text like positive negative or free I have to identify like what kind of text is it is it a sarcasm or like you know sentimental analysis of the text will it work for the same options yes and so it is a case-by-case see when you do data augmentation but remember there are two things trade-offs one is you have more data to prevent overfitting at the same time you also have introduced noise into the system remember the original text is still there you now have replaced you have created another text and if it so happens that that data point removes the crucial word so sarcasm is actually the sentiment analysis do terribly at but let's say that the sentiment analysis is trying to look at and the word hate is there and intently you replace the word hate with something else suppose you did one random character change hate becomes mate Now the sentiment has changed from negative to neutral. So there is a risk of that. So you might, so see all of these techniques, nothing comes free. The only thing, the only gold standard is go to the actual field and gather as much data as you can. And usually what it means in real term is, this is the job actually of the product managers and the business people. You have to push the client to give as much, you have to cajole them, you have to tell them that the more data you give is enough. And never say we have enough data. In general general that's one thing I've learned I always say that you know a more data would be really useful but you know what at least with this will start because if you ever say that this data is all enough they'll get lazy and never give you any more data it's just practical experience so get more and more data from your client because the more data you have, the better model you can make them and ultimately you're serving your client. So that's that. See what happens is that decisions to use AI are made in the boardrooms by the executives. But the data is provided to you by somebody in the basement as the joke is. Some engineering resource, lowly engineering resource, who finds it rather boring to do this data extraction for you. So he's not terribly motivated to think deeply and be motivated about creating the best data sets for you. Sometimes they are highly. Sometimes you find very talented people at that level. But quite often you'll find people who just want to go home, finish the work and go home. So they'll do only what they are told and no more. So you have to keep pushing the client so that somebody senior up, some architect is paying attention to how much more data can we extract so because he's been sensitized to the fact that there isn't enough data to do a good prediction it helps therefore see one more question on extending what Prachi asked right so in terms of data augmentation is there a notion of if I have to pick another word and substitute if without breaking the sentiment of the passage is there a notion of a library from where you can go pick those essentially you need synonyms in that synonyms yeah you do synonyms see amongst the many data augmentation techniques right don't do random word replacements always pick synonyms don't tend to perform that well they do work well because synonym or pick more better in the embedding space pick a nearby word that will often be the synonym but it will be a generalization of the concept of synonym okay so pick something there and we'll talk about embeddings very very soon when we move here so the reason i want to do activations today is i want to keep the next week open for images and the third week for text and the foot and the last week for the graph neural networks okay so there is a small little interesting thing i want to tell you about the, you know, we are talking about brown folks. So, you know, Indian prominent personality Shashi Tharoor, right? Yes. So he came up with the new line, so pack my box with five dozen liquid jugs. This sentence is smaller than the other one. Then he came up with one more sentence that had only 30 sentences. So it's a little interesting thing to read about. He's a language genius. Very, very genius person. Then other sentence was, how quickly daft jumping jabberwock wax. So he had only 30 seconds. Could you please post it to Slack? I'd like to read it. Very interesting story. Yeah. And then other people, they think, yeah, you don't have anything else to do. But very, very genius guys. Very nice. Yes. I'll remind you of something in my graduate PhD years. Under the door, some graduate student had written, because you know, good research is when you don't know what you're doing. If you know what you're doing, you're not doing good research. You're doing incremental work. Good thing is you're venturing into the unknown and you have no idea what you'll find so this is this graduate student had this on his door he used to always say your tax dollar set play and some of the students who came by they they would actually take offense. They would say, you know, you guys, all you do, like we work hard, you know, the undergraduates would say, we work hard and you give us C grade and there you are just playing. So, all right. So other words are pretty straightforward. Early stopping is aimed at what? It's a regularization technique. We talked about it. So I hope none of the other misleading answers clear you off. It has nothing to do with memory and CPU or any of those things. It's a regularization technique. Geometrically in laser regression, we do a constrained optimization where we search for the minimum total error within the regions of a defined circle of the relevant norm centered at the origin. Isn't it? This is straight from our notes. So enough said. This is the diamond. This reminds you of what? The Manhattan distance. So there it is. Easy one. Elastic net is just a combination of the two. By the way, the L1 and L2, there are other names for it. People call the L1 also, not just lasso, L1 lasso. Another word that you'll hear, especially in the Bayesian community is the Laplacian regularization, Laplace regularization. And the L2 is the Gaussian regularization because it has to do with something called the prior. The prior is the prior belief. The Bayesians say that you'll never know the answer but what you can do is you can have one prior belief and then data will change your belief to a posterior belief. An example is you land at night in a city. You don't know what the weather in the city is. Is it a desert? Is it a rainy place? So let's say that you hypothesize that it's a desert. It rains very infrequently. The next day it rains. Oh boy. You may think, well, that's a coincidence. Well, the next day it again rains. Well, now your belief better change, isn't it? And then the third day doesn't rain, but the fourth date again rains. So as the data accumulates, if you are smart, you will learn from the observation and your belief will change. It may go that probably it's not a desert. It's more likely to be a rainy place. You'll come to that. But you don't completely discard the belief that it may still be a desert it's more likely to be or any place you'll come to that but you don't completely discard the belief that it may still be a desert because it may be a desert and you just happen to be in the very coincidental uh situation where all the rains happened when you were there so that's that with data augmentation which of the following is true during training time? Again, this is just a number of steps in the epochs increase. Why do the number of steps in the epoch increase? Well, if your mini batch size is fixed and you have more data, you'll simply have more steps in the epoch, isn't it? The steps is the data size divided by the size of the mini batch. So if the data size increases, steps increase. It is likely to take longer to run an epoch of training. Why is it likely to take longer to run an epoch of training? Simply because there are more steps. Lasso regularization is superior to ridge regularization in making an accurate prediction model because actually it's not. Remember our no-freelance theorem. There's no such thing as one approach being superior to rich regularization in making an accurate prediction model because actually it's not. Remember our no freelance theorem, there's no such thing as one approach being superior to another always. Dropout is a potential loss function for regression in neural networks. It has nothing to do with loss function, it's a regularization technique. So that was quite literally misdirect. Minkowski is another distance is another name for Euclidean distance false. Minkowski is the generalization of the notion of distances, Manhattan and Euclidean. Then, by the way, guys, I tried to eschew mathematical notation but sometimes for clarity I do mention it are you guys finding it clear enough what I'm trying to say in these things or you think this gets confusing and heavy any feedback rotation is appreciated because that kind of standardizes the way we intake concepts. Okay, good. Just used to the math. Yes, nice, good. Geometrically, when we do not regularize a model, we are searching for, obviously, the absolute minimum error surface, irrespective of its location in the parameter space. So if your data is sparse and your model has enough parameters, what will happen is those parameters will become big. In other words, your minimum will be far from the origin. So that is the unconstrained minimization. Data augmentation is a regularization technique that works by augmenting the training data set with a small proportion of randomly generated data. No, you don't know randomly generated data, but you do selective data generation says that the information is preserved. The rich regularization loss function for a polynomial regression is this. So this is your sum squared error term. This is just your, you know, L2 term, the distance square, x square, what you would say the equation of a circle, x square plus y square is equal to some constant. So here in the parameter space, it is the beta squares. Right? And oh, I had a lot of questions in this one. Which of the following is true? Well, this is just deliberately confusing it by putting a mixture of all sorts of answers. You just have to know. It's just knowing that the L1, LASSO is L1 and RIG is L2. Compared to an identical network without dropout, an epoch of training a network will dropout will be quicker. Why will it be quicker? Because you have a thinned out model, you have a smaller model, isn't it? So both the forward and the backward passes, it will be faster, less computations. Yes. So the model is smaller because we trained nodes or or is it because the my understanding was the math. The time it takes to compete for the math would be the same. So I was wondering how does it become exponentially smaller. is that suppose your network had 100 nodes and your dropout factor is 30%. So what does that mean? At any given moment, when you're taking one mini batch through the network and training the network with that mini batch, the forward pass is a multiplication, but that multiplication is happening and activation is happening on only 70% of the nodes. So it will, the forward pass will run 70% quicker. Likewise, the backward pass will run 70% quicker. You're just doing 70% less, only 30% less updates, isn't it? Multiplications and updates of the weights. So it will run faster simply because you're doing less computation. But that is only for one mini batch. Now you run through the whole epoch, which is made up of n mini batches. Then it will all run faster. But there's a down, the other aspect to it. Generally, it takes more epochs to train a network with many batches, with dropouts. Why? Because now it is an ensemble of many networks, right? So you would, and any one network is getting trained maybe just a couple of times in an epoch. So you need a lot more epochs to train the, all of those conceptually, all of those thinned out neural networks that form the ensemble. Okay, thank you. Dropout regularization works by randomly ignoring a proportion of the many batches during each epoch of the training falls. It has nothing to do with many batches. It has to do with nodes. The dropout mechanism helps mitigate co-adaptation in neural network. We talked about that. By the way, the concept of co-adaptation comes from biology. And if I'm right, and maybe many of you will connect me here, people with bio, co-adaptation is the degradation in a gene pool when there's a lot of people as I suppose that's a reason why inbreeding leads to problems, more diseases and so forth. We need mutations to fix it or external things. So for program, a polynomial regression without regularization the model tends to overfit the data and we see that what do we see the minima is far from the origin it means that the parameter coefficients begin to blow up and that is it All right guys, that finishes our quiz for today. Asif, this is Harini here. Yes. With the lab session, you said we had to do a few lines to get it working right in Colab? You had to, I said that there is one example, toy example, but to make it with the river data set you'll have to change the code. Not that as if the whole neural network because of version problem or it's not working yeah so let's take that actually a little bit i've been talking now for an hour and a half okay can we do it tomorrow yes let's take or maybe today some point in time let me think no today i have again a long session right two three hours in the evening let's keep if you don't mind let's keep it for another day two three hours in the evening let's keep if you don't mind let's keep it for another day tomorrow okay thank you python general teaching uh debugging lab time would be great would be great so i can do that guys and any topic that enough number of people are interested i'd be happy to do um actually frankly i was a bit surprised i thought you guys needed one more python session but i'm happy to see that you guys are already running and don't need that we'll use the time for something else there are issues that we have in terms of running the labs so if you if you want to open up a slack channel where we can post those questions and then you consolidate that and we have a session that'll be wonderful too okay let's do that and in fact let's do that let's use the afternoon saturday the time that has opened up for clinic hours the only trouble is that um for some reason i see by friday i get very tired with office work so sat Saturday is my recoup day in some sense so I don't know for what that may be a reason it's just a hypothesis but for some reason even though we do this quiz for only hour and a half I tend to get a bit tired and need some rest so that's why I'm a little bit hesitant and doing too many things after that but if if you're not having the python session we'll just replace it with the clinic session every week how about that yeah that will help us let's do that yeah and the time you can choose when you are convenient and let us know yes let's do that and today evening try your best to attend guys it's a very important topic uh activation function i have to see we are moving quite by the way how do you like the pace of the class is anybody feeling we are going too slow no i think it's fine lots to learn i don't think so it's slow it's not slow right yes so um yeah it will pick up the pace if you notice is gradually dialing up. So, and so let me give you a bit of news. You did one quiz. This week I'm going to release two more quizzes right after the activation function session. I will release two more quizzes, one on activation functions and one on gradient descent. gradient descent so please do the quiz I believe that there is a lot of learning from the to be gained from the quizzes or at least that's my intention when I sit and craft those quizzes I try to do all of them and it is alright the first time not to get a good score because you know it tells you where you are strong and where you're weak so you go read it up and where you're strong and where you're weak. So you go read it up, and then again, come back and take the quiz and do better. It will prepare you all for your job interviews, and not only job interviews, in your workplace, in your conversations, and thinking through the matter when you're dealing with real life data. So. Last week's concepts were a little complicated to grok, right? So it'll take us, I mean, at least for me, I need some time to digest it. So that's basically what I do over the next few weeks. There is no one who can honestly say, no researcher in this field can honestly say that the back propagation they understood the first time. And then at some moment it clicks. So I would suggest that I will be doing the back propagation session again and again in the coming weeks. Attend the repeat sessions. At some point it clicks and then you wonder why it was so hard to understand. But everyone that I talked to say that the first time they encountered it, they sort of got the feeling that they are understanding it it but they don't have a handle on it they just get the rough idea so one suggestion asleep is you know I went through your session so we had your session on back propagation and regularization and then you recommended that put those three videos remember this yes yeah that actually helps a lot so if you go printed I suggest that go And then you recommended those three videos. Remember that? Yes. Yeah, that actually helps a lot. So if you go, I would suggest that go through those three videos. It's like 15 minutes video each, right? It's pretty good. And it clears a lot of concepts. Yes. I'm liking the book. The PyTorch book is pretty good. I haven't had the time to actually work through it. That's basically where I am. Yes. time to actually work through it that's basically where i am yes the book is pretty good also and it helps actually going through that after the class yeah thank you sencha for bringing it up so guys here's the thing in the portal right i put a lot of resources i don't just you know populate it with resources i know that your time is precious and i generally don't put a resource unless it is very high value to the learning process. It is a significant didactic value. I won't put it there. So please do go through it. I expect you to go through it because that's a follow up after the class. Sanjay, thank you. Nice to know Sanjay that you did that. Sanjay Gupta Yeah, it actually know what you thought the back propagation and if you just note down while that guy is explaining in the video it's very good I mean I like liked it nice I haven't seen that so thank you Sanjay I'll take a look today you guys please do that and can I just show you what I'm referring to? Yeah, it's a nice to see you, honestly. Yeah, so if you look at the website, let's go back to the website and to our class portal, you will notice that there is a wealth of material that are here. So everything, you know, for example, if you want to do interview preparation, there are tips for interview preparation I've added some basic things I'm adding here as Arctic tricks pandas by torch tricks performance tricks jokes and more there lot of resources some of you ask about the mathematics so this is a great place if you go and look into this textbook you will see back propagation are discussed here right and the three blue one brown channel i highly recommend it to you guys that's a good channel a very good channel and so now if you go to any topic that we teach you'll always find that it is there in the table of contents so let's go to the activation function i will teach that and after that if you guys can go through these resources it will help you right likewise loss function and gradient descent yeah they're not that many things but these are it these videos and i only put things that when you read you'll agree that they were high value in the learning process so do please go back to those sections in the portal and take out time to do that. So you guys are the end of it, right? It's the, you know, remember that 10,000 hour genius rule, outlier rule, 10,000 hours of deliberate practice, but you don't have to reach 10,000. By the time you reach 200 to 400, you're already doing doing very well but you need to put in those 400 hours into it right so let's say that let us take an extreme example suppose you are free completely and you are doing this course and very few of you are but suppose you're putting in the kind of energy that you would put in if you if you really felt that this is going to be a life now let's say you put in 10 hours of solid work into it you realize that in a month you have already accumulated 300 hours by the time you finish this workshop in six weeks you're already bubbling with confidence you know you have a sense that you're going somewhere. And by the time you finish it in January, and that is the hope that you guys will be absolutely, you know, everything on your fingertips when you're interviewing. Should be. One participant once told me that when he interviewed with Google, they grilled him on data structures, algos and so forth. But the moment it came to machine learning, they asked one or two questions and then they gave up. They said, no, no, we know that you know, we don't want to ask anymore. You want to be in that situation where the best of the best acknowledge that you are in their category. As you brought the topic of the data structure and algorithm, I know you took a class on that. You happen to have a notes or something that if you can share whatever without your IP protection, if you can share that. I will see that. I have to go back and look. See those were the times when I used to give impromptu class lectures and we weren't recording any of it. The video camera came only in January. I tried to set up. If you remember, those of you who took ML 100 with me, the first lecture of ML 100 this year year we did a video recording and it was an in-class lecture you remember that guys yeah but before that and that's when I had bought that very expensive professional grade video recorder mounted on the wall well so the time I gave the data structure cell goes class, there's no trace of it except in people's notebooks. If there are any notes or anything. I can give that class it depends if they can we do it again if there is time. Yes I create an interest list you know what my basic rule is the number of students in that have to be beyond a certain threshold. There should be at least two dozen students, otherwise it's not cost effective to give. But there are two dozen students, I'll be happy to give a six weeks workshop on that. By the way, those people benefited a lot. They all did very well in the interviews after that. But again, at that time, they used to say that I'm keeping them busy all day. Yeah, also, as if people who want to know about data structures, there is a book called Cracking the Coding Interview. Green Book. In fact, that's the one that is a reference in our course. We use that extensively. And then the other thing we do is after people have gone through each topic and so forth, we go to lead code and we make people do all the problems in that topic or as many of the problems in that topic as they can. can. Especially as if the lead code is very helpful, actually people who wants to really know to the data structure, right? So especially their discussion groups and it's really helpful. Yes. No, it is extremely useful. The question bank is excellent. So see, here's the feedback I'll give you Praveen from that batch. Were you there in that? You weren't there in that. So what they said is that i made data structures algorithms easy you know i explained to them how to think about it so they feel that in college everybody has been through data structures algos course they haven't learned to reason about it systematically but to know how to reason about a problem means any problem you can you have now an approach how will you solve it systematically so that was the feedback i remember getting in that the other feedback i got is i did push them hard that was that who wants to do this one back propagation right so there are a couple of on data science a couple of examples where they do a feed forward and back propagation so it's very helpful you can easily understand with the example than the math actually yes yeah by the way having said that just a basic question and honest feedback did you guys find my explanation very confusing or was it clear for back propagation okay it was fairly clear i see yes okay because if it was not very clear i can redo the session uh also i just wanted some uh real life examples to be explained along with that it was like more on the mathematical side which made the concept clear but then if one associated example with that i suppose that could really help uh that's what i think practically back propagation is the internal internals of the engine so you don't have real life examples of back props because it happens all the time but it happens in the inside as an engine is the learning part of it so the every problem that you solve with neural networks is a practical example of prob but I could create if I got time I'll create some visualizations that you can play with you can say you can see the weights in the update themselves in the backstage let me create some visualization there are a couple of examples if you I think just Google for back propagation example, there are quite a good simple example where you can really clearly understand how the weights change a section. So that's really helpful. Oh, yes. And that reminds me of something, guys. I need you to contribute to something. Do you notice that we are talking about gradient descent back propagation? In all these sections, do you see there's a wiki here? Yeah. I really appreciate if you find these resources like yours, Praveen. Would you mind adding it here? Yeah, sure, Rasa. I really appreciate it. Or if you feel that it's too much to add it here, just post it to the slack and i will add it here for you see we need to collectively find the best resources on the web that will benefit right as if to your question about was it clear when the lecture was clear right i think the the concept where i'm i'm looking at it is I'm kind of wrapping my head around something that I haven't done in a while, right? So I've now taken all these things, but I need to kind of detail them out, which I haven't done. I haven't watched the videos. I haven't read the book. So that's where I am on my journey. So in terms of your question, was the concept explained well? I think yes, but I need to go fill the little holes in the cheese yeah see I'll give you my own experience when I first learned backprop right in the class I got lost I remember I was young and I got lost and I was like yes it is a chain rule that you know you see the the symbols flying around and it all makes sense it all made sense to me. I could see that, yes, this is a chain rule and therefore you could compute this. But the whole thing still looked so dark and mysterious to me that how did it happen? And it took me, if I remember right, it took me a whole week actually, or more than a week, 10 days to puzzle over it. And finally, at some point, actually, what am I saying? 10 days, puzzle over it and finally you know at some point actually what am I saying 10 days that's an exaggeration it took me a month actually if I remember right before one day I was walking around you know and it suddenly clicked it just came to me and I realized oh okay now I got it. And a lot of people I've talked to, they say the same thing about backprop. You know, you can see the symbols and it all looks very consistent and everybody seems to be confidently saying, oh, it's just the chain rule, nothing. But then you have to wait for that click moment. Till that click moment, till that click moment happens, right? Everybody seems to have a click moment. Until that moment happens, you will never feel confident. And usually that click moment never happens in the class when you, where back propagation is taught. See, for every other concept that I will be teaching in this workshop, I'm hoping that the click moment happens when we are either in the class or when you're reviewing the video. But backdrop I must say that from my own experience I found it hard. And later on when it clicks you just wonder why was it so hard. It was straightforward. Asif, was it just like a click moment where it made sense to you words so there were Sarah like any particular confusions that you had you can I literally could visualize visualize the updates happening and the gradient moving back I like I used all sorts of wave, you know, physical intuitions. I thought of it as a domino effect. You know how the dominoes fall? And literally in my mind, I could see the fall of the dominoes. The gradient, first the gradient, you know, is the force that's moving back, a wave of the gradients moving back and then the um these things getting updated the weights changing so it is like you know the the forward pass and in the backward pass i could in my mind right i could literally see that first layer weights are updated now the second layer weights are updated and the third year, it all happens. But before any of that can happen, first the gradient of the activation must be found in the first layer, then the gradient of the activation in the second layer gradient of the loss with respect to the activation in the third layer and it keeps moving back and you realize that you could not have found the last gradient you realize that you could not have found the last gradient with respect to the activation in this layer till you had found it in this layer and that needed the layer ahead of it to be found all the way to the output layer. And then only successively it keeps coming back. And once the gradient of the activations are found, the gradient, then the gradient of the weight is in terms of the activation and then comes the delta rule, the update update rule right the gradient descent rule so see it is this is the three basic concepts i was saying first the gradient of of loss with respect to activation one layer's grade you know a delta l loss with respect to the delta activation of layer l depends on you having computed it is in Jay Shah, Dr. all the gradients. Now that you have computed all the gradients of the activation, then the gradient of the weights is in terms of the activation. And if you know the gradient of the weight with respect to the weight, well, that's all you need to do the gradient descent step. Because what is the delta room for gradient descent? W next is W minus alpha, the learning rate, times the gradient of loss with respect to the weight isn't it and so all the weights can now be updated in one fell swoop we just need to compute the gradients are back propagate the gradients and then in one single swoop you can go update all the weights you see it makes sense to you as a physicist to visualize everything yeah Yeah, you should always do that. Let me finish it. Everybody has an idiosyncratic way of looking at the world. Here is my world, and I'll justify it with a grain of truth. See, as a physicist, you believe that the world is made up of atoms, atomos as the Greeks used to say. Then you realize that atoms have substructure. So I worked in particle physics looking at all these substructures, the neutrons, protons, electrons, and then they have substructure, we think in terms of quarks or quarks or in terms of superstrings and so forth. So that's the way that a physicist would look at the world. And then I encountered a famous statement. I wish I remembered the author. It's a famous person who said that the world is not made up of atoms. The world is made up of stories, of narratives. And I remember it hitting me very hard because that is indeed true. Everything that we think we know is actually a narrative about that event. It's a narrative in our mind. So human beings learn through narratives. If you notice my teaching style, I try to weave the concepts into a narrative because it is my belief that we are all children who hunger for stories. I mean, there's a reason why all cultures, whether it's Middle Eastern or Indian or Chinese or it is, you know, African or European and the Greek culture based civilizations or the Nordics and so forth. civilisations, or the Nordics and so forth. Everywhere you look you find mythologies. The human mind needs the mythologies. It hungers for it, you know, to grow up. It's nourishing. It helps us grow up. But those mythologies are not as rubbish as people like to believe. There is not that much difference between mythologies and science. Science is its own set of narratives. That is much more rigorous though. More rigorous. And you can't just create a story out of the blue. It has to be founded and grounded with evidence and data. So you would say it's just data-driven narratives, all of science. So create narratives, guys, create narratives. That's one thing I've been telling you. Remember when we talked of the river dataset in ML200, it was just a dataset. River was the narrative that you imposed on the dataset to understand it and to grapple with it. So everywhere, whenever you're trying to understand a concept, make it very real in terms of trying to explain it to a kindergartner, whatever it is. How would you explain backpropagation to a kindergartner? If you think you can do that, you have understood it. If you cannot, walk away from it. Reminds me of an incident in the life of the great physicist, Richard P. Feynman, who is very much revered in theoretical physics. So he was giving a lecture at Caltech. It was on some topic I forget. I think it was. So he explained it to people. It was in quantum field theory, some topic, to the students. And he asked them, did you understand it? They said no. Then he explained again, and they didn't understand it so it went to knock up a couple of times and then he just stood still and he said you know what it means that i don't understand it and it's a very famous statement of his and then he walked away and i believe in a few months he came back with a completely elegant theory that reformulated the entire subject that and re-explained the whole topic in an elegant and beautiful way right so that is that is a benchmark of how it should be if you can't explain whatever you have learned to people to ordinary people to children to college kids you probably have not understood it so try to do that do this experiment by the way this is called the Feynman method of learning you learn something try to explain it to somebody find somebody give him that explanation freely just say i want to explain this to you can you just listen to me and tell me if you understood it right or as they say you know half the learning comes when you teach go teach somebody all right guys so with those words i'll stop the recording