 All right, we'll leave it at this. So this paper, let's get started, is actually quite an interesting paper. And thanks, Kate, for pointing us to it. It goes to something called evolutionary algorithms and the idea is very simple. In nature we believe that this is how evolution works. You have, so the blueprint of all life is in our DNA apparently, in our genetics, in our genes. So imagine a long sequence of genes genetics, in our genes. So imagine a long sequence of genes and that is a code of life in some sense. If you have two animals, if they have identical genome, then the presumption is that they would be exactly the same species. Now even within the same species, between two human beings, there are continuous mutations of the genes, something or the other, that keeps happening. So within our own lifetime, I believe the word is epigenetic or some such biological word. I'm not very familiar with it, but there are things continuously happening. happening and it is these mistakes that is somehow implicated in the evolution either during reproduction or during a lifetime so you pass it on so the basic idea is that if you take a population each of which with slightly different genes within a species and you take a habitat the habitat has certain stress response response you know stress in the in the habitat like for example there may be a drought there may be food scarcity there may be a predator out there so then as the genetic mutations keep happening, some mutations, some mistakes will be favorable. It will it will favor survival given the stress in the habitat. So, for example, if there's a drought and there is a gene that helps you drink less water, feel less thirsty, I suppose, or something to that effect, then some gene expression. So then that particular mutation, people with those mutations will tend to survive. And so there's this whole evolutionary theory of the survival of the fittest and so on and so forth. And much of the genetic algorithms are based on a similar idea. What you do is you take a very simple initial state and then you take a habitat. Habitat would be a framing a problem of reference in compute in this particular problem. They take MNIST as a particular habitat and then they will induce various stressors into it. But for now you take the habitat, the problem statement is, you have the MNIST dataset. And what you do is you start with a blank slate. You have three methods. There is obviously this method learn, set up and predict. If you remember, that's what we do in a typical machine learning code. What we do is we do some initializations and so on and so forth. After that, the forward pass is the predict and then the backward pass is the train of it, you know, the learn part of it. And so they allow for only three functions, but they leave the functions completely empty. They say, let's start with an empty set, right? It is as basic as it can get. And then what they do is they take a vocabulary, a fixed vocabulary of operations. It contains all sorts of basic mathematical primitives. And the idea is that this, when you start with an empty set and you start with a few of these, strange things can happen. The things that you can do is you can randomly, suppose that it was not empty, suppose there were a few lines of code already there in those methods, then you could randomly delete a line, or you could randomly sort of insert a line, or you could randomly change a line, right? So you could just insert some line of code just about anywhere. And then the idea is after these mutations, after these mutations have happened, what do you do? You try to see how well it does or adapts to the habitat, which is in this particular case solving the MNIST classification problem. So it's extracted from this, I believe they have made it into a simplified MNIST, a binary classification problem, and then they see how well these little mutations survive in the presence of it. And the best one that survives, they keep it. That becomes the parent. Now, given the parent, they clone the parent. So they keep the parent there, but they take a clone of it. And then they again make mutations into it. And once they make mutations into it, once again again they see how well it does against the other things that are there. Now, as you can imagine, this is computationally, you would imagine that where in the world will this all lead to? Will it actually be able to discover something useful? This seems like random mutations and the space of possible code that one can write to fill up those methods is close to infinite. But the evolutionary part that comes is that there is a controlling environment. You're forcing it to solve the MNIST problem and the suboptimal solutions you are throwing away. And you're only keeping the best of the breed in a sense. And as you keep the best of the breeds, you keep evolving as you keep the best of the breeds you keep evolving it and then there is one more additional sort of a trick they have of their sleeve which is that they do this in parallel because they are able to do some 10 000 operations per second i believe a per core of the cpu and these days on on massively parallel machines you have access to tens of thousands of course right for example if you remember in support vectors itself on a per machine we have 100 cores so if you imagine that you have 100 such machines dedicated nowadays it's even more actually nowadays it's quite common for a machine to have let's's say, 40 cores, 160 cores, 200 cores per machine. And so if you have 100 machines, right, a cluster of 100 machines, you're looking at tens of thousands of cores. And each core, if it is able to process 10,000 sort of operations or sort of mutations it can generate per second and you're testing it out but then you have a pretty large you know computational power now the counterpart to this computational power is this vast amount of computations that you have to do. The search space is ginormous, you know, it is extremely sparse search space. There'll be in that search space, just about anything is possible, but the good solutions are very, very few. It's a very sparse search space, sparse from the point of view of good solutions, meaningful solutions. And so it is a match between a ginormous problem of search and a guided search in some sense and computational power. You match the two and you try to create genetic algorithms or evolutionary algorithms that sort of adapt and learn and make progress. And what they have found is the end summary of what they find is is something remarkable that they seem to have in the very simple situation of amnesty binary amnesty they see. Aditya Karia Sinha, PhD.: What seems to be a retracing of all the ideas of deep learning being rediscovered you see that the system in the evolutionary algorithm essentially rediscovered. You see that the system, the evolutionary algorithm, essentially rediscovers back propagation, it rediscovers gradient descent, it rediscovers normalization and random initializations, it rediscovers a lot of the concepts that we use today in deep learning. It seems to have rediscovered it And it goes through a progression of ideas. You see it first discovering linear methods, a path of linear regression kind of approach. And from there it evolves and evolves. And ultimately it seems to come to a place at which it has a code that you wouldn't have anticipated a human being to do. But it is able to write a code that sort of works. Now, at this moment, it hasn't actually discovered any new algorithm. But the very fact that it sort of has reconstructed so much of our theory on its own, just through an evolutionary process, opens the door to a sort of,, opens the door to a lot of new research. So that is a gesture of what they're saying. And it falls into the topic of what we were doing this week, which was automated machine learning. And so let's see how it goes. When I read through this paper, first of all, it's very, very impressive that they got these results and you stay sort of overwhelmed with it for some time. The only thing that I felt was perhaps sort of stood as an inductive bias to the whole process was the vocabulary of operations. In some sense, you know that today's solutions are made out of these basic instructions. And so you're asking the computer to go search for algorithms built out of this sort of pieces. And sooner or later, after a lot of computational power, it seems to have discovered something along the lines of what we have. So that is the one thing I felt, that to what extent that vocabulary sort of preconditioned it to rediscover what we had. I don't know. I don't think they really talk about that or I'm not an expert in this field. I'm sure that the researchers must have thought quite a bit deeply about it. But we as coming from outside and reading this paper, it still is very impressive. Other than that fact, it seems very impressive that you give it a whole vocabulary of basic operations and using genetic or sort of an evolutionary process, it rediscovers a lot of it. So that is the gist of what this paper has to say. Now, the way it does is I'll just read the sum, the abstract of it. So, by the way, it's important that we get the big idea before we go into anything else. Are we all together, guys, in the big ideas here? Yes, yes. Right. here yes yes okay so then let us go and try to make head and tail out of this paper so machine learning research has advanced in multiple aspects including model structures and learning methods which we know the effort to automate such research known as auto ml has made significant progress and obviously this week we're studying that however this progress has largely focused on the architecture of neural networks where it has relied on sophisticated expert design layers as building blocks okay so this goes on to say in this paper that, however this progress has largely focused on the architecture of neural networks where it has relied on sophisticated expert design layers as building blocks are similarly restrictive search space. So what he's saying is that, see when you do, I taught you hyperparameter optimization. So you give which hyperparameters to optimize, isn't it? The structure of the algorithm is already in place. You just have some hyperparameters to optimize. Or in neural architecture search, which we didn't emphasize too much, what you do is you give some building blocks. You give the con nets, the D players, and you basically tell that, okay, go play around with these and wire them together. Add a few residual connections, this, that, and build a network that will solve this problem. In either case, you sort of have, it is almost like microwaving food that has already been cooked and put in the supermarket store in some sense and you are just microwaving and adding some spices or something like that. So the basic idea, this is from their perspective, and so basically they say, can we build an algorithm from first principles, train a machine learning system to go discover a solution just from first principles itself? And the question is how would it do that? So that's what we're going to come to. Our goal is to show that AutoML can go further. It is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias to a generic search space. So that is a thing, and in some sense they're saying that, well, in some, like, for example, I do believe that by putting up that vocabulary of operations, which are I believe that by putting up that vocabulary of operations, which are sort of the staple diet of machine learning and much of this, in a way there is some inductive bias, but it's still much broader. Like you're not saying it must be a coordinate layer or it must be a dense layer or anything. You don't even talk of gradient descent. You don't talk of back propagation. You ask this thing, go discover a way, just write code, just insert random bits of code from the vocabulary and create an algorithm and see what happens. So then they go on to say this is the main result here, despite the vastness, and this is something that I would say, of this evolutionary surge, trained by back propagation. So this statement is pretty interesting. It says that a very simple neural network, but it is still able to discover it. It's able to do back propagation. Implicit back propagation is of course gradient descent. And that in itself is impressive that just through random mutations, it could figure that out. These simple neural networks can then be surpassed by evolving directly on tasks of interest like CIFAR-10 variants, where modern techniques emerged in the top emerging the top algorithms such as bilinear intersections normalized gradients and weight averaging so the remarkable thing is some of these things that we do today and that were discovered frankly in the last 10 15 years it turns out that if you follow this approach, it seems as though it is rediscovering it. So, Murthasi, the way I'm interpreting what they're saying is they have competentized this whole process, right? The concept that they outlined, that is the setup, learn predict and learn right so they've competentized all the things required towards doing a machine learning process yes in which the learn concept in some cases when it's like i'm using a neural network that's one way of learning if it's something that's using classical machine learning that will be another way of learning so the way i'm interpreting what they're saying here is despite the vastness of the space, evolutionary search can still discover two-layer neural networks, is when they try out multiple techniques, occasionally they find a neural network is a preferred technique for something that is a global optimization signal that they have. Is that the interpretation? It's sort of. What they're basically saying, close to that, is that if you take the specific problems that they have taken, for example this image classification problem, the simplified MNIST binary classification or CIFAR, then what happens is now you have a task at hand. To solve this task, it seems to preferentially build up what looks like a two-layered neural network with back prop. That is what it's saying for this particular task. So we believe that these are very early successes, But the very fact that it's, the impressive thing is that it seems to have, it appears to have rediscovered some of these ideas like normalizing the gradients, or why do you normalize so that they don't explode and so on and so forth, right? And weight averaging and so forth. So these are things we do today. Frankly, we weren't doing it many years ago. And now here you have a very brute force approach of evolutionary sort of evolutionary approach of home growing your own network or your own code, not even a network, just some code mutations. And it seems to be rediscovering it, albeit at a massive computation cost, right? Which is why it can to be rediscovering it, albeit at a massive computational cost, which is why it can only be done by this sort of paper, actually, frankly, can only come from the big houses. It's not something you and I can write because we would never have access to this computational power. So they go on to say, actually, that the amount of, obviously, there's a bit of guilt here because to run a computation of this scale you burn through gazillions of you know megawatts or gigawatts of power i suppose so whatever power they burn through they they make it a point to say that they compensated it by buying that much of power from renewable energy sources. So it sort of speaks to that, that these are brutal computations. Anyway, so we, as of this moment, very inefficient way of discovering algorithms. But then perhaps nature has taken such an approach. Nature, evolution is slow and it takes gazillions of years for it to happen. Many, many, many generations and slowly the evolution happens. Something like that may be at play. So I wouldn't read every line of the paper. This is just the background in which they talk about what is prior work, what is related work and so forth. And this textbook of Hutter, of course, we have been referring to the whole week, automated machine learning. So the important line here is, so they take an approach. So we discover and optimize a local learning rule for neural neuron, etc. Our search method is similar but represents a program of instructions. They don't use just basic. So in prior work people just use addition arithmetic but here they use a much more, much richer vocabulary and we will go through the vocabulary. It's not worth going into. They set up some basic go through the vocabulary. It's not worth going into. They set up some basic task and language to describe it. So the important thing is to look here. We have three methods basically. Set up, predict and learn. So what is the setup method? So when you do the evaluation, you can initialize the memory, you set it up. And by the way, the S, sorry, the S here always stand for, sorry, the S. When you see the anything start with S, these are scalars. That is what this line is saying. V, I'm just calling it out. Anything that has a V prefix is a vector. And anything that has an M prefix is a matrix. So before we go into this, just think through what happens when you write a linear regression equation. You say that Y is equal to W naught plus W1X. W naught plus W1 X. So if you forget the bias term or you sort of include it in a augmented feature matrix, this is nothing but W dot X, isn't it? Sorry, the X vector is equal to 1 X1 X2, so forth. And W is equal to W naught W1, so so forth if you think in terms of this y would be represented as a dot product between these two and what are these two these are vectors right so this is a very quick recap here, you will, so sort of, first of all, the main methods we will have is the learn, the setup, right, and the predict. And this is the main task. Ultimately, this evaluation, the ones that, those mutations that do well in the evaluation or have a lower mean loss, the mean loss is some loss function, right? They are going to be winners. So in this, the things that are out of bounces for the evolutionary algorithm not to worry about or play with is this main outer loop itself. It is the task. The only thing they have to learn are these three things. These three things. They have to figure these three things out in this algorithm. So now let's see how it does it. And this particular picture is quite illustrative. First of all, what you do is, assuming that you have a few, let's say five, possible mutations with you, what you do is something like this. So let's say that you take an algorithm. Let's say that this is the parent. Any one particular one that you like, or is as of this moment the parent, you take this and you can make multiple sort of mutations to it. One kind of mutation is, do you notice that at this line a new instruction has been randomly inserted? So you can just insert a random instruction at a random location. The second thing that you can do is you can take a component and just randomize it. So this instruction, right? It was S4 is S0 plus S1. And this one was something else. And you replace it with something completely different. Do you notice that? The right-hand side is completely different. You've just done a substitution with something else entirely, randomly. And the third is that on the right-hand side, that is on this side, this is the assignment part of it, the computational part of it, you just made a small mutation, S2 just became S7. So these are the three things you do to mutate it. And then what happens is that you then run, the rest of the steps is straightforward. You then evaluate what is good. Now, if after this mutation, it turns out to be better than any of the previous population, you kick out that oldest member or you kick out something from that, the worst member or the oldest member from the current population, you evict it. Now there is one more trick that they have up their sleeve which is of migration, which is sort of based on nature. So sometimes you know you have animals that migrate from one habitat to another, one habitat gets stressed, and then they migrate to another. And when they migrate, one sort of species may be more sort of able to survive than the other species. So they, and it sort of takes over the habitat or things like that. So similar idea they use, what they do is, each of these processes is doing its own mutations and coming up with its best of the breed. So every once in a while, what they do is that they will take the best of the breed from some other processor and bring it into this population. So you have a population and then you go to some other populations that have evolved in other processes and you just take it and insert some of them here. And when you do that, insert one of them here, it's another way of sort of bringing different ideas together, hybridizing completely different strands from different pieces of code together. So that also is a good thing and when they do this, the rest of it is sort of detailed. Nothing actually can read to it, none of this is very hard, it's just a remarkable thing that with such simple ideas and sheer brute force it is able to and this is the interesting part that you see you notice that very early on it seems to have discovered linear regression yeah isn't it? This linear regression stuff. Then it discovers, sorry, concept of refined regression. And so gradually, as more and more computations happen and as the evolution continues, it discovers a lot of these things. So let's look at this. Let's say that somewhere along the line it discovers this. Let us try to interpret what this is. You notice that what is M1? This is initializing it by random weights, some matrix. When you initialize a matrix with random weights, what does it sound like? It sounds like a neural net, right? Like it suddenly brought in a matrix. You can bring in a matrix only if you have a sort of one easy plausible way to bring in a matrix is to think in terms of neural nets. Are we making sure? So this weight would be a matrix. So it has taken that and then it seems to have created another sort of a variable like this and it even seems to have picked up a learning rate. And then if you look at the product, what is it it doing it is taking the input v0 and multiplying it with the first matrix so isn't it it is doing w dot x right to produce the result the y1 hat from the first layer right and then what it seems to be doing is it is applying, it seems to have discovered ReLU because if you think about it, what is ReLU? Flat this. It is the max of zero and X, sorry, Y or sort of X itself. Let's say X itself. This is your ReLU right in a standard form. And it seems to have rediscovered relo and then having applied relo what is it doing it is uh doing a dot product between uh prediction this uh this activation and v4 why was it oh now it is feeding it into the next layer right right? You literally see it feed into the next layer to generate the next layer's prediction at this particular moment, right? And how is it learning? Let's see the learning part. This is the gradient of the VLU. So it has computed that. Now, if you remember, what is the update method? W is equal to W minus alpha gradient, right? So it seems to have picked up the gradient in some sense. It seems to be computing the error, scaling it by the learning rate in some sense, not exactly this expression, but something pretty close to it, right? And what is more is that it seems to be, if you work its way back, it seems to be going down. You notice that it goes down to M0, the first layer, and it seems to be updating the weights. Now, you have to look at it carefully to interpret what each of these things means in terms of what we do in neural nets. And that is the most impressive thing, that if you run it for a very, very long time, it does it. Of course, the comments have been hand-added. Now, if you look at the, as it evolves, it starts very early on, it seems to have discovered a linear model, right? No stochastic gradient. Then basic ideas like loss clipping to prevent exploding gradients, I guess. Linear model. Then it discovers linear model. Like it is not just doing, see here's the thing. At this particular moment, it's not learning. It is just doing w.x. It soon figures out how to do w.x. After a little while, it figures out that, you know, let us do w.x. After a little while it figures out that, you know, let us do a gradient descent. And so it discovers a stochastic gradient descent. And then gradually it realizes that you can pick random learning rates. And so you can end up with better hyperparameters. So this is your randomized search for the learning rate, which is the only hyperparameter that seems to be here. your randomized search for the learning rate, which is the only hyperparameter that seems to be here. After that, it soon discovers that it helps to normalize the gradients a little bit. And then, after a little while, it seems to have a fixed on a learning rate that it likes, and sees that it works, tends to work, it to that, somewhere along the line going forward. And now it is going beyond this linear models, moving in the direction of a sort of neural net. It comes up with an activation function. And once it has an activation function, things move pretty fast. You have a randomized weight and you start seeing all these little things emerge from there right and in the paper actually it shows somewhere yeah look at this one look at this early stage what is it doing sorry um save water no look at this and let me just remove the finger touch touch off okay but if you look at this how does it learn it starts with some this is your why right the actual why it starts with s1 which is scaling the output. This one, if you look at the dot input. So this is your w dot x. And so, oops, sorry, w dot x. And it is taking that, it's scaling it with some value. It is doing S1. Like so it computes the error. So I don't know how this computes the error uh scale the prediction and as zero okay this is the label and s1 is this plus this actually i would have ideally hoped it was a minus but it seems to be a plus here it has taken the well so there's a little bit of an interpretational thing. You can just think of this as the difference between y minus y hat, right? So this is your y hat in some sense. And this is not y minus y hat. It's more like y plus y hat. But all right, let's take it to the grain of salt. I mean, what's a plus or minus? Then this is the gradient, right? So how do you compute the gradient in the case of linear regression? You can work it out. It will literally be y hat times the initial value v0, the weights, and it does that. And then here is the update of the weight stage. You know, this is your gradient descent state. And so, when you see it evolve these algorithms and do well, you have to sit and decipher what exactly did it do. Because it's doing random mutations and coming up with all of this. And so here is that, it sort of explains that, that if you look at this complicated algorithm, what is it doing? Forward and the backward pass. The forward paths being the predict, the backward path being the learn path. So in the language of PyTorch, we'd call it the backward path. Or the gradient descent back prop and update step. So this is that. This is the gist of it. Then there's some details, I mean we can go into the details. So, I wouldn't go into that. So anyway, the whole question is, now, this looks very nice. There is one thing that I felt though, is that if you go to the appendix here, supplementary material, then you see that, you see this table. It is allowing it only these 22 continued on the next page, not 22, there's 60 operations and more, 64 operations, yes. So you give it a vocabulary of so many operations and you're basically saying that these are the letters of your alphabet, go write something with it. And through a whole process of random mutations, it comes up with it. Now the thing is why these, why not others? Obviously there's a little bit of a inductive bias, you know, from prior experience that modern algorithms tend to use this part. And in some sense you are saying that if I give you this as a vocabulary, can you rediscover some of the algorithms or some of the basic algorithms? And that is what seems to be the gist of this paper. That is it, guys. It is a very simple, in a way, straightforward paper. But it is not like last week's paper. But this is straightforward, very interesting and thought-provoking. And it brings back the evolutionary approaches to machine learning have always been there and we we never really covered it in our workshop because of the limited time but maybe this was a good time to touch upon it so any questions guys as if um i'm kind of trying to relate to this also from the little knowledge that i have genetic algorithms there's a lot of similarity in terms of how they are explaining that it is that it is an evolutionary algorithm now have they shown how they are evaluating the the quality of a strand right assuming it's a the entire sequence is a connection of steps. Now to be able to say a strand has qualified itself as more favorable. Did you see where they're doing that in the paper? Yeah, that is it. Remember the evaluation loop. You remember the evaluation loop that they have, that is the litmus test, isn't it? Everything, any mutation that you create right any strand that you create ultimately the main loop is loop is evaluating the overall success of that entire the full stretch right but how about for the strand no it doesn't deal with that it just says that each mutation is the completes is a complete program okay it's a complete program that's a very it's only the complete set so when it gets two pieces like a part which is basically set up and learn and another part which is predict there is no way to say this predict is working better in many situations i want to use that along with that setup and learn, which I find working better in some several situations. Yeah, see, it is hybridizing the best of the, because it's bringing in populations from here and there, right, other processes. So it is hybridizing in some sense, the ideas from this and that. So those things are happening. Through random selection. Yeah. Through random selection. Yeah, through random selection. But at this moment, they have taken a simple approach, gone brute force, and seen how far it goes. And the remarkable thing is it has gone, it has created some noteworthy results. It's scary, actually. This is basically the computer can start thinking, it can get smarter over time, right? Yes. I do feel that at some point they will take up a lot of the cognitive tasks. They already are taking a lot of the cognitive tasks. Today it's difficult to tell whether self-driven cars are safer or human driven cars are safer. Self-driven cars make mistakes in very specific situations, but in a lot of other normal situations, they tend to drive better than humans. So we'll see. There was a hope, actually, with the 5G and the cars talking to each other. It would have made quite a dent or quite a huge improvement to self-driven cars. Because one car could have told the other car, look ahead, watch out, there is a bump in the road ahead. Yeah, I think that's still there, right? I mean, the whole V2i. The V2i, once it's in place, the V2V will take over. That is right. At this moment, the problem is, yeah, the 5G adoption in the US is happening. See, there's a lot of buzz in wireless space. There's a huge amount of buzz about 5G. It's penetrating, but slowly. Let's see. Thank you.