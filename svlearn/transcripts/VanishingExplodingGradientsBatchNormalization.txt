 There are three topics that we'll do. The main topic is computer vision and we will see that in this entire field of computer vision has been absolutely revolutionized by by AI and in a very very profound way much of what we used to do has been changed so we'll deal with computer vision and we'll in particular learn about the architecture, new architecture called CNN. This is it. And then there are two small things though I would like to finish the talk with. One of them is batch normalization and they're cousins of it, but I'll just take the batch normalization, which is a really small topic. And then the concept of exploding and vanishing radians. So why don't I finish the small topics first? Batch normalization is a very simple idea. See, when we feed data into a neural net or into any of the machine learning algorithms. Vaidhyanathan Ramamurthy, If you remember in ML hundred and 200 from your previous background. We just say, just like you wake up in the morning and brush your teeth in the same way when you encounter the data. The very first thing you should do is you should clean the data and you should standardize you should scale the data right scale normalize the data the data so you normalize the data before you feed it into your model except that in a neural network you don't in a way you can think of it is that you don't have one model you have many many models each of these neurons is a model right so then you say well that is fine the input layer the input data i'm going to normalize but then there's a there's an issue there these these things produce outputs the first layer of neurons they produce outputs and the outputs that they produce they may be they may have blown up you know some outputs may be much smaller some outputs may be much bigger these neurons are not you know they don't act in sync so now again you have a problem some of the outputs may have drifted and become too big, some axes. So remember that each of these, if you remember the paper that we did with Olaz on Sunday, each of these neurons, you can think of it as one more dimension. So the same problem comes as we used to have with input data. With input data, if you remember that the main problem was you don't want to have data, like for for example if you're looking at elephants your input data is about elephants and you're looking at the age of the elephant and Then you're looking at the weight of the elephant and you're looking at the I don't know the volume of the elephant and these are Big numbers age is not big number And you are also looking at something very small the size of the elephant's pupil you realize that that number would be very small and so when you build a model for prediction then your data your your gradient descent will get highly skewed right you will have a very eccentric loss surface i mentioned it to you that if you look at that it it feels like a ravine it does have a minima but it just begins to look like this it does not look like a proper like a proper hyperbolic surface or parabolic or sort of a convex surface it is convex but it's it might still but it may become almost like a ravine right that sort of a thing and that is depicted as we say in a very eccentric contours of the of the last surface and if you remember this sunday we literally saw visualizations of that. So how do you mitigate that? The reason this happens is that a small step, like let's say that one unit step. If you say I will make a step of one inch. Well, one inch when you make a jump, when you make and you're talking about the height of an elephant it's not much but one inch is already too much because that may be already the size of the elephant's pupil so you need to make sure that a step a small step is truly small step along each of the axes it shouldn't be that along one axis it becomes a huge step another along another axis it becomes a huge step, along another axis it remains a small step. So the way you do that is you scale the data. And then we talked about various ways. The most common way is the Z-scaling, which is you take the value, input value i, and then you subtract from it the mu, the mean, and divide it by the standard deviation. So when you do that, this is called, people often call this the Z value. And in your scikit-learn, it is called the standard scalar, the classes, standard scalar. You'll find it, I believe, I may be wrong, I'm just speaking from memory. It is in this SKKit, a pre-processing library, a pre-processing package. If you look at the, one of you, if you have your code open, you can look it up. SKLearn pre-processing, the standard scalar. Let me write it clearly. Standard scalar means it scales it down it's supposed to be the doer of scaling so then you can have min max scaling min max scaling which is different min max scaling says um let me call it x prime I is equal to X I minus X min and this is X max over minus X mean but this is the range of the data and you just look at the Delta how far you are from that how much how far you have come from the minimum value so all of these will scale the data how far you have come from the minimum value so all of these will scale the data you know if you come because the numerator and the denominator are exactly the same what is the effect they have they create dimensionless quantities so it doesn't matter you it for example it doesn't matter whether you're measuring the size of the elephant in millimeters, in which case the height of the elephant would be a huge value and with the pupil size will still be reasonable. But the moment you scale the data and you make it dimensionless, they become comparable. So that's the point of input normalization. But here comes the interesting idea though. All right, you normalize this input and these little models, each node is a little model and you have normalized input. Let me just call it normalized or standardized input. People often use different words for that, but they are producing outputs those outputs will again go to the next layer but these next years are also models right so and there is no guarantee that these activations from the first layer let me just call it I activations from the first layer yeah like the symbol as we have been using in the trial in the previous notes ai of the first layer there is no guarantee that they would not have blown up they would not have become very different in scale so what should you do what you should do is you need to again rescale it do the same thing that you did here right so that is called batch normalization did here right so that is called batch normalization so the way you do it is that you take a batch of data let's say 16 data points now what will happen is through this all the 16 data points when it comes they would have a certain degree of variation yeah you scale it you take the scale it down, this response, standardize it across the 16 values. So that is why the word batch is there. You're normalizing across that a small batch of outputs or activations. You do that. And so you do it for all of them. And so you repeat it at each stage. You batch normalization now there is a there is a something about batch normalization if you have very shallow networks you may not need batch normalization if you did this homework in the code the solution i have added batch normalization so that you know what the syntax is. However, because the river data set is so simple and you can solve it essentially with one layer or two layer or three layer, no more than that. And these are very small number of layers. Batch normalization, it turns out, you can sort of skip because you input data that is normalized. Once you normalize the data at the input side, then, and you use ReLU, you can quickly argue that the value will not explode. It won't become too big. Assuming a suitable initialization of the weights, assuming that the weights are initialized randomly to very small value. And that's another topic, what is a smart way to initialize weights? At this moment, just assume that random small values will do. But in reality, you do something smarter. There are two algorithms, Xavier and Key algorithm. And he's the state of the art, I believe, at this moment, which kicks in, but basically it just chooses the values from a good range. So if the weights are small, and so the input is being, the small inputs are being multiplied by small weights, then you would realize that the output of the first layer is not likely to become too big, especially if you're using something like ReLU or one of the sigmoids, in which case it is guaranteed to be never big right but with relu it might go up but still it doesn't blow up at all because really open gate it's one activation is just direct transfer and then and this is what we covered remember in the activation lecture and so couple of layers you won't see the effect or you won't see the need for batch normalization. But when you create very deep networks, then if you don't do batch normalization, you will end up with very ill behaved and highly non-convex loss surfaces. This is precisely what we saw on Sunday, if you remember, in a very dramatic and visual way the lost surfaces look entirely different the the same so that is the value of remembering to do batch normalization when you have a lot of deep layers many many layers in your network so that is batching the idea is being simple just treat the output of the previous layer as input to the next layer, the models in the next layer. So, well, you should always scale the inputs. So, therefore, you continue the idea of scaling the data into the layers. And is the simplest form of normalization there are other ways to normalize and that itself becomes an interesting topic there's instance normalization layer normalization very there's quite a few variants and more variants keep coming up so uh let's take that as a extra session in one of the extra sessions let's cover that topic so next i would go to another topic which is actually uh fascinating it's one of those things once in a while you feel the first time you read about it you think you understand it and then you scratch your head and think and ask yourself well did i really understand it deeply or is there more layers to the onion that can we peel the onion a bit more and go deeper into what is causing this concept and so I'll talk a little bit more about exploding and vanishing gradients when we talk about exploding and vanishing gradients. Let's bring that up. So exploding and vanishing gradients. And the vanishing gradients, this is by the way, particularly true of deep networks and of recurrent neural networks, because recurrent neural networks are an architecture which is deep in time and temporarily deep not deep literally in layers physical layers what happens is that you take the output of one of of a node and you feed it back into it as input right and you sort of cycle through it again and again. And you keep doing it along with the next input. So you take one input, you get an output, you take that output, you feed it back into the into the, let's say, the neuron along with the second input. Right. So it creates a sequencing effect. The output of the first one becomes part of the input, along with the second data point that is also an input the pair of it and so there is a you know it catches all of these relationships auto correlations and sequence relationships you know very well those sort of models but because it keeps looping around it essentially has the effect of having a deep network in time, temporarily, in the time dimension, you can think of it, in the sequence dimension. So whenever you have deep networks in some sense, you do have the problem of exploding and vanishing gradients. So let's talk about what it is and how we would look at it. So here is a simple way I will explain it. So imagine that you have a network i will just take a you know i tend to explain things using a chain network because everything looks so simple and it turns out see guys we do know that each of these is a layer but usually the arguments that you can apply to chain uh carry forward so i would always encourage you to think in terms of j so let us say that you're using a simple um activation we are familiar with two of them really let's try really so what happens is that input comes in x comes in this is effectively your z1 right z1 is if you think about the notation that we created z1 the is what is it the weight in the first layer this is the first year second year third year fourth layer and a left layer so if you really think about the this, what is it the weight times the input, the actual input vector in the first layer. The input vector you can think of it as the activation zero coming from the zeroth layer. This will produce an activation here, activation of the first layer. Did we see that guys? This is just a direct whatever X came you just multi in one dimension, you're just multiplying it by W and for the time being I'm skipping the bias term. Assume that bias term is there. So, this is the activation comes out. There is a weight 2 and so what would be the result here? You would again get a Z two. And so what would be the result here? You would again get a Z2. So activation one would be relu of Z1, isn't it? So now think about the relu. It will, what is it? It is either, it will either let the Z go through or it will stop it if the z is negative the output will be zero if the z is positive it will be just z itself right so let us say that the output is positive so the output here would be a1 would be nothing but z1 and that would be nothing but W1.x. So would be just this assuming that the gate is open, the relu gate is open. So far so good guys. Now in the same way we can argue that here Z of the second layer is w2 times activation in the first layer right and this activation coming out of the a2 what is a2 a2 would be the reloop of z2 and when you think about it the gate is either open or closed if it is open this is nothing but w2 times z2 w2 times the activation the input that is coming from a1 and a1 and that is equal to if you really continue this line of reasoning you will see that this is equal to of reasoning, you will see that this is equal to, now what is this? Let's put it here. W2, W1, X. So this is the activation from two. And so from this, you can extrapolate and you can understand that A3 would be W3, W2, W1, X. And likewise, the AL would be WL, WL minus 1, WL minus 2, W3, W2, W1, X. You see that? Assuming that all the gates are open, this is what the activation, this is the output that you're getting. Right? Y hat. Do we see that, guys, which is your activation of the last layer yes it is straightforward and now comes the interesting thing let us say that these weights are positive they are all positive and just for argument let us say that they are just one point some small number greater than one some number will take greater than one if w is greater than one then each of the w i's is greater than one so it could be like 1.1, 1.2, 1.05, 1.5, 1.9, whatever it is. Imagine a set of numbers that are all greater than one. So what will be the product of numbers greater than one? Let's say that the average value of this is something like 1.1. Let's be modest. We something like 1.1 let's take this be modest we'll take 1.1 and let us say that you take a layer that is a 10 layers deep could anybody tell me what is 1.1 to the power 10 Anybody with a calculator would like to guess what this comes out to be? One of you could you help me out or otherwise I'll have to do this. See what it comes out to be. It's worth doing this. Scientific 2.59 come again how much is it is it 2.59 3742 is that it that's what i get to too. So just 2.6 essentially. Approximately 2.6. So what have you noticed that this output gradually as you go from here it keeps on increasing isn't it and it becomes bigger and bigger. You wouldn't have expected the output of each of these layers would be a little bit greater than it could just shift from 1.1 to some number bigger than this 1.2. In fact, that's what it would be on squaring it. And then 1.4. So it will keep on increasing. It will keep shifting to higher and higher values, higher and higher values. And now take the fact that instead of 1.1, let us see what happens if it was something like 1.5, a little bit more bigger. So you realize that it will go something like 1.5 will go to 2.25 will go to the square of this. And one of you would like to tell me what it is, 1.5 times to the power 10. How much is it? 57.7. 57.7. Let's say approximately 58. Do you notice how much the number has blown up? And so suppose this was, this network, 10 layers are too little. Let's say that this was hundred layers so could one of you try this 1.5 to the power hundred on your calculator seven that's really big sorry four point.06e to the power 17. Into 10 to the 17. So you realize that this number is irrationally big. You don't want an activation like that. So for example, if you're trying to predict the house prices in California data set, you would probably agree that your house price is not likely to be anywhere close to this isn't it this is like a trillion trillion something in the other so it's not feasible something has gone wrong in computing the activations or computing the outputs like that and so what happens is that in the forward direction it is this what happens in the backward direction you realize that in the forward direction, it is this. What happens in the backward direction? You realize that in the backward direction, we used to have in the back prop, if you remember, when you're trying to calculate the activation of the L minus one layer, you have this term, right? The activation of the L minus one layer, which is the input to the L plus one layer, it is going to affect the z of the next layer the z of the next layer is going to affect the activation of the next layer and activation of the next layer is going to affect the here by l i don't mean the last layer i'm just comparing two layers l layer l and layer the one before that now let us say that this is computed what was this if you remember this term and this is just the activation of the al minus one and this one was what was it activation of if if you use let's say that you use a sigmoid or let's use tanh tanh activation from our previous discussion we remembered that this was equal to one minus a this a l square so the now think a little bit about this. If these activations begin to blow up, right? At each layer, as you go back, do you see that if I replace these activations with, what is it? Just the weights. You have a lot of weights, W1 to WL minus one. Sorry, I should use a substitute. You will realize that when you try to write it in these activations in terms of the weights, you'll again be in a very situation where you're multiplying a lot of them. L minus one, L minus two, all the way to W1 times the input. That is the activation here. This is your first term and then one minus now you have even worse w l w l minus one w one and the whole thing square that multiplied by the gradient w a so if you just think about it a little bit you realize that you're dealing with, if these weights are positive, you can have very, very high weights. It can just blow up in your face. Are we seeing that, guys? In simple terms, the activations, if these activations become big, soon your gradients begin to blow up. So the question therefore is, how do you prevent that from happening? Well, one easy way to prevent that from happening is what we just talked about. What did we just talk about? Normalize the output, scale it down. Isn't it? So what happens is if at each level, the output AL doesn't, so AL minus one doesn't directly go into the next layer and become the input. So in other words, if it is something like the next layer output ZL, we wrote it as AL minus one times the weight of the last layer, right? But suppose it is not this. Suppose it is scaled down version. You significantly scale it down. So then what happens is that the next layer activations, it doesn't blow up. If the activations don't blow up, then when you do gradient descent, you will realize that your weights, which are going like, and I'm skipping a lot of steps here, times the weight, the gradient with respect to the weight, and you can fill in the steps, you know, the easiest steps with backdrop that we have been doing over in the last time. These weights don't blow up. And so that is another reason you should always remember to normalize the outputs of one layer before feeding it into the next when you have deep layers. Otherwise the output can blow up, number one, or much worse is the fact that your gradients will become uh hopeless when your gradients blow up it is not possible to do gradient descent right one step will make it a huge negative value right another step will again oscillate it back to a huge positive value the weights and so learning would be severely affected so this is the problem of the exploding ratings so I see one quick question here now I follow the concept that we're doing right they're basically trying to normalize it so that the component here in terms of the activation yeah it it gets into like between zero and between the a zero and one right once you normalize but now if if you continue that thinking process all the way to the end then let's say the intent with the neural network was to distinguish between ten different numbers one through ten right at some point what you also want is as that input is kind of fact, moving along different levels, you want the 10 building up to a 10 and the one building up to a one. But if at every level you are normalizing it, you wouldn't get that spread, right? You're basically bringing down the spread every time by normalizing yes see what happens is uh this brings us the to the one activation function i didn't cover which i have to cover separately the soft max it's a special one remember i said we'll do especially differently so this is a great opportunity just introduce. Suppose you keep beating those numbers down. What will happen is, let's say that you end up with values like 0.9, 0.01, 0.9, 0.05, right? 0.0, I don't know how much left, 5, 6, 0.04, 0.04. I hope they all add up to one let's see so you you come up with value something like this even if you don't actually you don't even worry about it even if they don't add up to one what you do is you divide it by the sum of it the sum of these numbers now see what you do in the softmax you take the you exponentiate each of these numbers. Now see what you do in the softmax. You take the, you exponentiate each of the, and by the way, some of them may be negative. This may be 0.9 negative, and this may be positive, and this may be negative. So you have a bunch of numbers, but they are all small in size. You make sure that you kept pushing it down. But what happens is that the last layer, you make sure that you kept pushing it down but what happens is that the last layer when it produces you don't normalize the last layer you let those numbers be produced so those numbers need not be between zero and one or something they can even be two three zero point one minus two three some numbers like that but they won't be excessively large numbers, then what happens is you, so just take this example, you exponentiate these numbers, e to the, e to the 0.01, right, e to the minus 0.9, and let's say that you have, or maybe I'll leave this as positive, just for fun, this positive, something like that so this will be positive e to the 0.05 e to the 0 0 what is it negative e to the negative 0.04 something like this then what you do is you scale all of them by the sum summation of this when you scale it to the summation of this what you will notice is it has a very interesting effect which is why softmax is the is the go-to last layer for all classifiers if you in the homework you'll see that I have put in a softmax at the last year because what will happen is these numbers 0.01 0.9 0.05 minus 0.0 actually why don't you forget about normalization suppose I make it minus 2.1 and this is let me just make it 1.5 just for fun what will happen is this will remain it will almost become very small 1 over 10 to the something approximately this will become large this will be something like 90 0.99 this will become again a small number maybe 0.08 this will become 0.001 a very small number are we together we together? And this may, I'm just guessing, by the way, these are just guesses. You can try it out by doing it. If you exponentiate these numbers, what will you get? Let's say that you get 01, something like this. So now you look at this, this is 1% probability approximately. This is, I don't know, a 99% probability. So suppose this is a i don't know a 99 percent probability so suppose this is a horse this is a cow this is a duck and this is an elephant what what it is saying is that if the moment you scale it and make it into probabilities and you use the softmax function, this will be, I don't know, 0.8% and this is like 0.01, whatever the residual amount is, that percent. Something like that. So you realize what is your system saying with a very high degree of probability it is a cow. Premjit, you got the idea, the reasoning here. What it does is Softmax amplifies the big value automatically. It will boost it up and make it stand out. So one way of looking at it is these values will be like this. The values will become like this this will be the cow value this will be the horse value and the other ones will be like close to zero the ones that are the ones that are duck an elephant maybe duck is this little bit. And elephant is this little bit on a scale of zero to a scale of one. Let's say that this is the scale of one on the probability axis cow pretty much becomes near certainty. So I said, I have a question related to that. So the size of these weights. Asif, I have a question related to that. So the size of these weights are somewhat related to the features, is it like features of, meaning predictors, but in this case, these are all response waves, right? We are going to classify them as a horse or a cow or a duck or elephant, the response variables, is the correct yes so what you do is you ask these questions and there is this word i don't want to introduce it today it's called arg max let me give you the basic intuition it just says that which of these outputs produce the maximum response and this produce the maximum cow produce the maximum response and this produced the maximum cow produce the maximum response and the answer would be cow cow is the arg max of values these values that you are seeing here are you getting that? Cow is the class which if you assume to be there is producing the strongest response. It has the highest probability. If you could also like when you use the word softmax, what is softmax? Is it playing the role of the normalization in the last layer or is it playing the role of an activationization in the last layer or is it playing the role of an activation function in the last layer softmax is an activation function remember activation function is something that's nonlinearly distorting your signal so it is a activation function but it is unlike other activation functions its job is to do one thing take the space and make the that thing which is producing the strongest signal stand out okay okay that's all it does it makes it stand out so we'll talk about uh odd max properly in a mini session separately it's good dealing with it you must have seen that in the homework I used softmax. If you go back and look at the code that I gave you in the classifiers, always softmax is the last layer. Now, why is it the last layer? It needs a bit more discussion. We'll do that. But today we have other heavyweight topics to discuss. So let's get to that. Otherwise we won't get time for computer vision okay so let's take it as a fact that i haven't fully explained soft max and i will do that but just basic intuition but see conversely the other is also true when we look at this product of these weights which you can roughly think of as some average weight value to the power L, right? Effectively, this activation. So now when you take the power L, if the average weight is smaller than one, let us say that it is 0.9, right? So if one of you, and let's say that you have a hundred layers, can you tell me forget 100 layers just 10 layers what is 0.9 to the power 10. could somebody please do that for me quickly 0.34 what's that uh 0.34 0.34. What's that? 0.34. 0.34. So in 10 layers, even with a big number, it became that. And now, so this is, if it is just slightly below one and what happens if it is, let's say half, let's take a generally you'll encounter a number like that and 10 layer now tell me what it becomes it's 9 into 10 raised to minus 4 yeah so it is roughly zero wouldn't it be it begins to and by the time i go to 100 layers it becomes hopeless right it is like tends to zero so what happens is that both your output and the other end of it the gradients they will just vanish the trouble with vanishing gradient is is this suppose that you are suppose your gradient at a given layer at a given layer L is approximately zero Let us say so what happens to the update of that layer? The update in that layer will become the previous value of L minus alpha times DL D loss the WL and this quantity is approximately zero. So are your weights learning? Are your weights getting updated? So you would be saying that your weight next is essentially this, maybe minor creep up. In fact, if your weights are sir if these gradients vanish this will be exactly equal to zero because computers they don't register quantities below a certain amount you have underflow it becomes effectively zero so what happens is that no learning no learning of the earlier layers maybe the last and the second last layer may learn something, but as the gradients become smaller, you can't learn anymore of the earlier layers. Do you see this problem, guys? So now you ask yourself, when do you tend to have that? And if you look at activation functions that look like sigmoid, like the sigmoidals, what happens in this region? If the... Saturate. Yes. In the saturation points, in the asymptotes, what is the gradient? What is the slope? Zero. Is approximately zero, very very very close to zero so then when your activation functions are producing results so large value unless they're producing results very close to zero zero value in this range if they're producing if you feed in a signal that is not in this range it will basically produce zero gradients and so learning will stop in fact that was the reason people couldn't create deep neural networks for the longest time and people used to say that anything you you can do with two three layers you can do with just one hidden layer in fact that's a universal approximated film so it was a little hard to justify having lots of layers of dense layers with the gradients regularly vanishing with those functions so that is why actually into the early on when hinton jeffrey hinton moved away tan h improves upon the sigmoid then the the logistic sigmoid but so it the made made it possible to create a little bit deeper neural net but then when hint the Nair and Hinton came up with the re-loop that was a breakthrough and we covered that aspect last time because it does not it suffers much less from vanishing gradients it has more the problem of exploding regions but you can batch normalize and so forth and stay stay sane relu has another problem that we talked about dead neurons if the gate is closed you realize that if you have a neuron and this gate the the input that goes into it is less than zero the output is zero a of this layer will be exactly zero not only that the no the back propagation the gradient flow also won't take place we can convince you that the gradient ultimately will have a multiplicative factor something to do with the it will be it will be related to this yeah right and so if zero is coming propagating back basically this neuron becomes a dead neuron we won't go into all the details so for that those of you who missed the activation function session, please do see if you can find out time to go do this. So then you have a problem of, so anyway, that's a problem of dead neurons in ReLU. And then you try to solve it. And then people have solved it in many ways. Leaky ReLU, exponential ReLU, parametrized ReLU. Then came the new switch functions, which essentially multiplied a really like function a linear function with a sigmoid and then now there is a mesh function that recently came out right and I find it interesting that mesh function the creator of it is a person whose last name is Mishra. So he's got this function, the mesh function. So anyway, what that does is multiplies that with tanh effectively instead of the sigmoid x and it turns out that it does a little bit better than the the switch function, which was this one the signal so people keep coming up with new activation function and each of them have their pluses and minuses now one thing that I would recommend if you are using ReLU yeah in the earlier homeworks we have always been using ReLU or one easy improvement that I would suggest for your network is when you do torch.relu, this is how you define your activation function is equal to that, replace the word relu with clu, it will generally perform better. And this is again, something we covered in the last session. Alright guys, so that is about vanishing and exploding crates one way of explaining there is actually another way of explaining it and it has to do with a little bit of kiosk area actually it has to do with sinks and attractors I won't go into the basic idea is that but there are if you are familiar with Mandelbrot sets have you ever seen a Mandelbrot set visualization a fractal visualization of a Mandelbrot set yes this is a representation with bulbs and the bulbs on them so basically what it means is there is a certain iterative transform. Zn plus 1 is there's a certain transform of Zn that you do. So, it's very simple. I forget what it is. But if you take a point here, it will blow to infinity. If you take a point here it will stay bounded so what happens is that there are certain points that represent stable points you know you don't blow out but there are certain other points if you start from there it will blow up and in some sense what we are saying is that based on what your activation function is if the weights initialization is poor Then your Successively as you go on activating and reflowing the gradient path it will all begin to blow up or begin to vanish 10 to 0 or 10 to infinity So there is only a certain region or set of weights or initialization you have to be careful if you are not you are going to end up in trouble you have to be careful about your activation function you have to be careful about your um the weight initialization both of them right so the initialization for example is something that you should use and if you remember from our sunday we had this beautiful visualization we could literally see that in your highly non-convex last surface very close to a well-defined global minima you know the well-defined minima that you that you are rooting for in the neighborhood of that, the parameter space looks convex, isn't it? Locally, it looks convex. In other words, just to refresh your memory and for the benefit of those of you who were not there. So what happens is that when you use all of these good ideas of dropouts and batch normalizations and so forth suppose you have a lost surface that comes like this you remember that the good loss surfaces look like this isn't it so what what are the qualities we want in a good loss surface the valley the It should be broad and deep. And in the neighborhood of the valley, the parameter space, does it look to you more or less like a convex surface right maybe i'll cheat a little bit just to bring home my point um does it look like a essentially a convex surface it has a single minima and wherever you start you'll end up on this minima if you start within this box if you start within a parameter space spanned by this region you will end up you will you will do this you'll end up here you can start anywhere and then you'll end up here do we remember that place okay maybe we don't you all quiet. So I'll bring up that Raja Ayyanar? Is it okay, I'll give a couple of minutes to bringing back that visualization. Sure. Raja Ayyanar? The value should be deep and broad. Yes. What is the issue with the sharp edge? Oh, you mean what if the valley is deep but sharp? It is like this. What is the problem then? The problem with this is it is quite likely that this sample of data, the specific training sample that you took, produce this very peculiar valley. But if you take another sample of data, you won't get this minima at all. So this kind of minima is suspect because another data may just have a, may just completely walk over this, lost surface may just walk over this and it may not have this very deep crevice okay that's the thing so in other words a genuine minima has to be well defined it has to be deep and you have to fall into it convincingly fall into it so let us see that actually what i mean this is a so i remember that about only two-thirds of you were there in that part so it is worth doing through that where is my last surface loss functions which screen are you seeing are you guys seeing my um browser screen no just you're seeing your drawing screen drawing screen so let's change over to the browser screen for a moment so guys uh if you notice go to this link Go to this link. So these are various architectures. So one of the things I mentioned that good habits, like for example, the good habits that we said, batch normalization was a good habit, dropouts was a good habit. Then one of the things we didn't cover but i just mentioned skip connections if you short circuit connections it's a good habit you know it leads to much better situation so we are going to see that so look at this guys first before i go into this do you look at this lost surface what can you say about this lost surface this seems to be it seems to be infested with a lot of local minima you see that not only that you observe very carefully the actual minima if you look from this side there seems to be a reasonably well-defined broad valley here but if you just rotate it a little bit oh goodness now what happens it's actually a ridge isn't it it's a narrow valley it's a very narrow valley yeah so and there are a lot of local minima's here now let's see what happens with the same thing this is is ResNet 56, a famous architecture. In this, one good habit, one good practice, namely short circuits or the residuals, that's why it's called ResNet, is removed. What happens if we do bring it back? This surface, this other surface, and let me bring that surface back. Now guys, think about it. Do you see the quality of the minima? When you look at this minima, what can you say? Is it broad and deep? Yes, sir. And very smooth. And very smooth. And you also see that it's very unlikely that you will get trapped in local minima. Assuming you have sufficient momentum you're likely to tumble your way into this the the the the best solution that you are searching for right and that is a point that i'm trying to make here and see uh you can see that let's let's look at this from for another particular network a dense net is simply literally a dense so what does a dense net look like good thing with dense neural networks is they tend to generally have fairly good so long as you do the right batching, batch normalization and other things, they tend to have a good global minimum. You can see it in your lab repeatedly because what you are creating are dense layers, fully connected layers. And if you just keep on changing, repeat the exercise, you don't get wildly different answers. You don't get trapped in different minimizers. It tends to converge to more or less the same answers after a certain number of hydration but let's look at some other architectures let's look at what resnet is without actually i'll compare vgg look this is the vgg this was the other one that won the prize in 2014 uh and why it is good, it has its own reason. We'll come to that. Let's look at this. ResNet 110. So now you have 110 layers. That is deep, isn't it? So then what do you notice? Competing with this minima are other minimas nearby. So it is not really very convincing that this is truly the best minima, isn't it? You could end up anywhere here, which is why it is very hard to train. What people say is that you lose generalizability in your learning process but the same thing when you add the short circuits let's do that how do you want to look at two at once the wireframe option is really nice then you can kind of see where the minimum compare to each other that was a good idea thank you for saying that yeah let's do that yes so you can see one is blue the blue one is jumping all over it's very corrugated surface or what I could do is I can position so you see that the red one which is the smoother one which has skip connections the effect of it so this is just showing the effect of skip connections if you go to the website you will see the effect of dropouts and batch now and in the paper itself if you remember we discussed the effect of dropout the batch normalization and batch size how does that size affect all of those things we discussed so we are not going to rehash that but i'll stop here any questions guys um i have a question on the um vanishing gradient what leads to the dead neuron how do we get into that situation no dead neuron is something not so much to do with vanishing gradient it has more to do with relo see to solve the problem of vanishing and the exploding gradients if you take relo the relo is very simple relo is essentially if you give ReLU of Z, it is essentially Z if Z is greater than 0. We can't see your writing. Oh, sorry. It is. ReLU function says, activation state, that if Z is greater than 0, input is greater than 0, let it go through. I call this the open. It's a pass-through. Let the signal go through. On the other hand, if Z is negative, don't let it go. So what happens with the back propagation when you use ReLU, there is a risk that both your gradient and your activation the activation for their AZ is D it becomes zero so when both in the forward and the reverse direction you get zero what happens if you get zero in the forward direction in the backward direction you'll get essentially zero okay so then what happened? This neuron, what has happened to this neuron? If the gradient goes to zero. If the gradient doesn't pass through, it is not learning. No learning. Not learning. And if it is not learning, we call it a dead end. That is the reason. So the shape of the ReLU was like this right either zero or values below and equal to one so this is uh the slope is one this is 45 degree angle uh for values of z greater than zero right this is the activation so what people did is they modified it to be either like this. You see the yellow line that I drew there? Yes. So in the negative direction, if you just add a little bit, now what happened? You see that the slope is not zero. This here is not zero. It is not right which is not flat but even for negative Z Lopez not equal to zero C less and that all part of the part and then the question comes how much is the slope suppose the slip slope of this traditionally is written as a you can keep this a itself actually a is for activation so let me use something else here i'll just put alpha well alpha is for learning rate this is what happens you start running out of symbols let me just call it b well b is for by c well in terms of it you can make p that's the of the learning. You can find out what is the best value of t. How much slope should you give to have the best learning? When you do that, you have the parameterized liquid. With the slope, you have the liquid. Are we together so far, guys? Yes. Alright guys, so we have to talk for about a little more than an hour and I'm done with gradients exploding and vanishing gradients and normalization, batch normalization. All of these ideas are sort of tied together guys, and in practical life you'll see its effect when you do the labs you see that do please try to follow through the code that i am giving you in the labs there's a i believe that there is a lot of learning to be had in reading that so read the code reach out to me if you don't understand something and i'll help you forward from there as if i have a question so other than the bad size what other factors contribute to the momentum of the day see momentum is a different thing when we use the word momentum it is not in the sense that you do momentum is suppose you're running forward and suddenly you see that there is an obstacle so you want to avoid that you want to deflect, move sideways. The faster you are moving, the harder it is to move sideways. You can't just take a perpendicular step sideways if you are running. Whereas if you are not running, in other words if your velocity is zero, you can easily just step aside. Are we getting this idea? Think of it this way. Suppose you are a poet. You are walking this. And suddenly this obstacle materializes. If you are going very slow, you can just step to this point. It's easy, isn't it? On the other hand, if you are going very fast, then you have momentum. You cannot get rid of this velocity so easily. You have to carry a momentum. And so if you try to make a step, you may make a step in this direction. Your intentional step might go this way or it might go this way, but it certainly won't come this way. You are more likely to make a step like this right the faster you are going the less you will be able to deflect right are we together yes so that is what is meant in momentum when you do the gradient descent path if suddenly the local the new batch of data is pointing you this way it is saying come here but your momentum is in this direction so your path of gradient descent will actually be it will be what it would have been let us say that your path normally would have been in continuation of it it will just become a little bit like this it will instead go here but it won't come all the way here it won't be in the in the location without momentum okay let me make this picture more visual it will be somewhere here right it will be a compromise it won't be where it would have been without momentum it without momentum it would have been here without any signal without the signal it would have been here without any signal without the signal it would have been here so let me say a b c without momentum you can go to b easily a easily with and without any uh you know the latest step pointing you to go to a if you continue to your direction you would have continued to c but what happens is you you take a step which is a compromise between the two. You end up here. So just to be clear, what is analogous in this computation? What is analogous to the speed? So what happens is when you take a gradient descent step, remember that I said that the weight next is equal to the previous weight minus alpha the gradient of the loss with respect to this isn't it the reality is actually more nuanced the reality is you take the gradient the current gradient but you also include the fact that you have momentum that so this this expression needs to be modified needs to be momentum aware so it won't be a step see this is a logical way to understand but this in many ways is not how it happens for example we talked about weight decay so weight decay itself modifies this equation into 1 minus lambda W minus alpha remember we did this W next and now what I'm saying is that the W next is also affected by the momentum term and momentum is nothing in many ways those of you who study the financial markets you know what it is suppose your stock price is going like this what do you do to get a sense of it you take the moving average isn't it you would say let's take the moving average and we will take this as a moving average would be something like this so now think about it what happened your moving average because of this data was here right let's say that this data point is a p the next data point is q have you actually gone to q you haven't you have instead q is telling you that wherever you are you need to turn down you need to turn south but you just turn a little bit south not not by a huge amount do you see the difference guys you didn't jump to Q this is your moving average and if you are used to stocks and so forth you must be quite familiar with the concept of moving average or the weather's moving average the average temperature for the month when they say they're looking at a 30 day previous window or something like that. So that is what momentum means. That's all it is. Okay. Thank you. Any other questions guys? Asim had a question regarding the last landscape medium article which you pointed out. In that article at one point they were doing some learning rate stress test. Was that more related to just figuring out what the lost landscape, how it would change with the learning rate? Or can we also do that in practice wherein they increase the learning rate and suddenly they decreased it and that actually helped to achieve the minimum? Dr. There was a beautiful paper written by a naval officer, engineer in the US Navy, which has become very famous. And in reality, what we do now in the world, that is the state of the art with learning rates. It is called one cycle policy. He wrote a paper which became famous, One Cycle Policy. Leslie Smith. Leslie Smith, right. Thanks for pointing that. So this thing says something very interesting and I'll just give a baby version of it because it's a topic we'll cover in depth in the part two. What it says is that you keep on increasing you keep on increasing your learning rate till at some point you notice that your loss begins to, not decrease, but begins to climb up, right? So you look for a point at which the rate of decrease is not in your favor anymore. And then you say that that point I stopped, that is the learning rate i will hold on to and continue so that is called one side there's more to it i'm giving you a basic intuition so basic thing is learning rate itself is learnable it becomes a learning parameter of the whole model itself. And it adaptively changes as you go step by step, iteration by iteration. So if you look at the course, you'll see that there is a topic listed there. I haven't surfaced all the resources for it. It's for part two, if we come to that. Mesak, does that answer your question? Mesak Ramanan Yeah. So the question is more in terms of, was that process which he was using where he increased the learning rate and suddenly decreased it and all, is that related to this one cycle policy or was it more related to him trying to visualize? No, no, no. He does not. I don't think one cycle was there when this. Yeah. So what he was doing is he was varying the learning rate and the visualization or showing how the loss landscape kept on going down into the valley that is right so see here's the thing learning rate is another hyper parameter of the model it is almost like do i do batch normalization or not what should be my batch size remember we had all those plots about different batch size and it led to broader and better um generally minima when the batch sizes were small when batch sizes are big so yeah I was I was wondering from more from the point of view of was that an actual process when we increase and like suddenly decrease a learning rate to achieve a minima or was it just him trying to show you which paragraph of the paper you're talking about. They call it the learning rate stress test. Dr. Raghuram G. Yeah. So all they're doing is they're trying out different learning rates. Okay. It was just for visualization. Dr. Raghuram G. It's not something. It has no reference to one cycle or policy and so forth. But the state ahead in other words and this is the magical thing guys with this field almost every week i feel that something new has happened something significantly new has happened anyway we'll deal with that guys i would like to end this uh all this vanishing and exploding gradients and batch normalization and this topic can i have a computer vision can i ask one question topic I have question regarding when is ingredient so earlier you were saying that if the weights are not in its lies properly it could lead to about the problem exploding as well as when is ingredient trend yeah weight initialization is important what do you want to do with weight initialization you want to initialize in such a way that you're essentially close to the solution you're going towards right isn't it so you're less likely to get trapped in the local minimum of all of them isn't it so that is one and so good weight initialization is important then other factors come in batch normalization Keep normalizing things use the proper activation function so for example if you use reloop or any derivative of reloop pretty much a batch normalization and See even batch normalization you can skip for shallow networks. But a proper weight initialization is a must. Like if you set all the weights to the same, you have a symmetric situation and the entire thing at each layer at just one node. That is one extreme. But really, if your weights are not properly initialized, the learning will be poor okay yeah so my question was regarding how to make sure that the vanishing gradient problem does not occur so basically there are it's a mix of approaches like a host of approaches now now we don't really worry too much about it the vanishing gradient used to be the biggest headache a few years ago a decade more than a decade ago now it's not a big problem now we are creating detailed networks that are thousands thousands deep and the only reason we can dare to create such very very deep neural networks is because we practically consider the problem of vanishing gradient sort. But nonetheless, from a learning process, you should know that if you use one of the sigmoidal activation functions, you're falling into that. Okay. So those are the trade-offs. Thank you, sir. See, with deep neural network, there are a lot of trade-offs thank you see with the detail network and a lot of trade-offs in thinking and that is why I'm trying to introduce you guys to these visual tools that aid your discovery of a good model a good learning model that generalizes well all right guys it is 852 let's take a 10 minutes break and then we'll start with a new topic i think this is a good time for me to stop this video entirely so that i can post two different videos i'll call this video batch normalization and exploding and gradients exploding and vanishing radiance and then we'll start with the other one.