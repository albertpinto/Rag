 Once again, guys, those of you who are here, please join in. Join into the Zoom. Can you folks hear me clearly in this room? Okay. Yes. Once again, welcome to the Support Vectors Large Language Model Bootcamp. We are going to do a lot of very interesting projects. Now for those of you who have been coming here from the previous courses, a small introduction. It hardly needs an introduction, but for those of you who are here for the first time, I'll give you some background. This bootcamp is the practical aspect of a series which can be done backwards and forwards. We have a theory track in which we did NLP with transformers, natural language processing with transformers, using the transformer architecture. And we followed it up with a neural architectures course, which was very much theoretical. If you remember, we covered more than two dozen research papers in that. We covered all the seminal research papers, starting with attention is all you need, the BERT paper, the sentence BERT, the BLIP, and all of that we covered in our neural architectures class. The neural architectures class was theory heavy because we knew that we are going to keep all the practical, a large part of the labs and practicals for the boot camp. Now, this is the other track, the boot camp. The boot camp can be taken either as the terminal course in the sequence, if you have done all the theory, or it could be taken as the first course to get hands-on experience, and then you can go back and follow it up with the theoretical background. So you can do it, for example, by saying, let me see how these things work. Once I know once I can fiddle around and hack solutions, then from the going back from the hackathon, I will learn later, why in the world did all this work at all right so you can take those one of these two approaches now the way this boot camp will go a few preliminaries are in order we are going to have 12 sessions 12 weeks 12 saturdays now besides those 12 saturdays your projects will not be finished so there is one extra month in which you'll be finishing your projects, which is why the total duration of this workshop is 16 weeks. For this workshop, we have arranged whatever within the means of support vectors possible educationally. If you are sitting here in the classroom, you might notice that you're sitting on the best chairs money can buy. These are Herman Miller Aeron chairs. You're sitting hopefully on a good desk with good electrical connections. You have access to one server per team. Each team will get its own middle tier server. So you are going to build production grade solutions. You will also have a shared, time shared access to a powerful inference server. Each team will get, there are seven teams, each team will get one day on one day exclusive access to the inference server to do fine tuning and run inferences. Now, if you don't use up all your time, you can do all sorts of trading of the time with other teams. You can trade it in, but it is your time now those servers that big inference server and all of this hardware is sitting in the data center room we are as we speak uh enabling uh vpn access it isn't yet enabled so while you're here of course you all have access to those inference servers but once the uh vpn access is enabled and sequal here is helping us with that then you guys can all connect to it now a few preliminaries make sure it is a responsibility of your team to make sure that access to all this hardware is not abused. So this is a terribly bad idea to do crypto mining or to share this access with your colleagues and friends. If you do that, you will lose access. Then you'll have to go buy your own servers, your own infant servers. And those infant servers are not, let's just put it this way, that if you have to buy your income server it will cost you 35 000 right right so unless you're willing to spend 35 000 i would strongly suggest guard this hardware as much as possible it is a community hardware all of us working in this place, get to share in the hardware. And it is our joint responsibility to protect this hardware and this data center. Right. And we will, we are bringing online, more and more hardware for your use in the coming days in the next two or three days you will each have your own box now each team along the wall as you can see there are names of your teams those are your designated rooms project rooms you can treat this as your permanent rooms for the next three months which means every Saturday you come here you can leave your stuff here if you want your cookies here some of it may disappear I might eat it during the week but you can keep all your stuffs and your grab and your laptops etc if you keep your laptop here leave it here let me know so I keep track of how many how much hardware is here and take some measures to safeguard it. And if needed, we can lock up your laptop in the data center room. If you leave them here permanently or any kind of expensive equipment, we can lock it up in the data center room. Now the another couple of pointers guys, it's a community driven. As you know, this entire project, this boot camp is a no cost, it's a break even operation. To keep the tuition low, and believe me, the tuition here is very low, a comparable workshop, some of you, as you know, in San Francisco, just a few miles away these things cost between 15 and 25 000 we are charging a fraction it's a break-even cost but it stays break-even because a we don't have a we don't have a staff here so if the restrooms are dirty yours truly is going to go clean it i would be the one cleaning it right or my wife samitha is going to be the one cleaning it in the two of us so please help keep it clean if you find it dirty call me i'll clean it like if you find that somebody has dirtied it it means cleaning definitely call me in i'll come and clean it the same is true for your individual rooms keep your project room as clean as possible but We don't want to attract insects of any kind. This place is very hygienic. There are seven layers of HEPA filters on the ceiling, which means that in case there is a pandemic of any kind, you're not likely to get it from this place. You're not likely to get it from this place. So every sanitary measure has been taken. You can feel safe here. We can only take precautions. Nothing can guarantee that you won't catch a bug here. But let's try. If you fall sick, if you have the flu, please don't come. Work from home. These are basic etiquettes. In the Slack channel, again again the usual rules of etiquette supply let us be cooperative let us help each other out fortunately in the eight years seven eight years of support vectors our slack channel community has been extremely supportive we have never had an incident of rudeness but i do say this thing every time that we are a family guys. We learn a lot from each other. As you will realize, a lot of the learning will come, not from me, but from what you learn from each other. The LLM Bootcamp Slack channel is already active and it has been active for weeks before the bootcamp started. If you haven't seen the messages, you would want to go back and check out those messages. Next, we have a course portal, which I would like to show you now and walk you through the course portal in the next few minutes. And with that, we'll get started with the project. Please give me a minute. Yes. So your course portal is at support vectors. Now, when you come here, you should see under your name, based on which courses you have registered for all your courses. Now, you won't see what I am seeing because you have been registered at this moment, only to the boot camp. Now, some of you may wonder what happened to our old portal. The old portal is here. And those of you who have been looking for quizzes, et cetera, it is accessible. I haven't made it publicly accessible yet. I will do that, the old portal also. Now, it is available at this moment at the IP address that ends in 44. 192.168.1.44 is where the old portal is and all the old videos and the old quizzes are. Now this is the new portal. It was time to upgrade the software so we have upgraded it to the new portal. This is your new course portal page. I'll give you a quick walkthrough. Those of you who are not used to using a learning management system and the younger generation is all familiar with LMS systems with Canvas or Blackboard or something or the other but some of us may not be so to give you an idea everything that we do in this if it is of value to everyone else I will share in this portal and likewise you can submit things to be shared you can share it also you have access all of you have rights to upload content which you can so please do that now one of the quizzes here was in person hybrid or remote now for most of you i inferred what it was if you haven't filled it do that this is the zoom link a link to your zoom link now i'll take this link out because by now all of you should know the fact that you are here means you know what the zoom link is there's a glossary of technical terms we have put some technical terms there we are going to keep adding to that so if you ever get confused what is this term called nlp or something like that you can always come and look up the glossary of terms here and again it's a starter one we'll keep on expanding upon it one of the requests we had is which you must do if you haven't done give you a programming of fluency how good are you with. How good are you with Python? How good are you with REST microservices and things like that? Why are we asking you to give this? The reason we are requesting you to tell us your programming skill is so we can make sure that our teaching staff reaches out to you and gives you help whenever you need it. Are we together? And so that's that. There's surveys. One thing that I would encourage you to do is this survey. This survey, I don't get to see the results individually. It doesn't matter. And there are no right or wrong answers. But what it gives me is, as an educator, you have a sense of the cognitive style. How should I teach? Are we together? Now, in education, there are various modalities of teaching and ways to put emphasis. What it helps me do is understand the audience, understand you as a group, not as an individual, but as a group. And with that, help me out. Announcements, I may take it out. This was just there. I'll post most announcements to Slack. These are for the permanent announcements, some things that you want the course portal to contain in the long run. Now, today's activity would be, preliminary activities would be, first of all, first one, this is an introduction and a welcome message. So these are the sections, these cards that you see are the sections. Today, we are going to do five sections. The first is an introduction. I encourage you to read it, please. The second is prerequisites. So let me go to the prerequisites. And in the prerequisites, what should you have? You should have a laptop with at least 16 gigs of RAM. Now you may say, why do I need a laptop when we are going to have so much hardware here, high end hardware here? The reason is you need something to connect to that hardware. And when you walk home, you would like to do some simple exercises on your laptop itself rather than, you know, on big gorilla machines. And remember, the big gorilla machines are shared community hardware right whereas anything you do on your laptop is just yours so there's value in that you need a basic familiarity with python there's a few who are not familiar with python at all immediately reach out to the teaching staff so we can guide you on that now one thing please i want you guys to do right immediately is if you haven't taken the quiz and half of you haven't taken the quiz please go and take this quiz the purpose of this quiz is is mostly for me i want to gauge what is the baseline we are starting from now in this quiz even if you scored zero it doesn't matter it is not supposed to be a measure of you it's not it's not school you know you're not getting abc grades its main purpose is i should know what is the baseline of this cohort what things i should teach what things everyone knows and i don't need to talk about and that is important as an educator so please do take the quiz and once again i'm not going to look at the individual answers but i'm only going to look at the aggregate and see which topics people answered almost everybody got right which topics people didn't get right so i do when i speak i will make sure that i keep introducing some background on those topics right now uh one thing just to know in case this rows of tables fill up we have some extra tables that can be pulled in from the back and chairs from the meeting rooms can be pulled in to create another row at the very back and it's it's really nice to see all of you once again a very good morning now the next most important thing is where's my tribe right you should have formed little tribes of your own you should have formed we call them tribes to make it more colorful but you will see it there's a reason i'm calling it tribe. These teams that you are forming, you will do this project as a team. But as you do it. Very different from my usual style where I do most of the lectures and guided labs here, I will give you I will come and help you after you have tried, which means that you will develop your own voice, your own style of architecture. AI is a fertile field. To do a POC in this field is extraordinarily easy. You can go to Hugging Face, you can go to a Medium article, and it seems that the impossible can be done in three lines right so for example this entire project read a document be able to search for things in it can apparently be done in hugging face in three lines can you take that to production can you deal with the traffic that deals with thousands of requests per minute absolutely not but real life is production you need to do inference at scale production. You need to do inference at scale. And my goal is that out of this bootcamp, this bootcamp is an incubator. My hope is that you will use what you learn here and use this as a springboard to do your startups. For you to do the startups and success succeed at it, right. So as you know, one in 20 startups succeed. So to make sure that you're not among the other 19. It is my job to teach you end to end the entire process of doing a startup. You will learn every aspect of it. Now there are two kinds of startup. One is the traditional startup. Three friends get together, become co-founders and build a new product. But startups, a vast majority of startups are the invisible startups. These are the startups that are done inside companies. You work in companies, you form a new project, a new team, a new initiative. And those startups are actually outnumber the visible startups that you see. So whether you're doing a startup outside or whether you're doing a startup within your enterprise this experience is to launch you end to end in building real world products that mean something are we together to do that i'm hoping that each team will create a solution that differs from another team. Each team becomes a tribe with its own voice, with its own culture, with its own architectural perspective, because our field is highly contentious. To do the same thing, you can do it in many ways. Even when you read out the wrong ways of doing it or the suboptimal ways of doing it, engineering is all about trade-offs. There are benefits and there are drawbacks to every approach. Nothing is perfect. You could have made the Golden Gate out of titanium, it would be lighter. But we don't because it's not cost effective. So every engineering project, large scale real world project is a trade off. Your team will make its own trade offs based on its own culture and its own philosophy towards this problem. These are large problems. Each project that you have is worthy of a starter. And I mean it, guys. If you do each of the projects well, you can take it further and do your own starter and we'll talk about it so those of you who haven't joined a tribe let me know and we will add you to one of the tribes there's seven tribes here their names are quite colorful uh i will you can see those i'll show you what the stripes are and what the current memberships are. So there is the Ai alchemists, which which is these participants. I'll just increase the the size a theater. Manoj who's remote. Manoj, are you there? Harini, Subrata. Subrata is traveling to India. Sanjay is here. Sheenath is here. Sohrab is here. So some of you are here, some are remote. The Agnight Nexus. Abdul, the two Qazis. Where are the Qazis? i'm not seeing them here in the afternoon okay albert is here rikash is here dennis rapal guys are here very good and then you have the andes mountains right and patrick's uh sanjeev ravine harry priya i'm red and us this team has scope for one more addition if you are not in any team let me know a good fellows is ah what is this this is your teaching staff this is me at the top and then these are all the people who are going to help you for the duration of this course right and there are a few people who are in dual role uh for example uh kashish and athira are both participants in the projects as well as they're going to help you if you need help right without without letting out the secrets of their teams then there is a uh there is the oppenheimers which has a couple of people who are remote. I think Madhu is remote. Aditya, you are here. Abhijit, where is Abhijit? Yeah, you're here. Nice. Prashant is here. Satyam is here. Jayashankar, all of you are here. So you have that. Optimus Prime. Optimus Prime is entirely missing. I believe they'll come in the afternoon. Yeah, and Tenser Titans are here. Shalini, anybody from Tenser Titan who has already arrived? Yes, you have all your folks are part of the Tenser Titans. Tenser Titans is the only team that I felt had almost all people saying they want to be remote or hybrid right so i put you next to the break room so that food would attract you to come here more often so then there's the Wasserstein Wasserstein's are Manish is remote so Sh Shinda and Arul is here. Dhwani, Mani, Ajay, all of you. Hey, one of you, could you call up Ajay? He may have forgotten that it's starting today. All right, guys. So if you haven't joined a team, please reach out to the good fellows and they'll help you join a team. Now, going back to the courses, team now going back to the courses uh this is your tribe thing guys this is the most important thing now the rooms that you have is your room for the duration of this bootcamp right which means that i will also hold you responsible for keeping it clean right please do that um if you don't then i'll have to do that now uh to help you guys i have added all the jupiter notebooks from the nlp with transformers course right uh one of the first things you want to do probably is download this folder. All of you are familiar with Jupyter. Go play with it. And a hint, some of you asked for source code to get started. You'll realize that the first project is very easy and some amount of source code is already here. So please do read it. Browse through this. And then the next section is the vector database ab initio this is your project for today guys we'll start on this now today we are spending quite some time introducing uh stuff but going forward we will obviously keep this introduction brief we'll just launch into the project let me give you an idea or a very quick background on what it is. Abhinish is of course from first principles. You may say vector database, oh, let me go get chroma DB. And I'm done, five lines of code. Or what about Lama index? Or what about this? Or what about that? Or what about hugging face? Do you think the project is that simple? How many of you read the project specifications? A few of you did, right? So the point is not to use existing solution, but to learn how to build it. Remember, engineering is about trade-offs. Those projects made certain trade-offs. What trade-offs will you make? What will be your voice, your identity in the technical landscape? How would you build it up from first principles from Ab initio? So you will build a vector database from scratch. Now vector database, as you know, has been a very popular topic. There was an incident here in Silicon Valley a couple of months ago, three months ago, two folks left Google and they offhandedly mentioned on the social media that they're going to create a vector database. And from what I know from, I think Praveen, you told me the story, immediately three venture capitalists started chasing them. They have not even declared how is it they're going to do. But already three VCs were ready to fund them. So well, here's your opportunity. There are many vector databases that are coming up, VWET, Chroma, Pinecone, some commercial, some open source. But trust me, as a person who has worked on database kernels, worked at Oracle, at this moment, these vector databases are at a very beginning stage. And they have a lot of missing parts. This is an excellent opportunity for you to ask, if I were doing a vector database, how would I build it? I've given you a design in the document, in the specification doc. You can loosely adhere to it. But come up with your own implementation. I suspect I've revealed far too much in that document so that it's now almost obvious how to do it. It was the first project, but in the subsequent projects I'll be revealing a little less generously, so that your creativity comes forward. And of course I'll be, as you know, you're not hung out to dry i'll be visiting each of your rooms and talking to each one of you and giving you guidance on a one on an hdm basis throughout the day and throughout the week right so this project first is he had a few things to do what the first activity is go and explore chroma chroma db it's an open. So if you have never encountered a vector database and you ask, what is it? Go play with it and you'll have a sense of it. That's your first thing, guys. Please do it before noon. We'll meet at noon. We'll do a very quick break out at noon and we'll see how far it goes. Now i want your first impressions on chroma db as you play with it let us all share what we thought of it like what strengths we saw what weaknesses we saw right what we liked what we didn't like we'll talk about that then read the project specification so chroma db let me just go into that. This is it. Play with the open source database. Did I forget to put a link? Yeah, it seems I didn't put a link here. You can add your links. Now there is more to it. A couple of tips, URLs that are good hints for you that you'll refer to, I put it here. The main document that you need to read is this one. And this document is here. I would like to walk through this document. Let me do that. Bootcamp, this tool. this is the specification of your first project program let me if you see a document with this colorful picture you're reading the right doc now soon this portal will have a lot of documentation now this documentation by the way is a live document which means next week's project will show up in the same document so this document will keep getting updated are we together? It will forever remain a work in progress. Now, this thing together. The one point I want to emphasize is this is a team effort guys. The biggest thing that I noticed, see I've been in the industry now for 30 years. In this time, I've been in leadership role half the time. I've been, as you know, before I got into support vectors, I was the chief architect, and a senior vice president of a pretty large multi billion dollar company. When you do that, you get to lead lots and lots of teams, your teams that you have built teams that you inherit, right teams that come to you through acquisitions, you get to do all of that and you get to see the dynamics in different teams and you see what will happen when i say about tribal culture i really mean it each team develops its own voice in a tribal culture and you can see things that work in a team and things that don't work. When teams work well, it is like this. You see the picture at the bottom? By the way, this is one of the Yankees. Casey Stengel is, as you know, one of the Yankee coaches and he used to say, getting good players is easy. You can try your best and recruit the best, absolutely star talent. The trouble is and recruit the best absolutely star talent the trouble is you create a team of star talent and it's a complete disaster i don't know if you know that right it's a really bad idea to do so so getting them to play together is the hard part because they won't play together okay great teams are made out of people who want to play together Great teams are made out of people who want to play together. Otherwise, it reminds me of Leo Tolstoy's quote in Anna Karenina. He started his book, his great book, Anna Karenina, which I think was the cause of his Nobel Prize of Literature, with the famous statement, All happy families are alike each unhappy family is unhappy for its own reason in its own special way it's unhappy so um you see that as as leadership you see it in companies wherever you go you see that the dis the dysfunctional teams all have a special reason why they're dysfunctional. Right? But if you look at successful teams, high productivity, you know, heavy hitters, you notice that they all somehow have a commonality. They all function very alike. They have different voices, different cultures, but they just move fast. They move like this drawing here. So I hope you guys will give today to bond with your team and really move fast because it is the team that will carry you through this project. And hopefully that team will help you be the startup that many of you dream of being. be the startup that many of you dream of being. And so with that, today's topic is vector embeddings or vector databases. So I'll talk a little bit of a theory and then we'll finish and then we'll get started with the projects. See, search for the longest time was keywords based. What it meant is you would have a you would take a document find all the statistically significant words in it so for example if you found the word golden retriever let's say you found the world retriever right or you found a word puppy or something like that, you would create all the words, you would have a vocabulary. And every word of the vocabulary would be a key to a hash table. And the value to the hash table would be every document, the ID of every document that contains the word that you're looking for. For example, if you say retriever, then every document that talks about retrievers are there as a list. So when you search by keywords, all you're doing is, to the first approximation, in a very simplified, conceptual way way what you're doing is you are just looking up that dictionary it's called an inverted index you're looking up the inverted index and finding out where do you which documents have this right and even that list now it gets more sophisticated because some documents mention a word keyword many more times than others right so you would put that forward in the search results, something that just mentions this one would be later, then you bring in synonyms and this and that. So things like leucine, which is the foundation for both SOLR and open search and elastic search and many others. If you go back and look into the foundations of that, you see that it's an inverted search index and they have really taken the idea forward and made it very mature. In whichever company you're working or academy you're working, you probably have an inverted search index or a keyword based search index running there. It is the de facto industry standard. It's everywhere, right? There is a problem with that though. Human beings don't think in keywords, right? So when you want to explain to your child what you're looking for with a phone, like you say that, you know, there is this book which talks about this, and I think it has a red cover. Right. Go to this shelf in the living room and you probably will find it there. You realize that you're not talking about keywords. You did not mention a word in the title. You mentioned its color. You mentioned its location. That's how human beings think. its location that's how human beings think so keywords is something is a very contrived way we are not evolved to think in terms of keywords a resource of learn it's a learned behavior to search by keywords isn't it and it is easily rigged as many of you who are into web development know search engine optimization is a cat and mouse game. To improve their website ranking, people stuff their websites with keywords and then the search engines try their best to mitigate this nuisance. So what is the alternative? It is in this space that when the Transformers came about in 2017 December when attention is all you need paper came out people knew that the world had changed in profound ways like people returned from the holidays and in January 2018 the whole world knew that the world had that we were looking at a new world in the air in natural language processing and And there was rapid progress. One of the earliest results was the bird architecture. And immediately as a consequence of bird came the sentence bird, which used contrastive learning. And it learned from triplets. What it did is it said if every text can be made into a vector. And by the way, the people use the word sentence in natural language processing computer science, it has very little resemblance to a grammatical sentence, a grammatical sentence is a punctuation Marx's boundary, isn't it? But in natural language processing, or in this computational linguistics, a sentence could be anything you want you, you mean it to be. It reminds me of Alison Wonderland asking, what do you mean? What does a word mean or something? And the response was, a word means exactly what I mean it to mean, nothing more and nothing less or something like that. So sentences like that, it could be a paragraph, it could be a genuine grammatical sentence, it could be just a word, it could be paragraphs of text. So be aware of that. So given a sentence, so just think text. Right, but not encyclopedia, the whole of it. Don't think of that as, that's taking it a bit too far. So given a sentence, machine learning doesn't deal with text, it deals only with vectors, right? What is machine learning? At the end of it, AI boils down to linear algebra, probability theory, and vector calculus, isn't it? So none of which has a place for words. So we need a bridge from words, from text, So we need a bridge from words, from text, to vectors. Right? Because machine learning models deal with vectors and vectors only. And it's generalization, tensors. So the thing is, you can take a neural architecture and you can create a vector. You can just create a random vector out of it. But what people do in sentence birth is they take triplets. They'll scrape the web, they'll take a sentence and they'll take an adjacent sentence or nearby sentence, which is hopefully similar, or they'll take a sentence, the cow jumped over the moon, and they'll take a similar sentence, right? Whatever it could be. And then they'll take a completely random sentence. So they'll create a triplet reference distance between similar sentences and maximize the distance between dissimilar sentences. Does it make sense? And when you do that with just this simple bit of logic, practically magic happens. This thing, this neural architecture, this transformer will magically somehow begin to grasp the semantic content of your sentences. So two sentences which use entirely different words but mean the same thing end up at the end of the neural training being proximal to each other, whereas the sentence and its random counterpart would be far apart. Right. Now these vectors are in very high dimensional spaces. High dimensional spaces suffer from the curse of dimensionality. You can't use Euclidean distance, it's not advisable. Why? Because, as you know, as some of you know, in Euclidean, in high dimensional Euclidean space, every point is ridiculously far from every other point. So the question of nearness gets somewhat diluted, or actually completely diluted and squashed. When everything is really, really, really far away from everything else, you're talking about degrees of farness rather than degrees of nearness. Right? So a better way is the way we look at stars. When you lie down and you gaze at the stars, you say that this star is near that star. What do you mean? You literally mean that the angle between them is very small, isn't it? They're practically in the same place in the sky. They look to be. So you're looking at the angular distance between two stars. Am I making sense? When you say that I'm looking at Orion, and if I go to Sirius, I just need to turn this way from Sirius a little bit, right? You know, you just say a two or three finger lens from Sirius or from the Orion's belt, I'll get Sirius. What are you saying? You're saying angular distance. So it turns out that in high dimension spaces, angular distance is your measure of distance. That's what you use. In reality, two stars, which have very small angular distance may yet be very far apart right but it turns out that it is it's good enough to train a neural network and get semantic invariance so those vectors that you get are vectors that mean something and so if you create a space if you take all your documents and you shred them into a chunks the size of it because sentence encoders they can take sentence of only so much token length so many words or word pieces then so anything that exceeds that you need to break it up into chunks you do that what do you do and you keep projecting it into this uh into this space into this vector space then all these points, they become little things in that vector space, right? It's almost like a metropolis, where people live, except that these are points living there. And each point that is there, in a way, it is the meaning of that sentence, isn't it? It is not the sentence, but the meaning of the sentence. Because if you take another sentence, which means exactly the same thing, it will land close to this guy am i making sense that's about sentence embedding so that's why uh to be a bit i took a bit of poetic license and i called your chapter heading the latent metropolis of meanings right so uh please do read it. Now, when you, I just sort of walk through this. This architecture that I'm proposing, the first thing I must give you a caveat and tell yes, it is your first project, it is a starter. You can critique it all you want, because it is not a perfect architecture but it works and it is the simplest one that you can start with and you can accomplish like as they say a crawl walk run let's learn to crawl because we are going beyond pocs beyond those three liners and prototypes we are building something for real even this simple thing to build it for real can easily keep you entertained for a whole week and kudos to the team that finishes it by end of the day right it is possible you can do it but i would like to see it at production scale okay and we give you all that and guys i, I hope you appreciate this. In your workplace, you won't get to touch production grade hardware for development. How many of you are given like a 1,000 core machine or a 100 core super server with one terabyte of RAM or just things out of the gate and say, hey, this is your development machine. Go develop it and try it out. You don't, right? You get some lame laptops so use this use this opportunity guys and use this opportunity uh read through this doc so the first process is to convert it into vector embeddings then you once you have done that obviously if you if you want to search for something all you you do is take the query text, go into the vector space, find its neighbors, those are your search results. As simple as that. Then, what is the architecture? Now, one of the demerits of this semantic search, and by the way, when semantic search came, this kind of search came, it took the world by storm. came, this kind of search came, it took the world by storm. Right? It completely, should we say, toppled the apple cart of the old search. You had to rethink this in the enterprise world, because it just works so much better. I can attest to that. I come from the learning industry um card store we used to have 100 million learning objects or think of them as textbooks or videos and audios and so on and so forth to search for anything in there or any course that was meaningful was we used to use the usual elastic search was not effective when when by the way i came to that company to acquisition of my startup so when i came we were the ai team one of the first things we were given was to do this with ai we took this as a project this was 2018 and 19. all of these things were breaking new ideas then we built actually before those days when these things are not so commodity they are available in the open source. We implemented the entire thing from scratch ourselves. When we did that so I'm essentially taking you folks through the same journey. Right. When we did that, our clients just couldn't believe it. Just couldn't believe it. What happened. And paradoxically they got angry. So if this could be done, why did you make us suffer for all these years? Right? Literally it comes to that. But it feels like magic when you see the transition from inverted search engine, whatever the search engine is, to semantic search. It just works like magic, especially because you can talk to it. You can say, you know, what is that animal that has a long neck, right? And a cute face. And believe it or not, it immediately pulls up pictures of documents on giraffes. Whereas in your query, nowhere did you mention jr. That's the beauty of semantic search, but it has one demerit. Its weakness is what is the strength of the inverted search. Inverted search you can do with one keyword, one word, and it will pull you good results. One word is hardly enough context to get the semantics out of your what is it that you are looking for sometimes. So when the number of words entered is few, it turns out that in these occasions and this is just one of the many occasions where a traditional search, keyword search, beats semantic search. So what should you do? The official search, keyword search beats semantic search. So what should you do? You're sitting on the horns of the dilemma. Should you use semantic search? Should you use AI search? Do we do case by case basis? Or what do we do? Any ideas? You combine, you hybridize both of them. You get the search results from both of them. But now you have two sets of search results. You have long list of them. You need to re-rank it. How can we re-rank the results. And you would use a much more sophisticated transformer than just sentence embedding, because this time around you are literally comparing the query against each of the candidates' search results. And coming up with exact similarity there and then re-ranking it. And that is your project, guys. Do it like this and you'll be surprised. I don't know how many projects are actually doing it like this. In fact, I don't know any one of the open source project is doing it right. I mean, doing it to the full extent. Prove me wrong. I would love to be. And guys, this is a collaborative thing. I will learn as much from you as you will learn from me and go ahead, Albert. So we do them. And then how do you do the elastic search and combine them. Oh, you put your text, you put your text both through the embedding and jointly into elastic search. So you do two activities in parallel. Right? That is it. So the traditional activity happens as usual. You take text and you just throw it into Elasticsearch or by the way, Elasticsearch, Solr, OpenSearch, pick yours. I'm just using Elasticsearch as a proxy for all of them. So basically your documents that you have, you crawl them through? Yes. Yeah. We can do that. Sir. Please go ahead. Asif, you talked about the sophisticated transformer for re-ranking. Can you tell me about that please? Yes. See what you do is, look at BERT. Are you familiar with BERT? Yes, yes. Yes, I am. So BERT takes two sequences, two sentences, isn't it the first sentence make it the query now you have a few candidate search results some coming from elastic search some coming from your search compare feet each of the going on 11 yeah guys can you please so so okay guys so um uh when you so imagine that you have a query uh are you are you able to see me you have a query. Are you able to see me? You have a query and you dip into and get one of the candidate search results. And now you feed it into BERT and you ask BERT how similar they are. Right. So you have trained that BERT for sentence text similarity search. These are called matching laws. The last function that you have These are called matching loss. The last function that you have is for a matching loss. How much is this sentence matching the sentence? So it will come up with a score, a probability, right? Or a number between zero and one. Let's say that you have last layer is a largest unit or whatever, you come up with a number and then you use that number to rank you know sort through all your candidates does that answer you it's as simple as that these are called cross encoders can you repeat that i'll say that again see you take your query sentence and you you got some results from elastic search you got some results from elastic search, you got some results from your semantic embedding. You take one of them, put it next to this, or feed it through BERT-like architecture and coda architecture. And coda architecture takes two sequences, two sentences. But what have you trained this model on? It has been trained on a loss function which optimizes, which calculates the similarity between these two sentences. Right. So it will come up with a matching or similarity score. Now you can do it with the next candidate and the next candidate and the next candidate and you will get for each of the candidates such, you'll get a similarity score to the query. Right. You sort by that and you produce the top end. Is it. Are we making sense, guys? Are we understanding this? OK, so that is called a cross encoder. So you cross means the query and the other guy. So freeze the model. Yes, you freeze the model. You don't you don't like inference time all the models of us. We add more elements in your search in your search database of our. No, it doesn't matter because the cross encoder just looks at similarity. It is, it is, just think of it as a, see when you do vector embedding, it is fast, right? But it, you have lost something in the sense that the, in the sentence word similarity that you, I mean the, the, the, the full bird which takes two sequences and does a matching loss, is actually far more accurate. But computationally it's brutal. So the whole semantic embedding movement came about because this was cheaper. So what you do is you use a cheaper method to get a lot of candidate search results quickly and then now you have only let's say 50 results and you're comparing your query only against 50 results to re-rank it and then you use the full bird to do that but the bird has been trained for full similarity that's the purpose exactly that's what you do go ahead the view exactly that's what you do go ahead so sometimes it would be and sometimes it could be that the uh the elastic searches are came up and therefore it was meant to be answered to okay keyword based search and therefore it's yes that is right that is right and then it will get deprioritized it will go down the link but we are not looking across keyword search and and it's not no no no see imagine so imagine that these results are elastic search results and these two let me this is my breakfast these two came from ai search now you put them all in a box and take the query and compared to each one of them, you don't care where they came from, I mean together you don't care about that so you just. R. Vijay Mohanaraman, So as if the output of this will be will have the intelligence of both semantic and keyword search. R. Vijay Mohanaraman, Exactly, because it is doing something beyond both of them it is using a more full transformer which does a much better similarity between a relevance comparison matching loss but the reason you don't use it for first you know you do it in two stages is in the first stage you want to use sentence embedding is far cheaper this this kind of a query against a million documents is brutal because the inference time will kill you see if even if it takes think about it this way even if it takes 10 milliseconds for you to do uh this high quality comparison right you realize that you can do only 100 comparisons a second right which means that it will take you 10 000 seconds to respond to a single query and that's the point that's why you keep the best for the last got it so so Got it. So, so, so go ahead. What makes it that the approach that you just told me, what makes it less compute intensive? What makes it less? Yes, very good question. See, what happens is when you went and got the grapes, you got only this tiny little bit of elastic search results right let's say 20 right or 10 and then you you went and got some 10 of the ai results semantic search results so now you're not doing even though you dipped into a vast ocean of 100 million documents you came back with a short list of only 20 documents and you're doing this very high quality comparison of a query against just 20 documents. So let's say that each comparison takes 10 milliseconds. Right. You're looking at 200 milliseconds, which means that you're responding to a user's query in 200 milliseconds with very high quality results as opposed to if you were doing it just using the high quality guy throughout it could take you 10 000 seconds right which is not feasible so one second so that was the big breakthrough actually the big breakthrough that we have is that now i must say and i know that some hands are waiting for a question please hold on a minute uh one thing i must say guys that that this is the real search architecture do it this way but not many people do it fully right everybody is in a hurry to show hey look i do have a search engine right or something which is good to read medium articles which is good to do medium articles which is good to do and it's good to learn but it is worth doing it right when you do things in real life for industrial scale remember at the end of it you're a startup you have so much runway so much capital you don't do it right you'll be toast just what is good for demo is not good for going live. Believe me, a lot of things work great for demo, don't work in production. Do it live. Now, you may say, hey, my semantic search results are very good. Just semantic embedding results are very good. Why do I need a cross encoder? Performance. Because sometimes it is not good, right? Sometimes Elasticse search beats it and so you want to do best of both and even if you didn't use elastic search remember there is a high quality search comparison matchup with you why not use that too in the second phase to re-rank anyway Right. That's it. Satyav? What do you say about the quality of the output? Let's say if we had internet resources and we could actually do bird and have the time of the world. What would be the difference in the quality of the output? If I ran the items and I did... Almost nothing. People have done experiments and not only that, Satyav, it's not even if nobody in the world has so much money yeah that they can ever create a search engine purely based on the cross encoder. It simply is not possible. Yes, and that is right. In fact, completely compared. completely comparable right uh go ahead so how do you quantify so when you say results oh that is good very good question jadav here asks uh how do you know the quality of your search results right so i'll give you some simple ones and i know some questions i'll just wait a minute uh the answer to that, think about it this way. Suppose, see, these are the standard machine learning metrics. One of them is, think about recall or the hit rate. What it is is, suppose you know that if this is what I'm searching for, these 10 documents are exactly the one that should show up in the search results. So now you look at the search results and see how many of those made it there. Recall is how many things that should be there are there. High hit rate, high recall means you've got it all. Then the other is the precision aspect. Precision is the ones that you got, how many of them are actually relevant and not irrelevant. Because see one easy way to increase recall is just indiscriminately go scoop everything you want, everything you can, right? But that won't help you because it'll pick up a lot of junk. So precision is the other way, but I will let you. So those are traditional machine learning things, but search has its own special like when you do information retrieval. And in fact, this project is a good trigger point for you to, in your team, discuss what are specific to search. A hint, it has to do with the ranking. The ranking metrics. So those are points to investigate and learn. And in a way, the reason I'm not telling you the full answer is because then you won't go and read. Right? Albert, I think you were next. Yeah, you talked about the rankings of Elasticsearch and AIsearch. So are you going to look at the rankings of elastic search and the AI search. So are you going to look at the rankings of what elastic gives you? No, you ignore that. You ignore the rankings that these individual indices give, individual elements give. And pass that again to another transformer. Yes, you pass that. Just one big collection watch me no you see the Yeah, see, when it comes to this one good news is the sentence embedding the vector search is lightning fast, because the approximate nearest neighbor search algorithms, which we'll talk about in a moment, have they have been around now for decades and things like fast scan etc are just industrial strength magically fast so the question is do you really want to keep a giant index in memory right sometimes you do if you have enough memory you can keep it in memory the entire index not just the search result of some previous queries you can keep the entire index in memory it the search result of some previous queries. You can keep the entire index in memory. The vector indices are not big. I mean, by today's scale, you know, I mean, today, what is it? Today, people don't even bat an eyelid before they stick in 2 to 10 terabytes into their server, single server, right? As RAM. So you can keep the entire index there. And you're searching the entire index for that. Okay, guys. So any more questions? Ask if I got one question. Sure. As if I'm not 100% clear as to how to present the both the results together and and sorry if i missed some things in between but it's it's not perfectly clear in my mind one way i would think is to kind of do it very similar to how for example google does it today where they present both the results as part of the board on the top and then rest of the results in the bottom but is there um you can do it there's a latitude manish i am not very specific on how you show the search results right but just show it in the relevancy order. Manish Parashar, The most relevant first but and, and this is the, this is the creative license you have how you design the user interface. There is a reference user interface design that we have posted that you'll see here. But beyond that, and in fact, you'll see that. Okay, so this is about I'll come to that. So how about that? As you do it, I'll come and help you. Your team will help you and it will become clearer. Would you zoom in the document a little bit? I think it's hard to read. Okay, sure. Let me zoom in. So, guys, I'll talk about the architecture, but before that, let me take a few questions. Patrick, you had a question. So this ranking crossing figure, is this trained specifically just for your data and sentence loss or can this be transferred across another set of things? It is a matching. It's just sentence matching. So anything has been trained on matching loss. So you can use it for many things in fact you should a chunk should be compared to query not the whole document right yes absolutely in fact i expect you to use fast or scan right yeah don't try to create approximate nearest neighbor on your own yeah it's very tough it's tough so all right guys remember this is your first project so it is easier than you think right but you will learn how to do things at scale properly so what are the pieces that you need this is your architecture when you take a document you will shred it you will extract the text from it because documents can come as pdf as microsoft word as you know uh html and markdown god knows what format you need something to extract plain text from because plain text is the journey is the midpoint station on the way to becoming vectors. So all the sentence encoders work with plain text. How do you do that? I would highly there are many, many ways of doing it. The moment you bring about extracting text from documents, most people quintessentially think PDF and immediately they start thinking of all sorts of Python libraries to extract this from here. Now here's a suggestion I will give you. Text extraction is not something you, NLP is an old domain. There is a very, there's some very mature tools. In particular, Apache Tikka. Apache Tikka has been there for decades now. It has matured. It is battle tested. It's used at industrial scale. And it supports a thousand document formats. Just ponder over the thousand there. I didn't even know that there are thousand document formats. Right. But it supports a thousand document formats and it auto detects your document. What it is, PDF, HTML, what it is. Really, you think that your little other Python library will be anywhere close to this? I don't think so. So if it is, I would love to learn about it. But I would strongly suggest use because you're building something for keeps for real use Apache Ticker. It has it has a Python wrapper. Now, I would suggest you do it using three languages for this project. Python is something most people are familiar with, you could use that. You could use Java or Scala. You gain a bit of performance if you do it in Java and Scala. And when you do that, imagine that you're not parsing one document in your mind frame. And this is a question I'll keep on coming and asking you and your team. How does it do when you throw a million documents right so you can write a little script in jupiter to deal with one doc are we together but jupiter notebook doesn't scale to a million dollars in production environments so what is it that you need? You need big data technologies, data engineering technologies. For that, PySpark. Use Spark. If you are in Java or Scala, use Spark. If you're doing Python only, PySpark. It should be a tool. You could use some alternate. There are a few big data tools. If you don't like Spark, use something else, but use something of industrial strength scale. Right. You need to, I mean, this is the learning guys, this is taking AI to real world. Then, store the original file, store the text here. So you need to do a database schema design. I'll give you guys a hint on the schema, but I want you to try out what it is. The big hint is it needs three tables minimum. So that is almost a giveaway hint. Then elastic search, vector search, of course, is the architecture. So what you would do is you need a microservice. If you're doing it with Java, obviously, I would start most likely you're going to use Spring Boot or quarkus i favor quarkus use spring boot or if you feel like using raw java microservice by all means jacks rs go right here on microservice from scratch do that if you're doing python i would encourage you fast api is a pretty good choice I would encourage you, Flask API is a pretty good choice. And you could use something. Some people say, no, I want to stick with Flask. Do that. It's slower. Flask API is really scaled out and proven in industrial environments, scaled out environments. Or use whatever. If some new library has come up, use it, share it. We'll all learn about it. So that is that. So that's about your data pipeline. Now you have all of these jobs, how do you stitch them together? You won't be running those things on the command line. You can make, if you want, cron jobs out of it, but better still, use a framework, a pipelining framework like Jenkins or Airflow. If you're not familiar with jenkins or airflow ever if you have never built a pipeline now is the time to learn to build it now guys i'm deliberately using a general purpose pipeline tool jenkins right let's stop airflow now guys when you do ml ups as we forward, we will use and get introduced to other things like a Qflow and Ray. But to keep today's lab simple, I want to start with simple tools. So if Jenkins is too much for you, for today's project, it is okay to do it by hand. Right. But going forward, have a good mature pipeline to execute your jobs. Jadhav is ahead. Jadhav? So could you give a sense of the responsiveness of this application, for example, a second? I would expect minimum 100 queries a minute so each query should return in sub second erase no you will get a you'll get one server and one inference machine they should work on that server and the inference machine so your cluster it is a powerful machine. Right. But it is one machine. It is. Yeah, your architecture should be scalable to a cluster. Then on the API, we go with GRPC or something more responsive? Absolutely. So very good question. Anubhash raised the question which I'm delighted to hear come up. The end user, like the client, will talk through REST because that's a lingua franca of the web. On the other hand, how should the microservice talk to its individual pieces or internally if there are multiple components. I would suggest the gRPC is more efficient. It uses protocol buff for the data format. If you know about gRPC, use it. If you don't, just use REST. So the choice between REST and gRPC is up to you, your team. And this is where I spoke about the tribal culture. I would like to see a variety of implementations and architectural choices. So that is that. That is your data pipeline, guys. The three main jobs. What are the three things you will do in this project? First is you will, in this project, plain text extraction. We talked about it. Use Tika, Apache Tika. By the way, ticker does it in two lines right that whole intimidating ginormous piece of technology in python shows up with just two liners or three liners so there you go right then you need to chunk it right and be careful you don't want to chunk in the middle of a word or the middle of a sentence so do intelligent chunking i would like to see how you do it and then so you take a document you pass it becomes plain text you chunk it you get chunks you persist it into a database right so what do you get you get chunks plain text chunks you store it into a relational database now you create sorry i got a question so after you have uh done the task of document parsing and then doing the semantic chunking on it now semantic chunking request that we do this in a certain database format. And given that it's a text, there are no kind of sick columns to do, right? As in there is only one column that you would store it in, right? No, I mean, see, given a document, you have the full text and then you have a list of chunks. Right. How you store it, you have freedom. You can do it relationally. This time I require you to do relation because some expertise in relation is a must in AI projects. It is not the best choice. You could use a document oriented format like Mongo, Elasticsearch, etc. You could use Postgres, which has support for both. What you use, you have a certain degree of latitude. It's pretty much your choice how you save it. Okay. Yeah, I was kind of experiencing some issues last night. Maybe we'll discuss it offline then yeah sure and I'll come to each routine yeah and we'll uh I'll help you uh get over those issues okay so this is the process guys and then vector embedding treat it as a separate job now why shouldn't you do everything in one job see this is basic thing complex stars shouldn't be because if it fails all of it fails right so in the industry, you always do things in small pieces, so that you only redo the piece that fails and you but fix only the piece that keeps failing don't go hunting everywhere where did it fail. So it's called the single responsibility pattern in the world of architecture srp says that one job or one class for example should do only one thing well it should have only one responsibility so text extraction should be its own responsibility chunking should be its own responsibility uh vector vector embedding should be its own responsibility and then finally indexing should be its own responsibility so create different jobs sir sir uh uh sir maybe a stupid question but are we still talking about vector database ab issue project? Yes, yes, yes, yes. Believe it or not, and you're going to finish it before 4pm. We have one week to do it. Okay. Can you explain what is chunk here? Oh, chunk means, if I have a text, let's say that is made out of 20 paragraphs. So each paragraph could be a chunk. You just the reason you chunk it or you break up the text into smaller chunks, smaller pieces, because this you will see that this AI model... You lose the semantic or you lose the context. You lose the global context. Yeah. But you retain the local context. It's a price to pay. You can't help it because these uh transformers they have a finite finite length of text that they can see so it's a fact of life go ahead any tips oh no no no no tips that you should have to figure out on yours right okay the search index after that so guys this is the process schema design uh now i'll be brief and finishing. Schema design, guys, I will be releasing a reference schema design in a couple of hours. But before that, I want each team to have on their whiteboard a schema design for me, because I'll be coming to your rooms and seeing what schema you designed, right? And object-relational mapping, if you're using Java, Scala, once... Oh, Mohsme, am I waiting for you? Go ahead. Yes. Yes, yes, you have to do that. Yes. So then object relational mapping, guys, you don't want to do prepared statements and SQL query. So if you want to do it, do it it most people use some form of object relation mapping. Anuj Malakasembele, Ph.D.: Right and the object relation mapping in the Java world, which started this whole movement, the big gorilla here is. Anuj Malakasembele, Ph.D.: A what you can tell what's a big object relation mapping technology in the javas color would hibernate exactly hibernate which is has you can do genvc's faster how can you can be tuned actually to be low latency but yeah anyway your choice and if you're using python then a much more recent heritage okay and in fact i kind anas who is probably anas where are you remotely okay oh yeah there yeah so to introduce us to it is the sql model sql model is based upon it's taking pydantic most of you are familiar with pydantic for data models data beans and right uh it puts it together with sql alchemy is done by the same people who did by i believe who did by that right s. SQL model all those links are here. You see my screen? SQL model. So guys, there is a lot of learning of how to take things beyond a medium article into full-blown product that you'll be doing after that. Write your search service. I won't talk about it. It's easy at the end of it, this is some simple UI that you can build. And do you notice that the entry to that interface is not keywords? Observe, guys, that you will not be entering keywords. You'll be talking like a human being to this machine. So create a machine, create an appliance that actually talks to human beings in an intuitive way this time just produce search results your next weeks the coming projects will be will build upon it at this moment you are talking to it and it is producing search results in the subsequent project you will talk to it and it will talk back to you in words. And in a subsequent project, you will talk to it as in voice and it will talk back to you in a voice in your favorite accent and language. Right? So it will be a fun journey. We'll do all of that. And thus, Anxia project. So as I said, at at some point it will speak back to you that's the next project it's a preview of what's coming and with that guys i'll let you uh let the project begin please do study this dog if um it is worth studying the dog i don't know anybody who has read the dog would conquer with that you updated it but i i nice very nice very nicely oh thank you very easy to understand yes and hopefully i've given enough hints for the project guys but i'll be coming to your individual rooms and guiding you lunch will be here shortly um you'll have lunch snacks are here you will remain well fed and hopefully you'll go back home three months later 15 pounds heavier. Alright guys, so any other questions? Otherwise we'll end this. Alright, I'll stop there.