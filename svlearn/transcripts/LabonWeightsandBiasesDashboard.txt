 So guys, what I want to do today is introduce you to something that has come around these days as quite important. It is the ability to see your machine learning models being trained, seeing the loss function, making plots of the loss function and so forth. So the question is, is it at all possible that these would be easier than writing a ton of code and building a UI on top of it. Now, if you notice in this project, I've given you helper functions, I've given you loss history and the loss data object. And using that, we make the loss plot, we plot the loss. It is baked into the library that I have given you. Now, generally what happens is once something becomes necessary like it would be hard to argue that there are situations where you don't need this where you don't need to have monitoring of the loss function or things like loss over time over iterations and things like that monitoring of the gpu cpus and so forth so as you do deep learning one of the things that becomes of a rather significant interest is monitoring the hardware not just monitoring how how your machine learning algorithm is learning and by the way it's all of machine learning, not just deep neural network. How well is your algorithm learning? But how much of the underlying hardware is it consuming? Or how well is it utilizing? So that brings us to the world of metrics and measurement and tracking. It is a very important part in the lab aspect of it. It is something that I've been meaning to get you guys introduced to for quite some time, but we haven't had an opportunity. So I thought today we will do other topics too, time permitting, but one of the core things I want us to do and do a lab not just me talk and show I want you guys all to do what I do and create your first dashboard and in effect your first report kind of thing we will use there are many ways that we can do it the two de facto tools that are both worth using together are TensorBoard and which is from Google and it weaves very naturally into Jupyter notebooks and in a PyTorch code in which you can see your model the graph of the model what it looks like execution execution graph, and you can see the actual execution over time of your neural network as it is getting trained. So one cannot understate, one cannot overstate rather, the importance of being able to use these boards dashboards tensorboard is a very important one of them and the second important one in this category that has emerged is called weights and biases it's an interesting name i suppose it refers to the fact that a neural network all the parameters are weights and biases in the deep neural net. So it's called Weights and Biases, W-A-N-D-B. And what I would show to you is how to use it. What are the steps needed and what does it look like? So first I will walk you guys, today the way we'll do it is, first I'll walk you guys, I'll show you the features. I'll start with weights and biases for today. I don't want to cover both TensorBoard and this. TensorBoard we can keep for another session whenever time permits. That will be much quicker if we understand one of them. I'll take weights and biases today. that if we understand one of them, I'll take weights and biases today. And we'll first take a tour of what this is, what it does. I'll explain that to you. Then we will do some lab, I'll do some, then I will expect you to do it along with me in your code so that I make sure that this very crucial tool of creating dashboards you have mastered. Now, just to be clear, weights and biases is a company in its own right. They are very permissive, just like Google is a company making Tensor board with their released it to open open source likewise for weights and biases and but the thing is their hosting is in their cloud so when you do your code the dashboard you get is in the cloud and it's free for development for individual use for open source project etc so it follows the model that if you can pay is if you're using it for commercial value, then you pay. If you're using it for learning and open source project and contributing, then it is totally free, which I think is a permissive license. It has gained a lot of attention in recent times. I do know at least a couple of people here, one person who's using it. Praveen, you're using weights and biases, isn't it? Yes, sir. Okay. So before I start, Praveen, would you give some your impressions of it? I just started using it as I find I was just observing like all this uh hyper parameter tuning right so it gives you the log so that uh each time you iterate over different changes right so you can track the uh logs and see what makes the best for it absolutely yeah so guys uh as i go and do that let me before i go to the website let me give you the gist of it see you have source code and the source code i'm sorry are you sharing your writing board no no i am not i am actually deliberately logged into the unix machine so i could show you code in action, the Linux machine. This one doesn't have the writing code. So I'm just speaking. And unfortunately, added to that is the problem that my video is not in the best of shapes from what I can make out. So today is really an odd experience. So, but if you're listening to me, let's just continue with that. Are you able to listen and follow me clearly? Yes. Yes. Yeah. So the idea is, see, how do you keep track of your source code? When you write software, any code, it is prudent and best practice to keep your code in a version control system. Now, it is one of the oldest tools developed in the software industry. You always version control. There was CVS. And then after that, there has been many, many things. But today, the king of the hill seems to be Git. you can have it github gitlab bitbucket whatever it is or you can run your own local git server but at the end of it you keep it in git or in some some version control system that you like and the reason you do that is you can compare if your code develops a bug or you don't like it you can compare with previous version you can rewind essentially rewind and it also gives a way for people to merge if two people are concurrently working and their works are additive it's a it gives you a systematic way to add the two work together merge the two things. So the value of version control is undeniable. We all have been using it for years. But when you look at the machine learning community, what we do, and if you observe what you have been doing is, you go back to your Jupyter notebook, you change some parameter, hyperparameter, you change the learning rate, andameter. You change the learning rate and then you see what's the accuracy. And probably in a notebook you write it down that at this learning rate accuracy, at this number of epochs, at this momentum, at this. So all sorts of hyperparameters, if you're talking of a simple feed forward network, and these many number of layers, you got the best results from for some problem, some data problem. But But then what happens, you create that model, the next person, suppose you leave the company, the next person who comes in, sees the data, and thinks, I will try it out, Even if the person sees your model, doesn't know that your model was arrived at by trying out all sorts of hyperparameters. And for all he knows, you might not have tried anything. You might have randomly picked those numbers and said it's good enough. So somebody legitimately may ask this question, why should I not start from scratch and do a hyperparameter search? So that is in now that you would say that, well, you know, the community has for these years been living with that. They have been saying, that's fine. You know, that's how we work. We don't do proper versioning and so forth. And the problem that it brings to is not only wasted developer resources, data scientists who are wasting time retreading the same path that others have well trodden and realized it's not a good solution. But more than that, it is an ecological disaster. As I mentioned mentioned tremendous amount of energy goes in training these neural network models you burn a lot of electricity and some of you are planning to purchase expensive hardware under your table when you do purchase those hardware you'll realize literally in terms of the electricity bill you get at the end of the month you will realize that it is an ecological disaster right for example because of the computer workstations running here at home my electricity bill exceeds sometimes a thousand dollars right now i do know that if none of the computers were running it would be a modest hundred hundred hundred fifty or maybe less than hundred dollars. So there is a massive cost. But it is not just a question of cost, it is also a question of a tremendous impact on the environment, that electricity has to be produced, coal has to be burned, or natural gas has to be burned, and so on and so forth. So essentially, you know, to repeat the same thing that somebody else has done has an environmental cost. Therefore, the question comes, could we not have kept a track that of all the hyperparameter values that have been tried? Right, and every time you try it, capture the hyperparameter values, capture the model state, capture this thing, or the accuracy is the performance metrics, and also capture how much of the hardware did you use? How much of GPU, CPU, et cetera that you use, so that you have it and you can do a nuanced, careful comparison between different runs right so for example should you run one one epoch ten epoch right at the end of the day at what point do you reach saturation point and then there are many degrees of freedom what was your learning rate what was your momentum like what was your dropout you What was your momentum? Like what was your dropout? You guys are seeing it, right? What is your weight decay factor? And how many nodes were there in each layer? So how many layers were there? Even in a simple feed forward network, all of these hyper parameters essentially kick in, right? And of course the ultimate hyper parameter is what neural architecture to pick at all, or whether to pick that or to pick, say support vector machines or random forest or something else if it is a structured data problem and so forth. So there has been a trust these days to create tools that essentially version control your model and its execution. That is one. And it also goes towards reproducible AI, reproducible research. Trouble is these days that if you read a paper and the paper claims that we build this neural network using these ideas and then we got these results. Unfortunately, in research community, a large, quite a bit of the research is not reproducible. So there has been a trust to create research that is reproducible. And how do you reproduce it? For example, one very good initiative is called Paper with Code, a website called Paper with Code. It basically lists only those papers where either the author or somebody, authors or somebody else has contributed a fully working code associated with that paper and that reproduces the results of that paper. Now, you say that, well, that is enough. But it's probably not, because sometimes you may have the paper, for example, for BERT. You may have the paper. You may have the code that the paper uses. But nonetheless, you can't run it. You can't run it because the hardware doesn't have the, any reasonable hardware that you can buy will not run it. It will take excessive amounts of time. You can't pipe that level of data through it. And so what can we do? How do we trust a body of research? Or how do we trust some result? Or how do we compare our architectures with each other and so forth? So one way is now again, going back to version control, what if we could not only release the source code, which is great in the paper, we could also release the logs. We could show that with these hyper parameters, each of these various hyper parameters these were the performances that we got and here is the trace of it it serves two purposes one is just the research itself it prevents wastage of time where others are just repeating that they can go look it up and then move on the other aspect is that uh it's like you have somebody who doesn't want to repeat your research, trusts you, but wants to see how did the execution go, what happened, can quite literally then go to your dashboard and see that. that so we will take one dashboard which is the weights and biases and by the way obviously i'm not a sponsor for it or in any way connected to weights and biases as you can imagine so i hope when i'm saying that these dashboards are important i hope it doesn't come across as a selling of any one particular vendor but i'll just take weights and biases as an example, because it's simple at least. And I invite you guys to contribute other examples, other things. So first, what we will do is go there and see what it is. So this is the weights and biases. Let me just go to that. Actually, if you want to go to the weights and biases, you say w and b.ai. If you do that, it will take you to their site. Well, actually, it took me to my site, my data, which, okay, for what it is worth, it has come here. Here, you notice that I have a lot of runs that we have made of the projects. I will deliberately take what we have done before, the auto encoders. And I will show you how by just changing the code a little bit, not doing much, you can quickly integrate with this dashboard. But before I show you that, I want to show what is the value at the end of it, what is the gallery, like gallery of what things you can produce, and that is gallery of what things you can produce and that is worth looking at i highly invite you guys to go here and play with this it integrates with all sorts of platform by the way with scikit-learn also right very valuable you can see whenever you now write any code in python i would strongly is python only so anytime you write any code in python in machine learning i would strongly strongly advise you to integrate with bits and biases or anything equivalent whatever it is that you think is worth it and to just show what these things are i will just take an example so these are all pytorch examples um i will do so these are you can go to natural language uh no i wanted to do one particular one which was beautifully done. I mean, one second, Praveen. I wanted to highlight a couple of beautiful notebooks, popular reports are trending, let's say. Load a model, transfer learning using phytorch lightning. Okay, let's take popular. Raja Ayyanar?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati? popular one or some good one, right? Visualizing model prediction learning dexterity into it. The one that I was looking at was molecular simulation. I think I should be able to search for it somewhere. Okay, we are not able to do that. So I will just take just randomly one of them. So you guys are all familiar with articles in medium. Medium articles have this, but they don't have interactivity. Do you see that you can actually play around with this data? So what happens is that the graphs that you've built, they come alive. So this is the attention model, by the way, sequence to sequence encoder. A woman with a large purse is walking by a gate. Do you see attention come live in this paper right so you see that and you have graphs which you can embed but the thing is how did these graphs come about like do you have to write tons of d3 and javascript code to make it happen And that is the beauty of it. You don't actually, and I will go through this exercise in which I'll show you how quickly and how easily you do it without knowing anything in JavaScript at all, except all you need to know is your data science and your Python. And that is all. That is all you need to do. We can go back to the gallery. One of the things we will do is explainability. For example, let us look at that. Since it is a topic, okay, by the way, this is the one that I was looking for. But let me pick this one because it goes to the explainability of AI. So one good thing is that you can go straight to the code right here. For example, if you click on this, it will take you to the code. You can give you a link to your collab. I wouldn't wait for this code to come up. This is it. Then you can go to the research paper, etc. So on and so forth. What they're basically it has to do with the interpretability aspects of it. Then you can go to the research paper, etc. So on and so forth. What they're basically it has to do with the interpretability aspects of it and saying, where are you paying attention? Most likely, I don't know. I haven't read this paper. See that. But do you notice that you can see all of these things that are involved in this particular paper or this particular research it looks like a medium article sort of like on a blog which again is like a medium article or something but the difference is that your data is live and it is generated these charts are generated not by you but just by running your code and all of this you can see how does you know these are your hyper parameter values and how do you get good loss right what are the choices that helps you get good loss our combinations that help you get good loss and so forth right minimum loss and what are the factors that matter so there is a lot happening here and what i will do is take anyway just just as one last example i'd like to take as a gallery of what can be done i would like to take this one. This is a very, very well done notebook. And what I'm seeing is that what these things which look so professional, there is a very easy way to generate it actually. So this is of course the molecular, these days a protein structure has become a big deal being able to infer the protein structure and so forth so here's the corona virus rendered rather beautifully in many different ways so well anyway we will go and see what we can do ourselves so let us go to the code and i'll walk you now through the code and show you so you guys remember the auto encoder did you guys get a chance to read the vanilla auto encoder source code that was your assignment isn't it to understand and to read it and to try it against different data sets so i don't know how many of you got time to do that, but I'm going to take today to walk over this data set very quickly in case you didn't get time to do so. And you can then stop me today. Today, as I said, is the last day when we can, we are going to go at a usual pace and you can stop me. So these are the imports right the all of the imports are exactly the same the only new thing is this one let me put a comment here include by the way is the font big enough oh As if if we can increase it a little bit. Okay, I'll increase it a little bit. Is it better now? Can you read it? Waits and bias library. So you're including this weight and bias library, weights and bias library. I don't know, it is biases I believe. Biases perhaps makes sense. Weights and biases. I didn't do anything else. This is the only line that I'm adding. Then what I do is I go, when I run it, in the training loop, when I do train, here it is. Here is an interesting thing. What am I doing? I'm making the weights and bias library, van B, to watch the model. This code, if you remember, is the model. So maybe I should go from the beginning of this code to remind you what it is. These are just output directories. So when you take a class, when you create a class and you give nn module in there, what does it mean? Would somebody like to mention? What is it standing for? What does line 29 mean? What is it saying? It's referring to the sub module subclass. So in other words, a base auto encoder is inherits from an end module, and therefore is itself a PyTorch module. A PyTorch module and essentially it's a neural net structure, either a neural net or a part of a neural net. So layers, etc, etc. They are all modules at the end of the day. And generally in PyTorch, the whole network is a module. So this is it. If you go back and look at all the labs, you will see this common structure in the classes. Now, in this, we have three parameters. What is the input dimension? Of course, if you want to write an autoencoder for anything, you need to tell what the input dimension is. What is the latent dimension that you're picking, and how many hidden layers you have, right? Hidden layers means between the latent dimension layer and the input layer, how many layers do you have? And the converse of it, between the latent representation and the output layer, how many layers do you have, and the converse of it, between the latent representation and the output layer, how many hidden layers are there, you are going to put there. So this, I hope, if you were trying out the lab of last week should look very straightforward. What is this bit of code doing? Input dimension is less than two etc these these in design we call preconditions check right don't do anything till you are sure that the input is correct and so we are making sure that the input is correct and then we are doing some magic if the hidden layers are not given, we nonetheless add at least one hidden layer because we know that stacked autoencoders perform better than the absolute basic autoencoder, single layer autoencoder. That is that. So we are just building this up. Then after doing all of this hidden layers, et cetera, latent dimension setting the variable, we have a build encoder, right? And so we can go to the declaration of the build encoder. What does this do? It builds that part of the neural network that is the encoder. And I believe we covered this last time. Do you guys remember that we covered this? Yes. Yes. So we did that. This builds the encoder part. This builds the decoder part, some helper functions. Because we are switching back and forth between the Vanilla encoder, it takes the square image and makes it into a single vector, linearizes it. So we need a method to shape it back into an image later on, that linear vector to go back and forth. So that is what it is, a shaped image, just a thing. And the build input does the opposite, what the shape of image does. The opposite of that is to linearize, right? So we linearize that. So now we'll come to the train train method which is the main method so what do we do in the train method if you notice i've given some default values batch size is 32 epox is 5 learning rate is 10 to the minus three one e minus three means 10 to the minus three and output directory if you don't give one we'll choose so all of them come with default values so you can call train without giving any parameters at all now the first thing i do in train is i say while you're training watch the model model. Now, why would I watch the model? Why is the model itself worth watching? That itself is the model, right? This here. Why is it worth watching during training? Would someone like to hazard a guess? It will build a log, right? Not really. What else? Any other guess guys it probably keeps track of what combinations of hyper parameters resulted in what kind of outputs close close Rafiq and not not exactly anybody else very close though you're essentially giving the van db uh class or the objects within van db access to the entire model by passing itself that's right so the weights and biases it is sensitive it knows how to interpret a PyTorch neural network. And so when you give it, what it does is, remember the forward pass, the weights are sitting quietly and they make prediction. In the backward pass, when you have the gradient flow, as the gradient flows back, what happens after that, after the back prop? You update the gradients. You update the weights and biases, isn't it? You update the parameters. Do you remember? That is the whole point of the gradient descent. So what this will do, what this library will do, is it will weight and record every change to your weights. It will record the flow of gradient through it. The flow of gradient is important because for example, if you have bottlenecks or at some point, a gradient just dies or something terrible happens or bugs happen, you can see it happen basically. So it gives you some insight into what is happening when you run your neural net on the fly. So that is the value of it. Otherwise, if you try to do it on your own, you'll realize that it's pretty hard to do it. The rest of it is just TensorFlow normalize and Y normalize. Why normalize it with 0.5? Why normalize it with 0.5? Just a reminder, we did this in the first session, in the session number one. Why do we normalize? Anyone? For images, why do we normalize when we feed them to the neural nets? We have forgotten, so I'll repeat that. We normalize because neural nets work best when the input values are between 0 and 1. So if you normalize it and the values are small, neural nets work well. But if you give it big values and normalized values the then the gradient descent becomes adversely affected is it the vanishing gradient issue you discussed earlier in one of the right this one is not so much the vanishing gradient but the fact that the contour surface is not so much the vanishing gradient but the fact that the contour surface of the of the last large surface it becomes highly eccentric you know imagine a circle now as you deform a circle and make it an ellipse right what is the extreme state of the ellipse you keep on increasing the eccentricity it becomes a line isn't it it becomes a line segment do you see that in your mind yes stretches circle and it will become a line so you're increasing the eccentricity when you increase the eccentricity or you have high eccentricity in the lost surface then what happens is imagine that you are traveling along the direction, along the axis, it will be very hard to tell which is the minima, where to stop. And if it is very thin, close to a line, it's very easy to make a small jump and outside the region of optimality, very quickly, you can step out, you are constrained to a very narrow region. And so in all of, I mean, the whole ways of intuitively looking at it, but the basic point is that with large values of weights and biases, the gradient descent gets affected. Not the back prop, the backflow of the gradient gets affected for different reasons, exploding, vanishing gradients. And it's nice that you remembered that, but it's not related back prop, the backflow of the gradient gets affected for different reasons, exploding, vanishing gradients. And it's nice that you remembered that, but it's not related to that, it's related more to the fact that, I mean, maybe it's affects that also, I'll have to check, but primarily it makes your loss surface eccentric. So that's that. And so the rest of the code by now, guys, I hope is very familiar. What is MSC? It is your mean squared error. You use it in situations of regression, isn't it? you're looking at some form of regression situation. And of course, if you recall the autoencoder, we were building the loss function out of input minus output squared, some squared error, mean squared error. Now, Adam is of course the optimizer. It is the one that is going to do the gradient descent and this step, right? And the back prop and everything. So it needs to know the learning rate. It needs to know the, by the way, weight decay here is hardwired, but you can give your own weight decay. This is your regularization parameter. Now let's move forward. We come to the main loop. Everything, the code is the same. You notice that. So what is the one line of code i added just this line and then then there is one more line of code i add this line that's it so you know we are computing the loss value i'm creating a variable it takes a if you notice that there's curly braces here because it takes a dictionary of key values. So to this library, I'm calling the dot log loss, loss value. So now when this is running at full speed, there is a concern which is that, will it not slow the whole execution down? Because ultimately those logs are going to the web to the cloud the answer to that is the way they did it is pretty smart it is written to your local file system like any other log and that local file system that directory is synced up by this library to the cloud so asynchronously without affecting the performance of your execution it is pushing the updates to the cloud right so therefore you end up getting the best of both the wheels now this is it guys do you notice that i have added what one line here to log one line to watch the model itself one line to watch the model itself isn't it and we did an import of the library we are almost done so just inject this tiny little bits in the code and then let's see what happens i will go down to the main question there in terms of the logging what is it logging every iteration in which case it's adding a io operation to every iteration right absolutely every step yes to every iteration in which case it's adding a io operation to every iteration right absolutely every step yes to every iteration is adding an uh it is adding a io operation but it is not a big deal because a much more thing is to move data from memory to video card and there's a lot of latency happening here anyway okay and there's a giant massive massive matrix multiplications happening and so forth so this logging in itself is not substantive if you want you can do every 10th you know iteration or every hundred titration God will do that people do that So state that it's logging right? Complete, it's only logging the end state. And that is right. So what happens, look at this, we are inside a double for loop, right? Where is that double for, for every epoch, for every mini batch of data, you are logging. So every step you're logging, now now you may not choose to do that so for example if i move this line to inside the if then what happens it gets logged every hundred steps okay right and so it is purely uh personal taste and how much you want to log and how much it is necessary right so here i've just put it outside, kept it simple. We will see what is the effect of that in a little bit. Now, in this code, guys, I've put in the best practices and this is the recommended way of using weights and bias library. Now what happens? It is done. Whenever you're running the code, it may be a Jupyter Notebook or here, I will run it straight from the PyCharm itself. This is the main method. So those of you, I don't know if you understand, the main method is the entry point in your code where execution starts from. In a Jupyter Notebook, it will be the first cell think of it like that but when you write code outside jupyter notebook in a python script then there is a reserved special area in every python script it goes and you can identify it by this signature if made goes and you can identify it by this signature, if main to that. So basically, if you ever run a Python script standalone from the command line, what happens is that the Python interpreter will search for this and the name of that execution will be underscore underscore main. That library will be named underscore underscore name it's a reserved word and then it will I mean sorry that yeah will be module and that will get run so now let's look into this first line is van DB dot login right so it begs the question login where so could you guess where is it logging into? Cloud. Yes, excellent. It is logging into the website, the cloud server, and basically your init is where the real logging begins to happen. So now the whole question is how does it know my username, password? Well, you have to tell it. So what you have to do is step number one, which I hope all of you will do today. Go to the Rates and Biases website and log in. Just use your Google login. That is what I prefer doing. If you use your Google login, then you don't have to, you know, create one more login ID and this and that. When you do that on your local machine, there will be a key created API key. And when you do login here, all it does is it goes and make sure that your API key is correct and informs the server that you're logging in. You will see that in a moment when we run it. Now comes a part is correct and informs the server that you're logging in. You will see that in a moment when we run it. Now comes a part of the best practice. The best practice here says that, see, create a dictionary of your hyperparameters and anything else. So for example, this is just a text. It's not really a hyperparameter. It is a declarative text. It says that this is, we are doing an auto encoder architecture. It is a word of our choice. If we change the word, it doesn't matter. Dataset ID is MNIST, so I put it here. So what happens? You can change as you change the datasets, you can track that with this this data set it did this way now you notice that i put epox is equal to five and learning rate is equal to 0.001 right you can pick whatever value you want now there is something else then you do uh van dot init you initialize it think of this is your real process of starting the login. You give it, it is always a best practice to give a project name. Also, see all of these are optional, but it is a good thing to do. I find that you should give notes and tags so that you can search by tags and notes and so forth. And then you say the config is, this config thing object that you created you pass it to the init right when you do that basically well this line is redundant let me just mark it as okay i'll leave it here for the timing and then the rest of your code is untouched this was the code before i'm just putting it in a more succinct way. In your notebook, I have written it with a lot of English explanations, but the actual code was less. But look at the one change that I made. In the train, now I am giving the learning rate and picking it up from the config, the configs that I have set up, right? The advantage of doing that now is when I run this code, what will happen is, and let's run this code and see what happens. We can do that. And then it is always a good idea at the end of it to also say, config loss, plot loss, when we finish just to let the cloud know that you are done with this particular and otherwise it's left guessing so let me run this and see what happens now I will try to increase the font here also. This is by now, if you remember the lab, I print out the structure of the network input dimensions and then it goes through the epochs. Here are the epochs that it is going through. You may remember this. And we'll wait a little bit. And guys, today, I want you guys to do it before we end the session. So I don't want it to be just me walking through the lab. It is important enough that I would like to see you guys do it too. Just wait a minute. And so what can we do in the meanwhile, we actually don't have to wait a minute. We can go and where is the weights and biases thing regression. So these are my local notebooks rates and buys to include the library. okay it seems that i love to log in again okay let me do right here right me.com yes so what is the name we gave to our project do you guys remember it was auto encoders isn't it ml4 auto encoders so let's go there where ml4 400 auto encoders where is it if you go back to our code we will notice that it is here do you see this line guys yes so this is where it is coming from so now let's go in here You see this line guys? Yes. So this is where it is coming from. So now let's go in here. Okay, I'll leave this. ML4 autoencoders. This is it. So I will go to autoencoders. Now what has happened is every single run that I have made of this autoencoder is here. Okay, this will take a little while to load. And while it is loading, what I can do is something more actually, this I added just to illustrate that we could. Let me first remove it, delete panel. This is what you will see. The most important is the sequence of course increases, we don't worry about it. straight that we could let me first remove it delete panel this is what you will see the most important is the num the sequence of course increases we don't worry about it steps increase the epochs etc now i'll go to the latest one right so what one of the things it does that i like is it will always give very descriptive names to your run. So I'm on the seventh run. Let me go down on the seventh run and see what happens. It's a very entertaining names. Yes, very memorable names, right? Imagine sending an email to one of your colleague data scientists and saying, look at the results of the purple dove or something like that all right guys processing stuff named after sesame street characters like burton you know yeah elmo and elmo yeah right so guys this is it now you remember that we were building this graph ourselves in Jupyter Notebook, right? I had given you my own library to do the plot loss. Do you remember this graph? Let me make it bigger. Do you remember this graph from the last lab, guys? I was doing it because in the code I had a method called plot loss, which function I have written and given as a utility to be used in Jupyter, etc. Because you may or may not want to use the Vant library or somebody else has some other favorite library. By the way, you can do it with Vant, you can do some of this with TensorFlow, you can do it with other, I'm sure competitors are coming. But I want to illustrate that no matter what dashboard you use, use a dashboard. So when I go here, I see all of that. Now observe the gradients. Do you see how through the entire process, you see the gradient flow everywhere, how the gradients are moving back and forth and so forth. So you get more visibility into this. I don't want to get into everything. So this is one and you can go add things to it. So for example, if you feel like it, you want to publish this result. Let us say we like this result. Actually, let's do one more experiment. In the world of data science, we call it experiments. We ran it for five epochs and our learning rate was 0.01. Let us see what happens if you make a learning rate very aggressive, 0.1. Maybe worth trying, right? And let us say that you try it only for just in the interest of speed, we try it for only two epochs, right? And let us see what happens to it. This time around, we'll again run it. And then it should finish hopefully pretty fast you guys notice that the last seems to be significant this time right and so the two epochs have gone through let's go and see what is happening when we compare it compare our runs ah do you notice a new graph here guys this is of the latest run the latest run is called a firm frost here it is firm frost the eighth run do you see it guys and then it gives you a basis for comparison right the first few runs were with low learning rate. And this is with aggressive learning rate. And what do you notice? The losses... Looks like the head of a dragon. That's right. Yes, exactly. It's having a hard time converging. Obviously because why? It's making strides all over why is making strides all over right two bigger strikes all over but what did you just do do you see guys that you have created a log of all your losses gradually as you keep doing experiments you see more and more what happens and now we can go back well i don't have too manyeters. So let us say that we made it 0.01 and number of epochs, I still leave it as, let's go through four epochs and we can keep talking of five epochs. And let us see what happens. Actually, let us see what happens while we experiment with this. And look here, what else do we see here? with this and look here what else do we see here so steps etc are fine you have this the this part I wanted to show you do you see that it shows you your GPU power your usage in terms of wattage how many watts of power you're using, what is the GPU time spent on all the metrics. Now, this is good. Like when you're using your machine a lot, you sometimes want to see how it is doing. And especially, for example, if you notice your machine is getting a bit slow in computation, it may be because your GPU temperature has blown past the safe level. It is maxing out and so your gpu is deliberately throttling down the speed to survive right so clearly this code was no big deal for this uh particular gpu the utilization barely even reached 20 percent right and the temperatures remain more or less modest where are you giving the GPU information the hardware information for it to do this calculation yeah this part comes as a freebie you don't even have to do anything you don't have to write a single line of code it will pick it up from the underlying hardware on its own ah pretty nice isn't it yeah yeah so guys it is an essential tool use things like that pick your pick your tool pick your dashboard whatever you know open source project or thing but what i'm trying to introduce you to is the concept that dashboards are worth using, right? When you do your experiments, because as you search for the best hyperparameters, you should be doing this exercise, right? So are you getting the sense guys? Now let's see what happened. We have a ninth run. What happened to the ninth run? Lovely. For example, this is a surprise. If you decrease the learning rate to just 0.01, which is what we seem to have done, means you don't have to do 0.001. It converged pretty quickly where am i the gallery the the hanging code research where is that particular ml yeah this one so you notice that at i just made it 10 times smaller and now already it has become in fact it has converged faster so when you look at this guys what will you conclude which is the best learning rate 10 to the minus 3 10 to the like 0.1 0.01 or 0.001 which one would you prefer looking at this graph which is the one 0.01, the yellow line, right? Because it very quickly drops and stays down. So you can actually stop the learning early on if you wish, somewhere around this. One or two epochs would be enough. You don't even have to go through so many epochs. So this is how you explore your model systematically. Now assuming that you have explored, what else can you do? So by the way, I deliberately created a few projects, very simple projects that I want to walk you through to illustrate that you don't have to do it only for neural networks. You can do it only for neural networks. You can do it for anything. So for example, this is a simple scikit-learn. If you include dashboarding into it, this will be too trivial. It will be more meaningful when you do a hyperparameter search, a randomized or grid search, and play with that. So it will load in a moment. See, for quick question, while will be waiting for it to load yes what you illustrated right now is a plot of the loss function yes looks like by just the name of that um the library it's weights and biases dot db right what is it doing with the weights oh it just showed you the gradients of those weights you know the plot of gradient flow and it's remembers it see when you save your model then it captures the model actually i didn't save it in this code try that out just save the model itself and see what happens with this weights and biases but you also saw the plot of the gradient flow right we'll get back to it in a moment so you know even for a very simple model by the way this is linear regression just for linear regression remember what matters rich the the outliers and the so forth because linear regression can be hijacked by that so it shows you the cook's distance for the leverage points and so forth cpu utilization gpu for something a problem that small it is trivial right now not very impressive but the point is that you can do that right you can go and do so we'll take the auto encoders and we'll make more progress and guys after the break you guys will repeat it on your own yes so look at this i don't know this was some random code basically uh some values are moving so the point is that you can you can do or plot anything you want you can throw images into it and you can do more i want it to be a simple day of getting started so i didn't create a fancy uh project and so while it is loading guys do you have any questions uh as of uh when you visualize when you visualize the gradients how do you diagnose when something is wrong you don't initialize gradients you initialize the hyperparameter no no when you visualize it no you never initialize gradients dennis you initialize the weights visualize what was that visualize okay when you. Okay one very easy thing if the gradients are not flowing, if the gradient is pure absolute zero what does that mean? For some nodes if the gradient is zero what does it tell you? Vanishing gradient right? You have a deadlock, you have a dead node. For example with ReLU you have this problem. What happens when you have negative values? The activation produces the negative value, you're screwed. You have a dead node. So you can see those, see all of these things when you actually work and try to squeeze performance out of it, that's when you start seeing the value of this so okay guys this is ready you used to say that there are cases where the gradients resonate with each other what would that look like the the the word we use is not uh resonate but what you have, there is such a situation. Certain nodes can become locked in step. They may start doing, they may start essentially, what you're using the word resonance from physics happens, that some combination of nodes will mimic another combination of nodes exactly, or more or less exactly. So then you have sub-optimality, right? One area of the node is redundant, one area of the network is redundant. In the extreme case, of course, you initialize the entire network with the same exact weights, which is silly. Remember the symmetry breaking and symmetry thing. Then of course the entire network access one particular node, right? So, I mean each layer acts as just one particular node because every one of them are resonating with each other. So that is that guys. So here we go. You see that, so you know when you look at a graph like this in an experiment, you feel comfortable. Why? Because the losses are decreasing. Right. You notice that epochs going up makes no sense. Of course, it will go up. Accuracy is increasing. Validation accuracy is increasing. So this looks like a success. This basically looks like a successful experiment. Isn't it and there's only one run for it so we're looking into that giddy line run I wouldn't go into that but now I want to introduce to one you to one more thing before we take before I hand it over to you guys to do something and show let's go back to our auto encoder see the idea going back to the theme of reproducible research or literate programming and so forth is is that when you have experimental results, instead of printing it out on paper and sharing it with or mailing it to another researcher, the gold standard is you give him a live interactive experiment itself. If you could put the entire lab in a box and ship it, that would be the gold standard. And so here, what you see is your, you know, you have the trace of each of these experiments, what is happening. All right. So why not create a notebook or an article in which others can play with this and see what has happened as much as you can. So we will do that. So to do that step, let us go here. We will click on Create Report. And it will tell you how many panels to choose, system panels, chart panels for. So we are going to ignore the system panels because let's stress that your other scientist doesn't need to know how much CPU you utilized and so forth. So we are going to create a new report on this project with this experiment. Let's see what happens when you do that let's see what comes through do you realize that your thing is here section one let me just make it look like a real report I'm just giving you a sample of what you can do. Something like this, and a tagline. And then you can start doing your standard markup let us say that i say introduction introduction and let me just put some latex here just like you would do here let us say y is equal to sum limit Y minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus minus Let's see what happens. I step out of it. There we go. Oh, I forgot the square sign for the loss. L math of the year x. And this will be fun only for those of you who are familiar with LaTeX, x tilde, right? This is it, let's try it. There you go. The loss is given approximately by this, right? The basic autoencoder loss. So what can you do? You see these results. You can add a panel of scatter plots, bar plots, all of these parameters, importance, so on and so forth. You can do that. You can add code. For example, you feel like putting your source code, you can do that you can add code for example you feel like putting your source code you can put your source code here and save settings docs enable saving etc etc i mean i wouldn't go into that you can add more text blocks So guys, at the end of it, what you end up with is an article in which the data, the graphs, all the graphs of your paper, they're live and interactive. Are we making sense, guys? And you can throw in images and things like that you can preview what this will look like so this is it and when you're ready you can go ahead and publish it so guys are we are we getting it let me review the steps. And after the break, actually, I'll have the break. After the break, I'll just be in a QA format answering your questions. Because today, I really want you folks to try this before you leave the session. To summarize, what did we do? We did one line, import weights and balance. Can we do this using the the jupyter notebook absolutely anywhere there's no reason why you should do it here okay right anywhere the same code so the whatever you want to track i want to track the model in the loss function, it is here. You want to track the, and you initialize. If you want to do multiple runs with different hyper parameters, here it is. Right? Right. So guys, should I just paste this code into your Slack? You can use as an example or upload this code. Sure. Let me upload it to the website. You can use as an example or upload this code. Sure. Let me upload it to the website. You can use this as a template and right away make progress with this and see if you can make it work. But Jupyter Notebook, exactly the same thing. Maybe if you want, I can repeat the example in Jupyter Notebook. So interact with yourself. Do this, guys. If you're stuck, let me know right now, because it's important to do it. By the way, are you guys having fun seeing this? Yes, it looks pretty slick. It's pretty useful, yeah. And it is very useful. When you actually do your labs like once you have learned things you know then you have to know these things to make your life prevent to preserve your sanity otherwise life gets crazy doing so many experiments throughout the day And then at some point, I'll take up time, maybe on a Saturday, to do TensorBoard, which is another of that, another of this kind and also very popular. by the way tensor board and weights and balancers they work together very well uh where am i code nlp topics practicals hands-on labs and solutions i think this does tensorboard work with pytorch too oh of course very much so shankar now that you asked the question, I regret, maybe I should have started with TensorFlow. All of them work, you know, they are very good thing is that in the fortunately, in this deep learning world, all the end machine learning and broadly in Python world, almost everything interoperates with everything else very well so we'll do that lab shanka on a saturday someday we'll do that also it is just as good an integration as with tensorflow uh intensive this is so for work related work uh job related work uh is it better to use tensorboard than weights and biases means you need to go to a third party to get the account load the weights yeah there is always an issue about what your company's security policy is shangha i use a combination of both because you can mix the both together like for example a tensor tensor board will help you visualize your network. If you give it a PyTorch network, it will draw it out and things like that. And a couple of things they do better. Some things, you see, literally, they're overlapping sets, but one is not doing everything the other does their differences are we together so for example this version control mentality of your models that tensorflow doesn't do so well it will just produce directory after directory you'll have to visit each directory separately to see one particular experiment you You got that, Shankar? Yeah. Okay. Thank you. So that's the difference. That's pretty much where you differ from that. So, all right. We go back to ML notebooks now as we learn. And this topic was autoencoders and vanilla autoencoder so now this has your weights and bias code built in please take it guys run it and then you can do it in your Jupyter lab so in the Jupyter lab what do you have to do I see here is the same thing I did before you let you guys go you ask this question can I do it in can I do it in jupyter itself so how would i do it you see what i wrote in main is here do you guys see that the dictionary and the configuration then the rest of it is the same code that we had from before but i condensed it to remove all the english text from it i took another particular example the denoising auto encoders here and i'm training it when i train it like this what happens all i had to do is uh make sure that i'm logged into the weights and balance actually uh rand d dot login d.lockin. There we go. And then the rest of it is simple. I can go, you can see the waiting for waits and while process definition. And then here it is. It's giving me a dashboard to go to and see this whole thing run. I will fire it off, right? So as it is running, and I'll fire this one off also. As it is running here, I'll fire this one off also. As it is running here, I can see it run with some luck. So you can see it in real time as the run progresses. Oh, it turns out that we are not logging anything at all. Why? Because we forgot to, I think, go and log. Did we forget to go and log? Oh, no. It has just started. It will take a little bit of time for metrics to come in. So gradually it will start coming in, but it takes a little bit of time. But anyway, this answers the question, can we do it from jupiter isn't it guys same code doesn't matter where you do it from jupiter from pie charm from eclipse from just the command line and that's that and so uh so yeah i guess it will take some time for it to gradually show up in a bit. Let me see how far it went. It's still running through the sequences. Any questions guys before I end the first part? The second part is up to you guys. If you can give an hour or half an hour to be able to reproduce this and do it some, write some other simple code and where you can create some of this tracing and then create a report go all the way to creating a full report as if one needs an account actually it's ready in oh yeah yeah so what i'm saying is when you go to the weights and biases just use your gmail account it will take it you can log in with your gmail account uh when it says you create an account right so instead of creating yet another username password you can just start using your gmail yeah all right guys so with that i'll call a break and i'll see you guys in 10 15 minutes and hopefully you guys take a break obviously you must be wanting dinner after that let's do this lab guys let's do it right here before we call it quits today and as if are you pushing the file through Slack or are you pushing it into the workspace? The file is, as of this moment, let us make sure that it is already contributed. Handsome Lab. Transformers. Wasn't I introducing it right here? Edit file. Hang on. Yes, yes, yes. It is already on your website in the lab section now okay thank you see here it is week 12 labs weight and biases so this week we had two labs one on transformers and one on weights and biases and dashboard basically uh nothing heavily for this one this is very good by the way it's fairly popular um praveen how much is it used in your company are you still there i think praveen has dropped off still there how much do you use it a lot do people like it yeah they have security versions you know they don't worry about the situation of exposing things outside so that part is covered by the company oh nice you guys of course have an enterprise license to this yeah they even come and give lectures and get us started on things like that nice so you were already familiar with it you were using it oh no i'm not using it per se uh so i'm not on the uh you know using usage side of things i'm more on the uh not on the core modeling side so okay not at all nice but i see when in places where they have grafana kibana and all do you think they will hook it up to that or people would still stick to weights and biases grafana and Kibana will not. See, the burden of doing all that coding is on you because the moment you use, see Kibana is using Grafana. So you of course don't want to use Kibana because that's specific to Elasticsearch. But Grafana is an open source, a time series data visualization tool. So yes, you can do the loss functions and things, but the value of weights and bias is not that it gives you all this visualization of time series data, like loss over steps. Its main value is it gives you version control over your models. Models with different hyper parameters, right? You keep accumulating those experiments in different hyper parameters, right? You keep accumulating those experiments in a sensible way, right? So imagine that you have five different hyper parameters. You're looking at a five-dimensional space in which you're searching for the optimal solution, optimal model. You'll do a lot of experiments or grid search or this or that. Suppose you're doing randomized search grid search and you tie it to this what happens you can use this to keep track of how each of those hundreds of experiments went and which was the best isn't it so you may pick the best but then the next guy who comes into the project he won't start from. He knows that this is not a value that works, but this is the best in view of all these experiments. Asif R- That is the value of these dashboards. Thank you. Srinivasan Parthasarathy, So I said, how does it compare with H2O AutoML? Because they also give similar graphs and they track your experiments. Yeah, a lot of people are doing it. See, H2O started out as a big data company that wanted to do AI at scale, you know. And in that area they have done well they are creating a dashboard spark is creating a dashboard they all create dashboards they are all differentiating when it comes to ai and regular ml my my own preferences i prefer weights and biases and tensorbolt combination of these two oh so between like version control with a visualizing network yes yes so i use the mix of both and to me that works best so i'm teaching you guys uh you know things from my own practice So I'm teaching you guys, you know, things from my own practice of what I do. Any other questions, guys? Then I'm taking a break. This is 9-11. I'll meet you guys at 9-30 in 20 minutes. And after that, please, and I will take any need for help questions. But if you can please stay back, don't just walk off, do a lab, do the lab, if nothing else just repeat, run this code, put it in your project and run it and then create a Jupyter notebook with simple basic syntax and see and make it work. And if you go to the weights and biases website, there's a ton of resources there on getting started.