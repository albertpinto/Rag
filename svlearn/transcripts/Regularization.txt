 So guys, I'll just quickly review what we did the last time. We looked at the fact that you can take data in a certain feature space, input space, and you could lift it into or transform it into some other space in which the data gets linearized. So we saw this example of a desk. Points on the desk are blue, points outside the desk are yellow. There are two ways of doing it. One was the transformation that I mentioned, x1 prime being the square of x1 and so forth. And what it led to is this triangular situation. So in this little triangle in here, everything is blue. Outside the triangle it is yellow. And it was one quick way to see that yes, transformation leads to simplifying the problem, linearizing the problem. So that the decision boundary, as you can clearly see, is just a line, is just the diagonal line, right? The off diagonal line rather. So, and the thing to observe there was we did not necessarily live to a higher dimension. Isn't it Then the other way is, and the same thing. Just, we can do it in multiple ways. So this other way that we could do it is you could actually live to a higher dimension. what you do is you create a variable x3 which is the as which is the square of the distance from the origin of each point so you will because as the square of the distance it is this thing x1 square plus x2 square it will have a parabolic shape and in the parabolic shape the blue points will be towards the bottom of the parabola parabolic surface and the yellow points will be everything above the above it and so you can draw a hyperplane that we see which is a constant hyperplane x3 is equal to R squared means at a certain height from the from the ground which is defined by x1 x2 you have x3 constant and this ring this circular disk is the disk that we see in the original data space that becomes your decision that is your cutoff point that's your decision boundary but how are we defining the decision boundary in a higher dimension space we have linearized it because for us it is this x3 is equal to R square plane it's just a simple flat horizontal plane that cuts through the parabolic surface and it's an illustration of the fact that you can take data lift it to higher dimensions and get away with linearizing it. And once you have that, you solve the problem. Oh, there is a, okay, I noticed there's something, a little bit of, I should do. This is A minus B, a magnitude for polynomial kernel. I apologize, and this is some, not just one, some arbitrary number. So the kernel, if I defined it, is the simplest way to think of a kernel is the very simple kernel. Arjun Sajeevan G. Is just a dot product of two vectors, but more generally a kernel is the dot product of a dot product in the highest higher dimensional space. So it is actually here also there's a typo. So it is actually here also there's a typo. I think I was being absent-minded last time. So what it basically, let me explain this concept again, because I seem to have moved a little bit. The kernel, the basic, the classic definition of a kernel is the kernel of a vector X and Y is first what you do is you take X and Y you take the points X you take the point Y the X vector Y and you take them to some higher dimensional space some space need not necessarily higher dimension people often say that the space is infinite dimensional because they always think of radial basis functions. Not necessarily, but you just go to some higher dimension space, and this is another vector. It's a vector, some vector in a higher dimension space, and this becomes some vector in yet another higher dimension space, right? And basically, it is the dot product between these two it is a dot product in that space between these two transformed points and they're much simpler so that's a classic definition of a of a kernel kernel x y is actually nothing but the dot product in is basically a dark product in a transformed space between of of the points that's what okay so can you call phi as a transformation to that higher order space or in a sense to the higher dimension space so for example examples of phi are quite literally here yeah if we go ahead and of course the x square and so yeah these are all the mapping functions so the whole magic of kernels is that the surprising magic is a that you can do the transformation at all and by the way these spaces are, that you can do the transformation at all. And by the way, these spaces are called, you will often hear the word Hilbertian space or Hilbert space. Hilbert spaces are very interesting. Now, what is the most simple intuition I can give you? Hilbert spaces can be infinite dimensional. These are functional spaces. So potentially infinite dimension. They don't have to be like, for example, in this example we saw we got away with two dimensions. Shubham Tulsiani, With they could have been higher dimensions sometimes infinite dimension spaces. So for us to wrap our head around infinite dimensions is a bit hard, but the way to do that is carry your intuition forward from three dimensions. Hilbertian spaces or Hilbert spaces are something that physicists absolutely love because all of quantum mechanics, all of the theories of physics ultimately more or less have a deep connection to Hilbert spaces. And so these sort of arguments that you see, let's map it to a higher dimension space, etc. This is very close to the work mathematical physicists do, which is why you often find that in machine learning, a large part of the literature or the terminology quite often is borrowed from theoretical physics, the concepts and so on and so forth. So as always happens, math and theoretical subjects are a couple of hundred years ahead or a hundred years ahead of engineering practices because reality takes a lot of time to catch up but the human imagination is way ahead. So a lot of this theory was developed a long time ago and now they are they are proving to be very very useful though a hundred years ago people didn't see all the use practical use for it now it's all proving very useful so simply put as I said the intuition is that think of dot products dot products capture a very rich relationship dot product in the initial space are in the target space, they are all wonderful things. We did that. Then dimensionality reduction we talked about. We know just as a recap, the principal component analysis was wrapping up this data, shrink wrapping it in an ellipsoid. Now you may or may not be able to do it. If the data is linear, you can do it. If the covariance matrix is, if there is a positive covariance or a negative covariance, you can do it. But sometimes the covariance is zero. Like for example, in the dataset two, we noticed that you won't get much covariance because sometimes it's positively correlated, sometimes negatively and so on and so forth. So what can you do? You can go to another space. You can use the kernel mapping. Go to another space where the data gets linearized. So you apply a kernel mapping to go to that space. Once it is linearized, then if you're lucky, you can then do PCA in that space and actually come to a lower dimension space, which is why I mentioned that it is a good idea to do kernel PCA like the next thing we talked about is the another concept which is that of kernel distance which is actually different the meaning or the word the kernel here is used in a slightly different manner. Here the kernels are distance kernels and we applied it to the k and n. We realized that when we take k is equal to 1, one nearest neighbor, the decision boundaries are very, very, you notice that they have high variance. On the other hand, when you take k equal to infinity, then the decision, or k is equal to 20 in this case, the decision boundary is just meaningless. It devolves to the baseline situation, the majority situation. So somewhere between k is equal to one and k is equal to 20 is the perfect answer, is the best value of k. K is the hyperparameter of your k nearest neighbor algorithm and it has been obviously a way to find that you can try to find it. As you increase k you can see that the green line is better the red is perhaps not so good it starts getting bad and by the time you reach this orange line it becomes or the Indian red, it becomes sort of terrible. So you know that the, here the best solution potentially is the green solution. And you discover that through obviously cross validation in the data. Now, one of the ways to mitigate this whole business of finding the perfect K is that you can use a distance kernel you say that the immediate neighbors have more influence than the far-off neighbors and so then the whole question is what is your notion of distance we talked about the Minkowski distance and the L norm this is just a review of that that L is equal to half, looks like this circle. Circle, L is equal to three, and L is equal to infinity, the infinite norm. Minkowski distance is something that you should really review. Oh, by the way, I didn't post those notes, the ones that I showed you guys from my book. I'll post it, do a slack. Then the k-nearest neighbor. So obviously what you need is some distance function. Once you have a notion of distance, you need some function that decays with distance monotonically. It may not decay at all. It may just suddenly fall off or it may remain constant, but something, but it certainly shouldn't increase with distance or swing back and forth. It must have a monotonic behavior so people have come up with all sorts of distance functions to to have the best care nearest neighbors we went through a review of that last time if you remember we covered about a half a dozen or more distance kernels. Mostly the parabolic kernel tends to do quite well, but almost all of them do equally well. You know, when you're trying to squeeze the last bit of performance out of the model, then the last thing you do is you tweak and you pick your kernel, but broadly speaking the kernels in the beginning, the last thing you should worry about is which kernel am I using, right? Focus on cleaning the data, getting your distance measure right, doing all those things right. Then as at a later stage when you are beginning to do quite well with your kernel KNN, then also search for the best possible kernel. That brings up this whole thing about hyperparameter search that I would like to talk about today. See guys, do you notice that in ML100 there were not that many knobs to turn? You had linear regression, logistic regression, and so forth, LDA, QDA. They did not have too many hyperparameters built in. In fact, there were not any hyperparameters. But in ML200, we are sort of infested with hyperparameters in all our machine learning models so the machine learning models say that if you tell me what the value of K is we can build the best k nearest neighbor model for your data but you have to find K so what you have to do you have to continuously go search for the best value of K right but then you have another dimension which is the best kernel so now you have to search in which is the best kernel. So now you have to search in two dimensions, the best kernel and the best K. Now, usually what happens is a kernel KNN, they sort of mute the importance of the hyperparameter K. It still matters, but it doesn't matter so aggressively. So, for example, the best k and the neighboring k values would all give more or less the same accuracy or the same performance, but now you certainly do have a problem. You have to find the best k and you have to find the best kernel to use, distance kernel to use. So you're searching in two dimensions, isn't it? And now suppose I had a few more hyperparameters in some kind of a model. Let's take random forest. What were the hyper parameters there? Well, we are building trees. So how many trees to take? Well, there the simple answer is take a lot of trees. I start with typically 400 in most situations, then and go up from there, depending on the complexity of the data set. Then even for very simple data set I'll take 100, 200 trees. But then the depth of the tree. How deep should the tree be? Each tree that you're building, what is the max depth that you will allow? That is a hyper parameter. It can go all the way from one to a surprisingly large, you can go whatever whatever depth then at what cut off like how many points do you want to leave in the leaf node before you stop splitting you can ask whether i use genie or cross and or sorry junior entropy as my impurity measure isn't it so there are quite a few and the other thing of course in random forest is at each split of the tree how many features are you what subset of the features or what are more formally what subspace in the feature space should you consider for splitting at each split in the tree you don't want to take all the features but how many features do you want to take just one feature two feature three feature all hundred features square root of uh square root of the number of features, which would be 10, log of the number of features. These are all choices to be made. And at the end of the day, in the more general situation, in practical situations for one data set, some of the hyperparameters matter quite a bit, but for some other hyper parameters may not matter at all but a priori before you do this experiment you don't know what the situation is are we together you don't know that so what do you do? You have to search. You have to search this hyperparameter space to find the optimal combination of hyperparameters. But by now, you realize that the choices are many. It is a combinatorial explosion, even if you take discrete number of discrete values for the hyperparameter. So for example, if you take the depth of the tree is discrete, 1, 2, 3, 4, 5, and so forth. The number of features, max number of features to consider is discrete. The Gini and Gini versus entropy is, again, a choice, binary choice. Number of estimators is, again, up to you. These are all discrete. But let us say that in each case, you had a choice of about 10 possible values. By the time you have four hyperparameters, you realize that 10 times 10 times 10 times 10 is 1,000. So what you're doing is you're building 1,000 models to find which model is the best on cross-validation. Isn't it? So that's a pretty aggressive computational load. So this way of brute force are just creating a grid of that values and searching for it, used to be called the grid search method. The grid search method for hyperparameter tuning has been a classic. It is also a staple diet for interviews. The grid search method for hyperparameter tuning has been a classic. It is also a staple diet for interviews. So when you, should you go for interviews and your interviewer says, well, what is grid search? Now you know, or the other way they can say is, how do you find the best values for the hyperparameter? So quite often they're expecting you to say grid search. And so you should start by saying, well, this is the way you could do it. But remember, this is not the optimal way. But then the other thing you could do for reasons that we'll understand later, or maybe today itself, if we get time, you can actually do randomized search, take random values in each of the dimensions, hyperparameter dimensions, and then search and then what you do is once you find the best value then you search in the neighborhood of that value to find even better values and so forth you sort of zoom in you can do that that's called randomized search it is from experience we now know it is better than this brute force grid search. And lastly, these days actually, there's quite a bit of theory on doing this in an optimal, in a much more efficient manner. And that is the domain, that's one of the topics that we'll cover in automated ML, automatic ML. One of its main burden is how do you do optimal hyperparameter search? Because you are how do you do optimal hyperparameter search because you are not just searching along the hyperparameters remember you're also searching along many many algorithms so for example if it is a classification problem then given the no freelance theorem you don't know which of the algorithms which of the classifiers would be best and each classifier comes with its own bag of hyperparameters, isn't it guys? So the search for that becomes computationally very, very expensive. You need a systematic way to go about doing it. That is the new frontier. I would like to, we are running out of sessions of course, and the syllabus of this course, we have two more topics regularization and and gradient boosting both of which I would like to cover before the courses are so I will do automated ML in one of the extra sessions will certainly start the bootcamp with automated ML so anyway that's the summary guys of the last time. I would like to now move on to the topic for today which is a continuation of what we talked. Now today was supposed to be lab but I thought about it and I sort of changed the decision. I felt that regularization is such an important topic we cannot miss it. So we should do it today. And we should move the lab work to a regular. We'll do it over Saturday. So I swapped it. I said we'll do regularization on the weekend. No, instead we'll do it today and we'll do the lab session over Saturday afternoon. I hope that is all right with you and the boosting is also an important topic. It needs its own lab so the next week will give to boosting and to its lab instead. The recommend that we are going to anyway cover in the math course so it will get covered we don't need to worry about that. This is the way we do. So let's do it properly so that we get to do a lot of labs in each of these. Today, because we have already covered support vector machines, and this notion of kernel distance is a bit theoretical, there was no very explicit lab that I could have given. So instead, I decided I'll do regularization. So that regularization lab is really important to to do and we'll do that on Saturday. Any questions guys before we start? Can I ask a question? Yes, please go ahead. So when you go to when you say, you know, we can go to higher order space, right? When you say we can go to higher order space, right? But if we do not know that space, let's say example of size and weight for cow and duck, right? But we want to go to higher order space, but we do not know, let's say color or other thing. Then what do we do then? No, no, you have to know that ultimately the higher dimension space is manufactured out of the input space the input space so if you don't know the machine learning algorithm won't be able to discover see it doesn't know what it doesn't know right that's the problem with all machine learning algorithms remember we talked about the irreducible error in ml 100 introduction. But that is the irreducible error that you can't get away with. It is true with human beings also. When we believe something is to be true, we base that judgment on what we know. But there remains the fact that we do know what we don't know. We can just be aware of the fact that there must be things we don't know. I hope one of the things we learn here is that what we don't know may fatally compromise our understanding. It's almost, it is obvious, but it is the irreducible error in machine learning. And that no algorithm will be able to surmise you have to bring it to the table so I think question is that you know when you say for currently do not have to know that it's all you have to do is dot product in that it's basically that means to the kernel trick is see what happens is that some of these ways to map are quite powerful like linear and polynomial are good and so for example you know that polynomial will work for the river data set easily then then linear would have worked easily for the straightforward classifier one problem then the radial basis function works for a vast class of problems. When you use RBF, you're actually talking of infinite dimensional spaces. At that particular moment, the infinite dimensional space is so rich, you're pretty much guaranteed a linear decision boundary, more or less, a very high probability that you solve the problem so you don't worry about that you you take this kernels and go question that arises is could you hand construct a kernel a mapping just but as a feature engineering exercise think it through and give your own mapping to the higher dimension if you can do that all power to you actually it leads to some very efficient solutions because computational runtime with the RBF kernel is quite high. You know, you're taking it to very high dimensions and you're searching for the solution there. If you can do it with your own homegrown kernel, the performance is better, even the computation times are better and so on and so forth. But if you can't, RBFs are very powerful. Generally they do solve your problem. And then the other interesting thing is that obviously it is the dot product that matters. The kernel trick, the heart of the kernel trick, it is not a theoretical breakthrough, but it is a computational sort of a cheat way or a shortcut. You manage to not discover the mapping function itself, but you discover that whatever mapping function there is, you discovered the dot product of it. And if you can reformulate your problem in terms of dot products, in terms of kernels, you have a winner. And that is why kernels became so, people said, wow, that is amazing. Given that that is true, can we then reformulate a large class of problems as kernel problems? That's how the whole kernel method thing started. And they were very, very surprised actually. You can write many things and with a little bit of thought, you can rewrite it as a kernel function. Amazing, just surprising it just shows that how powerful dot products are so a lot of work in that space thanks any other questions So guys, remember that we are coming to the close of the workshop. We still have two important topics to cover, regularization and boosting, both of which needs dedicated labs. So we have one session short. So I use this session for regularization and we will have Saturday. So do treat it as a mandatory session guys. We must finish the syllabus, which is that we must do the lab in regularization and next week we'll give to boosting. So, all right, so I'll move on to the regularization now. What is regularization? See when you, and I'll take the example of a polynomial regression to to sort of motivate the concept of regularization suppose I give you data let's say that and let's let's take a simple situation of X and Y where we are saying we are in search of X is equal to some function some function and let us say that there is a ground truth you don't know there is a ground truth this is the ground truth of the data this is ground truth of the data and let's say that you have these points. Let me take these points here. And Let me say, now I'll just take these few points. I hope I can illustrate the problem that I have with this So I will keep this point and let's see what sort of curves we can draw through this one One thing you could do is you could say hey This okay. Again, let me be clear with the terminology. G is our ground truth. It is that which you don't know. It is the function that has actually produced the data. What is our task? Our task is to build a machine learning model, which is, let me just make it y hat. You're going to pick a hypothesis, going back to the basics, hypothesis function such that y hat is approximately y for most values, isn't it? In other words, the real value is y hat plus some error, and you want this error to be small. some error and you want this error to be small. A good model is one that makes fairly accurate predictions that are close to reality, close to the real values of y. Now if you take the case of polynomials, polynomial regression, you realize that you can write fx as beta naught plus beta 1 x plus beta 2 x squared plus, and you can keep going on to beta n x to the power n. Right? Do we remember this thing, guys? This is your polynomial regression equation. Approximating the data, trying to fit the data to this polynomial. Yeah. Now the question is, what is n? Here, the big problem is, what is n? What is the best in Trouble is, if you take and essentially becomes the hyper parameter of the model isn't it, it has And so And so you have a headache. You have to build models over and over and over again and see which value leads to better results. You can keep trying. So one of the questions that you can ask is, let's see what happens and see if there is a shortcut through this. If I take n is equal to one, what will this look like? n is equal to one would be this, right? In simple terms, what would you call that? Straight line. Straight line that is like this, isn't it? This is n is equal to one for n is equal to one it is the straight line. For n is equal to two, it would be some parabolic curve. I don't know if this is exactly the one but we'll assume that this is the one and is equal to two, then for n is equal to three, let me take another color. Why am I having this color? This color. So n is equal to three, maybe that it is this one, right? let me make the points big and bold so that we remember those points and then what about n so it is two bends n is equal to three what about n is equal to five what will happen what will happen is as you go to higher and I will pretend because I we should mathematical we should programmatically draw this out to see which one fits, but just human me. This is too hot. Me take something like this. Okay. So if you were to take this It would be at least a four bands right so it would be something like 123 Sajeevan G. A for bands. Right. So it would be something like Sajeevan G. 123 Sajeevan G. Let me take it like that. I'm just motivating the example if we actually did it in a Sajeevan G. Model, it might look something different. Maybe I should have done before I came here. So this is n is equal to five. different maybe I should have done before I came here so this is n is equal to 5 what do you notice happening and let me as a without actually having done that let me say that suppose I had done that for n is equal to 10 what would it look like so now I need 9 bends and your data will begin to look, oh gosh, it's too fast again. Your data will begin to look like this. It will look like, and more than that actually, it will look like this. Something like this because you need nine bends. One, two, three, four, five, six, seven. Actually I didn't introduce nine bends. We need to make it even more curvy. So who knows? We need to add a couple of bends right here. Make this more like this so what happens is if you go to n is equal to 10 you have a complete fit to the data it seems to go through all the points isn't it but do you think n is equal to 10 something like this is a correct solution what does your intuition say what is the problem that this model is having it has over fit the data so? What is the problem that this model is having? It has overfit the data. So what happens is the more parameters you have, polynomial terms you have, the more knobs you have to turn, you have to turn in the data. It's a very flexible model. I'll give you an intuition. Imagine that it is a sort of a, what do we call it? that it is a sort of a what do we call it uh are you familiar well think of it as silly putty something like silly putty right if it is very stiff you can't bend it it's like the n is equal to one line now gradually as you make it more and more pliable, you can bend it more and more, isn't it? And so you have bent it, actually it's a bad example, but think of something that has some degree of elasticity to it. The more elasticity it has, the more you can bend it. So the higher degree of polynomial, the more pliable it is, the more you can bend it. So the higher degree of polynomial, the more pliable it is, the more you can bend it. What happens is if you take a high degree polynomial, you have too much elasticity. You can just fit the polynomial, that curve, to the entire data perfectly. But you know that the data is just a representation. You take another sample of the data, and this curve will be all wrong, isn't it? You will end up drawing a completely different curve to this data so how do you cure this problem this problem is the problem of overfitting isn't it so you do know that flexible models do tend to fit the data but you don't want it to overfit. So just as a reminder, the total error of a model, model is equal to the bias error plus the variance error plus what else about sorry the the variance error plus what guys do you remember what is the other do civil error irreducible error right irreducible error. Thank you, Kate. Irreducible error. It is the, it captures noise as well as that which you don't know, that which the model doesn't know. So, see, if you try to reduce bias, variance increases. Are we together, guys? That has been the bias variance trade-off. So, let me draw the bias variance trade-off again. Jay Shah, Dr. For example, in the case of polynomial, as you increase n, the two, the errors, right, the bias error of a model, as you build more and more complex models, will keep going down like this. Right. What about the variance errors? got the variance errors. The variance errors will start the, actually let me use another color so that I can use white for the final solution. So let's say that your bias errors go like this. They keep on decreasing the more flexible you make the model. Your variance errors, on the other hand, begins to go like this. And let me give names to this. Errors. And this is errors so you know that somewhere in between those two points is the perfect line it is this line and let me use white itself but a thicker version of white hopefully so the total error if you add these two up you notice that it goes like this and whatever the value of K here is this value this is the best approximation. So suppose f n, I will say, of degree n to the ground truth. It is the closest you can come to the ground truth. And this error that you're left with is obviously the part that you can't get rid of. It remains at the end of the day, it's the residual error for you. Are we together? It's made up of whatever little bias error remains, whatever little variance error remains and the irreducible error. So as a recap, guys, does this look ring a bell? Are we familiar with this so far? We did that just a couple of months ago in ml100 so this is it so now the question is here you have this big search from here to there can be the question to ask is what happens if you make an overly simple model a biased model, a step model, its errors will be hard to deal with. It's a sort of fatal, you can't get rid of that. On the other hand, if you make your model too complex, too flexible, the question that arises is, going back to this data, do you notice that the complex model n is equal to 10 or n is equal to 5, green and the pink lines, they have too many oscillations. They seem to be oscillating quite a bit unnecessarily. So it is tempting to ask, can we apply a damping factor? Can we dampen the oscillations? damping factor can we dampen the oscillations it's bouncing around too much in the feature space this curve can we reduce the bounce can we dampen it are we together because if we could dampen it somehow then it could be it could solve a problem we could start with a flexible model and we could dampen it down So the basic idea is that can we start with let's say a green line or the pink line But somehow dampen it down to the yellow line now the yellow line is the perfect line right in this case seems to be the best model that is a n is equal to Three Isn't it guys? So the idea is suppose I took a more complex model like n is equal to 5. Can I bring in some mechanism that in spite of my taking a more complex model the things the extra degrees of the extra flexibility somehow magically disappears and it begins to approximate the yellow line, the yellow curve. That is the intuition of this whole problem or this whole bag of tricks called regularization, this technique called regularization. Are we together? It is also called shrinkage, shrinkage methods. In fact, your book calls it shrinkage methods. Statisticians call it shrinkage method, shrinkage. Now, the machine learning crowd tends to use the word regularization more often. Shrinkage means you shrink all of these oscillations and you make it look like that yellow curve. That is what we will learn to do. But for us, so the answer to that is yes, you can do that. It turns out there's a very elegant way of solving this problem. And the solution to that comes from a field, a very simple bit of mathematics called, and it is a problem in constrained optimization. Now I already, optimization, I already sort of introduced you to the concept of constrained optimization in the support vector machines thing. In other words, we will find the maximal margin hyperplane constrained by a certain budget of mistakes that you can make. Remember? So we will use the same things of constrained optimization today. But before I go into the mathematics of it, I just wanted to say that, see, this theory is very elegant. It's one of the crown jewels, actually, of linear methods. The fact that you can take a fairly complicated polynomial, which is still considered linear because it's linear in the polynomial terms, and then regularize it and get to the solution. Regularization is one way. There is another way, actually, to solve this problem. The other way is to this problem the other way is to just bring the other way let me just write it in a sidebar the other way other option and probably the preferred one I probably preferred probably probably preferred something is wrong with my preferred it's a spelling uh preferred way the preferred way would be what get more data. So what happens is if you get more data somehow, it is possible to get more data. Suppose you have points like this. Lots and lots of data. Lots and lots of data. And I hope I'm not making enough number of dots, but imagine that this is very densely packed with data. Then what happens is the curve that tries to overfit, it won't get enough flexibility. It may sort of ripple around a little bit wobble around a little bit but it will ultimately look like the even a high degree polynomial will look like this do you notice the small wiggles that I have put in the line there may be small ripples of wiggles in there but broadly speaking because you know a 10 degree polynomial needs 10 9 wiggles it will still be there but it will be unnoticeable right so what has happened is you have given it a lot of data and it is forced to fit with a lot of data so overfitting disappears or gets muted so here overfitting So here, overfitting or high variance, if you want to put by the way, the preferred way, if you can get more data than do it. So what is the catch here? Why not just use more data? Why, why develop a whole new theory of regularization and so forth? Can somebody guess? It can be expensive to get data. It is expensive to get data. It is expensive to get data. Yes, that is very true. That's one reason. Or it might not be available. Like if it's a rare cancer, the data might not be available. Yes, that's the point. In a lot of fields, data is very, very hard to get and you have to make do. You want to model with what you have. You can't say that, it's like asking for food on a silver platter. Well, they may not be. So you do want to make do with what you have and more data may not be available. Another way to look at it is, see, remember I told you that the higher dimensional spaces are very sparse. No amount of data will look enough because all the data disappears in that space. So overfitting happens when the data is sparse and you're fitting a very flexible model to it. When you do some of these modern algorithms, let's say a deep neural network, which you will do shortly in the next workshop, can you guess how many parameters we're talking about? Here we are talking about a polynomial of degree 10. In a typical deep neural network, how many parameters do we typically talk about? A few millions to a billion. Exactly. So if I'm right, yeah, millions of parameters, even the basic ones have millions of parameters. And then I think GPT-3 has what 500 million parameters or something like that. So the parameter space is just ginormous. So you might come there with 10 million data points, but your 10 million data points are a drop in the bucket as far as that huge, huge hyper parameter space is concerned. So overfitting is a very serious problem there. This getting to wrong answers, so overfitting problem. And so you have a lot of overfitting errors, high variance errors in your problem. And so the theory of regularization, you need to find a way to regularize. So today, hopefully based on how much time we have, I will teach you at least how many techniques, three. I'll try to teach you at least a few techniques of regularization and on Saturday we are going to do a lab on those those techniques so how do we do that let us let us go back to the basics suppose you have a function why let's go back again write down what we wrote before we are trying to find a function y hat is a function of x. And you know the ground truth, y is actually a function of gx. So you know that y is basically y hat plus some irreducible, some errors that get made in your model. And your pursuit is to find an fx that approximates the ground truth. This is our ground truth. How in the world are we going to do that? Let's go back to basics. We have a loss function, the error function. We know that each time in, so let's look at regression as an example. And this will generalize over to classification also. What is the error term for each data point? The error is equal to y minus y hat, isn't it? This is your residual. Now your sum squared error was summed over all i. Do you remember that, guys? This was your sum squared error. Now, given this, and now what you can do, this error, if you just expand it out as y minus y hat i squared, this comes to, and let's do the little bit of mathematics here this comes to y i minus what was y hat beta naught let me just take the term beta 1 x1 right plus beta 2 x2 etc etc but let me stay with one dimension for the time being. It is this. You agree? And we can sort of add more dimensions to it. It's not going to change the thing. And obviously I shouldn't forget my summation, I. This is your error term, guys. Any mysteries here so far? So when you have this you know that when you are doing your problem a data is fixed during learning data is fixed of course. Because you're learning on the data. So the values of x and y are not changing x i y i Engine. So this he what is it really a function of he is therefore A function of The turn out in function of yes beta naught beta 1 isn't it this is what is really happening here let me remove this sidebar line here this is this is it so I can write therefore this equation as e beta naught beta 1 and then I can go to beta 2 etc et cetera, et cetera, if I so wish, is the summation over square of yi minus beta naught minus beta one xi. And if I want to add higher order terms, I can do that. Let's stay with one dimension. This is what it is. Now, if you expand this out for each of these terms you'll realize that it expands out as y i square plus a beta naught square plus beta one y i x i minus and obviously we'll have to put a big summation over all of those and what else is there we missed one more term minus um minus beta naught beta one one yeah two x i x i right so if you look at this equation right and then um it's a it's a bit of a long set of terms but do you know that what is the highest power of beta that you see here in this equation is the square term say beta naught or beta 1 square so you would agree that this is this equation this equation the important thing, quadratic in beta. Isn't it? It's quadratic in the betas. No. So now quadratic curves look like, how many bends does a quadratic curve have? One. One. And so in higher dimensions, how many, the generalization of a curve is a surface or a hypersurface. In this case, just a surface. So what will it look like? Let's look at this axis, error. And this is this. This is beta naught, let's say axis. So now, remember, we are not in the data space. We are in the hypothesis space. We keep using this word hypothesis space. We are in the hypothesis space. So here, if you remember, we talked about this. This is just a recap guys of what we did. This is for each value of beta naught, beta one. There will be a certain amount of error. And this is your curves and they will exist a point a perfect beta naught. Let me just call it beta naught beta one with a tilde on it. The perfect values, the best values. the optimal value where E minimum of E minima is achieved. In other words, what you are searching for in this space is this point. This point. Beta naught, beta one Represents in the beta space. Right. Let me just write it as a beta well till day vector, but okay, I'll just write it like that. This is what it is the point where it is that value of beta naught beta one where this achieves the minimum. The error achieves the minima, right? That's just a summary way of putting it. What value, what argument of beta naught beta 1 will minimize the error? That represents our point here. Let me draw this picture a little bit better. We see the parabola, actually. I need a bigger screen. That is AVZ or that is ARG? Archmin. Archmin. Archmin, okay. This word is, let me say, ARGMEN. The reason I introduced this word today is because you'll encounter it in books. So gradually you'll notice that slowly I'm introducing the mathematical notations, droplets so which value of the argument what are the arguments to this function beta naught beta 1 right so which argument will minimize the function argument means let me write this word down the definition of argument arg men of a function is those are argument values that will this is the definition of argument by definition, def. So likewise, you can define the argmax, you know, if you want to maximize something, the counter term or the equivalent term would be argmax. argmax would be those values of the argument that will maximize the function. So the words like argument argmax are pretty common in this literature. These words, they keep occurring in the ML literature. And this is just, do you realize that this box that I have here, it is a very succinct way of saying, what are we searching for? Our optimization journey is to find this thing beta not beta one tilde the beta tilde does that make sense guys we're just recapitulating what we did in ml100 here but if you have not understood speak up guys i'm not getting enough feedback you want me to repeat i'll be happy to repeat so i might have missed it but is this beta zero and uh or the beta zero tailed and beta one tilt this is a specific case for one white one error right yeah yes so what happens is that see in your model your model is this beta naught plus beta one x right based on this we come into the we create the error function beta naught beta 1 error function it look in the hypothesis space it looks like this surface right this is the error surface what are you looking for you're looking for the point of it is the summation of all the data points. E is yes, you're right. See, E is the summation over the errors from all the data points. You notice this. The definition of E is that it is this. Remember, we started out with this derivation. E by definition is the total error over all data points. So this is the total error, which is the sum of square of all the residuals. And then we expand it out mathematically and then we notice that when we expand it out it's a quadratic function in the beta naught beta 1 right so in the hypothesis space whose axis is beta naught a parameter space hypothesis space or parameter space It looks like this parabola, parabolic surface. Did you get that? Should I re-explain? No, I understand the parabolic part, but when we are trying to minimize, is it getting minimized for only one data point or how can we ensure that it is minimized for the summation of the data points? E is a function. E is derived from the data points. No. Summation over all the data points. But once we have created the summation over all the data points, the only things, the only knobs that you can turn are the beta naught beta one. Right? Because the points are there. You have calculated the total error for any given value of beta naught beta one. You know what the total error will be. Okay. And so you're trying to find that point, that Dota, right? This total error is also called the sum squared error ssc sum squared error some squared error got it got it okay so now comes the theory so this is a recap of what we did in ml100 here the now here is the intuition intuition see what happens is when you add higher degree polynomial terms so you are you're expanding your hypothesis space isn't it so that polynomial go ahead yeah the definition that you wrote the argument of a function is those argument values that will minimize the function. I didn't get this last part that will minimize the function. Are you by that do you mean the length of Sajeevan G. An example. Example. Let us take one dimension. Sajeevan G. Y is equal to x squared plus 10 right now tell me at what value this function is y as a fx let me write it this y or fx so what what value what will minimize of x will minimize the function zero x equals yeah x is equal to zero. So you say that x is equal to 0 is the argument of fx. This point, the value x is equal to 0, is the argument of the function. It is the place of the function. It is the place where function is minimum, but the minima of the function is what? Minima fx is what? That will be equal to 10 because if you put x is equal to zero, it will be 10, right? So the point x is equal to zero, fx is equal to 10 represents your minima. Represents the, and you can call it Y. And so you can say Y is equal to this. Does this look simple guys? Right? So if you plot it out, it will look like this and this will be 10. So this point X is equal to zero represents your argument. The point at which this curve achieves its minimum. So, uh, so we will write it. So the way you would say that is argument. If you want to use this notation argument that is x fx is equal to zero. In other words, the value of x that minimizes the function is zero is the are we is this getting any clearer guys yeah right so this is the this is the meaning of argument right so for example let me deliberately complicate this problem i'll make it x is equal to three now what value of X would minimize this problem can you get 3 so so because 0 is special I don't want to use 0 here so this is it and so this would be arg min would be 3 here and arg min here would be three, right? And on the other hand, the min of min fx would be 10. The minima that you would achieve would still be 10. That's the concept of argument guys. So what we are searching now, let's bring it back to our real life. What we are saying is we are finding that hypothesis in simple terms those parameter values beta naught beta 1 that will minimize our error term is this think about it for a moment is this clear guys if not and repeat it again anybody who wants me to repeat say so. Asit can you repeat it again? See what happens is, let me put it in a slightly different way. See, when you have a total error, that error is associated with some hypothesis, some value of beta naught beta one, some parameter values, because your hypothesis is parameterized by beta naught beta one. So any value by beta naught beta one so any value of beta naught beta one that you take there will be a certain degree of error right in simple terms in this beta naught beta one plane this plane that is there right this is your hypothesis right let me call it the hypothesis plane or space whatever would you want you for every point here there is a certain degree of error any point you take associated with it is a certain degree of error isn't it and so what you're trying to do is finding that point where the error is the least because you're trying to build a model which makes the least prediction error isn't it and therefore that is your that's your best solution that is the best you can do you can just learn your way towards that and just as a recap in ml100 what we would do is we would start at a random value and then we would do gradient descent here so in other words around this point they used to be this remember this is the projection of these shadows of these circles or these are called contour maps here do you remember this guys i hope you remember this because we'll use this to move forward towards regularization now the surface here it's it's uh shadows of these circles this contour maps uh sort of level surfaces are these and so you take a random point suppose you start here it has a huge error and what you're looking for is you're finding your way you're just finding your way through gradient descent towards the center this is your gradient descent as it is Then is the argument sort of either very close to or just the irreducible error then? No, not necessarily. See, even when you reach the minima, you may have reducible as well as irreducible error. What does that mean? See, for that that model it is the lowest error but you could build a better model. You can dial in the complexity, take a different value of n and so forth, isn't it? So in other words there is still residual bias and residual variance along with irradiazable present in the model in the in the minima okay in other words let me go back to the terms where we define the error where is it that we define yes a buck a bias errors yes look at this pink equation see irreducible error will remain it represents that which you don't know. But if you look here at this optimal point, right, do you notice that the pink line is not zero? The value of pink is not zero. There is still bias error as well as there is yellow line is not zero. There is also variance error. isn't it yeah that's the thing so even at your best model points there is still some bias error some variance error and there is irreducible error in fact just we did decision trees what happened even with pruning and so forth it was not possible to reduce the iridium i mean the by the variance errors too much it's still stuck there so we went to random forest which made the thing a little bit better there are many adaptations of random forest that try to solve that problem of overfitting so that's that so far so good guys so let me recap what we just did. We are saying that in the hypothesis space, first of all, okay, et cetera, is equal to y minus beta naught minus beta one x i square. data and as somebody else over all data points now in the hypothesis space so the data exists in real space whatever the data looks it exists in the XY space, the real data space. But in the hypothesis space where the coordinates are, by the way, a bigger drawing board is on the way, so I won't have to scroll up and down so much in a few days. have to scroll up and down so much in a few days. Beta node beta one. In this space, the hypothesis space, we have a parabolic or a surface or this convex surface which achieves a minima at some best point beta I'll just represent it as beta till there you can assume beta naught beta 1 is built out of it it is this but I'll just represent it as a beta to the vector this is it what exists and these things they form contour lines. These are, if you project down this, there are these contour lines in this. So if you start with some random point, you will have to find a path going to this. And this journey is your journey of gradient descent. So we won't recap that. I mean, but if you want to remember the rules of the gradient descent, it was beta minus alpha grad of E and the derivative of E. So this is your ML100. So go back and look at the nodes. You'll see that this is your gradient descent step. Gradient descent step. In other words, go opposite to the gradient, right? If you're here, go opposite to the gradient. Actually, you know what? Let's go through the derivation of this gradient descent. How many of you would want me to recap the gradient descent step? Can you do it, please? Okay, let's do that. But if we do that, it's 8.25. Guys, give me about, let's take a break actually okay let's take a 10 minutes break 10 15 minutes break and then come back so to motivate this I will just take a simple example of what a gradient about gradients to illustrate a point. See if you have a function, let me take a function y is equal to basically your fx that looks like this. It could for example be something plus something x squared or something like this. This is your x axis and this is your y axis. I want to explain what gradient descent is the basic intuition and again I'll move a little bit faster because we have done this before gradient descent is a way to start from any random point and reach this destination this point what is this point this point is your arg in a new language argmin of fx, isn't it? It is that special value of x, x tilde, where this function is minimized so far so how do we reach this how do we how do we come to that there's a very interesting thing to observe here and that observation has to do with the slope of the line look at this what is the slope here guys at at X tilde slope is is it flat rising up or falling? Flat. It's flat, slope is zero. Now the definition of slope is, let me just remind you, the definition of slope is increase in height, height for unit movement, unit increase of X, unit increase of X. So suppose I'm here at this point, if I make a small increase, delta improve increase Delta X and this is the step forward so I would go from the point this point to this point so have I increased or decreased in height so when I go from this point A to B, let's take negative slope X and B is equal to X plus Delta X, right? The height decreases. So you say that in the vicinity of X the slope is negative right the slope so y decreases in going from a to B ie from X to X plus Delta X Isn't it guys? So why decrease decreases in going to that and so the conclusion we have is slope is therefore at a Slope is Negative Negative now, let's look at the point this point let me call this point Right or At x prime. If I go a little bit a unit distance forward. Delta x forward. I go from, let's say, see to D. Right. So when going from C to D. from Delta X prime C is equal to X prime to D is equal to X prime plus Delta X right do we increase or decrease in height did we what did we what did we agree it increased positive slope yes it. So we increase we went up the height increased So slow Is positive. Slope is also called the derivative of a function. Right? Slope is of function so we can say it's usually written as a D effects DX or a dy DX and sometimes for shorthand people often use a prime to signify right pronounced as Y prime they often use this I wouldn't because I tend to use X prime Y prime just for a point so I will not be using this notation right so not be using this notation will not use this rotation so these notations are a comment and I'm just recapitulating the calculus that you have learned in high school and this is just a recap of what positive. So here the slope is positive now comes a interesting thought guys at this point suppose you are a day if you want to your goal is to go home to find the optimal point, this. So suppose you are at A and therefore this is the error. In which direction should you go to decrease the error, decrease the Y? Should I go in the positive X direction or negative X direction? At A. Positive X. In the positive x direction right so you can say at a at a we should go go in the positive x direction. Slope is negative. But dy dx is less than zero at this point. What about b? At c, where is c? Let's go back and look at c. At c, if you are at C, guys, do you want to go C? This is the F-C. And do you want to go in the positive X direction or negative X direction to decrease Y? Negative X. is y negative x direction at C we should go go in the negative x direction here Here slope is positive. Is positive. So do you notice something very interesting if the slope is positive, we want to go in the negative direction. If the slope is negative, we want to go in the positive direction. Isn't it? And if the slope is zero, we want to just stay put. Isn't it at this point?, we want to just stay put, isn't it? At this point, do we want to go anywhere if we reach this point where the slope is zero? No. If slope is zero, so let's write that condition down at go nowhere. go nowhere. We are already home. Right? We are already home. So observe. So one of the things you can do is you can say that at any given point to make, to minimize the function, point to make, to minimize the function, go, the next value of x should be, or let me put it this way, the next value of x, that is, should be the previous value of x. In this case, you want to move in the positive direction, right? But the slope is negative. i did this dy dx and the slope can be a big value i multiplied by a small number a small step i take a small step against the slope the slope is negative i want to move in the positive direction at at a remember once again this is a this is C a day slope is negative dy DX is less than 0 dy DX is greater than 0 just to remind you so at a what do you want to do if you take a if you multiply the slope by a negative number negative of a negative is a positive number isn't it guys yeah and so you're taking a step in the right direction would you say that the next value of x x next will be somewhere here this will be your b larger than right it will be where you want to be so this is it at now let's look at this at c what do you want to do slope is positive once again your next value of x should be a hop back here you want to hop actually let me make this hops more visible use another color you want to move in this direction here you want to move in this direction isn't it so you want to do at c what do you want to do you slope is positive once again you want to go against the slope the dy dx does this make sense guys if you take a little step like this and alpha you can take as a very small amount like 0.01 you take a tiny step it's a tiny step in the right direction and you can keep making those tiny steps till you reach home right here is your x tilde so to go towards x tilde to go here also towards x today is this obvious i think i'll say for your point c you are missing an equal and x there i believe for your point C, you're missing an equal and X there, I believe. Oh, right, right, right. Oh, I'm sorry. I forgot to write that. You're right. Thank you for pointing that. So minus the previous value of X minus the negative of the slope, because here the slope was positive negative and here the slope is positive and so you realize that irrespective of where you are on the increasing or decreasing slope The general rule to take a small step towards home, towards X tilde home is that your next value of X should be the previous value of x minus some small number some small fraction of the dy dx right this is it do you see this guys i hope this is uh this intuition is uh clear to you from this i'll recap for any function if you go against the slope you're going towards the minima right many ways of saying the same thing. So now let's generalize it to higher dimensions. Suppose y is a function of Is a function of x1 x2 X n. How do we generalize. Now there is no notion of dy dx because the question is what is x x is a vector now right and so it turns out that what you can do is you can say that suppose i have a surface you can say that suppose i have a surface like this i can make a small step like at any given point i can either move a little bit in delta x1 direction or delta x2 direction let's say okay if you make it x1 x2 then i'll give the i should reverse the order delta x2 x1. Are we together guys? I can see how this function fx varies as I move in this direction or this direction. So you can say we can look at the change in y as we move a step along one of the axis variable axis isn't it all you can do is take a small step in this direction or this direction each one of them will produce as derivatives the partial to this the slope specifically specifically along each axis keeping other not moving not changing along other axis that's important criteria not changing X value along other axis is called uh it's a partial slope right it's sort of a partial it's just a slope if if i just change x1 right so how much it changes the value changes for partial derivative derivative this is what a partial derivative is and the way we write it is you'll see me use this notation dy dx I write our DF X vector along this particular direction. You'll see these two notations often used here notation. So then comes a very interesting question. Suppose I make a change from the unit vector x to X plus delta x means if I make small changes along all of these, if you make along each of these values, then what happens? How much change do I get? It the what is the value of the function x plus delta x and that value actually can be represented right by a saying that it is actually just as in the case okay so let me before i go there let me bring up a concept so if you put all these, let's say that there are two axes, x1, x2, right? And you're looking at y as a function of this. So you will have dy, let me just write it as dy, dx1, dy, dx2. Both of these you can write isn't it so there are two components here if you treat these two components now here's a magical thing you can do and you'll see why if you can treat these as components of a vector see X is a vector X is equal to X 1 i plus x2 j. This is how you write the vector, isn't it guys? The standard notation. And it is also, but in machine learning, you don't tend to write i j k, you tend to write x1 x2, a column vector. A column vector. Vectors are written in machine learning as column vector. Vectors are written in machine learning as column vector. And it is absolutely equivalent to the same thing. It's just a different language in which we write it. So X is a vector. Now we notice that when you take partial derivatives, you can take partial derivatives with respect to each of the Xs. So we can therefore ask, what the what is this vector what is this x 1 d y d x 2 what is this column vector which is the same as d y d x 1 i plus d y2 j what is this it turns out that this is a very useful thing it is called it is very instead of just like you can have a slope it is the generalization of the concept of slope It is this notation, nabla, or the grad. It is also called the grad or the gradient. These words are all used, nabla is the Greek letter. So it's the nabla of a y. It's called the gradient of y. And more generally, people can just write it as an operator as an as an abstract operator you say this is just this it is an operator that is waiting to eat something 1d x2 you notice that this is very funny a partial derivative of what Right. You haven't specified that. But so if you put y here it will become this Right. This is and so this is equal to by definition. This is the partial this is called the gradient of why now. We realize that in ordinary differential equations, the vibe. Plus Why plus delta y Was equal to basically x plus you know it was basically delta x so i wouldn't actually let me not go into the calculus just take it as a fact of life delta y in single dimension in one day variable one variable is dy or dx times small change in X a small change in Y is this slope times the small change in X right which is sort of our intuition is and it's very obvious here that you are here if you make a small step forward the decrease is proportional to how big a step you made and how steep the slope is isn't it yeah so that's the idea how much you lost in height or gained in height is proportional to that. So this is it. It's generalization. It's the change in Y in the, in a vector notation as a vector, let's just say that, sorry, the change in, sorry, in a, in a Y when X is a vector is actually gradient of Y dot product with small change in X, because now the change in X could be as a vector, right? You, you made a small change delta X and so it is given by this equation small changes in this now what does it mean let us study this gradient and will be practically done with the so this is just the mathematical background to understand regularization what what does if gradient is a vector do you realize that gradient is a vector then where does it point vectors point in some direction, isn't it? So let us study that. Consider a circle. If you have a circle, Let's take an arbitrary point. This point is a P is XY right from the origin the line that connects to this is the P vector is that obvious so now what is the equation of this point Y right at this particular point do you realize that the equation of this function is fx? Actually, let me use the notation of x1, x2. Yeah. Not use y. X2. I can write it as x1 square plus x2 square. Right. And the circle is, x1 square plus x2 square, right? And the circle is fx1 x2 is equal to some constant r squared. Would you agree? It is just a constant, what you call a contour line of this function. Now let us find the gradient of this function and see what it comes out to be gradient of this function is equal to partial of this function with respect to X 1 partial of this function with respect to X 2 now look at this if I take a partial derivative means x2 doesn't change what is the derivative of x1 2x1 and the derivative of with respect to x2 is 2x2 and you observe something very interesting for a circle you observe something very interesting for a circle and this happens to be just pointing in the direction of twice just the P vector isn't it P is your P vector is given by X1, X2. Are we together? Right. So what does it mean? Where does the gradient point? Gradient is just a vector that points. This is gradient of FX1, X2. Isn't it guys so far so good guys now there is an interesting lesson if i look at lots of circles of different these are all the functions have different constant values so you would agree that along the circles the function has to be constant right those are constant value surfaces or constant value curves for the function what is the gradient point relative to those circles what is the direction of the gradient is it parallel to it is it inclined to it or is it perpendicular to it it's perpendicular. So we notice the fact we notice that Let me write it in a different color so that this is emphasized or maybe I'll use the same thing. The grad f The grad F, let me write it as x vector, is, to the constant curves, i.e., the circles, is some constant r squared, right, for some r. Isn't it it guys? This observation is very general. The point is what is true for the circle is always true. This is a general fact. Gradient of F is perpendicular to contours of F effects for any and every function. This is a very general fact. I used the example of a circle to illustrate it, but actually this is a very general fact. It's a thing you need to remember in mind. Well, that's a long detour. So where are we going with all this math? That's why all this mathematical preliminaries. So here is where it is coming from. So let's go back to our energy surfaces are sort of error surface. So this was a beta naught, beta one. These were our These were our These were this thing. So here, do you notice that you had These contours surfaces right also observe something where does the gradient point the direction of the first of all, it is perpendicular to the constant surfaces, but is it pointing in the direction of increasing F value or decreasing F value? Does the value of F increase as you go in the direction? Like what does the radius increase as you go outwards in the direction of the gradient or decrease? Increases, isn't it? In fact, it increases the most. Perpendicular to the gradient is of course the flat, the constant surfaces. So the second observation is every F and points to the direction of maximum increase of fx at that point it's a very general observation the gradient gradient points to the Jay Shah, Dr. gradient descent and therefore when in our case what do we want to do we want to decrease the error the error so we go go against the gradient, right? In other words, if you are at any given beta value here, let us say that this is some arbitrary p given by beta naught, beta one. What is it? What do we want to do? What, and this is the perfect, a beta naught tilde, a beta one tilde. You want to go here. What is the way to go there. What do you do. You will say beta not next value should be equal to the previous value of beta minus alpha times. Yeah. So in vector notation, you can go if you write just beta minus the gradient of the error function right This is the equation the celebrated equation of a gradient descent That is essentially the heart of machine learning the learning of machine learning often amounts to the gradient descent towards the minima that is why I took some time to explain this concept because it is not just a concept in machine learning it is one of the concepts central to this field right it's the gradient so if you break it out into components, you could say that beta one or beta zero next zero next is equal to the previous value of beta zero minus a d e d beta zero beta one next is equal to beta one minus alpha d e beta one right and so forth you can keep going down to other values of beta n let's say suppose you have beta n next is equal to beta n minus suppose you have n dimensional space parameter space the beta n right it's just a general like it's just this thing rewritten in simpler, broken it out into components. This is your gradient descent. This is how you solve the problem. Now let's come back to the machine learning part of it. This was a detour through calculus, basic calculus. We took the detour. Now what happens is and if i'll mention this without giving a reason why when you take high degree polynomials and when you have overfitting when you take large n for poly regression regression you will you will what will you end up with you will have beta 0 beta 1 beta n isn't it guys you will have different parameters beta they tend to get very loud. Or large. So what will suddenly happen is your beta naughts, good values of beta naught will be less than 2, minus 1, or something like that. But these values, they will suddenly start looking like minus one five seven nine six three right this will look like plus one sixty eight two one four two do you notice that these values of beta they blow up the values so every factor seems to matter a lot each of the parameter seems to matter a lot it seems to become very loud you know very large values geometrically would and I wouldn't give a proof of this but just take it as a fact that this happens so what does it mean in real terms it means that if you look at the error surface this error surface right it just runs away. The minima here is very far from the origin, runs away from the origin. That's the problem. And that is what causes, this causes oscillations. we observed we observed for large n right overfitting is there high variance our terms so what does it mean if you go and look at this curve, let's go and look at this. Yeah, let's go back in this picture. What you will notice is that for the blue line, the betas are reasonably small. Beta naught, beta one. For the, for the, where is this parabolic curve? For the red line, you'll have beta naught, beta naught beta 1 beta 2 they will be also reasonably small for the yellow line three bit four betas beta naught beta 1 beta 2 beta 3. they'll be a little bit pronounced but still reasonable but by the time you get to green lines your parameter values are big and by the time you go to the pink line your betas are big. And by the time you go to the pink line, your betas are really big. Are you getting that guys? They become really big and that is what is causing oscillation. One is pulling it in positive direction, the Y value, one is pulling it in a very negative direction, right here, let's do that. So if you have very large values, you could realize with positive and negative signs, they are all pulling the Y value in different directions, positive and negative. So you start seeing a lot of oscillations in the function. So what do we do to cure it? You basically say that we are not going to take the absolute minima that is here, you know, of this function. Let me draw it out again. So what do we do? How do we solve this? Guys, I'll continue on for another half an hour. I want to finish this topic. It's an important topic. What happens is that if you look at this bowl and our origin is here. Sorry, let me draw it here. Let's just say that our origin is here so you notice that this has run away here very far off right I'll be turn on it's a classic imagine that there are lots of meters this is your error surface right so here are the contour lines. What you can do is you can say regularization says that we don't want the absolute minimum. We are happy so long as we go a limited amount from the origin. Let us say that you set a limit radius R and you say that around the origin I will go only radius r but no more than that. So whatever is the minimum value of error lifted out from here wherever it meets the error surface, whatever error is there within this desk within this space is what I will consider her. Let me draw it out in two dimensions in 2D. Sajeevan G. In Sajeevan G. The contour view. Sajeevan G. What you have is this Sajeevan G. And again, I'll just draw it. You're seeing that I have a certain radius of a budget. Right. And I am not willing to go or find a solution above it. the real solution is here very far away and it has its contour lines like this so and then more contour lines go here more control more contour lines go here so you notice that inside the disk let me mark the disk as this let me mark it with big fat strokes oops that didn't Let me take this. This is where you want to be. There's one intuition I can give. In India, we have the story of the Ramayana. And there is a scene in the Ramayana, the Panchavati, in which the story goes that two brothers, one is a king named Ram and he has a brother named Lakshman and they are very, very close of course and Ram's wife is Sita and now they are the good guys and there is a demon called Ravan who is always on the watch to grab or cause mischief. So Ram and Lakshman, the two good guys, they have to go out during the day. They're in the forest. They're in exile in the forest and they're supposed to go out and hunt for food, find food, look around for food. So while they do it, Sita is alone at home and there's all sorts of danger. So what Lakshman does, the brother does, is he draws around the house a circle. And that is our circle here. You draw a circle. your is your limit and you just say that so long as you stay within the circle you're safe is the enchanted circle no demon and no bad influence can find its way here find its way into this circle right can come into the blue area. Think of it like that. In terms of our regularization, what it means is that you don't go out of the circle because you know that if the values of beta blow up, you will have overfitting. So you set a budget. You say that all the betas must, the sum total of all the betas the magnitude must be within this point right so you could pick any point here but it cannot be more than a radius let me just put this here and the radius this is the origin it cannot be more than a certain distance r from that. So now the question is, you realize that above it is the error surface rising. You realize that this is the point of minimum error. This is your beta tilde here. This contour line represents, if you think about this, below this are these contour lines right they are they are mapping to this contour lines so what happens every concentric circle that is the further you are in this a concentric contour lines does the error increase or decrease they represent surfaces of greater or lesser lesser error this is the point of least error this will have this will be this error surface increase increase increase but what you're saying is so what is the point of the least error inside your circle so let me call this surface e a this surface is e B this surface is EC this surface is ed so which of these if you want to find within your circle a point which has the least error which would be would you would you agree that ea all the error so ea constant error surfaces ea is less than eb is less than ec is less than ed yeah yeah that is clear so now i ask you this question what is the point of least error not absolutely least error which is here in beta, beta tilde, but the least error within this enchanted circle. Just at this point, isn't it? Let me mark. This point is some point this, this point, isn't it? This is my, this is the best I can do. Within the enchanted desk. We agree with this, right? now let's think about this at this point see this is a circle we just realized that in a circle where does the gradient of the circle point so this circle is given by F a beaten beta naught beta 1 is equal to beta naught square plus beta 1 square and we can generalize this is the equation of the circle does it make sense guys we are in the parameter space so here the axes are not x 1 y 1 but beta naught beta 1 so this is the equation of the circle, isn't it guys? This makes sense, isn't it? And the circle is just, is equal to some constant R squared. I'm sorry, R squared. So far so good, right? So now what is the gradient of the circle? Gradient of F is equal to whatever. You can just say 2 beta naught to beta 1 so 2 times this it would be at this point which is where would the gradient of this point and the origin program trees and it would point here gradient of the the constraint is this right constraint surface beta is this now what about this error surface you see that the error surface EA let me go to the white color now the EA this error surface this is a contour line of the error surface point yeah right where would the gradient of that point which is the direction of maximum increase of error let me know positive opposite direction isn't it let me use a color that would be visible in this. So I've used maybe green green might serve a purpose. It's a green Dhanushka Samarakoonalakrishnanan. Yes, let's take this. So I'll just make it like this. So the Dhanushka Samarakoonalakrishnanan. This would be the gradient of the error surface. Would you agree the gradient of the error surface would you agree yes the gradient would point like this and so you observe something that you be observe that gradient of F and gradient of E point in opposite directions. Opposite directions. directions. Right? And so we can write gradient of F is equal to minus the gradient of E, but we don't know what the magnitude is. So we'll put a lambda parameter here. Right? So far, so good, guys. The gradient of the constrained surface and the gradient of the error functions, they point opposite to each other, then there must be up to a negative sign and a proportionality constant. This must be the equation. This is how you write two vectors which point in opposite directions up to a proportionality constant. They should look like this. constant they should look like this yeah that's right so it turns out that what is what i just showed you is not just true of uh just error functions it's true of any function and any constraint here i made the constraint into a circle you could take any constraint surface surface right and you could take any function the point the optimal point where these two things these two surfaces touch just like this you know they glancingly touch tangentially touch and represents a point of the best you can do is always given by this equation and it was discovered by the great mathematician Lagrange. I hope I didn't mistake is it Lagrange or Lagrange? Could one of you please verify? I think it's Lagrange. Oh no it's not it is just L apostrophe oh goodness I should know this. Could one of you please verify and tell me which is the correct one? So it is the Lagrangian. You say this is the Lagrangian multiplier. Maybe it's Lagrange. Lagrangian multiplier. L-A-G-R-A-N-G-I- g i a n lagrangian multiplier yeah so the name is lagrange isn't it the person lagrange yes magician lagrange so you typically pronounce it as lagrange matthew matty or at least i do i don't know if that is correct. The mathematician, he was a great mathematician. He discovered not just this, his work is prolific in calculus and analytical geometry and analytical analysis. So this is actually what I did is using the example of our error surfaces and constraint and regularization. Actually, I took you guys to the discovery process of a fundamental result in calculus. This is, in fact, constrained optimization. This is it. Optimization. And so what you can see is this is constraint optimisation and when you do it when you apply the disk a budget a unit circle so what so let us summarize my spelling is getting more atrocious summarize with unit circle as constraint. Right as constraint right you get at a We can find The constraint, we can find the minimum error within the circle. That is the solution we will take. So let's explore this a little bit more. There's some very interesting consequences with this. So we have constrained E, this is equal to minus, okay, I think I don't know which way I put the lambda, I put the lambda on the E. Let me do it the other way around, actually, because that is much more conventional. Let me put it doesn't matter, right, if we are just defining the lambda. So who cares? Lambda multiplier. Typically you put the lambda on the proportionality constant here. So I'll put it here. This is more traditional. Are it is by the way, just convention nothing nothing more This is it now let us rewrite this equation what we are basically saying is e a beta plus lambda F beta Gradient of this is 0 this is what we are trying to find isn't it I just rewrote this equation there's and from this we conclude that in other words we minimizing e beta plus lambda f beta. This, right? What you call it, see error function we were minimizing before. Now we are minimizing a different function. This is called the loss function in machine learning. And now you got introduced to your first loss function which is not your sum squared there actually we did there was another one in logistic did i do the mle derivation otherwise in mathematical methods will of course do that function loss function this is called the loss function the word loss function is pretty very Jay Shah, Dr. Anand Oswal, Jr.: So, Jay Shah, Dr. Anand Oswal, Jr.: So, Jay Shah, Dr. Anand Oswal, Jr.: So, Jay Shah, Dr. Anand Oswal, Jr.: So, Jay Shah, Dr. Anand Oswal, Jr.: So, Jay Shah, Dr. Anand Oswal, Jr.: So, Jay Shah, Dr. Anand Oswal, Jr.: So, Jay Shah, Dr you say solution, the optimal point Point is where Is where point beta. Let me just say This is The point beta till day is where it keeps. By the way, do you notice that I'm using a very scripted L, right? I'm not just writing it like L beta. So it is much more common to use this calligraphic L for the loss function, right? Mathematicians tend to use the calligraphic L for the loss function. Mathematicians tend to use the calligraphic L. If you use just ordinary L that is used, but more common is to see this calligraphic L used for the loss function. Loss function is one of the most important constructs in machine learning. You can say that machine learning walks on two legs. First I told you that you quantize the error and then you decrease the error. Now I'm extending it a little bit more and saying it's not just the error, it is a loss function. In any given problem in machine learning, you first write a loss function. Once you have written a loss function, then you can do gradient descent on the loss function to find the minima. Right? So use gradient descent to find beta tilde. Beta tilde is arc min beta for the loss function. Right? For this. It is that value of the beta that will minimize the loss function. Are we together? And this is the, this is a core result. This was where the long journey was coming to. This result is not just true of the topic at hand. It is generally true in a wide variety of situations in machine learning as we will realize we just did constrained optimization now let's look at it remember that I told last discussion that we did that there are many different ways of defining distance the unit circle I will go back to the top the mannequin distance equilibrium yeah that's right I will remind you that we did this is Did I do it in this one? Or I did it, yeah, this one. L1, L2, yeah. L1, L2 norms, etc. So this is it. Yeah, L1, L2 norms. Yeah, we were talking about this distance notation of norms so we will go back to that and ask this question I'll just recap what we learned there so you do you guys see that you know all these ideas tied together in machine learning right you may say wow it's such a vast space of theoretical constructs but you would be pleasantly surprised to know that pretty much if you got here as far as practical machine learning is concerned this is all the concepts you really need right a loss function gradient and its gradient descent that's all that's all you really need in machine learning to a large extent I will come to that so what are the Minkowski norms if you remember the Minkowski norm of two points was X prime was equal to X I X 1 minus X 1 prime to the power L plus X 2 minus X two prime to the power L. This, the whole to the power one, right? Now, now comes the surprising thing. Look at this little desk that we made in the Lakshman Rekha that we drew here, the yellow line. This is a unit circle, isn't it? What is the definition of a unit circle what is the function definition for any seek for L is equal to 2 the effects 1 so now let me write it in the language of beta because we are in the parameter space beta beta naught beta 1 would be beta naught square plus beta one square, isn't it? So the constant values of this function were the circles. What is the gradient of this? So well, you already did that before. So this is it. Do you notice that? And what was error in in this thing in the loss function if you remember the loss function was was equal to the error function of the beta plus lambda F beta and let us replace it with what they really were the error beta was actually equal to, going back to the old story of where we started from, it is y minus yi minus yi hat square. Remember, y hat beta naught plus beta one xi square plus lambda. Now, what is fx? Beta naught square plus beta one times square plus this, right? And wow, this is the equation that people write in books. Though I would highly recommend that you think in terms of the way I taught you as a constrained surface between an error that you want to minimize given a certain constraint. This is the constraint. And now your book essentially writes this equation without, and there is a pictorial explanation for it. So this is what they're trying to say, right? I've given you a much more detailed picture of this and you write therefore L beta is equal to sum over all the data terms of yi, yi minus yi hat square plus lambda sum over beta i square. This is beta j, right? Beta j means j is equal to 1, j is equal to whatever number of dimension there is n dimensions right sorry whatever degree of polynomial you are taking beta naught beta 1 beta 2 beta 3 whatever does this look like it so when you use n is equal to 2 this is called the ridge regularization then you say alright that L is equal to one right then the L is equal to one Minkowski norm means what remember what does it do to your constrained surface now it became a diamond yes I'm so diamond right diamond becomes a diamond so let me go back and color this diamond big bold strokes okay I think I'll have to write it so now the in this one it is still remember this is a circle. It's a unit circle, isn't it? Looks like a diamond, but because of the L1 norm, this is how a circle would look. All points on the boundary are unit distance from this origin. If your distance, if you define your distance where the L1 norm. This is where we go and this is your thing let's go back and draw our um the contour lines contour lines where do they seem to touch corner on the axis this is something very interesting this uh this is the x1 axis very interesting this this is the x1 axis and this is the x2 axis what happens is more often than not this pointy edge notice how the pointy edge Pointy edge of the diamond pricks the error surface, error contour surface. Isn't it? This is your point that you're looking at. It's no surprise actually that very very often because it's sticking out if you have your you know imagine walking into the bar and you have your elbows like this have you ever tried doing that imagine walking into a crowded bar and you have your elbows like this what will happen you'll make friends not yeah yes exactly you will hit a lot of people and annoy the maximum number of people that you can so that is it so these are the elbows sticking out so it is much more likely to hit the contour surfaces the contours of the error the error contour surfaces of the error are in this in this space in this plane and so what it means is now what happens when it hits it what is the value so by the way this is not x1 x2 i apologize remember we are in the parameter space this is beta naught one right uh beta one in this space so what does it mean at this point what is beta 1 at this it is 1 0 right yes exactly so what will happen is at the optimal point beta naught will be will be some value right and beta 1 that will be beta naught today and a beta 1 will be will will vanish perfect value of beta 1 will be 0 do you see this guys this point is beta naught tilde and 0 so what does it mean in terms of our equation what it means is that Y as a function plus a beta 1 X what happened this term just disappeared right means X doesn't matter in effect so now the way it really works is not like this the way it works is let me just call it deliberately I'll cheat let me go to n dimensions it's a polynomial of n dimension right so the n dimensional space so beta naught plus beta 1 let me just say beta nth dimension right beta 1 X plus beta 2 X square X X to the power n what will happen is when you hit it along this axis a lot of the beaters will start disappearing right so what will happen is out of the beaters that you have with a 1 and beta 2 beta, beta n, beta i, let me call it beta j, beta n. What will happen is because the you punctured the contour along this, some of these bet beta may start disappearing and your problem therefore reduces to a lesser number of terms in the equation. Do you see that guys? And so what will happen is your model will be simpler. Not only that you have done smooth and reality reduction as a side effect. You have Done Dimensionality. In what sense You said that all of these high order terms matter. They don't matter, right? Reduction. And now let's generalize it to many variables. Let us say that you write Y this, suppose you write Y as a function of beta naught plus beta one X one plus beta two X two. You know, this is really a different feature. really a different feature. This titular regression, the Elven norm will start destroying some of these features. So you will get, it will say that those features don't matter. You can get rid of it. What does it mean? You have reduced those features, remove those features from your feature space. Therefore you have reduced the dimensionality of the problem this is actually one of the great benefits of L1 norm there is a name for this it is called the LASSO regularization surprising that it is actually this well ridge regression was discovered a long time ago LASSO was discovered somewhat more recently and the authors of your textbook actually they have done considerable amount of work with the LASSO they even have a book on LASSO regression is just L is equal to 1 norm and so your beta function is this is y minus y hat Square summed over I. But what is your function now. This lambda. Remember What is your function. This This is equal to the unit circle is defined as a beta not absolute value plus beta one absolute value plus beta two absolute value so on and so forth right so i can write this as l beta hat is equal to sum over i y i minus y i hat square plus lambda summation over beta j absolute value j is equal to zero to j is equal to suppose n dimensions right whatever number of dimensions that you take it will be this and so when you minimize this is the loss function for lasso this is your loss function see guys this these things are given in books. Typically it writes the equation and quite often you don't know where the equations are coming from. So I went through the derivation. Now, why did I go through this long mathematical derivation? Last functions are at the heart of machine learning and quite often last functions have constrained terms and so forth. So I thought that if today I introduce you to how people create loss functions how they reason about it how they think and find ways to improving the model or regularizing it or doing things you have gotten to one of the key activities researchers do in machine learning. So last function for last two. And this is, I promise you, the most technical we'll ever be in our machine learning workshops. But do this much level, do please try to understand because you need this much. If you really want to be good in this field, you should know the foundations and this is these are the foundations. Right. So you should know that. And by the way, all of these things we will review again when we do the math of machine learning right here. I'm more focused on the regularization aspect of it. So this is the loss function and it has a beautiful effect. What happens is that there is this regularization parameter you know this lambda what what lambda does is as you increase lambda the value of the different beaters they they begin to go to the let's say that they start with certain values they go sort of they go right? Or let's say that you normalize, actually, let me take a more normalized view of it. Let's say that they all started some normalized value at the origin, this betas. So what will happen is some will fall to zero here some will fall to zero and some will lot of them will fall here some will go here so suppose you take a cut off of beta here this point suppose you take this as a cutoff so what will happen at this cutoff these values these betas will disappear right these beaters these these these features will be because associate is each beta is associated with a feature these features will disappear why will they disappear because their coefficients their parameters have all gone to zero at this value of the lambda. So what you do is you choose your lambda wisely so that a few at this moment, this particular beta is alive, this is alive, and this is alive, but these ones have So you do dimensionality reduction by sliding Lambda back and forth, right? Sliding the Lambda back and forth and you get certain feature reductions. Go ahead. As if in this graph, you're assuming that, that you know all the betas are kind of in the beginning and the lambda is zero. But ideally, they will be different, but still the effect will be the same. No, no, whatever their values are, yeah, it will be different. One way to normalize is divide everything by normalize, normalize it to one and then see but in reality see I do that more complicated picture so this is what happens you have dimensionality reduction so we learn two methods l2 which is more common is the rich it's called the ridge regression dge ridge regression ridge DG rich regression, rich regression been practiced for many years here. The constraint surfaces is some over beta I square L1 lasso regression. You want to use this, f beta is equal to sum over the absolute value of i, you know, the Manhattan distance, unit circle of Manhattan distance of this. So this is it. And it's quite interesting that if you understand Minkowski norm, you understand both of these ways of regularization so easily. Not only that, you can cook up more regularization so easily not only that you can cook up more or more regularization methods of your own for example why not try this why not try L3 I mean you can do whatever you want basically now one of the questions that arises is which is better they both have their advantages and disadvantages lasso has one advantage that certain features just disappear you have dimensionality reduction rich will never make the features disappear it will just make it very small because it's circular so all the features all the betas will have some more or less non-zero value the ones that really matter will have bigger values the ones that don't matter will have smaller values but they won't disappear so that is one of the downsides of the rich people say but you don't get the dimensionality reduction benefit but the problem with lasso is that it over suppresses right it is too pointy sometimes it gets it's of too many features right and so what people do is between l1 and l2 again you have to try and see which gives you the best cross validation error. There is also elastic net What does it do it basically says What get a loss function With both l1 and L2. So then you end up with two lambdas, right? One lambda, one associated with lasso, one lambda, two associated with ridge. And so you try to optimize this. Now you have two hyperparameters to search for. So ElasticNet is a very obvious extension. It says in the loss function go at both the terms. Both the Minkowski L is equal to one term and the L is equal to two term that becomes elastic net. So guys, this weekend on Saturday, we will do a lab in which we'll see the effect of all of this regularization. So guys today, today you know i developed or recapitulated a lot of math we took a lot of time recapitulating the math which we have done in the past but we needed it today to understand the regularization but once you understand regularization let me summarize higher degree polynomials without regularization they tend to oscillate. You can suppress it with more data, which is the best way if you can, but quite often you can't have enough data. Data is rather an expensive commodity. So you have another technique of constraint optimization. And again, as I said, in very, very complex models like deep neural networks, it is a given that the parameter space is vast, hundreds of millions of parameters. And even if you have hundreds of millions of data, it's still not sufficient. So you need regularization. You need the technology of regularization and constraint optimization. So that is a given by putting a disk around it. So we talked, we went through the whole explanation of what is gradient descent. Some of you asked for it. So I did the recap of that. So at any given point, the function achieves a minima. You go in the direction of the minima if you walk against the gradient. And we saw that both in one dimension effects and in multiple dimensions. Gradient points opposite to the... Gradient is the direction of maximum increase. So if you want to quickly decrease, you go against the gradient. If you are in Mound Diablo, the last thing you want to do is go against the gradient. What will happen if you go against the gradient if you are in mount diablo the last thing you want to do is go against the gradient what will happen if you go against the gradient you'll tumble down the hill isn't it because gradient is the steepest path right up the hill you don't want to go the steepest path downhill because the steepest part downhill probably means you're falling down okay so this is it this is what we are saying that this at in a when you have a constrained region and you say give me the minima here then at that point the minimal point point, the gradient of the constraint surface and the gradient of the actual error function will be opposites. So in other words, you can write it in this way, error is up to a proportionality constant and a negative of the other one. And that is essentially the equation of Lagrange multipliers. This is called the Lagrange multiplier. At some point in calculus, you must have have learned this many many years ago in your calculus anybody remembers learning this in calculus so but if not then well today you learn something very important it's constraint optimization thing and so regularization is as simple as saying what if my constraint surface are the minkowski norms you know unit circles with different minkowski that's it all of regularization can be asked in one simple statement what if i apply a constraint surface constraint in the unit circle constraint in the parameter space in in the hypothesis space, that the values of the parameter must be within the unit circle. The moment you do that, both of the regularization come together, come like this. So in other words, if you were truly a very, very skilled mathematician who knew all of this theory very well, the way to explain regularization to that person would be, oh, it's constraint optimization in the hypothesis space of the error function. The constraint is the unit circle with different norms. That's it. So that gives you your LASSO and your ridge regression. So that's what I meant. That's a summary of what I am saying here. Elastic net is the hybridization of the two so I would like to stop you   .  .