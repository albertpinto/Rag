 Now, let's go and do another problem, another thing now. We will, this thing, do you notice that this distance keeps coming up all the time? And quite often, or basically our intuition often deals with Euclidean distance. Now there's a bit of bad news I'll mention. When you come to distance, especially with Euclidean distance, but generally with uh there is a problem it is called the curse of dimensionality i will start with the curse and then i'll talk about another class of algorithms called nearest neighbor methods so what is this curse of dimensionality So, what is this curse of dimensionality? It sounds like a Harry Potter curse, right? Terrible curse from a Harry Potter story or something like that. So well, it is this. I'll mention a fact. Tell me if it sort of agrees with your intuition. So think of 10 friends in your mind. Actually do that experiment in your mind. Write down the names of your 10 friends and remember where they live. So just write it down on a piece of paper, 10 friends, actually do it and write down where do they live i'll let you do it for five minutes what if we don't have 10 friends 10 acquaintances if enemies are allowed to now i have a lot. And take them from around the country. Don't just say 10 of my neighborhood friends. Take friends who live in other cities. Thank you. Make sure that you don't pick friends from the same city. Just make sure that all friends are from different cities. And any country is okay, if you want. But make no two friends should belong to the same city are we done are you still writing some of you are still writing my two friends lived in the same household no that's not allowed My two friends lived in the same household. No, that's not allowed. All right, guys, those of you who are on Zoom, are you also doing this exercise? Please do it. It's a fun thing. Okay. Now, did you do that guys? Are we done? Okay. Now, make an interesting observation. How many of them live either next to the state border or next to the end of the land, you know, next to just the ocean or one of the big lakes. In US, the lakes themselves are practically the sea, size of seas. How many of them live next to that, very close to that? They don't live in the center of their states, but they live on the edge, towards the boundary of the state. Put tick mark against people who are reasonably close, let's say within 3040 miles of the boundary of the state. Or next to the sea or something like that. See next to a big lake. Lake Michigan, Lake Erie, something like that. Let me know when you're done. Is this tick marking then? Six out of ten. Six out of 10. Six out of 10. Anybody else? Same. But different friends. In your case? Three out of 10. Oh, boy. Nine out of 10. Nine out of 10. Three out of 10. Three out of 10. Okay. Six out of 10. Okay. Six out of ten. Okay. So the reason I asked you this, the general observation is, in my case, whenever I look at it, everybody seems to be near the sea or the end of the state or something like that. And here's an explanation for that. So obviously, ten is a very non-representative number. If you do it with 100, this becomes more obvious. But look at this experiment I'm going to do. Imagine a circle. I hope I know how to draw my circle. Shame on me. I used to be an artist and could drop pretty good circles. Okay, so suppose your friends are uniformly distributed in the circle. Imagine that land is a circle and your friends are uniformly distributed. So this topic, by the way, I'll spend some time on this topic because it is interesting. So imagine this is a uniform distribution. I hope I've made it reasonably uniform. If your friends are uniformly distributed, I'll just say points are uniformly distributed. I ask this question, where do half your friends live? So you would say, well, you know, this is an area. I will look at this, right? I will ask what radius, how far do I have to go from the center of the circle to include half my friends, right? So you would say that, see, this is a pi little r squared and let's say that the outermost radius is capital r right let me for sake of argument without loss of generality i can say capital r is one you know i scaled it down to a unit disk is that okay are you convinced that i'm not cheating i can just scale it and shrink the picture down to a unit disc. Well, states don't look like one inch long, but imagine you can project it down onto a map and it will look one unit long, one unit circle, right? Now comes the interesting. What is the area that you would agree that to get 50% of friends, you would need to have the inner area A, A should be a half. Would you agree? To reach half your friends, you need to cover half the area. We all agree with that, right? So now the question is is when do we reach half you know that the area of the circle is pi r squared right and to reach over and pi is the total area of this what is the total area of the whole disk pi right so to this should be a half isn't it? And so what does it mean? R squared should be a half. And R is equal to the square root of a half. And that is equal to 0.7. Isn't it? which should be a surprise you would you would have thought i would have to go half as far half the distance isn't it but you notice that you had to go 70 of the distance to get half your friends which means that other half of your friends live in the 30 percent of the distance to the boundary right they live here half your friends live in that outer circle more than half i mean say half of your friend lives out there and only half your friend live in the center right this gets so this is 0.7 and this is 0.3 there is an asymmetry right this is much bigger distance and this is much smaller distance now imagine that we are little particles in a sphere. Well, states don't look like spheres, but now that metaphor is beginning to break, but imagine how far do you have to go from the center of the sphere to reach half the friends if they were all scattered inside the sphere? So there what would happen is the volume of a sphere is four by three pi r cube. And let's say that this radius is one again, and the volume of the entire sphere is four by three pi. Isn't it? So this constants go away. You would say that this needs to be half. And so r is equal to cube root of one third can somebody tell me what that is 0.79 so let me start writing it when it is a dimension two dimension okay dimension one suppose all of us lived on a line uniformly spread. How far would you have to go to get half your friends? 50%, right? So you would have to go 0.5. Now, for two dimensions, you would have to go 0.7. For three dimensions, you have to go practically 0.8, right, up to the first decimal place now imagine uh likewise generalize it to let's say a 10 dimensional sphere right a hypersphere and if your friends are scattered inside a 10 dimensional hypersphere how far do you have to fly to reach and meet in what region would you find half your friends so then r to the power 10 would be half and r would be a half to the power one tenth isn't it and could somebody tell me what is the tenth root of half 0.93 so you notice that which means that half your friends seem to be sitting on the edge of the sphere, isn't it? Do you see this, guys? That in that tiny 0, 7% of the distance from the center, way out there, at the edge, most higher and higher dimension, something funny happens. Most of your friends seem to be living, I mean, not most, half of your friends seem to be living, I mean not most, half of your friends seem to be living here. Do you see that guys? And what happens when you reach, you know, we are dealing with data sets. What is the data set that we dealt with most recently? Housing. We had how many factors were there? Breast cancer. Breast cancer. Breast cancer, right? It had about 30 dimensions. So let's find out what is half to the power. So now you realize that the radius you have to go to is half to the power of 1 over D. D is equal to 30. What is the value you get? 97.7. 97.7. Practically 98. So what do you notice? And we have just gone to 30 dimensions. These days, we deal with data typically in no less than 100 or 1000 dimensions. And natural language processing is very common for us to and computer vision, very common for us to deal with at least 1000 dimension. So let's go 1000. Can somebody tell me what is the thousandth root of half? What does it tell you guys? That to reach even half your neighbors, you have to practically go to the edge of the sphere, hypersphere, isn't it? Because it seems that other half are practically camping out there on the very edge, it seems. And why is that so? The reason it is, is that as you increase it, imagine this, you have people on this point. One intuition I can give you is imagine your friends are sitting here. Then you have to go half the distance. See, look at this. You have to go half the distance. Well, let me cheat a little bit here. Yeah, this is your origin. Now suppose you bring in one more dimension. Well, it was a circle, so I shouldn't bring in a rectangle to confuse. Oh, sorry. Yeah. So suppose I make this into a big round circle. Now, all the same four, one, two, three, four neighbors, well, this is not a good circle. I apologize. This is a terrible circle. I have to make a much bigger circle. This circle. You want to uniformly distribute your eight friends. Where will they go? They will all be all over the place, right? Take a little bit of an argument to convince yourself. Here, let me just do it better this time. One, two, three, four. One, two. 50%. Now, if they scatter all over the place, does it look like intuition that you really have to go far to meet your friends? There is just way too much space, they will just fly off all over the place. And you have to go rather far to go meet half your friends, right? Much further to meet your half your friends. So in other words, in higher dimensional spaces, there is just way too much space out there, But in higher dimensional spaces, there is just way too much space out there. When there is too much space, it creates problems. Problem that it creates is higher dimensional spaces are sparse. Do you notice that one line is dense, but in two dimensions, the points have just scattered, flown away. Higher dimensions, they're even more flown away. And so assuming uniform data or uniformity of data, you would realize that your friends are all really very far away and that space becomes very sparse. Now, in reality, as we know, when we are clustering, data is not uniform. It's non-uniform. Otherwise, there won't be any clustering that we can do. We all agree with that, hopefully, isn't it? If data was truly uniform, there are no clusters. You have to go with the assumption that data is non-uniform. So it turns out that while, so this problem is called the curse of dimensionality, that in higher dimensional spaces you need a tremendous amount of data whatever data you have it looks sparse right so when that happens your question of neighbors begins to break down right you don't have real neighbors it's like you in Bay Area, if you want to find your neighbor, you have to go. Well, the way the houses are these days near each other, you probably have to stretch your hand from the window and shake hands with your neighbor. Right? It's that close. But in the case of suppose you were living in, I don't know, Idaho or somewhere like that, in a large farm farming area, to go to a neighbor means getting into your car and driving for half an hour. Right? Those are very sparse regions. So then it sort of begins to dilute the meaning of neighbor itself. That's a problem. That is the curse of dimensionality. Now, how does it affect clustering in particular? It turns out that some algorithms are more robust and some less. A K-means has a few problems. I mentioned K-means. So I would like to go back and revisit some aspects of K-means. Yes. So how is this connected with the law of large numbers? Law of large numbers doesn't come in. It's unrelated. No relationship. No obvious relationship. You can, of course, start connecting the dots, but you wouldn't see a law of large numbers. Just say that patterns will be stable, visible. Patterns will emerge and you can trust the patterns so so well that's one way of putting it actually let's not get there at this moment k means let's say k meansmeans we are there. One of the problems with K-means was remember instability. If you take different initialization points, you can end up with different problems or disadvantages. The second problem was that it has no outlier ability, outliers,, outliers. No outliers. Another problem with k-means is that, so suppose your data has outliers, these outliers, you know, suppose you have this and you try to find a centroid, where will the centroid be? Will it be truly in the center or will it get pulled towards the outlier? So suppose it declares this to be the cluster what will it do to the centroid it will get pulled towards the outlier isn't it okay let me make this thing very obvious what's what whenever you look at averages or centroids or centroids base in one dimension it is basically your average rate of locations average think of the make it very real if you ask yourself in your company what is the average salary that people make in tech companies when you look at the average salary, you might be surprised that you're making less than the average. Why is that so? The CEO takes a whole bunch of money, 40 times more. Yeah, exactly. A CEO will take home $800 million and his pack of wolves, which you call the executive staff or the C staff, will take another maybe 100 million each or 50 million each. Right? So I'm sorry for being... I'll take it in a lighter way. And then when you look at your paycheck, all of a sudden it doesn't look as good anymore. Right? So averages are skewed by big numbers the average salary in these companies is not representative of what employees really make so how do you stabilize against that you take the median salary if you really want to know what is the salary you can take the median salary. If you really want to know what is the salary, you can take the median. You can take the harmonic mean. You can take geometric means. You can do some things. But median is often commonly used. That is why if you notice in US and every place in economies, when people are asking for incomes or anything, income especially, the census data always looks at the median household income never the average household income right looks at the median and so our k means is susceptible to outliers for exactly the same reason you're averaging your centroid is just a high dimensional average location mean right now the way to then say well all, I'm not going to do k-means. I will define my center of the cluster to be the median, right? So then you start key midoids and so forth. There are other alternative adaptations you start doing, and they are adaptations. But one of the problems problems deeper problems is in higher dimensions dimension because quite often you use the ingredient distance what happens is that there's a peculiar problem that you face all points seem to be equally far apart it's very hard actually so the way to look at it is if you look at the pairwise distance between all points within a cluster or just. Not even just the whole data set you look at the pairwise distance and actually square it there's a there's a word for that is called dispersion. A fancy word, just square of distance. So in a way, when you did WSS, in a way you're looking at the dispersion. So all right, xi, xj. When you look at the dispersion of ij, when you look at the dispersion measure and you plot the dispersion, you build a histogram, in lower dimensional spaces you may find that things have a bell curve reasonably wide bell curve structure right then and the average so suppose this is zero zero zero there is somewhere the mu is there and somewhere the sigma square is there. Am I making sense, guys? Welcome. This looks common sense. In high dimension, what happens is, or data may not even be actually like this. What happens is data, no, I take that back. It is more like, this is more likely to be true. There is some, this is more likely to be true. Because obviously they're clusters so these are distances so yeah so something like this what happens in high dimensions is that from this and it is like mu is close to the origin in high dimensions this thing becomes first of all the distances the average distance between points is huge right the mu the all points are far apart they're sparse and the other thing is the distance between all the points they begin to look the same in other words they are all peaked right so you know would you really consider that two points one one point is that one light years away from you, and another point is 1.001 light years away from you. Would you consider that one belongs to the cluster and other doesn't? You wouldn't, right? You wouldn't. so a very similar intuition applies what happens is in high dimensional geometry our points become almost the same distance from each other so detecting clusters becomes much harder right that is one of the problems that you have with k-dimensional data so what should you do give me a moment guys and raise stand up okay so what happens is when you try to apply clustering algorithms directly into high dimensional data, two problems come. One is that it is terrible computationally, very, very expensive. Well, that is not bad enough. What is worse is all that expense would be worth being, especially by the way, when you're doing these computations in the cloud, it means real dollars and cents. You rake up a bill so let's say that you um you would do that you would say well all right what is the benefit it turns out that these clusters these algorithms that begin to do a little poorly k means in particular begins to do very poorly uh with euclidean distance so a lot of people have studied it and there's a whole body of work you know on this problem of high dimensional spaces today all we can say is we are beginning to understand high dimensional geometry right we we don't really have as strong an intuition of high dimensional geometry as we would wish we are beginning to understand it you'll see a lot of interesting books coming out on high dimensional probability theory or high dimensional statistics and so on and so forth. People are beginning to get a sense. So sometimes people point out that see data truly is not uniformly distributed. So this argument may not be true. There may be cluster, there are clusters there and so forth, but the curse of dimensionality is real. You have to struggle with it. And K-means does weaken becomes problematic Euclidean distance does very poorly in high dimension right all of the if you look at the dispositions the uh the variances all just too tight variance is too little you know all the distances begin to look the same more or less right and they're all big distances. So not a nice situation to be in. So what should you do? Well, that brings us to an interesting topic, dimensionality reduction. What you can do is you can project the data down to a lower dimensional space. How do we do that is an interesting topic. There are many methods, the simplest of them being principal component analysis. At some point we will do it in this workshop, but not for today. So the idea, the right approach is that use something else. First, reduce the data, project the data down to a much lower dimensional space. Once you have projected it down to manageable, like for example, first try whatever the data, try it out in two dimensions and see how does it look. Doesn't look good, three dimensions, how does it look? Well, when we do PCA, we'll go through a more formal process of finding what is the right number of dimensions. And our ELBO method and script plots will come back. But there is a way to determine what is reasonably good. There are many methods to do dimensionality reduction. It's a big topic in its own right. We'll cover it separately. But given the fact that those techniques exist, whenever you see high dimensional data, your first temptation shouldn't be just to go cluster. Your thing should be, let me first reduce the dimensionality because clustering would be probably more effective in lower dimensional spaces. Are we together guys? The other thing is that sometimes use alternative algorithms. So for example, graph algorithms tend to do much better in Maybe I'll do that after lunch, but after lunch we have a lab. Okay, if time permitting, I would like to do this method, clustering. It is inspired by graphs, right? It basically uses something called eigenvalue decomposition, and eigenvalue decomposition must be very familiar to those of you who attended my math class or from your college textbooks textbooks if you remember that. Eigenvalue decomposition of something called the Laplacian matrix or La plus matrix. It comes from graph theory. And you can use that to cluster data. Now it's a little bit of a big topic, just like density-based clustering. It will take its own couple of hours to, or maybe at least an hour, time permitting, we'll do it today, but not now. So one of the things you could do is a try out spectral clustering or density based denture etc also work reasonably quite well actually you can try that out so there are many many clustering i'll end today by saying there are many many clustering methods right what you should do, and this is where the fun is, which method works takes a lot of trial and error, a lot of trial and error. There are no perfect things. Typically in a domain, once you have hit upon a good one, you sort of stick to that. It isn't that next month's data will require a different algorithm. It doesn't work like that. One domain's data, quite often after a lot of trial and error, you'll find that this tends to work on this problem. Right, so try out many, many methods. There is Birch, I didn't teach you Birch, there's Birch. There are many methods that, clustering methods that we can use. So, but I will summarize by saying, we studied K-means clustering. I will summarize by saying we studied k-means clustering. Its softer sort of a version is called expectation maximization, which also we didn't do. I'll try to do it in one of the Tuesday sessions, if time permits. Then we did hierarchical, agglomerative hierarchical clustering, agglomerative hierarchical clustering, and we did density-based clustering, isn't it? Three classes of algorithms. And we talked about the curse of dimensionality, and we talked a little bit, just glancingly, that there are other algorithms, like for example, the clustering, the spectral clustering algorithms that use eigenvalue decomposition of the Laplace matrix. Now what in the world are these big words? If you're not familiar with it, just altogether ignore it for the time being. When the time comes, we'll learn about it properly. So that's all for the theory sections. We have 10 minutes. I would like to take questions. I would like to take questions. If you allude to using a graph based approach, spectra, perspective, plus kind of double click a little bit of understanding of how it translates into the underlying. Because. When we assume a graph based approach for each of the nodes, they likely have multiple attributes and there's a notion of what each of those attributes are and the significance in the domain. Not necessarily. Which ones to make participate inside a distance function. Hold those thoughts in your mind. I want to do it properly, right? Because I need to build to it, slowly build to it. Spectral clustering deserves a session of its own. So when I was trying to at least build an institution, was it a mathematical technique to get around the problem or is it a technique where you're able to use more attributes? No, no, no. It's a mathematical technique. It's a mathematical technique. It's pretty elegant. See what happens is clustering or a topic like this, do you notice a classification? People can bring completely radically different approaches or ideas to bear upon a problem. The way hierarchical clustering thought of the problem is entirely different from the way K-means thought. And the way density-based methods thought about it is entirely different, isn't it? So the beautiful thing with machine learning is people bring very different approaches and ways of looking at it. And they build it up, and they have success. When they have success, then you have to sit and sit and parse okay what are the strengths and weaknesses of it and sometimes they have surprising strength that other other algorithms don't have right so when you take graph based and these days by the way graph based machine learning is all the rage it's the hottest thing right so graph based we are realizing actually that graph-based approaches to machine learning have been somewhat underappreciated and they really need far more attention than we have been giving. And we are getting those attention now. So I have one basic question on dimension here. So our physical dimensions are just three, right? And what we can read even with instruments is basically three dimensions. No, no, no. Just one question here. So this is only in space dimensions of course there are other dimensions you are looking at in terms of other things which we could not perceive perceive right dimensionality reduction reducing all those and trying to get down to what we perceive is it can you visualize like that no no not like that let me make it real to you see when you think of space yes it is true xyz, y, z, right? Three-dimensional. You can go sideways, you can go forward, backwards, you could go up and down. And that about limits it up. But that is space, nature of space. And if you bring in time, space-time, it is something that we as human beings are embedded in. But when you think of data, it lives in a different space. For example, let me give you a very real example. In one of the early projects that I worked for NASA, we looked at all sorts of things. So one of them was, okay, let me make it very real. Why go there? Very real. So suppose you're looking at it temperature pressure right wind speed wind right wind speed humidity now you need one more dimension humidity so now any observation so suppose you take an observation you realize that it will have temp temperature pressure humidity we did the weather data and speed wind speed okay so what are you looking at each data point exists so you have a value here are you looking at each data point exists so you have a value here some values here so each row exists in what space each of this is a scalar field this is r this is again r r technically it any point is a tuple in the abstract sense this is some temperature value it is some pressure value some humidity value some wind speed value so this exists as a point in belongs to a point in a four-dimensional space but it is not real space it is the space of uh these uh measurables measures isn't it data measurable so that's how you think of higher dimension. Data comes in high dimension, you're not thinking of space as it is. So that's why people use the word high dimensional spaces but when we use the word spaces in machine learning, we almost never mean the genuine space that we occupy as human beings we mean the data space like this measurable space of measurables so dimension is basically a parameter that you are providing into the problem basically yeah it is the number of features here you you have literally that's your dimension here i have four features temperature pressure humidity wind. So my data is four dimensional. Okay, four dimensions, but when we thought of dimensions, we looked at circles and spheres. That's basically a spatial representation of these. How does it translate to that? Very simple, because the geometry that we have in real space translates to this also. In the abstract, you can see, for example, let's take just temperature and pressure. I can create a circle and say temperature and pressure, the sum of them must be a t squared plus p squared must be less than some value x. And so what have I done? I have said temperature and pressure cannot exceed a certain amount give me all data points which which fulfill this constraint okay okay that is it okay this is a solution domain that's more geometrical this problem is different yes yeah so that's all it is so what happens is that see when you come to machine learning sometimes, you the way, in fact, is my fault also, and I try to avoid that fault, but the textbooks and all of these things, people come with a background in say there are two kinds of people who come into this, people who are coders so they are very that okay forget all the theory tell me how i code right so uh they're like and to help with how db scan works i just need one line of code db scan this is the constructor these are the arguments what exactly do i need to specify epsilon and n and done. This is the perspective of machine learning. The trouble is these coders are not innovators. And they don't see the limitations of when it works when it doesn't work, basically, you have to be go deeper in that. Then the on the other spectrum are the people who come from deeply sort of a mathematical background, right? and they want to understand what is happening and they use they always think of facts see there are two schools of thought in uh okay i'll give you my background most of you know that i did my engineering so maybe you can stop the recording please