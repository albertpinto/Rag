 Repeating the step helps. Does it work? It is now. So alright guys, we are going to cover today. Today is Saturday the 12th and we're going to cover an interesting paper that has a lot of relevance for those of you who are interested in applying AI, especially large language models to the healthcare domain. And I want this to be collaborative in this audience we have, we have physicians, Patrick, we have people who are from the healthcare and pharma expertise. expertise. We have people who go to healthcare. And people who use healthcare. This users and product. That's right. Yes. With Burzapak representing Pharma and Patrick being a physician and the rest of us being their clients. I think we have it all covered. So this paper should be of general relevance to all of us i'll just start by reading this paper it came out very recently i don't know the exact date i forget uh when it came out but it was in the last couple of weeks i believe now it has come out as you can see google google is involved in this and it turns out that some of the data comes from the apollo hospital in hadrabad in india and some of the data comes from the Apollo hospital in Hyderabad in India and some of the data comes from the north western medical and medicine university and some of the data on which the model has been trained is in the open source domain when you look at this you will see a pretty long list of contributors as you can imagine this is a pretty large project large effort and i suppose the abstract two is long yeah please go ahead somebody has a question yeah okay so uh this the background the way they mention it and i just repeat it obviously i won't go over the paper line by line and we will all collaborate and we discuss. I would like to learn actually a lot from the folks with the medical background, how they weigh in on this paper. Artificial intelligence systems for medical imaging have traditionally focused on highly specific tasks and have generalized inconsistently to new problems. We know that that has been there. And then it goes on to say the combination of large language models and vision encoders offer the potential to address some of these challenges. Have we seen this happen folks? And those of you who have been taking the neural architecture course would be reminded most recently we did who have been taking the neural architecture course would be reminded most recently we did many things we did video llama for videos we did uh blip to blip you know that blip to introduce the qforma architecture and that precursor to that was click the first i suppose very successful uh image text cross encoding cross embed, or multimodal embedding. Clip was very popular. And you could do image embedding very much as you do sentence or text embedding, thanks to the VIT paper, the visual transformer idea, which could essentially treat a picture as a visual language. And so there is a lot of work behind it. Then what we will do, and in a way I apologize to those of you who are new or who are not part of this study circle, but I hope still it would make sense if you are familiar with those neural architectures, we'll very briefly cover those architectures so the preface or the app the background here would obviously you would resonate with very much the statement is the combination of large language models and visual encoders offer the potential to address some of these challenges this sentence and then in this work we present an approach that enables efficient training of multimodal models using routinely collected medical images and their associated text reports and added the ability to perform a diverse range of tasks with rich expressive output. This approach unlocks a potential for a new generation of medical AI applications supporting workflows including high performance zero-shot and, and data-efficient classification. So this point is important to note that supporting workflows, including many, many tasks, many downstream tasks. Now, if you remember, when we did the clip paper, do you guys remember us doing the clip paper? And just to wet your recollection here is the clip and the clip two paper uh i'll just bring the pictures forward so that it will remind you but we'll cover it a little bit we'll review it a little bit if you remember this was the contribution of the blip paper a blip sort of extended the work of clip right and i will i may not go all the way back to CLIP, but OK, we'll start with BLIP. What it did is in its architecture, if you remember, there were three loss functions. One was the contrastive learning, image-text contrastive learning. One was the image-text matching loss term. And you added that loss function also to the collective loss. And then you have the image grounded text encoding. And likewise, because of the way these things work, it also ensured that minimizing the loss could only happen if the image encoding itself became smarter and very aligned to the textual descriptions. that we talked about if you remember that will form the background of this now if you remember in this paper we had the decoder and we had the encoder and the big change that happened with blip2 i hope this screen will realize that, you know what guys, that the decoder, now that this large language models have matured, why are we even bothering to train the decoder? Let's take a decoder that is frozen. Let's take one of the known large language models, whatever you want to take, your Falcon, your Llama, whatever, Palm, etc. And even image encoders are pretty mature. Let's take a very mature image encoder and therefore freeze it. So what it will do is it will just leave the inner two pillars to be trained. The text, the image, sort of base text encoding pairs to be trained, and that will be a much, much smaller neural network. And that was the idea of the Q-former. Do you guys remember this paper? The Q-former, right? That was the idea. And that is the main idea that we are going to leverage. And when you train the Q-former, the insight is that you are training a much smaller beast, right, compared to the billions of parameters of the, you know, the models like the language models, for example, this paper refers to, I believe, Lama, using those Lama's with a billion, 70 billion parameters or whatever it is, you'll be training a Q-former, which is just a couple of, what is it, 30-40 million parameters? I forgot the exact size if anybody remembers that. The Q-former is a tiny little beast to train. Yeah, here it is, 188 million parameters. So literally two orders of magnitude smaller than a large language model, two to three orders of magnitude smaller. So that is what it made for. If you remember in the Blip paper we talked about the fact that there's a two-stage learning. First learning is the real one in which you take the image, you take the image encoding, and then you do, you feed it in that with the Qformer, and whatever comes out, the encoding that comes out, that you feed into the decoder, right? And when you feed it into the decoder, you feed in the text, your query, and then you expect a different text, right? You can use a decoder-only architecture, or you can use an encoder decoder architecture. Now, why do we have a fully connected layer in between? First of all, you have to do a matrix, you have to do a linear transformation anyway to reshape the output from the Q-former to whatever is the token, you know, vector embedding length and the token length etc of the decoder you'll have to do that anyway while you're doing it it's always a good idea to stick in some what we jokingly call a peanut butter jelly sandwich in the in the sandwich a few fully connected layers and then and the same thing if you're doing both an encoder decoder language model then no problem the same thing you feed into doing both an encoder-decoder language model, then no problem. The same thing you feed into the encoder, it'll go down to the decoder. The only thing is that you break up the label and the input as prefix goes in, and the label is the second half of the text goes out. So I won't cover this. We have done this. Suffice is to recap how well it does in some of the classic tasks. For example, look at this photo. Is this photo unusual? And it came back and said it is a house that looks it's upside down, right? How would someone get out of the house? And then it says there's a slide next to it. These are the things that were generally not quite possible a couple of years ago. These are all very recent developments. You can see the convergence, the power of the large language models being brought to harness along with very powerful visual encoders and in between a two former. So it's a complex architecture that puts many strengths together so what's who's asking those questions i mean in what context oh you just give it a picture and see remember this this is what is the multimodal learning so you're, here is a picture. And here is a question and answer it. Right? And isn't it amazing that it can do that? It's like a chat chat was emotional. Exactly, exactly. It's a full data like language model and being done with it because of the decoder because of the generative nature. Right? This is the blip to architect. This is the Blip2 architecture. So this is like you feed the image and it gives you like almost a good, much better description. Indeed, it gives you a much better description. And we went through that, if you remember. You said like you can generate labels from it. Yes, you can generate labels from it. And that was the original blip paper itself. If you remember, let's scroll back. I know I apologize if it looks like me, like giving you vertigo by scrolling up and down. But if you look at the original blip paper, because of this, if you and just to recall, how much better does it do? Look at this image. Remember, we did this image that, oh gosh, this picture, this picture came with the caption, from bridge near my house, which isn't very descriptive of this picture, but a blip generated a very different caption for it. It said, a flock of birds flying over a lake at sunset. So, I was like, where would we then use this? Like, let's say this text that's generated. Yeah. So, in a way, this is it. Alix, today's work in healthcare is a very clear demonstration of how you can use this powerful architecture, which is why I picked up this paper, because I thought it's for us, of all people, for us guys who have taken the neural architectures workshop, this paper should be dead simple to understand. We should finish it within an hour and you'll realize that it is in many ways just a domain adaptation of blip to this domain, to the healthcare domain and healthcare data. So, as of today, I have a question. So, I mean, the thing about healthcare domain is radiologist goes through like seven years of medical school and several years just to figure out what an abscess is or how liver cirrhosis looks like, MRI images, CT images. So we'll label these images. And, you know, you did mention the Apollo Hospital and it says some 2000 images and they're only able to diagnose this many cases. So, I mean, since, you know, this seems like a very domain and subject heavy, like you really need to be an expert at reading these images and people train for several years um what are some of your thoughts and ideas on um I mean yes of course I mean with two cats and dogs I think people can figure that out but when this is so I mean it requires so much much heavy expertise to read MRI and CT images. What are your thoughts on how good, or in terms of labeling the data set involved and how we're actually processing these images to get to this endpoint? So that's all the question I have. Shalini, thanks, actually. That's really the perfect question to ask before we start on this paper. So let me try to answer it and maybe Patrick can weigh in and Mustakia can also weigh in. So here's our situation. to the start of the pandemic 2020 the ai community was with all the confidence that engineers always have right i always jokingly say if you ever ask an engineer can you solve this problem you know always say of course even before he has looked at the problem right so with all the confidence that we engineers have we were sure that we could create a diagnostic tool using AI for detecting COVID by looking at lung x-rays and so on and so forth. The reality was that actually it was a uniform disaster. People can say we didn't have proper data, we didn't have this, we didn't have that, but it may also be that the state of the art wasn't advanced enough. But the reality is that the only test people could trust was the RT-PCR test that most of us went for. It was a gold standard. It was, in taking it was a little expensive, the government covered it and made it free for us. But it isn't as instantaneous as for example, having walked through an x-ray room and having knowing the diagnosis right away. It didn't rise up to the medical clinical standard needed for a wide department. That was 2020. I don't know how we would look at the situation today because the state of the art has moved forward. The problem then was, the genuine part of the problem was, I think two parts. I think the state of the art needed to move a little bit more forward before it could be solved. And the second part was that genuinely data was hard to get and it was chaotic and it was noisy and it was very difficult to train models. Some people tried actually, but I don't think they succeeded very much at all because it was never widely adopted. But the second part of it, that is one aspect. The second aspect is, see, most of these battles are fought incrementally. These models are getting better and better. We are all moving towards the tipping point. Have we reached the tipping point? I don't know, but are we close to the tipping point? I think we have pretty, it seems as though we are pretty close to the tipping point. Now I say that with caution, for example, in nuclear fusion, being partly a physicist, in nuclear fusion, all the physicists are sure they're close to the tipping point. And they have been sure for the last few decades. So it's like peeling an onion, every time you solve one problem, the other one shows up underneath it. So we don't know whether AI is really close to cracking the healthcare is really close to cracking the healthcare in a significant way, or it is a battle of attrition, will first solve the easier problem. I do know that it is beginning to solve a lot of the, there is a subset of problems that are solvable and AI is beginning to make a non-trivial contribution now. So the things are beginning to happen, but have we reached an inflection point? Time will tell. But one thing I must say that 2023 is the year when a lot of those ideas have come together and started yielding fruits. It's almost like years of research now finally has reached a level at which all the pieces of it need to come together. Like this paper, Blip 2, if you look at the date, June 15, 2023, when did we cover it? My guess is we covered it in July, in July this literally is a domain, in many ways, a domain adaptation of this paper has literally come out in what is it August or July end. Patrick, do you remember when this one came out? Elixir, it's here. It was published June 15 there on the paper. So June 15 itself. So obviously these guys must have been communicating with these guys how can you publish the same thing on the same day oh no no no Alexa Alexa I think less than less than a little over a week ago over a week ago yeah that that is it so guys you see how fast things are moving so that is the other thing shall into your question is how fast things are moving. So that is one aspect. The other thing, Shalini, to your question is, see, it is not magic. These large language models at the end of it, what are they? They're just the calculating machines that use calculus, right? The magic of calculus, the magic of linear algebra and probability theory, and good architecture, good solid computer architectures and hardware and software. Many fields of engineering and mathematics have come together, yes, but they are being taught by great experts. All these models have been taught on what? Years and years of narratives, explanations, medical notes that physicians who have gone through 20 years of training 30 years of training radiologists and those are the people who who have written notes and in a way they are the educators for these large language models it is their intelligence their wisdom their experience or whatever you call it that is being distilled into this model so or whatever you call it, that is being distilled into this model. So the fact that these things are enabled, literally is a gift of the vast armies of radiologists and physicians who wrote those notes. Hey, Asif, this is Shivani. Just wanted to chime in on that part. In the field of medical imaging, this not using the large language models, but the earlier modes of machine learning, it's very prevalent. We work on optical coherence tomography a lot because we are basically in the eye space. And we have employed reading centers that have their own NLP algorithms and CNN networks, things like that, the ones who have neural networks that are actually training. These models are trained on reading images, whether they annotate it for physicians, from physicians, or annotating it by themselves. And then they are providing, because to launch into our clinical trials we basically need it in the form of data as well as image and it's translating it into data and we are sending it for submission to the FDA today that's happening right in addition to that for the radiologist part of it that you were talking about earlier that's why this paper is very interesting you know using DICOM standards at the back end and using the radiology standard template reports that are available, there's a lot of play that's already been done in the AI and machine learning space where folks have tried to take the PAC system, right, the picture archival system that runs in hospitals. Sivani Murali Is it okay if I can say something? Yeah. So Shivani, you said it's the PAC system. I think this was about 10 years ago when this was my first postdoc at Stanford, which was doing exactly that. You said it converts it to data and there is some set of transformations that are happening. Is this via GE? Is this something that I know a lot of time people publish patents, not exactly papers. Is this something that, you know, we can look up or something we can see? And I know Asif sir mentioned that, you know, a lot of radiologists are annotating it when, you know, I believe in that we're headed in that direction. But we did have a, let's say, a presentation at work where my focus, I did do radiological imaging, but I also did a lot with material science. The funny thing is we had a presentation at work and somebody was talking about drugs and what they presented through the large language model was cyanide which you know is kind of poisonous so it was kind of funny in that sense so it's you know my project at work is hallucination with these um large language models so um I guess I'm just asking is there some place you know not that you need to give us the exact description is there someplace where we can read uh more about yeah u.s progress thank you yeah sure yes there is and there are papers that are published but as i said uh they are on the earlier architectures and i can show them actually go ahead i wish i could give you a microphone would you mind actually coming here I wish I could give you a microphone. Would you mind actually coming here? I'm sorry, I'm making some arrangements so that we can hear the audience here also, but I'll request Mustaq to come. Literally into the microphone. I think also like if you look at Google, Google has done a lot of work in this whole imaging space and so they used to have Google help. I don't know what it's called right now, but, you know, they actually partnered with India and some of these remote villages in India, collected a lot of images and developed algorithms. And then also like in digital pathology, for example, it's different type of images, DP images, but same thing we like, we apply the traditional models, I call them right, where you can then find out patterns from it. Right. And same thing with oncology, where we have these tumors, and we can like really look at the tumors and see like where the tumors are, and how much is the tumor burden. I feel like reading this paper, the big difference is, and we'll see when, as we go through it, is what we did not have was converting that to text that was meaningful for humans. I think that's the probably link I see that now you could actually understand what's happening in the tumor, but then hopefully the model can then also write a report, which was not the case before, right? So almost like linking the language to the image is like something that now, you know, with these new models you can get it, right? So I think in this example, say something like abscesses, which, you know, I did work at a radiology lab looking at something like abscesses. So what are large language models going to do in case of tumors? And, you know, I know they have depth and dimension and can image in 3D. can image in 3D. So can you elaborate in a little bit more depth? Because when I read this paper, I saw a fairly small sample set for, you know, even radiologists 10 years ago might have complained. I mean, not that I'm complaining about this paper, but as a scientist, I have to ask questions. Maybe like, let me give a different example, because I think this one is like focused on x-rays, right? But let's, let me give a different example so for example uh in our case you know we can take the you know we can take the different ct scans and create a whole body image and then we have algorithms that will go into the body and then segment the body image into lung or different organs so lung kidney you know liver and so forth right and then we have algorithms that can detect tumor and then say this is like with the tumors right and then there is another algorithm that can detect the total tumor burden so that you know if a tumor is like focused on breast and if it's like spreading to the different parts of the body and how much is the total trouble burden right but like if you look at of those, it's basically other set of images or videos or numbers because tumour burdening is like a number, end of the day. What we don't have is then you need someone who is a trained radiologist to go in and write the report. And I feel what could happen with the LLMs is exactly that. So now given like all those data feeds and like I think, you know, the few short learning type of things, hopefully then, you know, instead of the radiologist going writing the report, you know, you could have an algorithm write the report. And that is like a big deal because like if you look at the cost of doing that in a clinical trial is like significantly higher like you hire the reading centers have them like read all these kind of images write reports so if we can automate that with a higher degree of accuracy that would reduce the cost of healthcare quite a bit and also like speed up the whole process. I think the only question I have is you know what exactly do we need to train these llms with because um even with like simple things that a lot of these llms seem to like make things up they make up uh dependencies they make up python libraries that don't exist um i know while that's funny at work for something, you know, innocuous like music, I think in the healthcare space that could lead to like a lawsuit. So if we are trying to replace these LLMs with, if we're trying to replace these radiologists with these LLMs who can write reports, they still have to go back and do significant editing because even as engineers, while the projects we work on are fairly innocuous, we still have to go back and do significant editing. But we can laugh about it because nobody cares. But I think in the medical field, there is people could come back and sue and things like that. So I think, you know, the question I'm asking is, what kind of, I guess, the depth of the data set, how many, what kind of data are you training with? Yes, people are like so enthusiastic saying, oh, we can train all this. But, you know, what do we have? I guess that is, that is, if I may chime in, actually, on the qualitative side, morphology and reflectivity. Actually, on the qualitative side, morphology and reflectivity, those are the main two things that are already predefined. There are predefined standards for measuring morphology on an image as well as reflectivity. And then on the quantitative side of images, usually it is thickness, mapping and volume. So, for example, if I'm taking the picture of an eye eye what i would want to understand is the fluid volume within it so there are already algorithms out there there are already standards out there that have been doing it for the last 15 years this is nothing new so the only thing that has now changed is like you said if i know all of this if i can add the agent aspect of it, which is all the analysis that a radiologist does at the end of the day, is subjectivity involved with it, we can actually reduce it to a very small degree. The review is not going to go away. The human factor is not going to go away because it is compliance at the end of the day. But it can be reduced where I needed five radiologists. I actually need one. Yeah, actually this, thank you. This is a, this is really great comment. Somebody else saying something else. So should I add something? Yeah, of course. So let me answer, Shalini in my current job, what I'm doing is i'm annotating the data i'm preparing the data for training you know all the images radiology images path images ct pet scan mri they have an associated report from the radiologist so that's my annotated data already. That goes inside and machine learns from that. So we, then after that, you can give the machine unlabeled images and it will tell you exactly. And then this is actually, me and Patrick, we are actually doing it. I am the one who is annotating the data finding the report trying to piece out the things which we want to feed into the machine so here is something funny so we have a few days ago we were discussing something so we have tons and tons of x-ray so patient comes to the emergency room and they're having some cardiac problem, let's say heart problem, they cannot breathe or difficulty in breathing. So there is a measure you take the x-ray quickly and then you do, you know, quick ultrasound on the heart and you will know that what's the cardiac output is. the heart and you will know that what's the cardiac output is. So somehow these by giving the X-ray to the you know trained model they started telling us that this patient has this much of cardiac output. So you know that is a measure to you know whatever you want to do with the heart if your cardiac output is you know above 60 you don't need to do anything if it's going below 50 then it's a warning sign below 40 you have to immediately do something about it so all the physicians in my group they were wondering how that you know uh this machine is so intelligent that you know is finding out We were thinking that there is a measure like, I think over there they mentioned it, that is called the Cardiothoracic Index. You can have a horizontal size of the heart divided by horizontal size of the chest. That will tell you how much your cardiac output will be but we we are not sure it's a black box kind of thing one other thing we noticed there somehow giving the chest x-ray machine was telling us that teeth patient have diabetes suddenly wait it's telling you it has diabetes wait what yeah yeah so that's so funny we were all scratching all the physicians like how can you can tell by giving x-ray the machine will tell you that diabetes so somehow from the back end when the data is fed in the training data there is information that these patients have diabetes so you know this is called correlation or something like that, like more patients who have more heart problem, they also tend to have diabetes more. So those kinds of things, these models automatically learn. So that's, that's, I think, hopefully answer your question. They are trained. It's, you know, this is the thing. So yeah, have you heard of Brian Johnson? So he's the guy from Frank who apparently wants to reduce his aging or things like that. I know because we're so he wants to. So he's in nearby southern San Francisco. Yeah, so it's not blue. Tell them you're a guy. you're right we are seeing just to chime in we are seeing through oct alzheimer's yeah yeah and then i also said i'm not going to take too much time we uh in san francisco my boss he worked on the brain waves to diagnose uh different kind of syndromes in the children like all these different different syndromes you have so just you can have brain waves and then and his paper was quite famous like how you can just by taking EEG you can diagnose syndrome acts and other kind of things in children you know I don't I don't doubt what you guys do, but I think it's the fact that we're using telegrams to write these diagnoses. But this is, in the beginning, the answer is yes, you have to feed the trained data. And we have tons and tons of trained data. You have the X-ray picture, black and white, and then you have labeled report with that. So X-ray plus report, machine will learn from that. This X-ray associated with this report and in the impression in the end where you see the result, so machine will learn from, start associating with that. If, you know, if I'm correct, that's what I think. But of course, Asif, you have more. Yeah, so I'll let Asif serve. You know, right now we're probably going to debate over this, so we'll let Asif serve. Yeah, let's go back. Yeah, actually, this is, I really enjoyed this discussion. You guys are domain experts, physicians and so forth. It was really nice listening to all of you. Now, just a bit of a tidbit I'll add before I go into this paper. As you know this company, our training place is called Support Vectors and guess where Support Vectors, the best, some of the best textbooks of Support Vectors are from an end user applied perspective. It's actually in the medical domain. They have been using Support Vector machines to study medical images for a very, very long time. So there is obviously a long history of successful use of AI and even basic things like breast cancer detection, et cetera, et cetera, that we use. I don't know how much they are used in practice, using fine needle aspiration and AI, but it's certainly there in the textbook so long history I suppose the only question with it is how are like what are the big strides are we reaching an inflection point and as all of you are saying see it's an interplay these things are not replacements for physicians these are co-pilots right and soon the table may turn we may reach an inflection point when the physicians will become co-pilots to make sure that these things don't make mistake and we'll rely on these things a lot more right and these will become the pilot i do feel that it will be it will always need humans but they'll become the co-pilots right whereas today it's much more in contemporary literature or language it's much more common for us to say ai is the co-pilots right whereas today it's much more in contemporary literature or language it's much more common for us to say ai is the co-pilot so with that all right let's go a little bit into this paper elixir so the title itself says a lot i think that pertains to our discussion that we're having towards a general purpose x-ray ai system through alignment of large language models and radiology vision encoders so this word is important guys what was blip doing blip was essentially creating alignment blip 2 and blip were creating alignment between vision encoders simply put image encoders if you remember uh vit clip kind of thinking and text isn't it and so the q former is a mechanism to align a video or a sorry vision encoder to a large language model that's literally what we saw in the previous article. This very tiny 188 parameter Q-former sitting in between, forming the bridge between the vision encoders and the large language model. And it got it. And literally, that's what the title says. But where is the key part? The key part is the application of it. I think Mr. I just asked blip to what is the use of like where is it applied and literally this paper, it is also thank you. This paper has come out, which is the application and and when you read this paper, you realize that it's a very successful application, and we'll go through that to the X ray images. It seems to have succeeded quite a bit, and perhaps another step in making these machines at least a co-pilot that will for all the things that Shivani said and Shalini said and Sukpal and Patrick and Nustark said reduce the cost of healthcare accelerate the diagnosis times reduce the manual labor generate the text so that it is much better to be reviewing the text and see whether for example shalini brought up the fear of hallucination right is hallucination taking place and uh sukpal brought out the opposite point sometimes it's not hallucinating sometimes it's seeing something that human beings have historically always missed and to the great surprise ai finds it right so many, many things. There was one thought that was coming to me yesterday when I was out on my morning hike, was that these things do hallucinate. Of course, they are next-world predictors. So to the extent that they have their next-world predictors, at the end of it, if you ask them to produce a lot of text, they're more likely to go astray or hallucinate or do things than if you ask it to produce very succinct little bits of text. It's the very nature. See, next word prediction inherently, when you sample off the softmax, as we have been talking about it in our neural architecture session over and over again, you sample off a softmax with a certain temperature, there is a probability you'll get it right. There's also a probability that you'll get it wrong. Let's say that every word prediction has a tiny probability that it will be wrong. So the probability that it is right is less than one, minus epsilon but if you generate a lot of tokens one minus epsilon to the power hundred is practically zero isn't it take 0.99 and you take the power 100 actually that was my sort of an intuitive way i always look at it that it's a diffusion process and so there's an exponentially rising likelihood that the machine will start going astray after some time so i don't know how much bearing it has to the discussion we were having but i thought that may have but okay let's get to the paper so this thing says the method they use is quite literally pretty much in a wheelhouse after the course. This our approach which we call embedding of large language image aligned x-rays. Do you see the word for embedding for language? And this sentence is a clear giveaway that we are talking about things like blip to leveraging a large alignment language aligned image encoder combined or grafted onto a fixed LLM. Fixed LLM, if you remember, the word that people used in the blip2 paper was frozen, frozen LLM, right? So in the frozen LLM to perform a broad range of tasks. a broad range of tasks. We train this lightweight adapter architecture using image paired with corresponding free text radiology reports from the MIMIC CXR dataset. Now, MIMIC CXR dataset is sort of openly available or relatively openly available dataset. MIMIC, all of you are familiar with. In fact, we'll be using it in our project going forward. Now let's think about it this way. When we image to text alignment, have we done that before, guys? Starting with clip, clip, blip, blip two, we were busy doing it. And then we'll do that. Evaluation of zero-shot and data efficient classification was performed using public, text expertExpert and ChexX-ray 14 datasets, as well as private datasets from five hospitals in India, the Apollos. Now, when they say data efficient classification, what could that mean? I suppose what they mean here is that it took very little data to train it. Usually to train a classifier, a normal classifier takes vast amounts of data. If you see the prior medical models, the classifier models, they have been trained with a lot of data. What they are saying here is that it takes very little data to do it. I suppose zero shot, few shot, right? Asif sir, do you think, sorry to interrupt. I mean zero shot few shots right awesome sir do you think it's sorry it's all right interrupt I mean this is medical stuff I mean it's going to affect all of our health in the future sorry health not health all of it's going to affect all of us in the future do you think this is a good idea because there's something about this experimental design that's starting to bother me a little bit? Shalini, that's a good question. See, in medical, the problem is, what is the baseline you're comparing to? From personal experience, once I had this issue, I went to three specialists who supposedly were dealing with that. One was a gold medalist from Delhi University. He gave me one diagnosis. I went to another who gave me another diagnosis. And I went to a third person at UCSF who later on, and now he and I are good friends, a professor there, and he gave me a third diagnosis it turns out that the third one was right but what was interesting is that all the three diagnoses were completely divergent right so medical say human body i believe some word people use terra incognita the unknown territory it just so far remains you know that's exactly why i'm asking because um when i was at the department of radiology i mean even as engineers we had to go to several seminars at 12 p.m we got free pizza um so so all those jokes aside i think one thing i learned was that there was a gentleman from imperial college london saying that, did you know that Indian people can still be skinny and have all this cholesterol in their blood? And, you know, I was there eating pizza. So, you know, but jokes aside, you know, it's it's also different. So how can I think I'll I think I've made my point now, but it just seems so, like we're all so divergent and there's so many variety of cases. How is this LLM gonna, you know, write good reports? It feels like, you know, we're getting, you know, I don't know, the medical reports might not be accurate it was written by an llm that's all i'm gonna say and i'll leave it at that i'll leave it to you this is a this is indeed a good point see the best that i can say and this is this is a very subjective opinion if you look at the reason mayo clinic and uh cleveland clinic are doing much better than the regular hospital they have a a reason. You know, we learned in AI about ensemble learning of wisdom of crowds. They literally take a committee. Oh, I'm sorry, I'm getting echo bad guys. So what they do is they will have the same patient history and observations across multiple physicians and they sit together in a room to come up with a diagnosis, because we know that each enriches the other. And usually the diagnosis you come to collectively, the collective wisdom tends to be, well, evidence is Cleveland Clinic, Mayo, etc. They are doing better than the run of the mill hospitals. So in that spirit, I would say, see these, consider this too. When you take in the notes produced by these AI machines, don't replace the physicians. Add it as one more member to your committee and then come up with a final assessment in which this too is there in the committee and as a member in the committee I think it's a very valuable member in the committee to come up with a plan you know the whole medical thing subjective objective assessment plan it can contribute to that very well. Mustaq please. It's funny that this morning i was listening to daniel pink so daniel pick is one of these like autologous and psychology and so he had this article on timing and when should you go to the doctor where should you go to you know ask for a raise and things like that and he said you should go to the doctor in the morning if you have serious medical procedures because the the human beings are more vigilant in the mornings compared to then you have like the vigilant then you have like a throw and then you come up and recover with this right so he said like i would like if there was a serious procedure i would go in the morning and um and radiologists are the same so what we have found at least in our research uh is uh they make mistakes and because we are human beings right so if you're looking at image one you know in the morning versus in the afternoon that they might give very different outputs you know in both of those areas right versus machines are like very consistent right so what we do is like you know if we use ML, then we have human in the loop at the end. Oh, let's say for example if you are in extracting text or structured data from medical records, you put it in the database and then you So then they validate basically. And I think medical, at least for a foreseeable future, we'll always have like in the loop. Yeah. So I'll repeat what Musnaq said because people may not have heard it. What he said is that human beings have a variability of mental equity during the day. In the mornings, if you want to get a surgery done or something serious done, go in the morning. Humans are very alert in the morning then they have a slump then they bounce back now it turns out that the same person may give actually different diagnosis or different assistance at different times of the day but a machine is consistent so his as he points out what they do is in his organization which is a pretty large farmer one of the biggest farmers the biggest farmer or second one one of the biggest farmers in the world global farmer organization uh they keep humans in the loop they let the machines do things but have our humans evaluate eventually for error and that seems to work out very well error and that seems to work out very well all right so um yeah um yes uh so uh one other thing probably is the uh the uh the ai engine could also give some probabilistic diagnosis with different options as well saying i have confidence in this to this degree. Whereas a human may just say, this is my conclusion kind of thing. So then, in that committee that you mentioned, you can actually weigh in that input and maybe also different possibilities as well. The other thing is there is a possibility that there could be collusion between i'm not saying it always happens but there could be a collusion between say uh the surgeon and the radiologist and the radiologist is just always uh uh you know marking off that the knee surgery should happen kind of thing uh for insurance right? So if you have another thing overseeing it, then that's a good thing that the AI engine is also involved in the loop. And finally, the human could take a decision with multiple inputs. But this actually helps avoid corruption in the industry as well. Yes. Hey, Dilip, thanks. Those are very insightful things. I'll summarize the two very insightful things. I'll summarize the two very insightful things you said. First is that physicians tend to pick one diagnosis and say, okay, this is what has happened to you. Because we expect certainty from our physician. We don't trust physicians who are less than sure. Good thing with the AI models is they always give you the top five predictions or top end predictions, they give you the probability score so you can actually and from what i know is that physicians use this they often look at the second third fourth prediction from ai to be interesting so that is one the other thing is the collusion factor the collusion may be deliberate which may be like as in fraudulent. X-ray person will forever say that yes, this guy needs knee surgery because his best friends are the knee surgeon. So an AI may disagree with that. And that may trigger off an audit. The other thing is, and again, to continue your thought, I would also say that people always develop a group thing like human beings to the extent that we associate with each other we always develop a group thing we begin to believe that some things are self-evident which to outsiders look puzzling and so any hospital all the physicians are talking to each other likely to have a group thinking biases they'll be much more biased towards or very basic thing you just bought a new mri equipment it needs to be paid off right suddenly you see the need for mri everywhere in every patient so all of those things so yeah machines do have a use all right guys so let's uh let's a funnel back now to the paper did i summarize it well did i miss anything a funnel back now to the paper. Dilip, did I summarize it well? Did I miss any of the points? DILIP PANJURAWALAVANI- I know. Great. Thanks, Asif. ASIF SHAHIDIRAJANANI- Yeah. Thanks, guys, for being here. This is really a fun discussion. I'm enjoying it quite a bit. So- DILIP PANJURAWALAVANI Do these models even factor in the bias? Like when we are labeling data, there is, you know, it's not always accurate, right? There could be a bias in the data itself. Yes, so see, Haini, here's the thing. Does bias exist in data? You can just assume it is. The only difference against it is sort of the law of large numbers. You hope that over a sufficiently large data set, those biases or the errors fall on both sides of the right answer. And say they sort of have a cancelative or canceling effect against each other, but it doesn't rule out systemic bias. For example, you know, people at a given time all are reading the same textbook for the same thing. They may write the same notes and they may all be wrong. We don't know. So some systemic bias may still be present, but generally, the best you can do or the best vaccination you have against very overt, very gross biases to have sufficiently large data data from diverse sources okay thank you all right guys so uh with that we've had a pretty lively discussion i'll go into the results which are pretty remarkable and remember we are still reading the abstract uh elixir achieves state-of-the-art performance on zero-shot X-ray classification. You will see these examples. What are we classifying in the X-ray? We'll see a few diagnostic categories, we can say, and data-efficient CXR classification, the chest X-ray, I believe, CXR, chest chest x-ray classification and data efficient just means that it learns from very little data i suppose and it gives you an au roc of close to 0.9 you know 0.89 is let's give or take call it 90 percent here and now as you know area and rsc curve when it is 90 percent you better take it seriously it It means that something is working, isn't it? Doesn't mean it's working perfectly, but it's certainly, I mean, you would have profited to be 99, but 90 is pretty good and impressive. And if you can achieve that with only 1% of the training data, look at this, they just took out of the millions and millions of images, they just took one person's image and 10% training data, you know, 1% and 10% training data for the two different, for the different. This is something academic. I'm just putting it out there. Academics tend to do this. So I hate to be the Debbie Downer here, but you know, I'll leave it at that. No, no. I mean, please elaborate a little bit more on that. So, you know, this is the thing. So when I was at the radiology department, so this was at UC San Diego. It was funded by GE. So it was a radiology lab where we're looking at pulse sequences, machine learning and things. So that is kind of what started my PhD program. So when we're looking at this, a lot of what they realized was that each person was different because what they realized, because we were in San Diego, there were a lot of people of Mexican origin who had high fatty liver, even though they did not have the alcohol problems, but Caucasian people who had the alcohol problems had fatty liver, even though they did not have the alcohol problems, but Caucasian people who had the alcohol problems had fatty liver. So there were people of different genetics with different. So when I look at this data set and they're telling me this, this kind of feels like, this kind of feels like, you know, they box something into, they're not telling us all the questions. I'm just being honest with you. It is true, Shelly, if I may put more color to what you said. Recently, there is a lot of attention being paid on the quality of the training data. You do, what you say is right. These images, to the extent that they are derived, for example, there is a huge bias towards the indian subcontinent because data has been gathered from the indian subcontinent which is the funny thing about apollo it's all the way it's it's a it's a pretty big hospital it's a big hospital and it's a little bit caters to the affluent crowd so obviously are we are we finding things that are specific to the affluent Southeast Asian? So are we finding things more generally? At some level the truth has to be somewhere in between, right? Ultimately the human body has a uniform structure but there are specifics, that is one. And so, I mean, yeah, I mean, Shalini, your point is absolutely true in the sense these days as people are looking into data, they went back and look at, I mean, yeah, I mean, Shalini, your point is absolutely true in the sense that these days, as people are looking into data, they went back and look at, I believe they mentioned it in other CIFAR, et cetera, data. And they realize that it's a disaster when you carefully look at how many mistakes there are. And there's quite a bit of effort these days to clean up the data. So, yes, with that, that is true. Asif, if I may chime in on both the points. Yes, with that, that is true. Asif, if I may chime in on both the points. On the point of diversity, actually, FDA requires, so these drugs come through clinical trials at the end of the day, and it is absolutely required for us to have diversity in our clinical trials. So there's a big push on the drug manufacturers right now to have that diversity so that it's not just tested on Caucasian populations or Asian populations, so on and so forth. That's one thing. Second thing, I think, Shalini, you brought it up again and again, where you're saying that everybody may have different genetics and different demographics, and that may impact how the drug reacts in them. So the whole push in medicine, as far as the pharma and life sciences is concerned, is towards personalized medicine. And this personalized medicine has already been tested. It is tested on a small scale. I strongly disagree with you on that 100%. I'm sorry? I'm sorry. I don't believe that 100%. Let me complete my sentence and you might after that so um the way the personalized medicine is working is not person by person that's not the meaning of personalized medicine what it means is powering one's own immune system to fight diseases so they recognize that everybody's immune system is unique they recognize that everybody's genetics is unique. And therefore, cell therapy is one such measure, which is very predominant in oncology currently, which is one to one. So you basically, you take the cells from a cancer patient, you inject it into a hamster or a subject that can take it. And then you grow the antibodies there, and then you give the antibodies back to the patient. This is a very common thing in oncology. It's also picking up in HIV and all of those things. What the next generation of medicine is doing on personalized front is to take it . So that is the holy grail of pharma and where they're trying to do right now and all of us are striving towards that. Okay guys, so yeah this is nice for the very lively discussion. I think it's going to be a fun reading so let's move forward. Actually this is amazing, so much different experience and deep experience and deep perspective. And Shivani, thank you for being here. It is a pleasure. I think it's the first time you're here. I hope you'll find it fun and be here again. So and today, how many new people do we have? Thank you for being here. And anyone else who's new here for the first time? For the paper reading? No, we have all been here. So it's a fun community. Thank you for helping us out. And so guys, I'll just go and give this finding. As you can see, I mean, the data being as it is, with Shalini being a little skeptical on the data quality and Shivani giving us much more detail on the FDA regulations on the data and how it has to be applied with that, which is quite insightful. Let's move forward and see what this model does achieve. Interestingly, I noticed something that is this, which is very meaningful. And I think this was a core contribution, could have been a separate paragraph itself. It says, Elixir requires two orders of magnitude less data. You notice that? less data to reach similar performance means compared to prior models you can train this with much data now guys this would all look very shocking and revolutionary and it is from a practical perspective but having come from the blip paper blip to, you would all agree that you would not have expected any less, isn't it? Am I right in saying that? Knowing the Blip2 architecture that we did in our neural architectures course, you would say, of course, it's such a wonderful architecture that when applied to specific domains, you expected to see some degree of learning, and which is what they are reporting. Now, they also go on to say it shows promise and cxr tasks demonstrating overall accuracy of this so guys those of you who are experts here uh shivani and patrick and suphal and musta and shalini all of you five of you do you guys have any comments on this is this good accuracy or bad accuracy? On visual question answer. I think you know my answer, Dr. Kumar. I'm sorry, say that again. I think you know the answer to that question. I saw this paper even before we started this and that's's kind of why i don't mean to be antagonistic but i'm very skeptical um i'll i'll leave it at that and i'll leave the um there's no antagonism let me ask why why you're so skeptical i just i just i just think they picked and choose. It's kind of like, you know, it's picking and choosing. They picked and choose things, but they're not really speaking the truth. And I think that's what troubles me about this whole thing. And it's what troubles me about papers in general, which is, you know, which is. Sorry, go on. Yeah, no, I think you are correct in some way. There was a study last year where it said all the papers, I think 90% of them are like bad ones. It was a big video on YouTube from Veritasium. But, but, you know, as we have more and more curated data, so the accuracy will, you know, go up high. But, and the question you asked, 68% is not acceptable. Usually in real world scenario like when we are working, we shoot for uh recall 95 percent above 90 and positive predictive value above 95 percent five percent yeah five percent is okay error you know remember that p value thing for you to do in stats yeah so yeah you have to have a three standard deviation or you know to have a three standard deviation or you know target uh nearby there like i think you know i mean i mean i'm a lay person i mean you and patrick have the md degrees but as a lay person who has worked in an engineering program um but with doctors you know when i look at this it doesn't it's it i mean yes fda requires diversified but people also fake results so you know i'm skeptical because i've seen things and um yes you know machine learning and engineering we can sit here and do math until we're blue in the place but uh affects people it affects people's lives i think that's kind of the direction I'm headed. Yeah, that's actually a bad thing to do. But on the other hand, we just actually, I don't know. I was watching a video. Now we have actually AI models which can detect the fraud if you are synthesizing the data by yourself. So that's also a good thing. So they will detect the bad patient, bad data or something like that. I'd like to say something. I'd like to say something about this. So I think one thing that we have to bring Sukpal to the front, to the lay people, is that a radiological report, it has, there's two ways you can write the radiological report. One is just a straight up radiological interpretation of the data that you see. Like essentially, if you scroll up, you'll see that they tried to detect five kinds of patterns on the data. They said the atelectasis, here, here. Cardiomegaly, consolidation. Yeah, cardiomegaly. These, when you talk about these five data points, These are standard. These are very, very standard and obvious patterns. It's really hard to miss. If you looked at a few pictures of atelectasis, you'll be able to detect this even as a layperson, because this is a standard pattern that you'd see on a chest radiograph. Now, the second part of a radiological report is the clinical correlation, which is in the context of this patient's case or condition, what is this data likely referring to? And to Shalini's point, if the radiological reports are included in the training data, and we know for a fact that in radiology, I doubt if it's still possible, but it's really difficult to distinguish fluids difference, whether it's water, whether it's water with bacteria, like an exudate, like pus. It's still difficult to determine these things through x-ray technology because it will just pass through. That's why we have different modalities to do this. So a lot of X-ray findings need some kind of context. And the context that we do is we give a history. So if this patient sounds like this patient has had pneumonia because the signs and symptoms or the clinical presentation sounds infectious in nature, rather than someone with a heart problem this patient has had pneumonia because the signs and symptoms or the clinical presentation sounds infectious in nature, rather than someone with a heart problem and there's just water getting trapped in the lungs. So the chest x-rays can get you to some level, some degree of interpreting the patterns of the data. But the second part, the second part where it's, interpreting the patterns of the data. But the second part, the second part where it's, there's a clinical correlation. So we don't know from this result from ELIXIR, whether the clinical correlation had been brought in as part of the report, or was it just the radiological interpretation. So we need to see, we need to see when they say they're demonstrating overall accuracies and annotations, whether, whether it's interpreting the second half. If it's just interpreting the first half and a radiological interpretation, and it's 58 to 62 percent, then I agree with Sukpal that this might not be the best tool to use, because if some of these patterns are highly recognizable, some of these patterns are highly recognizable that even the traditional tools that we have now from PACS are probably so much better than this for the first half. If this includes parts of the second half where there is some clinical interpretation and that's what's causing this model to go down in accuracy because it's trying to extend that inference to what this clinical condition is, then yeah, if you don't know if this patient has a cardiac problem or a pulmonary problem, then saying between congestion in the lungs due to heart failure versus worsening pneumonia, versus worsening pneumonia, worsening findings of an infection, that sounds like a 50-50 shot. So being able to figure out really what, I think it would be good to be able to run this and see what kind of report is being churned out to figure out exactly how well this model is performing. But it sounds good. Since they mentioned that they're able to detect those five patterns really well with an AUC, like 0.9, then I would be likely to want to test this out. When it comes to the interpretation and writing for the radiologist, that's yet to be seen. I would definitely want to run this and see how it does it. And then, Patrick, Palm 2 from Google has 95% accuracy. I'm not sure why Elixir is performing that bad. And people are using palm 2 in real world scenarios that accuracy is only for generative cars but not not overall not in classification that accuracy is only for a visual question answer which is a much tougher problem yeah by the way yeah every trained radiologist they always write please correlate clinically that they never forget their life so without clinical hpi you you don't have you know nothing so yeah and a good radiologist will always ask for the clinical history of the patient yes because but but that that does lead to some kind of bias because if you know that the patient has had some kind of infection in the lungs, then they're likely to look at this and interpret it in terms of a pulmonary infection. So AI might be good here that it gives you an agnostic, very clean data interpretation, but we don't have that second step of bringing it back to the clinical case. So there might still be another bridge that we could use here, or depending on how this model actually creates the annotations, I think we'll see the actual value in this. But without actually having, like I know this paper had some examples, but those are mostly just pattern detections and just interpreting what some features, so some segments based on the question, but those are just the first half. So it doesn't sound like it's actually doing the clinical correlation so i i don't know i have to i have to see this in action to actually make a a full assessment of how well this or how how good is this for for for clinical practice a quick question as if in game uh this uh um the system does do the inputs have like annotated things like you know where people, the radiologist circles, hey this is the place where I see this. And then there's an arrow and there's some text saying what it is or something like that. I do believe that what they threatened was images. I don't know how many images had this sort of boxes or annotations on the image. But they were followed with text. They were paired with text. Pointing to where the problem might be kind of stuff? So which section of the image? As you move forward, you'll see those examples. Oh, nice. Nice. Yeah. But for thing, though, the AUC number As you move forward, you'll see those examples. Oh, nice. Nice. Yeah. But you know, for thing though, the AUC number is pretty good, right? For class. Yes, sir. AUC 90% is okay. But I'm talking about positive predictive value and recall. So you have, it depends on, you know a problem to problem but uh actually we you know at my workplace it should be above 95 but again it varies from project to project uh and then I think this is a deep thing because the data set they have not been honest about what they have. I've looked, I'm literally right now re-looking up they got the data set from. And it seems like they handpicked, I know I've said this twice already, but it seems like they handpicked and they put these results together. So this is why I'm kind of not impressed with this paper. I have to be honest. You know, I think Shalini, even if they handpicked, it's not a bad model. So for example, It is a bad model if they handpicked it. Hang on, hang on. It depends on what problem it is that we are trying to solve. Right. So I'll give you an example of a trial that we just conducted, right? And had we known that a particular set of patients actually responded better to our drug, we could have actually passed a phase three. And this was basically eye medicine. And in our inclusion criteria, exclusion criteria, we failed to look at one simple thing, which was how do pseudophagics, folks who've had their lenses, you know, who have lenses, artificial lenses, how do they respond to our drug? And because we didn't put that in an exclusion, now this is, I'm being very specific, because we did not look at that population, what ended up happening was the whole trial failed. But when we segregated the trial for only the pseudophagics, our drug actually performed as well as our standard of care. So it depends on the use case. It depends on what it is that one is trying to achieve. So even if this proves, even if it is handpicked, but it proves a particular concept that is of value, it can be adopted by some of us where this value would actually be realized so we can generalizing it that we may not be the right approach because it depends from use case to use case yeah also also charlie maybe no all the inputs are not asking for generalization i'm asking more for delineation if that's something you know some inputs may not have the adequate information maybe they don't have the annotation so you can't you have to filter them out and then take the ones that actually have all the information that feeds both image text correlation information kind of thing for training so may have to select, you know, for training. No, we have to be waiting for some time. No, we'd like to go Asif sir, we're so sorry. We're so sorry. No, no, no, it's a- We are so excited today. Yeah, it's a very exciting, but I wanted to give Mustaq a chance. He's been waiting for a very long time to chime in. Mustaq, please go ahead. I think all I was going to say is I feel like this AI is, you know, I call it like traditional AI and these new models and everything. It's like almost like a step change that's happening. And to me, I look at this paper as, you know, one of those things that you do on the road to a longer road. And I think it has some good elements of it. We could argue about the accuracy or not, but I think it's something that we could say, now I can see the potential and hopefully people will build on top of it. So I'm assuming the pharma companies and medical community and they start using this as a stepping stone and then run their own test and then continue to build on it so i think to me it's just a step in the right direction yeah i'll just say what mustaak said because people remote may not have gotten he also chimed in with something in cycle he said that see it is clear that with his large language models coming up and these transformers there is a step function effect compared we are able to do things that we weren't able to do before. That is it. He also pointed out that see this paper is is a step in the right direction. It's certainly a valuable step. We'll get there. But it's certainly a valuable step in the direct in the longer journey that we go to. And I suppose, see guys, this is the fun part. We have different perspectives, lively. I would like to a little bit summarize it. My own perspective is, and obviously I'm not a medical expert, so you guys know better. My feeling is knowing the Blip2 architecture, it was almost like lovely, lovely application of Blip2 to the healthcare domain and the fact that it works. So I would not doubt that it works. I would, I think it's a, and I personally wouldn't doubt that it's a significant contribution to this space and then adapting it to the healthcare. There is some skepticism that Shannon brings up of whether people cherry pick the data or honesty let's hope that by the time this paper gets peer reviewed we have a different set of reasons much more rigorous results and they will come through also to Shmurin's point in general in this space highly regulated industry papers being papers but by the time it actually gets adopted in practice the FDA will make sure FDA standards are extremely rigorous as the all the pharma i can see here must like taking his head pharma companies know it's very very high bar very high bar to meet so by the time these things actually go into practice obviously a much much higher bar will be met but certainly i mean when i look at it researchers i mean think about it the guys who wrote this paper they are ai scientists right they make do with whatever data they have access to and perhaps there's a little temptation to maybe cherry pick it or not cherry pick it i don't know i am not qualified to speak to it but certainly it looks like a good contribution members general input though regarding the uh you know you were trying to echo what the audience is saying. I'm hearing them loud and clear and remote so your microphones are working beautifully, you know, picking up the audience very well. Oh yeah okay so you're picking up my stock also wonderful so I won't repeat it then. Nice. All right. So any more thoughts, guys? It's lovely that we have such divergent views because it makes it. I think it would help us if we go through all of the in terms of an engineering detail, because I have a feeling that we will get down and fast in terms of the debate, because I've. Yes. Yeah, the debate is not for us to resolve. And I think the best debates are never resolved. It's good to continue having different perspectives. But yes, let's get into the technical detail. So one thing that they mentioned, which to us, for people who have been taking this, or we have been going through this whole process, we know that this was almost inevitable. The statement, using a traditionally fully supervised approach, training CNNs and vision transformers is an expensive and time-consuming process that requires large quantities of expertly annotated data. True, isn't it? We have known that that is true to pre-train and we would use the word pre-training those models takes a long, long time and training it for a specific domain after that fine-tuning it also takes a long time so what they are proposing at some point they say so explicitly that they're using the flip and adapting it to it so i'll just read out the key points we described the key advantages of alexa and now we are going into the technical aspects of it. Elixir achieves state of the art performance for zero-shot classification. Right. Which is interesting. It does some classification without further notes. Data efficient classification. And here, obviously, there's a bit of caveat that Suk sookpal and patrick brought in they seem to have picked some of the easier uh easier things which physicians are perhaps would find it uh the radiologists would find it more their 101 level stuff but it seems to be getting it semantic search of thoracic conditions across the range of data sets it's still good that it got there. Adapting an image encoder to an LLM using elixirs of fast and resource efficient method of training. A resource efficient, fast and resource efficient method of training competitor. So guys, if I were to ask you just as a question about understanding the neural architectures, where do you think the fast and resource efficient method part is coming from? Knowing the blip architecture, what would you ascribe it to? The Qformer. The Qformer, excellent. This is great. It is the Qformer that sort of... Because remember that what did Qformer, the blip to basically says that get a great vision encoder, get a great language model. Let us just align the two, the Qformer, the blip to basically says that get a great vision encoder, get a great language model, let us just align the two, the Qformer aligns the two. So it will be obviously fastened, because it's only 188, like the original Qformer was 188 million parameters, right? Much like today, it's almost like a baby model, right? In the world of this ginormous models. So it's a fast and resource efficient method of training compared to fully, full fine tuning of, today, when you hear the word full fine tuning of LLM, like after learning about all of this, Q-formers, distillation, LoRa, QLoRa, etc. You almost would, you would almost ask like, okay, have you thought of all those things before fine tuning a full element? Those are the natural questions one would ask, right? And so here, obviously the Q form is the approach they have taken. Now, building models on top of the elixir can be done rapidly to prototype new use cases. So this part I actually like. Remember that a good work always opens the door to more work and I like this part. So we can take this further. I went to Hugging Face and saw if this model has yet been released. The Hugging Face link, it gives you a link to the paper and the abstract, but so far I didn't see the model there and i don't know if anyone has information of whether this when or if this model will be released to have you face anybody who knows so elixir synthesis of images takes unlocks a new generation of medical applications so this synthesis of course comes from i think this is this is a synthesis I mean do you bank on your word synthesis this is synthesis between two different groups um so I think this will be released or somebody will release it and so on yeah so yeah nice yeah let's see Shalini when it comes out I'm eagerly looking forward to it at least what does it mean by building models on top of alexa means further adapting for the adaptation that is it so think of it for the fine tuning or or using this and putting something before and after or something key question because i don't want anybody building models on top of elixir unless it is certified by a whole bunch of doctors. I'm sorry, but this is really troubling because I'm a human and I might have health issues. Yeah, no, Shalini, see your point is valid, but there's also the opposite thing. Researchers can do whatever they want because they have the license to explore and experiment and so forth. But we have to trust the gatekeepers the fdas who will make sure that these things never never never go near patients or like or even production even um even pharma companies or hospital systems yeah they're also like reputation risks right yeah reputation and then also they want to like make sure like you know people working there they care for the patients that. There is a lot of medical centricity that goes in. Right. Hey, Asif, one question for the medical experts here. You know, Patrick was talking about five different classifications that was there in the introductions. Yeah, I can go back. five different classifications that was there in the introductions yeah i can go back now when we when we have to do correlation with the text uh is there a one-to-one correspondence like you know if you have this cardio mega lee classification is there only one output or there could be five different possible outputs or is there sub-classifications that we have to consider within a particular class to then figure out what is the visual output no you can have all all of these present so no no you have all of these but within the class of say if you get if you classify something as oh you're saying these could exist yes all of these could exist in a highly severe patient yeah i can i can answer that like so sir all these five conditions as patrick said these are pure mathematical formula you don't have to be a physician to diagnose this on x-ray with a little bit of training with a little bit of math you can diagnose for example cardiomegaly cardiomegaly is shadow of the chest versus shadow of the heart on the x-ray so one is numerator other other is denominator right so by that if that ratio exceeds a point 0.65 then your heart is dilated or you have cardiomegaly. So this is how you, so yes. So when you, and most of the times you said they are right directly on the x-ray or not. You don't, I don't think so you have to directly make a, you know, cut off on the thing. When you have associated report with the picture, I think this model will automatically know this is the shadow of the heart and this is how much wide it is and this is the shadow of the chest and this is how wide it is and it can automatically create a ratio out of that ah okay okay so it can produce a result based on the outcome of for all of these conditions all of these uh i i guess you have conditions all of these i guess you have all all possible uh you know n choose k kind of options here yeah these are pure mathematical formulas to you know diagnose these conditions on the x-ray so any two of them could happen and then based on that the visual yeah that that depends on the condition you know you can have a combination of any any of these so that would be interesting if the that correlation part in the system is actually picking up these uh these combinations and permutations yeah you know once you have one thing then you can keep on developing another thing another thing yeah right yeah and as much that was saying it's a great first step right it's almost like a law it's almost like a laura for each one of these yeah like like uh yeah cardiomegaly is uh describes the part the heart so the size of the heart and you measure you can measure that with like as i said as super mentioned the cardiothoracic ratio so these are things you can actually teach to to non-clinicians to look out for and detect this pattern so you have radiology technicians who can actually label these things as well so these are these are not these are not things that you have to have deep clinical knowledge in terms of calling it out. Dr. Kirit Parikh- So over time, you should be able to I mean if this is mathematically determined over time, the system should be able to improve its visual output. Dr. Kirit Parikh- yeah yeah. Dr. Kirit Parikh- For example, yes, for example, right now in image that what do we we keep on giving the images of dog dog dog dog and ultimately. Dr. dog. And ultimately, AI learns that this is a dog. So if you keep on giving images of chest x-ray again and again and again, it learns that this is the heart. Yeah, yeah. So hopefully, Shalini will not be misdiagnosed. No. Soon, soon, I'm telling you, soon we will diagnose Shalini with an X-ray. Yeah, I have a follow- up question on that, if you don't mind. So from here, the journey to clinical assessment from here, what does that entail? Because that that's where it can be to us this point of co pilot for the doctor. So what is what are the database? What is what does that entail? What does it look like? You'll need a clinical history first from the actual record. If you just give me an x-ray without any clinical history, I can only take you to radiological interpretation. If you give me some kind of clinical history, then I may be able to infer what the likelihood of this x-ray interpretation means for the patient. You can only go so far. I can only go so far. means for the patient. Dr. You can only go so far. Dr. It can only go so far. Just as a straight up, without the history, it's like reading off of a data table. This one's just segments in the lungs, in the heart, in the airways, blood vessels, everything in the chest. How can we interpret this? But how to infer what the data means will require some history about the chest. How can we interpret this? But how to infer what the data means will require some history about the patient. Sometimes if it's a foreign object, then yes, it's easy. It's easy to figure that out. But it's like you can only take it so far, just from radiology alone. Right. Is it true then? Those are two pillars. Then the third pillar is the domain knowledge that you have, Patrick or Supal, that you have, right? So if we had the domain knowledge that Supal has, and then also the clinical history, and then these findings, that should take us very far. Yes, but you'd have to create like an adapter to the QQ former to incorporate that patient data and information. Because right now it's an image encoder. But if you're able to add that to the latent space and say that this is what the patient is suffering from or has had a history of these things, then maybe it might be able to add those. So like as Mustaq said, this is is definitely a bridge but we still need more pathways to actually get this fully yeah fully robust for for uh day-to-day diagnostics go ahead yeah go ahead hey um you know some of these like you know cancer like people um you know some of these like you know cancer like people um um you know they leave us let's say they leave us in this physical domain they pass within several months and doctors only diagnose and saying you know do you have two months three months but before that it's all very what's the word elusive like people are not really clear. So I think that's the thing, you know, yes, medical science is not exactly, exactly exact. I don't know if that's a word. But we don't know who's going to leave us with a heart attack, who's going to leave us with several other issues, who's going to get a cancer tumor in several months because this is where it's at. And some of the things that are being said here is that, oh, we were so precise and we're so concise and we know everything that's going on. I don't believe that's true. And it's fed to us so rigorously. I don't believe that's true. And it's it's it's it's fed to us so rigorously. I don't believe that's true by any stretch of the imagination. So leave it at that. We are engineers and you know, we're, we're trying to help everybody. Shalini, I can I can give you two real world uses right now we are using it. First is incidental finding means we have all sorts of old x-ray and ctmri pictures and ai goes into those pictures and looks for the nodule lung nodule and you know those kind of things so we have tons and tons of uh you know those lung nodule patients so uh there is a criteria classification if you have this this much of a lung nodule patients so there is a criteria classification if you have this this much of a lung nodule you have to follow up with the patient those are ignored in emergency room and in regular day-to-day practice because doctors they don't have time for every single small finding so that's a real world use we are using right now other other huge cases are so you actually make a point because emergency rooms are crazy, you actually make a point. Dr. Shubha Tole- Yes. And other huge cases, real huge case nowadays, Google, see you already mentioned it, it's called in our language, PRIAS. So you want to identify which patient is more serious, which patient is- Dr. Shubha Tole- PRIAS, what's that pretty eyes. Yeah, they are. They are I is so three hours. So where you have, for example, tons and tons of CT MRI taken in 24 hours. And next day you have one radiologist and he's going to go through all the pictures and images. Well, there is no way he had, you know, upfront classification ready, which one is more serious, which one is less serious. So AI goes first and that it classifies the images and as per AI, some pictures looks more serious. So it just picks some and gives to the radiologist first, please go and pay attention to these first. And so then those patients can be taken care of timely. So this is the real world use case. Many hospitals, I think they are using it three hours and incidental finding but as a model see one. Dr. Nisansala Thilakarathne Muthunayake, Ph.D.: You know, as a human I respect, you know your humanity, where I think I think you made a point Thank you so much. Dr. G R Narsimha Rao, Ph Thank you so much. You're welcome. Nice, guys. So after all this deep discussion, can I lighten it up with a really bad joke? Not a bad joke, but a dad joke. Actually, my daughters tell me that there are degrees of badness. There are bad jokes and then there are dad jokes. So I'll say a dad joke. So this is for you physicians uh if somebody has a you know is a very generous person we typically say he has a large heart should we diagnose them with cardio megalith in his life so it's it's uh you know depends on how big chest they have so if they have also big chest depends on how big chest they have so if they have also big chest that they may not have cardio so usually person with a large heart also has large chest too all right so uh now let's get to this the method because ultimately how did they do it the method part I thought for us uh fourth point please don't forget that that's very important in some of them we can get in and do more debates that's right yes go ahead you want to make a point no no the fourth point the one we were talking about they actually is fourth point describes that what kind of data they trained it on okay yeah this one yeah here so paul and patrick by the end of this discussion if you can explain to me um as mds and people with a clinical experience how you feel that you they train this data i would be a very happy person because when i read this paper i was fairly distraught you know not that I'm getting in your way, but if you can explain this to me, I will be a happy person. I'll leave it at that. Yeah. All right. So Praveen wants me to mention the fourth point. So fourth point says that we have trained using paired text-to-text and free text-to-text data that are ubiquitous in healthcare so guys yeah you see the word pair between two modalities in neural architectures what does it remind you of. R. Vijay Mohanaraman, Ph.D.: vector embedding using contrast contrast of science, so you would know that this is this will be at the heart of it somewhere. you would know that this is this will be at the heart of it somewhere right it's very much like if you go to a play and you see uh on the wall a gun line you know the gun will be used at some point sorry i'm getting into a lighter mood so this is it uh the expensive manual curation by an experience this training process does not require blah blah blah so let's get into that guys the method is quite interesting actually and I find that interesting because I felt that it's some of the things that you found abstract when you were doing neural architectures. You'll see how directly usable they are. So how do we train the elixir? It is in two stages. Does the two stage again remind you of what we did with blip blip 2? The same thing right. First we train the Elixir C part, which is the contrastive language image pre-training. Literally the word was waiting to show up, isn't it? Figure one, this uses radiology reports to align our previously published pre-training supervised contrastive learning. So apparently they had a supervised vision only model. So they have a model to do what? Imagine they have a vision model, a model to do, take input x-ray, let's just call it CXR, just x-ray, and create an embedding vision vector. there right now what do you need to do you just need to align it right with the language encoder right and if you remember the way we did that is we take the text description and here they use for that they use a t5 which is i suppose it doesn't matter which one you use those are specific details you get the text encoding now if you remember the whole point of contrastive loss was that you want to min for two like for i and text associated with the same the pair you want to minimize the loss minimize the loss between the v i, t i, which basically means if you take the cosine between these two vectors, v i, t i, you want to minimize the cosine, like you want to maximize the cosine, you want to make sure that this, those two vectors are aligned, right, vectors are aligned. Minimize loss means you want to, in effect, have a large number, a similarity measure is cosine. On the other hand, if you take it via a random text, you want to maximize it. That's your sense of contrasting. Are we together? Image is your reference. Take a text that is actually meant for that image or is the annotated medical note for the image, and then go get a random text. Make sure that you maximize the loss here and you minimize the loss. To do so is contrasting learning. What does that mean? Make sure that that text, random text, is as much as possible in the vector space as far. So suppose this is the Vt and this is the text Vi, text I. Make sure that the text, the RT is way off. Right. As far off as possible, probably in the opposite direction. Once again, just to recap, why do we prefer, somewhere in there you'll see mentioned that they use a cosine distance and all of that. Why cosine distance? In higher dimensional space, contrastive learning, representation of our image and its associated text. Closer together in higher dimensional space while pushing their presentations apart. So you can use cosine distance. Usually you can use dot products and this and that, usually not euclidean in higher dimension spaces. But I think they mentioned somewhere that they use cosine distance. Asif, is this first step absolutely necessary or could we just use a pre-trained uh mid like clip or mid clip if you could use mid kid the idea is that my guess is if i had to guess what they are basically saying is that we did this step but is it strictly like if people already have notes and text? Oh, yeah. Why would you want to do that? The reason for that is, OK, if you go back and look at the blip, see, this was the same question we encountered, if you remember, with the blip also. Let's go to the original blip. That will make this clearer. Oh, why has this frozen? I apologize guys, this UI is a little bit freezing up on me. I'm not able to scroll down and the reason for that is strange. Okay. So yeah, if you look at the original, I'm going to scroll guys. One note does this weird thing on so i'll just close it and reopen it is there with me for me now the open. So, roughly speaking, the point is that if you, it goes back to the fact that, remember the contrastive loss fact, the contrastive loss is made up, I mean, not called a blip loss, it is made up of three loss terms, contrastive loss, image, text, and matching loss. And then of course, on the modeling so what happens is that just for this part if you don't use that joint learning then your contrastive loss and your image text matching loss will be disjoint they may say opposite things you realize that you want them to co-learn together so that the when two vectors are close together the when two vectors are close together the image and text are close together they represent not just i mean both of these loss functions i agree that they are similar so so as if if we use a just just any image encoder without having to fine tune? Or was this just a fine tune of an image encoder, essentially, under the hood? Here is my way of looking at it. Patrick, look at this picture that we covered before. If you just look at this part, the image text contrastive loss, this is your essential clip architecture. And the question that you're in essence asking is, they can be a medical clip, essentially. And why couldn't we have just used that? Am I getting your point right? Yes, Asif. So is it something that you can fine tune, medical clip, or are you stuck with with whatever uh contrastive loss they've done see what happens is that when you yeah you could i suppose you could start with that but ultimately you'll have to fine tune uh how will you fine tune you want to make sure that two vectors where this loss is low this loss also is low image text matching loss is low, this loss also is low. Image text matching loss is also low. And if you remember the sentence bird paper initially, what did we do? We got some, we used contrastive loss to quickly get results. But then if you wanted to refine the results, you followed it up with a cross encoder, which was basically a binary cross entropy loss function. right, which was basically a binary cross entropy loss function. So there is a sort of an echo of that same thing happening here. When you jointly learn, you could start with that and then start fine tuning. But the thing is, at the end of the day, you will have to do ITC, the contrastive and the image text matching. At some point point you'll have to bring those two together that's all that we that is needed so i don't know if that answers your question yeah see what it does i think i think it's just i'm just not familiar with how to fine tune the the clip architecture or the male clip so maybe maybe this is patrick if you remember the clip architecture or the main clip. So maybe this one. Patrick, if you remember, the clip doesn't do anything. It only has, so suppose you have a vector, the text vector, text I, and you have the image I. Who is producing the image I? This is, let's say a VIT is producing the image I, right? And who's producing the text? Just text encoding, of course, you can use whatever you want for text encoding. Any of your bird, right? A flan, T5, whatever you want, you do that, right? Then what happens is the clip is literally contrastive loss between image and text. So you start minimizing, you want to minimize the loss between text input and image input and maximize the loss between the image input and some random text, let us say. I understand this part, Asif, but let's say we wanted to fine tune this. Does this mean that you'd have to fine tune the vision transformer and the text embed? Effectively, you'll have to open up the... because what happens is the only way you can do it is you'll have to open up the parameters to training again in fine tuning but you do have a point you know instead of starting from scratch you can start with clip and then move forward from there which is what sort of they may have done they may have started actually uh let's look at this paper more carefully to your point right they may have see they have right look at this what did they do they used a contrastive learning based supercon yeah supercon is their clip effectively right supercon is the clip trained on chest x-ray data just so yeah because this is important for us because we cannot train a model from the scratch so we have to have pick up a baseline and then fine tune it. Yeah, that is it. And so they do that. And so this is it. As Patrick, this is your basic clip in the world of medical. And then you have a language encoder. And those two, now you can feed it into the second part so here you notice that paired you leave this you leave this and then you have contrastive similarity you do some training what it will do is it will create alignment between the two right so this so so this opened up this was the clip architecture essentially as if that they opened up so that the image encoder and the text encoder were to the two components that they hand picked that is right that is right the only thing they have done here is they have instead of taking a base vit they have taken a nicely already very good medical medically relevant find you know something on medical data so that you land much closer to the solution. I see. So now, Asif, if we wanted to use this on something a little bit similar, like maybe an x-ray for a different part of the body, then we can just use this whole architecture, this whole elixir C that they built, essentially. Yeah, you can use the same architecture with one change see the supercon uh soup corn it has only been trained on the chest x-ray so you'll have to replace it with a fine-tuned model or a pre-trained model on that part of the body let's say that you're looking at kidney or something like that you will need a model and one should presume that such models probably exist by now because if they have done it those guys at those uh I'm sure that the nephrologist must be clamoring for their own model and there must be one for them yeah CT abdomen would probably be good yeah yeah and here here is the answer for the lips of questions area so image and uh text embedding and hype you know hyper dimensional space they they coincide with each other right and then you have the answer yes that's it so i so for the contrasting learning uh i hear agents so much. So what are you guys using agents for? Because there are people, so we've had talks with the Langtian people and there are people that just came into the building and said, we don't get what the value proposition of Langtian is. So I just want to know what does the medical community think about this agent stuff that's a great question shall we would you mind if we just spark it let's get through the architecture and i'll remember we'll bring up the questions okay uh a question on the contrast of learning the um the random text that's generated that's completely random or is it a different medical report yeah it's a different medical text what you do is see you have lots of pairs like images and text so your positive pair is a true is that image text genuine then you randomly hop into the data set take any other pair and pick the the text from there okay so it's not some from charles dickens novel or no no it's not it's not gobbledygook or nothing from charles dickens right uh it's uh it is a it is medical name and that's how you train it to differentiate isn't it thanks it's mentioned over there it's a free form report medical report right radiology reports that's right that's a free form report, medical report, radiology report. That's right. That's right. Free form text. Yeah. So the B part of the training is the real part, guys. So now you look at it. What do you have? You just created a like image encoder, image text encoder right something that it will encode an image and bring it it will basically impose a language sort of on the image semantically aware and very of the immediate now you freeze it you see remember this frozen picture that is quintessentially there from the blip paper it's there even the icons sort of match and. And then what do you do? You take it, you feed it into the Qformer. The three losses, ITG and ITM. Can somebody say what these are? Image-text contrastive loss, image-text generative loss and image-text matching loss. loss and image text matching loss. Exactly. Thank you. So it is the, see what happens is that as the errors happen from the text generated from the palm, from the large language model, as it generates a text, you can't touch the palm model. You can't touch the large language model. That is frozen. You can't touch the image encoder. Now in the second phase, it is frozen. frozen. You can't touch the image encoder now in the second phase, it is frozen. So all the adjustment or adaptation that has to be done is by the Q-former, isn't it? So all the back propagations will make, will readjust the weights of the Q-former in between, right? And those three losses will be there. Image-text contrastive loss, image-text matching loss, and of course the language, contrastive loss, image text matching loss, and of course the language, the ITG is basically the language modeling loss. What is ITG? Actually, no, sorry, I take that back. He has a language model. Oh, by the way, I should also say that, remember what we call a peanut butter and jelly, the feed forward layers that we stick in. You remember that in the paper, the blip to paper. I don't believe anybody outside our little study group calls it potentially like to call it that. So contrast. So going back to this, guys, do you realize that here, this fully connected layers, do you notice that? These are your MEPs. Some grounded text, something like that. Yeah. So that is the cross-attention part at the lab. What it is, is when you train this it is is when you train this Qformer and the initial go to blip paper over a normal encoder architecture there is one additional element they introduced it is the cross attention do you notice the cross attention is the one novelty that makes a whole difference in this where Where is it? It is here. Yeah. You see cross attention. What you do is the image encoding, you feed in while you're doing text encoding in this pillar, the third pillar, which will later on become part of the Qformer in the second pillar. So the normal attention is there, but here you do cross attention between the text encoding and the image encoding that has come to finally create image grounded text encoding. So for that, we'll have to go back to a previous. And that was a key idea that this thing introduced. So, all right guys guys so here we are what is itg iphone did i miss something here uh we may have missed something itg let's go there oh it means grounded text no no no it's the same thing image grounded text generation itg loss okay so nothing new uh this is it. Say that again, Dylan. No, no, no, I was saying G is grounded. Grounded, yeah, image grounded text loss. Which is the loss basically there, then matching loss. Then this is it, guys. So the architecture, like if you look at the Qforma qforma and the blip2 straightforward architecture so so what is the innovation here guys if you ask for it what is the innovation it's a domain adaptation to the healthcare domain to chest x pretty much using it and just like the blip2 you needed a really good image encoder they took they basically started with the soup corn whatever it's called and did its further training and for the same two phases of blip two are there now let's look at the results any questions guys on the architecture is the architecture obvious after the blip two discussions of lip to discussions. Yeah, everybody is checking. It's pretty obvious. I'm just a little skeptical about how this translates to the healthcare domain. That is pretty sophisticated. Let's leave it at that. No, definitely, Shalini. I mean, that's a healthy skepticism because like if you look at the AI engineers, they're always confident they have solved the world's problems. You go and talk to the medical community, they have a different opinion. And that's why, I mean, that's why to Shivani's point, that's what FDA is for, right? They make sure that- They ask a nice pun. You said healthy skepticism. Yes. Thank you, I didn't notice that. Oh boy, I wish my previous joke was laughed at. No one laughed at that one except me. Can you repeat it? I just, I asked the Patrick and Sukhpal the physicians here that if like in common language we called a kind-hearted person a person with a large heart right so would we accuse them of miguelo what was it cardiomyogaly cardiomyogaly so all right guys so here's the thing for better for worse, see one thing it does guys, that is not arguing. The fact that you can train with far less data is the power of blip2 and that power shows too. Okay, why? Because you're training at the end of the day, you're training just the, I mean, yeah, you're training a few feet forward layers, but a few feet forward layers and basically the Qforma. That's what you're training, isn't it? And of course, you're opening up the, you're fine-tuning the image encoder and the text encoder a little bit, T5 and so forth in the first phase. So fine-tuning somewhat big models and then are followed by a training from the ground up, which is an alignment training of a much smaller model. And if you look at the hardware specs, it is pretty modest. The first phase, which is, see, the first phase is fine tuning. If you look at that, what they call Elixir C, that is a big thing, right? Because you're fine tuning two large models. You would imagine it would need a lot of CPUs, GPUs, sorry, GPUs, a bank of GPUs. Then the second phase is just tuning or training the Q format. So what would you expect? It would need far fewer GPUs. Just bringing it down to engineering, isn't it? You can get away with much less hardware. And that shows through in the data. But before we go into that, I'll go to the hardware part first. Somewhere in here is the hardware part discussed, and then we'll look at the results. So let's go through the realities of training these beasts. Patrick, you read through this paper, do you remember? It's in the appendix. It's a supplementary text. Yes, here. Yeah, this is it. If you look at this, the Elixir C part, the first fine tuning part is the big one. It was trained on 60, the batch size of 64 across 128 TPU cores right now this is the big deal they used a lot of hardware to train this the rest of it momentum blah blah blah but the thing that sort of will get us thinking is that okay this is one big expensive exercise to find. That's right. All right. They used TensorFlow. Then obviously. So clearly they have obviously these are Google people. They're using Google course and Google technology. The second part, on the other hand, we used query tokens in a max length and blah, blah, blah. Do you notice the contrast, guys guys it was across only eight tpu cores do you see the difference literally one order of magnitude difference right 8 versus 128 so and that again speaks to the fact that Q form was a tiny piece in between. Now the results are good. I mean, at least some part of it to me look, and obviously now that you guys have been in, I have to rethink, but to me they look pretty interesting and good. So I will let's start looking at the results so sir if we you know we need to compare it like that if we do the coin toss i still be 50 correct and they are claiming 68 yeah 58 and i'm a visual qa they're claiming 58 and for a classification it's close to 90 percent yeah that's yeah that's really interesting that's it that's good right um so let's look at the evaluation of these models a uc what are this what are the metrics they use so for classification they used auc so obviously they and for semantic search this is visual qa they used accuracy but with human judgment like is it accurate or not so they would ask a human physician to tell that that the emitted text is it a correct assessment is it almost correct or is it totally off that's how they did that so see yeah here is it i said that they use cosine similarity here is the cosine similarity is it i said that they use cosine similarity here is the cosine similarity and everything remember that cosine similarity if you remember the old sentence it tends to prefer a shorter text over longer text dot product prefers i know you highlighted this can you explain because this sentence says a cosine similarity was then calculated between the image embedding and these two text embeddings. So this seems a little abstract to me. Can you please tell me what you understand from this? Because I don't understand. Yeah, see Shalini, it goes back to higher dimension space. Typically in three dimensions, two dimensions, when we think of distance, our first instinct is to pull out a ruler and measure the distance between two points. That's a Euclidean distance. Then we realize that if you're in Manhattan, or any city, a taxi driver would disagree with you if you just pull out a you find the shortest you know the geodesic between two points and call it actually doesn't go like that right so they would go but in the imaging in the medical community I mean I get it when we're going from Jeep point A to point B I just don't it, when this person is speaking about like medical imaging. Oh, okay. So I'm coming to that, Shani. So basically, if you have an image, see, an image, there is a cat, right? Or some radiological image. It becomes an image vector. And then you have a text. It becomes an image vector and then you have a text it becomes becomes a text vector now it these vectors belong to very high dimensional space they typically belong to 768 or 1000 something 1024 dimensional spaces now the question is you encoded them into vectors. Now what happens? These vectors, you want to impose a semantic on, you want to say that I want to train the neural network in such a way that if a text and an image are saying the same thing. So for example, you see a cat jumping over the fence and the text says this is a picture of a cat jumping over the text. These two vectors, I want to make sure that in the embedding space, in the vector space, these two vectors are nearby and a random text is far off. So you just imposed a loss function, the contrasted loss function. So how would you do that? First question that it begs is what is your notion of nearby? Nearby notion, Euclidean distance or Manhattan, any of the Minkowski distances, they actually don't work in higher dimension spaces. It's the curse of dimensionality any two points are very very far off in high dimension spaces it was one of the things we covered in ml 200 if you remember yeah but i think we're talking about medical images here and you know since i'm i worked with radiologists it it's very confusing, like even cancer between a female and the male is confusing, even to a radiologist who's been there for 15 years. So the points, so the mathematical points might be 0.5 and 0.53. So I think that's kind of why I asked that question. Oh, yeah. OK, so now I better get your question. I asked that question. Oh yeah, okay, so now I better get your question. Let's see, two points may be mathematically near, but from an interpretive point of view, they may be practically vastly different. In other words, it is a very subtle thing in the X-ray that makes a difference that will affect differential diagnosis. And you're right, sometimes the training, makes a difference that will affect differential diagnosis. And you're right. Sometimes the training, like when you do this cross loss, it may catch it, sometimes it may not catch it. In fact, if it is very subtle, it won't catch it. So these are not perfect measures, Shalini, you're right. These are not perfect measures, but this is the best we have got at this moment. So the lab I was working with at UC San Diego and then at Stanford, they were working on MRI techniques that could measure some of these things. And the measures were very minute. So I asked those questions. I mean, I appreciate you in the sense that, you know, with cats or with cats or dogs, we can really see that because as humans, we can see that. But in medical imaging, it's so minute that only somebody who's an expert can actually tell that. And as engineers, we have not been able to quantify that. So this is my opinion that as engineers, we have not been able to quantify that as, so this is my opinion that as engineers, we have not been able to quantify that as yet. Quite likely, quite likely. I mean, at this moment, right, we are just getting our feet wet, if you look from AI perspective. It has made tremendous advances, but it has far more distance to cover. In some sense, we are just getting our feet wet. We are at pretty early stages and a lot needs to be found. It will come. So let's celebrate what it can do and also be mindful of essentially huge gaps. We are applying blunt tools to things which are very, very subtle about the human body. Your point absolutely stands. So when you look at this result, I find that very interesting. See, what I found interesting is that it can capture misdiagnosis. And so here is an image in the results that I found quite interesting. Look at this picture. See findings. Show me examples of central venous catheter, right? Now, not as a physician, but I would imagine that, and I don't know, but I have no idea actually what am I talking about, but most likely you're talking about these lines. Am I right? This is the central line. Yes, yes. So as a visual search, it did find the right pictures. Show me, then laterality is important, the human body, like people want to know left lung, right lung, where exactly is the problem? Show me examples of left pneumonia. And it seems to do a pretty good job to me. And again, I'm not, so Patrick, watch out. Tell me if I'm right. This is what it was zooming in on. Yes, yes, this is correct. And what it says is that these are both incorrect. I don't know why it is incorrect, but it seems to say it. By the way, what is nuanced and finding? What is nuanced and finding? What is nuanced versus finding? Well, that's my question as well. I don't know. I don't know what these terms mean, but I guess we have to read the paper a bit more carefully. It turns out that, oh, laterality, finding, both incorrect. Okay. So show me examples of left pneumonia. Actually, looking at this picture, Patrick and Siphal, from what little I know, one would agree that there is pneumonia, right? There are all these botches and all this congested chest there, left chest congestion. Is that right? Yeah. Yes, sir. Yes, sir. Laterality is correct. So, yeah so there yeah left left left side is correct and central venous line is correct in the left side and new maybe it's saying these cases did not have pneumonia although it's saying there is some problem with the left, but because that's why maybe finding is both incorrect that they actually did not have pneumonia. So the problem here is this patient is not a patient of pneumonia. This can be a patient of CHF, congestive heart failure so picture in both of these conditions will be same it will look like pneumonia so ai is diagnosing these pictures as pneumonia but these are actually patients of congestive heart failure but don't you don't you see this is the issue that we're seeing here this is what concerns me because i guess so yeah so this is the limitations of accurate data you you're training on very limited set of pictures here but more accurate data you have i think ai will eventually get more and more accurate i i think if you have additional cardiology data, then it will correlate better. And then Mark. Yeah. Yeah. A hundred percent. I think that's what the new ones. Yeah. Actually, sir. I think this is. Clinical. When you were saying something. I'm saying as if the nuanced one is actually on the next page. What is next page? It's just a pagination problem. Oh my. Sorry sorry sir yeah this is actually ai is correct all the time yeah so it is both are correct yeah it was a case of laterality yes uh okay so this is if you scroll down the newest, newest. Can I see the picture? What is newest? This is what happens when people don't write their paper using like, you can see that. Oh my God. Now people pick and choose as if I know I sound like a bad person, but you know, it's, it's the truth. person but you know it's um it's the truth okay this is beautiful sir yeah so show me example of small right plural effusion no left pleural effusion so we in you know medical school we have predefined mathematical formulas and here ai automatically reverse engineered that formula yeah exactly that is see guys at the end of it think about it how long would it take you to uh diagnose this i think that's the question i have to ask so you i yeah so in first year second year of med school you have tons and tons of pictures of x-ray so the main thing uh in the school first thing is learn the normal thing you automatically will learn abnormal so that that's the fundamental yeah so if here if you know if AI knows this is the normal picture it will pick up the abnormal things so as I said earlier even you know I give results of uh radiology reports tons and tons of results and x-ray pictures eventually it will know this is hard these are ribs these are lungs this is you know humerus uh clavicle bone it it starts to know that so this is happening you know same thing here now you know in the label data for the same picture there is a one angle is obliterated and it's like no no so sorry you go ahead no so uh what the radiologist i worked with he imaged my liver i think it was the position i was in the mri but he said i have a huge liver for somebody my size i'm not a very like huge but I'm not Shaxs O'Neill or anything but you know we were laughing because it was imaged in a different way so it's kind of like photography it's in a different position. So the only thing I'm asking is that so can they actually delineate in terms of what disease this is or can it say this is normal or abnormal? That's all I'm asking. So yeah, that's what I'm saying. As we go down, right, it will show you, you'll see those examples. So as you give the normal and normal, more and more normal feed, it automatically starts detecting abnormal feeds. And this is how human learning works. If I know what's normal there's also there's also there are key characteristics to look for like blood yeah yeah that's not the focus and like there are telltale signs that you look for it's yeah there's a level of interpretation jumping from normal to abnormal uh yeah can you just delineate i mean since you're since you're radiologist so since you're not but can you delineate some things like i i see that some things are lighter and uh yeah a person's chest so wow that's that's sad that's all it's the variable contrast that's it some some areas are black, some areas are not black. So that's it. So if we go up, Asif. Oh, welcome. So we have one more. So guys, see, all of this, I find it impressive. I don't know. Can you imagine that something as elementary as training through back propagation, a bunch of weights, simply by looking at cosine similarity loss can make it at the end of it, this neural arc, blip at the end of it, once you get the hang of the blip too, you know, there's a lot of magic there, but once you get used to it, isn't it still totally unbelievable that it can actually do this and do it so well. unbelievable that it can actually do this and do it so well like to the to the fact that i don't know whether ai is good that's i leave to the experts uh whether ai will has reached a level at which it it has achieved utility and uh obviously for some things it has as you only pointed out for many things it is yet to and fda hopefully will us from that. But I find that very impressive mathematically that we can even do this, isn't it? It's just awesome that we can do this, I feel. So just look at this and it does get it wrong. So I guess this was the example when it said findings both incorrect. Show me examples of fractures and it comes out with this god was really good fracture maybe it looked at this thing and called it a fracture no no bad positioning also as if this this looks like a patient who was lying down in the bed yeah you can see the black thing at the top yes your positioning is a problem that's right yeah no the black black things just probably either they they identify this or right so fracture can have so many variations there are 12 ribs and then so you know you don't know where the fracture is in the training data right so ai doesn't have enough sufficient sample to learn so it was asked for a fracture and it basically pulled up an anomaly and just like yeah so if you if you have uh let's say if i have one thousand one one hundred thousand pictures of first rib fracture of first part then ai may be you know able to learn and pinpoint it okay now i know where the fracture is yeah so, maybe that given some broken ribs. It would have been right. Yeah, yeah, that's the yes it's really hard because the variables are too many. Yes, to be honest, I mean also Patrick and to qualify went to my and I saw this. I would seriously I mean I mean I have to be honest here. My mother is an attorney. I would seriously freak out if somebody wasn't attending to me 100%. What is this? So this is not very good. This is not putting it together. Where's the pulmonary? I mean, I've worked with radiologists. They're very detailed and they're very depth. They don't write things in one sentences. It's driving me crazy. I mean, it's a work in progress. It's a work in progress. I think maybe Stalin is thinking that it's used by the physicians when they're taking the patient, right? Yeah. I think what I can see the use of this is more when the researchers actually looking at the images. So let's say like you're developing a new drug or you're creating a new trial, right? And you're trying to do the analysis and see like what would work in what patients. So you're like masking all the data. So I think it's more applicable in that scenario to start with. And then at some point it might go to the physician and that other people yeah shall we repeat what mustard said he said that see uh he believes that we are thinking that this will actually be used in practice whereas he's pointing out that this is all in the laboratory and all for research got it got it got it i think i think even in r d this is um this is rather troubling and i i don't think there's enough discussion about you know it is it is yeah we should i mean yeah let's it's like a i'm sorry i mean i i mean i do aerial silks i haven't broken a lot of bones but i do have some concerns about that you know um and i might be one of those people try to give ai a chance okay okay i'll leave it at that okay i'll give a chance as much as i was the original ai engineer you know creating um we'll leave it at that but you know um i'll give ai gems but i think you know with radiology it's deep yeah it is indeed having said that we are making baby steps in that direction actually though radiology has as if us radiology has made a huge difference on cancer detection right tumors yes yes sir incidental finding uh as I said where you know they're detecting these nodules and all the things positive to Charlene's point though um it's true the way the way it's presented here uh not that good yeah the physician the physician will not be able to maximize the utility of this because it's just essentially it's it's looking for keywords that that actually have deeper deeper descriptions underneath the hood so let's say plural diffusion uh you're looking for something like like a description something like a blunting of the costofrinic circus which is you know how on the lungs on the very lower outer edges it's almost like a pyramid or like like a triangle like like on this image on the right side you can see it's almost forming like a triangle yeah if that starts to it's if that starts to to disappear you know that there's water, there's water in the just in the nematodex. I have a question. Actually, I thought when I looked at this image, I was impressed whereas you are not. When I look at this image. No I'm not, this looks terrible. No, as if what this does is it's abstracted the steps to get to it and just tells you if there is or there isn't. So as a further radiologist, what happens is they would add those descriptive terminologies way above. pulmonary edema present or cannot determine if pulmonary edema versus, you know, like worsening heart failure or congestion. So those kinds of things. So as a clinical tool for decision support for radiologists, maybe it's still a little far off, but as a tool that helps us but sorry as any strength best use is for triage right now so given all these pictures let's say we have all these patients in the er right now which patient needs more attention so if you have all these x-rays ai will tell you you need to take care of the pneumothorax first. Sure, sure. Yeah, so that's, so that's, yeah, that's a good use case of AI. Yeah. So it's okay. But I think my what Patrick is saying is right, which is, you know, there needs to be some descriptive words and things like that. And I don't know if they've actually labeled that correctly. So this is, as an engineer, this is the question I'm asking. Yeah. So if you look at this picture on the right side, so it's an actual report that the radiologist is hoping to send. It says stable appearance of right side pleural effusion, pneumothorax has resolved. This is actually just to summarize the final diagnostic, but above this, usually a radiologist would explain the actual findings, like interpret the actual signs or different characteristics of this image. So if there's cardiomegaly and all those things, they will give you a full report above. This is just the part where it summarized everything. And I understand why they made it short, because they're using cosine similarity. But here, if you look at the answer, it's just too brief. It's just saying, yes, there is right pneumothorax uh so it just it just picked up and it's just it's just detecting so right now it's just detecting the key points that need that it needs to focus on so it's just highlighting the presence absence it's just it's just creating essentially the segments in the imaging that are pertinent this might be useful but like like for triage or just those quick rapid diagnosis yeah but yeah but but maybe for medical stuff we have to find a different kind of uh similarity cosine rather than cosine similarity in hyperdimensional space and then maybe we move it or i think i think we should just really test how how far they're able to take this because this is just if this is the best like officiality said if they're cherry picking already and this is the best then it's not really very uh like like very useful for deeper clinics maybe for for limited use cases but if there's there's more that we can test out of this, then sure, sure, maybe there is something more we can uncover. Can I ask you guys, since you have been, so why is it that, so when I was in the radiological department at UC San Diego, they said that African-American ladies had the highest, what's the word, they had the highest what's the word? They had the highest numbers of patients with breast cancer, which was pretty sad. And why is that? Is it their food? Is their diet? And there was some other people- Genetic predisposition. Because the genetic predisposition. Okay. They have special kind of special kind of chromosomes and genes that predispose them for higher cancer yeah i would need more information to to give a straightforward answer in this yeah i'm not too familiar uh a small small introduction I'm not too familiar. Dr. A small introduction. Let us continue to discuss, but I'll stop the live stream. Dr. Sorry, Dr. Kamari. We were just deep in our discussion. Dr. No, no. It's a fun discussion. Please continue. But I'll just stop the live stream just so that you can hear. Dr. Oh, dear. Did we just say something terrible?