 The particular classification algorithm that I will discuss today is called logistic regression. Now despite the name regression, it is actually a classification algorithm. So the name is sort of in it's of historic relevance. So you may just consider it to be odd, oddity, we'll leave it there. Logistic word comes from the fact that the equation that is involved was initially created in the context of logistics, you know, moving food to an army and things like that, you know, working out the logistics. Like for example, when you hold a picnic with 30 students, or a lot of families go out on a group picnic, somebody is managing the logistics of it, you know, making sure everything works, food is there, everything is taken care of, hotels are reserved, and vehicles are there. So that is the meaning of the word logistic. Now, both of these words, I mean, they together are there, but as we'll see, it has nothing to do with either logistics or with regression. It's a classification algorithm, just has that name for historic reasons, and because the logistic equation actually is involved. So what is this? I will motivate it using a slightly different example. Suppose you have, imagine that there are two fruits that you have. Imagine that there are two fruits that you have. That could be, so remember that these are all contrived examples. These are only of educational value. In real life is different. So imagine that there are two fruits, a blueberry, where is blue? Here's blue. Blueberry, which are small. And so the axes are weight and size. Right. These are the two axes. So then you have blueberries, which are. Like this. In weight. You get and then blueberries and then what should we say actually in India you don't get blueberries but think of a thing called do you guys know something called bear nobody knows yes right so you can think of that and now think of something which is a little bit heavier. In US, I would call it cherry, but in India, what should we say? A little bit more heavy and bigger fruit, something like that. If you guys can think of something, think of that. Jamun, jamun is certainly not heavier than bear. Bear is heavier than jamun. Strawberries? Yes, think of strawberries. You have strawberries there in Bangalore, is it? Okay, so that's a good example. So suppose you have data like this. And so what will happen is occasionally you have a small strawberry you may have some data like this and you may have big little bit occasionally big blueberries so things may be like this now your goal is to write a classifier a magic box which does this which does this. Given the size, let me call this x1 is size, and x2 is the size, x1, which is the size of a fruit right fruit features right so these are the features of the fruit fruit features you have to determine this question blueberry or strawberry. And you know, just looking at the data, that perhaps size and weight are not enough to tell them apart. Because when you look at the data, do you see that there's a degree of overlap? Isn't it? And even though there's an overlap nonetheless you still have the job of writing a classifier that sort of works in this situation and so what is the best you can do you can say that my classifier the best that i can do is something that is that goes like this yellow line and the rule that I can make is on this side of the yellow line, it is a blueberry. And all data points that are on this side of it, they are strawberries. So suppose I have two points. I'll take a point that happens to fall here, A, and another point I happen to have B. Right, A and B. Now, how do you determine whether A is a blueberry or a strawberry? Looking at this picture, how would you determine that? I give you a point A with certain weight. Let me just call this a data point. Certain weight and certain size. Weight A and size of size A. How do you determine whether it's a blueberry or a strawberry? Would you like to hazard a guess? So air looks like what, guys? Does it look like a strawberry or a blueberry? If you had to guess. Strawberry, sir. And why is it a strawberry? Strawberry. if you had to guess strawberry and why is it a strawberry color no remember you did not get color as an attribute you just got weight and size as an attribute and you get the yellow line yeah it's weight and. Yes. So what you say that it is on the upper side of the line, isn't it? Of this yellow line, it is on the upper side. A is in this side of the yellow line and B is on the other lower side of the yellow line isn't it guys can you say that yes yes so for that reason this so this yellow line helps you decide whether a fruit is likely to be a blueberry or a strawberry now notice that i've used the word likely. In machine learning, you're never sure. You never have complete data. Do you notice that there are regions where blues and reds intermingle? So in machine learning, in data science, you always say, what is likely to be true? So you say that below the yellow line, it is blue. Above the yellow line, it blue above the yellow line it is a so this line that helps you decide in this field it has a name for it the name of this is and this word that you see all over your textbook will be a decision boundary by the way it is high time i mentioned uh this thing to review what i'm going to teach you today and what i taught you last week read chapter four of the islr textbook read as much as you can understand but so long long as you watch this video and understand what I'm explaining, you get the gist of it. So that is why the word in the literature, in machine learning literature, this sort of line is called, it has a very important role. It helps you decide or classify. So to have found this line is to have built a good classifier because all you need to do now is to tell whether it's above or below it. Now we can generalize from the line in logistic regression. It's a straight line in higher dimensions, it'll be a plane or a hyperplane, but in other classifiers, it need not be straight. It may be bent, it may have all sorts of irregularities to it. So it will be some curve, but in logistic regression it's always a straight line. So now you ask this question that, see we talk in terms of the probability that a given point x1 x2 is blueberry or strawberry so when you talk of probability pick a class maybe we'll pick strawberry so we will say probability of a strawberry and by the the way, can I just represent it as a vector x, x being made up of x1 and x2. Now you may wonder why am I not using xy that you have in your school algebra. It turns out that in machine learning, we reserve y for output, and all the inputs are given tags like x1 x2 x3 it's just a convention nothing magical about it so you say that strawberry given what is the probability that it is strawberry given some data point right so this vertical bar which i'm introducing now it stands for the word given whenever you see the word given it it stands for given, right? So what you're seeing is the way to read the sentences. that it that x vector that x vector locates a strawberry does that make sense the point what, what is x vector? x points, x features. Certain weight, certain size. It locates a strawberry as opposed to locating a blueberry. That's how you say. So probability that this implies this. What's the probability of a strawberry? So then you say that, well, that is very easy. All I need to know is this line, right? Now, what is the equation of a straight line? The equation of a straight line is, remember we said that we usually write mx plus b or c in school, but in our regression class, we learned a different notation, beta naught plus beta one X. Do you remember that? That we don't use this notation in data science or machine learning, we use this notation. Where beta, this is a beta naught is intercept and beta one is the slope. and beta one is the slope. So we know that if we can write this, discover the line, let me just say the line for the decision boundary. If you can write a line, actually this is not the line for the decision boundary, I take that back. It sort of is a line, there's a line and a line is always represented with something like this. But I was oversimplifying it actually. I think I get it. So you know that the probability that it is a strawberry, right? So I'll just not write strawberry, I'll just say probability of a point given a point being a strawberry is somehow related to the line. the line. Decision boundary line. And it turns out that there's an interesting bit of mathematics and explanation to it. Again, it's a video on my website. I won't go into it to respect the fact that some of you here don't have a background in engineering. I'll just state the result and you take the result as a think of it as a god-given truth. The result is actually a complicated function. It says that the log of the probability divided by and I'll explain what this is is equal to beta naught plus beta 1 x good grief what is all this business of log and all that so what happens is suppose you say so look at this let's break it up into pieces this part looks simple right linear equation this is your linear equation that you're familiar with from regression, isn't it? The familiar linear equation of line. It is that. What about the log? And so ignore the log for a moment. If you have the probability, suppose the probability of something is px of strawberry. of strawberry, what is 1 minus px? It is the probability of it not being a strawberry. So if it is not a strawberry, what is it? It's a blueberry, isn't it? And so in a way, suppose I were to give it a notation qx, qx for the probability of a blueberry. So this term is actually the probability that it is a strawberry versus the probability that it is a blueberry, isn't it? This stands for strawberry. You're saying, what is the difference or ratio of their probabilities or their chance of being this, right? Blueberry. Now it turns out that this number is a pretty good measure. See, if the chance of it being a strawberry is more than the chance of it being a blueberry, then the ratio will be more than one would you agree if let's say that probability of its being strawberry is 0.6 what is the probability that it is a blueberry it is of course a 0.4 because the thing has to be either a blueberry or a strawberry and so 0.6 divided by 0.4 is a number greater than one. On the other hand, when it is likely to be a blueberry, then let's say that the chance of being a strawberry is just one fourth, 0.25, and the chance of being a blueberry is threeth, right? So then the ratio becomes 1, you know, 1 4th over 3 4th, and that is equal to 1 3rd. This number is less than 1, but it is of course greater than 0, right? So what happens is that based on whether the number is greater than 0 or less than 0, you can determine whether it's a blueberry or a straw, greater than 1 or less than zero, you can determine whether it's a blueberry or a strawberry, greater than one or smaller than one. It determines whether it's a blueberry or a strawberry. Now, a very simple or a very rough way of looking at it is when something has to be decided at one, right? Less than one is one thing and less than more than one is the other. Then you can sort of zero base it by simply taking the log. So what is so this is a little bit of high school algebra. Log of one is zero. Log of a number fraction, let's say one third actually negative. And log of a number greater than one is positive. So if you take the log, right, the statement becomes very simple. By looking at the log of the ratios of its, there's a word for it, by the way, it's called the odds. Odds of being a strawberry is the chance of it being a strawberry divided by the chance of it not being a strawberry means having a blue bay. In English actually you often see the word odds. Sports when you watch cricket you may have heard the commentator saying the odds are decked in favor of this batsman because the bowler is an easy bowler or something like that. You hear that language so this is what they're talking about. So the log, you take the log in some sense so that you are zero based because below this, it is a strawberry. I mean, sorry, a blueberry. Above this will be strawberry. So that's just a way of thinking. And so this is the famous equation. It is called the logistic regression equation. Equation. It was, as I said, discovered not in the context of a classifier, but it predates that. It was actually discovered in a slightly different context. This is a very famous equation, and I will just draw it out to you how it looks how does the px look like what is so this thing right let me just call it z right so you realize that okay so a little bit of so this part of mathematics if you don't understand don't follow don't worry about it so I will just give you a sidebar on the logistic function because we have suddenly encountered this peculiar function. So imagine a function pz, 1 minus pz, because now I can write it in terms of z, terms of z is log of that is equal to z itself where z is that beta naught. So this equation is used all over mathematics and it has a very interesting shape. It's a well celebrated shape in mathematics. Suppose this is the z-axis. Let us just call it x. An equation that looks like this. No, because we have used x. Okay, let me stick with z. What happens is that, and this is p of z. So you realize that this equation is some curve. This curve has an interesting shape. This curve has an interesting shape. This is 0. This is 1. And this equation, now let me draw it out in a way that will make sense. So it has an upper limit. It goes at 1. It has a lower limit, which is at 0. And it never quite reaches 0, but we will pretend it does. And its shape is 0.5. So I'll shoot for this it goes like this are we together it goes like that or maybe i'll squish it to to bring out the shape i'll squish it a little bit i'll make it like this Now guys, look at this shape. It flattens out at one. It is called the upper limit, the technical word is upper asymptote, but we won't use the word. The fancy word doesn't add anything to it, so we'll ignore it. And this is the lower limit. So the value, so for very negative values of z, very negative values of z, minus infinity, let us say it goes to minus infinity, and it goes to plus infinity. At minus infinity, if z's value is very, very low, the probability is zero. At very high values, it is equal to one. And at some value, at half, the probability at zero, when z is zero, it is half and half. So the question is, what is the intuition of that? The intuition is very simple, actually, and that's why I use the yellow color. If you go back to this figure, that Z is actually the distance from the decision boundary. So you realize that if you're sitting upon the decision boundary, that yellow line, then it is half and half. It could be blueberry, it could be strawberry. If it is less than like negative, then it is a blueberry, positive, it's a strawberry, right? So that that is sort of the threshold, that line is the threshold in which you flip from one to the other, and it represents this point, right? This point here. Half and half probability that it's one or the other. So this famous equation, notice that it has a way, how do you remember this equation? So this famous equation, notice that it has a way. How do you remember this equation? Write S in English. This is how you write S, isn't it, guys? And now stretch it out in the two directions. And when you stretch it out, it will become. Would you agree that it out it becomes like this do you see that guys yes sir you can see this is it and that's why this logistic equation is also called sig, because of the S, sigmoid. It belongs to a family of functions which are called sigmoidal functions. All of the sigmoidal, there are many such mathematical functions which look like the stretched out S. And I wouldn't go into many. There's tanh, there is inverse sine x, tan inverse. There are many such functions that have this peculiar shape, this familiar shape. And this shape is very used. Let me give you a little example outside data science. me give you a little example outside data science. So suppose you look at population growth on an island, right, and let's say that the initial food is very, there's a scarcity of food. So let's say that this is the normal, zero is the normal amount of food for human beings or any animal rabbits to be on that island. So at that level, the, you know, the popular, if you get normal food, what happens? In the absence of other factors, population grows, rabbits will grow, more rabbits will be there, they'll reproduce, one rabbit will, mother rabbit will give birth to many more rabbits. Let's say two rabbits. They will give birth. You know, the population of rabbits will multiply. But after a little while, what will happen is on that island, there is only so much food available, you know, rain and so forth, the crop, and there's only so much of grass and nuts available so there'll be a limitation of food when there is a limitation of food the population cannot increase do you see that guys because the the ecosystem cannot sustain a population bigger than that the the huge population of rabbits will start starving and so there will be death from starvation. And so the population here will, in this region, it represents a limit at which you have reached a point at which population, even though more rabbits are being produced, other rabbits are dying of starvation at a pretty brisk rate, so that the two forces balance themselves out. On the other hand, if you look here, what is happening, the population of the rabbits is very low, and probably at this limit, by the time the rabbits reproduce, maybe disease or something else kills them. It is not the scarcity of food, but it is just the fact that the ecosystem is a little bit harsh and maybe there are preys, there's a who eats rabbits, maybe wolves and foxes. They come, wolves are there and they'll come and eat up the rabbits, right? So they're barely getting a chance to survive. Not enough of them are there, right? So ultimately you need to have more rabbits than needed by the predators, by the wolf or something like that. So this is the classic population curve and there's a little bit of it, if you look at it, it's a familiar population growth curve of everything. One of the things, for example, is if you look at India and all of these countries, Sri Lanka, China, and Thailand and so forth, they all got their freedom pretty much at the same time from the Britishers. So before freedom, if you look at, India used to be in this zone, population zone. The country was, like, see, there was food. We used to have famine, of food hoarding as a prey upon the population, as a predator, but there was disease, a lot of diseases were there. So India's population was actually not rising at a big rate. Then what happened, somewhere around the time of India's freedom, you realize that something interesting happens. Suddenly, the country, and this is, I mean, if people argue that colonialism, like people write the books that colonial, like the Brits were actually good, they were helping India and so forth. The single biggest proof actually is a population equation like this. When they were around, India's population was actually pretty stable but right after they left the population started growing why did the population grow because suddenly the country managed itself better there was more food available less famines the number of famines that are there in india since freedom less, they have been there, but they are less than they were there before the freedom. And somebody has an estimate that 11 million people died between 1900 and I believe 1947 when the Brits left. Something like 11 or 13 million people now we know have died as a conservative estimate just from famines under the British rule. Obviously, the death since then has been much lower from famines. We did have a famine actually after freedom in the early 90s, but we got a lot of financial aid at that time. Actually, of all things from the Americans. I don't know if you guys remember the history that was just before the Green Revolution in India. So there was at one point, some data is that one ship was leaving US port for India, practically every half hour, completely loaded with wheat, so wheat and powdered milk and things like that and we got a lot of aid actually to tide over the famine but that was again something as a independent country as an independent government we could ask for aid and get it and then the country got serious about food found ways got into the green revolution which was happening around the world and i don't think we have had mass famine in india after that but anyway i'm just putting it in historic context what this logistic function looks like and how it describes population just as an example of things beyond just data science so this equation is a very famous and very useful equation in mathematics. Remember that. That is at the behind this classifier. But so knowing this classifier and knowing this concept of decision boundary, I'll just review what we learned today. We learned, for example, that there is a concept of a decision boundary. Decision boundary is sort of the fence. One side of it, one side of the wall is things will be more likely to be one thing and on the other side they would be more likely to be the other thing. But the closer you are to the wall, the more unsure you are whether it is one or the other. Like close to the wall, if you're on one side it may be a little bit more likely that it's a blueberry. If you're on the other side of the wall, literally standing next to the wall, if you're on one side, it may be a little bit more likely that it's a blueberry. If you're on the other side of the wall, literally standing next to the wall, it is maybe strawberry. And the further you go from the wall, the more sure you are. And probability is exactly that, how sure you are or how certain you are that it is strawberry, for example, or blueberry, for example, right? So classification, therefore, we learned something here in the theory of this. Classification, more broadly speaking, when we now going back to classification, the key concepts that we learned is there is a classifier. A classifier is your magic box that can tell the answer. But the way the classifier magic boxes work with logistic regression at least is it doesn't say it's a cow or a duck or it's a blueberry or a strawberry. It instead says a more precise statement because there is a degree of uncertainty. It comes up with, given any point in the feature space, feature space means height and weight space, it will tell you the probability that it is a strawberry or it's a cow or whatever is, right? The probability and the set of all points, all points where probability is half, they make up a line. The famous decision boundary. So next time when we meet, we'll do the lab. Again, for those of you who joined late in order to celebrate Diwali, we will not be having a class the coming weekend. We will be having a class the weekend after that. But the week after that, after Diwali, we'll have two sessions, not one. One session will be to give you guys lab help. It's optional if you want to be there. And in the meanwhile, do please reach out to Anil. He'll help you. And then we'll have the usual saturday night in california and sunday morning in india session so far so good ask one question ask one question and go ahead yeah see this uh sigmoid function or logistic function we also represent like one y one plus e minus x right yes yes so what i wrote actually i wrote in a more intuitive way which i prefer because the meaning of it is easy to write it is z so now what happens is since you asked for that bit of mathematics so it's a very simple math so now i can write write this PX, 1 minus PX as E to the Z. I exponentiate both sides. So this becomes this. And now what happens? It is P. What happens if I add this quantity to the denominator, the numerator to the denominator on both sides? It becomes this E to the Z. And if you think of it like this, one plus e to the z. All I'm doing is adding the numerator to the denominator on both the sides and that typically is possible. And now if you look at this equation, therefore, this becomes, now suppose I divide numerator with e to the z and denominator also with e to the z. So it becomes the more familiar term that it is written in books, e to the minus z. Do you see that? Right? But see, when you look at it this way, right? It is okay to put it this way. And for some reason it has become historically the familiar expression. But in my view, see the purpose is it's an explicit form. That's the reason in a very extra sorry p to the z i should say you know i mixed up my notation everywhere i kept using px px px knowledge pz so like both are same basically right just like yes yeah exactly it is both are the same most of your textbooks they will use the word they will write it in this familiar form, right, more common form. But if you look at this more common form, and for example, your textbook does it, it doesn't explain why this weird and strange expression, how it came about. how it came about right you don't get the intuition of it whereas if you read it the way i explained it to you the intuition is more obvious it is log odds let me write the word log odds of it being strawberry is equal to z right this is the intuition behind the logistic equation isn't it that is why i find i prefer the more the unconventional form but that is more explanatory in some sense right but it is the same it is equivalent this is therefore equivalent to this so guys i apologize this uh I introduced a little bit of mathematics. So those of you who are coming from business background, you can gloss over the mathematics. It's not that hard, but it will take you a little bit of time to reason through this. And when you read the book, you'll get a good review of it. The book actually does not give you the explanation or the roots of it. But even I skipped quite a bit of the mathematics here. There is a video that is on my site, the decision boundary, right? Distance function or something called the distance function. Look up that video. There I have given a fairly detailed explanation of the underlying mathematics of the logistic regression, and not just logistic regression, to a whole class of classifiers that uses a line as a decision boundary. That includes, by the way, the linear support vector machines and things like that. support vector machines and things like that. So you'll find that video there for those of you who are interested, but we don't need to know more than this. So what was the whole point of going into the theory of this from a practical perspective? See, at the end of the day, you'll probably forget this theory. When you work in the industry, people just give you data and they want you to model it, make good predictions from it. And so you do that. This is just, it is worth knowing at least one classifier and see how it has reasoned out things. And so I thought I'll give you guys an intuition into at least one classifier, namely the logistic regression classifier. It's a beautiful, I hope you guys appreciate the beauty of it. It's a very beautifully argued thought. This classifier is, and there are many classifiers behind all of them are very elegant mathematical theories but you don't need to know those theories to use it in practice i mean i know it for a fact because i interview a lot of people to hire for data science positions and usually most of them don't know this theory and i just probe a little bit and then don't go any further but it's worth knowing that something beautiful exists under the covers all right guys so this is it the most important thing is to understand the concept of a decision boundary and to know that classifiers the goal of a classifier so let me state the goal of a classifier there whether it is logistic regression or whatever it is, the goal of a classifier is the goal. A classifier. Is to. Draw. It's a drawing problem draw a decision boundary or a separation boundary. between the classes, like you know, cow, duck, etc. all that if we add t feature space feature just space means that axis of cows and like for example the weight weight and size actually here i shouldn't use cowdice i gave you the example of strawberry blueberry right strawberry blueberry for example, a strawberry, blueberry, strawberry, blueberry. Those two classes. In the feature space, feature space is literally the space of your inputs. So this is literally what it does. If you can draw a decision boundary, you have essentially created a smart magic box, smart classifier. And there are elegant theories of finding this decision boundary in the data. It's something hidden in the data. You can't see it. You can't uncover it using a SQL query. You can only find it through machine learning exercises. And that summarizes our understanding of classification. I will stop here. If you guys have questions you can ask but I'll stop there. Thank you.