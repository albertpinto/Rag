 In the previous session, we talked about a word, how to represent this building blocks of natural languages, words, sentences, paragraphs, documents, represent them in a vector space representation so that they could become input to machine learning models. Once we can do that, it opens up the door to classification, regression, topic modeling, clustering, and so on and so forth. So we gradually covered two, three approaches. Words could be, we could do one hot encoding. Then sentences and paragraphs and documents could be just some form of summation of those words. Either you concatenate those word encodings, one hot encodings, or you sum them up, aggregate them. And so, for example example you can get the count vector or you could just get a unique vector by concatenating those word vectors then gradually we came to a different word different document representation in which we did we replaced the word count with instead it's DFIDF which is more representative of what this document is about. You can look at the vector space and tell that this is about Cheshire cats or this is about the politics and so on and so forth. So DFIDF was good good there is a problem though with TF IDF it is not very context aware so see in language the proximity of words matter we tend to put Cheshire and CAD together but we don't tend to put for example a truck usually together with back propagation right so for example if you wouldn't when you're talking about transportation let's say that you there is a your brother or your sister has a is an entrepreneur dealing with a lot of trucks and logistics and picking up things and having them shipped to some other place and this and that using trucks. So the vocabulary or the sentences that your brother or sister or your entrepreneur would say are very different from the words that you would use if you're talking about, let's say Cheshire cats would use if you're talking about, let's say Cheshire cats, or if you're talking about machine learning, AI and back propagation, the vocabularies would be different. And so it raises the question that see, words don't exist in a vacuum. One hot encoding is not a good representation because the dot product between any two words is zero. Remember, we noticed it. And the Euclidean distance between any two words is always square root of two. That is not particularly useful. We want a vector space representation in which words that go together in sentences and documents tend to they tend to go together they also show up in the vector space representation next to each other right so a twin words like Cheshire cat should be next to each other right a king and a queen should be next to each other, relatively speaking, compared to a king and back propagation. Are we getting a sense? So can we have a vector space representation that looks at the relationship between the words? And so there are a few approaches that do that. And we're going to talk about those approaches but they search for a different word vector space where relationships are captured so I will start with that so let me put the main idea there we want a vector space space that captures the semantic relationships between words right so for example we should be able to say things like king and queen are near each other, whereas a word like king and backpropagation are not words that are near each other. Isn't it? in some sense we need we need things like that for example so how do we create such a vector space representation that captures word relationships. Notice that when we create a TF-IDF, we absolutely did not use machine learning. That was a basic statistics. Do you agree guys? When we did TF-IDF, it was just been counting. We just count the frequencies of things and that set of words and in how many documents they show up and so forth. You get that. But now we are saying, can we look at the corpus of words? Because a corpus of documents is a corpus of a lot of words in the relationships, right? In a sentence, words, they don't exist in jumbled up order. They exist next to other related words, isn't it? So, for example, when we say the quick brown fox jumped over the lazy dog. Quick brown fox. These words are related. Right? We are likely to say them together. Isn't it? So, lazy dog, for example. Or when we say the cow jumped over the moon. There is a relationship between words. So how do we search for that relationship? The way we do that is we take a classic machine learning approach. So what is the machine learning approach? It is this. You say, all right, let us imagine that we have that vector space you work from the solution backwards so you say first of all assume that such a vector space exists how many dimensions do you want to make it so pick a dimensionality a very common dimensionality that you may pick up is like 300 or 500 this is very common to do so. Now, a dimension space. Space. And then you realize that that is a huge reduction compared to the initial vector space where a word was represented with 10K dimensional vector, right? If the word was one hot encoded as a 10k dimensional vector minimum or 100k dimensional vector or a million a dimensional vector but you're saying let each word be represented be represented by a vector so suppose you have a word vector, initial vector, V input for a given word, for a given word cat. You want to map it to another word. Let me call it E of cat, E vector. Now this E vector belongs to a much smaller dimensional space let me just call this dimensional space the embedding dimension let me use a little word d prime right and this is D so you're taking v input belongs to r d d and you're mapping it to another vector and you want to create a one-to-one relationship to another vector and get a simpler representation so you say well don't you lose a lot of the information when you do that and the answer is not necessarily you don't lose information so see the intuition that we all carry is that if you take a two-dimensional picture of a three-dimensional object you lose information you take a building you take an architecture it's a three-dimensional object you take a picture of that using your camera you lose information now you can only see the side that you were seeing when you took the picture the three-dimensionality of it is lost are we understanding that guys so whenever you come down when you represent something from a higher dimension to a lower dimension you're projecting down and the question the most natural question that you would ask is are you losing information and the counterintuitive answer I suppose in some sense or the surprising answer is that not really you in fact don't lose any information in that process now the thing is how could that be that would be something interesting to think about how can we project something down how can we a project is like looking at the shadow of something or looking in the mirror in a lower dimensional mirror at something and how can you not lose information so i'll give you a different example to show that it you can actually do it without losing information so let us say that in a painting if you think of a painting as a as a narrative. Let's say that you have a painting. Painting or image and let's take a simple image four by four image. Well, A four by four image right now each point here you think of it as a word each pixel so word mapping to a pixel image the the document is essentially the word is this maybe your sentence is each row if you want and document is in the vocabulary of language the document is the image itself the full image all these words all these pixels put together so if you look at a pixel a pixel can be represented by some color isn't it now your modern televisions modern lcd screens they contain approximately 32 million colors are we together they're approximately 32 million colors that a typical LCD, for example, the monitor on which you are watching me at this moment talk to you, probably can show 32 million colors. Somebody who is in graphics or web design, please correct me if I got that number 32 million right or not. Vaidhyanathan Ramamurthy, I'm just remembering from what I heard. I might be wrong. Maybe it's 3 million colors or something. It doesn't matter. It's a lot of colors. So every pixel. If I look at this word w one w1 the w1 is in effect a value I can one-hot encode it of all the 32 million colors it is that one color and not any of the other colors zero everywhere else right so maybe this is the salmon color W2, let us say is the second pixel happens to be again, somewhere there's a one, zero everywhere else. And let's say that this is a midnight blue. Are we together? So you realize that each of these colors, each of these so-called word in some sense in the image is one hot encoded very much the way we one hot encoded the words in the vocabulary. Isn't it? So you can say that the vocabulary of a painting or an image is made up of 32 million colors and any one pixel could be any one of those colors. Is that looking familiar guys? Is that an equivalent way of saying? Yes, sir. Simple, right? We agree. And yet now, suppose I ask ask you do you store pictures or images you know the jfrgb i mean png and all of these formats do they really store each pixel as a 32 million dimensional vector right so think about it this way wi belongs to a space that is 32 So think about it this way, W I belongs to a space that is 32 million dimensions, right? Dimensionality is this much. That's a massive dimensionality. And it would take a massive amount of space to store it. And we know from experience that's not what we do. In fact, each word, each so-called word or pixel can be uniquely defined by three components how much of red it is how much of blue it is and how much of green it is isn't it guys right 0 to 256 255 this is again 0 to 255 and this again is 0 to 255. And this again is 0 to 255, right? A very small integer space, right? And maybe if you want to be generous, you can also include an alpha channel, which also will take on a value from 0 to 1. Transparency channel. This is the transparency red blue green transparency are we together guys that actually a pixel can be represented in just a four-dimensional space or traditionally just a three-dimensional space rgb space but when you represent a pixel not by a 32 million dimensional vector but by a three-dimensional or a four-dimensional vector are you losing information ask yourself are we really losing information let's think about it would anybody like to volunteer and answer no we don't want to represent it because it is a completely it is a complete representation and it is a complete representation because there is an underlying relationship between colors in in the language of natural language processing you would say that there's a there's some semantic relationship between all the colors they are not just hafizat right for example the salmon color in the indian red color and the red color and the orange color and the brown color they are fairly adjacent compared to blue light blue you know ultramarine blue midnight blue and so forth which have themselves close to each other so there are relationships and not only that because of the theory of light which says that out of three primary colors, you can manufacture any color, right? Therefore, you get away with it, right? Therefore, you are able to do a complete representation just using a four-dimensional space. Now, if the theory of primary colors and lights coming out of primary colors were not true then the story would be different right then they would the stories would be really different but that is what gives us and so in other words between the colors there is a structure that helps create the words now in language we of course know that words are interrelated to through the semantic structure so we ask ourselves therefore in the same way as a picture can be represented with just four dimensions or three dimensions rather than 32 million dimensions in the same way can we represent words in a lower dimensional space and still not lose their semantic meaning in fact in fact not only not lose but gain semantic relationships we live in a very high dimensional space where every two words are perpendicular to each other practically right every two words if you think they are perpendicular to each other right so you can think that see that that square root of 2 is very interesting if you think about it imagine that you have a x axis and y axis one point is at unit distance one and one point is at a unit distance along the y axis a and b what is the distance between a and b it will of course be square root of 2 the Euclidean distance and 0 for cosine distance between this vector and this vector do you see that guys So every two words are perpendicular to each other when you think about it in this one heart encoding space. So if all the words are orthogonal to each other, relationships are simply not captured. Whereas we want a lower dimensional representation, typically three or 500, you pick your dimension, you can pick 100 or whatever, a dimensional space and say, can I project all the words down to these dimensions? And that dimensional space is still very rich. What you're seeing is, can each word be represented by, let's say a smaller number of dimensions, 300, 500, real attributes of those words, linguistic attributes of those words, or semantic attributes of the words, says that the relationship between words are captured. And that is a pursuit. So when you do that, the word that people use in this literature is embedding. Word embedding word embedding word embedding word embedding in other words is a projection is lower dimensional lower people often use lower dimensional and more mathematically rigorously people will use lower rank representation of a word of words right so let's do that. So the traditional machine learning way is if you say, what is the relationship? How can I do that? You start with a very simple way. You can say, hey, take words, each of the words, W-I, and to each of them, just assign some random values of 300 dimensions. Let's say, I'll'll work with 300 i don't want to keep saying 300 or 500 or whatever i will just or let me just say d prime right d prime where d prime is for example 300 or 500 or whatever it is so in this so one easy way is to do this you take a vocabulary of words and each of the word is you realize that the word each of the word is let's say 10k dimension they are because they're 10k words you will start with our work or whatever the first word in the vocabulary is right and you will go with Z Z Y whatever last word of the vocabulary is right I don't know what the last word is but the last word that I know is probably like zygote right so let's say you go from a a art artwork to the zygote or something like that. Right. Now each of these words you're saying that you're representing it. So how many, what is the height of this, this is 10k. Now, this is This is a B prime right the dimensionality of the number of cells here are that. So you see that, see, suppose I start with a situation like this, a matrix like this, such that each represented by this and then What is it if it was perfect if? we had Literally put some values here these values if we had Perfect Embedding vectors Vectors then for example let's say a daffodil would be closer is near iris but far from far from let us say a scissor things like this would be true are we together guys right suppose we had successfully done it. So now the problem becomes how in the world do we populate this with the right values? What are the right representations for each of the words? So there are many algorithms that people use. They all depend on one basic idea. You feed it sentences. When you have a sentence, let's say line from a poet, a British poet, Wordsworth. It is... breathes it is my fate that every flower enjoys the air it breathes again it's from for lack of any other sentence that would occur to my head this one popped up so I put it here now you you you can look at this and say you know what can I predict the next word or can I predict that these two words these two do you see that they they tend to go together right flower enjoys or yeah these words are much more likely to happen together in this sentence and perhaps in many many sentences we all breathe air if you look at this sentence these two words the proximity in this sentence is reflective of the general proximity of meaning or the context of air and breathing would we agree guys for example it's unlikely to be a sentence that it is my faith that every flower enjoys the air it scissors right that is unlikely to be there or horse some things like that So we start with this and we we use as a proxy or the fact that Empirically words that are related They tend to be proximal to each other in sentences So far so good guys? Yes. Yeah. So then almost all algorithms that find a search for word embedding, they have an implicit assumption that a proximity is a good empirical evidence of semantic relationships. Of nearness, semantic nearness. And they exploit this fact. So there are many, when we do the natural language processing month, one month of natural language processing in this workshop, as we move forward, we will do many of these algorithms in detail and they are continuously coming. I'll start with the earliest one. I believe it showed up in 2013 it is called Word2vec. So one implementation is Word2vec word to it's a very good embedding it's a very popular embedding of words so how do you generate this you can generate it in two ways either you take the general purpose corpus like the, so people have created massive corpus, Stanford University and this and that. They have created large corpuses for example of English text. Every country has created a standard corpus of lots of documents in specific languages. And you can therefore look at a proximity of words across the massive amount of sentences in those languages right and therefore you can learn word embeddings from that or alternatively what you can do is you can hand build your corpus and do that and there is a third way of creating embeddings which we won't go into here. You can create an embedding implicitly in the process of doing something else as a side effect. For example, you're writing a classifier of words, but you feed it initial one hot encoded words. The first layer is there is an embedding and you train the embedding as part of your journey of training a neural network to classify the words into something, you know, the sentences into something. For example, whether it's about politics or it's about physics. Right? So things like that. You can do that. But today we'll start with the original one, the word to make it turns out that the word to make itself is to a popular representations it has the bag of words continuous bag of words bag of words and then the other one is the called skip gram with actually the people often just call it skip gram but the full technical spec is the paper is a skip gram with negative sampling so now these are rather big words I'll just talk about it in very, very simple words. The continuous bag of words I won't do because today we have limited time and we would like to do one properly. So I wouldn't do continuous bag of words, except give you the basic idea of how to do it. So look at this sentence. Suppose I i could give it is let's take a window size of three arbitrarily and people often take odd size window sizes and you say suppose i fed in it is then input so imagine that you could write a sort of a predictor, right? A classification engine that could say it is, if your input is X1, X2. So imagine that you have a two dimensional feature space, right? It is, well, actually not two dimensional, but it's higher dimensional. But so let me not say whatever the initial dimensionality is, the initial dimensionality is that it is you should be able to predict my you should be saying is my should predict the word why so think of it as y hat or why that you want to predict the label that you want to predict it is my faith right my faith these two should predict the word that do you see that what you're doing is you're sliding a window through the text you first slide it here the next position you put it at is this and the next position you put it at is this and you always take the first two words or you know three words or four words whatever it is as your predictor predictors and the last word as that which is predicted right so you can move forward with that but it has one limitation because of which it is not so much preferred these days it is that it is a directional process what do I mean by that if you look at this suppose I When I went into the meadow, I met a happy cow. Now, think about it this way. Suppose you have a prediction window like this. Suppose this word happy wasn't there. you have a prediction window like this suppose this word happy wasn't there it is I met a cow right if I had not left it like that I would have suppose I had given this problem to you matter what you would probably have guessed in a matter what do you find in a middle oh? Oh, you find a cow. You would have said cow. But if I had given it to you like this, I met, the whole sentence, I met a space cow. Now you know that you can't fill in a cow here. A cow cow would not make sense. you're you're forced to pick some adjective to a cow you can say a big cow a happy cow or whatever it is then you're not you're no more likely to say a cow but you'll say happy right so now there comes an interesting issue that see if you just slide the window forward then your predictions are not as good as when you look at the word beyond the blank that you're trying to predict. In other words, a word on this side matter as much as word on this side. So when you are sensitive to words on both sides of a word, of a particular word, the context word, then you take a slightly different approach you say it is not whether it is on the left or the right or before or after that word i want to say that let's say that we are looking at the word um happy right the word happy co-exists with these words let me take two words after that. Actually, let me take a word which has a lot of neighbors. Let's take this word meadow. Then you can create a window. Let's say that you take a window and I'll take a window that people take in practice. For example, the Gensim library takes a window size of five. Gensim is one of the libraries. Five, so what you would say is the word meadow, you would create a table like this, meadow. Meadow. If you have the word meadow and you have the word the, you did encounter it in the sentence, you give it a label y, think of it as x1, x2, and this is your target your y is one your data if if likewise you can say meadow um i one right again meadow met one so these are the combinations that you did uh actually find right uh in, and so on and so forth. Now you say that suppose let's be clever. Suppose we write a classifier. Right. We, we assign word vectors in such a way that if I ask that in this, suppose you have some form of a word embedding, what is the likelihood that this word and this word, they will be neighbors? Right? So it should say, yes, I can create an embedding in which these two words are near each other and this word is near each other and this word is near met and so forth but then if you try to train that network the network the neural network will be clever it will soon figure out that all it needs to do is give every word exactly the same word vector because then every word is similar to every other word and it will match all your situations right because all of your positive test cases where you're assigning the value zero it will match so to break this what you need to do is whenever you take a word let's say you take a word meadow and you take the word the right or meadow and met let me take something right, and you give it a value one. You then do another thing. You take the word meadow and then you replace it here with a random word from the vocabulary, right? So for example, you can put the word, I don't know, backprop. Now, we know that meadows and backprops are rarely ever mentioned in literature together. We are probably the first ones to do that in this session here. So you would put a value zero. And then you would do it again. You put a few negative samples, meadow and let us say, what can I put I put medicine zero again you would say meadow and let's say you put I don't know suitcase zero so this is called negative sampling negative sampling you have created a lot of negative samples so this particular approach it is called skip gram with negative sampling there are are two hyperparameters. So let's acknowledge the hyperparameters here. One hyperparameter is window size. So how many elements on the left and the right, so suppose there's a word, how many elements on the left and the right, so suppose there's a word, how many elements on the left and the right will you take? So you usually pick the same number on the left and the right. So you would realize that your window size will always be odd. Because if this is the same as this and this guy is sitting in between, you will end up with an odd window size. So your window size is a hyperparameter. A common value that these libraries tend to take is five, but you can play with window sizes. And they have certain implications since we don't have enough time at this moment. We'll do it when we do natural language processing in depth. We'll talk about that. The other hyperparameter is how many negative samples for each positive sample so you need to pick a number, three, five, whatever it is, or three, four, five, whatever, pick your number. How many negative samples are you going to produce? Now that you have produced this negative sample, you realize that the neural network cannot play a clever trick and immediately just give all the vectors the same value because then these two will not be orthogonal to each other meadow and backprop will not be perpendicular to each other isn't it guys do you see that yes so it's common sense so now comes the whole process the mechanics of it and the mechanics is actually extraordinarily easy after this so once again i will just go and say what you do is you take two like actually only logically in your mind you think of it as two so you write all the words you are work right and what was the last word I took? Maybe Zephyr. P-H-Y-R, Zephyr, or something like that. And so each of these words, there are 10K of these, let us say, and I'm taking a small vocabulary. And this is your D prime. Let us say D prime is 300 dimension. What do you do in the beginning? You take, this is your embedding. Think of it as a rubric or your matrix this is your embedding embedding matrix in the high tradition of deep learning initialize it with random values right I pick any random values you want so this would be a lot of this is this is literally embedding weights so you pick some random values everywhere or do the same thing logically speaking with another imagine that you have another matrix I'm just writing it down as a once again exactly like this you put the same works here are what Zephyr this will call it the con the context context matrix contracts matrix context rate so whatever it is that you go and randomly initialize it. So let me write this word down. Randomly go initialize it. Now do this. Now the process is quite simple actually. What you will do is suppose you have the word, this word is there. Let's take the word. What are the meadow has the word met back prop and medicine. So let's take meadow. The word in context is meadow. So let's just say that this is the word meadow. Right? Somewhere in here is the word met. Somewhere up here is the word bath prop. And somewhere in here is the word medicine let's say and somewhere way down below is probably the word suitcase well let me I didn't keep enough space for suitcase so let me say okay so suitcase is here right so now what can I do we play an interesting game if we take the word meadow yeah meadow and you will dot product with with every word that it was associated with so now I can create a dot product with the backdrop, for example. So remember, guys, this is a vector and this is a vector. And I can do the dot product. Remember, I tend to write a vector A dot B that you would write. I often write it as A comma B, just to make it very explicit that I'm taking the dot product or the inner product. So the nature of inner product is if two things are aligned, vectors are close to each other, the inner products would be high, right? If on the other hand, they are completely orthogonal, they are unrelated, then their dot products will tend to be zero. And then you can also have opposites, love, hate, and things like that. So now what you do is now we'll start the game of the learning, the machine learning game. Remember, what is machine learning game the learning part of the game is this this learning part is this what we can do is we go back to our table and let me just reproduce this table here so that we can let me just for a moment shrink the size so I can see it okay Meadow met Meadow backprop Meadow medicine and Meadow suitcase Meadow, Meadow, Meadow, Meadow, Meadow, Meadow, Meadow, Meadow, Meadow, Meadow, Meadow, This is Maddox suitcase. And remember the Y values that we gave is 1, 0, 0, 0. Now what you do is you put this X, this word, and let me just call it X1, X2. You take the, what you do is, you know that you have represented this as an embedding vector even and e2 right for each of these words meadow there is an embedding vector you can go look up this embedding vector here where is the embedding vector for metal, right? This is the embedding vector for medicine. Likewise, this is the embedding vector for meth. This is the embedding vector for suitcase. So now what can we do? We can look at the dot product of these embedding vectors of meadow with everything else, right? Or of every first column versus second column, because it wouldn't just contain meadow. As you move your window around, there would be lots of dot products that will emerge. So what will you get? get you will get if you look at the cosine distance you will get numbers you'll get some numbers right so meadow met maybe let us say that this is 0.7 back prop is 0.01 but this is a medicine is let's say minus 0.3 and meadow suitcase is 0 point i don't know i'm just cooking up numbers 0.5 something like that you get this the word people often use for something like that is this is the the raw energies that you produce but then you do something else you either do sigmoid or you do softmax and what it does is it will amplify the biggest of them so what will happen is maybe this will become as a probability it will make it as a probability it will become a number between zero and one in both these situations so it doesn't matter i'll just write sigmoid here it will become let's say meadow met will be i just said 90 right this will be like uh three percent right and i don't know so so on and so forth small numbers are we together so now you got the this is your y hat right you got your y hat here and now the next thing that you do is here actually I are probabilities. The last thing you do is you find the error here. So what is the error? Error is this minus this, y minus, this is your y hat, right? These are your predictions, y hat. Remember, we always compute error as y minus y hat compute error as y minus y hat in a standard classification problem right and so this begins to look like what 0.3 this begins to look like I don't know 0 minus 0 point minus this or this is 90% so no 0.1 the no, 0.1. The error is 0.1. This here, the error is, what is it? It's pretty big, 3%, 0. I don't know, 970 minus 3%, minus 3%, and so forth right you can get the numbers here you treat these as your errors now what do you do all you need is a loss function if you remember all that we need now is a good loss function and because we are talking about sigmoid or softmax a good loss function would be cross entropy a centipede loss could be perfectly good or any other you can go cook up your last one so guys it's a very long chain of reasoning that we are following I hope you are with me it's very simple reasoning though now we have we have errors in machine learning the moment you have a way to quantify errors you know that essentially the the war is won isn't it because the moment you have a loss in errors you can write a loss function any sensible loss function you can cook up the moment you can cook up now what can you do you can apply gradient descent the standard gradient descent processes and ultimately what can you do you can shift you can change the weights using gradient descent of all these weights here you can change until the next time the next epoch or the next iteration that you do through the data makes the error smaller do you see that and at the end of it what will happen is this matrix this matrix of values would have reached a situation in such a way that the dot product between the vectors of words between words that are proximal to each other or tend to be proximal will be much smaller than the than the dot product between words that tend not to be proximal to each other. Right? And so now you have created a vector space representation that you were searching for that is context aware. Do you follow this reasoning, guys? It's a pretty clever, it's straightforward reasoning. It's pretty clever. And so what you are doing in effect is you're taking a vector if you now turn this argument and see what you're doing you're taking a vector which is a 10k wide and you're narrowing it down to a smaller vector yes yeah to a D prime is equal to just 300 a dimension vector so for every word wi there exists an embedding embedding vector of wi which only belongs to a lower dimensional space right and when you do that this is when you know something most remarkable happens this was first done I believe by Google researchers in 2013 around that they were starting to come out with the word embeddings and what happens is you start seeing relationships which actually are so spectacular that you wonder that you know you hoped for semantic relationship you know the relationships between words to emerge but it completely sort of exceeds in the beginning when it came out it exceeded everybody's wildest dreams of the relationship it was just absolutely fascinating what they found and it was the beginning of a very high quality embeddings for vectors so the state of the art has moved forward today we do even better embeddings we will use transformers to do embeddings bird embeddings and so on and so forth so in many ways the word to wear the glove and all of these are beginning to So in many ways, the word to work the glove and all of these are beginning to, you know, they are becoming very mainstream and they're beginning to look like classical word embeddings now. But still, they're very fascinating. So for example, you notice some interesting relationship. You see, for example, that King minus queen is equal to the difference between these two vectors is the same as difference between man minus woman vector so if you go and look into the embedding space not exactly but approximately equal so that you can say they are practically equal and it's very fascinating to see this relationship because what it implies is you can say that a queen is King minus man this woman vector and in some sort of a very intuitive way it is both delightful and it seems to make sense Isn't it you start seeing other relationships? For example, you notice that you could say things like Us mine USA minus Washington Is equal to let's say India minus X right and so you can say X is equal to well India this Washington DC let's say you see minus USA. And so what you're saying is it's relating a country to its capital. So suppose you did not know the capital of India, you could actually write this equation, go to your embedding space and see what is that word which is closest in value to this. And when you do that, you will find that the word is New Delhi or the praises New Delhi happens to be closest to that and the same thing will work with UK and London and so on and so forth right and that is the that is the magic and the power of word embedding so now what we do is we have now discovered a much more powerful lecture I say a powerful embedding or vector representation of words so word embeddings powerful and semantically aware aware representation vector space representation now this has become a given. You take a word, right, and you first thing you do is you convert the word to its embedding vector and then you work with that. So suppose you have a sentence. Well, you can again do what you did with bag of words. You can put the sentences, just adding up all the embedding vectors or concatenating the embedding vectors. So one of these two, you can go ahead and do that. You can do the same thing with documents. Are we getting that? And that becomes the input to your classifiers, your regressors, your clusters, and so forth. So, and these things are real with these things. Now, for example, we can do sentiment analysis. We can do something called language model. Language model is predicting the next word. So for example, have you noticed that when you're typing on your cell phone on your smartphones, you type a few words and it pretty much keeps guessing the words that will come afterwards that you're likely to type. That is again, that is called a language model. And you can very clearly see where it is coming from. The roots of it starts with a good word embedding because word embedding is exactly that is the relationship between words It can tell which is the next most likely word you like because that is exactly how Your embedding was trained in the first place So guys today we covered a lot of territory. We have 10 minutes left before it is 10 o'clock. I would like to review what we learned. In the first half of this today we learned that we need a vector space representation for language, for words, sentences, documents, paragraphs and so forth. We started with something very simple, one hot encoding of words, and gradually we started refining and getting somewhere. Then we realized sentences could be written as word concatenations, word vector concatenations, word vector summations. We realized that we can do better. We can do TF-IDF vectors. Now remember, when we do TF-IDF, it is a word. It is about a word in a document. So there is no such thing as a globally correct value of TF-IDF for a word. So you take a word, let's say the word horse. It will have a different TF IDF in one document versus another document. So TF IDF is document aware. So what TF IDF gives you is that when you look at it as a vector, a document becomes a vector right where each of the word placeholders contains the tf idf of that word with respect to this document and so every document therefore becomes an input vector to whatever machine learning algorithm you want to feed it to right so that was good now one thing to note is that that is not an independent representation for Word. People started looking for document agnostic, a word embedding that had special qualities. It was a lower dimensional matrix, lower dimensional representation. It was context aware. It was semantics aware. And so came this whole breakthrough of word embeddings the first one was word to work which has two representations the continuous bag of words which is sliding window forward or the other one which is skip gram with negative sampling so I taught you skip gram in negative sampling now when you really think about it it's quite easy Once you start representing words like this, you say, in the meadow, I met a cow. Meadow met is together, but meadow backprop doesn't happen. Meadow medicine is not there. Medicine suitcase. So you just populate your table, your data set. You create a data set in which the positive cases you get from your sliding window right but your negative cases you get by taking the word and putting adjacent to it just a random word from the corpus or the vocabulary sorry from the vocabulary and assigning a value Y of 0 so now you have a perfect data set you're basically saying what is the probability of these two words being together one the probability of these two words being together zero that's a that is the ground truth and now you you go and say that if these words for vectors in this sense embedding vectors i want this embedding vector weights to be such that things which whose joint probability is one the dot product should be one and the probability of I mean not the product but dot product then converted into a probability relative probability it should be one whereas words that don't go together that were not evidently together their dot product and their probability of joint happening should, according to this vectors representation, embedding vector representation, should be very low, zero. And it works out. The beautiful thing is that you are able to create the error and see once you can systematically quantify the error, once you take the dot product from there you get the sigmoid or softmax convert into probability and then you see the difference in probability y minus y hat and so on and so forth but then you know the battle is one you already have this you're trying to minimize there are many ways to do that cross entropy will say y log y hat summed over negative y summed over all the instances. How do you minimize this? So you have the y, you have the y hat. Y hat is this column and this column. And so if this was only logically speaking that you're trying to minimize the error, the cross entropy function will be different, slightly different, y log y hat and so forth. But the bottom line is that you will be able to Rajat Mittal- Write a coherent loss function and that loves the moment you can write a coherent loss function. Rajat Mittal- You can do gradient descent. We are familiar with gradient descent and the moment you can do gradient descent. Therefore, what it does is iteration, as you step through iteration after iteration, it goes and fixes your matrix, you know, that embedding weight matrix. For all the words, they continuously get updated, right? And so after a few epochs of training, you have pretty much found the embeddings for all the words in the corpus. have you have pretty much found the embeddings for all the words in the corpus and those embeddings now are a much richer representation of words first of all their document agnostic you have created a representation that has been derived from the entire corpus of all documents so the richer the information the more likely you are to find true relationships within words. So that is that. And these vectors, these vector representations, therefore make the bulk of natural language processing. It reformulates the world of natural language processing back into the machine learning world of classification regression clustering topic modeling and so on and so forth right and we will not get time to go into all these details today as we know this was the fundamentals workshop series here my goal is to as quickly possible, introduce you to all the key architectures in deep learning. We had six weeks. So this week we learned about the architect, at least got started with the architectures for natural language processing. Tomorrow, we'll talk about something called recurrent neural network. neural network. In that family, you have the simple recurrent network and then you have the little bit more sophisticated sort of a downtown brothers of them, cousins of them called LSTM and GRU. These are neural architectures within the RNN family. We'll learn a little bit about them. These are autooregressive models, which means that like time series, like a stock market. The value today or the weather, it depends upon the prior values that you can use to predict it. The words are like that, languages like that is sort of autoregressive. The word that will come next depends upon all the words that have preceded it. So RNN was the first autoregressive model architecture, neural architecture. Its refinements are LSTM and GRU. We'll talk a little bit about it, but I won't talk too much about it. The reason is I need to cover a much more important topic, which is quite dominant today in NLP. In fact, it's become synonymous with NLP. It is transformers. Today, when we look at natural language processing, it is dominated by two key ideas, word embeddings and transformers. It used to be that the field used to be dominated by recurrent neural networks. So, but now the recurrent neural networks are still used but they are not dominant. Transformers are dominant. And before I close that and we go to tomorrow, I'd like to show you some resources on our course page and so that it gives you some material to read about and for those of you who have time I would encourage you to go and listen to a couple of talks we have so let's go here and you will find that NLP's after the loss. Remember we have done the topics before that, loss landscapes and so forth. So NLP is a world in itself. These times, the universities are now a very seriously considering having, see, as you know, these days, there is a master's in data science, right? It has branched off from computer science as a speciality of its own. But there's even further specialization. People are now beginning to give, some universities are beginning to give a master's in just natural language processing. So we obviously couldn't possibly have covered it in one evening. It is worthy of two years of study, a whole master's. It's a big field now and a lot of activity is happening in this field so to get you started I put all these resources that you see hang on let me increase the font size are you guys able to see that so there is an old classic book which is being rewritten I would highly recommend that you know you should have one book which is sort of conceptually or theoretically rigorous. Just like for deep learning, we have the deep learning book by Ian Goodfellow and all this, the big guys. In the same way, one great book which is being rewritten in the third version is this book. It is still in the online stage. It is online and it's being written, I would strongly encourage you to go and start reading chapters. If you are really serious about NLP, get a head start. We will be using it in the month when we do NLP very seriously. We'll do that. Then there are some other people who have written some NLP books and NLP overview not of the same high quality but still pretty good here is some here so this is an effort that is in progress that that goes into and you can review the topics that we covered these are also being talked about in this book your deep learning book what we talked about there is there are a few books which are more accessible so one book that is very easy to read and accessible is this natural language processing in action I found this book to be like a very easy read actually you you finish the entire book over a weekend at most so you can consider getting this book especially if you know neural networks and so forth after doing this workshop you'll realize that you can pretty much skim through the first half of the book practically pretty quickly knowing PyTorch and there's some other books and I've mentioned those books here now one in this world so then there are some articles now I put some curated list people have created some awesome that is somehow the word awesome seems to be there associated with it there is an awesome NLP. It's a curated list of lots of NLP resources. I would highly encourage that you bookmark it, these ones. And it's a wealth of information that you can get here. Then you have awesome deep learning for natural language processing. yet another curated list. Now these people that I talked about who are writing this book, they also have a YouTube channel. These are Stanford guys, by the way, Stanford . So they also offer the Stanford course. So this is it, Natural Language Processing with Dan Jerofsky and Chris Manning. I would suggest that you can go watch these videos or certainly use it as a reference. Then besides that, there are some excellent podcasts, really good podcasts. Usually I don't think I've mentioned podcasts on any other topic but here are some very good podcast that keep coming up it just shows how hard this field is then there is one website that i must point out and say is very good is sebastian judas website on nlp excellent site this is a site called sebastian jud that and he has a lot of things on various topics on many things but certainly on NLP he has it then you have besides that there is a like if you want to follow on Twitter what is happening on NLP in NLP this is the Twitter sort of a channel for NLP ears continuously something or the other keeps coming up so you know there was there's a wealth of information available out there another curated list for NLP is this this is also a very good list And when you look at this long list and the number of libraries that people have created, it's just absolutely amazing. It seems that you have entered a city that you didn't know of, and you're walking down the main street of a city that is bustling with life and creativity. That's the sort of sense you get. Now NLP has many frameworks which are quite mature. The typical starting point people give is NLPK. I mentioned to you guys that NLPK is written in Python itself. It tends to be slow. There is a tendency everywhere to give tutorials in NLPK for a reason. It's very easy to get started, but it's big. Even simpler version is TextBlog. Remember in the first week I started you out with TextBlog. It's fairly good, but then more industrial strength libraries. So if you use NLTK, what happens is, for example, it doesn't have semantic aspects to it, topic modeling and so forth. Then you'll have to use Genseng along with it, which is a bit of an issue. You would like to stay within the scope of one library rather than hop around. So that's that. So for high performance, spaCy is very good. And Allen NLP, which I like actually is is very interesting because it is based on PyTorch and Spacey putting it together then hugging faces is about transformers hugging faces the sort of the Mecca of all transformer based stuff we'll talk about that tomorrow so that is there now Stanford's core NLP is very prestigious and very, very good. Written in Java, very powerful. There is a Python dialect of it also now. Then PyText is PyTorch and NLP put together. Now one thing that is not mentioned so often is Baidu. Baidu is actually perhaps the leader, if there's any one person, one company that is ahead of even Google, Microsoft and Facebook, I would say it is Baidu. It's a Chinese company, but they're doing very, very well. Andrew Ng used to head the AI lab at Baidu for some time after he left leading the AI lab at Google.u for some time. After he left leading the AI lab at Google, he went to Baidu and left it. Now, of course, he's fully devoted to his Coursera stuff. So Baidu's earning, again, a great site to go do that. Now, there are a couple of things that I'll give you as reading. Today, I'll say for NLP, the word embedding is important enough that it merits a section of its own and they're very interesting embeddings for example if you have structure hierarchical structures and so forth you can use embeddings that are specific to that now here I've given an embedding called the word to wake the illustrated word to wake please do take out time and review this. So the reading material I'm giving you is read word to wake in your books. But more than that, go and read this blog. This is the review that you should do as reading material. alamar has given an excellent excellent uh explanation and very well quoted explanation of word embeddings in particular here he has talked about word to whack so do please go and play with these resources tomorrow will be a little bit of recurring neural networks mostly to beat up upon it and show its limitations and then it will in other words be a straw man and then we will move on to the transformers and we will have a bit of a session on transformers now while i do that if i may and obviously this moment, there's a few who have to drop off, but I would like to show you some things about the power of NLP. So let's go to NLP and let's see some demos. These are real utilization of this. So for example, this says reading comprehension, right? There is a particular model. These are all transformer, transformer et cetera based models. Let's take an example. You can enter your own passage and you can ask it questions. So for example, they give this example. Here is a text that has been entered. And now to this text text you ask this question how many partially reusable launch systems were developed and that the NLP has to read this text and from that figure out the answer and the amazing thing is that when you run it it will immediately come out with two. And it just, you notice that it sees, it highlights the word two. Shows the power of where NLP is. And in fact, the power of transformers, et cetera, is. And if you ask it to interpret the model, it will give you the saliency of it. Like, where is it paying attention? Right? What are the words that it used to decide? These are the words that it used to decide? These are the words that it used to decide. So it can literally give you two partial reusable. It literally latched onto those words. And so on and so forth. And this speaks to the interpretability of the models. Very, very powerful. And you can literally see how far things have gone right you can take another example suppose suppose you give it a picture and you ask this question what game are they playing so see the interesting part right this will ultimately come up with it will run for some time as you can imagine this is computationally a little bit interesting and it says that with hundred person score it is most likely baseball next is likely to be softball now if you know these games you probably would agree that that is what it is and with lesser probability this and you can of course upload your own image and do that but this too is natural language processing right another is named entity resolution proper nouns right and so forth so let's take an example this is an example and you run it and then it finds out what are the proper nouns. Grandpa's Joe and deep learning. Right? Do you see? And not always perfect. It came out with deep learning as a location, which probably isn't deep learning as a subject, but Grandpa Joe's is an organization. Pretty good thing here. You could take something else. You could do it. See, it says at Allen NLP and PyTorch are organizations. Allen Institute for Artificial Intelligence is an organization. Seattle is the location. So this one, it got quite right and you can play with that then you can take other things for example let's take let's run this and then it comes up with a statement published it so I publish the theory of relativity in this so it just finds out the pieces of the language the temporal temporal modifier and the parts of things and so forth. Sentiment analysis is almost both loved and hated these days. It is loved because where it works, it works just very well. Where it doesn't work, like for example, it doesn't quite understand sarcasm. A lot of effort is going into creating Ability to be sensitive to sarcasm and irony in sentences, but that is the frontier. And you can see what is it doing that you know you can ask it to interpret the model, a very well made entertaining picture. So it comes out with a positive sentiment because it finds the word entertaining there then dependency parsing it shows you the relationship between the parts and this of course we when we do the natural language processing in depth i will explain to you where all of these things are semantic role labeling i'm just giving you an idea, not necessarily explaining everything. So let's run that. And again, it's showing the semantic relationship. The key which is needed are related to modifiers to access the building. Then core reference resolution. You find all expressions that refer to the same thing. Right? So let's take one. You can see all the blues are referring to similar things and so forth. Right? So I won't and quickly sort of running to protection entailment decompose so it tells you whether one thing follows from the other or not and tell me that is it right so this is it. Right? So this is it. It is very likely that the premise entails the hypothesis. Right? Here is the premise, here is the hypothesis, that they are related. If you help the needy, God will reward you. Giving money to the poor has good consequences. So you see that the two, there is a relationship between the two. Right? It finds finds out language modeling is of course completing the sentences the sentence is this allen nlp is right prediction the so you can say is uh great now let's see great example example, great tool, right? A great resource, program. If I say library, immediately it says for. Let's go with the word for and see. For the, and so on and so forth. After a little while things can get a bit lost. So that is that mass language model. And you can contribute. So Allen NLP, this was a good demo I thought I would show you. So NLP is a very fast-growing field, guys. What are people able to do? Let me give you an example, two, three examples of the cutting edge of that. When people write legal documents, different parts of the documents are cut and paste from different places. Nobody these days is able to write a 400 page legal contract just from scratch overnight. But lawyers habitually produce or regularly produces documents overnight. The way they do it is they take pieces from other documents, copy paste and do it. The trouble with doing that is one part of the document may weaken another part of the document. The legal language may contradict each other and open loopholes for the violation of contracts and so forth. People are using NLP to find such areas. It is able to do that. One of the interesting frontiers of natural language processing is people do visualization data visualization. Data visualization is a tedious thing you take a group of people who sit and create those visualizations for you or you sit and you write matplotlib and so on and so forth. And write code to create data visualization. All of you as data scientists have had experience with that. So how would you feel if I told you that there is already significant progress that you can give natural language commands, to say, give me a visualization of this from the data, right? And you give it the data, it will understand the data, and it will create the visualizations that you are looking for. And those visualizations are real. They are not just simple bar charts and so forth. They will create an entire dashboard. They will write your D3JS, your Vega code and so on and so forth. Everything they'll do it. And today we have natural language processing systems that can do it and as you probably are aware gpt3 which is one of the largest transformer models in fact the largest transformer model is so insanely smart you say i want to create a user interface which contains the ability to do this input this and do that and right away it starts writing react code for you. Those of you who know UI framework know React. It will immediately start putting React, Bootstrap and all of that. And before you know it, it has written fairly credible and good user interface for you and also bug free. So that just shows you the power of natural language processing and how far the field has gone. So having said that, on Wednesday we will take some simpler examples because I want to leave you guys with some homework that will help you practice some of these things, sentiment analysis and entity resolutions and so on and so forth. A little taste of everything you get to do in your homework and in the guided lab however when we do the one month of nlp then the six weeks of it then we are going to do it in a much more intense way everything here is just introducing you to the fundamentals we are going to do a lot of in-depth practical examples. So wait a minute. By the way, that also raises another question. But before that, let me stop the recording. The theory part of the session is over. Thank you.