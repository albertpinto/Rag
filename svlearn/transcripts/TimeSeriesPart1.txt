 So, I will go into the presentation mode. So, what are we going to cover today? Everyone's seeing my screen, right? Yes. seeing my screen right yes okay great so we will what what I'm going to do is walk through a few approaches used which will start with the classical concepts so what exactly is a time series I think all of you know what a time series is let me just write it for the sake of let's go to a whiteboard and let me. So this is our typical problem that we are trying to solve. We have. Exit time one, exit time to. Dot exit time T and our goal is to predict what is going to come next so this is the typical the time series is these are all at equal intervals of time so you have time one two three they're all spread out at equal intervals of time and the goal is to really forecast the future sometimes it is also to fill in the holes but But most of the time, when we do time series analysis, the goal is really to forecast for the future. Okay. So this is the goal in all the time series problems. So in this situation, the data date is used to forecast into future there are two main approaches used there is a time domain approach and a frequency domain approach so in frequency domain what we are actually up i will not cover too much on the frequency domain today but i will cover a little bit on frequency domain so that there is a completeness. So there is a time domain and frequency domain. I will cover the frequency domain very briefly, but most of the session will focus on the analysis in time domain, because most of the deep learning based approaches also, they use the time domain approach. And within the time domain approach itself, there are different kinds of techniques. One is the smoothing or averaging approach and then there is the univariate models these are the classical models which use auto regression moving average and then follow up with what is called arima which is basically integrating auto regression moving average as well as an integration in between and s arima stands for really the seasonal arima which is integrated with also seasonality seasonality means having periodicity in the series so whenever there is a like for example something is having an annual cycle or a monthly cycle or a weekly cycle you know that there is a seasonality so that is brought into the whole model and then finally we will cover on the deep learning based approaches so this is like the table of contents so let me uh before i go into a lot of details i'll probably cover the frequency domain to begin with so that we have a understanding of what frequency domain means before i go into that more time series based approaches so let me again go to the whiteboard in case i'm going too fast or too slow please interrupt me and of course interrupt me with questions because we are a small class today we are just having around 10 or 11 folks feel free to interrupt me and i don't think there is any issue with that so when you say yeah sorry this is balaji i just wanted to ask you would it be possible to stream on the youtube but the same is that possible we just tried it see i do not have access to it um in fact uh asif was suggesting me not to do it and when we just tried it i did not have access because uh i i am not signed in okay so he he suggested that i just do the recording and he will take care of the youtube part later got it sure so it will be up tomorrow or sometime right yes yes okay got it thanks sir thank you okay fine i'm anyway the recording is in progress so in that frequency domain so let me just so frequency domain problem what we are trying to achieve is we have this let us say we have this x1 x2 dot dot dot up to x n minus 1 we have let us say we have n and n n times okay so so if we try to draw a graph so we we have 1, 0, 1, usually it starts with x0, sorry, because only then we have n terms. So 0, 1, 2, up to n minus 1. So when we are trying to analyze the same problem in frequency domain, we make an assumption that it is actually periodic with time n which means what let us say it has some random behavior we say that whatever is between 0 and n minus 1 starts repeating from n i mean it's some kind of circular dependency you can say so it's again another so this is it's basically a repeat of whatever you had here you again have here so this is this is the assumption made whenever you have let us say n entries in a time domain and you want to convert it into the frequency domain you create it into a periodic form where you have whatever is from n to 2n, from 2n to 3n, and so on. So it becomes a periodic waveform with the period being equal to n. Of course, there may be more periodicity. It may have been periodic within itself, but that is what we are trying to analyze. In the frequency domain problem, what we are trying to analyze is how much is is uh let me move it up a bit uh so chander so time series means only like we are predicting only a number a forecast like it's not a classification problem uh it is not a classification problem it is only a forecast problem okay so uh what i had here is uh these are this is all in time domain this is still in time one to n minus one and then you have xn you repeat whatever xn is equal to i always miss the zero it is equal to x of zero and then dot dot dot, dot. Okay, so now in the frequency domain, you are trying to replace this because we know that this is periodic. We know that there is, if something is of this form, E power, sorry, with a you know that when n n reaches capital n when if n reaches n it's all the way going from zero up to capital n now you know that if n reaches capital n if n by n becomes one and we know that e power i 2 pi is one so there is a we rewrite this whole in frequency domain if you have this x of n you can rewrite it as k equal to zero to n minus one let me explain i'll first write it and then i will explain why this is true is it clear what i have written so what i have written is at the same time series x of zero i'm rewriting it in terms of another function x tilde of k where k is really like the frequency which goes from zero to n minus one with a e power i two pi n by n k so what what this ends up doing is if k is equal to uh k could be anything from 0 to n minus 1 we know that whenever n is equal to capital n all of these are going to be equal to this term becomes 1 which is same as e power i 2 pi of 0 so this this function is periodic with n is equal to uh i mean the period the period hey chander you're saying i2 pi what can you explain the formula a little bit if you don't mind yeah like sure sure okay let me i think um i went a bit fast here because the first part is not very clear to me like when you say i equal to and then okay okay let me try to explain in a better way so here you have something from zero to n minus one and i'm saying that it is periodic with period n you you realize that we had this waveform which was some random waveform but that we deliberately made it periodic with we repeated it after the time interval of n we repeated the whole thing correct yep so whenever we repeat something we know that it is having a periodicity of at least with this as the time period it may have a time period which is smaller but we know that it is going to be periodic with n that means there are you can try to rewrite it as many functions where one function is a dc function dc function means which is constant okay then another is so let me go a bit up up here so um here this there are there is a dc function which is completely constant and then there can be a periodic function which is having exactly This is the sinusoid with period N. Okay. Now we could have had one more, which is a sinusoid, which is having a period of n by 2 n by 2 and then we can go on like this so what we are trying to say is if something is periodic with period equal to n it means it is composed of many functions which are either periodic with which are either a dc function or something which is periodic with n something which is periodic with n by 2 something which is periodic with n by 4 because all of them are periodic with with n got it right so you could have either had something which is having period of n by 3 n by 4 and so on so all of them because you can you can end up having all of them will end up having periodicity with n right because if something is periodic with n by 3 it means you are going to have three of those waveforms which are going to end up have three of those waveforms which are going to end up completing before you reach n is equal to n if you have n by 4 as a time period it means you will have four waveforms before you reach n equal to n so the most basic being the dc so if you see you can have either a dc or you can have a dc or you can have a um so so this is so basically any random waveform can be decomposed into either a constant waveform or a periodic waveform with period equal to n a periodic waveform with period equal to n by 2 a periodic waveform with period equal to n by 3 and so on so in other words you can rewrite your x of t which is your time series as so this is this is the notation used x tilde of k e to the power of i 2 pi n by n k where k could have been anything from 0 to n minus 1. now why is the same as what i just explained so because this is this is the most general periodic waveform with time period equal to if you see the time period of this its time period is uh uh is n by K this is going to be the time period for this waveform why is that because when n equals n by K this becomes one right if n is equal to n by k this becomes one right if n is equal to n by k this becomes n by k by n by k and it becomes one and e power 2 pi is one right e power 0 is also one so that means whatever was the value at 0 repeats at n by k n equal to 0 and n equal to n by k are the same value because e power 2 pi of 0 and e power you know that right e power 2 pi of 0 is 1 e power i 2 pi as well as 1 because that is what the in complex numbers you know that they go around in a circle and then they come back to the same value so this is the e power i 2 pi when the angle is 2 pi it becomes 1 again so the fact that the most general periodic waveform can be written as a sum of many different waveforms each having different time periods all the way from n equal to you can say n equal to when k is equal to 0 n by 0 is infinity that really means it's a it's a flat waveform because if k is equal to 0 this is always constant there is no um it's it's a dc waveform when k is equal to 1 it is having time period 1 k is equal to 2 it is having time period 1. k is equal to 2, it is having time period n by 2. k equal to 3, time period n by 3. So this is the way in which we rewrite the same time series in terms of the frequency domain. So what happens is if you have a random waveform, and if you- I basically like- I is the square root of minus one so this is the okay sorry i i assume that most people know this notation but what what invariably what we do in this time series analysis is we rewrite in terms of a fourier components so what i showed above was a sine sine waveform if you see sine of x has this waveform where it starts and then goes down and then comes back so this is sine of zero sine of pi and sine of two pi similarly we have another waveform which is cos of x which is very similar but it starts at 1 it goes down to 0 at pi by 2 becomes minus 1 so this is minus 1 and then it goes up and then it comes back so sin x has this waveform cos x has this waveform. And in all periodic functions are basically linear combinations of these two. They can be either A times sine of X plus B times cos of X. most general waveform with period of 2 pi this is going to be it now if you are looking at a general waveform with period of not 2 pi but period of pi okay then it would have been a sine of 2x plus b cos of 2x if it was for pi and similarly you can keep going to the thing is if something is periodic with period equal to pi you know that it is also going to be periodic with period equal to 2 pi which is what i just mentioned because something is periodic with 2 pi and something else is periodic with pi which means it is like this you know that it is also periodic with 2 pi because it is going to repeat at pi and therefore at 2 pi likewise something is periodic with period pi by 3 it means it is going to be periodic with period equal to 2 pi by 3 as well as sorry 2 2 pi by 3 means 2 pi by 4 pi by 3 and 2 pi again so you can keep dividing by integers and all of them become periodic with 2 pi as well now the sine x and cos x are the real ways i mean they are the real functions which enable you to write a most general periodic waveform but it turns out that there is a more neater way of representing the same thing e power i theta is can be written as cos theta plus i sine theta in case you you are not aware of this formula it is okay i mean this is basically the you can think of it as more like a definition but if you want to know some more details there is there is basically a power series expansion for any exponential so if you write e power x it can be written as e 1 plus x plus x squared by 2 factorial plus dot dot dot you can so if you if you write this e power i theta in this power series expansion it turns out that it can be collected into terms which are of the form cos theta and then an i times a sine theta so if you want you can think of this as a derivation of this formula but it turns out that instead of trying to deal with cos and sine separately it is better to deal with a e power i theta which is why i when i went here i directly talked about e power i 2 pi n by n k because in the back of my mind i have sine and cos because those are the two basic periodic functions for any particular period so in general if you write e power i 2 pi it covers both sine and cos because if you see that it has a a sine and a b cos right and together both of them are covered in the e pi theta because you can have a real part and an imaginary part and they separately cover the uh two independent periodic functions so forgetting about all the details of what i said the fact that any time series can be rewritten as something like x tilde of k okay times e power i 2 pi n by n times k where k varies from 0 to n minus 1. now what is the reason why k only has a finite i mean i i told that um why does k have to be only up to n minus 1 and not go beyond it the reason for that is actually quite simple because if you go back to the time series um if you see uh if k is equal to capital n then this becomes a wave function which is something like this okay this this becomes a wave like this where it is actually every interval so this is 0 and this is 1 and this is 2 now there is really no difference if you see this there is no difference between this and the dc waveform because it means it is constant at all these points because we are sampling x0 x1 x2 and so on up to x sine minus 1 now the fact that if if k becomes equal to n even though there is a periodicity here it is not apparent when you are actually doing the measurements because you are only doing measurements at 0 1 2 at all those places it is going to be the same so you can actually substitute and see let us say you put k equal to n into this you get 2 e power i 2 pi n by n into n which means e power i 2 pi n and for all values of n e power i 2 pi n is 1 which means what which means that it is a constant which means you have actually come back to the DC function, which is why you have a upper limit on this K. You have a lower limit of zero, you have a upper limit of N. Now, this is the frequency domain description. Why is this important? Now, many times what happens is when you have a random waveform and you have something at X equal to one through three and all that it really helps to do this if there are there are packages there are package we'll actually do this experiment in the lab we will have this a notebook where we are doing the fourier transform so there are packages to compute this x tilde of k from x of k x of t and you will realize that in fact there is a better way to write this instead of writing it as k equal to 0 to n minus 1 x tilde of k there is a better way which is written in the packages use this they write it as k equal to minus n by 2 to plus n by 2 i mean leave aside the there is a minus 1 or whatever so this this is how normally it is written so if you if you are rewriting it in the frequency domain in terms of k so k is the frequency domain there is a there is a minus half to plus half where we are normalizing with the n so this this this really becomes k by n then so in the k by n space um it turns out that you have an x tilde of k which means you have a lot of values for um at different so it goes all the way from um minus half to plus half because if you see k by if k is going from minus n by t to n by 2 k by n goes from minus half to plus half so by studying the pattern of this we can actually tell which is the frequency which is having maximum component and so sometimes by looking at the how the spread in the frequency domain is we can make predictions on how the waveform is going to look in future how is that because let us say there was only one particular frequency in this which was dominant. Let us say most of those were noisy terms. So we know that this is the one which really matters. So this has the highest amplitude. And in some sense, it is like doing the PCA, principal component analysis, where most of the other frequencies are neglected. You use this, and when you're using this as the frequency representation then it means in this x of t the x approximate of t is just going to be maybe one or two frequencies let us say there is a k equal to some k by n is equal to 0.2 and k by n equal to 0.4 these are the two values which really matter so in if if we can rewrite it like that then x tilde i 2 pi n times 0.2 because k by n is 0.2 plus x tilde of k corresponding to 0.3 e power i 2 pi 0.3 so i don't know if i let me probably rewrite it in a better way so let let let us say that it turns out that in the frequency domain, only k by n equal to 0.2 and k by n equal to 0.3 were significant. What I mean by they're significant, when we run this package, computes the fourier transforms it turns out that the main amplitudes were there only at these two places k by n equal to 0.1 0.4 this helps in generating an approximate distribution x approximation of t which is having the essential components of the x of t which will be x tilde corresponding to k equal to 0.2 e power i 2 pi n times 0.2 plus x tilde of 0.4 e power i 2 pi n times 0.4 so this is a very uh this is useful because then we can we know we know what these values are we know what this is this means we can predict what uh it is going to be for any value of n because we have already we have a waveform an approximate waveform for x of t in terms of some parameters n so we can predict we can therefore predict predict x of t for random n including the n in future right so this is the in some, this is how the frequency domain approach works. So the beauty is that we are, there is, it is a non-parametric approach. We, we do not have any parameters. We are just making some basic assumptions that if you have values of X of T from between a zero to a range of n minus one we just make this assumption that whatever are the values now they repeat and using that we create a model the model being the frequency domain approach and in the frequency domain we take the most significant terms and therefore we predict the value of x of t for arbitrary t okay so any questions so far shall or i will move on to the rest of the domain approach um i have a question so i understood that the k in the formula is the the sampling rate where we collect the data uh and capital n is the periodicity where the pattern repeats i did not understand the small n uh no no uh small n is the actually the okay let us say we have the sampling is happening at an interval of time t okay this means your actual time t is equal to n times t okay this being the sampling period okay if this if t is the sampling period which means you are having x at zero Exit 1 is actually exit time t. Exit 2 is actually exit time 2t. And dot, dot, dot. So exit n really means exit time nt. So this is, so that the correspondence between t and n is T is equal to NT. Now K is the frequency. K is not the sampling rate. K is the frequency. So K corresponds to the frequency at which, so if you are doing a frequency decomposition of the signal, K corresponds to the frequency terms which are significant. So the exact equivalence between the frequency and k is again just like t is equal to n t k is actually equal to k is equal to f by t i think let me just confirm if this is true by t. Let me just confirm if this is true. By t. Yeah. So if k is equal to 0, it implies frequency is equal to zero okay if k is equal to one implies frequency is equal to actually it should have been nt sorry frequency is equal to one over nt okay if k is equal to 1 over NT okay if K is equal to 2 it implies frequency is equal to frequency is equal to 2 over NT right because then 2 over NT by NT becomes 2 so so this is the correspondence so k is equal to n minus 1 implies so f is equal to n minus 1 by n t so this is the maximum possible frequency if k is equal to n it implies f is equal to n by n t which is 1 over t but then we already saw that if frequency is 1 over T it is equivalent to not having any frequency because if you are having if you are if you are having a frequency of 1 over T it means that you are repeating after every interval of time which is same as saying it is a DC frequency so this is same as this is it clear what I am explaining here yes yes how do you know which components are significant no no that is what our frequency when we do this frequent Fourier transform we don't know we when we do the Fourier transform we see that some frequencies turn out to be significant some frequencies turn out to be not significant I mean that is that is the problem I mean it may turn out that there are all frequencies all these frequencies are equally important it may turn out but in most of the problems when we are modeling a time series it turns out that there are certain periodicities like for example let us say you're modeling a problem in like temperature at a particular city you know that it has a annual cycle so there is a periodicity with period one year right right? So, and the fact that it has a periodicity with period one year means it also has a periodicity with period of two years, three years and four, so on. I mean, whatever it is, sorry, one half an year, one fourth an year and all that, because all of them, so if you are having your temperature is equal to let us say you model the temperature across let us say you have 0 1 2 and you're all the way up to 365. you know that after 365, when you come back, very likely you're gonna have the very same temperature. Right? So the fact that you have a temperature which is repeating means that you can model it as a summation of a DC term which is fixed because that may be something which is dependent only on that location plus something which is having exactly a periodicity of one year this is plus something which is having periodicity of which is because they again they are also periodic something which is having actually i missed the half this is the period of y equal to one let us say something with periodicity of y by two okay and then y by three and so on so you have all of them because all of them are again going to come back to the same value at the end of a year so you are just saying that your whatever is your actual temperature is a sum of many of these waveforms either it's a dc waveform a periodicity of uh with a period equal to one a periodicity with period equal to y by two um i'm not very good at drawing this so basically you got what i'm saying yeah no like i i guess the intuition but yes computationally how do we say you know the signal is noise you know like uh is there like a quota that we have to make right so when we do this fourier transform there is a package there are mathematical packages which we can do to compute the fourier transform i mean we can calculate it using integration and all that. There is a way to calculate all these coefficients, X tilde of K. There are ways to calculate it by doing an integration. But we don't have to worry about it. There are packages which take care of calculating this X tilde of K. So let us say it turns out that when you compute this x tilde of k you end up having something which is like this and usually it has to be symmetric why it has to be symmetric is because these are all real i, we are always dealing with real values for X of T. So when we deal with real values for X of T, in the K domain, because we are going from minus half to plus half in the K by N domain rather, it has to be a periodic, it has to be symmetric. Because if it is not symmetric, it means you will end up having a an imaginary term because when you are trying to add up all these terms unless you have equal value for plus because you remember we have an e power i 2 pi n times k by n right so unless you have a corresponding term for the plus and the minus there won't be a real waveform coming out so because of this reason let us let us assume that your actual frequency domain turned out to be this this is coming from some computation either through integration which you do by hand or through a mathematical package now by looking at this you know that the significant values are here here here and here so so by just by the fact that the amplitude is much higher in these places so it's it's just it is not that it it it's not guaranteed that this is going to happen it may happen that your actual signal is uniform in k k space okay of course in that that case you don't have a nice way to describe it in the frequency domain but but if it is turning out that there is a frequency which is significant and another frequency which is i mean there are some frequencies which are significant and others are not then the frequency domain representation helps in describing the way from well. Okay. Did I make it clear? Yeah. I think what will also help is, I don't know if you have a background in terms of like a practical example where we would look at this, like when I'm thinking through this intuitively, I'm kind of thinking in terms of, let's say, things which have circular motion, like a turbine and so on, in terms of being able to predict something in terms of the noise that's coming from there. But do you have an example that you can give from a more commercial context where the periodicity matters? And then I'm also looking at, there has to be the example. No, the example that I am thinking is more like temperature, which is an annual cycle rate, that may be a good example. I because there we know that it has a periodicity with period equal to 365 right yeah that's okay that's the seasonality part of it but i was trying to get a business context business context you see most of the time series let us say in a typical scenario you are trying to forecast revenues so when you are trying to forecast revenues there are there are firstly there are there is a trend maybe a gradual growth happening on in the company in the in terms of trends but then there is also every you know that the q1 may be always a little low compared to q2 so there may be a periodicity in the quarters so where the it may in general happen that q1 is always the lowest and q3 may be the highest for example I am not sure I mean in terms of revenues so there is a periodicity which is again following an annual cycle where the Q1 revenue is the least and Q2 is probably a little higher and Q3 is the highest and then again Q4 becomes low and q1 is the lowest for example so that means there is a trend which is probably following this where it is from the low it is coming back and then going to the highest so here's the thing that is that's running through my mind as i'm listening to this um in terms of this whole idea right yeah if you look at it from a business perspective there will be the need to be able to separate out the non-cyclical components from the cyclical components yes and then be able to figure out for the cyclical components what are those two or three dominant frequencies so the frequency domain helps in doing all of it. That's what I'm trying to say in the frequency domain. We have a DC part, which is like a steady fixed. And then, okay. The fact that there is a trend. Yeah, I agree with you. So we need to, we need to do. Yeah. In fact, I will come to that a bit later. What you're saying is there is a trend which we need to we need to do yeah in fact i will come to that a bit later what you're saying is there is a trend which we need to probably separate out correct that's what you are trying to say which is absolutely correct in fact one of the points of my discussion next is to convert i mean most of the time series problems in fact there is one key uh observation is that our goal is to determine so let me write it here determine an equivalent time series or I won't say equal and a derived time series which is stationary so in this case let us say you have something like X 1 X 2 and dot dot dot up to X of T let us say they follow a steady trend where you know that X 2 is going to be greater than X1 by some constant x3 is going to be greater than x2 by another constant so you can now derive a new series which is called y which is equal to x2 minus x1 that is going to be your y1 and y2 is going to be x3 minus x2 and so on so if you if you if you define a new series which is y of T which is composed of basically these differences now this trend is removed the trend is removed and now you can deal with the remaining series which probably you can analyze for periodicity right Premjit you are the one you asked the question right okay so you right yeah okay so uh you got this point so that you can derive a new series y of t where each term y of t is equal to x of t plus 1 minus x of t so if you do this then in this new series you have removed the trend and you are now having something which is probably flat but again it may have a periodicity and now you can you can analyze this whatever you are analyzing on this spectral decomposition this is whatever we did is called a spectral decomposition you could do it on this remaining stationary in some sense the series which is not having a trend so this is this is in fact one of the points which is I have covered it in the next slide actually I think you came to it even before I okay and then assuming some of these would be dampened as well right some of these may not be linearly additive it would be a of course a dampened oscillation yes yes yes so so yeah i mean this is a special case where you are deriving something based on the fact that it is having a steady trend but it may not always be a steady trend it may be a trend which is saturating it may be something which is gradually like like in the sigmoid case right it may be going towards a fixed value where it's not steadily increasing but still there is a some transformation which you can do to make it into a stationary series where the only things left are probably seasonality and maybe some noise okay okay so the goal the goal in all this time series is to reach a derived series which is as stationary as possible because if we are able to come up with a derived time series which is stationary then from that derived time series we can go back and explain the original because let us say in this case we are able to model the y of t very well then from the y of t we can come back and model the x of t because all we need to do is we keep on like in some sense we keep integrating we we from y of t let us say we know y of 1 y of 1 if we add x of 1 to it we get x of 2 then if we know so if in this example let us say we have modeled y of t very well so all we need to do to determine x of t is going to be equal to y of t plus x of t minus 1 right so if if we have modeled y of t very well we can use this because we know that y of t is x of t minus x of t minus 1 so if we add this we get x of t but then you'll ask how do you know x of t minus 1 we know that x of t minus 1 is equal to y of sorry y of t minus 1 plus x of t minus 2. So if we have modeled y of t, we can actually figure out what x of t is. Because at each point we can, and again, we keep going in cyclical fashion till all the way we reach x of 1. And x of 1, we assume that we know it. Okay. Right? Yeah. of one and x of one we we assume that we know it okay right yeah okay shall i move on to the and then just just to sort of concrete the understanding aren't these though if you look at a bit sort of 2t time frame aren't these just different components leaving noise aside of the series uh sorry uh question again i didn't get it so what you're talking about x of t or x of t1 or or x1 and x2 yeah those yes yes if you look over the time period t or 2t you know if you include uh sort of longer horizon then these are just different components of right if you just ignore the noise these these uh series the the granular series that you're saying aren't these different components of the bigger series there bigger components separating out the components of it or no no what we are trying to say here is this n we are assuming is much larger we are assuming that we have a data of zero one two this we are assuming that this n minus one data points we have is much larger than any of the seasonality and other things that we have in the graph. For example, in this situation, let us say we are trying to analyze the temperature at a given location. We are assuming that we have this data over maybe last 20 years. Only then it makes sense to come up with a seasonality of one year. If we have data only for a year, then we will obviously not be able to predict any seasonality. one year if we have data only for a year then we will obviously not be able to predict any seasonality let us say we are we are collecting data where n n happens to be around 365 times 20 which means around 20 years of data we have then by looking at the time series and we see that it has a periodicity which is maybe at after 365 days it is coming back and then it is going back and then we see there are these 20 such waveforms then we see that okay there is a seasonality of 365 happening here so to make that assumption that rather the prediction that there is a seasonality we really need to have data points which are much larger than the seasonality. Did I understand your question? That's correct. So what you're observing is the reputation, right? That repetitive pattern you're observing because you're looking at a longer period. So I understood that. What I was saying is this at a particular point when you are analyzing how i got that you are breaking that the times the bigger series into smaller i thought that aren't those different components if you just leave noise aside then aren't you breaking a series into a different common series and determining okay how i got here sort of uh different components maybe maybe you know let's continue maybe i can i can put it in different words as we go on yeah sure so the components are coming in the frequency domain not in the time domain so i maybe i confused somewhere see the in the time domain we are not talking of components but when we convert it into frequency domain, we are talking of components and trying to say that some frequencies are more significant than the rest. So that is the hope. I mean, it is not guaranteed that that is going to happen. signal is composed of a few distinct frequencies and you use the frequency decomposition to arrive at what the significant frequencies are okay all right okay so uh i think that was a pretty long day i thought this will be the fast quick part but it looks like i went a lot of so okay so here is let me come so most of the rest of the session will be focused on the time domain approaches okay so like I kept referring to temperatures because this is a easy example to understand so typically a time series you can have all observations are independent of each other okay so this is the probably the case where every time every element i mean in the words of premjit this is the stationary distribution why is it stationary because there is no trend no seasonality nothing it is just that each x of t is independently, I mean, you can say IID, independently distributed. Okay, so that is where each one has its own, it is probably having a fixed mean and it's having a fixed variance, but there is a noise term. Okay, so this is the situation where all terms are independent of each other. And then you have a situation where they show a steady trend upward or downward. And of course it could be a damped up. I mean, the upward could be saturating towards an upward maximum or again downwards. It could be saturating towards a downward. So in the third case, we already touched upon is showing a seasonal trend periodicity. And then of course, you can have a mixture of all these. You can have multiple seasonalities. So you can have a weekly seasonality plus a yearly seasonality. For example, with volumes of, in a, let us say, Walmart is trying to predict its sales volumes there will be a weekly seasonality because probably on holidays they will have more sales compared to weekdays so you can see that saturday sunday typically will have more so there will be a weekly seasonality and then there will be an yearly seasonality as well because towards the the Christmas and New Year, you'll have more sales than probably the rest of the year. And of course, there will be a probably one of events where some particular, I don't know, event will end up having a large sale. So all these nonlinear relationships will get mixed. So this is this is the different scenarios and in all these scenarios i mean this is a point which i made when preenjit brought in this observation in all the models our goal is to strive at another derived time series which is stationary and then work backwards from there to model the actual time series okay so i think this let me go on so here now i will cover i'm most of the time i will be focusing on the time series sorry the time domain based approaches so there are two main approaches used in time domain one is called the smoothing or averaging method and another is the univariate model approach frequency we already went through in quite a bit of detail just now that is that is considered a non-parametric approach because there are no parameters here we are just using the whole series and then coming up with a frequency domain representation of it and using that to figure out which are the most significant frequency so it is non-parametric we don't have any parameters to tweak to come up with the model here but for the time domain approach we do have some parameters so let me i'll start off with a little explanation of what we do in the time domain so let me clear this the most basic approach is the averaging approach. So this works well. If you have a completely stationary distribution, which means you have, I i mean let us say at x of 0 1 2 3 4 and that if you are independently looking at them there is not much difference between i mean there is no trend there is no seasonality nothing so in this situation let us say i have values all the way till x of uh maybe 10 i mean x of 1 0 all the way to x of 10 and i'm trading trying to figure out what x of 11 is what is the best best bet the best bet is just going to be the average of whatever i have so far right so that is the most naive way of explaining the averaging approach you take all the values till now you divide it by the total number which is in this case 11 and you say that this is going to be my prediction for x of 11 i mean this is not really a prediction it is just saying that i am i'm seeing a lot of noisy. Any individual observation may have been skewed because it may have been an outlier. We don't know. But the fact that they're all noisy and there is a lot of up and down means that the chance for X of 11 is very likely to be something which is an average of whatever I have observed so far. So this is the averaging approach. I mean this this is probably the most naive approach of forecasting so but in the moving average approach you do a slightly better job what you do is you say that whatever was the trend in the last maybe three whatever was the last three values you say that that is going to be the next value so let us say you have x of 0 x of 1 x of 2 and so on up to x of n now you say that your x of n plus 1 is an average of x of n plus x of n minus 1 plus x of n minus 2 and then you keep going on i mean so this this the difference between this approach and this is that you are giving more weightage to something which is recent uh everyone with me on this i mean these are simple concepts so this is the moving average approach is it clear yep okay so nothing much here it is just saying that you are taking instead of taking the average of all the observations still now because something way back in the past may not have been very important so you take something which is recent and use that to predict the future so again this works so long as there is no significant trend because the moment there is trend or seasonality you cannot naively assume that the average of the last few values is the next okay so that is the drawback of the moving average approach but it works well so long as you are having only noisy behavior and then there is this what is called a single exponential so this is where the first time you come up with some parameterized smoothing approach so single exponential smoothing approach so here what you do is you introduce a parameter alpha so when you introduce the parameter alpha so what is the it's a very interesting concept so what you're actually i mean intuitively what you're doing here is you're saying that your x of let us say the n plus 1 it depends most on on x of n and less on X of n minus 1 and so on so you use a parameter alpha to say how much it starts it will exponentially decay the weight the so there is a the way it is the I will first write the formula and then you it will become apparent to you why this is actually a meaningful formula so let me write it as S of n is equal to alpha times X of n plus 1 minus alpha times s of n minus 1 okay so this is the there's a parameter alpha here now the actual time series is x of t the time series is x of 0 x of 1 dot dot dot x of n is the actual time series this is the actual time series now what is s s is a smooth version of the to write s of n in terms of x itself so x n plus 1 minus alpha and let us times X of n minus 1 plus 1 minus alpha times s of n minus 2 so if times alpha becomes wait did I do something wrong here? so alpha times x of n minus 1 plus wait it should have had a factor which is decreasing i'm just trying to see whether i missed something here wait for x of n minus one. you let's probably try to expand it out and put one more term and see what happens X of n plus alpha X of n minus 1 plus or other minus alpha squared X of n minus 1 plus 1 minus alpha times okay 1 minus alpha whole squared it S of n minus 2 so this Let me keep it times X of n minus two plus one minus alpha times S of n minus three. So I think we should be seeing a pattern here one minus alpha one minus alpha whole squared yeah I think this is yeah so apart from a overall term so let me separate it out into so there is an alpha which is common everywhere okay so I can take the alpha outside yeah this is I think I missed that part so there is an alpha which is common everywhere okay so i can take the alpha outside yeah this is i think i missed that part so there is an x of n plus 1 minus alpha times x of n minus 1 plus 1 minus alpha whole squared times x of n minus 2 so you see that pattern here so it goes on and on like this the alpha i was getting confused because i was trying to expand here itself but if you see there is an alpha which is common everywhere so if you are expanding it out all the way you end up having a series which is having one if you see the weightage of the last term is one the next class term is one minus alpha then one minus alpha squared and so on so it is it is similar to the average that we saw already in the previous example except that we have a constant if you see here we had all of them had a weight of one whereas now it is different uh sorry is this clear to you i mean have i explained it properly or have i uh confused so it is basically an alpha here and then if you see the expansion it is x of n plus 1 minus alpha x of n minus 1 plus 1 minus alpha whole squared x of n minus 2 plus 1 minus alpha whole cubed x of n minus 3 and dot dot dot uh any uh anyone any questions or is this clear to you hello okay so what what we have what we have achieved using the single exponential smoothing this is called the single exponential smoothing what we have achieved here is we have achieved a smoothed version of S from X where the S of N is an averaged version of all the values of X, but with greater weight being given to the latest values of X of N. The smaller values are given to previous and even smaller to the N minus 2 and so on. And it's a geometric series. Okay? So this is the weights. And why is this useful? This basically means that you are giving a, once you come up with the smooth version, you can use the smooth version to do the prediction. So your S at a time, sorry, X at a time T plus one, instead of just saying that it is the average with equal weights of all the previous values, you will actually say that it is equal to S of T, which is the smooth version where you are given most weight to the latest value, lesser weight to the next value, even lesser weight to the more previous value and so on. So this is called a single exponential smoothed model with a parameter alpha now what do you do with this parameter alpha so what you do is what we have already done in the past when asif has explained with all these regression models and other things we use we try to we have the actual values of X of t. So we assume that there is a parameter alpha and we come up with a smooth version of S of t. And then what we do, we try to minimize the loss because we know that S of t is actually the same as X of t plus one. That is what we are trying to predict. We are trying to predict the next value of x of t using the smoothed version of t so we do this if we do this let us say we actually had data for x of t for large amount of t so we can actually try to minimize this loss where t varies from i don't know t equal to you can start with some value t minimum till whatever you have as the t maximum and you minimize this loss and using this minimize you come up with the parameter alpha which is minimizing this loss so this is the way to do the single exponential smooth model now again the drawback of this model is that it doesn't account for trends it accounts only for I mean it gives most weight to the most recent term but again it doesn't account for trends so if you look at the future values let us say you are having only values from x of let us say you have values from x of 0 all the way till x of n and you are trying to predict what x of n plus 1 x of n plus 2 and so on are so it will turn out that your values of x of n plus 1 n plus 2 and all that will become let us say you have a lot of these values if you put this formula this whatever we and you you you it will turn out that your subsequent values will kind of flatten out so let me explain what i mean let us say you have these noisy values so it's going to give most weight to the most recent term and lesser weights to the other terms which means it will be some average so the next value let us say till here was your actual data and you're trying to predict so the prediction will be something which probably has most weight on this and lesser weight so it may come somewhere here now let us say this is your prediction now you are trying to predict at the next point it will give weights for this this and all this so it will turn out that it will it will be something probably here because it would have taken these two into account so eventually you will end up kind of it will go up and down a little bit but it will flatten out which means all your future predictions will tend to be flat so this again the single exponential smoothing model is good for modeling noisy behavior so long as there is no specific trend to it but the moment there is a, it will give a wrong forecast because it is going to forecast a flat value sufficiently into the future. Okay. So to address this drawback of the single exponential trending, we have this second approach called double exponential uh uh maybe what i would yeah uh quick uh so in the context of this exponential smoothing it's only for uh some if you have two points that are far off we are trying to interpolate written what is the real use of this uh i mean immediately interpolation came into my mind if you are missing some points in our data or we are trying to interpolate this will be of use more than interpolation it is all for forecast only it is because you are assuming that you have values from 0 to n and you are trying to find out what is the n plus 1 value right so so we are but but the assumption being made is that these whatever happens till n is not showing let us say it was always showing some steady trend of growing growth trend. I mean, apart from some noise, it will this will not model it very well, because what will happen is if you substitute you will it will turn out that because this has the future values will have greatest weight on this and lesser on the previous. So it will turn out that because this has the future values will have greatest weight on this and lesser on the previous so it will turn out that it is slightly less than here because it is okay okay and then yeah and then gradually it will flatten out maybe the next one will be even less so while while there may be a significant trend the single exponential smoothing will tend to flatten it out and eventually it will become a flat curve where okay so that is the drawback of the single exponential smoothing sorry yes yes yes so that is what I'm going to come to so what I will do today I don't know how much time we have normally uh we cover this for till when what time i have we have um we have already covered around we're close to uh in india we are at 10 a.m which means i think 8 30 p.m right right now yeah so normally we go like around 10 o'clock 10 o'clock so we still have time okay so what i will try to do is I will try to cover the double exponential smoothing, which accounts for the trends. And then there is a triple exponential trending, which not only accounts for trends, also accounts for seasonality. Those are basically with two more parameters. So if you notice in single exponential smoothing, we had a parameter alpha, double exponential smootoothing we'll have a parameter alpha and gamma and i will show you how it will end up covering for trends but it will still not be able to cover for seasonality and then seasonality will get covered with the third parameter beta alpha beta gamma which is called triple exponential smoothing maybe at that point i will stop for today because it may become too much to cover in a single day then i will go into the auto regression and other parts in the next class is that fine what do you suggest i let's see let's see how much it takes because there is theoretically these two are very similar to the single exponential i will cover these now and if we have time i will cover this otherwise we will move it to the next class is that fine yeah that sounds good chanda i think you could cover whatever we can cover till the time that we normally do and then that will be the lab a lot more well-paced yes yes yeah okay sure yeah i think if if we are going to not cover everything today i i think we will still have time to cover the labs because the labs are somewhat straightforward once you get the theory right the lab notebooks are basically applications of all these with real examples so i will yeah let's see let's see how it goes so okay so i have so far covered covered only the single exponential let me go to the double exponential now and which will account for so if you see in a single exponential smoothing we had this s of n we had equal to alpha times x of n plus 1 minus alpha times s of n minus 1 right this is what we had for the single exponential smoothing now in double exponential smoothing we add a parameter gamma how do we do it we say that there is also a trend b of n which is equal to gamma times whatever is s of n we subtract s of n minus 1 okay and then we do 1 minus gamma times b of n minus 1 so what does this do this ends up saying that whatever is your current value current smoothed value and the previous smooth value is similar to this this this equation if you see is very similar to what we had in the single exponential smoothing we are saying that it depends most on the most recent trend and lesser on the previous trend okay so usually this gamma if you if gamma is very close to one it means it very significantly depends on the most previous previous previous value and lesser on the previous values one minus gamma being very small of course you can tweak gamma if gamma is small it means you are making give more importance you are giving to the previous terms and then when you expand it out the way we did it earlier you will see that you have a one minus gamma one minus gamma whole squared so even if gamma is small and one minus gamma is very large because one minus gamma is a factor which is less than one eventually if you keep having powers like one minus gamma whole squared one minus gamma whole cubed and all that there will be lesser and lesser weight for the more older terms okay so let me get rid of this okay so now this is in the double exponential smoothing you have this extra equation and of course you also have you also add this b of n minus one here to merge it with the s of n so what you are saying is your s of n is having this is the forecast of the previous value you add it to the trend of the previous value and then you do together you have have both of these together contribute to the uh the new value of sf so your what what this ends up doing is you have a parameter alpha and gamma where alpha is as before but it is multiplying the previous so this is the smooth value of the x of t at that instant of n minus 1 and then you have a trend value of at n minus 1 and this is the trend value at n minus 1 is added with the most recent trend again with different weights to come up with the new value of the trend and you say that the actual prediction so unlike in the previous example you said that the except n plus 1 in single exponential smoothing was equal to s of n here instead you say it is not equal to s of n it is equal to s of n plus b of n because whatever is the value you are going to add it to the trend to come up with the next value of x of n plus 1 okay so this is the single exponential smoothing so what it is the why is it taking account of trends because if you consider the case where you didn't have any trends okay so if you didn't have any trends um it means that your b is zero basically your let us say your there is no significant difference between s of n and s of n minus one that means and your're in the past also it was zero it it goes back to the previous single exponential smoothing but if you did have a trend so let us say you did have a trend where you have let us think of some examples where you have a steady trend okay so you you let us say you have some noisy terms where it's going up and down okay so in this case uh let us say i have this x of zero x of one okay and i have some values still here and i'm trying to predict the value here now if you notice there is a steady trend here right there is a steady trend and i would like to have my future values obey the trend now if you notice this gets modeled how does it get modeled because if you see your s of n let us see let us say that the we have the value till X equal to n and we are trying to figure out the value at n plus 1 okay if you are trying to find the value at n plus 1 we notice that not only the value at the the s of n is the smooth value whatever we already had the smooth value of the previous values yes that is s of n but your x of n plus one is not just the smooth value there is going to be a addition of a trend which is coming where does this b of n come from if you see it comes from s of n minus s of n minus one so there is a s of n minus s of n minus one that component is there but then we are not only using that we are saying that it also depends on the past values of the same trend so it is the same argument so whatever you are you are adding a weighted sum of all the past trends with greatest weight being given to the most recent trend. That's clear, right? So if you remember the previous expression we had, if you try to expand it out only in terms of the S of n minus S of n minus ones so you will turn out that your b of n will be equal to gamma times let me write this s of n minus s of n minus one as delta of n okay it would have been gamma times delta of n plus one minus gamma times delta of n minus one plus one minus gamma whole square times delta of n minus two plus dot dot dot is this clear because it is the same argument whatever we already had so if you notice we are saying that your trend at any point b the at a time instant of n is actually the weighted average of the trends at all the past and so you are saying that all the past trends contribute to the current value of trend which means and of course giving the greatest weight to the most recent and that value is being added to the current value of the smoothed value of s of n and the smooth value of s of n we have already seen the smooth value of s of n is in some sense if you didn't have the trend it was actually equal to n X of n plus X of n minus 1 times 1 minus gamma. That was the unsmoothed, I mean, the smoothed value without a trend. But now the smoothed value has a trend. So each one of them has also a plus b n minus 1 and all that. But whatever it is, it is giving the value at this instant n, which corresponds to the smoothing of all the past values accounting for the trend okay now that value being added to this bn gives the predicted value for x of n plus 1 so this is so the advantage of this double exponential smoothing is it takes into account the noisiness in the time series as well as the trend that is coming in the time series so if you want to find what is the at not at n equal n plus 1 but any n plus M so that is the advantage that means we are able to predict the value of X at n equal to n plus 1 n plus 2 and they don't tend to a constant they actually tend to a it'll they'll actually tend to a straight line because at and the straight line will depend on the value of b of n here and the s of n here will be like the intercept so s of n is the intercept the smooth value of the s at this point so in some sense whatever was the series so far it may have been following some noisy term you are trying to model it in terms of the s of n and b of n so it will actually tend to be a it will it will go like this it will i mean the it will go in a straight line beyond the value of n equal to the whatever is the value you have chosen for all future values it will follow a straight line this part is clear yeah it's basically modeling the slope now yeah it's modeling the slope actually we can do a little better also instead of just saying that x of n plus m is s n plus m b n some places they also do you they use the latest value of because s you can actually because you know s of n plus one can be predicted you can use the s of n plus one to predict so it means you can continue to have some noisy behavior for some more terms because your s of n may not be equal to s of n plus one may not be equal to s of n plus two likewise your b of n plus one may be slightly different from bn plus one but eventually it will go to a straight line okay so this is the double exponential smoothing method and again i mean just like we modeled with one parameter alpha here we have modeling with two parameters, alpha and gamma. So again, you take the last term to be equal to your prediction. So let me call this the P, X of P. So X P at time, whatever time T, minus the X actual at time T. Let us assume that we have deliberately even though we had all the values from zero to i don't know infinity or not infinity but some large number we deliberately chose to start predicting the values of x of p from some instant of time onwards so this becomes the noise term then because you have predicted but you're actually something else if you knew both the prediction and the actual and you have the summation from the again some t min onwards to a t max which you have we have the data let us say we have the actual data of x of t from a team into a t max i mean we have from zero to t max but we are computing the predictions from a team in we can use this to figure out the best alpha and gamma right so this is the double exponential smoothing again when we do the lab rate we will see that we have these ways to i mean there are packages there are packages where we can figure out what is the best alpha and gamma and then once we have the best alpha and gamma we can use that to do the prediction for future values because we now have a formula where if you see the formula only depends on alpha and gamma after that if you go here your sorry your yeah here your asset any given time t depends on alpha and b and b depends on gamma again so using these two you can predict for any instant of time so once you have solved alpha and gamma using a loss or something by doing this minimizing of this loss you use this so these are all called parameterized models that is why if you look at the frequency based we didn't have any parameters we just had to do the fourier transform and figure out the components but here we are actually doing a parameter alpha and gamma are the parameters so so far i have covered the single and double exponential now similarly we have this triple exponential which is again i mean useful to go through triple exponential smoothing and why we are using this because in the single exponential smoothing we didn't have any trends double exponential smoothing we had a trend triple exponential smoothing we also have a seasonality so how is it handled so the handling is done using a s of n we have a similar formula we have the alpha term which is going to be X at n but now you introduce a factor which is called I yet I think it is called IETN. Let me just check the exact value. I have the reference here. I just want to be accurate with the notation. So I have it here on my reference. So it is. Yeah, it is using i at n minus l let me write it as n minus l so l is this is uh the time period of the seasonality so i'll first write it and then i'll explain the formula okay plus 1 minus alpha times s at n minus 1 plus B so this is a similar term to what we already saw be at n minus 1 okay and then we have the term for B of n we have already seen we had it already this is the trend term we had a gamma times s of n minus s of n minus 1 plus 1 minus gamma times B of n minus 1 this is identical to what we had already in the double exponential smoothing so we have the most recent trend plus the past trend a weighted average of this contributing to the current trend and the smooth value of sn is similar to the what we had in the past with double exponential where we had a 1 minus alpha times sn minus 1 plus bn minus 1 for the past and alpha times but instead of the x of n it is x of n by i of n minus l which where i is a factor so that is the third term which comes in the triple exponential smoothing and that comes with a term of beta so how does that work we have I at n is equal to n is equal to beta times the exit n by s of n plus 1 minus beta times i at n minus l okay so what is the meaning of this so this is these you can think of these as factors so So, okay, let me explain the intuition behind this before we get into the actual formula. So what really happens in all these, let us say we have a seasonality. And let us say we have a time series, which is, let us say it had a trend. Okay, there is a trend. And there is also a seasonality. So let us say it had a trend okay there is a trend and there is also a seasonality so let us say the the perfect example of a trend and a seasonality would have been something like this right this would have been a perfect example of a trend plus seasonality so let us assume that the actual values are something like there is a value a time equal to there is a value here then there is a value here there is a value here there is a value here there is a value here okay so so it is it is not only showing a trend, it is also showing a seasonality. Let us say we are actual values are this. Now, what we are trying to model with this factor I is you are trying to see how much is your actual X of n, how much is it greater or smaller than the average so your if you see your X of n by S of n X of n by S of n S of n is a smooth value the smooth value is going to be the I the smooth value of the s of n you can think of it as the value that you expect it at a given point without a seasonality now x of n let us say x of n is greater than that which means it is in between the and the season the seasonality this is actually l so this is this length is l which means let us say this l was in the case of let us say we are looking at 12 months in a year L would have been equal to 12 if you had monthly values 0 1 2 all the way up to 11 L would have been 12 okay so what we are saying here is these are seasonal factors where whatever happens, L time instance before is the factor. This factor is contributing with the most recent increase of X of n compared to S of n. So this factor, let us say there was no seasonality, it means X of n and S of n would to sfn so this is this factor let us say there was no seasonality it means x of n and sfn would have been identical okay it means that is in some sense this is exactly what your smoothing would have predicted if x of n and sfn are identical it means there is no seasonality it means that the factor is one okay but let us say X of n was greater which means that compared to the normal trend of the smoothing you are having a higher value so you are having a higher factor so this higher factor how much is it the most recent increased factor where I compared to the factor that it was L seasons ago so it is similar to the trend where you are comparing the most recent trend with the trend one instant ago right so here you are comparing the most recent trend factor where X of n is compared with S of n the most recent trend factor is compared with the trend factor L time instance ago and together it is com it is combining to produce a factor which is the smoothed version of the trend which means how much is your trend compared to the smooth version i mean how much is your factor this is your factor how much is your factor the seasonal factor the seasonal factor depends on the most recent factor as well as the factor l seasons ago is this clear just like we had the trend which was compare looking at the most recent trend and the trend of the previous cycle like that the ifn is the most recent seasonal factor and the seasonal factor l time instance ago because l being the time period of seasonality these two are combined to form the see the seasonal smooth version of the seasonality and what does this show this shows that the smooth version of the seasonal factor is what is basically you are getting rid of the seasonality so the smooth version of sn is saying that if x of n has a seasonality and it also has a seasonality factor you are removing the seasonality like let us think of an example where q1 is having 1.5 i mean i'm just thinking of an example of q1 is 1.5 q2 is 1.25 q3 is 1.0 and q4 is 0.75 so these are the factors for example so what we are trying to say is if your actual revenue in Q1 happens to be, I don't know, 200, you are dividing it by 1.5. Because you know that Q1, you expect it to be greater by a factor of 1.5. So the smooth version of the Q1 is 200 by 1.5 and then q2 let us say it turns out to be 180 so you are dividing by 1.25 because that is a seasonal factor you expect in q2 and in let us say in q3 you had 180 itself then you are dividing it by one so this is a smoothed version and let us say it was 160 so you are dividing it by one six by 0.75 so what what this ends up doing is this ends up creating a smoothed version which is probably all closer to each other i i don't know if i made the intuition clear so the from the original values of x of n which are having probably a lot of seasonality you are dividing by the seasonal factor so that you get a smoothed version which is devoid of the seasonality so this is what what in the beginning of the class I said we are trying to get rid of the all the trends we are also trying to get rid of we are trying to derive a new series which is devoid of seasonality is that clear we have a original series which has a trend as well as a seasonality okay the the original series x of n it has a trend it also has a seasonality we are trying to derive a new series which is devoid of the seasonality and trend how are we doing it we are we are getting rid of the i mean we are dividing it by the seasonal factor and coming with a new series which is not having the seasonality so you'll see s of n will be probably something more which is flat now from here how do we predict the uh next value of x of n plus 1 or n plus m it's going to be uh so there is again a formula for it let me copy that formula and then I'll explain it to you it will be s of n plus B of n as before this would have been the but then you will also multiply it by the seasonal factor which is n plus 1 but because n plus 1 is usually not defined you also do n minus l plus 1 because you know that it is defined for l seasons ago so you do this so you you use the most recent seasonality for l seasons ago so you use that factor so if you use that factor you come up with this is the smoothed version along with the trend and then you multiply it by the seasonal factor to come up with the predicted value for x of n plus one any questions on this or is it clear what i am explaining should i go over any of these a bit more yeah um chanter actually um a little no i don't i'm not sure x by n by x by n x x n is the actual value s n is the smooth value right on the third equation correct so what is i predicting is so l is also unknown parameter right so it's alpha beta three parameters so there are three parameters here alpha beta and gamma and l l is a factor which we see from experiments i mean it's not unknown we are actually seeing that we are seeing that there is a seasonality with i don't know l equal to 365 or l equal to 12 if it's a monthly thing or l equal to 7 if it is a weekly so we we are seeing it we are seeing it experimentally so it is it is like a hyper parameter it is not a parameter which we used to i mean in the language that we have done in the past it is not a parameter which we used to I mean in the language that we have done in the past it is a hyper parameter these are all parameters which we fit using the laws alpha beta gamma but L is a parameter which we throw into the model as a hyper parameter so we may have to experimentally look at the data and come up with multiple values of L and figure out which one is fitting best got so my other question is the third equation i am what is it trying to achieve like if it's a smoothing parameter we know we give high importance to the nearby ones exponentially decay the other one if it's the slope we give more importance to the latest slope than the uh previous correct correct for i what are we what is the intuition it is the same thing we are saying that the most recent let us say in the most recent observation your value for a given uh quarter i mean i'm just giving the example of quarter in the most recent observation the value for the q4 was higher than the average i mean s of n is in some sense and averaged over all four quarters okay so you let us say that your q4 revenue was greater than the averaged value for over the all four quarters by um by a factor of 1.5 okay so you're giving higher importance to that and lesser importance to whatever the factor was one year ago, one year ago, maybe the Q4 revenue was higher than the overall annual revenue by a factor of 1.3. So you're giving more importance to the current year's factor than the previous year's factor. Right, you got it? Because we are modeling the seasonality, right? So it should repeat every year around the same time because that got stuck in my mind. Yes, yes. It is the same thing. What I'm saying here is just like in the case of, let's say in the case of B of N, we said that the most recent trend is what has maximum weight we said this multiplies with sorry we said gamma oh yeah for the flow right yeah and then we said that the i mean the years, this is not really the previous years, this is a combination of all the previous years. All the previous years slope is having a weight of one minus gamma and the current years is having a slope of, I mean, current years is having a weight of gamma. Likewise, the seasonal factor, the current seasonal factor has maximum contribution coming from the current year's seasonality and lesser from all the previous years. and previous years is n minus l because l is that many cycles ago right so the this is the the interpretation is exactly similar the the current year's seasonality factor is a combination the smooth version of the seasonality is a combination of the current year seasonality and all previous year seasonality got it so my my other question is how come the x xn by sorry if i'm repeating how did x of n by s of n be came to be called a seasonality that part that definition is something still so the s of n is a smooth version s of n is you can think of it as the value assume that there was no seasonality if the s of n is basically the model where there is no seasonality so if there is no seasonality and it was just obeying a trend like this okay then you would have predicted that s of n is uh s of 1 s of 2 s of 3 but in reality it is showing something like this so you know that and there is a here let us say it is zero one two three four this is i mean this is a q1 q2 q3 q4 and then again it is repeating after four so let us say uh let us say i had a one two three four and then this is again one two three four something like that so in this situation the actual value let us say x1 at q1 is the factor is one here whereas at q2 the factor is actually this is a wrong diagram i should have put it this i think the whole sign will after yeah yeah whole sign so i should have put the whole thing sorry so let us say this was my one here this was my two here this was my three here this was my four here and this is again my uh the c the q1 of the next year okay the five is the one of next year okay so in this case your x n by sn let us say it was perfectly following the seasonal curve x n by sn would have been one here it would have been let us say this was 1.5 here it is again one here and then it is less than by a factor so it is 0.75 or something and then it is one here so this xn by sn is really trying to model how much is your x of n compared to the value so that would have come without seasonality you got it right so if if it turns out that your real value is somewhere, I mean, this is the perfect example. Let us say you're in reality, your X of n at some point turned out to be here. Okay, so your most recent seasonality is X n by SN, which is not not one, it is probably 1.5. one it is probably 1.5 okay so you you give most weight to that 1.5 but then all your past years would have pointed to a seasonality of one so you give weight of one to the previous years with a one minus beta and come up with a smooth version of the seasonality is that clear clear? Right. So if L is one year, so it's going to look at the 1.5 in the previous year and also the 1.5 in the current. I mean, L has to be the whole length, right? Yes, yes. L is full uh number of cycles required to make so in this case l is four in in this example i have taken the periodicity was four if if i had uh if i if my time series was every quarter once every quarter x1 x2 x3 x4 and x5 is actually the next quarter so the seasonality is four means there are four cycles i mean four time intervals to make a cycle can i try an example um and see that that intuition is covered by the example sure sure um let's say we are looking at our spending power and the spending power annually gets bumped up because of a growth that we get in the salary yes right and that growth comes every time in december and increases our spending power but the money that we have kind of we spend it through jan through december okay so the beta that we're seeing here would be basically the the bump that we are getting no not not not the beta the in is basically the the bump right yeah in in in is basically a factor you can think of it as a factor which varies from uh zero to one uh sorry not zero to one it can go from it can go beyond one also one is if there is zero means it is less uh actually it can go to any large number also but it's basically a positive fraction right it can go any it it can typically you can say think that it goes from zero to two but i think it can go to any large number because if you have a very seasonal which is having very high values it can go to a large value so it can go to one is basically the situation where there is zero seasonality and zero is the situation where it has gone all the way down to zero and so yeah if if you are getting a lot of spending power close to the beginning of the year it means you are going to have a very high value for i correct your i value may be close to i don't know compared to your average over the whole year let us say that your average over the whole year your spending power at any given i mean let us divide it over the whole 12 months let us say you had power of spending thousand dollars per month for the whole year but if you are looking at it as a seasonal thing, maybe in January you have $2,000 to spend and in February you have $1,500 and it gradually goes down all the way and probably it becomes just around $300 at the end of the year. Is that what you're trying to say? Yeah. And next year if you are averaging it over the whole year if you are averaging over the whole year you will get a thousand as the average per month so let us say you have two thousand thousand five hundred thousand three hundred and all the way up to going all the way down to say uh 500 as the minimum right this is what you're trying to say i'm gonna start from 2500 yes so it will again go up and it will come down correct yes so your seasonal factors because you have your smooth version is thousand uh so let us assume there is no trend here now if you add a trend then the gamma and the b factors come in but let us say that there is no salary increase and you're just having a same no trends it is just a seasonality then your factor i factor would have been two here 1.5 here 1.3 and so on and here it could have become 500 it would have become 0.5 right at the end of the year because you have only 500 compared to the average value of thousand does that make sense premjit um i would have thought the i factor only comes once in no i factor is there for all values although okay okay all the values but what we are trying to say is the factor i the the the real factor of i mean the instantaneous factor of i is the most recent value but the smooth value is going to con have contribution from the most recent and the past okay yeah got it and the past it is similar to how we are doing the smooth versions for the b and the s we are always taking a weighted average where the weight of the past has lesser contribution geometrically decreasing so that is why it is called exponential because one minus beta whole squared one minus beta whole cubed for as you go more and more into the past years the contribution of the seasonality your terms coming from there is decreasing geometrically okay okay so if we see the overall prediction your n plus m will be equal to similar to the other models you will have a s at time n plus m times b at n but you will also so this is the smoothed version you are adding to the smoothed the trend factor and you are adding a factor you are multiplying it by the factor which is n minus L plus M okay so this is your prediction for the future so you in this triple exponential smoothing you take the smooth value of the function you add it to the smooth value of the trend take m times the smooth value of trend multiplied by the smooth value of the seasonality and then you get the overall prediction so what are the parameters there are parameters alpha beta and gamma again you use the minimization of loss because you have a prediction you can take x of n plus i mean whatever the prediction is minus x actual at n assuming you have the actual values you do the squared you sum over all the values of n and you arrive at the best value for alpha beta gamma so i mean this is the theory behind it of course you don't have to do all this because there are packages that do it for you in packages you just say that i want to do triple exponential smoothing the only thing is with triple exponential smoothing you there is a hyper parameter that was a the hyper parameter of l l so we may have to experiment with multiple values of l or by observation we can see that i mean many times we know beforehand let us say we are trying to predict the quarterly revenues we know that l is going to be four in that case if If you are trying to predict, I don't know, daily temperatures, we know that it is 365, for example. So this is many times it is known by the problem. But we may have to look at the graph a bit to figure it out. But having done that, the packages will predict the best value of alpha, beta and gamma. OK. So this is about a triple exponential so i covered the so what all we covered today i'll just probably put a summary so that we have a clarity on what we need to cover so we covered um basically what a time series is the most there are basically what a time series is the most there are two kinds of approaches typically used which is time domain and frequency domain in the frequency domain generally it is considered a non-parametric approach. Because we are not modeling for any parameters, we just do the Fourier transforms and there are packages to do the fast Fourier transform. And we observe the spectrum and the ones which have highest values of frequencies i mean the amplitude we use that in some sense to derive the significant frequencies and that can be used to do predictions predictions for future values then in time domain approach most of the other methods come in we have this averaging slash averaging slash smoothing approaches they can cover seasonality and trends but the thing is they can only cover simple seasonality and simple trends. In the sense, you cannot have multiple seasonalities within the same model. And likewise, the trends that are covered by this are the linear trends, not more complicated trends. So that is the drawback of this averaging, whatever we have covered so far. Then I am yet to cover the univariate models univariate models so this there is a auto regression model moving average model and then I is basically the integrated model and combines to form the arima and likewise we have the s arima which I will cover probably I don't think we have enough time today because it's already I think just 20 more minutes this will take some time so let me not cover this today but having done this these are the still the classical approaches whatever in this time domain we have here are the univariate models then for completeness there is one nice package by Facebook called profit I will cover that because it turns out that this lot of these univariate models as well as smoothing they require quite a bit of expertise in time series in terms of trying to fit the alpha beta gamma and figuring out l so the facebook has this pack library called profit and they have a very nice theory behind it. I'll try to cover a little bit of the theory and also when we do the lab, it's a much simpler approach. Much simpler approach. Again, using classical methods. Why I'm saying classical is it's not using deep learning it. Classical methods to do time series analysis so this too we need to cover and then finally there are deep learning based approaches which is actually what we in actually we have already covered it in some are deep learning based approaches, which is actually what we are in. Actually, we have already covered it in some sense, deep learning based. So I won't take much time here because we have covered it, but I will explain why it is not new to you. covered sequence to sequence translations, right? So where you have a sequence and you're predicting a new sequence. Now a special case of prediction of a new sequence is basically the next value of the same sequence. So if you remember all the transformers using attention or the RNNs, all of them, what they are doing is you have a series and you do some transformation and you're getting a series out. So in the context of NLP and languages, you have a sentence probably in English and you're having a sentence in French. But if you see the architecture, it is ultimately a series and it is getting converted to another series so this was a more I would say a general case in our case we want to do something very simple we have a series we want to predict the next value of the same series right so it is a simpler problem this is a more general problem where given a sequence, you are predicting another sequence. But the time series problem is a special case of this where given a sequence, you are predicting another sequence which has also the next value of it. This I will cover also. It won't take much time because we already have the theory behind it. It's a special case of what we have already done. Okay, so this is something. So let me, and then we need to do the labs on these. So labs on above. So I will do some examples. So what we will do is this we will keep for the next class and this also for the next class. I think we should be able to cover both because there is not much to it. I mean, theory wise, it is not it is more I think the greater focus will be on implementing it in the labs than on any great theoretical framework here okay uh chander yes there are still 45 minutes uh do we have 45 minutes yeah okay i assume that we don't have 45 minutes okay we so we can actually cover univariate models then maybe we will try to cover the univariate models if we still have 45 minutes is that fine it's fine okay okay so good so then let's um okay uh i assume that we have only i i think i'm looking at the time wrongly because i'm looking at the india time i assume that i have time only till 11 which is okay fine so let's let's start with what univariate models are and go probably profit onwards we will keep for the next class okay okay so let me go back to the presentation and so that we come to the have a focus on what we are doing. OK, so this is what we covered moving average and weighted single exponential smoothing models, a trend model, a seasonal trend. You're seeing my screen, right? The presentation. Yes, yes, nice and clear. Okay. This is the part which we will start right now. What is an autoregression model? Let me go to the whiteboard again. I can clear the screen. green so that. OK, so in. If your except T. Is actually going to I mean, in the auto regression, you assume that your X of T is actually some linear regression of so we put some parameters alpha 1 X of T minus 1 plus alpha 2 X of t minus p okay so this is what we are saying here is your next value at any given point your x of t depends on values of x at t minus 1 t minus 2 up to t minus p with some parameters alpha 1 alpha 2 alpha up to alpha p and the goal is to predict of course you can also have a constant term you can also have a plus alpha naught so the goal is to predict uh not predict the goal is to come up with the optimal values for all alphas using the data that we have so this is just a problem in linear regression if you look at it this way if you have a lot of data points where you have time series data from x 0 to x n you have several data points because you have x 0 dot dot dot up to x p minus 1 will end up contributing to x of. So this becomes your X vector and this becomes your Y vector. Right. And then you have all the X ones dot dot dot up to X of P contributing to X of P plus one. So you can write it in a tabular form you can actually write your x vector which consists of x 0 up to x p minus 1 x 1 up to x p x 2 up to x p plus 1 because we have all these values with us and you have x p plus 2 and it just becomes a problem of feeding it to a linear regression model which will solve for all these parameters so this is the ar model so it's very simple i mean conceptually it is very simple this is the auto regression model which is having p terms is that clear any any questions on this it's clear clear okay so This is the auto regression model. Now the moving average model, which is not same as what we have done so far as moving average, but it is called a moving average model. I will just explain what it means. I'll also tell in what way is it different from auto regression model. In moving average model you actually say that your exit time t it depends on uh so okay going back to the auto regression model if you actually expand it out if you are trying to figure out what x of t is in terms of x of t minus 1 x plus t minus 2 and all that you will notice that x of t minus 1 itself can again be written in terms of x of because we know that x of t minus 1 is also an auto regression on all the past parameters of x likewise x of t minus 2. so it goes on in a chain. And what ends up happening is your X of T depends on values of X, which are way in the past, right? You get what I'm saying? Like if you try to expand it out, if you write X of T minus one itself as a linear combination of X of T minus two, X of t minus 3 and all the way up to X of t minus t minus 1. Similarly, X of t minus 2. So you end up having X of t becomes not only a function of the p past values, but also of the values of X even before it. So if you keep expanding it out, your X at a time t depends on an x which is way in the past which may have been like i don't know 10 000 time cycles time instance ago it would still even though your p is probably just some five or six still your x of t will depend on x of t minus 10 000 you get it right is that point point clear if you try to expand out the dependencies your x of t can still become dependent on x of t minus 10 000 even though your p is just three or four right is that point clear let me write it so that there is no confusion on what I am trying to say X of T can depend on X of T minus 10,000 for example even if P is just three or four, right? Now, this is not for modeling purpose. People are not very comfortable with it. They do not want something which is happening way in the past to affect what current values so you there is a alternate approach of moving average model where you actually say that x of t depends on some noise variables which are only occurring so it could have happened it could have been some so that let's again put an alpha not i'll not put the same alpha i'll put beta here okay an alpha naught i'll not put the same alpha i'll put beta here okay so let's say in earlier i used alpha so i'll say it's a there's a beta naught plus there is a beta 1 times n at t plus beta 2 times n at t minus 1 so i'll explain what this means plus beta q times n of t minus q so here what we are trying to say is something different we are saying that x of t depends on some random noise variables which happen at t instance of time in the i mean at q instance of time in the past but not more than that it could depend on the current so basically we are saying that your current value of the x depends on something that could have happened right now or one instant ago two instants ago up to Q instants ago so this is different from this because here your X of T could have had dependent on something which happened way in the past but here yours your you are explicitly ruling out that possibility you are saying that your X of T only depends on what happens Q instants in the past is this distinction clear yeah okay so and what typically what is being done in this moving average models is all of these are assumed to be noise which have the same value for mu and sigma so that means they are all some random variables which are kind of normally distributed with a given mu and a given sigma. So this is so the mu and sigma become some kind of hyper parameters of this model. Okay. And they are and they are normally distributed. That is the assumption made. so if you have a normal distribution with a hyperparameter mu and Sigma now the the objective of this whole exercise becomes to similar to in this previous auto regression model we tried to model with I mean figure out what the alphas were here the in this moving average model the goal is to find out what the betas are so again i mean this is the theory behind it and so the most general model would be an something which requires not just auto regression it will also have a auto regression plus moving average means that you're assuming your x of t could be having all these alpha naught plus alpha 1 times x of t minus 1 plus dot dot dot up to alpha p x of t minus q plus i'm not introducing because it's the same beta naught plus you have a beta 1 times n of T plus dot dot dot beta of Q and of t minus q actually i should have put q plus 1 here because n of t minus q because there is going from 0 t equal to 0 to t equal to q okay so this is the most general ar plus ma that this is auto regression and moving average so you end up having more hyper parameters i mean sorry more para not hyper parameters more parameters alpha naught alpha one up to alpha p beta one up to beta q plus one so you have all these parameters and um one question yes so so here i'll it won't go to alpha p right it should go to alpha q shouldn't it it should go to alpha sorry the first one would have been i it should have been p here so let me correct it this would have been p um but in in this case then the issue with the auto regressive model will still persist right yeah yeah yeah no no no no i i just gave a motivation for it i'm saying historically people started with an auto regression model and then they realized that auto regression model is having a drawback that it keeps on assuming that values way in the past end up contributing so they came up with an alternative approach which is having only the moving average but but still the fact remains that to describe a most general process you need to have both auto regression and moving average because you you can have i mean it this is just because it becomes difficult to model it is not that it is not unrealistic it it it can happen uh that your x of t depends on x of t minus one x of t minus two up to x of t minus p it it can happen but it can also happen that there is a contribution coming only due to what happened this time instant what happened one time instant ago up to q time instance ago so the most general time series problem is modeled using an auto regression plus a moving average model it cannot be just a moving average to write uh like the no no definition of this moving average only moving average if you do you get only a model like this okay so there there are models which are only having this term and not the auto regression terms then it's called a moving average model whereas an auto regression plus moving average model will have all these terms but the moving average model doesn't it it just it is just modeling noise so how can it model anything it is just modeling noise so how can it model anything it's possible no let us say in reality your let us think of a situation where your current value doesn't depend on the past value of the same variable how what would be a good example let us think of like a random walk random walk it depends on the previous value no the random work the current value depends on the previous value let us say um let me think of an example um okay let let me think of an example where the marks that you score in a test today okay the marks that you score in a test today let me probably put a uh there is a marks you are scoring in a test today now they can depend on the effort you have put today okay so some alpha times the effort today okay plus you could have put some effort maybe let us say you are putting on a weekly basis so the effort that you put so let me call it alpha 1 and then the alpha 2 times effort you put one week ago then alpha three times effort you put two weeks ago so you're you're just modeling your so let us say you are only looking at last three terms okay so your marks that you are obtaining today it depends on the marks you have effort you have put today last week and two weeks ago so this is one way of modeling your marks so here there is no reason to believe that the marks you got today is dependent on the marks you got one week ago i mean that that's not necessarily true right but there is another problem where it may be true like in the random walk your current position depends on where you were one week ago if that is the case yes then you will have another problem where alpha naught plus alpha 1 m at t minus 1 i i'm using same notation i should have i should have used beta here okay plus alpha 2 m of t minus 2 so this is a different model here here you are saying that the marks you are obtaining today depends on the marks you obtained last week and the marks you obtained two weeks ago but normally when you are looking at marks obtained in a test there is no reason to believe that it depends on the marks you obtained last week or the marks you obtained two weeks ago it's probably only dependent on the effort you put in today last week and two weeks ago so there are two different kinds of scenarios in there is one scenario where this is kind of a valid approach there is another scenario where this is a valid approach like in a random walk approach because your current position depends on where you were last week and probably where you were two weeks ago and or even if if you don't even put two weeks ago if you only put last week and probably where you were two weeks ago and or even if you don't even put two weeks ago if you only put last week indirectly it will depend on where you were two weeks ago right so when you apply auto regressive plus moving average to a data set yes what is the noise in that data set like are you trying is does the noise represent or the hyperparameters of the noise term represent the entire data set trying to be modeled using that new and Sigma or is it something else? Okay. So what is this effort? This effort is something which is beyond, we are not predicting it, right? What effort you put in last week or the week before or the week i mean if there is another model to predict it then fine but let us say we don't know about it all we know is that it is a normal distribution with a mu and a sigma so that becomes it is in some sense a nice term but it's not noise in the same sensor it is not an error term it is it is what you don't know what you don't know you are putting some distribution around it and saying it is a noise and it's it is something which you may have put in this much effort or this much effort or this much effort you don't know got it yeah it's clear now okay so the most general time series is a combination of both moving average and auto regression but again I mean there is a item which we have missed in all this which is that the integration terminal okay why am I bringing the integration term here it could have been that what whatever I mean you know in both these moving average and autoregression terms, implicitly there's an assumption of stationarity. If you notice in both of these, if you look at the, sorry, I am always bad at this. In both of these, if you see your x of t dependent on all this there is no difference between x of t and x of t minus 1 x of t minus 1 also had a similar dependence on all the past values so they are in some sense they are modeling stationary distributions. Stationary means there is not the trend part is not explicitly available. So because of that, there is this integration that comes in, which means whatever in the beginning of the class Premjit brought about, right? We need to, this X of T is actually modeling a derived value of a stationary distribution so let us say your original distribution is having some trends now you need to come up with a new t which is devoid of trends and then you apply this moving average and auto regression on it okay so how does that work so how it works is basically you the what i explained in the very if you notice y of t is defined as x of t minus x of t minus 1 for example this was an example where you had to do a difference once in order to come up with a new distribution which can be be modeled so this Y of T is modeled using a R plus ma now once you have derived Y of T using a R plus ma let us say you have predicted the values for alpha and beta using some model so let us say figured out the out alpha and beta then having figured out how do you get back x of t you get back x of t by doing the integration part which means you say that y of t plus x of t minus one is equal to your x of t so this is the integration part so all three get combined to form arima okay so this is I mean I would say arima is just an acronym for saying that you are doing auto regression moving average but also introducing the integration term which is basically saying that by itself the original time series may not have been conducive to just auto regression and moving average you may have had to convert it to another time series by taking some differences so you may have had to take a difference uh let us say you are taking it in this case i have taken a difference once so that means my d is equal to one but let us say the trend itself is increasing with time then i will have to do a difference with a d equal to two if you're if your x of t minus x of t minus one is a trend and let us say your trend is not steady it is here going up like this is not steady it is here going up like this which means i need to take one more difference to make it a stationary model so you can do difference with d equal to dot dot dot you can actually do difference with d equal to some random d okay so this arima model it's ending up taking parameters of hyper parameters of p q and d what are p q and d p is the number of terms in the past that we make your x of t dependent on q is the number of noise terms in the past that we make x of t dependent on d is the number of differences that we need to take to bring into a bring it into a stationary time series so your arima model has these three hyper parameters p corresponds to number of past terms to regress on okay terms to regress on okay q number of noise terms in past by noise terms i mean the unknown terms unknown terms to regress on so these are hyper parameters d is the number needed to make X of T stationary. Okay, so all three together bring the Arima model. So if you go to packages, you have to actually feed in the P, Q and D. Only if you feed in the P q and d the arima model will then come up with a model will actually estimate the alpha and the beta because the alpha and beta are the values if you see the regression coefficients and having done the once the model computes the alpha and beta the future values can be predicted right because you you have everything in terms of alpha beta and your x's so your x at t plus some value of m can be predicted so this is the theory behind arima model where you are doing your auto regression moving average as well as integration and how do you bring in seasonality into this i mean it is simple i mean simple as in theoretically simple of course computationally there is going to be another so what you do there for seasonal arima is you have one more set of parameters p q d and of course l also l being the seasonal period and p q d are similar to the small p small q and small d this becomes the number of past Q and small d this becomes the number of past seasonal terms terms unknown terms i'll say terms to consider and this becomes the number of difference operations required operations required to make it stationary and the difference operators will be between the value at t and value at t minus l even here so if you look at the x at time t here we are saying that in this it is going to be alpha times x alpha 1 times x at t minus l plus alpha 2 times x at t minus 2 l plus dot dot dot alpha p times except t minus pl so this is what we mean by this p term here that means there is a this x of t let us say we didn't have the other model at all we didn't have the arima model we only had the seasonal arima terms then X of T is only going to be having seasonal dependence that means your X at a current time T is dependent on the X at time L instance before X at time to L instance before and all the way up to PL instance before this is what the P term says and then the moving average is similarly saying that it will have a noise term, which is now plus a noise term, which is L instance ago all the way. y of t from x of t by doing difference operations capital d times and so if i write it here your y of t is equal to x of t minus x of t minus l instead of t minus 1 it is t minus l this is for difference operation of 1 but if you have to do difference operation with a capital d you are doing this d times you are bringing in a y d of t which is y d minus 1 of t minus y d minus 1 of t minus l and d minus 1 is similarly defined in terms of d minus 2 and all that and this is here uh y 1 so something like that okay so this is so this is the seasonal is it clear it's a lot of notation but conceptually not difficult so what we are saying is the d and l usage the last okay you got confused okay so okay let me try to re restate whatever i am saying so in arima model so first of all you have an auto regression model in auto regression you have p terms in the past right and then you have the moving average model where you choose with q terms okay q terms which are noise terms why am i saying noise terms because these are are unknown. This part is clear, I hope. And then we have the integration where we are doing D difference operations to bring in stationarity. So this was so far as Arima was concerned. in the seasonal arima we have similarly we have a capital p capital q and capital d apart from an l l is the seasonal period okay where the same whatever was p for p terms in the past is p terms in the seasonal past that means what you are saying is january uh let us say we are looking at uh i don't know uh january average temperature the jan january 2021 value is going to be the january 2020 along with january 2019 along with january like that you are only looking at the januaries so that is what and how many terms you are considering that is the p the q is uh similarly to the with the noise terms how many noise terms in the past uh for january of uh like let us say think of the concept of marks obtained so how many marks obtained in january of 2021 uh depends on effort put in january 2021 plus effort put in january 2020 i mean it's not a very uh meaningful model but let us assume that there is a seasonality in effort put in so then it it's that is the q here and similarly d is the number of difference operations in the seasonal past so this is the seasonal arima and both of them combine to form actually the s arima when you say it it means both the parameters p q d as well as capital p capital p capital d and l okay so your s arima when you are actually applying s arima in a for a time series if you go to any of the packages and apply it they will take in all these hyper parameters so it's a huge number of hyper parameters p q d capital p capital q capital d capital l so there is a lot of tinkering around that needs to be done so there has to be some level of understanding of when to use what parameter and there is something called auto arima in auto arima it actually automatically tries out all of these parameters and comes up with the one which fits the best ultimately what fits the best is what is the right model, right? So, yeah, this is, we will cover all this when we do the lab. There will be some examples where we are using, I mean, once you use it in a lab, right, it just becomes two or three lines of code. The theory behind this is probably a bit messy not difficult but too many variables and probably trying to what all these terms mean i mean when you are looking at the lab you will not get confused now because you know that what each of these terms mean but they are automatically done by some of these packages so auto arima actually what it does is it decides what is the best value for pqd and similarly for l and other things so that is what auto arima does but i think i will stop here. In the next class, we will start with profit, which is basically a package designed by Facebook. That is a nice paper. If you want to read it, you can go through it. We lose audio. Okay, we are not listening. Chander's connection seems to have broken here. He disconnected. I heard him say something about connection issues, so I'll just hang tight. Okay. Do you know, is he located here in US or is he dying from? Yeah. I'm sure. Oh, yeah, he's in. He's at Bangalore. Yeah. yeah he's in he's at bangalore yeah is he the same guy who took you know that paper reading once yeah okay yeah he's presented before okay each other you're back i think you're on mute channel hi um i think there was a brief outage of electricity. I don't know where I lost you. You were just saying how you're going to introduce us to the Facebook package. Okay, okay, great. Okay, yeah, yeah, that is when I think I had the outage. Sorry about it. Are you able to see my screen still or not yet? We see you. Okay, not the screen yet. Okay yeah what i was uh yeah i was just coming yeah i think you are able to see my screen now right yes yes okay so um what i will be covering in the next class is um the profit package it it uh if you see i mean there was a lot of there are these a lot of these parameters pqd capital p capital q and all that profit avoids all that by making some basic assumptions so we will go through the brief theory so what what essentially what they are saying is most of the real real life scenarios they have a seasonality of yearly annual plus weekly so we don't need to probably try to tweak around too much with l i mean so profit makes this assumption that we have either annual or we have weekly and they are also throwing in the fact that we we have this occasional holidays okay holiday so the there's a holiday season which will basically if it's like something like a Super Bowl event a Super Bowl event will trigger something so that is the kind of intuition so they have annual weekly and then they have occasional holiday events which will trigger something probably significant and they are also accounting for piecewise linear trends. So let me write it piecewise linear trends plus piecewise saturating trends. So there are two kinds of trends they are modeling. So they are saying that we don't have to do all this complicated modeling which is having arbitrary number of p q d and capital p capital q capital d and capital l we just say that there are two kinds of seasonalities annual or weekly and there are occasional holiday events and there are piecewise linear trends that means some there may be a growth which is growing steadily for some period of time and then another growth another growth okay so that means there are finitely many kind of cut off events until which you have some kind of trend and then you have some other trend. So using some very basic assumptions, this profit package is able to do a model many scenarios far better. So we'll just cover it briefly. And more importantly, in the lab, we will show how it is covered. OK, so this is what will be covered in the next. Fine. I think I'll stop the recording now