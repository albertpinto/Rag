 All right guys, this is Saturday. We have gathered for our paper reading and the paper that we are going to read today together is this very interesting paper that is supposed to be anonymous by anonymous authors. The title of the paper is Images Word 16 by 16 Words Transformers for Image Recognition at Scale. That's the title of the paper. Very interestingly, it has been marked anonymous and this is part of the effort to try to do double blind studies. In other words, the reviewer shouldn't know who the authors are and authors shouldn't know who the reviewers are. Now, typically in research community, there has been a tradition that when you submit your paper for review, you do put your name there and next to your name is obviously the institution that you belong to. And then you don't know who the reviewers are. They are kept anonymous and you get their feedback. Though sometimes you can pretty much guess who are the people reviewing your work if your community, your specialization has a small community and you know all of them. Now, but the problem that people face in research communities, of course, is that we are inordinately influenced by who wrote the paper. You see a major scientist or researcher and we tend to get influenced by it. We are much more likely to approve that paper, even if it may have flaws sometimes and we are not likely to encourage authors or we are very strict with authors whose names we have never heard of and especially with this dominant research labs like google and facebook anything coming out of them but there's always a general feeling that it is of higher quality than let's say a research that's been written by somebody from somewhere, you know, unheard of place or something like that. So people are trying to rectify it. It's a little bit hard because while the intention is good, quite often you can guess where the paper is coming from just by looking at the references the reason is that every paper builds on prior work because it builds on prior work the researchers when they write paper they tend to use their previous images previous graphs and so on and so forth and so one can quickly look at the published paper if you are up to speed on that you can guess who the author is but nonetheless it's i suppose a movement in the right direction to do that we are these days inordinately influenced by the big by accredited by the big houses big research labs and uh perhaps other people don't get as much attention as they deserve so it's sort of a rich gets richer mentality that has crept into the research community, which isn't good. People are trying to fix that. Asif, one moment. I think when you picked the paper, it was under anonymous, but I think since then they have updated it. So the folks who downloaded it today are able to see the authors. Okay. And the authors are all from Google, isn't it? Yeah, it's Google Research and Brain Team. That's right, which is pretty much easy to guess even without that. So very, very interesting. They have updated the names. The general idea was that there is actually some effort to try and bring about a more fair standard. Anyway, that's that. Thank you for pointing that out. The paper is now updated. So I think I do certainly have an older version of it. So what does this paper say, by the way? So with that preface, this paper is very interesting, actually. It goes to what a transformer can do. The transformers initially arose in, if you really think the roots of the transformers were, they were there to, or in the initial days, people were coupling it with LSTMs, right? The LSTMs and the RNNs, and they would augment those things to create your encoder decoder and things like that in classifiers so they could hold a long longer range context and then and it dates back to 2014 but then over the years and then you have image visual transformers so the basic the way I think of transformers and I think I should just mention it, the basic idea of the transformer is like this. You have, suppose you have an image. And there is a hut here. And you ask in this image, and then there are other things, some distractions here, and you ask this, you want to find out whether it is a house or hut or a dog, or what it is, and between this hut or cottage, let's say, cottage, and this dog, which one it is. And so the initial idea of transformer is actually pretty elegant what it basically says is that let's take a color is that you, you pick a region, so you pick a region, maybe i'll take this where attention is focused. You you take this region, I think, in this the heart has been lost i'll read the other. You take this region, I think in this the hut has been lost, I'll read the other hut. So you pick the region where attention is and then you defocus other things around it. Those of you who are used to photography, the one very intuitive notion, it may maybe maybe it's slightly wrong but i have in my mind is if you take your camera and you take the shutter i mean you you put the f stop something like 1.2 or very low like 1.8 or these are the standard numbers you find in 2.8, usually good, for example, lenses, good fast lenses, would contain this very high f-stop, a very sort of open f-stop numbers. So when you do that, what do you notice? That if there's a person, only the face or whatever you're focusing on is in focus, and the rest of the background has sort of blurred itself out so that is called a bouquet effect okay exactly that is the word bouquet it is called a bouquet and people pay a lot of attention to get a soft bouquet in photography right so for example a lens if you go and get a Canon lens which is is a 1.2 L, 85 millimeter, I believe, that comes approximately to 2K, a little more than $2,000 and so forth. So people pay a lot for that kind of thing, because it forces, when you take a picture from that camera, it focuses your attention to a spot. And I've always felt that in many ways attention is that. Attention is asking that suppose you have an image, let me call this image IMG, you're taking this IMG and you're multiplying it with some sort of a ring of attention, a ring of point plus bouquet effect and the way you do that is imagine that you have a bell curve so i'll just draw a bell curve so if you look at this page like this and let's say that the hut is here what you do is you multiply it by a let me give you a very simple intuition imagine that you are multiplying it by a bell distribution a gaussian distribution which is centered on the house in that hut here when you multiply these two together what will happen is you will in a sense get something like this right and in many ways you can say that this is your attention, this multiplication, this way is attention. It has figured out that we have to pay attention to this region of the picture. That's a way to look at attention in the world of words. The similar idea comes, suppose you have words A, B, C, D and A, B, C, D. The big concept of attention that came about it was self-attention so what happens is suppose your sentence is a b c d then the question is if you think of like a as the observer observing the entire sentence then where is then where is the attention of A being focused? Somewhat like this. Where is the beam of attention going? And maybe sometimes it is at two different places. So how much is it being divided between these different words in the sentence? From which we derive the word, the self-attention part. And then like this. And the breakthrough that came obviously in 2018 was the attention is all you need paper the well celebrated paper was that you don't need rnns you don't need lstms you can just use attention and attention somehow are able to capture the encode the context the thought vector there present in a given sentence, and you can move forward with that. Now, I use the word somehow. It's very interesting actually how attentions work. In terms of that attentions, when you look at the attention, for example, when you look at BERT, one of the transformer, attention based systems are colloquially, I i mean we say that they are transformers often so these transformers but in particular uh very interesting when you see how they are behaving so what happens is when you stack a lot of these um there's there's attention blocks together a1 a2 a3 therefore you notice something very interesting with words actually. What you notice is that they pretty much rediscover the field of, in some sense, computational linguistics internally without having been taught that. They rediscover on their own. So you see them forming word formations and the connections within the transitive verb and the noun or transitive verb object and the sort of things that I was talking about. In a very rough way, they seem to have figured that out layer after layer and built higher levels of abstraction, the whole point being the higher levels of representation they build it that is another paper actually I think it is worth covering and as part of NLP certainly it's worthy so maybe I'll do it in one of these weeks so now the history of the field is those transformers have taken the world of NLP by storm the paper came out in December 2017 and it was sort of a watershed moment. After that, NLP has gone through radical change. Radical change. Almost everything it has sort of pushed the LSTNs and RNNs over in many places, not all places, many places, and then taken over as the state of the art approach to problem solving. So people have asked to what extent we can apply transformers to images. Can it potentially do something like that in the world of images? So in the world of images, if you remember, something like that in the world of images. So in the world of images, if you remember, we have the congulation neural networks that we did in part one. Congulation neural nets, if you just to recall what were the great strengths, first of all, they were the number of connections you needed between two layers was much less. The number of parameters was much less because of the, so what we call the spatial invariance. The same filter you would slide over an image like this and it keep picking up. And so that was a very nice thing. And not only that, you asked the CNN to learn those filters. You didn't say that this is a filter, this is a wavelet filter this one will pick up a diagonal this one will pick up a vertical edge and so on and so forth you would not do that right which used to be the world before as deep cnn's but you instead said let the neural network discover what the best filter should be for these images for this particular task so even the filter weights were learned parameters were learned to back propagation now the if you ask what was the reasoning or how did it work in some sense see when you create a deeply connected layer one layer deeply connected to other layer all the weights you say learn all the weights right you know that there are lots of parameters lots of weights interconnecting them uh what i mean layer deeply connected to another layer all the weights you say learn all the weights right you know that there are lots of parameters lots of weights interconnecting them uh what i mean by this is let's say that you have a situation like this uh suppose you have nodes like this and notes like this in a deeply connected layer everything is interconnected right And before you know it, it's a just a, I will just, it becomes a, like a very, you see how dense this thing is. It's a dense set of interconnections. And so there are a lot of parameters to learn and you have made no assumption whatsoever about what the weights are. But if you think of CNN in some sense, what CNN says is that suppose I have a filter that sort of is looking for edges. So one, one, one, one, one, one, one, one, one, one, one, one, one, one, one, one, one, or something like that, you try to apply it to an image looking for a vertical edge. Besides the fact of what the shape is, what you're making an assumption is, and the fancy word you often use in machine learning is, that you come with some extra bit of information. When you bring some extra bit of knowledge to the table before you start the theory before you start your design you can accelerate or make the process more efficient and again those of you who did uh bias and learning you know the word for that is to say that you had a good buy a good prior a good inductive prior of prior prior hypothesis. In some sense, the prior hypothesis that you make in a picture is, suppose there is a hut here, that in determining whether this is a hut, this pixel needs to only look let me take another color this pixel needs to only pay attention to the other pixels near it that is what a filter does right a filter will sit upon it and then the information is contained at a pixel much of it by looking at the context in the neighborhood of that pixel right whether it's an edge or not, and so on and so forth. So there's a locality. The filter gives you a very local information. And it has been very effective. One way to think why filters with much less parameters are much more effective, and CNNs are much more effective for image processing, you can say that it doesn't just come with an empty slate, it comes with a good prior hypothesis that local pixels are paying attention to other pixels around it in some sense, that if you take a pixel and its neighboring pixels, you will get something of relevance, you'll pick up a feature. Take a pixel, look at the pixels around it, and you may pick up a feature. Take a pixel, look at the pixels around it, and you may pick up a feature. That's what Appentor does. And that leads to a lot of efficiency, of course. So in the world of images, CNNs have been the state of the art. They have done extremely well. And we did all of those architectures, the Inceptions, the VGG, and so forth. And there were many interesting ideas that came in along the way. The ResNet, which broughtGG and so forth. And there were many interesting ideas that came in along the way, the ResNet which brought residual connections now which are there everywhere in transformers everywhere. Now we do residual connection. So many, many big ideas came from the world of CNN. Now, recently there has been a question. You have CNNs for images and you have transformers and attention-based mechanisms for words, for natural language processing. And what this paper does is it actually asks this question, can we apply transformers to images? So let us reason to that, and that will give us the gist of the idea in this paper. Suppose you have a pic, you have an image. So you know that suppose you take an image which is 28 cross 28, your basic MNIST image, right, or you go even further. So this is what 764, right. So every, so if you treat each of these pixels and pixel here, this pixel, let me just call it x1, this pixel. Now, what happens is that when you look at this pixel, if you treat a pixel, a pixel, you map it to a word and you map this entire image. to a word and you map this entire image, you unroll it into a single linear vector and you make it the image, you, all the way to n words. And then, so it has to pay self-attention to itself. And so here, n is equal to 764, because the image is n is equal to 764 in the case of your basic, this MNIST example. Now, or ImageNet if you take 32 by 32 or so on and so forth. But you realize that it is a quadratic problem. You know, each of these pixels, it is paying attention to too many, like it has to figure out this. So the computational cost is order n square but do you see that guys each word needs to figure out which other word which other words to pay how much attention to in the world of pixel it means that each pixel needs to figure out how much attention it is paying to the other pixels is that argument clear guys? Are we making sense? Yes. It's a very simple argument that each of these pixels is paying attention. It needs to figure out this. And this has to do ultimately, and those of you who remember the attention paper, it has to do with the fact that whenever we do attention, ultimately it is kiqj, I think there is a dot, I will just write it as a dot product, up to a normalizing constant and then attention is, let me just put it this way, attention ij is softmax, marks that, and so attention on j therefore becomes just a ij, attention, that attention jth word pays to each of the words vi. Like how much attention it pays to word vi is just the dot product of these two. If you remember, we talked about that, but i'll just stick with this and if you look at this you realize that there are n values of i and n values of j and values of j and so this this matrix is n squared order n square matrix it's quite literally it's an n square matrix now there is a simplification because dot products are symmetric so you don't need to take all n square you can just go look at the either the lower half or the upper half of the matrix. It would be symmetric. But anyway, those are small mathematical details. We'll ignore that. The main point that we will look at is the fact that this is a rather quadratic computation. Now, what's wrong with quadratic computation? Would somebody like to talk about it? Why do we not like very quadratic computations? Costly first. It is expensive. So just see that 28 by 28 which by the way today would be modest. Nobody would buy a camera that only took pictures 28 by 28, which by the way, today would be modest. Nobody would buy a camera that only took pictures 28 by 28, right? Suppose you take even reasonably small image, a 256 by 256. You realize that this is approximately 100 times 100 is 10 to the power of four, isn't it? The number will be somewhere in the order of whatever it is. Let me just guess that it is something six into 10 to the power four, approximately 60,000 pixels or something. I hope my computation is right, definitely. So that's that, approximately six into 10. So you realize that even small images lead you to very large attention matrices. And so that's been a problem. That is a problem that you address in many, many ways. Of course, the next paper we are going to do is the performer paper, which generally tries to address it at the core level of the transformer itself and says, how do we speed up the transformer by not doing this quadratic computation, but instead replacing this computation with a linear computation and the way it does that is very interesting and those of you who with ml 200 background i talked about the kernel methods and so forth very very interesting connection to that but that's for next week this paper is actually quite straightforward what it says and it's so very elegantly uh stated it just says that what about this guys let's not a word need not be the entire just pixel but what we will do is we will take the image let's say that this is the image and there is a let me make a big thing here let me make a big thing here what should I make let me make a diagram that all children eventually make and then there is usually a sun shining in there and this is usually a green hill and what what else do i do maybe i'll flow a river down here while you are at it a blue river down here some bird salsas oh yeah some birds also right what what use are kids drawing so if we don't put birds in them so yes some birds yeah and then surely enough there is a cottage here There's some birds here. And then surely enough, there is a cottage here. Cottage here and there is even a tree somewhere growing. Let's grow a tree. Well, I'll grow a red colored tree. Okay, so this is, we have a picture now, right? And maybe a tree popping out from this direction also. There we go. We have a nice kid's drawing here. I'm sure that all of us have in our childhood drawn some picture, some variation of this picture. Let's add a chimney also. So now the question is, what is this paper trying to say? It is saying that, see what we need to do, let me take a color I've not used well I've you I simply used all the colors maybe orange I'll take the orange pen more colors already let me take orange color okay so suppose I take this it's still orange why did it not become orange uh orange yes good and the thickness of this good so what you do is you break this picture into grids now this is not bright enough let's take something brighter this here so suppose you take this image and you break it up into square grids. And you ask yourself this interesting question that maybe with all these pixels there, it doesn't quite matter that we look at each pixel. What if we just paid attention just look at this look at this when you take actually zoom out a little bit and are you seeing where i'm putting my uh marker here i'll just mark it even with red also in the center what does it capture it captures the sun isn't it the sun peeping behind hills. And in many ways, it has more sort of semantic meaning. It has more, it is more representative of something we understand in many ways, isn't it? And likewise for other pieces, by taking pieces, it begins to look like, what should I say? You know, we do the puzzles. All of us sit down in the holidays, we do puzzles, right? And over the weekends, we do puzzles with children. Those puzzle pieces, all those pieces come, whatever. And we fit those pieces together. And I think this is my interpretation, and obviously I'm going out on Olympia, is that in many ways, this paper is suggesting that why don't we dial up the. Raja Ayyanar? picture with a sort of puzzle pieces and then just ask each piece which other piece, it needs to pay attention to. Raja Ayyanar? Now the very difference from the puzzle is in the puzzle you ask which is the neighboring piece, but here we ask a different question. puzzle you ask which is the neighboring piece but here we ask a different question suppose that the purpose of this puzzle is that you don't have to fit it together but you just have to tell is it a house is there a house in the picture or not right or is there a sun in the picture or not or something like that what are the objects you find in the picture any one of these image processing tasks you do so then what happens is if you really think about it look at this hill this hill will pay attention to all this other far off pixels with hills and so forth right and this pixel this top will pay attention to this other bird right and it will pay attention to this bird right should to argue that you know we are looking at a bird also somewhere in there right so that is the essential idea it basically says that if you make it a bit more coarse grained then what you can do is i treat these pieces as words your words are so uh let me just call the new mapping now. The new mapping is, let me do it this way. So the new mapping is a tile, puzzle piece, if you call it a puzzle piece, if you so call it. Puzzle piece maps to a word. And obviously the image maps to a sentence as of course usual sentence so what happens is when you do it like that you realize that obviously the number of puzzle pieces is fairly modest right it is not so big would you agree isn't that nice i think this is not pixel right so it's just a blocks it's just a block. It's just a block of pixels, square block of pixels. Right. So suppose there are, let's take, let's make it very real. Suppose this is 256 pixels deep, right? And you took, and in fact, this is exactly what they did. They took a tile, I believe it's 128 they played with, but okay, some number, it doesn't matter. They took a tile i believe it's 128 they played with but okay some number it doesn't matter they took a tile of 16 by 16 pixels right so how many pixels will you have uh let's say okay let's make it even smaller just to illustrate the point suppose it is 128 right so what happens you have just six pixel here six pixels deep and six pixels. So you have just 36 tiles. Would you agree that you will be left with only 36 tiles? Praveen, you got that right? Yes, sir. That is it. So each tile basically is a coarse grained square like thing. And when you use that, you make that into a sentence, then what happens is, what you're asking is, what's the relationship between the tiles? Which tile should pay attention to what? And which tile should get a lot of attention if I'm trying to determine whether it's day or night? So you see the sun peeping out, so it probably means it's day. That's sort of the argument. You bring back the whole attention thinking into this. And then the rest of the paper is actually straightforward. There is a bit of mathematics to that and I'll explain, but it's very well sort of argued, simple argument and very nice actually. So they go on to say that suppose we tile it, they call this the vision transformer our transformer of images you just break it up now one question that comes in attention is all you need if you remember that that when you create a transformer for every word you had two pieces you had the word embedding and you had the position embedding you remember that position embedding and we did the position the original people did the position embedding very interestingly using all sorts of high frequency sine waves but here it turns out that you can do an interesting game you can say learn the embedding just let's run this entire game and find out learn even this learn the position and then so see you could if you look if you really look at this tile let me just take this you could index it by its row and column so for example you could say this is zero zero zero one one one this is a zero this is one. This is a 0, this is 1, 0, 2, 0. You got the idea, right? This is 2, 2. You could do it like that. Or you could do something like this. 1, 2, I don't know, 1, 2, 3, 4, 5, 6, 7, 8, 9. Oh, actually I started with 0. I should have started with 0. So something like this. Any one of these you could use as a position encoding for a tile. But what they did is they said, okay, why don't we just let the neural network learn the position encoding also of the word, so-called word, or the tile. And what they found was actually, and this is a picture that is worth looking into, Somewhere here in the paper, it is there. But here it is. Oh yes, here it is. Let me make this a bit bigger. So this picture is worth looking into. I don't know if it is visible, but if you look for the, you know where my mouse is, position embedding similarity and so forth, look at where the yellow dot is the shiny yellow dot is would you agree that this is in the top in the first one top left hand corner the yellow dot is literally in the top left hand corner of that little tile maybe i'll make it Yes, guys, just pay attention to this guy. Oh, maybe I'll make it thinner. Pay attention to this guy. Where is the yellow dot in this guy? Do you notice that it is in the top left hand corner? Yes. It is there. And do you notice that as you move to the next one, it has shifted one block. So what it has done is it has given you a very good positional encoding. It has automatically learned where the style belongs in the picture in a very intuitive way. That's very nice. The other thing that happens is, let's see where it's paying attention to when you do this kind of thinking yes do you notice that you have all of these things coming up like look at this. This could literally have been one of your CNN filters, right? Any one of these if you look at this, for example, any one of these, do they look like your standard CNN filters? If you're doing image processing, that's how the filters look. That's how they ultimately end up becoming the masks or the filters end up becoming. So the interesting thing with this paper is what it shows is that a transformer essentially learns to be a filter. So you say well, well and good, so what use is this? Why is this? Should I really care about? And the answer to that is that it contains the one feature that is there in this last image see this is an interesting thing see what happens is that if you look at a filter f1 what is it paying attention to it there a pixel here it is paying attention only to pixels um pixels around it right so what happens is that the if you ask do you how long range attention you have not much you know pixels pay attention only to locally now here the interesting thing that happens is you know you don't notice that most of the attention is to local pixel you notice that some pixels are paying attention to pretty far off pixel in fact as you go deeper and deeper into the uh more and more layers but that part you can ignore for the time being. Just imagine that you have a single encoder layer, I think this attention layer, it is able to pay attention not just to nearby pixels, but it, nearby tiles, but it is able to pay attention to rather far off tiles. Right? And so it is the pixels that they are made up of they're paying attention to a pixels that are quite far off and that is an interesting or remarkable thing that you you are able to pay attention to far off pixels in the in the picture and so that gives you a clue that perhaps it may outperform the filter filter has the limitation that the the focus is very limited around the filter and then you scale it up as you go deeper and deeper into the layer the coarse grain let's say that uh there's this this will now pay attention to a little bit bigger region and so forth uh those things add up eventually uh that's a representation learning, but the, how should I say, okay, I think that's the way to say that, that with attention you notice that you not only have, I didn't give in there, paying attention to the nearby stuff, but you're paying attention also to the far off stuff. And ultimately, if you think about it, that was the virtue of the attention over the LSTMs, the RNNs also. Attention, those RNNs, etc. They could not hold long-range context, whereas attentions could. And the same thing seems to be happening with the images. So that is it. I mean, if you get that idea, I think as far as I can make out. One moment before you move from the picture. So that network depth layer visual that they have in the paper, how are they conveying the intuition that the attention is actually across multiple layers? I'm failing to make the connection. No, no, no. These layers are, see what happens is you put attention layer one, right? A1, then you put A2, then you put A3, right? So what it is basically saying is, let's say that this guy is paid attention to these two, right, this cone, and then this is paying attention to these two. And now if this guy is paying attention to these two, right, then what happens? This guy is in effect paying attention to these two, right? But then what happens? This guy is in effect paying attention to pixels, indirectly to pixels that are pretty far off. This guy, A3, has now ended up paying attention to things that are much more far off. So what happens is that in the normal sense, for filters that effect holes, filters keep local attention a filter would have gone something like this perhaps but the whole idea is that see this is the point of detail why do you have many layers in a deep learning network because at each successive layer you're becoming more coarse green You have extracted a higher level of representation. In the early layers, you're looking for just edges and little things like that. But as you go to successive layers, you know those things, they come together and suddenly they begin to look like a house or a cat's face. Because you're gathering information or saturating information from a larger and larger region that you have accumulated from the prior small region studies are we getting that remember we did that in cnn the early layers the big very primitive uh geometric primitives edges and this and that the next layer begins to put those things together into a more structured thing. Next layer represents it as a higher order structure. And it continues forward. We'll see that again, actually. I believe we went rather fast in the first one, just very introductory level into CNN. But we have a whole month of CNN coming where we'll explore these ideas in great detail. We'll see it again. But at this moment you can take it as a fact that that's what you do. The whole point of deep, the deep in the deep learning is the reason we have so many layers is because each successive layer learns at a higher level of abstraction or representation compared to the previous layer. So I think when there two concepts here as if, so the CNN concept, yes, I'm kind of grokking that in terms of the, as you go deeper, there are higher levels of abstraction that is getting, the filters are essentially observing that, right? But now what we're doing in this paper is we are mapping that concept from the concept of attention, right? The Transformers concept. That's true. And the part that I'm not making a connection on is they're using a visual to say that it is similar to what is happening in a CNN. That is true. It is similar because ultimately a filter is a form of attention, right? But it is a very local attention. A pixel is just looking at the pixels around it in some sense. When you see, when you apply, let's say that you have an image, big image, right? That same house and sun and so forth. Suppose you have a pixel that I have, let me draw a pixel, like the filter is sitting here at this particular moment. What happens? The center of the pixel is here. Pretty much you're detecting what's happening in this region, isn't it? This filter when sitting here is not seeing what is happening here on the hilltop. It is sitting upon a section of the house, and it is trying to do feature extraction from that little patch of the house. Now, the statement that is being made with attention is that attention, because there's this whole self-attention thing, this style may be paying attention for whatever reason. Well, OK, house in this case is more or less standalone. So this house is paying attention for whatever reason. Well, okay, house in this case is more or less standalone. So this house is paying attention to other regions of the house. But this, let's say, bird here may be paying attention to this other bird and this other bird far off, which with a filter you can't do. Long range attention you can't do with filters and CNS. And that's what a transformer is able to do. That is all it is saying. So think about a filter. But if you take a filter with a bird, maybe at a higher level of deeper depth, it will say bird. But this bird has nothing to do with the next word right so uh i don't know it's a little it's like that attention in some sense has more uh content than a filter and that is the that is the main statement being made so there's a's actually, let's go through this paper. Other . Yes. And one second. And when you look at the performance of this, it turns out that in accuracy, it has two great things. First, it is actually computationally, it seems to be at far simpler than at CNN. So you don't add complexity to the whole thing. But at the same time, it is actually more accurate, comparatively or more accurate. Occasionally, sometimes it is not, and sometimes it is and so forth. But it is pretty state of the art performance you get with the natural images, real images, like MNIST and so forth. And that is the gist of what they're saying. Go ahead, somebody had a question? Yes. The question was on similar line there. What is the advantage? Little bit louder. I can't hear you. Yeah, question was said that what was the advantage using say, this attention mechanism for images in comparison to CNN so CNN can we understood it, understand it this way that in CNN since it pays attention locally there has to have multiple filters in this case probably less because it can you know attend to some information which is little far because of the deep layer connections. Yeah so Pradeep here's the thing I suppose this is let's read the abstract here because it says that just what they're trying to say is captured in their um this let's read the abstract they're saying that while transformer architecture has become the de facto standard for natural language processing tasks, its application to computer vision remains limited. In vision, attention is either applied in conjunction with congulational networks, in other words, people add a little bit of attention along with CNNs, or used to replace certain components of their CNN while keeping their overall structure in place. We show that this reliance on CNN is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to a sequence of image patches. So Pradeep, you remember the old paper, attention is all you need when you say that for natural language processing. What are you saying there? you don't need rnns basically exactly right and in a way this is saying something each unit it basically says that hey guys you use cnns and occasionally you throw in some attention into the cnn architecture but we show that you can get rid of cnn altogether that's right and so this would be i would say that the same statement attention is all you need. Even in the world of computer vision, that's a big statement, because nobody observed that. So that is a breakthrough. Then he goes on to say, we show that this reliance is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to a sequence of image patches. So that is the crucial word, when applied to a sequence of image patches. So that is the crucial word, when applied to a sequence of image patches, because image patches help you sort of tame the quadratic computation problem in some sense, right? Not only that, in many ways it has more context in it. When pre-training on large amounts of data and transfer to multiple recognition benchmarks, so these are your standard benchmarks image net c500 you're all familiar with this so then uh vita we haven't used with the image so it says that the vision so now they're using the word vision transformer they're calling it so vision transformer attains excellent results compared to this state-of-the-art condylational neural network while requiring substantially few computations so if you were to say what are the breakthroughs that this one mentions so in some sense what they are saying here this statement right this part you can think of it as something like attention or transformer is all you need actually it may not just because there are many layers of attention so just we can call it a vision transformer now this paper could also have been called vision transformers are all you need. Sir, one more observation. No, no, once we can finish. Let me finish the thought. One second. So this is one big value that it is bringing, right, in the paper. And they have said vision transformers is made up of patches of images. of patches of images. Attention on patches of images. Now the next thing that it goes on to say is that you achieve excellent results. I mean you achieve the state-of-the-art results so you don't lose anything or it is not just a novelty that you can get away with the vision transformers. It turns out that you actually meet or beat the state of the art results. But the other important thing is while requiring substantially less computational resources. The whole point of CNN was that it needed far fewer resources than very dense networks, which are impossible to train because the number of parameters. fewer resources than very dense networks which are impossible to train because the number of parameters now what they are seeing is that even less than the filters it needs much less computational resource to it right so this is the point number one point number two is there right so these are the two points so go ahead yes sir so my next observation was regarding point number two. I was thinking that maybe this will need less information in the sense that since it pays attention to far-off information, it can make up the information which is missing. Exactly. See, what happens with CNN is you need a lot of layers because it is very local. So a more global structure, a long range thing, you have to lot of layers because it is very local so a more global structure a long range thing right you have to go many layers deep before this guy can see put two and two together from distance distant you know things it has learned right but in our attention in the very first layer itself you can go longer distance and that is the essential that is why it is relevant That is the essential, that is why it is relevant. And another way to look at it is, guys, see, transformers are, we are realizing that it isn't something specific to our natural language processing. More and more, we are realizing that transformers are the crucial building block of neural networks. Fundamental building block. It is not to do with natural language processing or computer vision. It is attention. Attention is the right way to think about neural networks. You put attention blocks together, right, and you build a neural network. In some sense, if I can make a hand-waving argument, I would say that, see, at a very basic level, we think that a neural network is made up of nodes. Nodes and interconnection between nodes and layers, right? That is a very physical way of building a network, but slightly a different representation of way of building, thinking about a problem domain or a network is, you can think that you're building it out of more force-grain, these attention blocks, or self-attention blocks, and you're layering them together, which is of course the transformer architecture. You are using that, and it is general purpose enough that it can solve any problem that you could solve with other architectures. So what it is saying is that do what you do, but treat the transformers or these self-attention blocks with the underlying them as your building block. To me it seems that that is the direction to which all of these guys are heading. to which all of these guys are heading. So, Asif, one quick question on them. I think I was missing a certain backdrop under which I had to process this paper. And the backdrop, the way I'm understanding it is two statements I'm gonna make. One, here is an assumption. I'm assuming that the research that has happened around transformers and the extent of knowledge we have around how transformers can be used is far larger than cnn's oh no that no that is not true cnn's are by now classic but there is at least 20 25 years of study of transform of cnn's i mean the uh the initial leonet that was already doing optical character recognition of your US Postal Service and so forth. They have a heritage of almost 20 years now. So, students are deeply understood, very, very deeply and well understood. Transformers are the new kid on the block. We are still figuring out what this thing is. And every time we learn a little bit more, we realize that actually it's more powerful than we thought. Okay, I think then that rules out my first prior now. The second then one is CNNs need a lot more computational resources compared to transformers. So if we make an insight that a transformer will do a good job in terms of detecting images, then we have come to something that needs less resources. That is true, yes and no. See, here's the thing, CNNs are very effective. Any one layer, they have far less parameters. Orders of magnitude less parameters, they're not dense neural net neural net they're simple uh you know every node is connected to every other node in the layer before and after so that in some sense you're saying that you're building a network with no prior like no prior assumptions at all when you do uh and people often use the word inductive prior and such things in biasing but see uh when you look at cnn it makes a fairly simple hypothesis that for a pixel, the neighboring pixels tell a story what it is. If you want to know whether that pixel is sitting on an edge, you just look at the neighboring pixels, and you will know whether it's an edge pixel or not. It is the quality of the pixel in the edge pixel or not, featuring that. is the quality of the pixel in the edge pixel or not, featuring that. Then it was actually extremely efficient. You can run a very deep CNNs fairly efficiently. What this paper is saying that when you use transformers, very surprisingly, you don't use more computational resources than a CNN. You actually use less and say substantially fewer, they say, but I would say the way I would interpret it is that see CNNs take a much longer time, many layers deep and higher levels of representation, because they can capture the long distance or the global structure. distance on the global structure whereas attention from the get-go from each attention layer in the transformer itself already is looking at long range and that is why i would if i had to guess why it takes even less resources than cnn i would say that that is the reason that it's taking less okay less resources now this is this paper. When you couple it with the next paper that we are going to talk about, the performer paper, it becomes even more interesting. See the problem with transformers have been that the only people who seem to be able to train these very large models like GPT-2 and the big bird, et cetera, and all these big birds, things like that belong to the people who are extremely rich computationally like the likes of google right who have uh 10 000 who can easily throw without thinking about it or 10 000 machines that training a transformer and run it for and fill those machines with you know gazillions of uh tensor of tensor cores and run it for a long time right so the reason for that expensiveness is has always been this this one problem the attention block aij is basically when you look at the softmax again as i said k i q j the dot product between these two right and up to normalizing constant uh i will i won't go into that but basically it has to do with this dot product quadratic dot product right and now one of the next breakthrough therefore is very contextual, the next paper that I'll talk about is very contextual. They have figured out a new mechanism to linearize it. So what it means if you really put this paper and the next one together, it basically says that even as we are, this transformer, vision transformers are more efficient than CNNs. But when you put it together with the fact that transformer itself has gone through a way to make it faster, now you're looking at significant breakthroughs. You know how these breakthroughs sort of feed into each other and lead to very dramatic results. So I'm hoping that soon people will say that performer, vision performer in effect, if you put the two together, should be radically faster or just a magnitude faster at image tasks and so forth. And so the way I hope is that, see, there is a thing. And OK, so this is now. So far it is fact. Now comes some bit of personal reflection which is a bit idiosyncratic and may or may not be relevant but it's my way of looking at it there used to be a physicist named dirac right um direct was uh interesting fellow he sat down and he said that i'm seeing a lot of very messy equations happening when people try to put quantum mechanics of Schrodinger and Heisenberg together with Einstein's theory of special relativity. And so he asked this question that how can we put these two together in a simple way. So people had all sorts of complicated equations they were playing with this and that and so forth. He sat down and wrote a very simple equation, I won't get into it, it has to do with symmetry arguments, SU symmetry, SU crosses. So he sat down and wrote something very simple. And the equation, the beauty of the equation is it is so elegant and so simple. The argument is just you look at it and you smile that wow, if only it is true and Dirac then confidently, he looked at this and he says that something so beautiful, famously he said, must be true, that truth and beauty go hand in hand. But there was a huge problem with that. Dirac's theory unfortunately posited that for every particle in nature, like electron, proton, neutrons that were known, there is something the opposite of it. It is an antimatter. Now we call that antimatter. So for electron, which has a positive has a positive negative charge there is a cousin or a twin which is exactly like the electron except that it has a positive charge. And so for every particle there's a opposite particle antimatter and if you bring the two together they'll annihilate and annihilate into a massive flash of light or gamma rays or something right gamma rays. So well at at that time, nobody had ever thought of antimatter, seen antimatter. It almost looked silly to postulate a whole alternate universe of particles and matter that nobody had seen or even tried to see or gotten the least hint of. And nonetheless, he stood his ground, and he says that so be it. I think those antimatter exists. A pretty big statement, bold statement. And as it so happened, soon after people did experiments and positron, which is antimatter of electron, was discovered. So and he used to in all his life say that if a theory is not elegant, it probably is not, you haven't hit upon the right theory, the right understanding. Both Richard P. Feynman, another physicist, and one of the people that I know is Chandrasekhar, the Indian physicist, a theoretical physicist, used to say the same thing. Actually, I remember long ago, once we we, I was just a fresh graduate student and Chandrashekhar had come and we had taken him out to lunch and we were all sitting there and some guy, some young grad student, he was the enthusiast of quantum field theory. Now quantum field theory and the standard model is very messy. It works, but it's very messy. Kuntur Kaurabas Bila. And it has all these quarks and this and that. And so that guy asked some question about this and at him with utter disgust and said, you know, this is not your theory of quarks. And the way he said quarks with utter disgust, I still remember. So he was another person who paid a lot of value, attention. He talked about truth and beauty also, I think, in some articles. And he said that the truth has to be beautiful. It has to be simple and elegant. If it is not simple and elegant, it's probably you're still making progress. You aren't there yet. And he really believed that with his heart and soul. And it showed actually. Well, anyway, I'm going into a digression. I'm done with this paper. So let me go into this little bit of digression. digression. He is a remarkable physicist and as a physicist of course I'm bringing forward the people I really admire. He's dead of course in Shekhar, he got the Nobel prize and he died much later after that. So but he is, he has a, as a teacher he has a unique distinction in history. He taught a course in the 1960s at the University of Chicago, one course in which, graduate school course, in which there were students and every single student of his went on to get a Nobel Prize. So think about a teacher who can produce a batch of people, each of whom don't just achieve, but they each go and make profound breakthroughs into the subject. And each of them won a Nobel Prize. Very interesting person, Shaitan. And if you read his lectures, I remember reading something on thermodynamics from him many, many years ago, 25, 30 years ago. I still marvel at the clarity and simplicity of his language but that is something you mess you know when you come to ai we are at this stage when i look at deep neural networks you find a lot of very elegant and powerful arguments but at the end of it you still get the feeling that the sausage is still baking and it's still in the oven the whole thing thing is not elegant yet. They're not very simple explanations, deep explanations for everything we do. We are hacking away and we are hacking our way through lot of computing resources. Just go, we can't make it work, throw more hardware at it and it will work. And then hardware is cheap, let's throw more hardware at it. And so because of the Moore's log, hardware computing capacity tends to double every 18 months to two years, depending upon how you look at it. And so if you can't solve it now, two years later you can solve it. It has been an approach, but in the process, other people are looking deeply and they're trying to say, there has to be a deeper explanation of why these neural networks work at all you know how is it that they work and how do you come up with good architectures so there's a lot of activity and it's fun actually in a way you're seeing it being born in some sense it is boring when the elegant theories have already been found and all you're doing is reading it it's fun to read but you don't get to contribute to it or make any Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam of neural networks and thinking and so forth, artificial intelligence, we are definitely not there yet. Our theories are messy. And there are a lot of hand-waving arguments. Mathematicians are very, quite literally, I know, friends who are disgusted with the field. They literally call a deep neural network that disgusting approach that actually works. And the terrible thing is it works much better than all the elegant theories that they had in the past. So what it means is, there is yet another more elegant formulation that people will discover in due time that explains why this big mess of layers and nodes and so on and so forth actually does work and this is it and that's why i like this series of paper because you can see there's you know step by step you seem to be making progress or all of these research labs seem to be making very good progress in that direction so let's see one quick segue again so we were in the theory context right this is a paper you're reading the research state of the art research right now in terms of how things are evolving i have a question i don't know if you have any knowledge there to share in terms of if you take let's say tesla as a car and any other autonomous driving technologies way more and so on do you have an idea of what is the architecture that they are using and what i'm trying to do in my mind is kind of get a baseline of what have people done, which is commercially viable. I mean, the Tesla today claims to be fully autonomous, right? With the latest update that they've done, which means they have with their seven cameras, they're gathering images and with the computing power that they're able to put in the car, they're able to say with a reasonable amount of confidence that the car can navigate through unknown situations. So now that car, if you take the Model 3, is being priced at roughly 35K. So somehow they're commercially fitted it into a 35K thing, which includes the wheels and the car and the seat and everything right running there so a primitive um i think i kind of noticed um the scene is like they're actually losing money on model 3s they make money on model s's yeah i mean i think i should have not said the price point i'm basically looking for a baseline and i was seeing i was looking for a thought there in terms of what kind of architectures are they doing and what level of computing uh machinery needs to be there inside the car to be able to support that yeah so it's a very good part so obviously i'm not a deep expert in tesla but to the extent that they are quite literally neighbors neighboring within walking distance of support vectors and and their ai team is very good very very good actually okay so i can only speak because obviously i don't have any information uh privy to any of their confidentialism but here some things are given you know the image segment, detecting objects on the road, etc. That theory is well established, they're using that. And then a lot of these, like for example, what actions to take, the reinforcement learning and all of that, it is all well established. They're not using anything way, way beyond what is well known in the research literature. The thing is, like every company, they benefit from breakthroughs, and they make breakthroughs and contribute to the open community. So the AI community is actually extraordinarily open in sharing and helping each other out in this evolution. But having said that, see, if you look at the recent breakthrough, Tesla, okay, I'll step back and say one statement. What Tesla is betting on, fundamentally, it's a big bet. They are saying that see, the automation sucks now. They know that. There's no two ways of it. Despite the hype and so forth, they know that it sucks now. But they are betting on the fact that the rate at which the breakthroughs are coming in AI, it will work tomorrow. They don't know what breakthroughs will take them there, but it will work tomorrow. So what they've done is they have already built their architecture on the future. They have designed the entire car platform on the assumption that one day automation will work. There is a reason why their dashboard is completely empty. Simple. The idea is that if the car is going to drive you from place A to place B, all you probably need to do is feed in the destination and then listen to the music. So if you take that as the eventual goal and you work backwards, you can see the architecture of the car. Now the car has multiple pieces. They need breakthrough in battery battery technology which people tend to forget actually that is the biggest bottleneck battery technology has not advanced or had fundamental breakthrough in many many decades people have tried many things everything looks very promising in the lab and then fizzles out but there has been a tremendous amount of incremental progress recently they claim that they have quite a breakthrough a small breakthrough mode so that is one area in the battery technology moving forward and becoming cheaper, packing a higher density of information. Today, there is no battery which comes close to the energy density of gasoline or diesel. The petrochemicals have an extremely high energy density. So that is one big part. They are betting the future that we will make breakthroughs and get there. The second thing, and also it will become cheaper and cheaper to make electricity because of the solar and all of these considerations. Then the second thing they're betting on is that automation will come. So everything that happens. Recently what they did is the seven camera breakthrough. And obviously, I'm not an expert from what I hear, but I read is that see you synthesize a global picture, you know, overview a bird's eye view out of the seven cameras, when you do the whole picture, and then you try to do segmentation, we may object detection, and so on and so forth. So they used to be and then you make decisions based on that they call it the problem of the local minima that they are solved they used to get stuck the algorithm you know we studied a gradient descent we talked about the fact that you get caught up in local minimum occasionally if you are not careful remember we talked about the last landscape and so forth. So the same problem, everybody faces that problem and apparently they were facing the problem and in the recent breakthrough they managed to overcome that and overcoming that has led to a huge incremental improvement in their pursuit of automation. But it is even now not like people have tried, the reviews are there, the reviews are somewhere between being completely awed by it in certain circumstances and completely scared by the mistakes it makes. So the things have not been ironed out, we are not there yet. In my view, we won't be there yet for a couple of years at least, if not more. So we are inching our way forward towards complete automation. Tesla is making breakthroughs and benefiting from all the breakthroughs that are coming. I don't know when we'll have, I mean, I wouldn't, for example, put the Tesla on full automation and drive. But then I tend to be conservative. I'm even scared of full adaptive cruise. I'm always a little bit worried that it'll go crash in the car. So that's where we are. I mean, that is how much I know. If anybody else has a comment or knows more, let's share it. I think you're saying something. Asif? Go ahead, Paikin. Yeah, so I was able to look it up before. They use PyTorch and they call the infrastructure HydraNet. So yeah, and then it's based off of the seven cameras. And instead of just getting inferences from each individual one, they're able to sort of piece in the entire picture from seven. Get a bird's eye view and then make a decision in that yes correct yes yes so i just posted the link uh in slack if anyone's interested yeah this is all good so um what is that see guys it's very exciting times it's both frustrating and exciting like as i said i wake up one day feeling wow such such great. Another day feeling frustrated that there's still fundamental problems. Many practical applications keep hitting a wall. In some sense, with full automation, it wasn't as easy a problem as people initially thought. But I think it's still a solvable problem. And progress will be incremental. See, one more thing is there. Other things contribute. Now the 5G network is there, so there'll be car-to-car communication. Many people believe that that was the missing piece for automation. If all the cars in your neighborhood on the highway are communicating with each other, they can negotiate their movements, coordinate and negotiate their movements on the highway so as not to go bumping into each other. And that may also be a huge improvement in our pursuit of automation. So let's see if 5G is still coming rather slowly at this moment, but it's still coming. So when I asked the question, I was kind of being a little focused on the context of the paper. So what was running in my mind is more about there is image processing that's needed and they have seven cameras they're able to do all the processing inside an onboard computer which they've kind of packaged inside the car right yes and if you look at the pricing of you know the deep learning computers that we are talking about right now it's coming to about 7 to 10k already right so they've managed to do everything put it all inside the car plus everything else that's needed to make the car somehow so what i'm intrigued by is how did they do that what are the price points of each of those things and how much amount of compute power is actually there oh massive they have a lot of GPUs stuck in there. They are open about it. Actually, you can look up the information, but if I remember, it's a multi-GPU configuration. And see with automation, with automotive industry, the standards are much higher. Government won't let you put things which are not rugged enough. So you can't just put anything into the car or automate it. So I don't know the full details, but I'm sure that it isn't just that you take ordinary GPU and stick it in. The prices get even more because you have to make it extremely rugged and reliable to be put into a car. All the microprocessors that go into the car, they go through a tremendous amount of government review so they must have passed all of those gates it must be prohibitively expensive see if you really think about it the car is made up of an induction motor right a chest a battery and then a battery is the floor induction motor that propels it and computer it is just three things a a computer, an induction motor, and batteries. The bulk of the cost. Induction motors are the great workhorses of our modern world. People don't appreciate how much the induction motor is doing all around our life. Wherever you look in your house, you'll see an induction motor, your mixie, everything. Your fan. Right? So then I think they have an induction motor or a DC motor. I don't know which one it is. Maybe DC motor. But induction motor, DC motors, all of these are highly reliable things. The price points are optimized that I don't think there's further optimization needed. Actually, that's a good question. I never thought about it. Do they use a DC motor, induction motor? If anybody knows, please tell me so um so anyway with that those are extremely reliable very very reliable then the only two games that you can play are the battery and the computer those are your major price points steel is steel it will the price will not change okay and those are minor areas of optimization. But the main thing is computer and the battery. Asif, for the inference, the amount of computing power that we need for training and versus inference, I think we had a discussion. Inference you don't need much. But you still need it because you're doing it in real time. You cannot have any latency. You have a computer that needs to make real-time guarantees. You see that? When you write software, let's say you write a web server and it takes traffic, if the web page gets delayed, suppose the latency increases to three seconds or two seconds, even one second, it doesn't matter. A one second latency in a self driven car is a recipe for disaster. So the inference engine has to be very fast. It is true that training a neural network will take gazillions of computers and compute time. But inference is not itself cheap when you look at mission critical environments you know this is like a self-driven car failure is not an option latency is not an option you need to be able to move way I would imagine and again I'm not an expert I would imagine that you need to make pretty strong latency guarantees there and so you need very high compute power in the machine, even for inferences. And also I heard from one of the leader technologists that, you know, I don't know if Tesla is including it, but the lack of positional, I mean, communication between car is good when it comes, but when, you know, in the the cops car when the red and the green i mean the red and the blue lights go up there is a dynamic effect created on the camera unit that it fails to no it hasn't been trained for it before so there are dynamic situations on the road for which it's not trained physically like the physics of the lights are not being taken care of where it Tesla fails very badly where you know they are thinking is the AI failing in those I mean it's not failing it's the training that's failing but you know they could bank on lidar and other technologies to fill this gap yeah actually people have people have researched areas where the AI or the Tesla car fails at self-driving. Some of them, and this, my information may be old. So one of this information was this. Suppose this is the highway. Are you guys seeing my writing? And here is their lanes, right? So their lanes. And suppose you're going on this car. You're following a car ahead of you. Let me just call this car X1, X and Y. so suppose you are y and x is moving forward and suppose here there is some construction zone then what happens is uh there is some a wall or something like that and if this car suddenly like this is keeping its eye on y is keeping its eye on x the computers and the x1 sees this and at the last minute it changes lane. Y is not able to respond. Y will just rush ahead and hit the wall. In fact, that is how some of the well-publicized accidents with Tesla happened. And so I think many people are researching the special cases where it fails, and I'm sure Tesla must be improving upon it. I don't know what the state of the art at this particular moment is. So there are quite a few situations, I wouldn't say full automation by any means. We're still a couple of years out. You see that right, people on the highway, they go to sleep. They're early commuters, they'll get into the car and they'll go off to sleep while the car is driving them to the destination on the highway. One hour of sleep they'll take out if they have a long commute. In my view, it's very dangerous.