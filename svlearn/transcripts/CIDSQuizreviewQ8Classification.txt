 This is quiz eight on classification. Question one, the acronym MLE in machine learning stands for? Does anyone want to answer? Does anyone want to answer? I think Anil you've chosen minimum likelihood estimation. Actually, I wanted to select that but I think for some reason it got selected incorrectly. Yeah yeah yeah it's an, of course. Which of the following are properties of independent and identically distributed observations? The occurrence of one observation does not affect the occurrence of another. So that is true. The occurrence of the nth observation affects the occurrence of the subsequent observations. That is not true. Each observation datum is drawn from the of the subsequent observations that is not true each observation datum is drawn from the same population so that is true each observation is the same that is not true in the classification context each observation has the same probability of belonging to the target category so a few people had selected this option as well um that is not true because in the raspberry and blueberry example, each observation has a different probability of belonging to the target category. But however, they are all drawn from the same population. So that's why this is not true. But others are, I mean, these two are true. While training a regressor or classifier on a dataset, it is assumed that the observations in the dataset are independent and identically distributed. So that is true, that is one basic assumption. The joint probability of event a and event b given that they're independent of each other is so uh does anyone want to answer this yeah probably okay yeah i'm driving okay so i'm going to be on mute and just listening okay oh okay oh okay by all means drive safe yeah right so in the case of independence we have that we just multiply the probabilities together but however if it is not independent we subtract the intersection of the intersection of probability of a and b so probability of a intersection b we subtract that otherwise it is this consider a data set d with n data points okay so this is a very big question. A binary linear classifier classifies the data into two classes, cherries and blueberries, where subscript I represents cherries and subscript J represents blueberries. Probability XI represents predicted probability that the observation XI belongs to the target class and QXJ, which is 1 minus p of x j which is probability of x j being a cherry right so we do a 1 minus because we are looking for the negative negative case so then that is probability of x j being blueberries. So what is the likelihood function? That is a question. So here it's just an well this is a simpler case where we just have two events. Now we have say I mean we have n observations and so we have to multiply the probability of each observation n observations and so we have to multiply the probability of each observation and getting this particular outcome where we get say a certain number of cherries and certain number of blueberries that is why the likelihood function is given by this equation The range of the likelihood function is, so in this case again we know that if we multiply numbers between 0 and 1 because probability ranges from 0 and 1, we multiply them say n number of times, it can never go beyond 1 and it can never go below zero either so we have just zero to one given the range of the likelihood function which of the following plots correctly represents the log likelihood function so even in this case okay so log likelihood function. So if likelihood function ranges from 0 to 1, log of 0 is negative infinity and log of 1 is 0. So we get a curve like this. The relationship between likelihood and log likelihood is this. So let me select option A. The same question, but the negative log likelihood. So it's just upside down. So it's negative log likelihood. It goes from infinity to zero. And that is option B. Okay, so the likelihood function is transformed to the logs function as shown below so what is transformation one so we transform the likelihood function to log likelihood so why do we do that it's because there are three options here yes results in overflow the computation of the likelihood function results in underflow. So these are two valid options. This one, yeah, if you like it, you can select this too. So this one, we see that the number ranges from zero to one. And when we multiply a large amount of numbers, many numbers, it becomes very close to zero and it outputs a number too small to be represented by the computationality. So this is the right answer. Then why do we convert it to negative log likelihood? Because otherwise if it's log likelihood, we'll have to maximize it because the optimum beta will lie at the top of the ball but when we change it to negative log likelihood we invert the whole function and then we see that it's we change it from minimization to maximization the optimal parameters of a linear classifier can be computed either by maximizing the likelihood function directly or minimizing the cross entropy loss function derived from it but the latter is more numerically convenient for optimization so that is true that is why we uh do then we negate the function and we transform it to cross entropy loss function so that is true okay so categorical variables can be converted to dummy variables using techniques including so we've got one one hot encoding and label encoded so one hot encoding is where we encode the column with just zeros and ones. And label encoding, we give it a certain label. Am I right, Kate or Harini? Do you want to add something to this? Right. Hot encoding is the one that takes a million columns when you have a lot of variables. That's why we actually use label encoding right okay then we have one hot encoding of a feature takes values in the set so we just covered it it goes from zero to one and okay so Okay. So he like an input. We want to pass. You want like the length of the sentence to be the same. So sometimes we use that one hot encoding. Are you able to hear that Kyle because it's not he sounds very quiet to me. Yeah, yeah. Kyle because it's not he sounds very quiet to me. Yeah, yeah. The volume is pretty low. Can you turn up your mic? Yeah, now it's better. Right. So given a data set D with n features, the distance from the decision boundary of a linear classifier is given by this. Now points on the decision boundary will be described where so does anyone want to answer this anil the one where it is equal to zero yes right so it's zero simple and on the feature space the distance of a point from the decision boundary of the classifier can take only positive values that is not true so it can be on either sides and depending on which side it is it will be either positive or negative yeah sorry yeah it's false uh in data set d the feature vector x is given by you get a we have a vector here and then we have the we have the decision boundary now let p q and r be points on the feature space so distance of point p from the decision boundary is so here we have the three points and we see that one point, I think it's Q, it lies exactly on the decision boundary, while one point is pretty far away towards the positive side, one is on the negative side. So we calculate, if we apply, if we just substitute x1 and x2 with x1 and x2 of each point we'll get the distance of the point from the decision boundary so p it's 17 we can also see that. So. Calculation okay so it's 3 by 5 times 5 and you get plus 10 that is 17 so dx we have this equation here right so we just substitute x1 and x2 over here and then we get it and similarly for q if we apply minus 10 that will be minus 6 minus 6 minus 4 minus minus 6 minus 4 plus 10 that is 0 and the other one is minus 4. Okay so now how do we calculate given distance of a point from the recession boundary? So how do we calculate this probability? So we have these options and the right one is okay so this is the right one now if we change it okay and this one is right so both of them are right so now we apply this formula here so given these three points we already know their distances from the decision boundary so how do we calculate the probability that P belongs to the target category? So we see, we find what is P, what is P of this particular point P, the probability that point belongs to the target category. So we'll apply, we'll substitute 17 here, or we could substitute 17 here or we could substitute 17 here and if we do that we do some calculations algebra then we get 0.99 q we know that it's on the decision boundary so it's pretty obvious that it should be 0.5 and r belongs to r is on the other side and that must make it pretty low so 0.017 yeah like for here like for these two questions like if you can actually do those calculations in the answer i think that will be good Okay. So let's do e to the power to the power minus dx. So e to the power minus 17. Okay, something like this, then plus 1. So it's around 1.04. So we'll see then 1 divided by control V. so you get something like 0.999 so it's actually almost almost one actually and yeah so we do the same thing for all the other uh points we'll get similar answers it's actually almost one um so I shouldn't have given this option one. Okay. Yeah. Yeah, that's true. But yeah, that could have been confusing for people. Logistic regression is a method commonly used for performing nonlinear regression. That is not true because logistic regression produces a linear decision boundary. That is false. So the baseline or 0R classifier ignores all the predictors and instead takes the most common class as the predicted value always. Okay, that's the right one. Takes only one predictor.or no that is not true is that classifier which always outperforms no that's the baseline so we want other classifiers to outperform this classifier so none of these is rarely mentioned okay neither of them okay next the one r classifier uh so this one is the classifier which always outperforms no takes takes only one predictor whichever is the best and uses it to build a classifier that is true now you would consider a classifier of some value which has the following properties with respect to the baseline classifier. Okay, it is almost as good as the baseline classifier. Now, we want it to outperform it. If its performance matches that, it sounds similar. Don't need to compare the accuracy of the classifier. No, that is not true. Now, it has better performance than the baseline classifier. has better performance than the baseline classifier. That makes sense. Linear models limit to modeling best fit hyperplanes in the feature space, while complex models are more flexible. If so, while selecting models to fit a data set, why should we ever prefer linear models over complex ones? So we discussed this in class as well. over complex ones right so we discussed this in class as well and linear models can fit curves just as well as complex models that is not true however we could do polynomial expansion but still that that's not what they're talking about it's just we're talking about simple linear models so linear models are computationally less expensive that is true linear models have a higher prediction accuracy compared to the complex models that is not true in fact linear models simpler models have more bias errors when complex models have more variance errors. So linear models are actually not very useful and should not be preferred. That is not true. Linear models are more easily interpretable. That is true. Okay so we come to the last question. Okay I think there are two more questions. Okay so for this one, so we look at this data set set we look at the scatter plot and we have to decide which which of these options would be suitable and in fact which is the simplest applicable model right so the simplest applicable model does anyone want to answer this The simplest applicable model does anyone want to answer this. Yes. Yes. Yes, right. Yeah, correct. So the others, they might work but still they'll, they are not the simplest models that are applicable to this data set. Okay, so now and for this data set, we have, we have options, the same options again. And so we have linear regression. So linear regression is out of the picture because it is, it's not a classification. Yeah. Yeah. So a linear discriminant analysis model with features x1 and x2. So we've seen in, we've seen that. Okay, so we have linear discriminant analysis model we apply that and if you've done the lab you'll know that you get you get a prediction some somewhat like this where it predicts uh it's not a good model and let's go back here. A logistic regression model with polynomial expansion of the features x1 and x2. That is true. We look at logistic regression with We get a prediction like this, which is pretty good. Then a logistic, a quadratic discriminant analysis model which features X1 and X2. That also works well. So this is QDA. And that looks pretty good as well. Then I guess there's one more. Yeah, linear discriminant analysis model with a polynomial expansion. So that too works. So if you go try that out, you will find that it works pretty well as well. try that out, you will find that it works pretty well as well. I had chosen a polynomial of degree 2, but maybe if you increase it, you'd get a better prediction, which is better fitting. So that could work as well. So that's it. Let's finish the attempt. I hope I didn't make any silly mistakes. Let me call and finish. Great. Yeah. Yeah. Yes, great job. And thanks for taking the time to walk through the quizzes. Thank you. Thanks, Kate. And thanks, Harini. No problem. All right, guys. So the question that I have is, are you guys mentally fresh for an hour of a new topic or should we relegate it to the normal Saturday session? the normal saturday session guys are fine all right so this is actually a topic that's ideally suited for an hour and we can in my we'll finish it in an hour um are we still on record Are we still on record? Yes, yes we are. Just a minute. I'll just stop it and start.