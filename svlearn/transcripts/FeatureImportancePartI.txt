 . Alright guys, so today, the thing is, with some luck, this whole thing will get recorded. I'll be putting it on YouTube right after this. And our favorite family will all have access to it. And if there are braves who are not here, they'll be there for access. So today's topic is, what is today's topic? Or teaching. Is there anything you want to talk about? This topic is related to another topic which isicability in models. This is what we are going to talk about today. We'll close the chapter on outliers and anomalies and analogies from last time. Now, how many of you have finished at least two data sets there? How many teams? How many of you finished them? I would highly suggest that you do that. Don't limit yourself to just one. If you have done many, you should do two things. The weld, the shackle, these are fairly straightforward, busy ones. The medical data set, I know it's not hard. The credit card slot is straightforward. And the, the, uh, sorry? KDD. KDD. And then there's a KDD one. So none of these are very hard, but they are time consuming. Outlier detection and anomaly detection is really a very powerful technique. You can do a good run of a brain nucleated practice. That's good sometimes. And they go on to start with some standards. Which is what makes my interviews that nothing to do with us. I highly recommend that you take the time to finish it. However, in this workshop we won't have time. Because we have given two days to hold a story. Today is for a new topic. So, let me motivate this with the first gradient overview of the economic risk of the climate. See, suppose we have a model, a black box. It could be a deep neural network, it could be a secure data machine, it could be a random forest, and so forth. Suppose predictors, let us say that you have predictors of x1, x2, t predictors that go into the network. If you have some unknown transfer function, you have a, y hat, some unknown function of x. Now, if you knew what f is in a closed analytical form, you know what is happening, isn't it? Because you know what is happening in the little y, I argue, if you know the analytical structure of that country, you know how much each predictor is contributing to the prediction, how important it is when you do the prediction. So, to make it very real, suppose you are looking at the sale of ice cream on the floor, for example. You know that the temperature matters, proximity to the beach itself matters, how much quality will be there, but to what extent does it matter? Then how much does the quality of the ice cream itself matter in the total sale? Suppose you had a finite budget, and you can choose, you can spend it by building your cart, your ice cream shop, your shack, but closer to the beach, which would be more expensive, or purchasing better ice cream, a bit more expensive, or flavorful, more varieties of ice cream to increase the sale, you would ask yourself, where should I spend the money? Isn't it? And where you spend the money will come back from the market. The market should be able to tell you, not only that, this is the temperature, this is the distance from the beach, this is the quality of the ice cream, this is what you will sell, that would not be enough. You should also be able to sell, give you information on how you should divide your money up, where should you invest. Should I invest in better ice cream? Should I invest in a more expensive location closer to the beach? These are real questions, isn't it? Likewise, if you're looking at the probability that you will get diabetes, suppose you have a history of palliative diabetes. So you can say, all right, how much should I exercise? For example, how many minutes, how much effect does exercise have versus cutting down on calories? Isn't it? So suppose it turns out that cutting down on calories and your food intake has marginal impact, very low impact, whereas exercise has predominant impact, it would lead to a completely different type of behaviour from you, isn't it? You would just make sure that every day you are on the treadmill for let's say half an hour, one hour, whatever it takes, every time you feel diabetes, you would increase your exercise level, your sports level, isn't it? But you wouldn't worry about food at all. If on the other hand it turns out that exercises really don't matter, what really matters is the food intake, then you will become extra careful with your food and the sort of food that you eat. And then it makes sense to ask what is the proportionality of what you study is to your audience? How much of that focus here versus here? And these are very real questions. For example, I'm making examples of things that you can relate to. We struggle with these problems, or at least I struggle with these problems every day. Which one to spend, how much do you emphasise each of the factors? So importance of a feature, importance of a predictor, of a variable, in affecting the response matters, isn't it? And it goes to the very heart of the inter-religion of the model, isn't it? Suppose I were to know for sure that at this particular moment, if I became careful about calories, I decrease my calorie intake by a thousand calories, it would immediately have a response on my diabetes or diabetes, the sugar level, this much response in the sugar level, and exercise has this much response. Then I can look into my lifestyle, I can see this is how busy I am, this is how much I can take out time for exercises, and I can't take out more than this time for exercises. Means I need to make up the rest of the improvement through food, through calories. It helps you make decisions. Interpretable models are valuable. It is not just enough to say at this rate, this is the prediction. This is the prediction of the future of sustainability. But I make sense. Likewise for the sale of to the same flow. So in all of these situations, what we want to know is, what some sort of idea of what the function is. Now, when we talk about, let's take the example of linear regression and its associated logistic regression, logistic classic planning. In linear regression, what is your basic hypothesis? Basic hypothesis is y is y-path, or your prediction, I'm putting a hat for prediction, I don't usually do that, I'm just saying y is the prediction factor. I'm putting it here, is beta naught plus beta 1 x1 plus x2. This is your linear regression model, isn't it? So, a barring layer, a barring layer you can explain would be something like this. Now, assuming that the factors have been nominalized, they have been normalized, they have been standardized. Then they are all in the same square, the one they are in. Then suppose it turns out, let's just take a hypothetical example. Let us say that this is the sailor by stream on the beach. It is something like this. It is equal to some base value, let's say, I'll just take some numbers, one, 10, plus two times proximity, plus 10 times quality, same. Now let's ponder over this. It says that if you, the amount of ice cream that you would save is given by this equation. So the beta naught is this, beta 1 is this, beta 2 is this. So suppose you want to increase the save of ice cream, where would you rather invest your energy? Why? Because for unit increase in quality, there is a 10 times increase, I mean 10 unit increase in sale. But for a unit improvement in proximity to the beach, we have only 2 units of improvement in the sale of ice cream. Are they making sense? So, another way to look at it is, if I were to take, and this is a crucial thing, if I were to look at the gradient of this function, the gradient of this function, the gradient of f of this function, if we just put it this way, y hat is equal to x. This is the transverse function. The gradient of x will be what? The derivative with respect to each of the variables, it would be x1, x2, and so on. Let's just take two variables. Would it be this particular field? Isn't it? Made up of these two types. We remember the gradient and the elevators, and respect each of these variables. So what will that be? That will essentially be your beta 1, beta 2. Isn't it? In other words, if I knew the derivative, it gives me a sense of the feature importance. You see a very interesting relationship in linear modeling, that the feature importance is directly based on the gradients with respect to the features. And in fact, it happens to be simply the coefficients of the model. It's a different feature coefficient. But observe the fact that it is a gradient. Because we've generalized this beyond median values. So far so good, right? Likewise, suppose I'm determining the probability that somebody may or may not whether something is a blueberry or a grape. Remember I did other quintessential examples. There, what was the thing? We had another situation, log p one minus p was equal to beta naught plus beta one, so good. And then also the feature importance is directly related to it, because I can say that the log answers the function, right? Whose page did I do compute? And it will be . So for linear regression, for logistic classifier, you realize that gradients essentially become the coefficients for future . But what will you do when you have something like a random forest or a ? The problem is you do not know this, do we? We don't know this. It's a black box. And that brings up this whole question that how do I ascribe importance? Which feature is important? Do you realize that? How would we know? And you see the crux of the problem here. And yet, irrespective of whichever magical black box you use, you would argue that it is of central importance to know the relative importance of features. Not only that, to have some degree of interpretability. Isn't it? The fact that we don't have interpretability in these black boxes is leading to a whole sort of can of worms here. These days, for example, none of these black box models are creating bias. Bias has become a central issue in machine learning. We do not know. It's always become a force of evil. Because people are making decisions based on these black box models. Law enforcement is acting upon it. People who do not understand the nuances of mathematics are using it blindly. But these black boxes are biased, very biased. And those biases are not necessarily visible. Isn't it? So there are many stories that are told. One of the recent stories, as you know, was that of Amazon coming up with their face recognition technology and wanting to commercialize it. But then somebody just pulled the faces of the black pockets in the US, all the black senators, and gave it to the facial recognition system to identify and tell who it is. In most cases, it identified each of those senators or congressmen to be one of the serial killers and rapists. That's true. Yeah. Oh, good. That was really amazing. Yes. So you realize how preposterous it is. Here is one giant step back. Here is this magical, shiny box that is perpetuating stereotypes. And that is as regressive, as unprogressive as it could be. Yes. Yes. . Why? . Yes. . . So you see there, you see, when you think that we have this shiny box that is making very good predictions, what happens is that those predictions are sort of in a sanitized lab. They're aggregated over large numbers. If you look at the accuracy, I have 99% accuracy. And you feel really thrilled about it. Except that that one person that you get from really matters. And it can have devastating effects. Not only that, that 99% accuracy happens to be only in the data set on which you trained it. Like in the case of this Amazon facial recognition system, it could almost complete certainly fail on a different data set that it was not trained on, that it was set up for black hoppers. So that is the risk with this black box. If you don't know what it has really learned, whether it has really learned something or it has essentially lost its way, how did you know that? And you don't know that partly because you don't know what is the inherent subtle bias in the data itself. Isn't it? So another example that is given, I don't know how, it's sort of been in the communities for a long time though I don't know, I've never been willing to praise the artificial intelligence because it's a story worth telling. The story goes that at one fine moment, the US military went to some AI company, a startup, and says, we need to really be able to tell the distinction, a startup, and says, we need to really be able to tell the distinction between a military vehicle and a civilian vehicle. Why? Because you don't attack a civilian vehicle. So you need to be able to spot a military vehicle, for whatever reason. Maybe that's the reason. So the company said, that's perfectly fine. Give us a lot of pictures of the military vehicles and civilian vehicles. And the military said, well, no problem. They ordered their service. We need pictures of military and civilian veterans. They said, let's serve. And so lots of pictures were put in the military and civilian veterans. They turned the neural network into a therapy. The network did extremely well in the lab. It's had near perfect accuracy. As you know, image recognition in networks are extremely good results for convolutional. They did just brilliantly. Then the literary wanted to test it out, so they took a whole set of pictures that they had not given to this startup, and they fed it into it. And sure enough, the whole model worked just fine. So they took the thing to the field. When they took it to the field, the entire thing was a disaster. It just for inexplicable reason, the same thing would work wonderfully in the lab, but if you took it out of the lab, it just wouldn't work. And they were scratching their head to put some time in the building. It took them a while to realize that there is actually something in the data. What happens is, when you ask people to give you data of military rating, the soldiers see military rating in the morning and the evening, isn't it? When they come back to base, base is filled with military rating. So if they have to go out to do their rounds, right in the morning all the military riders will be parked in the same place. Parked late in the daytime. So you take pictures and they will have low Indian light. You walk in the daytime, each of these military vehicles is rolling around in territory. What do they see around them? Civilian riders. So they take a lot of pictures for civilian wages, for the data. And what the real vector caught onto was essentially the alien light. It came to the conclusion that the alien light is low, it's a military vehicle. If the alien light is high, it's a civilian vehicle. And you wouldn't detect that bias. It's sort of subtle. Do you see that? Isn't it? So that is the problem with the lack of interpretability in models. It's like, it becomes like a sorcerer. It's like, I tell you, you have to trust me because look at my accuracy of this model. Isn't it? But except that it doesn't work like that. You don't know whether it is really accurate or it is completely mistaken. So therefore the search for interpretable model these days is a very serious search. Because it's at the heart of not having bias. And the fact that these AI engines have bias is one of the core problems today. People have been very, very seriously looking at this problem. And obviously people like us, we've been very cut up in the new networks with all these technologies, random forest and so forth. I suppose it takes people outside the field, people in different areas like social sciences, who are getting far more alarmed because they are looking at it as a black box and asking how is a black box and asking, how is this black box behaving? And when they observe how this black box is behaving, well, it's not behaving well at all. It reinforces stereotypes, it's pretty regressive in its thinking, and so on and so forth. So, we need to open the box somehow and see it. There is a catch though, that has been there. We say that's all very good to say, but if we make interpretable models, those are linear models, simple models, or decision trees. Decision tree is highly interpretable, isn't it? Or, a linear regression is highly interpretable. If you agree? And, likewise logistic regression is linear modeling. Simpler models, they have the benefit of interpretability. So, if I take this axis, which is how sophisticated a model is, or how accurate or better it is at prediction, and I use the word accurate in a more sort of a diffuse sense to mean both regression and classification accuracy, regression, etc. At least we can do it, though accuracy is not a right term. We could use low prediction accuracy. Sun squared errors are low. And you see, mean squared errors are low. And then, for classification, of course, accuracy is the other way around. dan kemudian, penyelesaian akurasi yang pertama Jadi, kita ingin mengambil akurasi, semasa akurasi itu meningkat dan ini adalah intang-intangnya Unfortunately, and this is simply some sort of power of the people offering the speak of the power of an algorithm, though I would argue it's not really power because of the but it is . But, you know, these people, they will often get the power. But it's more accuracy, the ability to make good predictions. Suppose you dial it up. Usually what happens is you have, ideally you would like to have models which are here, highly interpretable, but they are, and models that are highly accurate, they are here. So you have a situation, people often sort of generalize and talk about a curve like this. It's not really that difficult. It sort of tends to have a behavior like that. That the more you have interpretability, usually the less accuracy you have, the more accurate the model is. For complex situations, the less interpretability there is. And the argument that is typically given is, see, these models, this is a linear equation, isn't it? Linear equations assume a linear relationship. Most of the dynamics in the real world are non-linear dynamics. The relationships are non-linear, the functions are non-linear. As you saw in your labs in the first ML 100 and 200, it is only the very first lab that you could do with a straight linear network. Isn't it? There is that one. After that, it was done. It got more and more complex. You can impose a linear model, but you have to think a lot to do it. And soon you reach a situation where linear models simply don't quite work till you do a whole lot of feature engineering. So what are our chances? One lesson we learned was that if you are careful with feature engineering, if you really pay attention, you can actually transform a non-linear problem into a linear problem. You can do it by thinking hard about it. For example, in the river dataset they just said we did that. For the hills, they just said we did that. We transformed what was essentially a highly non-linear situation into a linear situation. That is the gold standard. If you can do it, do it. So, how do you connect our first approach towards solution? Let me put it here, there's a lot of things we can do. What do we do? Future engineering, right? And in a way, future engineering is what gets de-emphasized in Russia. In Russia, they use counter-culturing. But I hope the whole burning of ML200 was pay attention to future engineering. It was not just for the class. It was pay attention to future engineering. Because the benefit is if you can sort with feature engine with a simple model, you get to retain your interpretability. So to say that in the river data set, whether you are in water or sand, depends simply based on how far you are from the center of the river. Well, that sounds like common sense to me, doesn't it? All rivers have a certain width. Therefore, whether you are in water or sand would depend, in water or in the sand, would depend completely on how far you are from the center of the river, the region right to the river, and however curved the river is. And so, that is the value of the relationship between the two. The second thing you could do, which also we have learned to do, is you could do, let's say that the problem is with 10-15 dimensions, but you could do some sort of principal component analysis and you could discover that in reality, the data can be looked at in a simpler space, two dimension space. And within that space the relationships are linear. Principal component analysis implies a linear relationship. It basically says, you know what, it is a high dimension space, but hiding in there is a linear relationship. Let us go find that linear relationship. When it does find that linear relationship, you have more interpretable genes. For example, in a lot of genomics and so on and so forth, there are many, many genes. But for any one disease or anything going on, it is a conspiracy of a few genes. If you're lucky, it is a monogenetic, one particular gene is disordered, expressing that, but not expressing that, it is badly. But in the case of the Moods disease as well, it is polygenetic, but multiple genes are involved, but it is a small number of genes from the entire genome. And so there are a couple of factors that will be involved that is the composite effect of a small number of genes. Are we together? Principal component analysis works very well. It not only brings dimensionality of that gene, it also enhances your interpretability to a larger extent. . . . . four variables x1, x2, x3, x4. And all of a sudden, I find synthetic variables phi1, phi2, the principal axes, that do it. Now phi1, phi2, to the extent that they're composites of these features, are held lost in the computer. And this is very difficult. We do this. Now, what happens though is, on the face of the question eclipse we have lost it. Usually what happens is that the rolling expert starts sitting at the end and says let's see, phi 1 is this combination of x1, x2, x3. In phi 1, x1 is dominant and x3 are dominant. These two factors tie it up. In phi 2, x 2-dominance, let's say, and get the answer. And in either of the two, x 4 seems to matter. So what have you done? What they have done is, they have said phi 1, which is called the learnings of the characters under the axis. The phi 1, they will give a name to it in the domain, they'll say, it is this factor. Right? So for example, I mean, a very silly example, suppose the factors turns out to be your height and your circumference matters, right? So you would say, well, 5D looks like a volume, right? Or the total mass of this person, or something to that, first of all, the size of this person. So, people create things of the nature of space. Now what happens is, when you're dealing with a genome that is 100,000 or something like that, then you realize that if you could find two or three principles of science, instead of interpreting a model in terms of so many, and each one of them seems to matter a little bit. And that's true. I mean, we don't know how to interpret this. Whereas, if you come up with just two or three principal components, then the job becomes to interpret what it means by looking at the elements. And use your domain expertise to give an interpretation to it. But once you have given an interpretation to it, the point to be is that you have actually advanced knowledge in that area. We have seen that there is a factor which is built out of these two factors, which is a new concept that we need to introduce in the field and take it seriously. Are we together? So that's the value of principle comfort analysis. But to finish that thought, hold on, to finish that thought, what we need is, we can go further and say dimensionality reduction, in any form, leads you to a simpler understanding of the situation. Are we together? Now, the most basic dimensionality reduction could be, you throw away factors that don't matter or marginally matter. Right? Now things that, for example, techniques like LASSO, they are good. Why? Because if you have linear models, right, in linear models, they help eliminate completely a lot of the factors. Why? Do you remember? They have pointy, the constrained surface has pointy elbows. So it sort of punctures the error surface. It does it along the axis. Because it does it along the axis, this is a very good recap, a lot of the predictors, they just disappear. Because if the x-axis is going in 20-20, then it's against y and z-axis. It means that y and z will matter. That's what they're doing. So, things like that. So, dimensionality reduction is another one. So, I would like to draw PCA dimensionality reduction as a second technique. Then after, it can a particular feature. And also to feature importance. In the first case, what you have is, you say, we know what features are important, but it is not what the inputs are. It is the features that we extracted that matter. In the case of, let's say, a PCA, you would say, alright, if you look at the loadings on each of the axes, it is these new axes that matter, and they have relationship to the inputs, input predictors, and this is the loading of the input predictors. In the case of LASTA and linear modeling, it sort of quickly eliminates that which doesn't matter. The trouble with LASTA obviously is that because situations are usually not very linear, there's a limited domain of that, the capability to which you can apply it. The other thing is it tends to be a bit too brutal in everything we do in a lot of matters. We have to be very careful. Which is why we think the whole theory of the range and elastomer and elastic matter and so on and so forth. So these are two things. Hold on to your questions. I'll let you go. So far I'll be together. And you'll also see the relationship of the contemporary and future in 47 and find ideas that are very different. I think you have the first question. So the question is, is it easier to do it on the future? It is, it is, it is. I mean, the first question is, are you? Yes, yes. Ya. there's a coming abstract in there. You'll do that. There's many people that face value-addedness. There's a person there who doesn't. So, I'm just thinking about this and what you are saying. If you have to take care of the ability of manual echo by things to do with your product, do you see it actually doing the same? It is essentially doing the same. It is an arm of the chain, you know. And the chain works. No So, what essentially do we do? There is a norm of feature engine and feature importance. Let's say that we do, suppose we found two axes that matter, principal component that matter. The first principal component depends upon hugely on X1, little bit less on X2. X3, X4, it doesn't matter. So, what can we conclude with? That X1, X2 together make a feature called phi1, first axis. The second axis depends only on X2. So you realize that other things don't matter. So already we are computing that X4 is irrelevant. X1 and X2 matters quite a lot, and X3 makes it matters a little bit less. Right? We're already getting a sense of how load- how are they loading on the principal axis? How the features are loading on the principal axis. And, basically, you have a sense of each of the properties. Isn't it? But for those of you who haven't read PCA, hold on until we do that, it's a new world. So there is a- you that's coming from PCA. Yeah, of course. PCA has to do with the loading. Yeah, yeah, you have the loadings. Once you have the loadings, you can... Now, given the feature, you can read the computer in terms of the access of that. It's just a matrix of that. PCA has to do with the loading. It's a good idea. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . y2, x1 and x3 matter, y2, x2 matter. So what has happened is, we have essentially the four dimension space found a three dimension surface of sorts. I mean, I think two dimension surface, which is a composite, which is sort of a it's a two dimension surface in the four dimension world, which is space. But it is treated in such a way that they are composite features. And we can make this here, how it defines such a structure. All right guys, any more questions? So this stretch of feature importance and interpretability is sort of a review, whatever we did, and reframing it within the context of this. But now, we can go further. We can ask ourselves, suppose we don't have a choice. Suppose you are dealing with a black box problem. What then? How would you interpret that? How would you know? And how would you know future importance? So, traditionally, when you do future importance, those of you who have been doing random forest and so forth, decision trees, very helpfully, both your R and Python tools, they do what? You build a tree, it immediately comes up with future importance. Like, if you do random forest, it gives you future importance. How many of you have seen that? You have been assuring it in your notebooks in the last few years. So, how do you know buttons, like random voices which will teach you buttons. How many of you have seen that? You have been showing it in your notebooks in the last few days, in the last few seconds. There is a problem, though, with that. It looks beautiful. The fundamental problem is . Let's take the quintessential example of the sale of ice cream. You realize that suppose you are within 30 feet of the beach. Do you really think that proximity is a big factor? At that moment, going from 30 feet to 20 feet, getting even closer to the water, common sense, will that have more effect or will improving the quality of the ice cream have more effect on the sea? Quality of the ice cream. Now take the opposite way. Suppose the quality of ice cream is pretty good, but you're quite far from the sea. At that moment you think that, suppose you're two miles from the sea, would you rather just go on improving the quality of ice cream or would you rather come closer to the beach? So what does that tell you? So think about what I just said. I say that if you look at this space, distance or proximity, opposite of distance, and quality. When the quality is low, and proximity to the seat is already pretty high, which factor matters more? Quality matters. Here, the future importance would say, quality. Quality should matter more. Isn't it? On the other hand, if I am here, my proximity to the thing is low, but my quality is pretty high, which factor should I focus on? Which is the more important feature? It is proximity. So, what does that tell you? That's right. So, in other words, feature importance is not a global thing, isn't it? It is not global, in other words. It is global only for linear models, isn't it? Linear models are simply the gradient, because the gradient doesn't change, isn't it? These beaches remain constant. I mean, you know. But, in complex situations, in each neighborhood of the feature space, the feature importance is maybe different. And now, I already showed you how would you, so the question is, how would you find that feature importance in a given neighborhood? So that is an important thing to know. Now you realize what I have been telling you, that what you have been doing is a very sort of a very blunt instrument. When you just take random points, feature importance globally, it is a very blunt instrument. When you just take random points, feature importance globally, it is a very blunt instrument. And it's a very clumsy instrument. Also because that feature importance, as you notice that you will realize, is unstable. You take a different measure of importance, the completely, all the ordering of features changes. You take a different criteria, information gain criteria or some other criteria, and then the audits are very unstable. So surely that's not the right way. Why is it behaving like that? Or you take a simple different set of data, you take a different neighborhood of the data, the future importance has been changed. You build a model of that. You will end the entirely different model with the different keychain partners. Dan? Is there any way you can do a transformation of one of these higher order data sets in the ? Yes, very good question. So I'll repeat what Dan asked. This function, when you do a transformation of this function, when you do a transformation of this function, you can do to, let us say that x from x I go to some other space. Let us say that I go to the space of, some feature space, where the relationship linearizes itself. Remember we talked about this map, that at the end of a neural network is now what? A linear unit. It is either a softmax or logistic unit, sigmoid unit, or just a simple linear unit. So what do all of these things do? What does support vector machine do? It just projects data somehow into a higher dimensional space. In other words, it linearizes the problem. And then at the end, it solves the problem. And so we do know that deep in there, there is systematic transformation of the input data and a creation of a feature space where the relationships are linear. Isn't it? And so in that space, of course, the relationships are highly, the feature importances are known. Am I making sense? There is only one catch to it, which is that those feature space is highly non-intuitive. What a feature space that a support vector machine finds out, or a feature space that, a representation space that a neural network finds out, you have very little clue. You tend to get some clue if you're dealing with images and so on and so forth, but it is still a bit hard. Isn't it guys? It's hard. The other problem that happens is that these representations are made because of the non-linearity of the relationship. So let me put it this way. See, when you project it down to other features' space. Look at this. Can you take that vase please? So if you look at this vase here, suppose this is your function. Let's say that y is the sale of ice cream, a hypothetical. Now we are departing from a real life situation. I don't think the sale of ice cream would depend like that. And let's say that one axis is proximity, another axis is quality. And let's say that this is the sale of ice cream. Now, what do you realize? That at certain regions, forget the base point, let's get to the top. So in certain part of it, you see that the shapes are different, the gradients are different. Importance is directly related to the gradient. So the rate of change. So at that particular point, at this particular point, the importance of quality is how much sale of ice cream would change if I change the quality one year. In other words, the gradient with respect to quality. Likewise, how much proximity matters is essentially the gradient with respect to proximity. Does that make sense guys? So feature importance is essentially the gradient, the local gradient at that point in the feature space with respect to the feature. Do you see the geometrical interpretation of that? Now look at this, you can literally observe this and see that the gradients are changing, isn't it? Slopes are changing everywhere. And in a very visual way, I hope you are seeing that each of these is a local phenomenon. Isn't it? So that is one statement I'll write down. Let's write down a few realizations. Future equities. This is local. So this is one of our realizations. This is a local phenomenon, isn't it? In the two sense. Now, you may average over it to get a global sense of feature importance, but you remember that you lose resolution. Global feature importances are averages over local feature importances. aggregation, aggregate what you're speaking, is that, gradients, The only, and the whole point is, the problem is, M is implicit and not available to you, isn't it? So, this is like going into an exam and then somebody is asking you to find the gradient at the point of a function, and you say, okay, give me the function, and he says, well, I don't know. Well, that sounds terrible, doesn't it? And the whole question is, can we solve that problem? Let us ask this question, this very magical question. Suppose I don't tell you the function. The analytical structure of the function, I don't tell you that. I just give you a black box. For every input that black box will make a prediction, what you have is just a black box. It goes in, and while hat comes out, that's all you have. You don't even know if this guy is sitting with a function. You have a good-faq belief it has. You believe, the whole of machine learning is preface to the belief that the machine has learned some internal representation. There is a function, except that the black box is not willing to agree with you. Isn't it? And the question is therefore to ask for future importance locally first we do the local because that's a very clear view. Locally is to ask for the gradients. Gradients with respect to each of the features. That's a way to interpret it. So how can you compute the gradient without having the form of the function at all? And therein comes a whole set of powerful techniques that we are going to learn today. Have I trained the problem properly for you guys? All right, so that's what we are going to do. So I want you guys to scratch your head and see if any idea pops up of how we could do it. And then we'll make some effort and try to come up with a few ideas. So you see this is an open field guys, I really mean it. It is a really new thing, people are trying really new ideas. If you really try hard, you guys are all smart people, you might come up with the next big idea. Because we are at the very early stages of, we have been part of the techniques, but in my view, we are at the very early stages of coming up with a theory of interpretability of this technology. I would recommend that. There are techniques, we learn those techniques. But think about it, think hard about it, before attending the solution. Because once you're attending the solutions, or the techniques that exist, you get sort of trapped in the mind frame of those existing techniques. Because techniques are still a few techniques. See if you can think from first principles, and come up and generate a few ideas of your own. Are we together? So I'll leave you guys for a break with that particular thought and after that I'll call you for ideas and see what ideas you come up with. See guys, this is all very relevant. I hope it's not like a sale of ice cream is a very broad problem. This interpretability is at the heart of it. If I am young, right, and let's say the factors that can keep me away from falling sick or having diseases is one thing compared to when you are middle aged, compared to when you are 18, 19. Isn't it? So, things that matter, we all know, just look at your life. The things that you gave importance to in your pursuit of whatever, knowledge, success, those features, those factors change. The importance of those factors change. Isn't it? You are in the workplace, at that moment you are far more concerned about knowledge specific to the work being done or the social habit, that is how you fit into the social habit. They matter a whole lot more. When you are a college student, you wouldn't care less. But your success academically is completely, to the fullest approximation, in the way of how good socially you are, or how well you fit into the fabric of, the social fabric of student life. You see how it is. In one place, the social fabric matches the world, in another place it doesn't. So, in some sense, all our life we have known that feature importance, we have been deltated basically intuitively, feature importance is very contextual to which part of the feature space you are in at that particular moment. Isn't it? So we will build upon this idea and after the break we'll develop a few methods. But I hope, guys, I have given you a good summary of where we are. Hopefully, I have connected you with the point that the way brute force switching and ordinances are done is probably not correct, or at least not as meaningful as we thought. How many of you agree with that? are at least not as meaningful as we thought. How many of you agree with that? Good, so let's start. So with that part, let's take a break, and no more with the techniques, don't make your parties and so forth guys, because some of you came late, you didn't get a chance to have a party, which is why I'm giving this segment a break. Now here's how the day will progress. We'll take a break after after that I'll explain these techniques. After that, there are five articles that are there in the teaching office, the directory of datasets, 100 datasets. I want you guys collectively in the team to read all of them. The third, part three, is a set of exercises. I want you guys to do those exercises. You three work your way through these exercises. Today's datasets are those. First we do those. Then the second part, time permitting, later we will also apply it to some of the datasets that you know. For example, West Kansan datasets, California housing datasets. I'll give you a few examples to try it out, time permitting. But today is a long day because it will take you quite some time to absorb this topic on your own, to review it, study it, and to practice it. So you will be able to, beyond the dataset, which is the US success dataset, that you will work your way through, you may be able to do one or two more examples. That is perfectly fine today, not any dataset, because it's very fast at learning. That's what we will do today. Your first presentation, so we'll have, before lunch, I'll finish my picture. After that we'll have lunch, after that you guys will review and do it and practice the five arguments, and hopefully by 4.35, I expect each team to present their work and explain all the concepts. Are we doing that? Alright, so, and therefore the techniques... Thank you.