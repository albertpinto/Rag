 So I'll just recap the theory that we learned because that's going to inform the lab that we are going to do today. So what we learned was the topic of explainable AI. Now, this is gaining a lot of importance these days because we have realized if anything has shown us in the last two, three years, it is that black box AI can do a tremendous amount of damage. In fact, the scenes that we are seeing in the Capitol Hill today, this mob, it has been indoctrinated through the recommender systems of this giant web things like Facebook and Twitter and Google and so forth. At the end of it our technology world has to pick some of the blame for what's happening today. So and that has raised this question that see we don't even understand what we have built and how it is working and that is not okay. It will tear apart a country, it will tear apart the fabric of civilization. We need to, there are very dire consequences and practical things. If from a medical perspective, you would want to know why the model, why the machine is saying that you are less likely to get diabetes in the next person. So that, that has caused a lot of research in the recent years. This research has also been fueled by some high profile failures, like the facial recognition failure in which senators were identified as essentially serial killers and rapists, identified as essentially serial killers and rapists, the Black Caucus. And other problems, like for example, the hiring system, which without explanation would recommend who to hire. And it turned out an investigation that it had severe bias against the protected classes. Now, obviously, if they could see that it is doing that, they would probably let us hope that they would not have implemented it. But they couldn't see it. They couldn't see the bias in the data and the bias in the machine. Many years ago, there was a similar case, I believe, against Kronos, which also tried to do algorithmic hiding and from what I remember they lost the case. They had to pay a tremendous amount of money in damages. So that has been there. So that brings us to the whole emerging and hot topic of explainable AI or explainable machine learning. It has even developed its own acronym, XAI. So traditionally, they believe that there is a really this graph that you see with red color, this graph shows. Maybe I can zoom in to this a little bit more. This graph, it, people used to believe that models that are highly interpretable, they don't perform as well on complicated situations. And models that are high performing are essentially black box, opaque box, opaque models or low interpretability models. And people were getting comfortable with that for many years, for almost 20 years. I remember at the turn of the century, the debate amongst researchers was that, is it like, till then it was a given that you would have a proper theory explaining what the model is doing. Then came the ensemble methods and the kernel methods and in both of them it was very hard to put a concrete simple interpretation about what the model is doing actually in making those accurate predictions and there has been a debate for 20 years about, is it okay to just have a machine that has high performance, high accuracy, but is a black box? Well, if, so people often believe that you can't have it both ways. You can't have the cake and eat it. So now that brings us 20 years forward and we realize that now we cannot have black boxes. We need to impose an explainability onto the model. So now what people are trying to do is they are saying that in a sense this thing, this thing needs to go up here. It needs to be high performance and high human interpretability. It is not acceptable for it to have no interpretability. So that brings us to this whole topic of how do you do that? When you think, especially in the context of a deep neural network, you realize that we deliberately bring about an activation function that does a nonlinear transformation. Without a nonlinear transformation, you would not be able to achieve the universal approximator theorem. You can't make a neural network a universal approximator. So once you bring in at each of the nodes a nonlinear transform, output of that goes to the next node, which also does nonlinear transform, pretty soon it becomes almost impossible to interpret what the overall function f is that it is using. You know it exists, but you don't know what it is. And so that becomes the problem. So now to explain this, I talked about a geometric perspective, I said, let's see, how would you achieve interpretability? When you have a function, which globally is very complex, and captures a complex reality. At the end of the day, if you make a hypothesis that the function is some manifold, embedded manifold in a feature space, then a manifold, and I gave you an example, is sort of like a bed sheet without tears or creases, a smooth sheet, but it can be turned whichever way. That's the intuition behind manifold. And now locally, locally as in the eyes of an ant or a bedbug crawler, the world looks flat just as to humans who are very small compared to the size of the earth. The surface of the earth doesn't look spherical or ellipsoidal. Instead, for the longest time, to us it looks flat and for centuries mankind believed that the earth is flat because that is the evidence our eyes are bringing to us. And it goes to show that in the very small, these manifolds, they look flat or they look Euclidean. So they look like a flat map. You can draw a Euclidean thing around it. And as an example, I just said that, suppose you take a curve like this. Then if you look at this point, you can make a little yellow neighborhood. And in the neighborhood, it is essentially flat. And then I brought in this little bit of Taylor expansion saying that the value in the neighborhood is the value at the center plus the slope times the displacement, the gap between x and x naught, along with high order terms. And the point is that if delta x is very small, you can sort of ignore the high order terms and you can assume that there is no curvature, etc., because locally it will be flat. So based on that hypothesis, when you generalize it to higher dimension, one of the things that we realize is that in the very local perspective, anything, no matter what it is, you generally end up with an approximation which is an additive model. In the very neighborhood of this, you can build a small additive model. Now remember the word is local, very locally. So the people have been asking, let's say suppose you have a function which is unknown. You can always try to approximate it with a less complex surface, a less complex curve. Let's look at this white curve here, which is complicated. You can try to approximate it with the yellow line. So you make a surrogate model which approximates the original model. Now the question is, what is the fidelity of the surrogate model to the actual F which is analytically intractable? So, well, globally it's a bit hard. Like you see that the white, the yellow line and the white line are not exactly the same, but obviously for the math that we taught locally we can rest assured that we can build models high fidelity. So anyway, so the thing is we agree that locally a surrogate model can approximate the unknown, the rather intractable function with a fairly high fidelity. So that brings us to the concept of lying which is that let's build a local interpretable explanation. And it doesn't matter what the model is, that F may come from a neural network, decision tree, random forest, you name it, because it doesn't matter. At the end of the day, mathematically, it is some manifold, right? To the extent that it is some manifold right to the extent that it is some manifold therefore locally of course it will be flat right and so one of the simplest explanations that you can do is you can essentially do a linear regression with some you know additions you do a kernel a kernelized linear regression means you're more sensitive to points, closer to the point that you're trying to explain and less sensitive. You do that and you regularize and all that. Everything that we learned in linear regression theory, the adaptations of linear regression, you do that. You can get away with it. Or you could do a decision tree. But decision tree, keep it very, very simple so that people can understand it locally. And so what you do is you create a loss function, which has two terms. You say, give me G is the here, G is the explanation, the surrogate model. You say, give me the simplest explanation, that explanation, maximizes the fidelity of G to F in the neighborhood of X, the point that you're trying to explain, point in the feature space that you're trying to explain, in the neighborhood of that, G should have a very high fidelity to F. Otherwise, you will have a loss. L stands for the gap between the two in some form. Now, this term, the second term, is the complexity term. It just guarantees that G cannot get too complicated. So in the case of a linear model, it's reasonable. But in the case of a decision tree, for example, a decision tree can become arbitrarily complex. So you need to guard against it by essentially regularizing it with a complexity penalty term. And so that's how line works. Now there's a bit of technical details of how it picks a point. What it does is in the neighborhood of the points it will do a Gaussian sampling and after that it will apply a weighted distance kernel. We need to give more emphasis to points which are very close, and then use that to build a local model. And it generates synthetic data by perturbations of the real data. Once you have the model, see once you have this black box model F, I can take a point and I can generate a lot of points in the neighborhood of that point. These are the perturbations of the data point. And so feed it into the function. Once I feed it into the function I'll get some output and now I have new synthetic data which I can now build a simpler model to explain in the local context. So that is LIME. So we'll do a couple of laps with line. Then there is Shapley. Shapley answers the question, well, in a sort of a... Yes. So what does fidelity mean, like in this context? Oh, fidelity. Fidelity is how close one thing is to another. So suppose I say say let's take an example suppose you make a curve like this and you have a sorry you make a line like this this is a you make another line which which goes like this d and then you make a third one which goes like... Of these, which one would you consider closest to white? Or the closest imitation of white? The yellow one. So you would say that the yellow has the highest fidelity of the three. Okay. say that the yellow has the highest fidelity of the three. So fidelity is how closely it mimics the other, so that if you could replace one with the other, nobody would notice. So that's the point. So that's fidelity. So now Shapley takes a slightly different approach. It doesn't take a geometric argument. It takes a game theoretic argument and does an interesting thing. It says that, see, if you want to find out how important a player is, in this case, a feature is, create every single model that doesn't have the feature. So that is obviously it's an NP hard problem because an NP problem, not NP problem, because it should be exponentially large number of combinations you can do or coalitions you can form without despair. And then each coalition, you see the performance of the model with the player and without it right and then at the end of the day you go and take some form of average of the difference between them you'll know how much this player has contributed then you do it for the other now this has one advantage it can be both local and global why you? You can do it across the board, and you will get a rough and ready global sense of feature importance. But more importantly, we know by now that, see, global feature importance is really not meaningful. I don't, to me, it's just a rough and ready measure. What really matters is in regions of the feature space, locally, how do the features matter? You can play this game locally also. You can find out how the model performs with and without a particular feature. You can come up with local values values and so therefore it begins to look like line locally but it takes a more game theoretic argument excuse me so that is that is shapely shapely again is is when you think through the implications of this you realize it's a it's a lovely simple idea but extremely powerful right ask if i have a quick question so when you say that you're removing one feature and then uh i mean you're reading the model in the first place and then you're removing one feature and then uh seeing what would be the output with the rest of the features going through the network no no no it's not like Okay, let me re-explain it. Let's take a game in which you have four features, just to keep it simple. You have features x1, x2, x3, x4. So what do you do? Suppose you want to know for x1, right? For x1, do one thing. What are the combinations you can do you can do so the here what are the this is called the power set in set here power set you create the power set your power set will contain zero it will contain x2 sorry a set of x2 right a set of just x3 a set of x2, a set of just x3, a set of x4, and then it will contain another set of maybe x2, x3. It will contain another set of x3, x4, and then another set of x2, x4, right? And then it will contain a third set of x2, x3, x4. Maybe I'm just writing it over, x3, x4. Do you agree with that? This is the so-called power set. These are called coalitions. In the world of game theory. This power set is essentially the coalition. So these coalitions are between the features, right? Four features. Four features, yeah. So what you would do is you have some target variable y. You would take the same model, but you would train it differentially. You will build a lot of models so take an example suppose you want to take this so you will build model a and model b model a will be built with x1 x2 x4 model b will be built with just x2 x4 are we together and so you would see the loss here loss a and loss b and what you're looking for is the difference between loss a b for feature one right and you do this game a b game with every element of this power set or of this coalition right and very much locally but uh it looks like we are building a model from ground up yes the original shapely says that you just build a model so what people do is that they then sort of do performance optimizations that bring in kernel methods or something to make it not so brutal, right? But if you really think about it, even with four variables, you see that the number of models you'll be building is huge. That is why she runs very fast. And you're only done with X1, then you'll repeat the same thing for X2, repeat the same thing for X3, repeat the same things for X4. And once you have found the average losses, the average differences that x1 makes, x2 makes, x3 makes, x4 makes, now you get a sense of their contributions. That's the point of Shapley. Now, when you think about it, it has a lot of nice mathematical properties which I won't go into, it's a little bit gets technical, but just assume that it has nice and robust mathematical properties and robust against one second it is robust against things like a lot of correlation between variables which you remember completely screws up linear models. So it is pretty robust against all of them. Yeah, Asif, you know, I got the intuition here of the global, could you just- What you want to do locally, what you would do is, suppose you want to explain this result. Let me call it X naught, which is made up of some value. Right? this result. Let me call it x naught, which is made up of some value. Specifically in case of Shapley, I got the global... I'm explaining that, just be patient. So of this particular point, right, and you have the y, right, And you have a model n, right? So now think about it in your complex manifold, right? That explains, it is this point. This is your x naught, all right? Now, what you do is you take a neighborhood of this. Again, I can take a neighborhood of this point and generate a lot of synthetic data. Isn't it? I can take this data, input data, first generate, first do the same thing, first generate synthetic data, data through perturbation. Perturbations are small deviations from this x0. And next, feed it into that data, feed it into the model, whatever model has been built that you're trying to explain, and you will get therefore the y of those values, right? Isn't it? You'll get the y-hats. values, right? Isn't it? You'll get the Y hats. Yeah. Right? You give this X in the neighborhood and you'll get Y hats. Now you have a data, you have a model, but now you start playing the game. You build locally, very locally, you do the same thing that we did. You figure out in this region, whatever this thing is doing, which features matter more. Are you getting it? Yeah. That's it. What you play globally, you can do it locally. And the power of Shapely, and when we do the lab with the shape package, the power of the shape packages that, see the shape pack, the library of in Python and it has been ported and all that. It is actually one of the libraries I like very much. It's very well implemented. I say that as a person was implemented it from scratch. So because it pretty much covers just about all the major things you can do with Shapely. More importantly, you see using that, that the factors that influence in one region of the feature space are very different from factors that matter in another region of the feature space. So I took an example. So if you live in a middle class commuter town where everybody wakes up, gets into their car or public transit, and then they all show up to work in San Francisco. That is one community. The house prices will have middle class values, right? Well, Bay Area middle class values are, I guess, a million, but let's take a normal US middle class values, maybe to 300,000. Then you go to Mac Mansions, another neighborhood where the house prices start at 5 million, right? So what is it? 17 mile drive? Is that right? One of them. Yeah. So and you come across all these big Mac mansions there. So I'm guessing that those houses are all in high millions. So now look at the one factor, proximity to highway. In the middle-class neighborhood, proximity to highway will hope, probably I'm guessing, be a positive factor for house value. Nobody wants to live in an interior where just getting onto the highway will take 20 minutes. Isn't it? Whereas in this place, in a Mack mansion area, they deliberately stay away from the beaten track, right? They'll be on the beach or wherever and they would rather no highway when near them. So there, the proximity to a highway will have almost an inverse relationship. Isn't it? You probably know, right? For example, in California, one of the biggest reasons we are not able to have high-speed trains is because all the wealthy neighborhoods have protested. They don't want the California high-speed rail to go anywhere near their towns, which creates a huge problem. So you see, at different places, different regions of the feature space, the factors that matter are different. And the same factor matters in different ways. Here it could be a positive factor, there it can become a negative factor. Are we making sense, guys? Is the intuition coming across from what I'm saying? Yes. Yes. That, yes. That's that. Another example is diabetes. In a five year old, you don't want to ask the child to lose weight, most likely five year olds are three years old, not obese. I don't know modern world is different. But normally, you know, five, six years old, skinny, highly active kids, if they have diabetes, you don't suspect it is because of bad lifestyles or something. You suspect more fundamental causes. In a 40-year-old, you suspect that if it is adult onset diabetes, that there's some lifestyle component has something to do with it. So what factors matter depends upon the region of the feature space. And that is what we are talking about. And that Shapley and Lange, both of them local interpretations, they give a very good job of it. And in terms of visualization, Shapley produces, the shape package produces beautiful visualizations. It really helps you understand what is going on. I like this. I'll start the lab now by doing that, but before I do that, let me just finish the review quickly. That was that. Lying shapely, we did that. The last we talk about is when it comes to images we talk about the saliency maps and the saliency is what matters what in a picture if you have a cat why did you call it a cat right where were you paying attention to most right so as you probably know human beings uh when it comes to animals it's a easy answer human beings at least are extraordinarily uh biased towards facial to to bias towards the face towards the eyes of any animal right eyes and uh mouth and so forth you know the claws i don't know the face of an animal all animals are keyed like that. They look for each other's facial expressions, mammals at least, right? But it raises a question that if we recognize animals by looking at their faces, what does a neural network do? It turns out interestingly, as we'll learn in this lab, that neural networks out, interestingly, as we'll learn in this lab, that neural networks actually, surprisingly, they come to pretty much a similar method. They learn to pay attention to, or they create a saliency map in which they're paying more attention to, or they're much more sensitive to, changes in facial regions. And how do we do that? The theory is very, very simple. We realize that if you, the derivative, another way to interpret the derivative is the sensitivity of the response to input. So if you look at the derivative of the response with respect to each of the input features, you will get which feature is proving much more sensitive in an image because each feature is a pixel, which pixels are important and therefore you'll realize that the pixels around the face have an extraordinary influence in the decision that the neural network is making, in the prediction that the response from the neural network. And that is a sort of just elegant and simple argument. Of course, you have to cultivate that argument into more and more and more things. So it uses the gradient. Now when you do this, the only difference between back propagation when you're training a neural network and when you're trying to explain a model is in training, the weights are adjusted the weights and biases are the parameters of the model and they are learning they are fluid they're changing they're getting better once a model has learned you freeze it no more back propagation of the parameters and updates of the parameters right so remember the step was you go the forward pass compute the output compute the loss from the loss so here's the thing you you go the forward journey you compute the loss you take the gradient of the loss number one you take the back propagation back propagate the gradients all the way to the first layer the weights everywhere so all the weights know the gradients and then the next they first layer the weights everywhere so all the weights know the gradients and then the next they all take one step forward right the gradient descent step and that's how they progress but in this the model has already learned so you freeze the weights and biases what do you do you still do the forward pass but you don't take the gradient with respect to the loss. You instead take the gradient with respect to the output. And you need not take the softmax output in case it's a classification problem. You can take the one just before it, just the raw score, because softmax doesn't add any value to you. So you can do that, but output, at least conceptually. And you take its gradient, and you start back propagating the gradient with respect to, and until it will back propagate with respect to the weights, but finally it stops because there are no more weights, but you force it to go even further, one step further, to take the gradient with respect to the inputs, and so you come up with the sensitivity of the output with respect to the input just a little bit of code and then you are able to do that and that helps you create these beautiful saliency maps and then there is a the activation maps the activation map thinking is like in a continent you take the last layer and then you basically say that I'm getting certain values. Now, if I like at any given, what is this filter? What is this feature map trying to tell me? How would I find out? One way that you could do it is you could just shake up the inputs, right? To figure out that where, if this particular feature is looking out for what in the input image and so forth. So there are all of these different ideas that you can do. We talked a little bit about the deconvulation and so on and so forth. I won't go into that anymore. We have covered all of that. We talked about the occlusion-based methods and we'll do that in the lab. We took the example of the lung pleurisy, I believe, is the filling of the fluid where we have a very clear angle that the physicians look for. And if they don't find the clear angle, as in this picture in the left lung, it gets blunted or it becomes smoother, then they know that there is a filling of water or fluids in the lungs. So then the question is, what does the neural network look at? Surprisingly, it turns out that neural networks are well trained, learn exactly the same thing. They manage to understand that feature and zero into it. You can do it through saliency maps. You can do it through this, that, and you can do better still. You can do by occlusion studies, right? Or ablation studies. You just remove little pieces of the image and see how much your performance or your score degrades and prediction score degrades. So that is it. So with that being there, that's a review guys. Should we now make progress towards the lab guys? Yes. Let's get to the lab now. Mine is kind of hanging on that plotting the individual tree just before the shapely the shop library oh that tree it takes a long time to draw it's a very busy so not your laptop it's just the nature of the beast so guys i'll start by showing you a simple situation let's go to the california data set simply because you have all done the California data set, isn't it? And let me do this. In the California data set, if you remember, is the font reasonably... Okay, sorry, it became too small. Is this good enough or I should make it bigger? Make it slightly bigger. Slightly bigger, okay. On my machine, it's looking, on my screen, 15 inch screen, it's looking huge. It looks decent on my, I'm not sure how many inches okay um one thing i find useful is that when i get a new notebook it's got all the nice output of it working yes i do a download as html yes i know how it's supposed to look before i start trying to run it and have specific environment issues in my machine yes environment issues in my machine. Yes. Alright guys, so I'll start with the California data set. Just to wet your recollection, you have done this in your past. Anybody remembers doing the California data set for regression? Yes. We all did that, isn't it? So this is that. So let's go and add Shapely to it. So if you remember remember we loaded the data then you visualize the data but at the end of it i won't go through the bulk of the analysis because you by now you should it's become almost a textbook case everybody knows what we did you do the missing value analysis you do and i won't go into it you can use the profile this thing I won't go into it. You can use the profile this thing to get a profile report. Let's move past that. You can do correlation studies and all of that. At the end of it, you can draw it upon a map and this is your just to visualize the data. This is your data by different factors. And this is it, just to give you a visual reminder that this was the data, housing data, house prices. And the house prices seem to depend on one, two, three, four, five, six, seven, eight, nine, nine features. Like 10th being of course the target value, the median house value. You have to predict how much is the median house value in a block in a city block or a street block so if you remember the data showed a pronounced skew if you you get better models if you address the skew so this was your heat plot and so forth address the skew. So this was your heat plot and so forth. Correlations and so forth. So all of that I'll skip. So data pre-processing. The first thing is you have a categorical variable, namely the ocean proximity. So we need to do one hot encoding. This is doing the one hot encoding. But something else, because we saw pronounced skew in some of the features, what do we do? We take the log transform of it to see if it makes it better. And when you do the log transform, the situation certainly becomes better. The skew is pronounced skew disappears from those features. disappears from those features which is a good thing because as we know when we experimented and found out that it does improve the quality of the models that you build with it so i will go ahead with it and i'll go straight to one of the modules random forest we built a random forest model which seemed to have uh what was it a coefficient of determination of uh here 80 84 person give or take another question is once you have this model right and we realize that in every way it looked good the residual plots looked good the error plot looked good but by the way this is this very, very complex tree that you're trying to visualize, Kate. It's hopeless, almost. So the thing is, random forest, people often use it for feature importance. And what I'm trying to tell you is, no, don't. It's misleading, actually. Mathematicians have written fairly detailed articles why it's a bad idea. Rather, you shouldn't trust it. Sometimes it works out, it happens to be correct. Quite often it happens to be correct, I would say, but it may crucially fail. So go with the gold standard, go with things like Shapley line etc. So what would Shapley do? First we install Shapely. So here is the installation of the Shapely. Now, Shapely comes with two basic explainers. Remember, explainers are in some sense local models, sort of, that you can build and then you can play this game of what is happening. So you create data from the f function and then you try to explain it with a surrogate function and so forth. So there's a tree explainer which works very well with models when you're trying to do a surrogate for a a tree based models like tree and decision tree, random forest, XGBoost, et cetera, these things work well. But there is another kernel explainer, which works for all other situations. And you can apply it to neural networks and you can apply it to anything. So the equivalent of what this feature importance is, you can get from Shapley, and then you can see if they match or not. And they sort of seem to match median income, ocean proximity, latitude, longitude. Proximity, latitude, longitude. Then comes population, total rooms, total bedrooms. Population, total rooms, oh the order has changed here. House, median, age has turned out to be a little bit more important. So anyway, this is it. But this is again a global, very rough and ready measure. You shouldn't look at that. What happens if we look at the value for some particular data instance, right? We just take our particular row of data randomly here. You take a row of data and then you say, okay, we are going to do the feature importance for this row. It happens to be 1 60th row of data. So every time you run it, it might pick up something else. Or maybe because I did random seed, it might do the same thing. Now, this is a plot, it is called a force plot. So do you see this is actually I find this to be very, very useful. And there is a lot going on here. So let me explain what exactly is going on here. I'm sorry. I will let me first increase the font actually not decrease the size yes let's go here are we able to see it guys so the effects the median house price after taking the log transform So obviously it becomes a little bit because I take the log transform of things. So it's 13 Right. So you can think that e to the power 13 would be whatever it is. That's a median house price or the not median average house price expectation value of the median house value. So that is 13. Now the question is, what are the forces that are moving it here and there? For this point, the prediction is, what is the prediction? We'll have to see. Okay, I forgot to make the prediction for it. That would be that's an illustrative thing i forgot you just make a prediction on this now the question is what factors are affecting it so here it is saying that and it does seem like that median income is a positive force. Do you see that it's red in color? And it is pushing the value higher. Means if the median at this house price or in this particular local market segment, the more the median income of people living around you, the more the house value. the more the house value right and median income seems to be the most the biggest sort of the positive force on the other hand the the biggest negative force seems to be what can you guess guys what does it say the age yes the median age of the houses in that block. In other words, how old is that neighborhood? It turns out that in this particular place, the older the neighborhood, it has a negative effect. So it begins to push it down for you. Then proximity to ocean, ocean proximity inland. So it basically say that wherever this is, if the house is inland, strangely enough in this particular price segment, it pushes the value of the house up. Then longitude and latitude. Well, if you ever talk to a real estate agent, you know, what do they say? That the only three factors that matter in real estate value, location, location, location. Well, it turns out that they would be surprised if they saw this, because here it's not just the location. It is the latitude and I mean, is the location. Location comes after. Well, sort of ocean proximity is location related, after income, median income. Then the location factors come. So the red are arrows that pushing it up. They're saying that these factors here locally have a positive correlation, are we together, to house price. And the H has a negative correlation. And it need not be always true. And as I will show you for a different point, we will come to a different conclusion. So now what happens is here is this plot, which is Asif, where is H? I don't see that H there. Oh, the blue, okay. You have to hover over it. If you have an HTML copy, you can too. Yeah, so now this one sample by similarity, you get, you know, you can pick the values. This is something very interesting actually. These are called force plots. There is a lot going on here. It is nothing but, see here it is for one target value, right? A house price value, right? And it is with respect to the, sorry, I take that back. Hang on. When I was explaining, I made a slight mistake. The base value, the average value log transformed is 12.08. This house seems to be at 13.05. This data point that we took, the median house value is higher than the average. Why is it higher? Because it turns out that the income seems to be contributing to it. It seems to be a slightly wealthier group of people, right? And location seems to matter, right? So always look at the base value and compare it to the value here, the value of this data point. And then ask this question, so which factors had positive influence, which factors have negative influence at that point, right? So the next is force power. So now this is for one data point. What if I could take all the data points, like in there, I believe 10,000 or more data points, 11,000 data points, and for each of them, there would be a certain Shapley value. And what I do is I, so you notice that the value is, fx is here. This is the value of the house. And the x-axis is the Shapley value. How big is the Shapley value? So obviously, at this point, the Shapely value, how big is the Shapely value? So obviously at this point, the Shapely value seems to be a pretty like a factor seem to have a median income ocean. Obviously every time it comes to the same thing, income and you know, housing data is in some sense a simple data, but if you look at it you can change it to other things for example you can change the axis by the way this graph has a lot going to it you can say a similarity or by population right so as population increases it do to the house value? So let us see. In fairly low population areas, it seems to have a positive effect. And the factors that are impacting, look at this, population seems to have a positive effect. And it always seems to have a positive effect from what I can make out. The more populated a place, the more the house values. Let's take something that may have a different interpretation. The ocean proximity is nice and simple. That's a whole lot of plots in one. It's a fascinating dynamic plot. Yes. Yeah, this is it. So what does this say? The ocean, if you look at the ocean proximity, the closer to the ocean you are, the more the value, the more it pushes up the house value. So would you say that it is true? It's one of the most straightforward plots. Yeah, that's right. So total number of rooms. Well, it turns out that the total number of rooms has a different sort of a, it's not very obvious how it is doing, but essentially it's a force for the good, it seems to. So longitude, latitude. so anyway, for different, it turns out that for different values of the number of rooms is still the contributions, the Shapley contributions are determined by pretty much the usual suspects. So anyway, this is called the force plot and there are more plots, there's a partial dependency plot, which I didn't do. So we will go through the documentation for Shapely a SHAP package and I'll walk you through more features. Now I have a homework for you. There are, I didn't do the partial dependency plot. It's very easy, a couple of lines, do that. There are a couple of more plots in the SHAP package. plots in the SHAP package, read the documentation, and become familiar with it, and use it. So you guys, the most important plot is this plot, if you can understand it. And also realize that, see, feature importance as a global factor is a very rough and ready measure. Don't trust it. Feature importance are much more sensible sensible locally because a highly complex manifold, you know, the curvature in any of the dimensions keeps varying all over the place. So feature importances are changing all over the place. And so go local and look at what is happening around the data point that you care about. So the same thing we do it with, I think I've repeated it with, whatever I repeated it with, support vector machines also I did it with, I think. Did I do it with feature importance? Which is it? No, this is for the tree. Now, let's see. Goodness, affect measure predicted error. Yeah. Oh, this is, by the way, a very important plot that I forgot to mention. So I've done it in the support vector thing. Guys, one second. I'll entertain the question in a minute. If you don't mind, is it okay if I just explain this before we take questions? Sure, sure, sure. Yeah, just a thing. So this is actually very important. If you ever want to look at just global influence, trust this picture more. What it is showing you are all these features and this is the Shapley value, zero. See, it is the absolute value that matters, not positive or negative. I mean, they exert force in the positive or negative direction, that is all right. Now, this is the feature value. So what it is saying is that the low latitudes seem to boost up, it turns out that they seem to boost up the house price values. Can you guess why that is in California? Is it like near the ocean? It's only with altitude. My interpretation for this is actually that it is San Francisco Bay Area tends to be fairly expensive, so it has produced. And the other thing also is too. So compared to the median, see what is in the center, the Central Valley, right? So low latitude and high latitude are probably, this is Los Angeles and this is probably San Francisco Bay Area, right? And they both have a strong impact on the median house price. So that is that. Longitude is the vertical lines on the in the clip. Here also you notice that they seem to have two clusters. The two clusters probably correspond to the LA area and the San Francisco Bay area because if you notice the state is along a diagonal. I don't know if you go back and look at the data. So that's the interpretation that I can think of. Yeah. Let's go back to the map. Yeah. So look at this, guys. If you look at this graph, Do you notice that the San Francisco Bay Area and Los Angeles, these are two population centers that differ in latitude and longitude both because the California state is a diagonal. Do you see that guys? And so you expect sort of two clusters when you see that map. Yeah, this map. So let's see what factors don't seem to matter too much. It turns out that total bedroom seems to have not that much of an impact. Do you see that if you have high number of bedrooms, low number of bedrooms still doesn't seem to have much of an impact so what happens if it is red and on the right hand side so if it is red and on the right hand side like for example this what do you think it matters it means that the bigger the the more red means high value right that feature has high value high median income leads to it is a positive indicator of house price does it agree with common sense guys yes that's right so that is what this is saying similarly uh let's look at this other thing uh i don't know total number of bedrooms it seems to have a positive correlation uh number of households i suppose it's a measure of how densely populated it is the sometimes some things look a little odd the the population low population seems to have a very strong impact on the shapely value value. I am thinking how that came about. And actually, low population, okay, this is high population. High population seems to have, do you notice that it's the other way around, it has a negative Shapley value. I am guessing that it's because these are apartment livings. Like in a block, if too many people live, those are probably apartments. That's sort of the interpretation I could come up with. Isn't it? Like, for example, the free standing homes in a block, there wouldn't be too many homes, but in an apartment complex, there would be a lot of homes in a given block as if sir so the value is increasing in x axis of the feature right no no no value the value the feature value is by color blue is low red is high correct that's for the shapely value no not the shapely value feature value so whichever feature you're looking at so suppose you're looking at population red stands for high population data for low population data right so here you seem to have an inverse correlation which probably should agree with our prep when we did the data exploratory data analysis we must have done that let's go to the exploratory data analysis and confirm whether that was actually true in the data it must have been true there must be uh okay here we go so we are seeing median house value and population. Yeah, do you notice that it's slightly, it is not blue, it is more on the mellow red side, isn't it? You see this population versus median house value where my mouse is? Right. It is slightly negative correlation. Okay. So that is, so that seems to be borne out It is slightly negative correlation. So that seems to be borne out by what this model is saying. So guys, this is it. These Shapely, these graphs that come out, you have to spend a lot of time because a lot is packed into each graph. It's explaining a lot actually. So you have to uh become i'll say for quick context question like uh at cornerstone an exercise like this yeah would be the data scientist or would it be the domain specialist who participates in this exercise see typically see our situation is different we talk to like see the problem is for example uh suppose i am looking at uh okay let me take an example. Recruiter efficiency, right? So the reality is, a company gets gazillions of resumes these days, right? We live in a dumbbell economy, which means that the entry level jobs are extremely hard to get. Actually, any given day, it's much easier to find a job, in my opinion, as a VP than to find an entry-level job because gazillions of resumes come from some of the common you know mainstream jobs the recruiter here's a dirty little secret and I know it from one of the biggest recruiting houses most places recruiters don't read all the resumes what they do is they have some computer system and they just do keyword search you see you know index uh text search a pull up a few resumes and from the resumes if some looks good you barely give like two three seconds five seconds to a resume that's all not even a minute just five seconds to decide oh you know do i want to look at it more carefully you take a small bunch of resumes you reach out to those people and you say all right let's have an interview so a tremendous amount of resumes don't get looked at and so forth so one of the things that i'm looking at is what factors are there in the resume that makes it more worthy to look at because every single company is losing out gems for For every great guy they hire, they probably miss 10 great guys who have applied but got ignored. It's happening. And the fact is when, so remember that, when you apply to one of these big boxes and you don't hear back from them, or you get a way of treatment, it may not be you. In fact, most likely it is not you. It is just that they are overwhelmed. So one of the questions is how do you determine based on all the factors of an applicant who, what factors are most likely to indicate a person of interest, a person who is worth interviewing, or you should prioritize interviewing. So that is one way of using Shapely. The other way in my case, when I was doing that early work in the startup, it was the happiness. Happiness is very direct. You have 200 factors. How much do each factors matter? You realize that is the central question, right? And that central question can... You're not making a prediction. You're asking a the central question, right? And that central question can, you're not making a prediction, you're asking a very human question. How much does each factor matter with respect to, let's say, attrition or productivity? Those are all indicators of happiness in some sense, quantitative indicators of happiness. So there it is shapely, right? So who does it? You sit with the domain experts to find what factors may be implicated. So you bring in those features, the Shapely analysis you do, and then you go back and show to the domain experts, let's see, this is what our data shows these what i sensed was the kind of thinking that goes behind using a package like shapely is a little bit of a higher level thinking you need to know something about the analytics of what you're working with but if you don't have a good awareness of the domain the insights don't speak to you yeah, right, you cannot extract information where there is no information. The purpose of AI is to squeeze the lemon to get all the juice out of it, right? But the lemon better be juicy. In other words, you better bring in data that actually has the relevant entropy, the relevant information. If it doesn't have, no amount of AI will squeeze anything out of it. You can't squeeze water from stone kind of a metaphor comes to mind. So once whatever forces are there, you can do a Shapely and see how those factors are affecting. See this sort of analysis interpretability to me is quite central. I care a lot about it. I feel that every developer should know about it. And because it looks complex and hard to explain to people, you don't find it in typical, you know, the machine learning books. These things explain very much. Though I would say that increasingly it is being noticed, right? Lime and Shipley, etc. They will gradually become mainstream, but they're not yet mainstream. Because this is very powerful. Like this is how AI can be put to work. Yes. To get the buy-in from an exec, you need to be able to substantiate. Yes, and this is a substantiation. This is the way to make a case for things. These things have very real impact, very, very real impact. So once again, you can do... So now let's look at another data point. So with support vector machines, I took another data point. So, okay, this is... Now you understand this and you understand the force graph and you do this. So I think, yes, go ahead. Because I was stuck with a question in the previous plot. So when you plotted the one, you had thousand or a hundred samples put into the Shapely and it generates this plot, right? If I'm right. Yeah, I'll tell you what happens. This computation is a little bit brutal. I had to prepare this notebook quickly, right? I didn't want my notebook to hang for half an hour. So I just sampled the data. Got it, I see. So I see, it didn't work on the whole set of data, but a sample data. Yeah. And then it drew its conclusion that's right i just took a random sample of the day because my thousand points imagine finding the the feature importance it is almost useless because do you really need to find all these points that look very very similar do you need to do feature importance for each of them because you know that it will be pretty similar in the neighborhoods, because they are all in each other's neighborhood. So the reason I'm asking this question is this is sharply on a regression model. There is other model that we look at mostly like a picture, like a 200-cross-200 cross 200 picture and takes only one picture and then puts the you know, bright red spots on the place that led to this particular class being predicted. Yeah, we'll come to that. We'll do it shortly. So I want to let's crawl before we walk kind of thing. So this was an exercise more in just getting you familiar with the thing and now that you understand Shapley in a see understanding it explaining it with image all this force graphs is a bit harder like here when I explain in terms of concrete variables median income etc it's more sort of a easy to. But I'm coming to images. So this was Shapley and oh my goodness, I am running behind in time once again, guys. So guys, is it okay if I go slow and make sure you understand or we should go a little bit faster? It's okay. Finally plotted that tree. It took 48 minutes plunging into the shop stuff now. Yes, yes. Have fun. And before you know it, I seem to be almost making a sales pitch for hardware companies, isn't it? Buy expensive hardware. You need to make it run faster. All of this. Anyway, so I'll take this. So I didn't take too many examples because the sharp documentation if you go to a sharp read the docs are you all of these links are there on our page and i'll also put it on slide copy paste it on slide it has a lot of lovely examples right we're going to look at some examples and uh let's look at, I don't know. If you look at, okay, this is, I need to increase the font. This is only image dataset. So what makes you decide, see, so see you realize that they're positions. So let me explain this graph is a bit hard to understand. This is the MNIST dataset. So how many digits are there? Digits are zero through nine, but there it just shows. That is the index. Imagine that horizontally, so forget the dark black. Dark black is the input. Each of this horizontal columns is placed for one of the digits. So this is the place for zero. And this is the place there for for nine. Raja Ayyanar?nilavakshmiasra?info Alright. Raja Ayyanar?nilavakshmiasra?info So when to when you give it digit two, it seems to go for this. And do you notice where is it putting the feature importance? It is not- We can go for all of it. Exactly. So in fact, it is telling you something negative. It is very clearly saying it's not three. If you know the shape of three, this, I don't know, it's a little hard to explain. Let me expand it a little hard to explain looking expand it a little more uh where am i okay so all right let me start from here uh start the image plots uh think about this do you see this little bit of blue yeah would you agree that if it was three, because this is the location for three, that's exactly where you should not find ink for a three. Right? Sort of. You wouldn't want to find ink there. Right? So things like that, you get the negative impression. And for where do you expect the positive impression to be? See for two, in the location of two, it's paying a lot of attention to this shape, right? So that is what it is trying to say. Where is it paying attention to? For one, of course it's paying attention, right? What you can't make out is the whole, the fact that it's a square pixel. So in the whole square pixel, for one, it seems to pay attention to literally the center, the vertical line, isn't it? You can, if you look at the attention, where it is, like which features or which pixels it is giving importance to, they're exact, they're almost the shape of one isn't it guys this is almost the shape of two this is almost the shape of zero and this is almost the shape of four and as we use other techniques you'll see that we'll be able to recover pretty similar results so you can see for the four and the nine on the far bottom right, that blue line saying, well, there's no line there. So it has to be a four instead of a nine, right? Exactly. So it is paying attention here and saying, oh, there is a gap. And by the way, here, this is missing, right? And therefore, this is a four. So, you know, this is it. You can have a lot of fun. I won't go into a lot of this. You can have a lot of fun when you do the text data and you see, for example, if you're doing sentiment analysis, the very basic thing. Let's take an example. I think, yeah, autotokenizer. This is obviously the hugging faces, right, which you are all familiar with hopefully by now. So you're giving it data like this text is angry, this text is happy, sad, etc., etc. You ask this Shapely to explain it. So what is it doing? It is saying explain it. So what is it doing? It is saying that, well, this end, the thing, it is coming up with features, funny, has a positive, nice, this is it, faced with tears of joy, it sort of latched on to some, I don't know what it was doing here. So I would ignore that funny, nice, haha, lot. These are positive connotation, even lost to, lol. These are positive connotations. Even lost it gave positive connotations. Beaming face with smiling eyes, right? It gave positive things. You can ask it, what are the top negative features? What makes it give you negative answers, right? Then look at that stupid crying face, sad robot. Oh, India seems to have a negative impact, that's sad. Tough words towards sad emotion, pain, crying, sad. You see how well these things work guys. This is computationally a bit expensive. Shapely as you will realize when you do the lab, is expensive, but it's really worth it if you have the time to do it. I think, sir, the other one, the one about this, where it was the one about this, sir. The top negative words for positive emotion. Can you scroll up a little bit? Here, this one. Yes, here. It says the top two words uh no no no the one where india was there sir india was there negative words towards emotion happy yes negative words but they they represent the positive emotion looks like oh they represent the positive emotion interesting crying face side reward perhaps so this is it i mean you can the point that i'm doing is these are fairly versatile tools that you can apply to this is why the question actually came to my mind both in the image as well as in the nlp you're just giving one sample and then it does the extraction of the feature importance the reason is you've already trained the model you're you're giving it a model and saying explain this model's prediction for this sample and explain why it is saying what it is that's the way to put the context the context is i have a data point it has made a model has made a prediction and you're saying okay here is the data point here is the model explain to me why did this model make this prediction here that is the problem i see so still a game theory has been used here to build uh of course because you can perturb and generate a lot of data around it and build a go about playing games got it okay got it if we assume there is no limitation on the hardware computational power there is no limitation on the hardware computational power, then can we say that Shapley is better than the first method where we were trying to basically create simpler model on the local neighborhood? See, generally, in some sense, Shapley is considered, SHAP is considered a superset of LIME. It is considered a superset. line. It is considered a superset. It subsumes line. But line is cheaper and faster. Right, right. So it depends on the time that you have. You want to do line. Like, for example, I always start with line. Even though I know that Shapely will subsume it, I'll start with line. Got it. Thank you. So this is a guy said, you notice that for words, for each of the sentences, the problem affecting people using older versions of the PlayStation three. So this is a blah, blah, blah, it is attaching is telling you where words matter, which words matter positively, which matter. So I won't go into more of it because we have a lot of territory to cover, but I hope you got the sense of Shapely, isn't it guys? And the sharp thing. It works for everything. Actually one thing that I like for, I don't know, it's a little bit, yeah, Patrick. Patrick and Sipwal. Either of you are here today? Sipwal, you are here. Patrick, where is Patrick? Yes, sir. Patrick isn't here. Okay, you'll like this example. Okay. So you're familiar with genomic data, isn't it? Yeah, these days, by the way, I'm researching genomic data at this particular moment. So in a different research project. So look at this course. So when you have seen the gene sequences, right? It is telling you which part of the gene sequence matters, do you see, are highly important. Yes, sir. Right? Isn't it amazing? Right? Exactly for this situation, what matters? I find that so lovely, you know, just to see this. In the long genomic sequence, oh, pay attention here, here, here, and so forth. Do something. So lots of things are here, guys. Tabular data example. They have other examples of tabular data using all sorts of things so we can go you can literally go gaga over it um we can do the diabetes oh yeah so iris i'll take let me show iris because i'm going to use iris for the next example for line so this is iris right you do so suppose you take the simplest model k nearest neighbor neighborhood based method what it is telling you that there is some data point whose the average base value is 0.35 this thing's value is less than that it's zero it seems that less than zero so it is pushing it to pushing it hugely in the negative direction because it turns out that the petal sorry this is the overall i don't know what is this overall value in a i think what is the target variable they have taken let's go and look at it it data split iris accuracy format okay kernel k nearest neighbor explain expected value shape value i location so it is saying that in explaining this data point the most important factor was petal length and because the petal length was short it pretty much could decide for that and this is pretty much giving you an idea so by the way this does this does this make sense if you know the iris data keep this picture in your mind the reason I'm saying it is it has a meaning to it the output value is here the output value is actually zero to zero one two three so it not yeah um suppose you say the effects this but anyway these things will remain the same keep these things in mind because i want to show you something in a little bit support vectors so anyway you can apply all sorts of models to it. Let me take a more different example. This is too simple. I'll use classification. Diabetes. Diabetes is more interesting. Let's go there. So diabetes dataset, as you know, it's one of the standard datasets. So here the target variable, I don't remember what it was. It was, anybody remembers what was the target variable in the diabetes dataset? We can look it up quickly. Was it the sugar level or something? It seems to be almost the sugar level, isn't it? So this looks like the sugar level or something? It seems to be almost the sugar level, isn't it? So this looks like the sugar level, right? Sometimes I do run this if I'm not careful. So it is, so okay, let's assume that it is that. I should have done due diligence and seen what the target variable is. So look at this, base value is 151 in this group. Obviously you're looking at people who have diabetes you're saying that if this particular patient seems to have this value what are the factors that are giving huge contributions bmi would you agree that bmi is a significant contributor to type 2 diabetes, the bigger the BMI, the more likely you are to have type 2 diabetes. Dr. Yeah. Dr. What is the first advice the doctors give you the moment they notice you are a bordering or you're getting into type 2 diabetes territory? The only one thing, reduce your BMI, lose weight. Then sex is a negative factor. My guess is that here it is taken as a number and women have been put zero and men have been put one because women have a lower incidence of diabetes from what I remember. These blood pressure seems to have a positive impact. Does that make sense, guys? Yeah, high pressure is not, blood pressure is not good either. That's right. And so we can see those values because it's a little opaque. I don't know what S1, S5, etc. It's a little hard to tell. And what you see is that, again, you get a map. So what things have a, so looking at this, let's look at what things have a direct positive correlation with Shapley values. The red and to the right, BMI. You see how important BMI is. Also this S5, whatever factor that is. And blood pressure. Get your blood pressure in order. Right. And I suppose this is not actionable, but it's basically saying women are likely to have a better outcome. So, well, that's that. I don't know how the gender is specified. I've forgotten this data set, actually. I haven't seen it in a long time, but it sort of makes sense, isn't it, guys? And then you have a dependency plot. What is a dependency plot? You take the BMI of people and you ask this question. So suppose you ask this question, see I know that BMI is important but what is the most important context of BMI? See, are two people who are 30 pounds overweight, one of them is 30 and one of them is 50. who is more likely to have diabetes the older the older one right so this thing has picked on to the fact that when you talk of bmi it interacts most given all other factors it interacts most with age most given all other factors it interacts most with age so it is the coming together of bmi and age that sort of determines the impact and one thing you notice the bmi has almost a linear relationship with the shapely value you see that right as you increase the bmi over the opposite this is respect to the average i'm guessing your shapely value begins to keep climbing up but at the same time they have color coded it by saying look at the age you notice that these places there are still some blue low age people with very high bmi i suppose in this particular community they were young and obese people I suppose in this particular community, there were young and obese people. That's very interesting. You seem to see a pretty interesting age distribution with respect to the BMI. I didn't expect that. So does it make sense? Forget the shapely value. Just look at the BMI and the color of this age. Is this something medically that you guys see that the BMIs are now pretty like spread out over all age groups? Yes. If you are. It doesn't matter if you're eating much, you know. Yeah. Whatever your age, you know, then your bm is high right here that's true makes sense well guys here is the force plot so at each value of the output here you can see that if i'm looking at let's say 41 or yeah 40 like you can just go up and down and we can see what is going on at this particular level what matters most so what is the thing having the most strong positive impact sex bmi and s1 whatever that is the one that is pushing it down here bp here in this particular range, BP and so forth. Now, the high impact areas at this moment, if you're very, very heavy, you realize that, do you notice that this graph is trending upwards? Right. In other words, high impact or high shipley values happen when you're rather lump then almost all the factors seem to contribute quite a bit. Jay Shah, MD, This thing is mostly read Jay Shah, MD, Whereas for very low impact areas are generally for relatively lower weights here. lower weights here. Anyway, so you can play around with it, guys. You know, these things you have to sit and think a lot about and come up with that. And this again shows that BMI, if you force it to have a Shapley, here it is engaging with S5. Why are they? Okay, the decision tree is coming up different plot it seems right so that is that guys and so enough of shapely let's move on to the next one which is uh and by the way this is the package if you want to go to github and download it and so forth this is the package all you have to do is pip install shaap. That's it. And the example on the GitHub itself is very well explained. One good thing with shaap is the quality of the documentation. Absolutely lovely. Now let's move to line. So before I go to the more complicated examples, let me go back to the notebook that we were dealing with. Where's that notebook book on XAI, my simple notebook. Let's start with simple. Oh, by the way, Balaji, thank you for pointing me to Captain. I was going to- Captain, yeah, that's what we are actually moving to. Oh, okay. Yeah, so I'll also move to that. Actually, I didn't realize this. I was using flash torch and something else my team was using. I forget what. So here's the thing guys think of okay so you guys have a homework this lime i'm doing it for iris and because you know we did shapely for the housing data set i leave that as a homework for you right so now let's look at lime line also is very easy to install just give this command it will install line do i need to explain this line guys what does this line do what does this cell do loads the data and does what splits into train test exactly and then this builds the model and predicts it so obviously i haven't put explanatory comments because i assume that by now this stuff has become kindergarten for you you get an accuracy score oh goodness grief this has come up with a perfect score no it shouldn't be like that let's run it again and hope it's not perfect this time yeah better 94 percent good 94 95 percent iris is a very simple data set of course you expect Yeah, better, 94%. Good. 94, 95%. Iris is a very simple data set, of course, you expect. So what I'm doing is I'm taking a random row and saying, explain this row. And when you ask it to explain this row, it says, if you look at this row, by the way, this is sepal length, sepal width, petal length, petal width. And what is not obvious from here is that the petal length and petal width are actually small relatively small right because if you look at the picture you'll realize that petal length and petal width what is the average it's about about, let's say 1.5 give or take for Petalbit. And for Petalbit- Is this a lot of work given earlier or now? Come again? Is this a, I only saw the housing notebook. Oh, this is a different notebook. This is the, I'm giving it to you, XAI notebook. This is the Iris dataset. So Iris dataset you're familiar with. this is the distribution of the iris data set so what is the average value of the petal length it's around four give or take isn't it look at this scale goes from one to seven so probably four and petal width it is about 1. So relatively speaking, you have gotten a data point whose values, petal length and petal width, are very small. Isn't it, guys? Does that make sense? There were like three clusters. Two of the clusters are close to each other, if I remember. Yeah, yeah, we'll come to that. But I want you to pay attention to the fact that in the IRIS dataset, at this particular moment, the average petal length is about four, and average petal width is around 1.5. And whereas here, the average petal length is 1.3, way below normal, and average petal width is also 0.2 way below 1.5 approximately right and so the prediction from this model is that it will it belongs to the zero class zero class is the cetona the sit there are three classes cetona versicolor and virginica right zero one 2 and in this graph 0 is marked as red so let's go and look at this diagram and i want you guys to pay attention to this diagram these diagrams are saying the same diagram diagonally right a petal width in other words they're just rotated versions of it in which the axes have been reversed so here what do you notice about cetona? The petal length and petal width are very small, isn't it? Yes. So if you have to find cetona, look around, would you, so think about the factors that will individually help you decide what is relevant. Can I look at this and tell that i should use sepal and sepal width to decide between the two probably not it's not a clear distinction this one maybe i could use sepal length and petal length potentially but then i'll get a mixture here. If you really think about it, of all of these, the cleanest is this. Do you realize that? Both along horizontal and vertical axis, you can create a box. You can draw a horizontal line here and you can draw a vertical line here and the red ones, the cetonas are well separated out so just looking at this figure you know that if it is cetona the single most important factor would be petal length and petal width are we understanding this explanation guys yes just look at that right and now let's see what is it saying? Oh, it's saying that its prediction probability, it is very, very sure that it is Sedona. And why is it Sedona? Because the length of this is very small. It is much smaller than these values. Right. And the feature values are 1.4 in this is the value that you find here. And it is much smaller than that basically, right? So it latched onto the truth. It latched onto the fact that these are the two features that most matter, right? So here we see the average values are 4 and 1.5 and this guy is like, yeah, okay. So what happens now, suppose I force the, force it. If you believe that petal length and petal width are most important, then would it be fair to say that if I exaggerate the petal length and petal width, it won't be Sedona anymore. What is it? Sedona. It won't be Sedona anymore. What is it? Sedona. It won't be Setona, Setosa, sorry. What am I saying? I went and stayed in Sedona, so I keep confusing this with that. Setosa, sorry. I think I've been saying- No, you meant- What not? All right, so you realize that if I boost it up, I will be here, isn't't it if i increase the petal length and petal width i'll be somewhere here and i clearly won't be a citosa make sense guys so that's what i do let's see what happens i deliberately in the same sample i exaggerate the petal length and petal width by hand do you see me that the line 62 is it self-explanatory guys yes yeah you do that and when you do that ah immediately it says this 83 percent show it's a versicolor right which by the way this green here is the versicolor right so it seems to have guessed it properly and you ask it what matters now, it again says petal length and petal width. Let's see if that makes sense here. Still it seems to be, it is not so well separated but petal length and petal width are still, if you look at the other figures between them, is still the best separator. So if you really think look at this figure you realize that globally this this tends to be a pretty good way of looking at it so this is it now install captain you have to just give this command i'll go to the captain website to show you so guys are you understanding line was that easy yes Yes, sir. Yeah, straight. So now let's go. By the way, I'm not giving you guys a break. I apologize for that. Let's go ahead and finish. The next thing we'll do is CAPTM. So CAPTM models. So pre-trained. Let me take some good examples. Okay, this is a good example. This is an example I have taken straight from there. I just downloaded it into my local, just so that I could show it to you in big fonts and give you the commentary along with it. So CAPTEM is very well integrated with PyTorch and thanks to Banerjee for introducing along with it. So CAP-10 is very well integrated with PyTorch and thanks to Banerjee for introducing me to it. So I've just been playing around with it in the last few hour, one day. So let's look at this. Guys, does this code look all like by now worth yawning at? This is obvious code, right? Remember image processing, these were the transforms, these were the normalizations and so on and so forth. And then you take ResNet, you take a picture, swan, right? The name suggests it's a swan. You take the transformed image, you know, you do all of the usual things we do in image processing. Just go back to your very, very first homework. You'll remember that. Now you have a model. ResNet is already able to do things. We are going and we ask it to predict what is it. And it says that it has a 46% chance that it is a swan. Sorry, it is a goose. Right. Now the question is, why is it calling it a goose? Let's go find out. So this is you use integrated gradient. Remember gradient is the same thing that I taught you about taking the derivative, the gradients, and from looking at that a sophisticated version of that. You do a little bit more but pretty much it. So this goose, when you ask it to create the map for you, gradient map, see what it has done. Look at this picture a little carefully, guys. Do you see something like a bird here? How many of you can see the outlines of a bird? The darker points are the points with greater gradients, as you can see. It's like its beak is pointing toward the right, as an ostrich-like body. Isn't it? It is, right? So it knows. It is able to recognize that this is not a house. This is not a ball. This is not one of the many things, aeroplane, and all the gazillions of things that are there in the ImageNet data sets and all that. But it happens to be a bird. And how did it... So then you can do some further noise filtering and so forth. So there's a bit of technology guys. So you play with the derivatives, you set to null, you smooth, you do this and that. And now, the same picture, you improve upon it a little bit. And now what does it look like, guys? You look at this picture. And suppose you had not seen the left one. You just see the right one. This is what the neural network is paying attention to. Does it, I mean, would it begin to look like a bird to you? Yes. Or a kitty? Yes. Mutualism. Yeah, pretty close to it. And see what this is the image. So you notice something very interesting. It has latched onto the feathers, which is a giveaway, and it has latched onto the face, which human beings tend to do right and the neck so if you really think what makes a goose a goose or duck like a swan so it is mixing up its swan ducks and goose but we'll sort of forgive it right so it's doing yeah then and so there are a couple of different techniques you can use, gradient sharp and so forth. So what has happened is that there has been a whole body of work, all leveraging that basic idea that gradients are measures of sensitivity. And so, and all those two, three ideas and the attention map and things that I taught you, a lot of variations of that and using this, you come to this map. And then the other approach was occlusion. Which part of the image, if I remove, will my score become the lowest? So it seems to be paying a lot of attention to the top. Do you notice that? Guys, do you see this? It's for some reason, it's paying a lot of attention to this, that if you remove these parts, it won't be able to tell what it is. It won't be able to tell what uh what it is it won't be able to classify as well so probably especially the beak seems to be the most critical for the model to predict so see um what i'm wondering here is so if this image where the the image of the swan if it were shrunk let's say 50 percent yeah is it likely that it will show the same area in terms of where it what it uses to distinguish or do you think it'll change if there are enough pixels you can assume that it will the reason for that is remember in congregation that we learned about the affine transforms the features that you discover are invariant of location and size okay okay invariant of location and size okay it is that it strengthens its weakness okay so guys is it fun i don't know ask one question yes so the um the picture is the input to the model itself, not to the LIME or the datum or CAPTM package, right? CAPTM is just actually trying to explain how the model came to the conclusion that it is a GOOSE. Exactly. See, the way it works, Rafiq, is just to recap. You take this picture, you give it to the model, it will come up with a prediction. It will say, I don't know, what did it say? It said goose, right? Now you go to CAPTEM and say, explain to me why did this model say goose, right? So then using one of the means within CAPTEM, all this attention, grad, so on and so forth. It will say, oh, it called Goose because it was paying attention to these features in the image. Right, yes, got it. So I think ask if the picture is given... You can just ask quickly. Sorry picture is given as the input to the model and to the CAPTEM also because you are asking it to see the way you do it is okay here is a model. If you give the picture to the model it will make a prediction. Now you give to CAP to captain you say okay here's my picture here is my model explain what in this picture this model is seeing do you see that so then you're not saying this is the label uh goose is given as input to the captain as well like the label output prediction uh typically you don't because you're just looking at the derivatives if you if you think about it uh visualization transpose original you notice that you traditionally don't give i mean you don't need to because it's looking at the derivative of the in the gradients the derivative with respect to the input remember our if you remember this formula okay I thought it was other way you yeah yeah so some algorithms do it so depends on that which algorithm but this particular gradient based remember what are we looking at we are looking at the gradient with respect to the input you see this this expression here right and that i generalized to this the gradient of y with respect to the input gradient of the response with respect to the input so what it does is it can compute it some libraries will say give me both or some libraries will say if I have the input I can produce the output on my own from the model remember it has the model right but the two things that you must give is the input in the model otherwise it will know what to explain and if you don't give the input and if you don't give the model it will not know what model it is trying to explain? Because the reason I'm asking is it's a multi-class problem here. So there could be a duck, goose, swan, and we need to know in the picture what the model needs to explain. So let me answer. See, here's the thing. This particular problem with nest net, the way it has been trained is in a large class of objects. So getting a swan and goose mixed up, like pretty much is doing very well. It's a bird, it's a water bird, right? Because they're trucks, they're aeroplanes, they are God knows pencil and whatnot. Right? Where did this notebook come from? This is literally from the CAPTEM website. Ah, okay, thank you. So I'll walk you through all the lovely tutorials they have in a moment, but I want you guys to understand that this, in a way, tells you what the neural network is seeing where it is paying attention to. Right. And you can work so the homework that I'll give you guys is do this. In this code right, all you have to do is go to this directory. Where is this? You see this directory and change this with some other object in the ResNet. When you run this, you'll get your ResNet data. Change it with something else. Make it a dog, make it a house, make it a truck, airplane, whatever it is that you want it to be right and then run it again and see what is the neural network paying attention to you will see something interesting you'll see how the neural network thinks are we together guys so guys is it is, is it understandable? I think that this is quite straightforward, isn't it? Any confusion so far? No, not that I know where the notebook's from. Yeah, that's right. It's straightforward. Yeah. Dr. Raghuram G. There are too many techniques. There are like at least 10-20 different techniques brought under CAPTUM, which also includes Shapely within it. I went through all of them. It's too difficult for me to know. Few of them you described I understood, but the rest is difficult. Dr. Raghuram G. Would you like to understand all the rest? So let me do one thing. Yeah, certainly I need to actually. Okay, I'll keep it. You know, one of the Sunday sessions or something. I'll keep explaining. See behind all those techniques, the papers that are there, now I'm familiar with those papers. So I can explain those to you. They're not very different. Like once you get the big idea, the gradient and the activation map to core ideas and the back propagation is being stopped, right? And so on and so forth, the filtering and so forth. There are only three, four, five good ideas. Now you start mixing them up and then you throw Shapley into the mix also while you're at it. Yeah. They are mixing Shapley and the integrated gradient, forward path, backward path, decon. Yeah, yeah. There's a lot of them there. You can do a lot of fun stuff with that. But you're just doing permutation and combination of just a few small set of ideas. But there are a lot of papers that have come. And if you really look at it, right, I mean, I'm sure that the researchers did excellent work, but in hindsight, they all looked like, okay, a small set of good ideas mashed together. In hindsight. So let's go to the CAPTUM website, guys. And there are a lot of, so I strongly encourage you to go to the CAPTUM website and you will see a lot of explanations. It explains BERT, it explains the word, which words and so forth. So let's start with CAPTEM. It gives you this one, hunger. Let me just make it big. It gives you obviously a lot of examples with things it will tell you this is your average feature importance in a global perspective right this and that and so forth okay i won't go through this by now you understand what feature importance is positive and negative so the first example is too trivial let me interpreting vision with c far so yeah this is, this is CIFAR. Let's see what it is looking at. What it is doing is integrated gradient saliency, dip, lift, all of these are common things. I didn't teach you the noise tunnel, but it basically has to do, let me remember, noise tunnel has to do with the back prop. Okay, I'll remember. I forgot. Okay, so you take the CIFAR data, CIFAR 10 means there are 10 classes, right? What are those 10 classes? Plane, car, bird, cat, deer, frog, etc. Let's see how well it does. Now, do you remember that CIFAR 10 has been trained in very, very small images, I believe they just 32 pixel by 32 pixel, tiny images by modern world standards. This class guys, when you look at it, tell me that this looks like basic neural net. Two corn blares followed by dense layer and then you're done. Easy code, right? By now you should be. Why cross entropy loss? Because it's a classification problem one of the 10 classes this is the standard you know training loop you run that what what have you done after you have trained it okay you are you're predicting ground truth is cat ship ship plane and it is predicting cat ship ship ship it's calling this a ship not a plane right actually there's so little pixel of information, I can quite understand why you would consider it a ship. But okay, let's see. Now, ask it to explain. This was a ship. Correctional image, so the one that it got wrong, this was the one that it got wrong, right? Let us put the gradients and see where is it paying attention to. So it turns out it is paying attention here, a little bit more here. It has latched onto the fact that this is most important, right? And if you look at this part, it actually doesn't quite see the shape. Can you just look at this and say that it is a plane? If you're paying attention here. That's really hard to tell. It's very hard to tell. So this is the one that got wrong. This is another example. Well, it is the same example. Here's a feature ablation example. What is feature ablation? Covering it up. So you look at this image. This is why it's an often quoted image. There are three wine bottles and a computer and a lot of clutter in the background. So if you do image segmentation, you'll say three bottles and a couple of screens right so suppose you have to tell the wine bottles right classify the image and you classify the image as wine bottle if you think about it let's look at the influence map for this. So things that are deep green convinces you more and more. They have strong impact in helping you classify it as wine bottles. Would you agree that this region, this deep green things, will this region, these pixels will help you decide that it is a, or strongly convince you that it's a wine bottle. That's a very good match. It's almost, yeah, a perfect match. At the same time, these have negative impact. Means it says that this is clutter. This is actually unconvincing you that it is a these are wine bottles right it reduces your conviction that these are wine bottles isn't it this screens because the screens are the clutter what happens if you remove so okay so this is again uh you do that i think you can check that if there's an occlusion right so inspect the influence part of alternative classification. If you were classifying it as screen, computer screen, I believe that's what they're doing here. Let us compute is the same image class ID 664. Okay, as a class as a TV monitor screen. So if you want it to be TV monitor, then what these two two become the positive factors for Shapely, right? Or for the, sorry, for the positive influence. And what do these things become? The wine bottle from the distraught letter. Yes, exactly. They unconvince you or so they shake up your conviction that these are screens. And so what you do is suppose you apply a mask, a black mask here, and then reclassify the image. What is it predicting? It is predicting it's a desktop computer, right? Output is a desktop computer. The pointer class is amongst the top five. So this is it. So this is a way to understand how the computer how the neural network is understanding things and you can do it with i mean obviously the simpler ones are the boston housing price right you guys all know we have done we have done uh but do you guys remember doing boston we did boston crime we didn't do the Boston house price, did we? In ML100, we did Boston... Yeah, and R. Yes, we did some of these. So yeah, it's a very straightforward data set, you can look it up. Now, I won't go into too much detail, but if you look at the different attribution algorithms, they will have slight differences though, but they will all more or less agree so it is this is one then nothing very different you just have to become familiar with the api and all these apis are straightforward and with that guys this is your lab So what are we supposed to do? I'll also post this XAI notebook in the website. And I'll also post these links. These links are already there on your course portal, but I'll also post it to Slack. These examples, try to reproduce it, guys. Bring it into your own notebook or download this notebook and reproduce it. It is really worth reproducing it. And you see here it is, playing with this in your own notebooks. Take all of these examples and play with it and then apply it to your dataset. In fact, apply it to the California housing dataset. So this is that. Then again, you can apply it, Shep, for PyTorch also. When you do that, this is this. So play with this, mnest, play with all of this. Play with Lime. The Lime website again has this issue. The Lime again has a lot of examples, right? So here it is. So look at this image. If you, what factors convince you that it is a cat? The green ones and what is sort of reducing your conviction that it's a cat the dog here does that make sense guys yes okay so this is it you know after a little while it all begins to look very intuitive it's these tools are very powerful tools i believe you should ensure you should use it every single time that you do model building you train a model must go and do explain create explainable models guys create explanations for the model after that it is doing due diligence otherwise it can lead to a lot of grief so so I'll stop with that. And any questions? Guys, do these labs and the ones that you see here, Captain, Lime and Shapely, these three. Just go and learn about these three and learn to use them api as you notice was very simple i showed you in my notebook very very simple couple of lines three four lines for each of them maybe five lines my california housing shop is at 16 now making progress good uh one question is there any uh we have these three different libraries um is there any specific field that we can use or uh or specific types of models to explain or they all can be equally applied to any type of model? For deep neural networks then, you have to look at gradient based methods because they will be faster than SHAP. SHAP can be computationally brutal, like Kate is finding out by running the notebooks. The gradient based methods will converge very quickly. There's gradient maps and things. So anything that has grad in it, gradCAM, so gradient and attention map put together, just the gradient thing, it's a gradient map itself. Then gradientSharp, gradSharp, which hybridizes shapely with grad. So usually, if you're dealing with neural networks, bring in your grad-based something, or attention-based something. Then, so not attention, activation. Why am I saying attention? Activation-based, activation. These ideas are cool. But if you're outside the deep neural network thing, then you can't quite get into that actually you can use that those ideas but generally shapely and line are your tools or they are my go-to tools like the sharp in particular i use a lot simply because of so much you can you know, unpack and understand from its graphs. So, Asit, the CAPTUM also includes LIME and Shapely libraries now inside of it. Oh, inside of it, is it? Yeah. They have a gradient SHAP and kernel SHAP. It's different. We are putting the things together. It's not different. But the LIME is also supported here if i i just always overview a couple of times nice look good i'll exclude this more and see what else it has There's a wealth of information guys. These are not the only libraries but these are certainly good libraries. If you stick with these, pick it up, you can pick it up in three, four days and use them for the rest of your life. All right guys, with that uh i am done with explainable ai at this moment there are many techniques we haven't covered but if we have to cover it all in a week i suppose we learned enough i introduced you to a field and now you can go and explore more and more and more biology you feel because you really cared about this topic, did you feel you learned something in this week? Yes, actually even before you started, I went and saw that particular package you had listed. That's where I started getting the idea of which paper, Shapely is where I started. Those were the ones which helped me. yeah so yeah but anybody else who's going to use this stuff explainably i use it guys use it anybody who's totally confused and needs a review so we all seem to have understood isn't it all right guys so this is great so ask if i on on ending remark i want to say something. In my team, there are people doing image modeling, text modeling, combined modeling, everything. And by just having an expertise in this particular tool, I can work with all of them and learn all of them while also contributing to their work. This is very interesting. Yeah interesting yeah see at this moment it's a very low hanging fruit biology most people don't think about this they will train the model very due diligence you know religiously they will look at the performance on test data and report it but that is not enough this is the very low hanging fruit you bring value into an organization just by doing this and we see a lot of team there's a backlash from a lot of teams because they couldn't understand the models our team builds and it doesn't perform well in certain areas and they go i mean they go to rule base uh i mean they don't they don't have belief in your network yes all they need to do is just do that you know the gradient maps you just make the map and see why is it classifying this for example that airplane right or why was it being classified as a ship and soon you understand why right you understand where it is getting confused and once you know that you can then tweak your algorithms and you it's very easy to tweak it and tweak it i think the pre-processing tokenization is where the nlp will benefit a lot. You know, when it predicts that this is a positive emotion, it will tell you. It will highlight the words, hotspot the words that it paid attention to, you know, bird-based. That is the bird-based stuff. By the way, if you're looking at it here, remember, this is much better done. I introduced you guys to Alan NLP. Okay, anyway, so you see this highlighting of words. But for this, I would like to just remind you, Alan NLP, that I introduce you to. Let's see. Let's choose a text how many run and it picked up two right if you ask for a simple gradient visualization what is this guys integrated gradient visualization smooth grad this is one reason I have been saying to you guys that saliency map, now do you understand this guys? These words you would not have understood before this week from Allen NLP. And remember for NLP, I said just stick to three main libraries. One is Allen NLP, Hugging Faces, and Spacey. And see, it is telling you, right? It is telling you where it is paying attention to the gradient map. Here is the integrated gradient map. You can ask it to interpret. Actually, this part I love about Allen NLP that it has being a very modern library it pays attention to interpretability or explainability and so you see it here come through there see i missed a couple of topics. For example, the adversarial attack. There is this classic thing that people realize to their great surprise. You take a perfectly good red car, you give it to the neural network and it says car. And then what you do is you take noise and you add noise to the car. And then what you do is you keep shaking the noise you keep changing the noise shape so that its confidence in car keeps going down and suddenly it is not recognizing this as a car but it's calling it something else maybe boat or i don't know what whatever else it is it is calling it something completely different and so what you have done is you have you have done an adversarial attack with a noise which if you impose on the image to the human being it still looks like a car but the neural network is now confused right so there are all sorts of things interesting uh side plots and themes here in this world of explainability so i won't go more into it please do go go play. One nice thing with LNLP, this demo is that you can see a lot of this interpretability in action. So please take this as a homework and play with this too. So this one Balaji for you will be useful for your textual part? Yes, I think the one of the authors three years ago who published this shapely package. Yeah, this from the NLP team, I guess. Oh, he's joined there. No wonder he's brought all his assets here. Yeah, so that's where I started looking at the shapely paper. I think his name is Lundberg or I don't know exactly the. Yeah, that's right. You need to watch it. The person Lundberg. So he has now joined Scotland. He has joined the Allen. Allen NLP has a lot of momentum. That's why I say Spacey, Allen NLP and the Hugging Faces. Most of your NLP tasks are taken care of. For semantic modeling, Gensim, that's it. I personally almost rarely ever use NLTK and other libraries. So our third homework assignment is to do what with LNNLP? Just go and play with it on the website. You know, when we went to where is the LNNLP? See, play with it and see the interpretability, how it is interpreting it. See, for example, all of these visualizations. At this moment, it has hung because it's obviously computationally a bit brutal for it. I don't know what it will do. Okay. All right, guys, that's all there is to it. That finishes our last lab. I am very eager to guide you guys on the project. So coming week, we should start. One of the things I'll do is you guys don't know how to take a machine learning model and take it to production in a high-performance way like create for example a native Linux binary out of it and I'm interested in yes and then take take a take it to, I don't know, take it to Kubernetes, scale it out, deploy it. Those are important things, guys. If you can do end-to-end, you can be a one-man startup. You have picked up a lot of things. You guys have all gotten interested in Streamlit. I hope some of you did React. Some of you took some other approach, I hope. Your different approaches to the UI. Some of you did it in plain of you took some other approach, I hope. Your different approaches to the UI. Some of you did it in plain HTML. That's all good. You all picked up a little bit of rest or microservices, but you haven't picked up some of the ways to make it robust and scalable so that you survive when large traffic comes and hits your portal, your application. Most, you know, the irony is that a lot of companies die on the first day because they can't take the traffic and then people get a bad impression and they never come back. Apparently it's a known thing in the user interface world and site reliability and all that. world and site reliability and all that. All right guys that is it. Thank you.