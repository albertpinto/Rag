 you rohit oh he wrote a lot of papers yeah they should yeah do you know him i think i'm getting i need to close this browser it was one-sided all right guys we are going to do image bind one embedding space to bind them all this paper is again it belongs to the class of multimodal learning and it is a good one to start uh it's been now 31st march it's been a couple of months old now right and much has happened since then but it's a simple paper the neat idea that works and therefore that is this virtue the basic idea in this paper is that if you um and we talked about it in the morning that let's start with clip. If you remember, what does clip do? Given an image and given some text, I and some text, let me call it the modality of text, text modality you know that these two go together if you scrape it off the web and you're lucky right so on the web you have alt you know image with the alt tag img src is equal to the image file, i, and alt will be hopefully the text, whatever text is associated, the caption. It doesn't quite always work because people sometimes skip the caption tag, but even when they do it, they typically do search engine optimization. They tag in the company's name or they just stuff it with all sorts of keywords to bring Google traffic to their site. But so given all those imperfections, Facebook was still able to scrape a large data set. That was moderately effective. And if you remember, one of the achievements of Blip was that you could use it to clean the data set that was used to train. So blip was self referential it cleaned its own data set and retrained itself. So that's the history. We have covered that in the previous course. So here, what we do is we know that if you have a vector space let's say that this is your latent space you have a way of doing semantic embedding of a text of an image and a text into this space are we together Are we together? You can associate text to this image. And the loss function, if you remember, what was the loss function? You use a triplet loss, contrastive loss. Contrastive loss. Well, it's not just contrastive loss, but it builds upon that. But the basic idea is that the loss function is the distance between an image, a text, relevant text, let's say, or actual caption and something random like you want to minimize this distance and maximize and therefore with a minus sign distance from image to random so if you were to do that trained with this loss function and how would you so this makes the question how would you, so this begs the question, how would you first convert a text to vector? How would you convert text to vector? We have been doing that, right? For example, we have been using sentence words and so on and so forth. And so the next step is how do you convert images to text, to vectors? We convert images to text to vectors we convert images to because for example use the same thing break it up into patches treat each patch as a word or a token right vit right therefore you can get a vector representation for the image also then you apply contrastive learning to this and therefore you come up with a semantic embedding for images. This was the gist of the clip paper and then you can do it better with blip and so forth. Now comes this interesting question. Let's look at a modality like audio. Let's look at a modality like audio. Suppose we take audio. What does what does an audio file look like? It will be some squiggly along the time axis, isn't it? And there will be different frequencies frequencies different amplitudes and so forth there's frequencies are in the squiggles amplitude is in the height of this signal are we together and then this is the time axis now order signals are usually studied using many many ways but one of the ways that you can study it is by doing something called a Fourier transform. And in fact short interval of Fourier transform, you take a snippet of that, a little part of the this thing and you take the fourier transform if you take a fourier transform of a little bit of a signal or what do you think you'll get you'll get all the frequencies the harmonics that make up that segment doesn't it and those harmonics they change they keep moving as you keep moving forward in the slices. So let's take one particular slice. In this particular slice, you will get frequencies. Each of these frequencies will be of different intensity. Isn't it? So what you do is you put a point there based on the intensity of it. If it is, so you can imagine that you will get up with some different frequencies a histogram of the frequencies are we making sense now what you do is you convert the histogram into some sort of a color map right red for big or something like that somehow don't keep it as a histogram because it's eating up one dimension, but change it to the color spectrum. Make it a color or make it an intensity, right? Something like that. And so what you will end up with is that for each point in each segment, let's say for this segment, you will end up with a column of color at different frequencies, f1 all the way to low to high frequency. You will imagine that these are segmented. You would imagine that each segment would have a different level of color and intensity, right? That's it. Now, as you go from segment to segment, this is your time axis. This is your frequency axis. What you will end up this is very sort of, I mean, obviously, if you come from signal processing domain, you may cringe because I'm oversimplifying things. But basically, this is your. What is this called anyway it's your spectrogram spectrum spectra the frequency part of it how do you go to the frequency domain this is your fourier transform you do a fourier transform now there's some technicality to the implementation of that short interval Fourier transform, but we'll come to that. So you get a picture like this. If I give you an audio signal, would you agree that it will become a picture like this? Now on this picture, there is, this is called a spectrogram but usually you tend not to use it directly because the human ear responds to frequencies differently some frequencies we don't hear at all there's only a range of frequencies we hear and then within the range of frequencies it turns out that we are extraordinarily sensitive to shifts of low frequency so when i'm talking to you to shifts of low frequency. So when I'm talking to you, small frequency changes will have an effect. On the other hand, if you go to the very shrill end, like a whistle, then small changes in whistling, we are not as receptive. Very shrill frequencies or high frequencies all we can feel is high frequency so what happens is that our sensitivity itself has a log scaling behavior right we you want to sort of what you want to do is squish it up the the things that are at low frequency wanted to be more dominant because subtle changes mean something at high frequencies, it doesn't mean as. So you need to expand it longer. And so that is called the metal transform. So you apply a form of transform to it. And at the end of it, what you end up, this is again very oversimplifying things, because there's beauty and there's a subtlety here, but this is your MEL spectrogram. Spectrogram. Forgive me if I butchered the spelling, but think i got it right spectro oh yeah rt is reverse i knew something was wrong spectrogram are we together this image but what is this melt spectrogram at the end of it it's an image isn't it so when it is an image and you are thinking multimodal embedding. Now can we do embeddings. We can. What can we do, we can take yet another vision transformer, open up its ways. Again to contrastive learning. Keep the image fixed because image we have learned how to embed a genuine image picture of a cat. Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam in the vector space you see how easy it is right and and then of course as a random like because you're doing contrastive learning you can take the meow of a cat right the image of a cat and the image of a house right and obviously the there should be a vast difference between the sound the meow and the house how is speak, they don't meow. Things like that. Are we getting the idea? So now we got sound down. Right. Let's look at so now we can do so we can do a joint embedding of text, image, and audio. And so one beautiful thing is that image is in some sense, the hub, not text, image, because quite often every other modality is somehow associated with image. You write something, you almost feel compelled to put an image there. If you do PowerPoint, perhaps it contains more images than text. Images are everywhere. See, eyesight is perhaps the most dominant quality of human beings. We have far better static vision than I think most other species. So you have the image I think most other stations. So you have the image. And going back with the dictum that image is worth, a picture is worth a thousand words. It just shows how compellingly we respond to images. You take that, you take text. We just got audio into the mix. We can align audio to image. Why? Here's a cat picture and meow right so we can do joint unwearing now you say well that looks good while we are at it let's take a thermal image a thermal is just an image it just is taken at a different electromagnetic spectrum the sensors are at different things right and by the way most light sensors they also read thermal images i i don't know if you know so when you get this high-end cameras or most cameras digital cameras these days the manufacturers actually have to put a thermal filter in it why because otherwise the thermal picture will show us without uploads right so you have a thermal filter so it can be used as a military which is so i'm told so this is how i was told that most of these devices are the sensor spectrum it't, it's not like that. There's a hard cutoff suddenly at beyond red, it won't go to infrared. Wow. So they have sensitivity there also. So you have to explicitly put a filter. So you can, so there's thermal. So thermal image therefore is an image at some sense. Then, so thermal is taken care of. Then you can go to depth sensor, maybe sonar things, and you bounce back and what do you reconstruct? A picture, isn't it? Picture is picture, doesn't matter how you reconstructed the picture, isn't it? So depth. So you realize that once you get your image thing down, one second, a lot of other things sort of become freebies after that. You just need some way to convert them into pictures. So depth echoes, you need to convert those echoes into a picture and the technology has existed for a long time. Go ahead, Prakash. So in the depth, I saw some example while reading this paper, the depth sensor, when it converts into the 2D image, we lose that, right? In the image. Yeah, the way they do it is at the gray scale, things that are further away and things that are nearer, you use the different gray scales for that. So once you know that it is gray scale, you can tell which is near and which is far. That's a fact. And then there is the next one is the IMU data. Now what in the world is IMU data? Yeah, the accelerometer, gyroscope, etc. It is international inertial measurement unit and it has a six degrees and six channels so it's just images with more channels right signals with more channels in that's what it is think of it as six like for example if you're looking at a gyros, you can almost think of it as a six channel audio signal. As the car is bouncing around or the object is bouncing around, you're getting the yaw, the pitch, the forward and the backward movements and so forth. Yes, sort of like that. Yes. So that itself can be converted into an image so. Is that an audio thing or is it the whole. The audio as well as the vision. No, no, so like, for example, acceleration data. You go from here to la so you accelerate you D accelerate accelerate, but at the same time, your car is doing as you make sharp turns the pitch and the yaw of the car the forward like you hit the brakes and your car goes like this you accelerate your car goes like this so is it okay it's a whole video of the situation or is it an audio no just just the numbers all different numbers okay numbers so it just says acceleration at this moment is this this this goes up goes down goes up goes down copies there slow down okay right so things like that so that's that's the data so all of that data you realize that once you figure out how to convert audio into picture the rest fall begin to fall in place pretty quickly isn't it similar techniques then comes the video video of course the the way these people did is nothing high-tech what they said is over two seconds or three seconds i forgot what they just take two frames so one image has three channels rgb channels and then add three more channels to it for the second image. So that is again an image and images we know how to make into vectors. So what do we learn? This paper therefore becomes using six different modalities. Actually when you think about it, it is like literally putting the last block of the puzzle in place. Somebody, I mean different researchers have built all these different pieces, isn't it? And you come and put the final piece in place and say, yeah. It's still a great, it's great work. I mean, this is how research works. There comes a moment where you realize that all the pieces are in place. You just have to put them together. Like integrating the core, you said. Everything is there, integrating the core. you realize that all the pieces are in place you just have to put them together like integrating the code you said different everything is there integrating the code integrating the code exactly go ahead so uh so for example in perfect one we did this semantic search right so uh text has a certain semantic meaning to it right and here you describe audio spectrogram which you said basically converting audio to image and mapping it to a space now how's this marriage of the text semantic meaning that the space to an audio's uh meaning how are the two brought on the same scale okay so i'll say that uh uh the shanti adiv has asked this question how do you make the fact that the text has came images we brought into the fold, but audio How do you do that. Well, it's very intuitive. What if you're literally reading the text and speaking without. So then obviously the text and the word has literally the same meaning. But now let's take it a bit further. Suppose you see an audio of like I took the example of a youngster swishing by you at 120 miles an hour. Right. So what is the audio? You would see the Doppler shift, the Doppler sound, the frequency of that increasing and then decreasing. Literally, you can see it in the spectrum, in the frequencies. So you get a unique audio signature from that. It will have a unique signature. And you can see that, by the way, these metal spectra are very beautiful. There is a car misfiring, a gunshot, all of them have unique signatures. You and me speaking, we have different signatures, even when we are talking about the same thing, it captures our accents and so on and so forth. So now comes the thing, a car goes by and you have a picture of a car, swishing by, you know, you see the bloody trail of a car. What do you, what do both of them convey? A speeding car. So you could therefore train the audio encoder, the spectrogram encoder, the visual encoder, actually the real visual encoder from the MEL spectrogram to adapt in such a way, learn from the loss function in such a way that it lands next to the picture of a car swishing bar. And therefore, semantically you have aligned the meanings of three modalities now. Audio, video and audio, image and this. And from image going to video is very easy. You saw that literally have a depth image, you can imagine that the car was far away, right, especially now it's coming at you and then it's going away from you. So you can align things, basically start aligning things together, meaning wise. But the beauty of this is from the fact that, oh, let me move this picture. Why is this become so big? Sorry, guys, I need to. Give me a second. I seem to have I don't know. Give me one second. I seem to have lost the ability to. Okay. All right, I'll leave it. Something is, are you guys able to see my screen? Yes. Yes, the paper. Oh, the paper. Yeah. Excellent. So this is something weird is happening. I will actually close this and reopen it guys and give me a second. Image bind. OneNote. Yes, much better. So now let's read this paper. Why am I getting two copies of the paper? Yes, much better. So now let's read this paper. Why am I getting two copies of the paper? I have no idea. We'll ignore the second copy. Yeah. Okay. So let's read this paper. What this paper says, guys, do you get the main idea of this? We can do joint embedding therefore. Now, what are the, so what are the virtues of the joint embedding? We're using it. So this works really well with retrieval. I can bark. Right. And as such, results i should get dogs isn't that is one so you can use it for retrieval task you can use it for classification task what can i do suppose i have a chirping of a bird and the bark of a dog i want to classify it between birds or dogs i claim that the moment you have this embedding you don't need to train a classifier an audio classifier at all you can actually use this joint embedding to do the classification how can i do that Yeah, all you need to do is take this bark and then you know that it will land somewhere in the vector space. Now just create a query that says this is the sound of a dog barking and another sentence. This is the sound of a bird chirping. Right. And code this into into the latent space and see it is closer to which of the two sentences are their vectors. Isn't it? So if it is closer to the dog barking, you heard a dog bark you see how simple it is so therefore you can do literally classification and this is what you call zero shot classification you didn't train a classifier at all it came as a freebie a zero shot usually means freebie so after all there is such a thing as a free lunch right let's see if i have a question, similar to the, you know, the clip embedding of symmetric cross entropy where you maximize the similarity of the bark sound and the picture of a barking dog. Yeah. Yes. You take the picture of a barking dog and then you train it together cross train it together during the training process hey asif i have a question sure um if you look at the bottom last two sentences of the introduction, it goes over the two-dimensional thing, but then it says the video audio embeddings cannot be directly used for image text tasks and vice versa. So does that sentence mean image bind is not good for videos? No, no, no. It is actually critiquing other people's works it's saying that in the prior work that has been done if you look at this sentence the broader context is uh give me one second read this recently many methods learn image features aligned these methods use a single however so it is pointing out to the limitations of those methods, not its own method that it's going to propose. Right. So what it is saying is, think about it this way. You have a fundamental problem, right? Suppose a video vi and an audio ei how would you create a joint embedding in a vector space you don't know right now you can try some things if you build a model only of this it will be just a very specialized model of these two it won't be text aware or image aware but if you use image as a hub these two, it won't be text aware or image aware. But if you use image as a hub, and you surround it, which is the idea that these people have, and everywhere, audio, video, text, modality, all the other modalities, because they all are associated with the image very often. Right? Therefore, it's very easy to align them into a joint embedding into a common embedding. Thanks. Yes. One question on this different audio or different persons, let's say Asian or Indian or different audio, how does it convert and refer the same image? So suppose you have a person talking and you have an audio signal. It's a person talking at the audio signal. And you have a picture of the person. It's a little harder because the picture is just static. So that is not a good training data. You take things where they are related. So now let's read the paper. But before we read the paper, let us look at the possibilities. Do you understand how you can do classification? But you know vectors, one of the good properties of a vector is you can do vector arithmetic. Right. So what you can do, and this is the thing, if you look at the item number two here. First of all, look at the row number two here first of all look at the row number one cross model retrieval you can give it the crackle of a fire and in the search results you can get images and videos of the fire that makes sense you could get depth pictures of a fire how far away it is any any sense any depth image associated with a fire, and you could get text associated with a fire. A fire crackling and so forth, and you see the results joint embedding produces relevant results. Likewise, a baby going. You see all of these descriptions come back. So that is a cross model retrieval is the core benefit that comes. Everything after that is a consequence or a corollary of that. Let's look at the number two, embedding space arithmetic, because you can add and subtract vectors. Suppose you take the vector of a bird and then you take the vector of waves, right? You could do it by just giving it sound of a wave and the picture of a bird, or you could have given it the word bird. And then you say, take these two vectors, add them up, and search for the joint for the A plus B. Suppose the vector is C. C is A plus B b c is bird plus waves search for the c vector so what will it do what will it be closest to in images it will be close to something that contains both waves and birds a bird on a beach isn't it and you see that so for So for this. To be using tools or can you just add what the records and. Amazing isn't it yeah. But if you think that this is a vector arithmetic or vector. So, then it can produce a different combination, right? Different pairing. Like let's say total is five. So one plus four is five or three plus two is five or two plus three is five. You do have that issue. All these arithmetic based systems obviously have this issue that we can do that. In reality, these vectors are very high dimensional vectors. Remember, you're talking scalars like three plus two but in a 20 dimensional vector with a unique vector for a bird and a unique vector for a sum it's very difficult to add up to another final vector and get the same answer by randomly taking a few others you can come close to it but it's not as common as this and remember these are very high dimension vectors 768 and so forth so it does that mean that this is only solving one set of problem not all this is very retrieval this was yeah this excel set retrieval and all the side benefits of retrieval right so for example as a retrieval task you can literally give it burden some waves and if there are in your database in the vector index that you have created if there's a vector indexing of a bird on a beach, it will find that in the census. isn't that amazing. Right and. Likewise, audio generation, what can you do? You can take a dog, right? And it will generate the picture of her. You can use it to, so generating is a little bit more hard, but let's take it like that. You can give it a vector and say, what does it correspond to, right? A create a dog or a engine a fire a ring and so forth so we let's go through this paper a little bit more carefully because we have a lot of paper to read but this is the core idea are we getting it and uh the the um image bind is just for vector operations right like generation we would have to use a different one. No, no, it gives some, see all of these are encoders. In general, encoders are not that greater generative tasks. Little bit is there, but take it like that. So, but does that mean that image bind operations require their own encoders or their own? Yeah, that's what they train. Yeah, exactly, they have their own encoders or their yeah that's what they train yeah exactly they have their own that does not like if i change some model on my search retrieval system yeah that this will fail yeah you have to be careful like use one encoder image bind is sort of one encoder to encode them all right and image is the binding the central element that you use and in a way it leverages clip the like things of clip open clip etc it's pretty good and in fact your exercise is to play with it in a project and see how well it comes out yeah yeah but the whole point is not to give you hints before yeah question you said this is great for classification and retrieval task but if somebody wants to do a synthetic data generation yeah based on image or audio what would good way to do that oh we are literally coming to that topic so hold um hold off for a little bit longer when we come to animal we will be talking a little bit about that When we come to animal, we will be talking a little bit about that. Okay, thank you. So, generally, let me put it this way. When you have things like this, and you bring in stronger generative elements, you get better results. So imagine you feed this into a diffusion model. You realize that now you're taking strengths of a generator and the strengths of a retriever so the vector that image buying gives how do we you know the generative model might not be accepting the same vector like how do you yes you have to put some glues in between you have to train a little bit all of those are there. And that's how so many papers are coming out. Conceptually, it's not hard. In practice, of course, everything needs GPUs, everything needs effort, everything needs data. So we read the abstract of this paper. We present image bind and approach to learn a joint embedding across six different modalities. Images, text, audio, depth, thermal and IMU data. Does that make sense, guys? We show that all combinations of paired data are not necessary to train such a joint embedding and only image paired data is sufficient to bind the modalities together. to bind the modalities together. And therefore, emergent behavior. See, image is the hub of all the other sports, right? And so what you can do is you can actually go across using the audio, you can retrieve images, bark, you can retrieve dogs, even though the system was never trained for that or classified. So this is emergent behavior, new behavior that just sort of emerges freely for you yeah that's it um i have a question regarding this vector arithmetic so it's essentially like mean pulling that we are doing no mean pulling is different mean pulling divides by n okay this is this adding straight as b okay so that's that so we show that all combinations of paired data are not necessary to train such a joint umbrella which makes sense because it's a hub and spokes model you're not doing every possible combination and training and that is the key feature that is a key feature. It's a much simpler approach. You are not going to create 30 pairs of data, vast data sets, audio and thermal, so on and so forth. You don't have to do that. ImageBind can leverage recent large-scale vision language models and extend their zero-shot capabilities to new modalities just by using their natural pairing with the neighbors. It enables novel emergent applications out of the box including cross-modal retrieval. We saw cross-modal retrieval. Composing modalities with arithmetic and cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state of the art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. And this is the interesting thing when you do that, it actually beats sometimes models that were specifically trained for that combination. Right? And why would it do that? My interpretation is it's a deeper learning because any information has multimodal representation. And in a way of forcing a model to recognize all of these different representations as being the same thing leads to a deeper semantic understanding of what it is. Right? So perhaps that's the reason finally we show strong few short recognition results outperforming prior work and that image bind serves as a new way to evaluate vision models for visual and non-visual tasks right so it is good result and of course these days it's almost hard to get any paper accepted unless you say that it does better than the state of the art. Yeah, I just I'm just imagining a scenario. Asif sir ringing this bell and his photo shows support vector class, not some other temple or something. Yes, that's true. Yes, that that's true and that is multimodal so i will go to the heart of the algorithm with this you guys have oh what happened to the rest of the paper yeah okay so anyway i i seem to be in the right space a zero shot classification i already told you how, why it happens. You can do that, binding images to modality. So guys, this in this entire paper, I believe this is the only equation. So let's look into this equation. Right. So what they're saying is something very well argued simple and simply said uh take this suppose i is an image are you guys able to see my writing yeah i stands for image in this equation and m stands for any other thing but image it could be an image also it doesn't matter any other modality so what happens is you are doing joint embedding between an image and something this means image is the hub right so now what would you do what does contrastive learning say if i take a query so remember what is a neural network you need one neural network that will take an image neural network you need one neural network that will take an image image i and convert it into a vector f so every neural network is nothing but a function isn't it guys it's nothing but a function and so this is a function that will act on i and let's let's call it qi it is the embedding vector and let's let's call it qi it is the embedding vector for that image are we together guys likewise so this is your neural net made up of your transformers these days transformers it is and let's say that you have another modality think text audio is, right? And that also is one network, neural net, right? Neural net. So you can think of that, that also is a function. So it will produce K i. So suppose you have a modality i, some modality i goes in, it will become Ki, which will be equal to g of Mi. So far so good. And now think about it this way, that if this is true, what do you want? What should you, what will the similarity, what will the similarity what is the measure of similarity in high dimensions that works like dot product and if they are normalized vectors it's cosine right so dot product so that would be this q i transpose k i right another is the same as I write it in this language Q I. You notice that I tend to use a slightly different notation for doctor, but basically this isn't right, but these two is a doctor. dot. or scale dot product, it could be the course. equal some in this case they are picked dot product now and you could if it is a normalized vector then it's the same yeah then into normalized embedding so normalized means it is actually the dot product it is the cosine but it becomes cosine go ahead so am i any modality any modality any modality any possible images i i and any i of any modality could even be an image something like that now comes the thing this is a similarity measure what do you want to do you want to do? You want to maximize this qi, ki somehow, and you want to minimize qi with kj such that j is not equal to i. Right? You want to minimize and you want to maximize. And so if you think in terms of logics, you remember that dot products can have any sort of values. You always normalize them, exponentiate them so that they all add up to one. So that's what you will do. You will say e to the q i k i, right, divided by everything else, e to the q i k i, that is one possible pair, and all the things that are not, dissimilar, q i k j, over all j not equal to i, isn't it, in in the denominator because that will be all the other logics right so let's say that the first one produces three the other ones are whatever they are three divided by that that will make it into a probability isn't it and exponentiation makes it stand out the big guy stand Now, you remember there was also temperature. What was the purpose of temperature? It's to damp down. So we can bring that in. If you notice that if I scaled it by temperature also, I could do that. By now we are familiar with temperature, isn't it? And you understand how easy the argument is? And what have you reconstructed? You have reconstructed the probability of the right image. You want to maximize the probability. For probabilities, what is the loss function that you apply? Negative logs, cross entropy loss, minus log. This is your loss function, therefore. And this equation is exactly the equation that you see here. I'll let you see it if we did the same thing. You look at the equation that is in yellow and the equation that we created is it the same equation right so this is the reasoning guys it's a very obvious reason this is obviously the the the the famous info nce loss equation but it has a very intuitive uh explanation of why it is the way it is. Makes sense, right? That is it. And so this is it. You use this and then you can have emergent alignment from all sorts of cross modalities. Audio and Kamal or whatever it is, right? So the rest of that is details of how you get the data, what you do, and how well it does. And you can see that if you look at these numbers, they are doing pretty good actually. You realize that doing image bind actually leads to close to state of the art results and often beating the state of the art results. to close to state of the art results and often beating the state of the art results. But I want you guys to in your project, this is your project guys, go use Imagebind, keep your other implementation and compare the results. Are we together? Go compare the results of multimodal search, go that see how well it worked so you can do few short classification right and and so forth so i'll let you read the rest of the paper right uh there's more to it but uh i suppose that's the gist of it i would like to move on to the next one like the mel spec program Are there interesting anything specific for IME data or like? But check it out. There must be. I didn't look into too much for me. It was theoretically obvious that you would do something like this, but find out what they do. I would like to know that. All right, guys. So that is finishes our first paper reading. We'll take a little bit of a break. Then we'll come to Laura. 10 minutes break makes sense. And Laura will be a segue into the second half of this bootcamp. Till now you have just been using models to do your work. Now you'll have to train the models, fine tune the models. And so LoRa is a good segue into that new territory. And I'll leave it to you guys to read the Annie ML paper. I'll pause the recording here, streaming here.