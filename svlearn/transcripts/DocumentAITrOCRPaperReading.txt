 you you you you you So, let's go over this paper now. Existing at which you you you Thank you. hey is everyone able to listen because i don't think i have uh i'm i'm able to hear anything same with me pointing this out actually i just realized i was a mute so i'm going to rewind and explain this again manish thank you for speaking up. So here we go. When we did, I'll just read the abstract all over again. So are you sharing your screen? I thought I was. I am not. I'm able to see. For me see okay for me it is rather white age but not black we can see it as it say that again you can see it it's fine it's fine okay see i'm not going to the paper yet are you able to read the text here okay then let's stay with that when not going to the paper yet. Are you able to read the text here? No. Okay, then let's stay with that. When I get to the paper, we'll do that. So see, historically, you look at understanding a document or an optical character recognition as a two-step process. Think of it as a sequence-to-sequence model. What does a sequence-to-sequence model. What does a sequence-to-sequence model do in these scales? You give it an input and from the input, it comes up with a hidden state. Input is a sequence of tokens. In a sequence, let me write it, a sequence to sequence model, input goes in, it gets encoded to a hidden state, and that hidden state gets decoded to produce the output. Isn't it? it isn't it so traditionally if you were dealing with text your encoder and your decoder would be rns isn't it in some sense now in the case of uh images the how do you process images traditionally using a CNN before the transformers came about. Your first go-to response dealing with images would be to use a CNN. Guys, is this ringing a bell? I hope you're not just sleeping after the video. So that's what it is, because you could, you would congulate with a filter all over the image, and you would essentially do feature extraction, so that a CNN would come up with all these features that it has extracted from the image. Those features you can treat as the hidden state, isn't it? those extracted features from the image and then you could pass it to a rnn a recurring neural network and say decode it right so remember decoders are always auto regressive you give it the hidden state you produce the first token y hat zero you take the first token pass it back as input along with the hidden state and to produce the y one hat now you take y naught and y one hat together and pass it in right to produce y2 hat the next token to produce y2 at and so forth right so decoders tend to have a auto regressive but the auto regressive yeah so i have a basic question here yes since you already since we already passed y0 to produce y1 hat yeah then why do we have to again repass y0 and y1 the reason for that is the last word may not be enough context because you have access to Y0 also, or every token that you have produced, it gives you a better context for producing the next word. For example, suppose you say, I swam across the river, I swam across the river to the other fill in the blank right so you would say if the only thing you had is the word other and maybe the hidden state it would not be as efficient as saying I swam across I mean knowing I swam across the river too it's practically begging for the word bank other bank so is it then it applies that multi-attention head to it and figures out what to pay attention to right that is right that is right so remember uh i mean we are talking about transformers but i'm talking here even with rns decoders you tend to give it as much information as possible to decode so that's the nature of that so this is your data now why do we use the word autoencoder just just to wet your recollection what is auto regressive mean recollection what is auto regressive mean yeah when the next output depends when the next value depends on the previous values so a typical example for example is the temperature if you're trying to predict the temperature in the next 10 minutes the value of temperature now and a series of temperature values taken in the last few hours would be pretty good input to feed in, isn't it? So it regresses on itself in some sense. Some slight digression. How does that cross attention work then for a decoder with a hidden state? No, no. Again, getting into cross attention. I'm not even getting to attention yet. We will come to that later, but at this moment I'm just talking in the abstract sequence to sequence models. The traditional realization of sequence to sequence models, like if it was text, it was just recurrent neural networks, LSTMs and so forth. And and if you like for optical character recognition tasks what would you do you had a picture so you the the way you would handle pictures using a cnn get the hidden states those would be the output of all those filtered states are passed through a suitable feed forward and then you ask a rN to decode it. That is why these things were often called CNN RNN or RCNN, recurrent after CNN, that technology. And RCNN models are still around. They haven't gone away and they do quite well. This was the way. So with that as a background, with that as a context, observe that if you use that as a context, you have to do a lot of training of the encoder and the decoder. Now comes transformer-based OCR. What is this paper trying to say? Now let's read the abstract all over again. A text recognition is a long-standing research problem of document digitization. Existing approaches are usually built on CNN for image understanding and RNN for character level text generation. Is this obvious now? In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. It means you generate all of these and then you really doesn't know. You feed it to, it's a hack, you feed it to one more thing, right, till it gets better. So this is what I call patchwork, right? This is not good? Okay, let's add one more thing, right? So it is like making a khichdi. It's looking bland. Okay, add some spice. Okay, that's spoilt. That's too salty. Right? Okay, let's add some gur now. Right? And let's see what that does and so on and so forth. so forth. So in this paper we propose an end-to-end text recognition approach with pre-trained image transformer and text transformer models. That word is important. Let me highlight those two words. In this paper, we propose to use image transformer and text transform. So for example, and the crucial word is free trade. And you can see that by the time this paper comes, you transfer learning was the norm. People realized that it takes so much energy to train a model, this foundation, this pre-trained models. And those models can be pretty clever when you train them for months on billions of tokens. Isn't it? Why not leverage that? Simply put, why not leverage that? So we are in the world of transformers and pre-trained transformers, both for text and images. So that is why the word is transformer OCR. Asif, can I ask a question now? Please go ahead. So as of the CNN RNN versus Transformer, can we say that the fundamental difference is they are passing all the image patches together in the Transformer case, the CNN RNN, you would be doing every patch in a sequence? No, no, not like that. See, the CNN also does, because it scans through the entire document, so it's very parallelizing. Okay. If it were text, yes, it would. If it was just RNN, then to create the hidden stake, you would give it one token at a time. But because it is a CLN, CLNs are very efficient, they work parallel. And the decoding is sequential anyway because it will generate one token at a time. The main difference is the lack of the attention mechanism is one. The second is these pre-trained transformers are huge. And they often, what we are realizing is that they're very general purpose machines. That's why people tend to fly the words foundational model and so on. They're very general purpose machines that adapt to many problems easily. But Asif, aren't we also seeing a lot more hallucinations with these transformer models? Yes. The point to that is, yes, it does. Though in this specific problem, see see hallucination is making up things. In this particular situation, you don't give it much chance to hallucinate. Occasionally, it will emit out words that are not there. But it's a much more tight boundary. Okay, so it's a more, the chances are much lower with this. Okay, so it's a more, the chances are much lower with this. Yeah. And the part of it has to do with the, when we go into the architectural details, it will become more obvious. Now it goes on to say that the transformer model is simple but effective. So guys, if you were to at this stage guess what would happen, you need an encoder. Tell me what transformer encodes an image into a into a hidden state image from vit vision transformers right you would naturally guess things like vit uh diet i mean the distilled versions and so on and so forth these will convert your these could be fairly good cannabis now as we read through this paper let us see whether they have used this or not actually i've read so many papers i'm getting a little bit lost what this paper said and i didn't get time to review it as i sat down but we'll find out what about the decoder text decoder you could use an llm or you could so there are many choices for example you could even just use a simple bird right encoder itself just a pure encoder model as a decoder how in the world would you do that you could pass the hidden state right and pass all the previous tokens generated and say that the next word is masked tell me what was the next one doesn't it so remember those encoder only models can be used as decoders you could use a full encoder decoder model or you could use a decoder model yeah you could it can do that it doesn't work as well as decoder only models but people have used that for what you said here yeah next word prediction yeah it can be done R. Vijay Mohanaraman, Ph.D.: yeah next word prediction yeah it can be done so now let's read the paper and i'll be reminded, I do remember that uses a big or some derivative of it, I forgot what the the other one is. R. Vijay Mohanaraman, Ph.D.: So let's read this the the Dr oca is simple but effective simple why you're taking pre trained models and running with it. Writing the code would be just putting the glues together. And yet, experiments show that the TR-OCR model outperforms the current state-of-the-art models on printed, handwritten, and seen text recognition tasks. What is a seen text recognition task? Like a billboard with something written on it, or this thing with something written on it. So now I won't read every line, but we are getting, let's get to the introduction, which will be a good reminder for me also. Optical character recognition is the electronic or mechanical conversion of images of images of typed, written, or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene photo, or from subtitle text superimposed on an image. So in other words, text anywhere will be one way of looking at it. Typically, an OCR system includes two main modules, a text detection module and a text generation module. This is an important thing, guys. This is part of the document AI way of thinking. You always distinguish between two parts. Document, like I would call it text detection where is the text right so for example imagine that you have a picture right and you have a big star and you have something something something and then you have hello. So text detection is what? Text detection is saying this is text, isn't it? In simple terms, that is text detection. And the other task that you have is, what is the other task task the text detection and text recognition model recognition what is text recognition what is exactly written in that box you found where the text is but what is the text you found where the text is but what is the text right so it's almost like you go the kids are found in animal and i locked it in the barn they have no idea what the animal is so then you go there and you look at it and say ah that's a sheep so it's recognition and extraction yeah i mean recognition and then telling you what it was. Extraction is implicitly. So these are the two. Whenever you look at this field, often you'll find this there. Now, here is a situation you understand. There are many tools which are very good at one but not the other. Or for example, in a project that someone did in my team, they found that for a particular class of document, they were healthcare related. Easy OCR was better at text detection. Tidal OCR was better at text detection. So there is this thing. Though I then later on, I looked at it more carefully and now they are much better models. Like I did that. So this is the text detection task is usually considered as an object detection problem where conventional object detection models such as YOLO and DBNet come in. YOLO we all know, YOLO, YOLO. What is it? V7 or V8 is the latest? V8 or something. So obviously, when this paper was written, V5 was it. Meanwhile, so what does YOLO do? Basically, it gives you the bounding box and tells you it's a cat. Isn't it? So YOLO will tell you, amongst the other things, it will tell you that this bounding box is text. So it's a specialized use of YOLO. Meanwhile, text recognition aims to understand the text image content and transcribe the visual signal into natural language tokens. The text recognition task, and this statement is important the text recognition task is usually framed as an encoder decoder problem where existing methods leverage cnn based encoder for image understanding and rnn based decoder for text generation right so we'll focus on this part what exactly is there in the bounding box? Right? You would agree that this is the encoder-decoder thing we talked about is what is used. In this paper, we focus on text recognition tasks for talking into it. This part only. So are we clear guys? We are only doing this in this paper. And by the way, this was pretty common. Much of the research would be on one topic or the other. The recent progress in text recognition has witnessed the significant improvements by taking advantage of the transformer. Transformer, Vaswani, does that ring a bell? Attention is all you need exactly architectures however existing methods are still based on cnn as the backbone where the self-attention is built on top of cnn backbones as encoders to understand the text image so guys do you see history repeating itself when attention was discovered even for text nlp tasks people used rN and then say, oh, attention is a good idea. Let us superimpose attention on top of RNNs. Until the paper came that said attention is all you need. In a similar way, what these people are saying is that until now, people were using CNNs. It would produce some hidden representation. Then they would say, OK, maybe attention is a good idea. Let us pass it through some attention layers also. This is what was happening by taking advantage of the transformer architecture. However, existing methods are still based on CNN as the backbone, where self-attention is built on top of the CNN backbone. It's a nice icing to the cake. As encoders to understand the text image. For decoders, CTC, connection is temporal classification, is used, is usually used, compounded with an external language model on the character level to improve the overall actions so what you would do is you would empty out the characters and then you would take a language model and say again and say does it make sense and if you said no that word is most likely not bananas but bananas right and say okay bananas right despite the great success achieved by the hybrid encoder decoder model so guys when you think about it does it all look like a hat right cnn then some attention rnn to decode it and then some attention right the whole thing looks like a hack and so what these people do it's somewhat in the flavor of attention is all you need effectively for the document. Yeah, they throw away the Cnn and they throw away the r. If you're looking at the picture from a cnn's perspective, what's the disadvantage of having a computational cost computational but but in terms of output let's say we have super computer clusters for both yeah so what happens is that today we know that even though we train these beasts with a lot of data the learning never stops we just choose to stop when we get reasonably good accuracy but why does it not stop because there are so many parameters in the model. So one easy way to actually get better predictions is to see which parts are unnecessary and not needing training and remove it. So if you just have this much parameters to train, given this much data, you're more likely to train it. You flatten out your loss, the learning curve, isn't it? So that is actually quite, this is one of the things that people don't think that given so much data, you have two choices. Either you build a very complex model and hope that a few epochs of the data at the end of it to flatten out or whatever it is good enough or should I take a simpler model in which it really decreases the loss much more and in some sense right when you say attention is all you need and all of this you're get to superimpose attention on the older thing is that may already have done some damage produce some inductive bias some things right and the next thing because it's a completely different framework is perhaps busy undoing some of it reinterpreting some of it you're not getting the full signal you're getting already interpreted signal so in some sense the way i look at it is is the game of telephones you don't know what the first person said you know what you're the guy sitting next to you told can you switch on your audio video say that again please can you switch on your video we are not able to see you're not able to see oh my goodness why is Why is that? I apologize. Is it visible now? Yes, thank you. Sure. So I said the embeddings or the outputs that you received from your RCNNs versus an image encoder are drastically different, compared to between image encoder and another image encoder? Yes. Why? See, think about it this way. What will a CNN produce? A whole stack of so-called channels of features. Because each layer of the CNN, what does it do? It shrinks the images, but increases the number of channels or features. Those channels you consider features. At the end of it, you'll end up with like 128 or 512 channels which are effectively features now you may you may pass it through a feed forward also to get as many features as you want right to add more some of it but at the end of it you get a hidden state which is extracted completely without attention which is extracted completely without attention. So it does not contain, if you think about it, it does not contain long range information. So see, conjugation is a very local process, right? Local process, right? So conjugations have always had, for example, this problem that whether the nose is below the eye or below the lip it makes no difference to it in a face right so whereas in a transformer we'll say now what what is this because it's looking at every token every patch of the image is paying attention to every other patch right to produce its own state its own attention state. Because it's deciding which direction am I paying attention to. In a way, the hidden states of a transformer, attention module, attention head, is saying that for every token, where is it more paying attention to? that for every token where is it more paying attention to right but also doesn't the cnn create that feature mapping itself right when you know when you create all these channels and then you send it to the big four network there's no doesn't it learn the feature marker it does it does a pretty good job cnn's are amazing there's no doubt about it and there is some evidence that i mean cnn's were discovered by studying how the cat's brain works right and the image formation is there so obviously there's a lot and these days there's a people try to hybridize both of these together right recently as you know there was a paper that showed that if you scale up cnn's to billions of parameters, it does just as well as a transformer. So the jury is out there. But what these people are saying, somewhat in the strain of attention is all you need, is that, see, for document-specific, document AI tasks, you have CNN, then attention heads, thennn and then attention decoder right you can just drop the cnn and the rnn all together right so it's making a statement that transformer is enough so computationally to be faster as because you just yeah and then encoding and then yeah but not only that the surprising thing is and this is not surprising but you would sort of expect in view of what we are arguing not only is it faster look at this experiments show that the tr ocr model outperforms the current state of the art models that's because you're smaller cnx right compared to like the id which is larger. Yes, yeah so today you can argue Patrick that see it's an unfair game. You brought an athlete level runner to a kindergarten race. So you could answer that. The question is now that we know that even CNNs can scale up and do bigger things, somebody will start revising all this, isn't it? Yeah, so there we go. See, this is a fun of research. No one thing is ever proven. It gets revised. Somebody will say, no, wait a minute. So let's see what happens in the coming years. Okay, so here we go. It's usually, despite the great successes achieved by hybrid encoder-recoder methods, there is still a lot of room to improve with pre-trained CV and NLP models, right, and modeling. The network parameters in existing models are trained from scratch. So the first problem with this model is they train it from scratch on images. So the point that he is making is why in this world of pre-trained models, why are you so antiquated as needing to train everything from scratch? So that's his first objection. With synthetic human level data, leaving large scale pre-trained models unexplored. The second point it makes is as image transformers become more and more popular. So the so rustic keys at all, the so the so risky at all is which paper. Vision Transformer, right? The 16 by 16, and so forth. I'm really sure that's what it is. Okay, and the to Viron. I don't know what that one is. I don't easily recognize that especially the recent self-supervised image training, blah, blah, blah. It is straightforward to investigate whether pre-trained image transformers can replace cnn backboards meanwhile exploiting the pre-trained image transformers to work together with the pre-trained text transformers in a single framework on text recognition tasks so today guys you know we live in the world of blip2. Right? It almost seems obvious, isn't it? Right? We are used to doing image-grounded text generation. And in some sense, you can treat this, like to me, there's a very good parallel between blip2 and this, right? because there also you were doing image grounded text generation and in some sense you can treat the whole OCR this a text recognition as a image grounded text generation isn't it right and so one would imagine that similar considerations come in um to this end we propose a tier OCR an end-to-end transformer-based OCR model for text recognition with pre-trained CV blah blah blah, which is shown in black. Distinct from the existing text recognition models is a simple, and this is its quality, it is a simple but effective model which does not use the CNN as the backbone, nor does it use the archive. Instead, so the rest of it is, if you remember your VIT paper, the rest of it looks obvious, right? First, you can resize the image to what a VIT expects, 384 by 384, right? And then you can divide it into 16 by 16 patches which is a giveaway to that paper the vit work used as an input rate standard transformer architectures with self-attention mechanism is is leveraged for both encoder and decoder I'm walking out in the cold. Decoder parts where word piece units are generated as recognized text from the input image. So basically what are we doing? We are saying take the sequence. The encoder is a VIT is a encoder in the sequence to sequence model encoder. Just put a VIT family family something right family here and then likewise you get the hidden state right edge and then pass it to a text textgen decoder right text transform the standard transfer and then you'll start getting your answers. So treat it as a sort of a text generation task, basically. That's all. Now, if you look at the model you notice in this picture that does this make sense guys you take the image slice it up this is standard vit right guys let's look at this image does this make sense? You took this text, you made it into these little slices. You feed the slices in, flatten it, put it in a sequence, flatten it, and that's your patch encoding, embedding. And then what do you do? You have to add This is your standard encoder, boilerplate encoder. Isn't it? And when you do that, you always, of course, remember the feed forward networks are your peanut butter jelly sandwich. You must have it. And then you produce some hidden states. This in the sequence to sequence model therefore this is your hidden representation and what do you do you feed it into a decoder right and then you start generating you you treat it as a text generation task make sense guys right effectively so the encoder, I mean if you read this you'll realize that it is basically describing VIT, similar to VIT. Decoder. Now let's go and I'll also remember what the decoder was. So Asif, you had a paper which talked about that snake letter. Yeah, right. So what the snake, so here the positions are, they are lateral going. In the snake thing, it would have to take positions which are like above the row or something like that, right? Yeah. See, snake paper is interesting. It takes a very geometric interpretation and i was really hoping somebody would take that and bring it towards the transformer world how would you do the snake work tech snake work but do it with transformers it's very possible in fact uh such an it could be a great fun project for you Sachin, it could be a great fun project for you. The basic thing is that there, see the meaning of position there is different. They are looking at it geometrically and they're creating hot spots for text. Then they're creating the median line and computing the angle and radius of the text and they are wrapping it up in circles and then they are asking text understanding what is the what is the character or the text inside that circle right so let's do it next time i believe that that can also be done using a transformer in today's world that paper was done with a cnn that we can do it with transform but anyway coming back to this decoder we use the original transformer decoder the standard transformer decoder also has a stack of identical layers so they are just taking the standard original when somebody refers to the original transformer recorder what what are they referring to attention is all you need diagram literally you take the decoder right and it makes total sense isn't it guys right it makes total sense the rest of it i you can go ahead and look at the paper i invite you to do that but suffice it to say that when you look at the performance, you notice that they are using a combination of encoders and decoders. By the way, this diet is a distilled VIT, basically. I don't know what VIT is. I'll have to look it up. Anybody remembers what Biat is? But Biat is the essentially the distant one. Roberta, of course we know Roberta is really a nice bird with proper hyper parameter optimization. Right. It's a well done Robert. It generally outperforms BERT in most tasks. So now you know this. So one thing you may. So if you were to look at this, guys, can you immediately spot some area where it could have been improved? Look at the decoder part. Would it have occurred to you to just use an encoder as a decoder? Encoder part, like when you use the decoder here, right? Roberta. So hold that thought in your mind, look around and think about it. Like for example, let me ask this question. You have this robot and so forth and it's doing a pretty good job. What would have happened if you had put a whole large language model there? Yeah, I don't know. Somebody should have done that and will probably, there's a paper out there we don't know about that does that but the results are pretty good as you can see that uh the so now but however observe the precision the f1 score of 79 80 what does it tell you guys 80 percent means what what does it tell you guys 80 percent means what it is good but it also speaks to the fact that it's not terribly good at this still uh this this entire field is a work in progress right and over the i mean in other words over the, there is a scope for a lot of improvement. You can see that whenever you scan documents, you always see errors today. So we are not, see two things at this moment are worth in progress. One is speech recognition, automatic speech recognition. And how many of you have ever been like, like I made the mistake long years ago, 20 years ago, of purchasing Dragon NaturallySpeaking. There used to be a software. I don't know if it's... Dragon Nuance. The only thing it did is it became a thorough nuisance. It never got it right for me. See, the thing is 80% accuracy or 90% is not useful because if you have to read the text to find mistakes and correct it, in that time you could practically have written the text. I don't know, Satyam what was your experience? It was the same thing but with a different perspective. You have to read, but it was introduced in a time where typing wasn't that common see nowadays every kid is like brought up with typing so the typing speed is very high right so today of course it doesn't make any sense but at that time some people saw benefit like i have seen people effectively use it oh okay no wonder they had a market yeah there's medical medical transcription yeah but now it is much better right the the latest nuance who owns nuance i think microsoft or somebody wants to answer yeah microsoft bought it microsoft got it yeah but now of course it must be using all these latest technologies and then they don't they post here happy to find using all these latest technologies. And then there was OCR, Abbeville, Feinglieder. Right, yes. But this Alexa world, everything is now very easy to identify. Yeah. Yeah, it is. I mean, Alexa does a pretty good job of ASR, but not a perfect job. See, here's the thing. I find that Alexa does better ASR, speech recognition, than Siri does. I don't know if you your mileage may vary how many of you agree with that like I play with both Alexa and Siri wholeheartedly agree Siri is a disaster oh goodness I apologize to any app employees oh whisper 3 is better than all of them oh goodness i apologize to any app employees oh whisper 3 is better than all of them no no google doesn't beat whisper google is google transcripts i just compared for my lecture didn't do it maybe it works better for american accent but for indian accent it's a disaster but they are all getting better. Maybe you're trying the latest. I should try the latest. They're all using the same model. Eventually, that's going to be more difficult. Respond this machine versus respond that machine. Right. So guys, I won't go into the rest of it. But does this paper look good? If you look at the benchmarks do you notice that something so simple achieves a pretty good result my favorite question so what's the innovation here like attention came out there was cnnrm so of course you want to use the encoded yeah there was an inevitability to it to some sense yeah there was but see the point is that somebody needed to do it combining this yeah basically the thing is there was cnn and rn then along came attention now as you were talking right you know somebody needs to redo it with the bigger cnn yeah right yeah these are like eventualities like i didn't see that uniqueness other than the fact that they were in the know and there was a timing aspect yeah to be in the right place at the right time a lot of research papers come out because you're one of the few people who have access to the tools and you have the ideas obvious ideas but though this paper is considered very good actually no the point is they're using the encoder decoder both of them are like existing they I'm missing the one block every research paper then they come up with by the way we did this enhancement in the middle yeah see so one thing yeah so to Satyam's point, right, what is so ground shaking about it. So, you know, this is Thomas Cohen's The Structure of Scientific Revolution. It's a landmark work he did in the 1920s and 30s. That book is really worth reading. The word paradigm shift comes from there. And what he studied, so these are people who study scientists as a tribe of people, and they see how science progresses. So when they studied it, what Thomas Cohen observed is that most people work within the parent. So, you know, these transformers, these ideas are there. So they will do all these mashups. And each of them is important. It is worth doing because it adds a grain of sand to a sand hill. If you happen to know the sand hill and you see this grain of sand and you say, well, what's the big deal? Now, you just face that argument. But the point is that that grain of sand is a valuable grain of sand that needed to be added. Typically as the sand hill forms, just like in sand hills what what happens at some point the whole thing comes falling down to me it looks like that but at some point somebody realizes now wait a minute or maybe this whole thing is complete i see the whole picture but there is something wrong with the whole picture there's another way of doing things right which was for example the epiphany of attention is all you need right once the picture is fully formed then you take it out and so this is like if you read any paper except for some very few completely unexpected papers most papers have a feeling of making valuable contribution but something that you know perhaps if you guys if we were equally bright and in the right place at the right time of course we also would have done there is a ring to it yeah i mean this question isn't to undermine what these people have done yeah but i really try to understand like what is the specific thing that they need no see when you read research papers, we all go through this stage. In the beginning, the papers feel very hard. I don't know what is it. And in a way, many of you started out doing machine learning with me. I hope that was your feeling. I mean, I know that that was your feeling, but I hope that over time the papers are beginning to look easy. But once you become very familiar with the domain, then you reach a stage at which you look at most of the new papers coming out and you say, yeah, good work, add some value, but I saw it coming. I sort of saw it coming, isn't it? In hindsight, it looks pretty straightforward. But two comments to that. If you have reached that level, it's a testament to how much you have got. Because usually researchers are at that level. Ordinary engineers, you take it and just talk to some of your ex-colleagues and so forth, they would look at this paper and say, oh my God, it's a tough field. It wouldn't look so obvious or inevitable. So it see i've progressed quite a bit right so uh this this all the investment of time and energy and learning has been worth that's the one way i would say other is wait for the big breakthrough because today for example people are questioning whether attention itself is the right idea right so i see two chain of activities again going broadly idea. So I see two chain of activities, again going broadly. First chain of activity says we are not understanding this KQV way of understanding attention is very information retrieval, Googlish way of understanding. So people in anthropic, for example, CS researchers and pretty deep thinkers are re-questioning. They say, okay, this attention is the right thing, but we are not understanding it right. So look at papers that start with the word attention circuits or transformer circuits. I forget what, whatever. I think either attention circuits or transformer circuits. Pretty landmark, good ideas, very deep. They reinterpret it because they focus on something that went unnoticed they focus on the residual links in this and then they they re-emphasize and they say that there is a channel and each layer is adding more information to the channel that the subsequent layer now has access to so in some sense even the encoder is a form of decoder because it's getting layer after layer after layer the next subsequent layer is getting access to more of the information produced right and they are all filling the slots they're filling this that's a transformer circuit idea and then there are researchers i forgot which paper it was that completely says there is an alternative framework to attention at all attention itself right the flash attention is still within the attention goal it just focuses it it's a focus focuses it but flash attention is a big i would call it see in terms of thermoscopes it is still within the paradigm you ask it more on the lines like the flow based models from the future but applied to that right it's just there's a trajectory that's right that was pretty much it yeah so all right guys so i'll go to them because i promised to let you guys go by three two okay yeah this is a quick one guys now now you have a flavor of it you'll see this one says forget even OCR remember the two phases were what were the two phases, the bounding box detection was. Yes, so now this one says why do that that is such a waste right and. Let me. yeah is this big enough guys I I think, somebody wanted a bigger font right so i'll read it and it will make sense to you. And the interesting part of both of these paper is that you know they gain something by subtracting not by adding you see their breakthrough is they take some things away they take a lot of clutch away so it was kludgy taking a cnn putting attention taking rnn putting more for language models on top of it right they just threw away the cnn and rn in the first paper now let's see what the second paper is going to do right so guys, you need to pay attention just for the next half an hour. I'll make it quick. Understanding document images, say invoices, is a core but challenging task since it requires complex functions such as reading text and holistic understanding of the document. So reading text is a more local thing. Understanding the document, putting it in context of what is there, is a more context related or global thing. Current visual document understanding. So term, visual document understanding. Guys, this word is important. This field, visual language understanding, visual document understanding, all speak to one fact. Our notion of language has gone beyond words, beyond text. We consider image as a way to communicate and therefore a language. Video as a way to communicate and therefore a language. So there is video language understanding. Or we talk of document, suppose you scan a document, it is communicating something, right? A receipt is saying, or an invoice is saying, you better pay me this amount. For example, I was saying, all of you now better start paying the remainder of the tuition. All of you now better start paying the remainder of the tuition. Right? So in that sense, therefore, visual document understanding. Whenever you see the word understanding, think of it that it is this? No, this is all I would say broadly, right? With the rise of multimodal learning, I would put it under multimodal learning. So that's all? Yeah, I would say. So I would say it's a specialization. A lot of people would say, oh, it is NLP. I wouldn't. Some people would say it's computer vision. A lot of people say it's computer vision, but actually it's multimodal learning. It's a mix of the two. Yes. Yeah. So, so current visual document understanding methods outsource the task of reading text to off the shelf object-character OCR engines and focus on the understanding task with the OCR. Right. Means find the bounding box and then tell me. Two step process. Although such OCR based approaches have shown promising performance, they suffer from two problems. I think three problems. Oh boy, three problems. High computational cost of using the OCR because you have to do OCR. Well, OCR is not that expensive but okay, let's go with the argument. Inflexibility of OCR models on languages or types of documents. How many things it can do? For example, if your text is written in a circle, which is very common in logos and things like that people write it in a circle but then you get into trouble um ocr error propagation to subsequent tasks so guys what happens is this is common sense right if you make your bounding boxes and then you understand it or you do the whole ocr like even figure it out if If the thing recognized is wrong, you miss some text altogether, there is no way you can understand a non-existent text, non-existent box, isn't it? So there's a downstream effect always when you use a pipeline. All the errors accumulate, isn't it, in any pipeline. To address these issues in this paper, we introduce a novel OCR-free VDU model called DO-NUT. DO comes from this, NUT, well, there's no U there, but U and T are there. So instead of calling it DO-UT, they added a n, donut. So donut. As the first step in OCR-free VDU research, we propose a simple architecture with a pre-trained objective. Now, what is a pre-trained objective? Cross-entropy loss? Whenever you see a cross-entropy loss, what does it remind you of, typically used for? Classification. So we expect something like that to come up. Donut is conceptually simple yet effective. Do you notice this key word that they keep saying, these two papers say? They are simple but effective. Means it will probably simplify something else. Right? Through extensive experiments and analysis, we show a simple OCR-free video model, donut achieves state-of-the-art performance on a variety of video tasks in terms of both speed and accuracy. So instead of two-stage approach, if you just do one thing, the idea is that it can go faster. We offer a synthetic data generation that generates. So this is another contribution actually, that data generator could, by the way, in my opinion, have been a paper in its own right. I believe it's called SynthDoc that they did. Very good, by the way user guys send down this one i may get the wrong i may have butchered the name we'll find out as you read the paper so introduction so look at this thing guys look at this picture here is a receipt right uh prashant this is the kind of receipt you're talking about right here is a receipt. There's somebody's hand or leg or sofa and God knows what in the background. So there's some clutter there. You are saying, first thing, what do you do? You highlight where the text is. Remember the bounding boxes second stage what do you do right so first is text so you notice that the goal is to extract structure stress from a given semi-structured document in the pipeline of b of d is this this is b i'm sorry this. I mean, doing B, the end result is this document, this image. Now what happens? Each of these bounding boxes, you need to make head and tail out of it, isn't it? So, for example, you could have gone back and used that previous thing, T know, TROCR or something to figure out what's there in each of the bounding boxes, isn't it? So that's the next thing. See, each box is passed to the recognizer to comprehend characters. Finally, so now it will recognize the character. Finally, and the final part is not's not that the recognizer task and its locations, location, where did the location come from? First part, the bounding boxes are the location. So what is what is a bounding box, guys? Who would like to tell me what is a bounding box? The shrink wrap box that captures any object. So if you have a cat, then you need the smallest box that will contain the cat. And so that's your bounding box. Bounding boxes typically, they give you all the coordinates, but the only coordinates that really matter is the coordinates for A and the coordinates for b isn't it so suppose i know x y here and x prime y prime here knowing that it is horizontally and vertically aligned therefore i know the bonding points isn't it so but typically some some soft ways they will give you the coordinates of each of the ends. Right? Bit of a new sense. Okay. What if the script is a little bit folded, then the longing box will go change? Yes, it does. And in fact, that's where we'll come to snake and that sort of things. So generally, rectangle based methods don't do well. What happens is when it is crumpled, people generalize beyond rectangle to trapezoids, right? So for example, these things also allow for this kind of a rectangle, or they can in general allow for any arbitrary trapezoid like this. So this is the generalization. This to this to this. You can see. Isn't it? And so the things that are capable of just finding the trapezoids that contain your text, they will give you all the coordinates, all four points for you to draw the trapezoid. But good question. Yeah, it's quite a moment for me to celebrate. When Prakash came about this workshop, he was very unsure whether he'll be able to do it. It's the first intro to machine learning and yet he has become one of our star participants. Nice. Okay, so you notice that these two tasks tasks OCR and downstream modeling or the understanding it says forget it let's have a single one, and then it shows you use less memory, you have better accuracy use less time. To me that looks good right isn't it. When you look at this graph so in the first one, less is more. You use less memory. What's the downstream model? Just the understanding part of it. It's not just that. Yes. And then it may be classification. But basically this at this moment, just think understanding part. Once you have the bounding box what exactly is it so less is better so surely it's less in fact you notice that and what is remarkable is the total time that the total memory consumption is less than that of our ocr which speak which is why they point out that ocr is expensive the time that it takes is a little more than just the OCR task, but overall, it's still less is more here, or 1.2 versus 1.7. Good. They're relatively. But look at the accuracy, guys. You don't expect a simpler model to off the bat achieve a plus 9% boost in the benchmark. That's when you say, wait a minute, maybe there's something deep going on. You realize that. Maybe we were doing it all wrong. Maybe sandwiching two things like OCR and decoder after that, I mean, understanding component after that, a transformer to understand it after that, was somewhat like using horses to pull a truck or something like that, a weird combination. You don't want to do that. This way, I thought this was sort of interesting and you can get that. Currently VDL methods solve the task in two stages. I hope I don't need to read this part by now. This is reading the text in the document in a holistic understanding of the documents. They rely on character recognition for text reading tasks and focusing on the part, blah, blah, blah. So look at the problems they mentioned. However, OCR dependent approaches have critical problems. First of all, using OCR as pre-processing method is expensive. We can utilize pre-training of the shelf OCR. However, the computational cost will be so there are many limitations ocia was never a perfect science those of you who are paying for oca software know about it the number of languages it will support there's that many limitations we won't go into the. So now let's say what do we do? We go beyond the traditional framework by modeling a direct. So this in some senses are many breaking out of the paradigm, two stage paradigm and saying, let's do it together. A direct mapping from a raw input image to the desired output without OCR. So guys now think about it. Just think about it. They're saying from an image straight to text. How would you do that? That will give you embeddings. Again, think in terms of your VIT kind of a situation. situation. The text will detect this and then it matches. Yeah, let's find out. Okay, we trained a fine-tuned scheme on donut training. In the pre-training phase, donut learns how to read the text by predicting the next word by conditionally joining on the image and previous text. So think about it guys. What is the core of a decoder? You have some hidden state, but basically the word that just came out becomes part of the context, isn't it? So it is a very beautifully architected argument. It says, okay, some hidden state goes here, but the words that have come out so far are token t0 t1 t2 if you feed it back in all of these all right then see the previous words the next word prediction is sort of a task that you can solve isn't it but what about the hidden state you it is like image grounding you use the image to ground it and you say if the previous words were this what is the next word you see that the hidden state came from an image embedding image embedding basically right and for that of course the vx vx gx are all there right so what they are basically saying is that we really rely on the words that we have produced because it's going to be some sentence right the words we have produced so far to give us a hint on what comes next. It's a autoregressive. And the contributions are summarized as follows. We propose a novel OCR-free approach to the best of our knowledge. This is the first method based on that. We introduce a simple pre-training scheme that enables utilization of synthetic data. Yeah, SynthDoc. Guys, this is really good. Like if you want to, like the best thing is free data, which is realistic. It does a good job of creating pretty good. I was working on a healthcare project. Healthcare is a ocean of forms, tons and tons of forms, right? So like you and I talk in English, the healthcare world talks to each other in forms, right? The Medicare will talk to you in forms, right? Insurance will talk to you in forms, pharmacy will talk to you in forms. Is that right Patrick? Everything is forms. So this is a good one. We conduct extensive experiments and analyze both public. So what is the method? Let's go to the method. So guys, look at this. You have an input image. You can use a transformer encoder. And I believe they use the standard document understanding transformer okay the encoder part right the visual encoder converts the input image into a set of embeddings where n is the feature map size blah blah blah so you realize that they say you use it in their architecture they say you can use a transformer but if you want you can even use a cnn it doesn't matter because ultimately you just need this hidden state h going into it right the rest of it comes from the context the previous words generated previous characters generated because it you'll heavily depend on that in this study we use a swing transformer what is a swing transform what is a swing transformer. What is a swing transformer? What is a swing transformer? Let's just say that it's a VIT++. It does much better than VIT. So swing is the state of the art. I mean, people use swing all the time. It uses shifted windows. It basically captures the local context a little bit more. See what happens in VIT is you take these disjoint patches. So in a way, one patch and the other patch, there's no cross information flowing. Swing uses a shifted window to bring that thing a little bit more local. So in some sense, the way I look at it is in a very vague way it tries to bring about a form of congulation yeah right that's what i mean that's what it does so that is a swing transformer how come i didn't cover the swing transformer in the deep the neural architecture schools did i not we did we did the weird right we did the weird, right? We did the weird, but we didn't do the swim. Okay. You will shift the window. Yeah, shift the window. That's it. That's what I'm saying. You will take three, four of them, four of them, squish it down to two. So then the information from four becomes two in that window, then you shift the window and so forth. You keep doing that. So Swift window, so this is the main idea. It splits the window into patches. Blocks considered a shifted window based multi-head retention and a two layer MLP are applied. Well, obviously from this sentence, you won't understand what Swin is but that's how it works yeah yeah so decoder what do you do in decoder you use given z a textual decoder generates a token sequence blah blah blah which pretty so the rest of it is one hot vector of that that is what you would expect that's that so we use bot as the decoder architecture what is bot sounds like the train service so a bot is what it's a transformer right there but uh it's a transformer which has both encoder and decoder there. I think Bart came out of Facebook and it's pretty good actually. Do you guys need a very quick... Well, we are running out of time. Maybe if time permits, I'll talk a little bit about Bart. But basically it's a full model, full transformer. You pass it through it and you use it as a decoder. Now why would I do that you realize that if you have already created image encoding hidden state and you pass it through an encoder and then you pass it through a decoder in a way you said why am I putting it to a encoder again it turns out it helps transformation of the image yes something yeah that is that so the rest of it is very straightforward guys so do you notice that there's quite a bit of similarity between this and the other one yes this is it and then you combine both right try it Try it. Those poor guys are busy uncomplicating things. You get back to matching them together. It's interesting as it is, you could use a CNN for this. Yeah, encoder could also have been a CNN. So here's the thing at the end of it, as always, it does very well. Experimental results are very good. You notice that the donut F1 is significantly better than other models. I won't go into the results. So just look at the results and you'll see. And if you look at this, look at this, guys, I talked about easy OCR, paddle OCR. Remember, I talked about those open source projects? And now look at Donut. It beats all of those OCR based models out of the water. So then why did I ask you guys to go learn those libraries? Only to tell you one hour later that don't bother. Because those are used extensively the paddle ocr mm ocr and easy ocr the reason is they are used extensively right so you will hit upon a code base that somebody will hand over to you and ask you to make enhancements so you can see those so that's that guys that's that, guys. That's that. So I wouldn't go more into it. I'll let you guys go. Do you guys want to spend a couple of minutes on BART? Perhaps if I shared the papers with code for BART. Oh, okay. You did. Let me just go click on that paper and bring it up a little bit it's the denoising that you explained earlier right that is right basically but what it does is it takes the okay guys i'll explain the bar to you in a simple term you have an encoder decoder right now think of an encoder decoder as autoencoder. Do you remember the denoising autoencoder rather than the transformer? And so it is basically saying, hey, can I not interpret the entire transformer with its encoder heads and its decoder heads as basically an denoising autoencoder? And you can, and well, if you treat it like that, then it is a bot. Facebook came up with it. Now, why is this relevant? Or when you pass it to bot, in a way because it has error correction ability, that benefits your system. So it can detect the different information so that you can . Yeah, it helps you out a little bit. Yeah, that's about all it is. So guys, that is it. You guys wanted to be left early today. So there we go. I hope you had fun. How many of you liked the topic? Was this fun? Yes. Nice. I'll stop the recording, guys. Wish you all a very happy