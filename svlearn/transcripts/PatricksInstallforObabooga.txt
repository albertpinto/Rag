 Hello to everybody. No, no, one second. I'm still doing it. Setting it up, you're meeting for YouTube. One second. Sure. You're live. Okay. Well, hello, everyone watching this live stream right now. Today, right now, we're going to run through the installation for text generation web UI by Uba Buga. This is also known as the most favored web interface for testing and prototyping and fine-tuning large language models that are offline. Today we'll run with a few steps. First is to have a content environment available. Second is to clone the repository and to be able to run the server so that you can download and test your models from this after we've installed everything we can run through a few a few items and settings in the models in the web interface that we can use to to figure out how to how LLMs in our local systems. So first, it's also important to know if you are running a Linux or a Windows environment, we'd highly recommend Linux. And second, just a brief overview, the Text Generation Web UI, it's a Gradio Web UI, which enables three different kinds of views, view, the text generation web UI, it's a Gradio web UI, which enables three different kinds of views, like a chat view, a more assistant instruction view, and something that looks a little more traditional, where you can like the playground view. You'll see this a little later. And a few features here that are that are interesting is there has different ways of loading the model on the back end. You can load it directly. If you don't have any GPUs, you can load models that just does all the inferencing through CPU. And then you have some that are either exclusively GPU or those that have a mixture of both. So depending on your setup, one of these settings will definitely run the model fastest for your system. There are also a lot of other settings for when running models that are quantized. for when running models that are quantized. A lot of packages also that are installed in the back end that you may not have to worry about, just have to be a little familiar with. And finally, there are also some beginning work about having multimodal pipelines, but I personally haven't tried these yet. And extensions, if you want to bring these bring the Gradio API outside to connect with other with your other scripts like launch and all those all those other programs that interface with llms uh programs that interface with llms uh as as uh in the in the chat bot space it's it's also this text generation web ui uga also has the ability to create uh personalities and if you want to have and create chat characters or or want to do a role play game or have a virtual assistant, you could set a personality so that it's very easy to talk to the model. You can also attach APIs like text to speech or audio speech recognition like whisper, all of these things you can add as additional add-ons. So what's the first thing we do when we install? So depending on your operating system, you want to either, for Windows people, you can go to the Windows installer. So one click zip and then it's going to install it by itself. But for the Linux users, we would highly recommend using Anaconda. So here's the assuming that you already have an Anaconda environment. The first step you do is to create your conda environment so we use in the terminal you use the command conda create dash n the name of your environment most people use the default textgen and the python version the recommended is 3.10.9. That's the latest version that you can use. And it's the one that's most compatible with all the other extensions. And after you create this, it will take some time. So we can do this right now. Well, let's create boot camp and then Python 3.10.9. Oops. And enter. So once this is done, you will have your own. Yes, this will start installing all the dependencies and now it's done. So to activate this environment, we want to use the command conda activate bootcamp. You'll know that your system is running the environment when the name of your environment is on the left side. Now when we have this, essentially we could do a clean install of UbaBuga. The next step that we do here is we clone the repository and install the web UI. Essentially all the code in the files and folders can get transferred over to your system and sync synchronized with the latest so sorry about that so let's copy paste oh i already have that folder. Okay. Let me, let's get clone. Now it starts to install everything. Now we can go. We can go to the folder. And then now we can click install all the requirements sometimes this might take a while so all the dependencies i think this this already installs spy torch 2 and the transformer from hacking phase and a lot of the other programs that help speed up the inferencing. So if you don't want to disrupt your environment or you prefer using containers, they have a Docker file that you can use. You just build it from there. So here now are all the packages for accelerating the inferencing and all the accelerating package, the packages for accelerating the inferencing and all the dependencies. So, this does have support for the NVIDIA Triton models and a lot of the other quantized loaders like other GPTQ, ExLama, parameter efficient fine-tuning systems and other fine-tuning packages. Okay. So we're done with this. Now we're already in the folder, but to start the web UI, just type To start the web UI, just type python server.py. And this will also set up a few things to start. And then now you can launch a URL, local URL. So make sure you have a connection to a local host. I know in WSL it can get tricky sometimes, but I think in Windows 11 it should be fine. Windows 10, there might be some compatibility issues, but if all else fails, just upgrade to Windows 11 or switch to Linux. Now, clicking on this link will open the web UI interface. So as you can see, there's a chat interface. It has a lot more features than chat GPT. This one, you can even pick different character personalities, give them a profile picture. So when you're chatting with them you can you can see their picture reply to you and you can shift you can change all the you know all the default default ways on how to chat but this is the chat interface this first tab on how to chat but this is the chat interface this first tab uh the second tab is the default interface so this one feels more like the the open ai playground where you you can give them their the system as the context and who they are and then and then send the instructions. And then this one is more of the playground. This one is more of the instruction from the instructions. So as you can see, this one also gives you the ability to, like if you have message boards and you want it to look like you have if you have uh message boards and you you want you want it to look like you're replying in a message board it has also that ability and markdown markdown outputs html outputs as for parameters you have all the you can have preset parameters uh i'm not too familiar with all of these, but it, you have the ability to maximize, maximize your all the tokens, you can stick, you can go up to 2000 tokens, you can set your temperatures, the top K, top B. And then you can have also seeds just to see if the generation, just to see if the repeatability of the generations. And then a lot more like penalties, penalize. And then a lot of the settings that we can be familiar with later on. And you can save those settings. Again you can load characters. So there are some, I think in the people who play the Dungeons and Dragons and role play, they have a lot of these characters that are already pre-made. You can essentially load them into this and chat with them, upload pictures. You can also have instruction-based templates already if you save, and you can have some form of memory also uploaded here. And then here also is the characters in another format. But these are parameters for when a model has been loaded already. Like here is the actual place where most of the initial settings we will do, and then you can save depending on your model. So right now we didn't download any model, but we can do that. Or I can just transfer model from my local folder. But the idea is, so here we have a download custom model or LoRa. So if we go to Hugging Face and you want to download a model. Oops. and you want to download a a model oops so if you want to download a model here let's say we wanted uh for for llm's stable beluga i think is the LLM's stable Beluga, I think is the... Stable Beluga is the most popular one right now. Oh, this one, just stable the fish. So you can download models easily just by. So this guy does a lot of the quantized models. So Tom Jovins. So let's pick one that can fit my current setup uh certainly yeah pick seven billion uh for a demo purpose sure sure okay here this one should, oh, this is up here, Laura. Longma. I don't know if long, stable. Okay, this one, Lama2, 7WeChat, GPTQ. Okay, so when you open this, you can see that it says, it says that in Hug Your Face, you have everything, has all the explanations. Also, it tells you what's the best use of the GPT-Q if you're using CUDA, or you can use all the other differences. They also tell you how you can clone this if you don't want to use the next generation. tell you how you can clone this if you don't want to use the next generation. So let's copy. Let's get the big four bit quantized version works for me. So this is okay. So all you do is you copy this and then go back to text generation. You can paste here and then you can just click download and that starts downloading the whole model. I can take a while. Now we can talk about, while this is downloading, we can talk about all the model loaders. So transformers, this is just a regular hugging-based transformers model. XLama, XLama HF, these are designed for quantized GPU-only inferencing. Auto GPT-Q is also GPU inferencing. That's quantized, so the Q is quantized. GPT-Q for LAMA also. These are the older versions. GPT-Q is slowly being deprecated. So if Auto GPT-Q works best, I think Auto GPT-Q works by handpicking already all the settings here for you. And then XLama does the same thing, but it has extra optimizations. Sometimes it doesn't work, sometimes it works. So if you're using GPU only inferencing, you might start with auto GPTQ. And then if you feel like it's a little slow, you can move to XLAM or XLAM-HF. Now, if you're running a GGML model, so here you can see there's GG, it says a GGML model, chat uncensored GGML. so this is this model has been uh they they had essentially i fine-tuned or they set this up so that they can do mainly cpu inferencing but with some gpu inferencing also so for those that are running on inferencing also so for those that are running on on macbooks and on you can use uh the llama.cpp uh model loader with a ggml model there's also an hf version so you can test that c transformer i think this is uh andre carpati's uh llama.c i'm not mistaken, that had come out recently. So we can also test that. But GPUs on the top and then CPU plus GPU in the bottom. And then, so those model loaders, usually they would automatically already either check these or they already have the settings already working for them on the back end. So there's not so much use for these settings anymore if you pick the right loader. But just for additional aid. So CPU is if you wanted to use mainly CPU inferencing, load in 8-bit is if you want, if you have the full model that you want to load it in and half weights or an 8-bit to get half quantization and then 16, we have 16 if you want the 16. Auto devices is if it can use all of the GPUpu or all of the cpu uh it's gonna maximize everything that it can without without without uh with and leaves but but leave resources enough to run the the web ui just uh i forget uh forget this but load in four bit is if you want the, so the one fourth, one fourth weights, so quantize to four bits and then use double quanta if you want to. If you're using a small model and you want to double, go out to quantization. And then these are extra quantization settings. It used to be when you downloaded an old model, you had to specify specifically, but all these loaders are already, all the loaders and the text generation web UI system is already able to figure a lot of these out. So these are just compression factors. So ideally, I don't touch this personally, but maybe we can figure also out why we need those. Now, while this is loading, you can also check. We can actually do a LoRa training already here. There's a tutorial for training LoRas. We could follow this. But you can name your LoRa. You can also set the dimensions and how much VRAMs needed. You could set the alpha on how influential. It's a mention like the strength and the influence, the batch size, and then the gradient. So a lot of the settings related to LoRa training is already set here. You can save it and then you can share it. They also have systems for perfect perplexity evaluation and if you want uh yeah if you want if you want to evaluate your loris so now the last tab is for the sessions the sessions is mostly for for if you want to have access so when you run this instance next time what other settings or what other extensions do you want running side by side with it so a lot of these things come from the documentation uh so if you don't want it streaming you just wanted one one full answer you can do that you could have some kind of multi-user, deep speed is for speeding up inferencing or for speeding up training from Microsoft. APIs, if you want to express an API, if you want to either connect. I think this spoofs an open API endpoint so that if you want to run it with long chain or have it connect externally, like to do some kind of react, I think that's how it starts. You also have the ability to, oh, I think that's where this was, API. You also have the ability to add text to speech from 11 Labs to read out your text, have Google translate, understand some kind, have some kind of memory and enhance longer replies. ng-roc, if you don't want to do the Gradio share and you wanna use ng-roc for sharing, oh, here, OpenAI is for the exposing to OpenAI. If you want to use ng-roc for sharing oh here openai is for the exposing today openair if you want to use stable diffusion it can also expose API uh which will connect to automatic 11 11 and other text to speech so a lot of these things we can actually see from the uh from from the ubBuga documentation. So if you need to know more information about that, you can just go to the documentation. And then it'll explain already everything there. So everything that you need about this and all the settings they'll have. So I would suggest just running through most of the doc, most of it will be the documentation. And then, but generally speaking, most of what you need is just to download the model and then load it. So now that the model has been downloaded, this model downloads it to your text generation web UI folder and then models. It's going to have its own folder there. So if you have models on your other system and you can just transfer it here, you don't need to download. It will automatically populate the the list over here so you should be able to see it here now and you can click on the loader you can tell by default it tries to pick up the gptq because it already understands that my system is mostly going to be GPU inferencing. So you can also say, let's say you only, if you have multiple GPUs, you can mention, depending on the device, so this is GPU, my first, I only have one GPU, so this is an index zero. But you can assign as much RAM as you want, the VRAM for that, as well as your CPU. So this is my CPU RAM, I have 128 gigs. So you can also do that if you have interest. And if we're, we didn't get a quantized module, but if you want to do four bit quantized and with 128 group size, that's how it was quantized. You could do that. But yeah, so xLlama, oh, I remember now. xLlama usually performs better if your offline model is LLAMA-based. So since this is LLAMA 2 that we're running, the chat LLAMA 2, we can actually, there's actually uh advantage in using xlama hf so it lets you use like a transformers model okay so there there we go so so now whenever you switch the loader it will try it will give you a status message here and you should be able to load from there and then any settings that you had, you can also save, or if you have the LoRa, you can add that as well, and then apply the LoRa. But since this is all set up now, you just click on load, it's loading the model, and successfully loaded. Now we can pick either of the three chat messages. And now you can ask any question. So. So let's try this joke. Tell me a joke that is a pun about a llama in an AI model. And then generate. So llama says, so as you can see, you can create yourself a profile or you can have an assistant. And the assistant, just like that, it was pretty fast. If you wanna see how fast you're generating tokens, you can just go to your terminal. It will tell you that it took, it generated 33 tokens per second for a total of 108 tokens. So smaller models could be faster if your GPU is faster. So yeah, so there's the stroke. We can also try it from a different system. Common sense questions and answers. So... answers so so yeah even if you try some some some prompts that that are confusing you can see llama2 chat is actually pretty good but but yeah it actually it actually tried to over uh over and over answer it added another question and answer so so this one this one is a different system and they could do the same thing as well for for this but you can you could set them depending on how you want it to look like. I think GPU4chan is the one that looks more like message boards or CASA defense system. And then QA is just the regular QA that we have. Alpaca with input, it just looks instruction response, instruction response. So depending on how you want, how you want your outputs and how your chat is going to be. But that's generally, this is generally it. If you wanted to regenerate the new question. So, okay, they're going to try. Yeah, so, so there. There are a few other things that we can add. We can definitely add a so you can do a start to start with this reply. I know there's also there's a there's a branch. There used to be a branch here for text for this, that we can actually attach long term memory. So every time you're chatting, it'll remember your previous conversations. But I think we'll have to consult the documents now because it had disappeared from the bottom already. But that's generally it. This is text generation web UI by Ugo Buga. That's it. Any questions at this point from from the people uh uh Patrick have you tried the apply Laura in the model train in the world actually I I I haven't personally tried one I hadn't had the need yet for LORAs in text generation. For stable diffusion, yes, I've personally tried a LORA. And we can do that. We can do that for stable diffusion. But for text generation web UI, yeah, there's just much. They even have a rope scaling. So if you want to extend beyond, I think the first Llamas only until 2000 tokens. I think you even do rope scaling to enhance the context, like I think up to 8,000 on a 16,000. So yeah, we have those capabilities, but it's really a lot, there's really a lot. And X Llamas, that was pretty fast, 30 tokens. You can see if you switch to GPTQ, and then let's say I'll do other devices to maximize both. If I ask the same question. So it's 23 tokens per second. So XLlama is just almost 50% faster. So if it's a LLama-based model or a fine tune of LLama, XLAMA might be useful for GP only inferencing. So that's it. I can move on to stable diffusion if you're interested. Patrick, your machine is running really fast with your 3090. Actually, this is just my laptop. My 3090 was fly, probably be sick. Oh, this is just a mobile GPU. So imagine how much more with more. But But yeah, no, I appreciate it. So it's it's been fine. Okay, so with that, do you have any questions, particularly, Asif? No, no, now we can move to the next one. Okay, so let me stop sharing first and then let me set up the-