 So the previous time we did on Wednesday, we went into an explanation of attention and and transformers. We followed that with Sunday, where we looked at how transformers essentially reconstruct the classical natural language processing pipeline. And that's quite a remarkable observation that internally, they seem to reconstruct and improve upon that classical NLP pipeline. It also explains why perhaps one of the reasons why they outperformed the classical pipelines. So today we want to do more on transformers as a lab. First thing what I would like to do is thing. What I would like to do is review what we did before. We did that quite a while ago. And I realized that some of you have entirely forgotten that we did a lab on transformers. So let us go and fix that. Asif, can you also do the youtube session you i am looking but something is very very odd with my no okay and trolls here it's not giving me it's not even giving me those choices today i have no idea why it is doing that i wish it did any people have the recordings by tomorrow afternoon okay yeah yeah so have the recordings by tomorrow afternoon. Okay, yeah. Yeah. So all right guys, so today we are going to start by reviewing what we did on Transformers long, long ago. I think this was one of our first labs in the deep learning series so obviously a long time has passed we are almost two months away from that and many of us have forgotten what is it that we did we'll start by reviewing that then we'll spend some time today with hugging faces or the transformer library it is the de facto uh library that everything integrates with when it comes to transformers. We will do two things. One is we'll go over code samples that show how you can do common things with hugging faces. After that, we'll also go through an exercise of fine tuning, taking, for example, a classifier and training it on the IMDB dataset of reviews, movie reviews. And this dataset is made up of polar reviews, about 50,000 reviews, which have clear message, either they're positive or negative. They're quite polar in their reviews. We train it to be able to successfully classify it. You'll see that training or fine-tuning a transformer, which used to be complicated code till just a little while ago, now has been simplified. You can solve very complicated problems with, especially with the Hugging Face library quite easily. There is another thing that we can do. There is a, Hugging Face has also released a library called the datasets. What the dataset library contains is all the pub, I mean all i would say but a large number of publicly available data sets and some of these are huge so it has made it accessible all you have to do is just say load dataset and give it a name and you'll be able to do that. So it is somewhat like pandas can do. In pandas, if you remember in scikit-learn, we had some data sets built in, and then we could use pandas to quickly create a data frame around it. This data sets library creates a data set structure, because I believe that you'll find it very useful in your work. You often start with standard libraries, fine tune with that. And then what you do is, if you make your dataset conform, I mean, your data that you have in-house data conform to the data set format, it becomes very easy. It's just one couple of lines of change. All you do is load your data instead of what you fine-tuned with and then go to the next round of tuning of the transformers. Are we together, guys? So we start today by doing, by first actually looking at the data sets. It's something new. Think of it something like a, it is not just a collection of standard available data sets. It has two other things. It provides you a standard format of seeing the data, a standard representation of the data, very much like Panda's data frame gives you a standard representation of the data that you use in your scikit-learn and so forth. So for Hugging Faces, for transformers, think of it as the data structure that you want to use. It also comes built in with all the metrics. When you want to look at the performance measures, you often look at quite a few metrics. They come sort of built in with it. And so those are the features of this library. So before I go into the transformers per se, let us devote a few minutes or a little bit of time to studying just how easy what this dataset library is and how easy it is to use it. With that I'll share you can see the screen. Are you guys able to see it? We can see it. Now, also increase the font to bigger size. Let me know if this is readable enough. That size is good, thank you. So now, and I'll post these two files here. Most of the code that I have today, I mean, is from the Hugging Face site. I would actually, I started looking at it and then I realized that the code is so simple, it's almost pointless taking it from the Hugging Face site and putting it into a separate notebook and so forth. It is far better that I walk you through the Hugging Face site, which is a site rich with information and documentation. So today we will literally spend the time, the lab time, walking over the examples there. But without it, let's go to the MLP library and here it is. This one obviously, this is your Hugging Face datasets library. Now, there are things you can do with this library. One is you could just explore the library by going to the website. For example, here, suppose you want to find a particular dataset. So IMDB. So here we go. You can go to the IMDB and it will tell you how to, it will give you the code on how to load that data set. It will also give you the research citation. Little larger, please. Larger. Okay. So when you go to any one of these, it will give you the code needed to load that library. And it will also give you the research citation and an explanation. So, for example, here is a data set for binary sentinel classification containing substantially more data than previous benchmark data sets. We provide a set of 25k highly polar movie reviews for training and 25k for testing. There is additional unlabeled data for use as well. I mean if you want all the data to be just as unlabeled, you have that also. This thing was, this dataset has been a while, it's year 2011. And it was, as you can imagine, at that time, the big things used to be learning word vectors, word embeddings. So it was created in that context. So this is an example of a dataset that you have. And you can go back and search for anything, any of the classic datasets. You can search for, for example, you can search for Yelp. So here it is, Yelp dataset. And this is a much bigger dataset. We provide 560 highly-coded reviews, et cetera. You can try that. And you should be able to interchangeably play with the datasets. The beautiful thing is, see how easy, if you randomly pick up any dataset, let's take this, one million news articles, it's just amazing that people have contributed so much. Do you notice that the lines of code remain exactly the same? All you did is you changed this name of the dataset, that's all. And then people obviously have spent a tremendous amount of time gathering these data sets. And I'll end that. It is a treasure trove guys, creating data sets is an extremely, extremely painstaking activity. And very, very expensive if you try to generate massive data sets in-house. It's very expensive. So when you have in the public domain all of these data sets available, you have to realize that it is a godsend. You have to treasure it. It's really, really a big deal to have all of these data sets available for you to learn from. And not only learn from, but use it in production as things to fine tune and so forth before you apply as a last mile, the last fine tuning with your own data set. So going back to that, so this is it. This is about this data set. Now, coming back to this notebook this is it this is about this data set now coming back to this notebook there is more you can if you want to look at the code and so forth you can go to the github of course this is the github uh this is a user github uh why am i here oh did i not give you the github i think that might have made a mistake in my link. Yes, I actually copied. I made a mistake. I apologize. So there's a GitHub and we'll come to that. Here's the documentation for this data set. When you go to this document, and this documentation is where we'll spend a lot of our time today uh we'll literally spend two three hours making ourselves very very familiar with various aspects of this library hugging face because if you are in the nlp space at this moment i can't think of anything which is of such overwhelming importance for NLP tasks as this particular library at this moment. It is very compatible with PyTorch, in fact it is implemented by a large, most of the things were implemented in PyTorch, which makes it even easier for us to use it. So we'll spend some time doing that but today now and then there is something even better in the Hugging Faces website there is this link which I like you can go here and suppose you're interested in any data set so let's say that you're looking at this data set So let's say that you're looking at this data set. You see the preview of the data, you can literally sample the data. I hope you can see this, right? And you can see where this data come from, show list view, right? And sorry. Are you folks able to see this? Right? So this is a data set whose description is a collection of email messages of employees of the Enron Corporation. Obviously this data set has been really well studied because Enron was the famous fraud that happened long ago. So this is a study of that and you can see the email and the subject line in this data set. So that's all you get subject line emails. So you can make something up this data. Data is split between test, train and validate. So for example you can look at the sample which is test data, a few lines of the test data. You can go to the validation set, which is you also get typically you get test and train. Sometimes you get only train. Mostly you get test and train. Sometimes you get test, train and validation data that comes in three different parts. And obviously there's nice explanations for each of the data. Gives you the background, gives you the original paper where it was talked about, and so forth. So I would very much suggest that you do that. Oh, by the way, here is the GitHub link. ASIF, if you see all this, they are running through the Streamlit. Exactly, very good point. And I was going to just speak to that so guys this is one of the streamlit is a library for creating user interfaces data driven user interfaces and i cannot praise it enough it's just absolutely marvelous um Would it be okay if I give you guys a small digression into it? Asif, before that? Yes. Can you get like a mouse pad? Because like the mouse is kind of like grinding on my ears. A mouse pad. Okay. See if I could do something about that. okay let's see if i could do something about that that the noise filtration should have taken care of one second all right is it better or is it still like that uh it's better much better thank you Did you share the notebook? I will do that. I have not. Today I'm having my own set of adventures with hardware. Actually, after six years, I made the folly of deciding to clean my room, my office. and it should go up in the history as one of the worst decisions because my room is clean and none of the hardware is properly working it will take me a week to settle down properly and get things to work so just forgive me for this. So, all right guys, so what you are looking at, let me increase the font. See, for creating user interfaces, web interfaces, we often use a React or we use Angela, or we use all sorts of framework, Django and this and that. There are countless frameworks there. But amongst those frameworks, if you are looking for a framework that can quickly create for you a data-driven user interface and is very, very Python compatible, you literally write Python code and what you have is a user interface come up then you are essentially I can't think of anything better than this so if you look at this demo that is here what is happening is do you see that a few lines of code I wish I can stop it somewhere the simulation doesn't stop itself so oh yeah here it is running stop. No, no, I can't do that. So if you just look at it carefully, all of it is just standard Python code. And with just a few lines, you're able to create very powerful, very interactive and dynamic user interfaces, web interfaces to this. And this is it. This is what you should do. If you can, become familiar with it. It comes with a lot of components. Like for example, a lot of charting components, just come built in a lot of people have integrated with it right uh and the specie has got an integration as well so yes basically has an integration very much so actually this is one of the so clearly uh praveen you use it yeah and you like it yeah it's uh it's the best way to go. That's what they say, because you don't have to deal with all HTML and JavaScript. So this is the right way to demo it. Yes. In fact, I could not agree with you more. This is my personal favorite. It's a bit, see guys, you're doing the project. You may seriously consider putting the front end of the project with Streamlit. The only downside is Streamlit has a slightly bigger learning curve than just basic HTML. So as you do that Eloquent Transformers project, a minimal viable project would be just making it HTML and, you know, just forms and so on and so forth. The next best thing that you can do is not go over to React or Angular, which can suck up a lot of your time, but to go with Streamlit. And we can give some time if you guys want over this weekend, I can get you, we can have a coding session in which I can start a user interface interface from scratch and just from zero we can build up a powerful data-driven interface interactive literally your models your transformers making predictions and so on and so forth yes if that will help sounds great yeah it's a bit of a digression from obviously a core deep learning step but it's really useful. I hope not too much of a digression. So I'll try to take out some time and do that. And people have contributed a lot. So for example, did you notice that just about every big open source library has its integration with it, find us profiling has it, right? Then everything, whatever you can think of has an integration with it, Finder's profiling has it, right? Then everything, whatever you can think of has an integration with it. And let's go. So D3, of course, we all know D3, that is here at a higher level of abstraction that becomes people. Vega-L a is probably the most popular abstraction on top of d3 makes it extremely easy to create user interfaces d3 visualizations rather than writing low level code and then of course there is altair and i think it has a integration with that too so all of it put together, see, this is very good. Let's look at some gallery things here. So I don't think we can play with it, but when we talk of Gantz, do you see that this is an example of Gantz? You can interactively play with it later. You can interactively play with it later. This is of course the main on the main website. This is the deep dream thing, playing with a photograph. Community apps and people have created a lot of apps to show cookies this and that. Anyway, you're using ad blocker please disable to view these tweets okay somewhere i'll have to disable pauses and i did disable it but still it doesn't like it. All right. All right. So I'll leave it as this. Some of these blogs, you can go read it. I won't get into that. People have done wonderful things. Anyway, going back from there to our stuff now. You come here and we have a visualizer for, where is that beautiful visualizer for the datasets? Yes, here. So let's go back and look at the data set we were thinking about in the example we'll do, IMDB. Yes, let's take IMDB. When you have IMDB, let's see what all it comes with. It comes with test, trained, and unsupervised about this data set. It is made up of 25,000 highly polar movie reviews, means strong statements about movies. I mean highly polar movie reviews, in colloquial you would say rants and raves. So you have 25,000 items of rants or raves and 25,000 items for testing, again, rants and r and waves for testing and if you want to treat it you also have unlabeled data so now this is how the data looks like this is a great shot i think every voice is done by blah and lately i have been watching a lot of tom hanks films and so on and so and old char old Charlie Chaplin is positive positive let's look at something that is certainly not positive these are all positive items surely will come to something that's negative offset by Yes, some negative reviews. I can't believe that those praising this movie here aren't thinking of some other movie. I was prepared for the possibility blah blah blah the ball again believers so they're negative reviews so this is the data set and just by exploring the data set you realize that one issue certainly is that the data seems to be sorted by label right and that certainly needs to be fixed now you look at this data there there are two columns. One is text, one is label. Label takes values negative and positive. So I hope that is enough introduction to this data. Let's go back. What do you do? By the way, for once, of course, on your machine, you'll have to do pip install datasets. I hope by now you're all familiar with installing libraries. You can install libraries. And once you install libraries, let me do that. I will pick up the IMDB dataset and I'll pick the training data split because I want to see what it looks like. Now, I want to determine how many rows are there. I can do the length of IMDB. It's an I triple object, so you can get the length. It turns out that it has 25,000 rows, just as the documentation said. If I look at the features of this dataset, If I look at the features of this data set, and by the way, the same code is applicable to every single data set available in this library here, which is a huge convenience. If you look at how different each of these data sets look, for somebody to have painstakingly put it in a standard form is a huge advantage. So not only do you get the data is being served to you on a silver platter. So here we go, text and labels, so the labels are negative and positive. So suppose I want to see the texts. You can just do this to see get only the texts, of course. Now, how do I put it into our data frame? Very easy, I know that the two attributes are text and label. So I can immediately convert it to a Pandas data frame. When I've converted to Pandas data frame and do a statistic, it seems to be that half of them are positive, half of them are negative. You see the mean is half. There are 25,000 rows, just as we expected. Max is one, min should be zero. Yeah, it is zero. And so there you go. Now let's display a few rows. So here we go. I'm displaying a few rows. By the way, sometimes what happens is that this is, you see this pandas set option, what it is doing is on the screen, it's making sure that it is taking the full width. Otherwise by default, the Pandas will show it not as wide. So this is it. It's saying, here is some texts that you can sort of study in a much more visual way. But the point was that I wanted to show you how you can convert, interchangeably go between a dataset and the Pandas data frame. I find that very convenient by the way to do that. Now, suppose you did not give the split, you asked to load the entire IMDB, what will you end up with? You'll end up with a dictionary and the dictionary will contain three key values. One is the training dataset, test dataset, and unsupervised dataset, right? With exactly the same columns, text label, text label, and then unsupervised will have just 50,000 items. So suppose, so here we go. You can do the same thing with the test data. You see me load the test data and look at the some rows of the test data just to be sure. Now suppose you have a, now that you have this data frame, is there a way to go back given any data frame, is there a way to go back and convert it into a hugging phase data set so it's very easy you look at this one line you give it any pandas data frame and it will convert it into a hugging phase data set object data structure and that data set has exactly the same columns as the data as the data as the pandonderous data frame next question can you read it from csv what about reading data from csv into hugging faces data set because that's what you will do in real life isn't it so for example you may have a large csv containing uh amongst other things let's say a lot of text right and maybe you have identified something about the text, what topic it's talking about, what subject it deals with, or so on and so forth, whether it's head speech or not, and this and that. So you may have all sorts of columns there. So how would you load it and feed it into your transformers? The easiest way you can do that is to first convert it into a Pandas data frame. Just as when you use scikit-learn, you tend to convert everything into NumPy, Array, and Pandas. In the same way, sorry, when you want to work with the Hugging Phase, first convert it into the Hugging Phase data set. By the way, this message needs to be emphasized because not many examples out there tend to emphasize that but your life becomes very very easy when you use the data sets abstraction or the data structure so when you do that here the california housing data set all of you have been through in my previous workshops that i don't think any one of you has not gone through it so So is the same data set that you are familiar with? I'm taking that I'm just pointing the file relative to this particular notebook, where is it? And so what does it take to load it? You see, how do I do that? As simple as that, you said that the type is CSV and data files you can give. And by the way, what happens if there were multiple files? If there were multiple files, you just give. And by the way, what happens if there were multiple files? If there were multiple files, you just give it as an array. So for example, you would say one, then comma, some other file. And if you give it as an array, what it will do is it will automatically glob the data into one single data set, right? Even if it is across multiple files, right? So those are the conveniences that come with it. It's very sort of intelligently designed, I would say. And so when you load this data set, we are all familiar with it, you ask what is this data set? If you don't tell what the split is, it will automatically put it in the train split. It will say this is the training data. And so it will tell you these are the features. We are all familiar with these features, longitude, latitude, housing median age, total rooms, total bedrooms, population, households, median income, median house value, ocean proximity. And if you remember, we had approximately 21,000 rows, which is coming through 20,640 rows. So what I showed to you, guys, is a different abstraction, different data structure, but as convenient as the Pandas data frame. Would you agree that it's simple to use, isn't it? Looks it. It is simple. Now why, so why in the world would I care about this dataset? Let's go and see an example that would illustrate this. Let me take, maybe I'll take transformers. So first I need to review the examples we did in transformers a long time ago, but before I do that let me first go and show what it means to use a dataset that we have. So, oh. All right, so let me take this example. So in the transformer, there is a lovely API called the trainer API. So it is an abstraction. It's a it's a Hugging Face native API. And this API actually works very well with the dataset, data structure. And within the two of it, it makes writing code and doing stuff very easy. So usually when I see complicated code these days, using TensorFlow, things like that, the first question that I often wonder is, why not first refactor it to simple, a few liners, so that the meaning of the code becomes much more apparent, and we just focus on the main things. So, well, okay, so suppose you have to install it. These are the things to install. Of course, these things are taken from the Hugging Face website itself, and I'll post this notebook. This notebook, by the way, is the same notebook. So this section is taken from that. These lines are taken from Hugging Face website. What to install in case you haven't installed it. Now, you guys have already installed the transformer, so you don't need to run that line. Now, let's look at it. What are we doing? By now, you must be familiar with this. If you remember when you did transformers, we did all of these from transformers import, BERT for sequence classification. You did that. We'll go back up and review the code. You have done all of this, this bird tokenizer fast, etc. But what is the new thing we do? We are bringing in the trainer and the training argument. This is the new API that Huggins Faces now gives, native API that Huggins Faces gives. The rest of it is straightforward from scikit-learn you have the accuracy score, precision, recall, F-score support. What is F-score? It's the harmonic mean of precision and recall. So now, if I remove all these clutters, oh, by the way, the weights and biases itself needs some amount of time devoted to it. Maybe in some time, as we go, and in the next few weeks, I will show you how wonderful it is to use this weights and biases facility. It's a website. So what happens is, see you experiment with your model a lot. You'll keep changing it, you'll keep tuning it, you'll do this, change the hyper parameter, this, that, and then you look at the performance and actually see metrics and so on and so forth. So if you integrate with weights and biases, what it does is it keeps exporting those values in the logs to the weights and biases server. And so you can visit the website, your account in the website, and what you will see is all the runs and the metrics from all the models, all the hyperparameter values and all the models and how well they worked and so forth. So you see it in a very nice structured way, because as we play with models, we tend to experiment with it a lot. We tune this, we tune that, we do many things. And ultimately we begin to lose track. We start writing down in some either electronic notebook or paper notebook and soon it's a mess. So weights and biases are a great way of dealing with that. So leaving that aside, how do you do this? And so one of the things is Hugging Faces encourages you to use that, or at least gives you support for that, which is why you get these warnings. This line you're totally familiar with, I hope. We have done this long, long ago in the fundamentals part of the Deep Learning Workshop. So just to refresh your memory, the first one BERT for sequence classification. What is BERT? Bidirectional Encoding Resource Transformation or something. Nice. It's basically a transformer. It's a transformer. It's the most popular transformer. So we are using BERT for sequence classification. What is a sentence or a paragraph? A sentence or a paragraph is a document, is a sequence of words, sequence of tokens, isn't it? So I hope now the name is self-descriptive. BERT for sequence classification means use a transformer to classify some text. Right. And how do we do it? You first start with a pre-trained model. BERT base uncased. So let's talk about what is BERT base. You remember that BERT regional paper, when the implementation comes with a BERT base and BERT large. Right? A BERT base has 12 layers, a BERT large has 24 layers or something like that. If I remember right, 24 layers. The hidden representations are thousand dimensions and something. We went over the paper, you can look up the details and or any documentation, it will tell you. Uncased means don't care about capital. Words are all in same case. So bird based uncase is probably the most popular one used for learning though, when you're going to prod or something, you tend to move over to bird large if you can afford to, if your machine can afford to do that. Asif, have you used other BERT based? Okay, the simple answer is very much so. See, you and I don't have a farm of GPU driven servers. So our first approach should be to start with Distilbert. Robert Distilbert, recent is Albert. So though. Yeah, so I would like, I tend to favor Distilbert quite a bit because it's cheap and fast. And then, but then I do try other things. The beauty of this is, see here's the thing, from a code perspective, you realize that it just becomes changing the name here, one word. You go look up that and you just put the other models name, pre-trained models name, and you can then check out how good the performance is. By the way way that is another reason to use this uh weights and biases thing because it keeps track of how different models are performing on your data set they are using that weights and bias so when i looked at you that nice do you like it i hope you like it yeah I hope you like it. Yeah. Yes. Yeah, pretty much. Very nice. Right. Lovely. I suppose they are to a machine learning what a Splunk is to logging. Yes. Isn't it pretty much. Yeah. I absolutely love, uh, rates and biases. So yeah, there we go. So we have the tokenizing. It's just amazing how people come up with wonderful tools to make development easier and experimentation easier. So all right, there is a simple method. What it does is given a batch of data, from the batch, what do you want to extract? The text. Remember our data had two columns, text and label. Do you remember this, guys? Let's go and check that. Where was I? Yes. Do you notice that text and label, these are the two columns? Do you remember, guys, in the IMDB database, these were the two columns do you remember guys in the imdb database these were the two columns so given that these are the two columns what we are doing here is where is my code transformer what we are picking up is only the text that's the only thing we want to tokenize right we don't want to tokenize the labels the target variable so this is it. Padding is equal to true. What in the world is that? Could somebody guess? Make it a square matrix or uniform? Yeah, uniform length. Uniform length, yeah. What happens is that some sentences are short, some are long. Some documents are short and long. So what you do is you make them the same length. And truncation is true means if it exceeds the maximum length, I believe it's 512 tokens, then what do you do? You throw the rest of them away. So that is padding and truncation. After that, the rest is very easy. By now, guys, would you agree that this is self-explanatory, this line? A training and test dataset is this. It just loads- It's a standard step. Go ahead, Kate. A standard step to split the data set between training and test data, building some of the data side for testing after training. Yes, yes. So data internally is split. You're picking up. You're saying that, take the train and put it in this, and take the split and put it in this. Because if you look at the data internally, it comes built in with the splits right we did that a little while ago so uh no here in the other i keep messing it up if you look at it and if you just load it as it is do you notice that this look at this database dictionary in this do you see that there is train and test, the two splits? So it comes pre-split. That's a nice thing. Unlike much of the exercise you do with scikit-learn, where you explicitly call a test train split. You don't have to train test split or whatever. You don't have to do that here. It's pre-split for you. So it's convenient actually. And so you just extract the two things out of it then what do you do now you need to take the data but you're not going to feed the data you're going to only feed the text column of the data and also you will do this padding and truncation so this is a map it's a transformation use this function tokenize to transform the data and you can batch it, you know, do the transformation series batches and so forth. As if. Yes. What's the advantage of putting padding if we're going to tokenize the data? Well, you don't have a choice, Patrick. You have to. The input, the X vector, remember remember is of a given dimensionality so suppose your words you don't have enough number of tokens to feed in what do you do suppose it's expecting 512 dimensional right or 512 tokens and the x vector the dimensionality is aligned like that and you have only 10 words in your sentence so you need to fill the rest with empties right zeros that is padding so it's more a necessity not not a question of advantage so is it like for the matrix operation right like so you want like the same size yes yes right see remember that all of machine learning is a vector calculus right it's all playing with matrices so there we go so that's the purpose of padding patrick does that answer you oh yes that's it thank you and truncation is what if you have more than five total whatever the limit is then what do you do you have to get rid of them, isn't it? So that's that. So this is it. You extract the training data, the test data. And then what do you do? Set format torch means make it in the PyTorch format. And then you need to give the input ID. So this is fine. You also need to give something else. IDs will just attach an id to it that's uh this matter it also adds a column attention mask what is an attention mask this needs explaining so suppose i use the word the cow jumped over the moon right the cow jumped over the moon right and full stop i have seven tokens in this sentence right but suppose it is expecting uh 512. now when the attention business happens i don't want the words to unnecessarily pay attention to those dead you know there's the empty tokens necessarily pay attention to those dead, you know, there's the empty tokens, the null tokens, isn't it? So what you do is in the attention mask, you will say 1, 1, 1, 1, 1, 1, 1, you know, you'll put seven ones and the rest all zeros. So you're basically limiting attention when the words start paying attention to each other. you're just telling the words that don't go and pay attention to those those zero those tokens which are marked you know because they are just a buffer they are just padding tokens uh so when you train so uh it will just go to zero anyways right for the empty tokens no not necessarily if you just uh you know if you just pad it with zero vector it might start learning and start paying attention to it unless you force it not to pay attention see origin a vector which is a zero appointed origin unless you from a vector matrix computation perspective it is not necessarily the absence of a signal. Right? Deep inside the encoding layers, it's not the absence of a signal. So it is best to mask it. It is just a small technical detail. It doesn't, it has no, see, there's nothing very deep about it. Just leave, think it's just the mechanics of the hugging faces that you have to do it. I'm sure in the next iteration, they'll clean up the API and you may or may not have to do it. It will automatically figure out what the mask is. I don't know why we have to supply it and the labels and so forth. So you do all of that this is just see this is boilerplate this part of the code is just boilerplate so i see if the mask is is it zero or is it one no no so suppose the cow jumped over the moon there are seven tokens right so the mark let's say that the the length you're expecting is 512. so now your mask will go like this it's an array that starts with seven ones right yeah remaining zeros yeah and five so the padding is zero yeah okay five hundred and five zeros that is that's it and the label is of course the label positive or negative you have to say what it is right so this is your training data and the rest of it is compute metrics and so forth so by by the way, you guys are all familiar. So you make the predictions, labels, predictions. By now you're familiar with the predictions. Now, what is this? Precision recall F1 support. Where do we get this from? This comes straight from your scikit-learn metrics, sklearn.metrics package. This part, it just is, if you give it the labels and the prediction, the y and the y hat, it will produce for you the precision and the recall and the F1 score for you. The other thing is accuracy score. That is also a function in the scikit-learn metrics package. You can see that just by looking at the input. Do you see this from scikit-learn? So in our previous lab, we didn't use this specifically very much in this specifically. The reason is what we used to use is the classification report, which is actually in many ways more detailed than just these two. Just a choice here. No particular reason to use this or the other. Here you break it down into all of this. So now comes the real code. So this is all setting up the metrics. Now comes the real code. Look from line 13 to 22. Is it clearly visible on your screen scans? Maybe I'm a little bit bigger. Oh, too big. Too big? Yeah. Where am I? Yes, let's pay attention to this. See, these are the two classes that you care about, the trainer and the training arguments. What you do is, the output directory is where it will dump the results, what it is finding number of epochs of training you set it to only one in this particular case now this example this is by the way an illustrated example this little patch of code is an illustrated example from the documentation of transformer so the batch size there is set at 16 and evaluation batch size is set at 64. And the only reason I changed this is, and I'll be posting this is, it won't work on your laptops. Most of you people, when you try to run, you will basically, this code will fail because you'll have out of memory errors your gpu won't support it so in order for your gpu to support it just decrease the batch the batch sizes to that even when you do this, remember I said that to train a transformer from scratch takes a lot of machines and a lot of days. And so you should only fine tune it. You know, take a pre-trained transformer as we are doing. We are taking the bird and then giving it a new data set, the IMDB data set, and doing the last mile training isn't it fine tuning it and what did you say we have to adjust for our computers in that code right there make it 2 and 16 then it will work i tried it on the laptop and i verified that yes it works so it should work on your laptop also So it should work on your laptop also. Or if you have the Colab Pro account, just run it in Colab. Better still, if you have a fancy notebook and a VM, expensive VM where you're paying $3,000 a month in Google Cloud, then by all means means set your values generously so these are all standard things the weight decays regularization parameters or warm-up steps ignore the first 500 steps and so forth. So the rest of it is logging directory. So when you're logging directory is important, we'll use it for example, look at the logs and the tensor board to see what was going on. Now, so this is just setting up the configurations. And these are pretty standard configurations you can set it up. Then comes the trainer. Let's see what do we do to the trainer. and this is and these are pretty standard qualifications you can set it up then comes the trainer let's see what do we do to the trainer now the trainer is doing fine tuning remember guys you don't want to take a bird and just reset all the parameters and try to train it from scratch that would be quite not the right thing to do so you you instead take a model. Here, what is a model? Do you remember, guys? Model was going up a little bit. Model is here. It is already a pre-trained BERT base uncased, right? Case-insensitive, trained on a case-insensitive data. It's the smaller BERT, BERT base is the smaller B bird with only 12 layers and so forth right so that we are using and that model so it is already trained so then what does training mean here so would somebody like to explain why do we need a trainer to train a pre-trained model fine tuning fine tuning excellent because now we are going to do the last so which layer is getting fine-tuned the whole of it huh the whole of it no whole of it mean what does that mean so what happens is that at that particular moment so how many layers are there and then which how many layers are getting fine-tuned oh the bird is a see the bird base has what 540 million parameters all of those weights across all the layers are being fine-tuned right see basically you have opened the flood doors to back propagation right once you open the floodgates to back propagation, the hope is that because it is already tuned most of the weights will jiggle a little bit they won't change too much right. Vaidhyanathan Ramamurthy, In other words, they are already close to the right answer it's almost like you, you know you have shot a spacecraft to the moon. Now it's orbiting the moon. You're just looking for a lunar landing site. And so that's how it goes. You just let it train itself. Now, sometimes you can choose not to. Sometimes you can choose to keep the whole classifier, the bird as it is, frozen, and and just screen a classifier layer and only train the classifier layer right there are all these things that you can do so even the in the model that we're doing right now you're not restricting the training to any specific output right let's say uh bird can do sentiment analysis. It can do sentence completion. You are, you are. Remember, look at this. We have taken a pre-trained model meant only for sequence classification, which has been trained for sequence classification. It is not from, you know, a clean slate, absolutely fresh, untrained bird. It is a bird that has already been uh trained very well on classification tasks okay so a tremendous amount of energy has been spent training it you and i are just going to do the last mile training so you do that so you give it the model arguments is the training arguments what are the metrics we are going to compute? Accuracy, precision, recall, F1. These are the metrics. Now, what is the training and evaluation dataset? These are the two, right? Train, evaluate, train, evaluate, right? So this is it. When you build it, when you use the trainer, you do the training, you do the testing. So this is it, which by now should be obvious. So now you start running the air clocks and then you run train dot train. In fact, even on a very good machine, even though you're doing the last mile training, how many seconds do you think it will take? Any guess guys? It should take longer because there is so much time. Four hours? Yes, it's going to take many, many hours. It depends upon your compute, how much power you have. So when you run a train, that is when you start reaching out to me and saying okay let's pool money together and order some hardware so all right guys so when you start running this so you start with a certain learning actually in my machine what happened is i ended the day work day and then i fired it off and i didn't get even then i didn't get like i fired it off around what is it four or something like that and I didn't get time to finish it in spite of that right on a fairly good machine so but anyway the point is that the learning rate this will go through its cycle and as you go down you will see that the evaluation losses are going down 0.46 and then sometimes it goes up and goes down. And remember, we are training it for just one epoch. Surely you should train it for a few more epochs. So, and then finally I stopped it at a certain point. You notice that it stopped with a pretty good evaluation loss. You had the evaluation precision has gone to 90%. Do you see the overall accuracy is in the high 90s now? So it is learning pretty aggressively, all of these things are getting better. And we are not even through an epoch. Right. So obviously, the data is very clean. And even with a few mini batches, it tends to be doing very well. And that is the beauty. And by the way, that is worth emphasizing. That is the beauty of fine tuning it. If you were training it from scratch, this, forget about anything like this. It would be hopeless the first few epochs. But here, because you're starting close to a fully baked, you know, you're just warming up sort of a pie that you may have baked. So the learning happens fast, but fast is relative. Fast is still a few hours. And so one of the things you can do is you can ask yourself this question, do I really need the entire dataset, 24,000 things to do? One thing you know, this transformers train slowly. Why do they train slowly? Like per iteration, that is per mini batch. And at this moment I've set the mini batch size to just two or 16, 16 plus two, 18 dataset points, two for testing, 16 for eval. It is still taking... It is able to process, if you look at it, I don't know if it... No, you're not seeing this. When you run it, you will see. It is able to process just about three records per... I mean, three iterations per second okay three mini batches per second or five mini batches per second so can you guess why it is so slow paying attention to all the different interactions quadratic operation no imagine that there are 540 million parameters in this model, even in the base model. The back propagation is going and essentially, potentially updating all of them. And there is a massive, massive amount of matrix computations happening. Isn't it? So it is a miracle that it's able to do those many computations at all so fast. So when you come to transformers and even fine-tuning with this, you realize two things. Training a transformer on your own, you need to really be a heavyweight, have heavyweight hardware and things like that. And the second thing is that the only hope you have on reasonable hardware is to fine tune it. And that also takes a long time to get there. So anyway, I didn't finish this. I'll upload the code. And so you'll go through the evaluation and finally it will produce a TensorBoard output. Now, TensorBoard is something that I have not, I was hoping that today I'll introduce it to you guys, but let's keep it for another day. I want to do weights and biases as well as tensor boards together. The visualization of the training process. So it comes under the, it's a nice thing, it's a nice way to see visually what's going on. So this is an example of fine tuning. You know, yesterday we asked this question, how do you do that? How do you apply a pre-trained model to our problem and just make it learn the last bit? So are you seeing it guys? It's not that hard at all. The code is very straightforward, but it's a new thing. You have to get used to it. By the way, you don't need to use the trainer API. Before that, you could directly do it using PyTorch and transformers, or TensorFlow and transformers, et cetera. The code is just a little bit messier, bigger. But when you do it using the trainer, which itself sits on PyTorch, a lot of the heavy lifting is done for you. And when you're in the business of solving a real problem, there's no reason to reinvent the wheel. Use these excellent libraries and these APIs and get done. So between your trainer and training arguments, you're essentially done. The main concepts are, and we'll go through the main classes now of Hugging Face Transformer. Let's take a break and it is 8.15. Let's get together. I suppose this is your dinner time. It's 16 it is now. So eight, would 20 minutes be enough for dinner guys? Oh, we can wait at 8.45 then. Yeah. 8.45, huh? All right, so we will regroup at 845 then, guys. So ask me a question. So for the, you said like it takes like a lot of time to do the training, like the retraining or the fine tuning. So is that like because it is doing that quadratic operations for the attention. Yes, exactly. It is the quadratic operation that you have in the... For the attention part. Yes, yes. And now we are doing quadratic operation on giant matrices. Yeah. So that's what makes it terrible. So obviously, you know, a lot of research has gone into making this transformers faster. And we covered the paper on the Sunday reading, remember that performers, that is our latest, using kernel methods, using kernels so i said i just remembered like i was going through that lecture again and the it is missing the last piece of the last piece of the conversation like where you talked about the lemma one uh it is missing that piece like it is not there in the recording oh well it's there just before that but after that like that particular piece is missing like where you talked about like after the beautiful function uh yeah so that piece like is it possible to get that piece back or i'll talk to the videographer i'll see what happened okay. Okay. Yeah. Unfortunately in Zoom, we don't, you know, you don't get too much space to keep. So you have to aggressively keep deleting the recordings. Okay. Yeah. And this copy of this Transformers notebook is in which copy of the homework or? So here's the thing. This is your, hang on, let me just decrease this, paint a little bit. So for me it's read over because these fonts are so big they're hurting the eyes almost. So almost free. Okay, so here we have the Huffington Face Transformers. Huffington Face Transformers? Yes, Huffington Face Transformers is there, right? What I did is just i added it to the bottom of it i added an example to the bottom of it today but what we'll do after the break is we'll come back because you may have forgotten about transformer and doing this lab at all we will review what we did for some time and then we'll go and walk through the hugging phase documentation carefully i will together so guys it's 8 20 so let's make it 8 50 half an hour for your supper let's meet after supper okay All right guys, so going back to our notebook and first thing is I hope just as a quick review we all understand that dataset, datasets is a great library. It gives you literally a data structure abstraction, which is called dataset. Very easy to understand. It's think of it as the transformer equivalent of Pandas DataFrame. Okay. Very interoperable with Pandas and CSV files and JSON files and all sorts of things so this is this code i'm going to post it right away to the course website any questions with that i hope we all found this part uh fairly straightforward good to know right yes uh it's good and use this guys you'll realize that uh it's almost the native format meant for transformers things happen quickly fast and you saw that in this example the quote ultimately boils down to just a trainer training arguments and then trainer that's all uh if we go back to the transformer at the very bottom and this is again a recap this is your hugging face transformer that you're familiar with. So notice that the code, once you load a pre-trained model, which is you do in two lines because given the model, you also this tokenizer to that will break the things into tokens so can you zoom in a bit zoom in a bit more okay yes that's good thank you okay let me just clear some of the side foot here yes so you can just two lines for you to just instantiate the fine-tuned transformer of a pre-trained transformer and you are ready to fine-tune it to glue it with the data set is very easy because given a batch of data here you can quickly do that all you have to do is extract the x the input because your, your data set has got the input and the target variable. You just have to say take the input, attach a padding, truncation, and you're done. Right? After that, you're done. And then the basic, some mechanics of it, the attention mask and so forth. Then the rest of it is just computing them, given prediction, how do you compute the metrics? Those are minor things. You can do it this way, you can do it any other way. We have done it in so many ways. But this is the main lines of code. Training argument, you just give output directory, again, no brainer. Number of epochs, one is less, but okay, because these things take a lot of time one epoch is good enough you set your batch sizes that weight decays and by the way evaluate during training means what what it means is that suppose you have a mini batch of data you pass it through the training and now you put your model into evaluation mode and then take a batch of data and evaluate right there itself so you get a sense of progress in the validation as the thing progresses. So that is a good part of it. You take a bit of evaluation data period too. That's all it is. Log directly, log directly is for the dashboard. This is the main code and you see the simplicity of it. You give it the model with its arguments and the metrics it needs to compute, a pre-trained model, and you give it the data sets that you need to fine tune it. And then you say, go fine tune, go train. But remember, this train is actually fine tuning it. And then this will run and it will run and it will run for a very very long time this can have a run time okay for three four hours and then you'll get to have the evaluation of your models so that is what uh that is what it is to use the hugging phrase okay so now let me go back and you refresh what we learned a long time ago this was one of the first labs you did when we we said that we don't know all of deep learning but we can still get productive simply using pre-trained models so it must be reminding you of things that we have done in the past. Go ahead. In that trainer call, you have the eval dataset and also the train dataset. Yes. The parameter which you passed is evaluated during training through. Does that mean that it's just one cycle of the training as well as evaluation using both the data sets how does that for one step for one mini batch you train your model improves a little bit then you take a bit of the evaluation data and right there itself you see how well it is doing on the evaluation data right and then you continue training But at the end of it, when the training has finished, you apply the evaluation on the evaluation dataset completely because why? You need to find the performance metric at the end of the run. Right. When the model is fully fine-tuned, you want to do the performance metric. So that's what we are doing here. So now going back to what we have, Hugging Face Transferbook. Excuse me guys. We have done this before. Remember Hugging Face, when you just want to use it, you don't have to do anything new. That's how we started this workshop. I said you just go ahead and use it. It is even easier and that's the whole thing about transfer learning. What do you do? The basic concept is a pipeline. A pipeline is by now NLP pipelines you're quite familiar with. So what do you do from transformers? You create a pipeline. Well, it turns out transformer pipelines are actually quite simple. They're not that deep relative to the traditional NLP classic NLP pipeline. So this is it. You create a pipeline. Here we do. Which pipeline? You say that go and pick up the pre-trained model called set for sentiment analysis. Whatever is the best model you think, default model you think at this moment in the library for sentiment analysis. That's what you want to do. Device is equal to 0 and push it to the, I believe to the GPU or the CPU, I forget which one. Then, so this, you don't have to by the way give devices, you can skip it if you give it i believe it pushes it to the gpu or something or not push the gpu i forget which so sentiment you you get this and then all you have to do is you have a model the model also and this is very pythonic syntax the model is in pytorch the beauty of that is your model to do the forward prediction the model is a function you just apply it to any input it will give you the prediction and here is the prediction it says some of us love deep learning workshops and support vectors it says well it's a sentiment. I hope you all resonate with that. And though by now we have covered so much material, you might be a bit tired. Parsing batches of text for sense. So you don't have to parse just one, you know, input. You can parse a whole batch of inputs. So for example, here it is. And then I put some positive and negative sentiments. I also just for fun put an empty statement it was a dark and gloomy night good grief some negative statements and some neutral statements so let's see what happens when you feed this in some of us love turns out to be positive today the sky is shining All is well that ends well. Positive. Seems positive. It was a dark and gloomy night. Seems to be very negative. Good grief. The turmoils of election are upon us. Well, we ran this thing two months ago and I think that statement I hope you all agree is still true, isn't it? The train slowed down as it neared the station. Well, this is where it got it wrong. The train slowed down as it neared the station is not a negative statement. It's a statement of fact about it got it wrong. Why? Because probably slow down has negative connotations. And that brings out the limitations of the state of the art of... Excuse me, today I've been busy since very early morning. So it brings out the limitations of sentiment analysis. The state of the art is this. So just like today we say transformers are so much more powerful than RNNs. And look, RNNs didn't do such a good job. Perhaps tomorrow there'll be some other constructs, some other breakthrough that will help fix this problem. Yelp review samples. This is it. So from the Yelp review, take the smaller set by the way. Yelp review is a massive, massive data set. You take a smaller Yelp review data set which is there. I think I created it by just taking only a few rows of it. Then you feed it again into what the Yelp reviews say and it will say these are the Yelp reviews and it will go about saying the sentiments you know. I put it into a nice funders data frame. This is the sentiment that it comes out with. As someone who has worked with many museums, I'm actually horrified, negative, etc. I'll let you guys have fun with this. So we have done this. I'm not repeating it. What else can transformers do? Well, transformers can not only detect sentiment in English, they can detect it in many other languages. They have been trained for many, many different languages. I think they're multilingual transform, multilingually trained transformers. They're transformers trained only for specific languages, just like French or Hindi or Tamil or whatever, and Spanish and so forth. And so you go and pick it up. Don't try to, I mean, if you're lucky, you'll find a transformer that has been trained very close to your practical use case. And you just try to, I mean, if you're lucky, you'll find a transformer that has been trained very close to your practical use case. And you just have to, you don't even have to fine tune it. Just use it, right, as in this case. It is amazing, actually, how often you don't even have to fine tune a transformer. These pre-trained transformers you just use it as a transfer learning exercise apply it you're done now auto this will this is the tokenization you remember this is the point if you take a model any one model doesn't matter what if you do auto model from pre-trained it will build a model and you can do auto tokenizer you can say say, okay, whatever it is, go get the tokenization from that automatically. However, tell me how to tokenize the data to feed into this model. This bit of code will do it for you. So this is all, you know, Hugging Faces has made transformers unbelievably easy. I mean, this is not how it used to be working with transformers, but since I increases this come about obviously things have really improved. You can actually go and look at the tensors in this if you run it, you can see what those tensors look like and you can feed those tensors by hand into the model and so forth. If you, I mean, and by the way, all of these are tensors. So PyTorch, the thing that I keep talking about is that they are tensors. Of course, all machine learning is tensors or multidimensional matrices of sorts. Right. So this is it. These attentions, these transformers are, you can ask for the hidden, you know, the hidden layer values. It will do that. If you remember when we run this, it produces a lot of value. I won't run this. Or maybe should I run this? Let's run it from the beginning. Some of it will take time. Some of it will take time. Because we're just doing, this is all text, everything run. I won't run it at this moment, it's perhaps dangerous. You all have this notebook. Do you guys realize that you all have it in your notebooks NLP lips folder, the Hugging Phase Transformer file notebook? This is it. And so we can run it and when you run it, it can answer questions. For example, many things it can do, like it can do the classification sentiment analysis. I'm just going back to the question some of you asked, what can transformers do in practice? So this is going back to that. We have done this in the first workshop. Question answering. You know, you have a text. We have options. This is a whole text bit of thing. And this, by the way, is taken from our website, course website. This is something I had written. In this workshop, an optional activity, as an optional activity, there is the reading of some research papers from archive. While it may appear intimidating, these papers are considered important readings if you want to be considered an expert. All papers broadly follow the IMRC format, introduction, method, results, conclusionsults, Conclusions. The easiest way to start reading a paper is to first read the abstract, skim over the introduction, short circuit straight to the conclusion. Once you have gotten a general sense of the lay of the land, now carefully read the introduction, preferably with a highlighter and pencil in hand. When you reach the method section, understand as much as you can on the first reading, the first careful reading. It may take a few steady iterations before it becomes fully comprehensible. So do not be daunted if at first study, it feels intimidating. In due course of time, it becomes familiar and easy. If the paper really interests you deeply, see if you can reproduce the results of the paper independently or check out the Python, PyTorch implementation of the ideas. Research is an open community and implementation of an idea gets shared very quickly in the open source domain. So this is from our website. Now the question being asked is what is the easiest way to read a paper? And would you guys like to guess what the what what it will come out with? Pass me the answer, correct? Yes, it will come out understanding in a detail. Let's go back and see how much I need to import. Text is this question. Okay, I need to import at least the pipeline. Let me do that. And then we'll run it right here. Only that section. I don't want to run everything because for some reason, I'm having stability issues today. So from this, ask yourself, how would you have answered this question? And then let us see how the system answers this question. This machine answers this question. So here is the text. And now let's see how it answers. So what would you answer, guys? What is the easiest way to read a paper? And the second question I wrote is, what happens in due course of time? Anybody would like to hazard an answer before we run this? Read the introduction. Yeah. So the answer is, it says that the easiest way to read a paper is first read the abstract, skim over the introduction. Would you agree? It's good advice reasonably good advice the second is what happens in due course of time and it's and it says it becomes familiar and easy once again would you say that that sort of talks to addresses that question so again guys this is the power of transformers. Three years ago, we would not have believed that these things are possible. We're able to do this pretty well nowadays. Fill in the blanks. Suppose I give it a sentence. Some of us, a blank token, the deep learning workshop training at support rate is very much. Well, let's hope that it doesn't come up with the word hate. Let's see what it does. It says sequence some of us enjoy the deep learning workshop. Thank goodness. The second thing it says some of us appreciate. Third is some of us enjoyed. Fourth is some of us appreciate third is some of us enjoyed fourth is some of us love and fifth is some of us like i'm happy it is all positive statements and transformers like support vectors or so it seems so i think so it gives a list of possible answers gives a list of possible answers. Is that it for this? Come again? So it gives a list of possible answers. It gives a list of possible answers, ranked by what it places. This is 0.4, 0.3, 0.17, and after that, 0.5, and anything beyond that, 1%. So you realize that any other score that it gives will be less than 1% relevance. So these are your top five answers. So far together. Likewise, let us go and look at text generation. Well, lo and behold, there is a transformer trained for text generation. So let's take a text, let's generate 60 characters, 60 thing. The easiest way to start reading a paper is to first read the abstract, skim over the introduction and sort, so get straight to the conclusion. When you run this, what does it generate? It says this, the easiest way to start reading a paper is that, skim over the introduction, sort circuit to the conclusion. And then it says, the paper is a good starting point for any student who wants to learn about the history of the world. It is also a good starting point. So here it seems to have lost it. starting point. So here it seems to have lost it. Would you agree that we need to really improve things? It's not a good beginning yet. So what happens if we take more more text here to give it more context? Let us say that we give it the entire text here. And try to give give it more information information let's give it a better chance let me feed it here this all right let's try a look oops what happened text generation pipeline pipeline is not defined we just just did crm transform as import pipeline. Where is it? Let's add what it comes up with. Uh oh. Okay, something isn't quite working. Maybe I give it to bigger text. Let's try a little. I think the moment you put special characters things begin to get bad. I'll have to remove the special characters. Yeah. All right. Let me... Okay, I will remove some things from here. So yeah, this is a bit of experimentation to see what went wrong. Obviously, I don't have the time to sit in the back. So do anyone if you see any weird or special characters here? Not created. Not initially. Preferably in the last but second sentence there are two hyphens. Okay, the context is 133 token, max length is 60. Okay, let's say 200. Yeah, that was the reason. There it is. All right, let's see what it said. After understanding, preferably with a highlighter and pencil, when you reach the method section, understand as much as you can on the first reading. Now, once, where is it, careful methods, understand as much as you can on the first reading. So now this is what it has written. The first step is to read the paper. The paper is a simple but important example of how to read the paper. The paper is a simple example. All right guys, so I like this. I'm rather relieved that AI is not yet ready to become our overlords. How many of you are relieved that it can't do everything yet? Definitely. Certainly, right? So here it is. There are many legendary AI, legendary researchers in AI. The most important researcher in AI is, now let's see who it comes up with. Ah, it says Albert Einstein. He was the first to use the term intelligent in his book, The Theory of Everything. He was also the first to use the term intelligence in his whatever legendary researches in agriculture. I just have one question. Yeah, go ahead. All these Little bit loud, I can't hear you. Yes, sir. I want to ask what is the incentive for Hugging Face as an organization to make all these applications available for general use i did not quite get the last part but i'm guessing you're saying what is the motivation to use hugging faces no no what is the motivation for hugging face to make all these pre-trained models available for uh you know general usage for everybody, you get, of course you get the, you know, it seems to address every aspect of the Bloom's hierarchy of needs, I would say. Think about it. You get self-actualization, you're building some things that you love building, you release it to the world, you gain people, you get respect of a wide community of very talented researchers. And lastly, you also make tons of money. If you go to hugging faces, face that face, you will realize that this hugging face has, it also has exposed hang on, besides this, besides the open source part, let me just go to the . I think they started working with Microsoft also on, I mean, they just used GPT-3 for them, something like that. No, no, more than that. They now have the, yeah, look at this. Models and inference API. Do you see the inference APIi right what is it now once you have a successful project you can always find a way to monetize look at this for low low price of 600 dollars a month they will let you do your nlp work for up to 50 million characters Your NLP work for up to 50 million characters. Right. They have made it into a cloud service. Correct. So that is that a lot of people, they don't have the staff or the money to hire data scientists, but they still want the results. So all you have to do is here you go. Or you just take some people to integrate it. And if look at the inference API, it should be pretty straightforward. Yeah, there it is. Post. They just made it into a REST service. So does that answer your question? Yes, sir. It does. Thank you. Likewise, named entity recognition, you can do what are named entity recognitions, organizations, geopolitical entities, person. So you take the centers and many excellent AI. Oh, by the way, we didn't run this. Let's see what it comes up with. Oh, there are many legendaries. The most important researchers is the one who invented the first computer. He was the first to use computer to solve the problem. OK. All right. After that, it gets repetitive. Would you think this is possible at least, reasonable? All right, sometimes it can get funny. Named entity recognition. So for example, it says there are many excellent workshops offered at supportors, which is located in Fremont. And this is one of the instructors. So it says that the word support. It seems to refer to an organization. Right. The word vectors also refers to an organization, it seems. So all of it put together seems to refer to, so you realize it's a suffix. So it has figured out that this entire thing is one entity. It's an organization which looks correct. It has come to, it has found the word Eiffart, which in itself is a location because it stands for France, but FR together with EMONT becomes Fremont, and that too is a location. And then the California, it is identified again as a location, as seems to be a person, and as if is also a person, and likewise, the rest of it is a person. So you see guys how effective it is at named entity recognition, right? And we didn't, right, we didn't train this transformer at all, it's free trained, just you're using it. And then when you have to train the transformer, we just went through the code for training the transformer. And that also is not hard, it ultimately just boils down to this. So anyway, I'll stop here and now I would like to go back to the Honeypaces page and walk you through some of that for the next 20 minutes. Any So, for this case, you are using distilbert? No, no. For each case, what you do in these cases is whenever you use a pipeline, named pipeline for a task, there will be a default transformer. You can ask it what it is. And if you find it too slow, you can instead pick up a Distilbird version of it. Oh, because the way you are running, I've used TensorFlow in the past. So to run one inference, it only takes like 10 seconds. In your case, it seems to run faster. That's why. See, Shankar, I'm not running it on a laptop. Oh, this is powerful. So generally, you know, so here's my thing. Again, it goes back to the things that you do. I tend to run inference. I told you I tend to run the training in Python, the inference in Java. And inferences on multi-gpo machines so are easily able to push uh like a close to 100 inference per second i'm just talking about this notebook the way you are able to run in one example able to run in one example. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. It's pretty fast. It is pretty fast. Yeah. The times are fairly modest. Yes, sir. What were the execution times? Let's go back and look at it. Mm-hmm. Uh, you named it recognition. Uh, see, most of the time, it's just loading the pipeline. Oh. Mm-hmm. just loading the pipeline so now we'll run this which will take time to load the pipeline loading it into memory which will take its own time but ultimately when you run in production you keep these things in memory in giant memories memory banks now let's see what the execution time is 142 milliseconds so these things are very fast and especially once you warm up the code it gets even faster so you can keep doing this so i i ran some cases for a classification problem using TensorFlow. So when we did the inferencing, we still had to lower the BERT coefficients plus the fine-tuned checkpoint. Fine-tuned checkpoint, in TensorFlow, they call it a checkpoint. You fine tune the model. At the end of the batch, it saves all the coefficients. The basic model you need to save and then you just load it. See, Shankar, my experience is that if you have a sufficiently good... You can do inference, by the way way the cpu level or at the gpu level uh if even if the cpu see here's the thing my basic rule is don't move it to gpu unless the cpu turns out slow right so for example if you're not getting uh suppose your actual practical load is let's say 20 in for instance a second right which is a pretty heavy traffic by the way but then and you are able to push that on your cpu then why go to gpu because you will have like a 128 like like for example a 256 gb or 128 gb of ram is pretty standard. On servers, 2 terabytes of RAM is pretty standard. But when you go to the video card, you know the video RAM, you get a huge premium. There, you're lucky if you have 24 GB, which is what you get in 3090. 48 is the state of the art almost. Inferencing mostly done in CPU. And then I can follow up on Amil's question. So when we do the inferencing, we have to use the base BERT coefficients, which is about all the 760, the base, uh, seven 60, uh, five sizes about, uh, and then the checkpoint, the model checkpoint, there's about one point for Jimmy. So yes, the footprint seems to be very mean. That's why I was curious. I have no idea what TensorFlow is doing. See, you know my opinion. Maybe I shouldn't say that. I believe that a PyTorch fixed all the bludgers as far as the API design is concerned. It is a well-des designed PyConnick thing and is actually faster. So TensorFlow got into a catch up mode. I mean, can you imagine TensorFlow without Keras? Yeah. So I agree, Alvar. But still, the PyTorch, it uses the TensorFlow coefficients, isn't it? And all that stuff. It's all coming uh i think python is getting from the same coefficients i guess it's not changing it is definitely true a lot of these things are re-implemented natively in PyTorch. I think so. But the bird, maybe not, I don't know. Actually, that's a blind spot for me. Would you please find that out? Whether HuggingFace just takes the TensorFlow 1 or re-implements it? My impression was it re-implements it. If it uses DistilBird, which is a much finer footprint, then probably it uses distal BERT, which is a much finer footprint, then probably it's much easier. But I remember you still need to load. I didn't use the hugging pills. For the time so much, it still weights the BERT coefficients. Yes. Quite likely it does. I mean, see, ultimately, whichever way you do it, model is model, right? Right. It doesn't matter how you implement it, so long as you know the weights, you are in good shape. Yeah. So I don't know all the details. I model as well as the trained fine-tuned model for inferencing. Yeah, see, I suppose, yeah, what I do for, I would just on my machine, you have seen my machine, so I would just load it for inference time computation on the server i would just look what is the load if the load is low cpu does it it's just safer i mean less crashing you know you don't get this out of memory errors when you put these models on the GPUs for inference, good thing is it's blazing fast. Bad thing is there is only so much of parallelism you can do. Because there is only so much hardware resources available in the GPU. So it becomes a little complicated doing it on the GPU. Thank you, Ashish. See, one way is that people do many things. One thing I've seen is the evaluation pipeline, they'll just feed it into the pipeline evaluation. And then it is being evaluated in many batches or things like that, it's going through it. Sometimes, I mean, most often the evaluation is just one, you know, one record. So what people do is they don't just do one record at a time. They let a mini batch come in, pass it through. So anyway, so all right guys, that is your, that is it. But generally, so I don't know, my experience with PyTorch is that it just feels responsive and good. And your mileage may vary. I used to like, see I'm told that TensorFlow 2 is just as good as PyTorch. Maybe. I don't know. So, all right, so we'll go now to the hugging phase. documentation, the data sets model have documentation. So here you can get a documentation, how the models are tagged, if you want to contribute models and so on and so forth. You can do that, you won't do that because it's too it's a bit premature for us at this particular moment but let's go back to hugging faces and hugging face transformers in particular All right. So the point that I want to show to you is that when you come here, and you must come here guys, spend time doing it. So the library contains a contributions made in PyTorch, TensorFlow, Flax. These are the different frameworks into which the implementations has been done. So let's go back and look at it from the bird. Yeah, so birds, since bird has been contributed by Google, quite likely this must be a TensorFlow implementation. Bird is conceptually simple, bird config, so on and so forth. Yeah, right. It's a bird model. It doesn't talk about it. Anyway, the important thing is that there are many, many models and they are all listed here. You see there's still bird and all of these things and what you can do is that you can go and summary go first thing you do actually let me start from the beginning uh summary of the tasks what are the things you do uh with transformers you do sequence classification we just did that a sentiment analysis is an example of classification. Then you can do question answering. So here is a question, extract a question, answer is a tri-store. Answering, we did that question answering just a little bit ago. Then you can do just a little bit ago. Then you can do it for, I mean, this is the code, you can run through the code and so language modeling. So language modeling is predicting the next words, etc., etc., and creating a model of the language and then doing it. Masked language models, again, you do fill in the blanks. We did an example of that. A causal language model, again, is the prediction of the following tokens. You can do that. And so for example, if you give it Hugging Faces is based in Dumbo, New York City, and and then hopefully it should come up with and has is the most logical word after that. Text generation. We saw an example of text generation. Not one of the great strengths of transformers as of this moment. How is it possible? Now, you can do text generation using ExcelNet. By the way, these ExcelNets etc right these are all uh architectures at some point i'll explain uh if we get time i'll explain to you the difference between all of this but just so you can just read it up uh see here's the basic idea burke is the and gpt are the sort of the root ways of looking at this. And the original transformer with Burton GPT are pretty much there. And people have created many variants of it, trying to make it faster, trying to make it deal with bigger text, remove the size limitations and so on and so forth. So a lot of body of work has come in. And at this moment, the situation is that for certain situations, for example, if you're doing with long text, long format is good, right? But at the end of the day, what you should do, and that's where your weights and biases comes in. You should try out all of these models that seems applicable and see where do you get the best performance. One easy way is to don't even bother fine tuning it. First, just compare the performance of the pre-trained models. Just do transfer learning and see which of these models gives you the best, reasonably good performance on your data set. Pick that and pick the top three and then try to fine tune the top three and see where it takes you. That could be one approach, sort of a practical approach. We can do that. So named entity recognition we just went through. Then a text summarization is important. Suppose we get a long text like this, an article, and your job is to, you ask this machine to summarize the text and say, give me a summary of length only 130. so let's see how good the summary is a new york cnn when leanne something something was 23 years old she got married in blah a year later she got married again in blah but to a different man oh goodness it seems to be a very complicated set of relationships she pleaded not guilty and whatnot at the of it, the summary seems to be this person is charged with two counts of offering a false instrument for filing in the first degree. In total, she has been married 10 times with nine of her marriages occurring between this and this. She's believed to still be married to four men. Oh goodness. A pretty dramatic paragraph and summary, I suppose. suppose so yeah there it is it can summarize text by finding the most crucial sentences or phrases in the text yeah go ahead this kind of model as a summarizer if you run it a second time will it produce a if you run it a second time, will it produce a similar but different article? It will produce this thing. Remember that when you have a machine learning model trained and you don't touch the weights, it is just matrix multiplication. So now it is a deterministic machines. Given an input, it will produce the same answer again and again. Okay, thank you, Asif. So it's good to know we can't use this for plagiarism. Translation, translating from one language to another. Again, English to German, for example, somebody has already trained a transformer to help you out. So go for it. And that's how you do that. Now, the thing that I would like you to pay attention to is if you go to the exam, so there are pre-trained models. This is the guide of pre-trained models. This table is useful. It tells you what is there. So see a bird base, we saw it in that last event in this Wednesday's paper when we did the bird paper. This is it right, 12 heads, 12 layers, 768 is the dimensionality of the hidden states. It has only 110 million parameters, somewhat small by today's standards some of these big models that are emerging uh but large now has 330 million 36 million parameters right and the numbers sometimes keep uh growing now let's go to this yeah distilbird base, do you notice that relatively speaking, it has a modest number of parameters, only 66 million parameters, right, which is something to be said in favor of distilbird. So unless you're doing something with very high degree of accuracy is required, you should stay with the things that are not that expensive basically. But if you do need high degree of precision and you do have the computation resource, of course go for it. That is that. Then there's some examples that they give for different tasks, some all sorts of tables, what would you pick? Like, for example, if you're doing text classification, what should you like to do the big table of tasks, let's say, here is something blue. The example, these are the example data sets. And let's open this in Colab and see what happens. Here is the text classification notebook. So see guys at this moment, a lot of this work has been standardized so your first response should be to go and look at the code people have written and make sure that you follow up, you know you can improve it, make better, add things to it but at least you'll get a starting point for your code. By now you must be realizing that this is beginning to look very familiar, I hope. This part. We have done the training arguments, the compute metrics, so forth. And then of course there will be the train and evaluate and so forth. And then of course, there will be the trainer, train and evaluate and so forth. So I see I have one question. The things like trainer and the training arguments are part of Hugging Face, is it? Exactly. So what they did is that, see here's what happened. Hugging Face started out with Python. Then of course, you end up importing a lot of these models coming from tensorflow then they started giving you know two code bases examples using tensorflow using pytorch it's a nuisance right and if you look at this when you do it using raw pytorch then it is just a few more lines of code but there it is so then they said why don't we create a very simple api of our own right and they created it that's your trainer and the training arcs right in a few related classes and for data sets it's really actually the data set partner when you struggle with data since then you'll realize how convenient it is to be able to parse datasets like this. So the models are coming from transformers, the networks. Sorry, the models are coming from? The transformer library. Yeah, yeah, transformer library. The community contribution is huge. So anytime you train a transformer of your own, and it's not related to the company that you work for in your day job, you should contribute. Because see, guys, here's the thing. If you really are training the transformer, the one thing that nobody talks about is the ecological cost, the one thing that nobody talks about is the ecological cost the environmental cost i mentioned that i read an article somewhere that said to train one transformer giant model these days takes the same amount of electricity or energy as much energy as would be consumed by by the manufacturing of 100 cars and those 100 cars running their entire lifetime on the roads. So these are huge power hogs and there's an environmental cost to it. So let us hope that we train them once and we share them. So from what I discussed today, so the transformers gives a wide range of tools. Bird is one of them. Oh yes, yes and a lot of these are bird derivatives. A bird base when you use, see people use bird base more out of laziness. If you see somebody's notebook and the only thing that has been tried out is a bird base uncased, you are meeting a person who needs some questioning. I think that's me. So that's that. I mean, see, there is such a vast variety of resources available, especially if you don't have to fine tune it. Even if you fine tune it, why not pick some good models to fine tune, isn't it? Good transformers to find you. So try this out and there's a vast, see the number of papers that have emerged on this and the architectures of transformers that have sort of done adaptations of the BERT and the GPT and all of that, enormous number, there are many, many architectures that are there. BERT has of course inspired a whole cottage industry of other transformers. And now, of course, there is GPT-3 that people are so excited about that they are almost claiming that this is the first model that is showing the beginnings of true artificial intelligence or generalized intelligence. I don't quite agree with the hype, but nonetheless, the results of GPT-3 are very, very impressive. Read up, so guys, maybe as a, so here's the thing, I'll leave you guys with some homework. Make yourself very familiar guys with this website it is in if you're doing nlp and you're not intimately familiar with this website right it is like you're living in the modern world and you're not intimately familiar with a good car you're not in the modern world and you're not intimately familiar with a good car you're not going anywhere without it is hiding face a commercial company yeah having faces a company in its own right now okay Started out as an open source project and now they are a company in its own right. In fact, the CEO of Hugging Face is a very young fellow. I mean, at least I guess by our age standards, quite a young fellow. He talks now at conferences and so on and so forth. Last year I saw the Hugging Face, there was a public website and then suddenly it began disappearing and then the link went to Transformers. So I was curious. Now say that again. Last year when I checked probably like 18 months ago, Hugging Face was still available in the public domain. In the sense that you can download it. It doesn't seem like a company. And then recently, probably a couple of months ago, when I searched for Hugging Face, it connected me to Transformers and I went down to the Hugging Face. Yes, yes. Now the library is now called Transformers and all the models are still freely available. The only thing they are selling, see, is the catch-up model. The actual IP is free, but now everybody is giving cloud service. So have they have they're monetizing it by creating a cloud service an inference api that you can call using rest and you don't have to know machine learning and just repeating your data thank you so anyway guys that is the lesson i would say i don't know some of you were asking questions, how to use transformers. Does this address your questions Premjit? You were one of them. Yeah, I actually caught up with the other lecture as well. I was kind of completely spaced out. Yes, thank you. completely spaced out. Yes, thank you. So guys, one thing I must say, when you run Transformers and it takes a long, long time to finish, even fine tuning, don't be disappointed, right? Don't run it in the, I mean, if you run in Colab where it is price limited to $10 a month, Colab Pro, it is good but if you are running it on a jupiter notebook in gcp you will rake up a large bill because you'll use a lot of gpu you'll be tempted to use a lot of gpus and so forth and before you know it to expedite it you're paying google like three to four thousand dollars a month sometimes seven thousand dollars a month so be on the be on the watch out um remember it is always a far better idea to write that kind of check to support vectors no one is amused okay so i think today's lab is uploaded Okay. So I'll say for today's lab is uploaded. I'll just upload it. But I won't upload the whole directory because it's just a couple of files. I'll upload these to Python. I appreciate that. Yes, of course. I'll do that. Run it. I have simply tuned the parameters to a level that it is runnable on laptops. But again, it will give it a few hours one of the things we haven't done so guys today this is a sort of the last week of nlp do you guys now feel that at this moment you are equipped to solve real problems in nlp now that's what you'll be doing you'll be using transformers you'll be using space. Use Spacey for the classical stuff. It's very good. It's very fast for all the classic stuff because sometimes the transformers are overkill. And use the classic Spacey for that. And use transformers for everything else. It boils down to just two simple statements. Buy and like. I mean, there's a lot in this field but by and large start with that and i hope i have given you a way to do powerful things with transformers with just a few lines of faith and with that i'm essentially concluding the the NLP, the last week of NLP. In fact, I'm thinking of stealing the next session for the next part moving forward now. Would that be okay or should we give Wednesday also to NLP? I think we can start with a new topic. But having said that guys, please, please do become intimately familiar with this website, Spacey and Transformers. If you, when I look at the scope of things, there are only three libraries I've emphasized, Spacey, Gensim and Hugging Face Transformers. Now there's some emerging libraries who's mature, which are gradually maturing and Allen NLP is very good. Give it some time, I think it'll come through quite well, but that's for future. But for now, just three libraries, just stick to these and you'll be in good shape of creating production class applications enterprise grade applications in the air right in nlp that's that so the three are hugging face gen sam and spacey spacey that's it in other words go over the notebooks if you go over it you'll realize I've very carefully limited myself to only these three libraries. Anybody who has been reviewing the notebooks would agree? I think with my mallet issues, I'll skip Jensen, though. It's good for topic modeling and a few things, but yeah. I mean the Mallet library would not run for me and the errors are very mysterious and it seems like there's unresolved stuff in general. Oh. The one thing that you'll find conspicuous by its absences, can somebody guess which library that I've carefully avoided? Jensen? NLTK. NLTK, yes. library that I have carefully avoided. Jensen? NLTK. NLTK, yes. On NLP, pretty much focus on NLTK. Now NLTK has academic roots. It is vast. It is completely written in pure Python. For that reason, it's great to experiment you know do these things it is not production it is not meant for production even though a large number of people are using it in production you pay in terms of hardware cost right deliberately i stuck to you know enterprise grade libraries right but so be aware that when when you pick up a textbook on NLP and you find everything is done in NLTK, which by the way happens to be the fact, you may wonder that why is that I didn't, you didn't learn that at support vectors and the reason now you know. It's slow. And also it's vast, it's just a very big stick to these libraries which are very, very focused, very clean API. spaCy has a very clean API, I hope you agree. You just create a pipeline and there's a lot of similarity between the spaCy API and transformers. And now something even better is happening. The spaCy 3.0, and you can get the nightly builds for that, spaCy 3.0 bridges the gap even more. The transformers are built in. So spaCy 3 uses transformers. So you can stay with spaCy itself and still be using transformers. So already in a release candidate, spaCy 3. Become release using transformers. It's already in a release candidate. It's become released. I was seeing it, it was still nightly. It's a nightly, but if you see the, it says RC2 actually. Ah, RC2, so you're using it, right? How stable is it? It's quite stable. It's quite stable, huh? Yeah. Okay. I think in a way, you guys are the last batch that is learning from SPACY 2. But next time when I start, it will be SPACY 3. That is lovely. So SPACY and transformers put together make a very powerful combination to be the, you know, more or less a one-stop shop for your most common NLP needs. And then you can throw in Jensen and a few other libraries as needed. I think just spaCy and Hugging Transformer will more than 90% will. Exactly. Exactly. Yeah. Thanks for confirming that actually, since you are using it in your company. With that guys, I am done. Thank you.