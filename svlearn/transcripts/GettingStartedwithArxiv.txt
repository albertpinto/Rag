 So in this workshop we have two projects that I asked you to do. One was prescriptive. I said go and analyze the archive data set. This data set is of researchers who have submitted their works, preprints of their work, pre-publication versions of their work to the archive site. Archive is perhaps the largest network or site of pre-prints. Whenever people come up with some discoveries, at least in the fields that I belong to, which are theoretical physics and computer science, you don't go to a journal and ask them to publish your first reaction is you you summarize your research and you submit a preprint to archive once you submit it it already becomes available to all the researchers in your community and they can discuss you know they can think about it and usually by the time something gets peer reviewed and published many many months have gone by these days especially in fast-moving fields like artificial intelligence or data science areas but you don't want to wait till something gets published which may take up to a year or more you want to know about the breakthroughs as they happen so it has become very common for us to every day treat archive as some sort of a new site one of the things i'd recommend it to all of you if you remember was this website called archive sanity you can go there and some people curate the best articles or what they thought are the best articles best publications research work and they make they sort of annotated with comments and this and that or something or the other and they make it available a couple of YouTube channels which do the same thing will take the breakthrough papers or some important papers being talked about and they'll discuss it in detail so So we'll focus on archive. This particular data set is there on Kaggle. Somebody has collected it. It's basically only the metadata of articles published there on the preprint site and is limited to the domain of data science. So it is AI, it is machine learning and so on and so forth. data science. So it is AI, it is that machine learning and so on and so forth, are these areas. So it is not all of the areas that archive covers. Archive is vast, but it limits itself to machine learning because we know this field. So when you do the analysis, you will have some intuition. So for example, if you find that, if you're looking for the most published author and a name pops up, you can decide whether this analysis looks right or not, because you probably know who the most published authors are. Right. Who are the most referenced authors or cited authors and so forth. So that is why I gave you the archive metadata data set. It is also something that is fairly manageable in the cloud and on your powerful laptop if you have one, actually even on ordinary laptops. Though I highly encourage you to do this in the cloud, not on your, if you don't have a very powerful machine, don't do it at home. Do it in the cloud. In this sort of analysis and more and more in the modern world, when we analyze data, there are three different axes, even for just exploratory analysis. The first is looking at the tabular aspect of the data, you know, a data frame. You have data as a data frame that you can do your basic bar charts and count plots and pair plots and all the things that we have been doing. Then a data like archives data, there is a networking aspect to it. Researchers collaborate, they network. Research papers refer to, or preprints refer to each other. They cite each other. So you can form, you can observe the collaboration network between these researchers. You can also create a network of citations from paper to paper or a bipartite graph of, you know, researchers citing certain other researchers. That is a citation network of researchers, citation network of papers or a bipartite graph in which authors cite some of the papers and so forth. You can do all this way, you know, straightforward way. These days, networking libraries have become pretty powerful. We will use a library that is quite popular in Python. It is called NetworkX. For R, we'll use a library called iGraph. iGraph is based or implemented fully in C. It's much faster, obviously. There is an effort to put iGraph to Python, but I don't think it is as popular as NetworkX. NetworkX is written entirely in Python. For that reason, it's a bit slow, but the syntax is very Pythonic, very natural to Python, which explains its popularity. We'll use these two to begin with. Then we can do text analysis. After all, these articles, these research papers are about some subject matter. They contain abstracts. If needed, we can programmatically go and get the content of those. We can get the PDF versions of those papers and from the PDF we can extract text. There are all these programmatic means to do that. Apache, for example, gives you lots of open source tools to do all this manipulation. For text manipulation or text processing, you should use the NLTK library to begin with. It's pretty popular. Let's start with this. Later on, you can use the Gensim and other things to do semantic analysis. And we can use that as a starting point later on. I mean as you grow this project and go into the deep learning, you can carry forward this project because you can do apply your transformers and other things that you will learn in deep learning on the same data set. So this starter project that I gave you deliberately was designed in such a way that you could carry forward to the next workshop. So to summarize, we'll do the archive data set today. I'll give you only the starter code. I won't do much. The idea is that you should do and learn. At subsequent time periods, I may release a larger and larger part of an analysis. But it would be far better if you took it in your own creative direction. But the three axes on which you must address this analysis is first as a data frame, you know, as a tabular data, the tabular data analysis. The second thing you must do is a natural language processing. Use basic stuff at this moment, since you are not experts in NLP, use the natural language toolkit. Do basic things, see if you can create a word cloud of the most popular authors, word frequencies. And some of these things I will do right now, you do that. And the third is networks are becoming increasingly a central topic and one of the central topics in contemporary data science. Everywhere we have networks, our friendships are networks, we have professional networks, researchers like paper, they have collaboration networks and you have citation networks and so forth so you must also learn network analysis now those of you who took ML 100 would remember that we went through a couple of sessions of network analysis before so have network analysis also as a dimension of your analysis or out of this project so with those words let me start sharing my screen now. Let's go with this. So here we go. This is a notebook, which I hope all of you can see I'll increase the font a little bit. Let you know if I need to increase the font even more. Is this fairly legible to all of you. Yes, it is. So by now you must be familiar with the Jupyter notebook and the way we write it. This is your standard table of Sajeevan G. Content. As you can see, it's a very short table of contents, unlike the other notebooks that have released. The point being that this is just a starter code. Sajeevan G. This is your basic preamble, you know, teams and other things. And this just ensures that all the libraries that we will need are installed. Actually we won't use Torch. Deep learning is for the next workshop, but as a motivation if you can it would be even better. So we're going to do exploratory data analysis on the archive data set. By the way, go beyond exploratory if you can build predictive models and so forth. Great, that's what I hope you do. But let's start with exploration of the data. So the basic inputs are, by now, these are all the usual suspects and you should be quite familiar with them. NumPy, what does NumPy do? It gives you support for high performance areas and matrices and operations on them. Pandas provides you a support for the data frame construct and its manipulation. Pandas profiling is an automated tool to give you a preliminary exploratory data analysis. Like now when it comes to handling networks and the investigations, NetworkX is a very popular library. Again, as I mentioned, it is in Python is very popular and for those of you who are doing it in R, use iGraph. It is a bit slow. iGraph is lightning fast, fairly, but NetworkX can be a bit slow for large graphs and you will see that when you actually run it. So once you're done, you know, once you reach the limits of NetworkX, then graduate to a bit more powerful library, for example, NetworkKit and so forth. There's a very close parallel between NetworkX and NetworkKit, and you can make things much faster, better. Because today is the first time you're doing the lab and network networks, I will make the code dead simple. I will not focus on making it prettier or more beautiful. So just as a recap, some of you may remember that we did in ML 100, I had looked at the Facebook graph in one of the sessions. I'd said that look at the friendship networks for a small group of people in Facebook. And when we did that, we created the graph, those of you who remember some of this analysis, and we created this beautiful visualization of the network. When you look at these real-world networks, you realize that they have structure. Do you notice how people or their relationships are clubbed together, friendships are clubbed together into communities? And this is sort of an ego network of a person. And when you look at this friendship, so you know your friends themselves form groups of friendships. And you know that you have a professional some friends are from your professional life some friends are from your social life some friends are probably from your tennis playing club or your photography club then within this you can also look at the degree distribution like how connected is each person to other people in the network so these are called degree distributions it's one of the things that you should do if you can in this it's really helpful to do degree distributions and then to detect the communities you know when you look at that they are all of these communities can you detect the presence of communities? It's very analogous to clustering in Cartesian data in Cartesian space, right? When you have data in Cartesian space, it's a real valid space. You do look for clusters, K-means clusters, density clusters and so forth. In the world of graphs, the more appropriate term is communities because you don't have a notion of, a very obvious notion of distance. You can create a notion of distance, but it's not a Cartesian space. You still can create notion of distance as in how many hops does it take to go from one person to another and so forth, but it's not a Cartesian space. So what you look for are communities. These are the communities that you find and And sort of we went through that. I will leave that aside. Today we are not expecting that level of sophistication. If you can get there, as you can imagine, these are all like four or five lines of code. But since you're getting started, some of you with networking code, I will skip that. You can now move forward. So that would be the networking library. There's another library called Py geometric for now, skip it because we'll need it in the next workshop when we do we do deep neural networks. Jay Shahzad- For plotting. Of course we use matplotlib and seaborn no brainer. By now you should be familiar with for machine learning will use psychic learn learning we'll use scikit-learn once again it's a ubiquitous python library for machine learning and we are all familiar with it pytorch a high performance gpu based or gpu optimized library for deep learning and finally we'll use folium for visualization of geospatial data now where you will use it i will leave it quiet You can figure it out how you can do any geospatial representation of the archive data. How do we load the data? So where is this data from? I've given you guys the link to it. So somebody has put just the metadata of it. There are multiple versions, there are a couple of versions, five gigs and nine gigs of data, which are complete data across many, many communities in archive. Those are huge data sets. It's much better to play with data sets that you can handle in your notebooks when you're just starting out. You don't want to run a computation that goes on for like two days. So the smallest subset of data is better. It's about 24,000 authors or so, or entries support. So let's look at this data. When you look at this data, oh yeah, 40, 41,000 entries. So how do you load data? This comes in JSON format. Pandas of course can load it just fine. So when you do it, change this line, change it to wherever you have downloaded the JSON file for this data. This should look familiar. What are we doing? We are describing this data. So when you describe the data, we notice that they are all of these fields these fields are described by the way if you go here and you click on this you will notice it described so you can these are all taken from the oh I'm sorry this data is based on about this file okay now we need to get to the data yeah this is the data and its descriptions are given somewhere where the descriptions given now the description seem not to be given but if you look at this this is actually a subset of other archive data and other people have given more of a, let's say we go to this 2 gig distribution. Yes. So when you look here, what you have is a subset of this. So what I will do is I will just copy this or maybe not. I will make a link to this for the documentation. Archive, university archive. Let me add it to this to understand this data is taken from Kato. So if you go here, you can see the descriptions there. So what are the things? ID is a unique identifier. Submitter, who submitted the paper? Not of terrible importance. Typically a paper has multiple authors. One of the questions is, in general, how many authors does it take to write a paper on the average? And we're going to find that out. Title, comments, number of pages and figures, journal rev, information about the journal this paper was published in. That is if the paper finally gets published, you put journal revs. This is a unique thing nowadays. People are making an effort to give a unique document identifier to all CS documents, public documents. So in that spirit, there is a DOI. Abstract is essentially a summary of the paper. It gives you what the paper is about, typically at the top of a paper, even before the introduction and right below the title etc you put the abstract categories speaks about which are the keywords or categories to which it belongs which is self-descriptive versions well sometimes papers can have different versions associated with them people keep resubmitting it after fixing typos or Corrections to their calculations or graphs and so forth. So that is that one good thing is that any one of these paper you feel like accessing you can always directly go access it. It's a very public publicly available data set. A typical data looks like this, as you can imagine. Entry in the data looks like this. So we'll go and explore it ourselves. We take this data, we load it into archive. When you look at it as a data frame, you notice that these are the columns. So by now they look familiar. There is only one catch if you look at the first few rows you begin to realize that see this paper has Gordon and somebody else so author is actually a string containing many authors all the authors of a paper so the title author is a bit of a misnomer. The rest of it is standard ideas link month, month means from zero to one to 12 year of publication, title of the paper, tags. So once again, you have the stack terms, you should give it the same treatment. So this is a point I was making. If you look at the first row of the data, some row, and you look at the author, it turns out that the first author, because it starts with A, is somebody named Mr. Osman and Mr. Washington something. Well, we need to break it up into two different authors. How would we do that into the number of authors? This is a little bit of regular expressions. Do you guys know string matching, regular expressions? When you get a complicated string and you want to extract only what you want, there is a very powerful technology. Regex? Regex, yes. Regex. That's what it is, regular expressions. It's a technology that helps you extract what you want in a very, very powerful way from the text. And so if you look at this, let me give you an idea of what it is. Look at the names. What we want to extract is this and this, isn't it? But you get this long, complicated string. How can you do that? You notice that semicolon, space and single quote this is a unique identifier that helps you helps you find names isn't it and it ends with a single quote so that's what I did I'm saying start with a semicolon a space and a code and after that as many characters as they are present till you hit the first end of single code. When will you hit the first end of single code here? So if you look at this text, you would get this kind of match twice on the two authors. So you compile the string, you compile the pattern that you're searching for, and now you can give it any string and it will return you the authors you'll introduce the string match now when it returns we don't want the first second third character you want to start from the fourth character and you don't want the last character either that is why you take the substring 3 to minus 2 so there's a lot happening here play Play around with it and you'll realize... List. Go ahead, please. No, sorry, nothing. Yeah, so when you go around, you'll realize that a lot is achieved in a single line here. We have managed to find all the authors as a list from a text. And this is the power of regex or regular expressions. They help you do a lot in very just a couple of lines. Like you notice that a function that is just one line has achieved a lot. Now you may also notice that I tend to give hints. You notice that there are hints here. One of the things I didn't optimize is to say that whether str is null. What happens if this is not actually, this will take it fairly gracefully and return you nothing, I think. But you can try what happens with null. Maybe it will bar, who knows. I didn't check that. I should have. So, all right. So, you get this. Now, what do I do? I have a function and I can apply it to every in the archive data set. You see that this data frame I called archive, isn't it? In this archives author column, I transform every value of the author column by this function to authors. So what will happen is the author column will now be replaced by a list of authors, isn't it? And we can see that. If you notice, we now have authors and then once we have them, we don't need the author column anymore. We can just delete it, drop it, and then we have this. Let's look at a few rows of data. When you look at this rows of data, do you notice that it comes out very well? All the authors are coming out very well, much better, isn't it? As a list of people. Now let us do some very basic analysis on this data. Let us look at, for example, how many submissions were made in a given year. That is easy. You know, some of these fields are numerical. For example, a month is numerical isn't it a year is numerical so you can do sort of group buys and counts on that so you're doing a count on the number of submissions every year so as you notice research has been rising up exponentially it's sort of one of the first things that surprised people. People think that research grows linearly, but actually more research has been done in the last two years than in, I think that statement is true, more research has been done in the last two, three years than in the entire history of mankind. So the pace of research is accelerating, completely accelerating. And you can see that effect. So for example, well, archive is not a really reliable measure of how much research is done, but as a very rough and ready thing, you can see if you take, well, 2018 is not complete here, so we should ignore it actually. This and this, if you put together these two, you realize that they will add up to a number. Let's's see do they add up to a number or not this is 12,000 plus 6,000 18,000 more than 18,000 if you add up all of these let's see what it comes to little more than 4,000 3,000 approximately for seven and another maybe 3,000,000, maybe a little more than 10,000. Let's round it up to 11,000. 11,000 and a little more than 2,500, 13,500. You get the idea, isn't it? Maybe there's a couple of 14,000, 15,000 and so forth. So you realize that if you add up the last two years, you'll end up with the same number as up till then. And it just shows you how fast at least this field of artificial intelligence and machine learning is growing. More work is done in the last two years than seems to have been done in all of the history of the subject. That is a remarkable insight, isn't it? And this is the point, you know, when it is one thing to pick up the technology, but gradually you should start thinking, what is this plot trying to tell me? We may say, do people submit more research papers or finish more research in any particular month of the year? When you try to plot it, you get a plot like this. So my observation was that there's nothing really remarkable about it. These small changes, I wouldn't read too much into it. Next year it may be different, going forward it may be different or something. These are fairly small variations from 2700 to about 4000 something, 200 or something. So not much, a little bit March and November seem to stand out a little bit. Right. But not much. When you look at researchers, one of the basic so now you can start posing your questions in experiment. In a field, one of the first questions you ask is, who is the most published author? Number one, who is the most prestigious author right that everybody is uh reciting or referring to who is the person people have done who is the one author who has done who has the maximum number of collaborators in other words has collaborated with a whole lot of other people right you can actually start answering those questions All we do is little bits of code. You take all the authors first from that archive. You extract all of the authors. Do you notice that from the authors column? What am I doing? I'm adding all the authors to an existing list. And from there, I'm saying we found these many authors. Once you find those authors, you notice that they'll be duplicates. The same author has published two papers. He'll show up in the list twice. Folks, I hope this is obvious to you, isn't it? If I just go and take the authors for each of the papers and add it to a master list of authors, suppose you have published three papers, your name will show up twice in this list, isn't it? It shows up and we do a distinct. You see that the 132,679 published authors but because they are duplicates the unique number of authors is 60,000 approximately. So guys, think about it. There are 60,000 people who have done some form of non-trivial research in this field and contributed to archive. This contains great luminaries, as you will see, and it contains, well, everybody else like you and me. Hope this is an encouragement for you to also do a good project and write your own archive, pre-print and submit A good discover something new and submit it. It doesn't have to be great. It just has to be original work of relevance to the field and of value to the field, right? So the only two things needed to publish. A, it must be original and it must have some use, right? So for example, you cannot go to an obscure library and count how many books of poetry are there and how many books of this and that or whatever. If you do that, you may be the first person who has counted that, so you have produced something original. But perhaps that data that you produced or the article that you wrote may not be of much value at all. So that is something to know. Moving forward, we can ask who are the most published authors? So we get this authors. Who are the most published authors? How would you do that? That is very easy. You could just do a box plot. Once you create, by the way, this library is very useful. I often see people use dictionaries to create, you know, word frequencies and so forth. Here the word being the author. You can do that, of course, by all means, but it is more elegant to just use this package. This library in Python does the job and all in one line. And so then in from that you can ask for the for example, I happen to pick a 50 most popular or most published authors and I created a data frame for it and then I visualized it when you do that. Do you notice that who's the most published author? Joshua Bengi. Anybody knows who that who that is? Has heard of Zosia Benji? No. Okay. He's the author of your textbook for the next workshop. And he's a luminary, huge luminary big guy. I think after Geoffrey Hinton, the next name you hear is his. But do you notice something quite remarkable? Geoffrey Hinton is nowhere there in the first, at least I didn't find him, in the top 50 published authors. So it just alludes to the fact that just because you write a lot of papers doesn't make you great. Sometimes great authors, they write sparingly, but great researchers, they write sparingly but when they write it's immediately considered a breakthrough you know they they come out with something very very non-trigger and advance the field okay so but zoshua is a very prolific writer and he's also a very very high quality researcher so obviously you see him here these names as you do deep learning they will gradually become familiar to you quite a few of these names will become familiar to you the next thing that you may ask is Authors collaborate So now you may collaborate with ten different people they may collaborate with other people and so forth So you can imagine that there is a network of collaborators Isn't it so for example you may ask this question between me and that other researchers are how many links of collaborators do i need so i may have written a paper with somebody who may have written a paper with somebody else who may have written a paper with the person the target person that i'm thinking about right so then you would like to for example pass a message through and try to do a direct collaboration with these intermediaries as your reference and so forth so it is quite common to look at collaboration networks uh I should say collaborate collaboration networks rather collaboration networks so guys you notice that I'm posing the questions, things from common sense that seems relevant and doing it. So this code, I've tried to keep this code as simple as possible. All it does is from the author list, if two authors exist in the same paper together, I add them to a set of pairs, you know, couples, A a comma b if a and b happen to be in a in a paper at the same time so suppose a b c are in the paper how many pairs can i extract i can get a b b c and a c three pairs right so we do that so this is what i do for every uh author list in the authors column every value for every the value is a list so therefore for every author list in the authors column, every value for every, the value is a list. So therefore for every author in the list and every other author in the list, X greater than Y is just to make sure that we don't have duplicates. Like we don't have A, B and then B, A. So this is a string comparison. Make sure that by alphabetical order it is there. And you add the pair of those people. And at the end of it, you'll find that there's actually a surprising amount of collaboration in this field. Research is a collaborative effort, massively collaborative effort. Very rarely people, relatively rarely, people write papers completely alone right so generally people collaborate and write this one for example you guys have formed a team and you're collaborating and writing so you form all those pairs and then now comes the graph part of it graph is very easy to get started with Network X you what do you do you create an empty graph and to this empty graph you add all these pairs you say these are the edges so here I'd like to explain what I mean. Let me go to the drawing board. See, suppose we have a pairs of people, let's say, a comma b. These two have collaborated. Then we have c comma d, then we have b comma c. have collaborated then we have C comma D then we have B comma C right and let's say a comma D let me take this example let the researchers do so suppose you find this collaborations present you made this list of pairs. Now what can you do? We can start with A or any researcher as a node, you can say B, you can say, let's say C, and you can say B. Now A and B have collaborated, so you can make a link from A to B. C and D have collaborated, so you can make a link from C and D. And these are undirectional links. So if one is collaborating with the other, the other has also collaborated with one. So these are Srinivasan Parthasarathy, undirected links. Srinivasan Parthasarathy, Then C and D have collaborated. Now B and C have collaborated and A and D have collaborated. Oh goodness, this makes a perfect rectangle. Let me make it something else. Srinivas collaborated oh goodness this makes a perfect rectangle let me make it something else uh a and c have collaborated let's say right a and c have collaborated right and maybe c and e have collaborated so do you notice that you have a collaboration network okay so if a wants to collaborate with E, all he needs is a reference from C, for example, isn't it? And one more thing you notice that C seems to have collaborated with everybody else. So what happens is when you get a network, well, obviously this is a very trivial network, but generally when you get a network, you start seeing some very interesting patterns in the network. But the study of complex networks and the structure, not just like drawing a huge network out and wondering what's there, but there are mathematical tools, there are abstractions. You look for patterns and structure and clustering and the degree distributions and so on and so forth. So that is there in my other book, the book that I posted online the network book which is still in the works that chapter so yeah it is there and of course you should read the network X documentation so read the network X documentation NetworkX documentation. Also, Barabasi's book, The Network Science, by the great researcher Barabasi is completely online, is online in a beautiful format. The book is also available in print. Needless to say, the print is one of the most beautifully printed books. It's an amazing book. If you guys get a chance or if you can, especially if your employer is part of your employment, you have a budget for books, you should certainly buy it.'s just it's not expensive I believe it's under $100 50 $60 or something so anyway so we can draw the graph now the question is can we draw this graph and see structures in it so I will go back now to sharing the previous screen in the code. So here you notice that how did I create this network? I created an empty graph and I added all the edges from where? From this pair that I created of edges, isn't it? Then when I print information about this network it tells me something. There are 57,000 authors. Amongst them there are 178 000 edges interestingly the average degree of a node is six uh what do you think the average degree means of collaborations each author has yes how many collaborators on average does an author have right how many authors i mean does an author have? How many authors? I mean, they may not all collaborate on the same paper, but typically an author has collaborated with about six other authors, six other researchers in general, six to seven. Now I started it to draw this- Asif, I have one question here. So if it's average degree is like the number of collaborations so shouldn't it be a whole number or like or no no it's a decimal because you know it's a fact it is fractioned over the whole it's just a number imagine that suppose you come up with um each one author has three collaborators that has four collaborators and suppose these are the only two i mean suppose they're taking the average of these two you'll end up with 3.5 collaborators okay so well that is it now i wish this nx.draw had finished by the time I was showing you guys. I'll post this code. Hopefully you run it, go to sleep, wake up in the morning, the graph will be drawn. Generally this graph of NetworkX is a bit slow in drawing graphs, really slow actually. For what it has worked, it takes a long time. It didn't finish and it should finish, goodness me. It was queued at 12.01. It's already a long time it didn't finish and it should finish god knows when it was queued at 1201 it's already 50 minutes and it hasn't finished at some point it'll finish so in other words network x is very easy to learn it doesn't scale very much to large networks so one easy thing you can do instead of just sleeping over it is instead of taking the entire graph you can take the graph of the thousand most or maybe 100 most prolific writers. So you can say, I will take only 100 writers and I will look at their graphs, the collaboration network amongst them. So that will be easy to draw. So you can take that as a challenge. How would you draw the collaboration? So let me write write it down. So this would be something worth, actually this sentence is too long, I'll leave this as an exercise for you. You can do that. Instead of drawing this big graph, you can draw that out. Now, what else can we do do this is just a starter code guys you can get the citation of each paper each paper cites some other papers and it is cited or referred to by some paper so you can create citation networks do you see that guys a sites b so those could be directed edges D. So those could be directed edges. You could create that network and see, then you will find that who are the authors or which are the research papers, which are seminal in the field, which have extremely high degree of citations associated with them. You can find in terms, and do a page rank of the graph. You see page rank algorithm is nothing but a graph algorithm. You realize that it is a graph of web pages referring to each other. And I invite you to look up the page rank and apply it to this graph and see what comes out. Are we together guys? comes out. Are we together guys? To find the most influential authors or research papers and so forth. So that is your project. This is a project that I'm hoping that you guys will do. So do you guys feel that this is a good enough starter? Yes. Yes. Can you go line 19? Line 19. 14, 15, 16, 17, 18, 19. There we go. Sanyam Bhutaniyya, You have collaboration is equal. Can you explain this code or you're calling the Sanyam Bhutaniyya, Calling the constructor of the graph. Sanyam Bhutaniyya, Okay. Sanyam Bhutaniyya, That's in in the next lab. Sanyam Bhutaniyya, That is right. That's a network. Sanyam Bhutaniyya, Sorry, that's funny. That's new to me. Sajeevan G. I also one question like on this line. He he. Yeah, so I didn't quite follow that logic for the X in author list and for why not list effects. Sajeevan G. Or let me draw an explain it to you. One second. See, here's the thing there's a archive data archive so we created a data frame like pandas data frame let me start with scratch in DataFrame called archive, isn't it? Archive, DataFrame, contains amongst other things, a column call. So there's an ID column, there is a title column, and there are other columns, and then it has author's column. Isn't it? Now authors column for any given entry, let's say two or something like that. It will contain a list of, let us say, A, B. Let me take this. So it turns out authors A, B, C, D may have collaborated or C may have. Actually, because it can confuse with the diagram above, I don't want to. Let me use some other language here. PQRST. PQRST. So let us say this paper was written by five authors. These are all strings. I'm not putting quotes in there. Maybe I should. This is the value sitting here in the author's column. And there are more columns. We will, we don't, we won't care about that. In this column it is it. Now what do you want to do? You want to do this for every, for this entire data frame. So how would you take the entire data frame? Archive. A R X I V dot authors. If you do this, what do you get? You get this column. You get this column. This is. Yes. That's a good thing. Right. What is it? It is essentially a series, basically an array, has an array, right, array of, and each of these is itself a list, isn't it? A list of authors. So array means each of the row is an array, array of rows. Let me just call it author rows. It is basically that. Now what do you want to do? You want to take for each row that is for each of these so you would say for author list in this this part is is now easy to understand in other words you want to take each of these things, items, and break it up into pairs, right? You want to make pairs. What do you want to do out of this? P, Q, Q, R, P, R, you know, S, T, P, T. You want to create all of these pairs. Am I making sense? So how would you do that you would say for every element for X in this author list so you take X and you take Y some other element Y in also the same list what do you want to do you want to make a couple you want to make a couple XY right so that you will get PQ and so on and so forth are we together guys yes yeah so X and Y in the list but except that you don't make, you don't want to make a pair of an element with itself, right? You don't want to create a PP, isn't it? And if you have created PQ, you don't want to create QP, right? So you want to avoid this and avoid this. Unique. this unique isn't it so one easy way to do that is if you put a condition if a p is greater than oh sorry x is greater than y what happens is x and y are strings right because x and y are strings isn't it these are p q r s t right these strings these are autonyms so when will x be greater than y it will have to fulfill both this condition a p it is not itself and secondly uh in in terms of alphabetical order right so for example a peter will come after adam is greater than adam why because a b c d the alphabet the alphabet, alphabetic order for strings. This is one of the tricks in the lang in programming both Java and Python support that, that you can compare strings. When you compare strings the one that comes later, whose first letter comes later in the dictionary is greater than the previous one. So you get the connection guys, isn't it? Yes. So if X is greater than Y, then what do you do? Suppose you have the pairs, pairs is equal to, let us say a list of maybe set of pairs. What did I do? I don't remember if it was a set. I think it was a set of pairs, so then you would say pairs dot add what do you want to add the x y so do you realize how these three four loops came through and this if condition came to now yes yes thank you so much for explaining yes so all of these things first think it through in your head and after that when you write code you will realize that if you have thought it through carefully the code will be very very simple you know now when you go back and look at the code it is actually pretty neat and simple you could do it in a more complicated way actually there are more elegant ways of doing it i did not do it necessarily in the most elegant way there are other ways also but it is certainly uh hopefully one of the simpler ways of doing it i think this is basically an upper triangular matrix with off diagonal elements absolutely you got it right and so after that i have all all the pairs. What are those pairs? Those are the edges in my graph. Authors are the nodes and the pairs, the collaborations are the edges. Therefore, I can draw a graph, which is what I'm doing in the next line. And that is it guys. I have one more question. Yeah. This is regarding like the original author list that you got like was it a dictionary or was it a list I sorry I mean I couldn't understand that original data Nancy okay let's go through this how the data comes so let's review what we did so besides this preamble and including the libraries, we just loaded it as a JSON file. When we did that, if you look at this description, there is only one field author. It's a misnomer because the author field contains actually a string of all the authors. So when I sample into it, you realize that it's a mess. Do you see how this is a messy string? But it is not a dictionary, right? Because I see like a name, colon. Yes, yes, yes. Initially, in fact, my first thought was that it is a dictionary. I inspected it and realized that unfortunately, it's not a dictionary, it's just a string. Okay. Colon separated string. See, I could have read it as pure JSON, but I read it as a Pandas. But when you read it as Pandas, it doesn't go any deeper. So maybe if I had read it as a JSON directly, I may have found it easier to get to the structure. So there are many Jay Shah, Dr. Anand Oswal, Jr.: So there are many ways in which you can address it. I addressed it this way. Then what I did is I just extracted the names. So now you notice me doing an extraction and creating a new field called artists, where the value is a list. Jay Shah, Dr. Anand Oswal, Jr.: Is a proper list of names. Do you see here list of names? Isn't it? Actually, let me do one thing just to illustrate this This is very wide. Let me just take This Sam So we can see it in all its glory So we can see it in all its glory. There we go. Let's try this. Oh, it can't run because this other project is running. Okay, let me go stop the other one. Oh goodness. All right, let me kill the kernel. Restart. So I can show you. all right if we kill the kernel restart so I can show you is it restarting yes so let's go from the beginning and run it this doesn't need to be run this is straightforward I'll keep running it yeah so where was I basic okay so we'll continue to run so here we are dropping the column we don't need. Yes. So do you notice that now we have it in the right way that we guys? The author's field is comprised of a comma separated list. Yes, yes. You elaborate. That is neat. And it is a list, not a string. It's just a list, proper data structure. So what do I need to do now is we can derive some basic statistics from it. We can do some fun stuff. We can do this. We can find the way. Typically this will run fairly fast. 50 most published authors are here right now comes the finding the collaboration networks and what you can do is insert above let's see you could print it out you know pairs let us see what it looks like. What does it look like guys? It's just a couple between two authors. They are the edges. If you treat an author as a node, these are the edges in the node. Make sense? That is it. I will delete this node. And then comes creating this graph. Creating the graph also is reasonably fast, but drawing this graph will take a long time, so I won't go into it at this moment. The node types are not as easy as the node types as strings. So this is better. And then this on my machine will take some time to run. So guys, that's it. Get started with your project, make some progress. I hope this is enough to get started. What I haven't done are very easy NLP things. Which tags show up most often? What are the most common tags? What words show up often? Do basic NLTK. Do that. So do three kinds of analysis, tabular data analysis, network analysis, and textual analysis. Three different dimensions. And then you can build sort of all sorts of models and so forth out of it there is id tags you know the tag terms you can also look at the tag terms all right guys so with that i will stop today's discussion.