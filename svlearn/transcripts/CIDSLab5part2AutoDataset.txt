 All right guys, so we'll do another data set. This is an interesting data set. So you have to assume that you don't have real world experience with cars yet, right? So imagine that you're an alien and you have come to this planet. And you observe that people tend to move around in these automobiles. And these automobiles are characterized by their horsepower, their weight, and they have these engines. Engines have displacement. What is the displacement of an engine? The CC rating that you provide? Now, what happens is that think of our engine cylinder, the cylinder of our engine is like this. Yeah. So what happens is that this thing, this part, it increases and decreases because inside there is a piston. And what the piston does is it pushes its way in so that the available air gap, the space available or the volume available decreases. Think of it as an injection, the way you do it in an injection. So you compress, the piston goes into the cylinder, the available space, you spray a mixture, atomized mixture of gasoline and air. And once that is saturated, that mixture saturates that volume, the spark plug, you have a spark. And what the spark does, it ignites that atomized mixture of air and gasoline. And it literally explodes. There's a fire it burns there's an exothermic reaction exothermic reaction creates temperature temperature creates pressure pressure pushes the piston back right yeah so the piston goes all the way back but when it goes back it doesn't just go back behind it at least in the older design there was a cam crankshaft right behind it, at least in the older design, there was a camshaft crankshaft, right? So what it would do is it had a camshaft design. It was a wheel there. Basically there's a way to convert this backward motion into a rotation. And so it causes a rotation of something, right? And that rotation forces the cylinder back because here, what has happened is once the exothermic reaction is over and the fuel is consumed, back because here what has happened is once the exothermic reaction is over and the fuel is consumed now this can push back because there's no nothing creating the pressure and so forth. So this can go back because the port opens, which lets the exhaust gases to leave once the combustion is over. So now this piston can go back because it will force all the air out of the chamber, ignition chamber. And this piston goes back. And then the cycle repeats as it goes back, more atomized fuel mixture comes into the chamber of the cylinder and ignites. And this thing again goes back. So you have this piston going back and forth, the camshaft, and it's causing a rotation, right? And that rotational motion is further propagated. And that is basically your internal a rotation, right? And that rotational motion is further propagated. And that is basically your internal combustion engine, right? Or very similar to the similarly a diesel engine, et cetera, except that you don't need sparking there. So that's how these fossil fuel engines work basically. So now what are the criterion? You're trying to determine how much miles per gallon of fuel can you get from an automobile? That's the target variable. Now, what are the features that matter? Common sense says that the sheer weight of the car or the automobile matters. if you bring a Humvee, which is I believe five or six tons, then the amount of mileage you expect would be different from the mileage that you would expect from a compact Prius. Right. So that way. And then another factor that matters is horsepower. Horsepower is like how much horsepower does the engine manifest in terms of its pulling weight, how much can it pull, right? And horsepower literally started with mean, we run cars of 500, 600 horsepower. It is, we never really think about it. It's fun to drive those cars. But if you really think about it, 600 horsepower is 600 horses pulling just you, right? The army of horses pulling just one person. So obviously these are very powerful engines here in the US. Then the other thing that matters is cylinders. So how many of these chambers or cylinders act in collaboration or collusion to cause those rotation, right? The primary rotations. So the more the engine, the better. In US, you talk of the circle, V engines are the most popular for the last 100 years. So the V-shaped engines have a body in which the cylinders are along the V, on the tip of the V in a line. There's a line of cylinders. So you can see, open your car and see that. So we have V4, which is the default. V6 is supposed to be for more powerful engines. And then if you really need a muscle car then you go v8 and if if you really have no consideration for money you can go to v12 or something like that there are such engines so number of uh cylinders matters the the v2 v4 v6 v8 represents the number of cylinders so the cylinders are there the weight of the car matters, the horsepower matters, and then the word acceleration matters. Acceleration, when we talk of a car's acceleration, we talk of it like this. In US, you talk of it in an interesting way. You say, how many seconds does it take to reach 60 miles an hour right so if it takes 10 seconds or 15 seconds it must be a very very weak car right if it can do 60 miles an hour in three four seconds it's a mighty car right and so the small so the acceleration is a very peculiar in this data set it's a you have to remember that it is not in the literal sense a physicist would say the stronger the acceleration the more powerful the car but here the way we speak of in acceleration in the car industry is how much time does it take to reach 60 miles an hour right so acceleration therefore big is big is weak car small acceleration is powerful car that is one thing about this data set you should remember so with that all of that there this data set was collected a few years ago by looking at automobiles of three countries that countries is given by the origin there's a feature here called the origin uh you will see do you notice that origin of the data the data set you see origin so one is zoom in a little bit sure yeah and also the initial graphic um that describes the issue. Oh, yes, yes. I'm not seeing that. This is, by the way, literally question eight and question nine of your ISLR textbook. Okay, it's in the textbook. Yeah, it is quite literally in the textbook. Let me, ISLR, okay, yeah, if you did. Okay. Yeah, because that PNG is missing. Ah, okay, I should upload that. I apologize. So it is literally question eight and question nine of chapter three of your textbook. So so this is how the data looks. If you sample the data, you notice that miles per gallon, you can imagine that this is. Would you say these are impressive miles per gallon by modern standards probably not most cars don't give uh give much higher mileage people are shooting for a 40 50 60 these days, as miles per gallon. Especially the four cylinder engines easily give you these days 50, 60 miles a gallon, especially if they're hybrid. And the six, the eight cylinder engines, of course, it is 15 miles a gallon. Do you think that is still reasonable or that's low or high? I think it's a good yeah yeah actually that's what my that is what my car gives me my car still gives me 15 miles a gallon it's a V8 right so displacement now you see displacement is literally a measure of the size of the cylinder because it is how far back the piston goes back and forth. What is the displacement of the piston? And that, of course, would be a measure of the size of this can or the cylinder, right? That's what it is. Number of cylinders and think of it as sort of the size of the cylinder. Horsepower. How many horsepowers is it manifesting? The sheer weight of the automobile, right? How much automobile there is. Now remember, acceleration is how many seconds it takes to reach 60 miles an hour. Obviously in those days, this used to be the amount of seconds it used to take. Today, how long it takes? Much, much less. The engines are far more efficient and powerful right and this is a display yeah see the same sample there's some statistics here miles per gallon the mean is 23 and standard deviation is about eight. And maximum mileage used to get is 46.6 and the minimum you get. Now, one more thing here, displacement, everything. Year, the minimum year is 1970 and the maximum year is 1982, right? So it is a study done over 12 years, over 400, about 397 data sets. Now origin. Origin minimum and maximum values are three. You can see that there are only three possible values, one to three. Now if you read the documentation behind this data set, go back to the original paper, it mentions that one stands for USA, two stands for Europe, and three stands for Japan. Japanese make, European make automobiles, I mean American made automobiles, European made automobiles and Japanese made automobiles. So let us do the basic exploratory data analysis. The first thing you realize, it takes a while to realize that something is wrong with horsepower. Do you notice that all of these mean, medians, everything is missing? Why would it be missing? You expect name. You see the name, the name Ford Pinto. You can't have the, it is a categorical variable. You can't have mean, et cetera, for categorical. It makes sense. But what's wrong with horsepower? If you look at horsepower, you suspect something is bad. And so you can check maybe some values are not numeric. And when you do that, you find that yes, truly. Apologies. It's just my allergies. So X for x in unique values if you make so what happens is there are about six data points that have question marks you can check right uh six or seven i don't know which exact number so they are missing values there what we will do for now is just delete the missing values right so we do some data in the beginning. This is real life data now. You have to deal with the fact that some things will be missing, some things will not be okay. So what I've done is I've converted horsepower, wherever it's missing, I've deleted the row altogether. And then after that, I've converted those values to numbers because horsepower should be a number, isn't it? Right? It should be a number isn't it right it should be a number i should i've converted it to numbers and origin which is a number i've done the reverse you know that these origins are these are not numeric two is not greater than one two is just a representation for europe one is representation for u.s made vehicles three is for japanese made vehicles so i've converted horsepower to numbers and origins to string. This is basic data manipulation to clean it out. Also, I realized that if you're trying to predict the name of the mileage of a car, do you think name is a good predictor? No, because if mileages could be changed by name, most manufacturers would go about finding the optimal name to improve mileage right it wouldn't it doesn't make sense so that is that so i just dropped name forget name after that i noticed the data looks clean missing number analysis shows data is clean info everything is as it should be miles per gallon is a float is a numeric number of cylinders is integer as it should be a weight for some reason is mentioned as an integer i could have bothered to convert it to float i didn't bother right the year of year the car was made is an integer this is it so now let's go one of the things i didn't do is this profile because it takes a while to run you can uncomment and run it on your local environment so now i take this and data visualization you visualize the data right so let's look at it case by case uh we are looking at miles per gallon is this data symmetric no it is skewed isn't it and it is skewed. So we need to worry about that. Number of cylinders, well, you don't get cars with odd number of cylinders. There seems to be some data point which is unusual, but be as it may. Most cars are, are they what are they? Most cars are V8s or what are they? Four cylinders. Six is in those days was not so common v8 was very common four of course the situation has reversed today we get a lot of four cylinder and six cylinder cars v8s are rare fairly rare actually it's hard to get a v8 a bit of personal thing. I actually grew up with V8s right from the beginning. I used to love sports cars. And of course, now I'm in my 50s, so it's a bit of an anomaly. But because my hand and my reflexes are tuned to a V8, as I grew older, I don't drive very fast. But because my hand and my complete sense of braking and everything is attached to a V8 engine, I still drive a V8, which looks sort of anomalous because here is a person going in a V8 and driving in a sedate way rather than accelerating like a young man, right? But so it is what it is. Displacement, how much displacement? So most cars have small displacement, but yes, data does have a skew here. Then we look at horsepower. Again, most cars don't have that much horsepower in those days. Wow. Most cars didn't even have, most of them were 100 horsepower or low. That's all, that's, world has changed. Weight, again, a right-square distribution. Acceleration seems, roughly speaking, evenly distributed, isn't it? Acceleration is this. And the year of the car, much cars were made in each year then i count the number of cars by origin by country of origin it turns out that us is the biggest maker of cars number three was japan japan is makes also a lot of cars and europe makes the least number of cars just a little bit behind japan but us is of course the is the land of cars and Europe makes the least number of cars just a little bit behind Japan but US is of course that is the land of cars right it's always been the land of a lot of cars let us plot this this data now this plot is interesting and there is a lot going on here in this plot I want to show you what it is so I will just because it's a blown up the screen this is a big plot so let me just for one moment shrink the size and show all in one even then it's not okay of course you have to squint to see this but you notice that this is a pretty busy plot about six seven one two three four five six seven eight eight variables plotted the way i've done it is and the way this plot works is called pair plots the diagonal is the frequency plots histogram right Then these ones are scatter top half, the top above the diagonal are the scatter plots between two features. Below the diagonals are the this kernel density plots so that we have everything now. So now let me just blow it up and show a few examples to see what it means. So look at this. This is the histogram. The way to look at it is histogram, miles per gallon versus what? Miles per gallon. This is the y, this is the x-axis. So intersection here is miles per gallon. This is the histogram we just saw a little while ago for MPG. This is exactly the same as this MPG, right? Miles per gallon thing, MPG. This is it. Now let's look at this row. This row is miles per gallon is along the y-axis and the x-axis is cylinders. Of course, you have to scroll down to see the word cylinders here. So this is the cylinder, right? I'll make it even bigger. Yeah, cylinders. So'll make it even bigger. Yeah, cylinders. So what does it say? As the number of cylinders increases, does miles per gallon improve or decrease? Decreases. Decreases, right? You notice that with V8 engines or the eight engine, eight cylinder engines, the miles per gallons is pretty low whereas for four cylinder engine the miles per gallon in general the middle is somewhere here pretty high the next feature is displacement here the x-axis is displacement how would i know it i can scroll down and see yes it is displacement other thing the way i use it is i just look at the order here third is displacement so here miles per gallon goes up or decreases with displacement what would you say guys decreasing decreases but is the decrease linear or what does it look like it's a non-linear yeah yeah yeah likewise the relationship of miles per gallon with horsepower is like this right which is sort of disappointing wouldn't it be a teenager's dream that mileage improved with horsepower mileage improved with horsepower. It's one of the disappointments. The moment you run big cars, sports cars, the whole thing just, it just guzzles up the gasoline. And it makes you feel guilty as well as disappointed, but it does, that's what it is actually. So the next one actually, I have a little thing. My daughters, I gave them pretty much the similar cars to mine because they liked it. So they were very excited in the beginning. Then gradually they went to college and I'd given them fixed pocket money to fill the gas tank. You can imagine the disappointment. They'll curse you. What's that? They'll curse you for giving them . Because they say, this thing just guzzles up the gas. They can't eat in the pocket money. They can't eat everything up. Yes, exactly. So now they came and negotiated that i'll pay for the gas and they will use they will not pay from their pocket money so this is a lesson that you see in this graph next is uh weight so what is the relationship of miles per gallon to weight shouldn't you have seen a reciprocal relationship? The more the weight, the less the miles per gallon. Why is there no clear relationship? It could be different to the materials used, plastic or steel. No, this is weight. It is weight. It is a reciprocal relationship. It is weight, it is a reciprocal relationship. It is the inertia. Yeah, the fourth is acceleration. Why does acceleration not have anything to do clearly with miles per gallon? Do you notice that the relationship is not so clear? You would imagine that the bigger the acceleration is the longer it took to reach 60 miles an hour, the more the mileage, because you know, weak cars, they take a long time to accelerate to 60 miles an hour and they give you perfectly good mileage but you don't see such a good mileage why is that i will leave that as an exercise the answer is quite simple but i leave it as a food for thought i should or should you want me to answer that? Maybe the newer cars that got better engines that they are more efficient. This is 182, right? So 12 years of difference. So that's a mixed number of efficiencies there. Why is acceleration and miles per gallon not having a very pronounced and simple relationship the more the acceleration the longer it takes to reach 60 miles an hour the weaker the car the you would imagine that the more the miles per gallon no it's also functions on the size of the car exactly so the size what happens is sometimes acceleration longer acceleration is not just because of a weak engine. Longer acceleration is also because you're sitting on a big massive engine. For example, most buses are heavy, right? And they have pretty weak acceleration, they take a long time to reach 60 miles an hour. Right? But so that is the explanation. The year also matters. I think one of you said year. Year is also a factor that interplays. The next one after acceleration is literally the year. So this is year versus miles per gallon. So I'll just blow it up. What do you think guys? Is the my is the are the cars improving with respect to their mileage fuel efficiency or getting worse with years? Better than us because we're come again, it'd be better as the year goes by because advancements come up. Yes, exactly. If you draw a line through this, you can literally feel that, yes, slowly, steadily, slow and steady improvement in mileage with passing years. Cars are getting fuel efficient in the 90s, of course, by 82. And then, of course, in the 90s, people discovered 82 and then of course in the 90s people discovered suvs and the whole situation got worse okay so it's never linear anyway now you look at the country of origin the last one is the origin i believe one quick question yeah in some of them, they have clusters broadly identified. But in some, they're not. It looks like Matelot-Levitt is using some kind of a deterministic. No, no, no, no, no. Here is a discrete integer variable, no? Okay. So it takes integer gems, one, two, three. Likewise, the country of origin, one, two, three. So you see that mileage of American cars are terrible. Mileage of the Japanese cars are best. Mileage of the German, European cars are slightly worse. Look at road two, first power. Because of the black lines. First power. First power, that's a between... You mean this one? Row two. Row two, first column. Oh, this is something different. I'll explain that. So I'm still interpreting the first row. So in the first row, origin, the number one stands for American cars. They always traditionally had bad MP miles per gallon. Japanese had better. Europeans were comparable to Japanese. This is by the way, this fact unfortunately is still true. You probably know that the same car manufacturer will make exactly the same car, but in Europe will give much better mileage than in US. So some of the fuel efficiency components, because they are expensive, they will not get installed in us right because the laws here are not as strict as in europe so the same car will give much better efficiency there can't do much about it it's a fact of life you have to get the laws changed california is doing its part but let see. Now let's look at this guy. Look at this second row now. This car is in, this plot is interesting. It's below the diagonal. Do you see that? These are called kernel density plots. It plots, well, we talked about kernel density plots. They plot contours of equal value, right? And what they tell you is, well, okay, they say a lot of things which I don't want to talk about until we get there, but basically they show you that how is the data distributed? So clearly data is distributed around six-cylinder, four-cylinder, and eight-cylinder. Do you see these bands of data, right? And it is a cylinder versus a miles per gallon and you can clearly see that four cylinders gives you the center of it is you know more to the right than the center of the six cylinder than the center of the eight cylinder what does that mean the average miles per gallon from a four cylinder engine is far more than from a six-cylinder is far more than from an eight-cylinder engine. You see that, right? So that's what you get from this density plot. So this is the way you interpret it, guys. I'll let you interpret. The most important line was miles per gallon and its relationship to each other. But then I'll show you something quite interesting. and its relationship to each other. But then I'll show you something quite interesting. Look at this plot and look at, for example, displacement. What is this guy? Let's look at this guy. This is displacement versus what? One, two, three, four, five. One, two, three, four, five. Displacement versus weight is this. This plot. Can you guys see where my mouse is what would you say looking at this graph what is the correlation between the two displacement versus weight positive very very strong correlation right you can see a strong positive correlation you also see a strong positive correlation here you see a strong positive correlation here. You see a strong positive correlation here, right? So while correlation of a predictor to response is a good thing, correlation between the predictors for linear models is not a good thing. It leads to a pathology which is called collinearity. What it means, I'll give you the intuition. We'll talk more about it. The intuition is, you know, if two variables, they are giving you the same information, right? So how should I say it? Let me make it a joke. See guys, we come from the software industry where we are supposed to work often in pairs or groups, right? Have you noticed that once some work is done, the first person who goes and talks about it pretty much steals the thunder of everyone else, right? If he doesn't give the credit or she doesn't give the credit to the entire team, everybody will think this guy did great work. The next guy who comes and explains it has almost nothing new to add. You know the phenomenon, right? It's literally somebody has stolen your thunder. They're going to say something very impressive, but somebody said it already, right? That is it. So when two things are highly correlated, right? For example, you look at two variables. One is the height of a hill above sea level. And the other is the height of the hill from this valley below, from Fremont. You would agree that the two variables will be very correlated for the for the regional hills behind a hub behind support vectors right next to this mission peak that is there and all these hills around you you would imagine that these two variables will be highly correlated and if you think about it you don't need both you just need one because one is as good as the other height above sea level and height above Fremont. The only difference will be the height of Fremont above sea level. Do you get that, guys? Does that make sense, guys? So if you have two features, one is height above sea level, one is height above Fremont, you would say that they are very, very correlated up to some noise factor. And common sense says we don't need both. So what happens is that it shows you the redundancy of information in your predictors. And for linear models, that sort of redundancy is not good. That highly high correlation between them is not good and there is a word for it it is called collinearity or more formally multi-collinearity what it does it screws up the the machine learning model it causes and there are many things it causes let me not steal the thunder of tuesday's session but it basically causes problems so you have to watch out. When you see that, you should eliminate, you should pick amongst the highly correlated factors, one factor, and drop the other ones. So that is that. So now looking at this, we make some more plots to see the distribution of the data. These are called violin plots. We see the data distribution of each of the variables and I've separated it out by origin. So you can see miles per gallon, where most American cars used to give about 10-15 miles per gallon. The Europeans used to give 25 miles a gallon. The Japanese used to give, oh God, they already were giving 30s gallon. So these are violent plots I mentioned it to you. They're common, they're pretty, they're useful and an alternative to them are the box plots. These are called the box, oh, I shouldn't say violent plot. I should change it to box and whisker box box plot of the features this is the box plot so in the box plot this is the middle value median value this is the way the data is distributed the way the data is distributed, the sort of average data is distributed. The whiskers represent the, I think, the quantile limits, Q, right? And then these are the outliers. So these are the standard box and whiskers that you use from college. Then you look at the correlation between the data. I'll slightly decrease the font to show the correlations. So when you look at this correlation table guys do you notice that miles per gallon is correlated to number of cylinders negatively the more the number of cylinders the more the less will the miles per gallon be does this agree with common sense it does displacement also the bigger the, the worse the miles per gallon. More powerful a machine you have, heavier the machine you have, less the miles per gallon. And the more the acceleration, the more the miles per gallon. But by acceleration, the meaning is inverse. How long it takes to reach 60 miles an hour right and also it has a positive correlation to years so the more recent the year the better the mileage does this all agree with common sense guys first row yes but now comes trouble in paradise let's look at any one thing let's look at the weight weight okay mpg is fine but you notice that it is highly correlated with cylinders what does it mean heavy cars tend to have big more cylinders which is common sense big heavy cars they tend to be v6 v8 cars, they tend to be v6 v8 displacement even more 93% correlation horsepower, again, a lot of correlation. So what does that mean guys, that if I take weight, maybe, maybe I can forget about a couple of these variables. These are highly correlated variables, at least these two i can forget about maybe keep horse power may not may not keep horse but this is close to nine anything like 90 you don't get much value by keeping those variables right so let's do that first what we will do and then this is by the way same correlation plot visualized. Very negatives are black and very positives are light and so forth. So the same information visually displayed. So now let's do one thing. Your book talks about doing analysis. What's the time now? Oh, it's five o'clock already so actually we are still an hour or so away from the thing are you guys open to a long session or should we finish at 5 30 and take it up next week i need to head out in a few months so maybe i can drop and see the recording let me finish at least the auto data set and we'll keep California for next week. Dr. G R Narsimha Rao, Ph.D.: The first part of the question question agent your textbook says do a univariate linear regression treat horsepower as the input miles per gallon as the output and see what happens, so we do that this is the plot of horsepower two miles per gallon. Dr. G R Narsimha Rao, Ph. What do you see, guys? Does it look like a linear relationship to you? No. It looks like a slide. Yeah, it looks like a children's slide. Yeah, that's a very good observation. Looks like a children's slide. So what can we do? My intuition, and whenever I see this, of course, this is the habit of years. If you ask any physicists, the very first thing they will tell you is, hey, this looks like one over X to the N graph. I actually mentioned this to engineers, and I was surprised that most engineers don't think like this. They don't think immediately a reciprocal power law, a graph. They just say, okay, it doesn't look linear. But it is, it is. And the only question that remains is what is n? So if this relationship is like that, what is the simplest way you can make it into a linear relationship? How can you linearize it? Old, old trick, just take the log of both sides. If you take a log of both sides, do you see that this relationship now becomes linear y is not linear in x but log y is linear in log log x isn't it so if you think of new variables y prime which is the log of y and x prime which is the log of x you see that y prime is proportional to x do you see these guys it is obvious anyway so what we do is we can verify my intuition you can take the log of both the sides and what does it do would you call this essentially a linear graph now yes okay this by the way it's a very common thing. I have seen so many people struggle with data analysis. This relationship, inverse relationship will be staring them in the face, and they won't spot it. And I hope you are not one of those. You will learn to recognize the inverse relationships and immediately know that when you see inverse relationships, you should take the log, right? Whenever you see what Kate nicely said is a slide, the shape of a slide, immediately take the log of both sides, because that will make into a linear relationship. And so we can do that. If you don't do that, if you do the regression in the original variables, you will get some intercept, something, you get the prediction on the test data. What is your R squared? 56.5%, right? 56.6%, let's say, is your R squared. Your residual analysis, what does it show? Does it show homoscedasticity? Yeah, there's a lot of pattern. There is a lot of pattern. And there's a very clear increase in the variance. So there is a distinct presence of heteroscedasticity here. Then here is one more plot that is useful. A predict the, if Y and Y hat are completely matched, then they would be along a 45 degree axis. But actually when you plot the Y and Y hat and you look at the best fit line, you notice that it is tilted, right? It hasn't gotten their relationship well. In the lower part, Y hat is an overestimation. And after in the right-hand part, like after a little bit, it is a underestimation. So somewhere systematically, you see the bias here. It is overestimating here, it's underestimating here, right? So when you visualize the model's prediction, It is overestimating here, it's underestimating here. Right? So when you visualize the model's prediction, you can see it distinctly. Do you see this? Very distinct thing, shape. Would you call this line a good fit to the model? There's still slide shape in the data. Yeah, you can you can. All it shows you is maybe you could go with it, but there is a scope for improvement. Right. There is a scope for improvement. So now let's try the log transform the way I mentioned, take the log of these two. When you take the log, repeat the analysis. What is it? Your R square went to 72 percent right 71.8 so from 55 56 you go to 72 would you consider that a huge improvement definitely yeah in business in real life that would be considered a huge huge improvement in in terms of real value right so it's a much better thing how about the residual plots does it look better with respect to the patterns in terms of real value, right? So it's a much better thing. How about the residual plots? Does it look better with respect to the patterns? Yeah. Very much. Yeah, patterns have not completely gone. There's still a distinct slight noticeable there, but much, much improved. And what about the prediction line and versus the best fit line? You notice that these are much closer now, right? It is not so far apart as here where they have like huge gaps here. The gaps have narrowed, right? So this is, and then when you visualize the model prediction, because now you have taken the log transform, would you say that the model and the data are in good agreement, the prediction and the data are in good agreement? Yeah, definitely. And already you have 72% this R squared. So you think they've improved. So guys, this is the power of bringing intuition, physical intuition into a problem. If there is one thing I can say, learn mathematics and learn to apply mathematics. I can't say that enough. I mean, if I have to quantify what I have learned in life over 50 years, or more actually, much more than 50 years, let's say, without giving my exact age, let's say five decades, what have I learned through education? One deep lesson I learned is know your math really well. Not from the point of exams and passing and getting good grades and things like that. Really have a deep feeling for math. Just as you can speak your native tongue, speak in the language of math. And wherever you see things, see that the thing, can you interpret it, reality in math and see what is going on. If you do that, you will actually do very well. So one point I was going, and this again is drifting away from objective and going into my subjective experience in this field see i did this i did the log transform but suppose you had they said no no wait what about the fact that we already have a power transform box cox and a human uh johnson transform what if we use that wouldn't it give you the same results no a blind use of a power transform. You knew that a power transform is called for. Why did you know that? Because both horsepower and MPG had right skew. See, look at that. Miles per gallon has a right skew and horsepower has a right skew. Do you see this, guys? Maybe more evident here. Miles per gallon has a right skew and where is horsepower? Horsepower has a right skew. Therefore, you could have said, yeah, let's apply a power transform. Let's find the right relationship and all of that. What happens if you blindly use the tool, which is what people would do? So the general reaction of people is, yeah, data is nonlinear, so I need to apply a power transform. Yes, you can apply it. When you apply the power transform, I make a pipeline of the power transform and linear regression. What happens now? Do you notice that your R square has gone up by six, gone up from 55 to 64%. That is a huge improvement, isn't it? But is it the best model so far? No, no, no. Just by using our judgment and hand thinking that we need a log transform, we got 72%. 64 is a far cry from 72, which again, I wanted to bring home the point that guys think about data, make friends with data, you know, know the data so that you can do it. The next question is, what if I tried polynomial regression? Won't polynomial regression give me the bends I need? Once again, if you do it, what happens? It is close to, it goes to 60, 6%. Nothing close to 72%, isn't it? Do you see how hard it is to beat well thought out models? If you can see things in the model, and you can recognize it and do your own transformations by hand, nothing will beat that. No automated blind application of tools will help that. So that ends the one dimensional or one variable regression. Now the problem number nine is to do a full analysis using all the predictors. So let us use all the predictors now there is a problem one of the predictors is origin which is like us europe and japan one two three how do you how do you deal with that so what you have to do is dummy encode it you have to convert it into numbers categories into numbers one way of doing it is to it's called one hot encoding you say you create three features origin one origin two origin three actually i should have named them origin us origin europe or is in japan but okay i was losing and then if it is a us-made car you say origin one is one it's true and for Japan and for Europe you mark zero if it is a European car you mark origin one is zero origin two is one and origin three is zero you see that right origin Europe is one so that way of doing it is called one hot encoding so partners give you a very simple way to do one hot encoding you can just say get dummies So PANDAS give you a very simple way to do one-hot encoding. You can just say get dummies. And you notice that it got burst out into three variables now, three features. Now I want to tell you something about a most useful regression model overlook. Before you build complicated regression models, right? You see one dimension, we use this null hypothesis, et cetera. In the equivalent, the closest thing to null hypothesis, in fact, practically the null hypothesis in scikit-learn is the dummy regression. What the dummy regression will do is, if you ask it to predict something, it'll tell you, should I predict the mean, median, something? Remember the null hypothesis would just return you the mean right the mean of the target variable do we remember that guys the null hypothesis returns the mean of the target variable in the data set in the training data always is the prediction whatever the input it will always predict that mean because it's the sensible thing because what does the null hypothesis say there is no relationship between target and predictors so the best prediction that you can have with the lowest sum squared error is just to return the mean and so dummy regression is in that same category you can return mean median mode whatever it is but here, just to emphasize the fact that there are multiple choices, I took median for no big reason. I noticed there was a small skew in the data, so I just took median. Null hypothesis has a, what is the coefficient of R square that you should expect from a null hypothesis? By definition, zero. TSS minus T by definition zero tss minus tss is zero right so this is it and you get a lot of mean square data so you should always compare because you know what you should look for is the decrease in mean square data let's keep our eye on that we take a model one no transformations all the predictors in there. Right off the bat, we get just blind utilization, we get 80%. What does it tell us? When we take all the predictors and we apply, what does it tell us? It tells us that horsepower is not the only thing that matters. In the one, only the horsepower based model, there was a lot of irreducible error isn't it that error captured the information there in the other predictors which we had not considered in the previous models now that we are doing multivariate regression we are taking all of those right away it has improved to 80 percent but but when we look at the residuals, of course, what do we notice? We notice, yeah, there's a pattern. There's heteroscedasticity. We move forward. Again, we notice that there is a gap. Not so much gap between prediction and this, but there is still a pattern. So let's take to model two. This time, we remember to standardize the data. We apply the power transform. And here we go. When we do do this do you notice that your mean squared error is continuously decreasing is there your r square has gone to 81.6 better let's then what about the residuals residual still has pattern isn't it some pattern is there despite the power transform but the errors seem to have a more or less a even distribution now. Let's move to another model. Model 3. Model 3, what did I tell you? Log transform, hand-built model, right? Let us go and take the log of weight, MPG and horsepower, these three. Also, we realize that it has a strong correlation with other things, displacement and cylinder. Let us go and drop those variables, right? Because that would cause multicollinearity effects. When we do that, what happens? Look at this R squared, close to 87%, isn't it? Do you see this guys? guys once again the same lesson holds when you carefully handcraft your features by looking at the data and bringing your own judgment on it you always get the best model possible now given these things they i mean you can try to improve upon it you may do it but not by much 87 percent is about as far as you can go right residuals of the learning and do you notice that between the best fit line and the i the 45 degree y axis there's almost no difference you see these guys here wow yeah it's pretty good so clearly we have a vast improvement over the prior models so now let me teach you one more thing it's called the cook's distance it talks about points of high leverage let me explain what these points of high leverage are before we end today see what happens with linear models linear models are the simplest Obviously, we'll deal with other kinds of models. Linear models suffer from three, I mean, many problems, three of which we'll talk about. One is the multicollinearity. We'll talk about it next time. One is outliers. So when you do some squared errors, what happens if that one point is way up there, away from the other points? It will exert undue influence because it's an outlier do you see that right so because the sum squared error of that will be huge and so it will have unusual effect that the loss function optimization will keep on trying to optimize for that outlier and it will pretty much pull the line towards itself so outliers have exaggerated influence right so you may say well no you can change the loss function to mean absolute error do something like that but anyway uh outliers are a problem but the point with outliers is they're visible you can see that they are outliers right and you can do something about it but linear regression models suffer from yet another problem which we don't which is very hard to see it's insidious it's points of high leverage you know it's like these quiet people in the sort of like the team who are invisible but they're they're the puppet puppet masters pulling the strings, sort of like that, metaphorically speaking. In this data itself, there'll be certain points which have undue influence on the training of your model. When you look at this, there are a few points, but not that many. Usually, anything above this line, dotted line, red line, this is called the fence. Points above the fence whose influence is more than the fence, they do have. So how many there are? One, two, three, four, five. Let's leave the small ones actually. One, two, three, four, five, six, seven, eight, nine, 10. About 10 points out of 400. It's okay okay you can either read them out or you can just leave them there it doesn't matter but you don't have as many as to get alarmed with still you do get the impression that maybe uh instead of using linear regression we can try more sophisticated models which are immune to points of high leverage right then comes the last question okay before i take the last question guys i need a two minutes break give me two minutes see feature importance is a very important topic. It addresses the fact that suppose I brought predictors. In real life, in business world, it's a day to day. How important really is this predictor in the model? How much influence does it have? So for example, let's take diabetes. The likelihood that you have diabetes. How much more, whether you take sugary drinks, is it going to make it worse? Like what makes diabetes worse? Assume that you have diabetes. What do you need to watch out for? Do you need to watch out for the amount of sugar that you take in food that matters more or the color of shirt that you wear while eating the food? Which is more important? So would you answer that? How would you answer that? Exactly. The number of the amount of sugar that you take in your food has overwhelming importance relative to a frivolous factor such as what colored shirt you wear while eating that food. But in the absence that you are saying because you have bringing experience to it, when a machine does it and you don't know which features are important, the most important question that you have to ask the model, one of the most important is, which factors were the most important in making the prediction, right? What mattered? Do we get that? What mattered is one of the most important questions you can ask the model. The model has learned something. You want to ask the oracle what did you learn right so that answer is the feature importance question now the thing is if you have standardized the data then in linear regression actually feature importance is very straightforward you can literally see the power of the coefficient. Why? Because if beta 1 is twice beta 2, means if I increase factor, the x1 factor by one unit, the increase in y will be twice that of the increase if I increase x2, isn't it? So suppose x1 and x2 are two factors. x1's coefficient is 2. x2's coefficient is 1. I increase both x1 and x2 by one unit, which will have more effect on the target. x1 or x2? x1, right?? x1, right? Because x1's coefficient is bigger. Whichever factor's coefficient is bigger, for unit increase of that factor, y will respond more. Common sense, isn't it? So you can get feature importance assuming you have scaled the data. That's a big assumption. Assuming you have scaled the data, all you have to do is compare the coefficients. When you compare the coefficients, let us look at the regression model's coefficients. Intercept is three, which we can ignore. Slopes are, these are the coefficients. Coefficients of what? Coefficients of these predictors. So what I do here is I just collate this this belongs to horsepower second one belongs to weight third one belongs to this i just collate it and then i create a map i create a hash map and also i standardize like i look if i'm looking at percentage relevance i have to add up all of those coefficients and divide each one by that so that everything is on a scale of one to 100. So then what I find, this is that the weight matters, 35% of the feature importance goes to weight. The lesson is if you want high mileage, don't drive heavy cars, isn't it? Right? So go ahead when you try to find the relative importance of the feature what do you do with the constant just dropping the beta not doesn't matter intercept doesn't matter they're not giving that any kind of any kind of relevance here so that that is it. So this is it. You say a pie is 100 large. How much does each of the factors matter in that? And so weight, of course, don't drive heavy cars. So can I ask you this question? This is telling you that weight matters. And weight matters quite negatively. The more the heavier the the car the less the mileage so therefore should you start driving very light cars there's a safety factor there exactly there's a safety factor there because if you drive very light cars or in the asymptotic limit you start driving a motorcycle which is very light, what happens? You become basically spare body parts. Right? So putting it harshly, but yeah, so in other words, a miles per gallon is the only thing you ever optimize on. But in this problem, yes. So remember that everything has to be taken uh in perspective of course uh perspective matters so weight of the so for example in my case i tend to be environmentally very conscious or i hope well i suppose we all believe we are environmentally very conscious when somebody looks at us objectively and says you're not i try to do my little bit. I'm a vegetarian. The biggest source of greenhouse gases seems to be the meat and fish industry. So I tend to be vegetarian. I don't go on expensive vacations flying airplane flights long distance. The only flights I take are business-related flights. My work requires me to fly. And whatever other means I can. But there is one place where I don't actually, I look like an eco-terrorist. And that is in the car I drive. As a physicist, I know that when an accident happens and somebody hits you at high speed, weight works in your favor. The momentum, mass times velocity. So if you are in a big vehicle, heavy vehicle, it'll absorb more. And so here we go. So this is true for linear regression, but there is a technique called Shapley values that I would say, it's the last topic of today, I'll talk about. There was a great mathematician, is i think he's still alive or not i think he may be alive shapely shapely have you guys seen this movie a beautiful mind john nash john nash's friend and colleague shapely he got the nobel prize for answering one important question he wrote a very simple equation which when you see it on paper you say well it's obvious almost right but he wrote that it was game theory john nash as you know did game theory of the nash equilibrium points excellent frame you know your game theory right so um did you read the book also? Or did you just see the movie? I haven't read the book. Oh, the book is far superior to the movie. Movie doesn't do justice to the book. And John Nash used to actually live close to the Princeton railway station. And I am pretty sure I'm not 100% sure. I'm pretty sure I've sat through his actual lecture at a time where I didn't know that he was such a great man. Long, long ago, it's ancient history, but I may be wrong. I don't know. So what Shapley answered the question was, suppose, imagine a team player. When you play a cooperative game, as opposed to a zero sum game, a zero sum game is chess, right? The only way one side can win is if the other side loses. The two don't cooperate and try to win together. You see that, right? Chess inherently is a zero sum game. One guy will win and one guy will lose. Those are called zero sum games. But then there are games like, let's say cricket or team sports. In any team sport, let's say cricket, which is most familiar to this audience here, what happens? All of the players, they need to work together to win. Otherwise, the other side will win. Isn't it? So it's a collaborative sport. It's still a zero-sum game between the teams. But within the team, there has to be a strong collaboration. It is a cooperative gaming. In such a situation, suppose the outcome is positive. You win. Now the question is, suppose I have a pot of gold. What is the fair distribution of that money? How would I spread it? How would I reward people based on their actual contribution to the winning? Now, you cannot just say the captain should get more or the leader should get more. So how do you determine the fair value of it? And there is a way of doing that. And I can go into that, but the simplest idea is that create teams or sub teams. Okay. One naive way, which is not Shapely. Let's call it baby Shapely. It's simply take a game, take a team in which this guy is there and this guy is not there. A team member is there and that team member is not there. And then see what is the outcome and see how much improvement in the outcome there is because of this team member. Are we together? Because of this team member, how much of an improvement in outcome was there right but then you can say well you know what uh there's a problem with this this team member didn't make much of an improvement because there was exactly one another person like him right so took over now what happens if you take both of those out and have a team without both of those and with both of them, then what happens? The difference is substantial. So you can say that half of the difference we can assign to each of the two. Right. So this is it. Shapely what it does is, now continuing that idea forward, what it does is it creates all sorts of teams with or without this person there, and looks at the average difference between all of them. And that is the contribution of this person. And then you do it this person. And then you do it for all the other players. So those contribution values So those contribution values are called shapely values. And for this particular idea And for this idea, Shapley actually got the Nobel Prize. Simple, but profound idea because it addressed a key question in economics, how much is a person worth being rewarded for contribution? Now, what has that to do with machine learning? It turns out, as people are beginning to realize, game theory has a lot to contribute to machine learning. People are only beginning to realize, game theory has a lot to contribute to machine learning. People are only beginning to realize that. They have realized it for some time, but more and more they're realizing that. So here, suppose we do this. You have a model which is effective. You got to an effective model, right? The effective model has all sorts of predictors there. How important was this predictor in have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive model, and you have a predictive right and so you can treat it as a game you can say that if you treat the predictors as a team and the game being make an effective prediction how much did each team member namely each predictor feature contribute to the outcome that is its features of predictors importance so that is what is the Shapely value. In Python, there is a package called sha, which helps you do that. It's a lovely package that is very deep and it has many things in it. We will gradually come to that, but for now we'll start with something simple. We'll get to the interpretation. If you do Shapely interpretation of which features are computing more, one has to necessarily make the assumption that the model that you're starting with has just the right set of features which gives it the maximum in terms of the R square. Not necessarily. All it means is that there may be still an unknown feature which is even more important, but it is relative importance of the features that you have built if you're happy with this predicted model you consider this is the best i could do given the data that i'm available the question still remains what is the relative feature importance so then if you stretch that argument you would say the ones that we remove for collinearity, we will still put them back in. Yes. So what happens is that that collinearity argument, actually, this is one of the reasons why when you do Shapley, you have to be careful because if you remove predictor, whatever contribution you gave to this, suppose this was one amongst three, you have to remember that this is one amongst three you have to remember that this is one third between those three predictors you have to remember to do that right so in other words if you say for example weight is most important so you you must remember that not only weight acceleration is motion i mean not acceleration but displacement is most important right and what was the other factor that we removed because of weight? We removed two other factors. Where are my factors that we removed? Cylinders, cylinders. So if weight is most important, then displacement is also most important and cylinders is also most important. So you have to remember to bring back that interpretation that is it and now what happens is this is peculiar to linear regression models when you go to linear models but when you go to non-linear models you never throw away the collinearity doesn't exist so you don't throw away correlated features right so this is the interpretation of it so now when you use the sharp package you will see that sure enough weight is number one horsepower is number two year is number three acceleration is four you see that right and you see it here weight horsepower year acceleration you see the same order right so for linear model the advantages and it's one of the things not many people realize, that if your data is killed, the easiest way to see feature importance is literally look at the coefficient. It is obvious from the geometry of it, isn't it? So, one more question. In the previous one, when we were just looking at the coefficient, we had to make sure that we had normalized each of those variables. Yes, yes, you must have. But for SHAP, is it necessary? Or would SHAP package take care of all that? See, here is a basic rule. To not normalize the data, not standardize the data, is almost like waking up and going to breakfast without brushing your teeth. You should always do it. It's not worth thinking, in this situation, could I have not standardized the data? You don't. You wake up in the morning and you standardize the data. Then you continue on to anything else. So just make it a habit. Always standardize the data. Linear model is the only model that sort of doesn't fall apart if you forget to standardize but even then the gradient descent gets adversely affected right but so just always standardize the data consider it to be good hygiene wake up in the morning standardize the data and then brush your teeth right sort of like that then there is something called partial dependency plot. What it means is that, so if you look at the weight and the miles per gallon, you're seeing if all other factors were the same. You know, people in economics use this language. How much does, for example, if government issues a lot of money, how much does that affect inflation? All other factors being the same. People are still manufacturing what they are manufacturing. So it is the partial dependence or partial relevance. So that is partial dependence. When you look at it, quite a bit, by the way, it's not weight, it is log miles per gallon and log weight. Remember, that has a linear relationship. Quite a bit. How much does the log miles per gallon depend on log horsepower? Quite a bit, but not as much. You notice that this is steeper than this or a little bit steeper. This is little, left most is little steeper than the middle one and all of them are far more steep than the acceleration acceleration doesn't matter that much right and you can see it in the feature importances also weight horsepower they they basically can compare to acceleration which has only a 6% influence. And so these are partial dependency plots. So guys, what I'm saying now, this is a known data set. Of course, many people have analyzed the kegel notebooks that this notebook, that notebook, medium notebooks, etc. Generally they all do a good job, but i'm trying to raise you, I believe, to a higher bar, to make you stronger than that, to make you at a place at which you do a rigorous and careful analysis of the data and build models very, very carefully. simple data set. But I want you to get used to this analysis, right? Thorough, careful, methodical analysis of the data and get into the habit of doing such careful analysis. Don't be like one of these people who just does a small analysis and promptly writes a medium, which is good. You write a medium article to say, hey, you know, I learned how to do analysis. It will be useful to somebody somebody they too are contributing value but you contribute more value to society and to the field if you submit a notebook in which you have done a very thorough and careful analysis of data are we together make it a habit because when you do very methodical and careful analysis of data you are much more likely to hit upon a breakthrough in case a breakthrough is sitting waiting to be discovered in the data right do that and remember all breakthroughs come from the data eventually