 All right folks, I remember we did an exercise right before lunch that we would try to guess how many stones there are in this particular jar. Now here's what we found. We got the guess from a lot of people. I removed the outliers, extremes, because they're typically anything that is an outlier and I kept all the readings as is. So the mean of the guess removing the outliers is 93. Two points for outliers. Somebody's guess was like 36 which is way different from other people's guess. So removing the outliers it is 93. Can you guess how many stones there were? How many did you say were there? 95. 95. The correct answer was 95. If you observe this in this particular case, it so happens that the guess, the average beats the best guess. Do we see that, guys? The average has beaten the best guess. But how do you know? Well, the standard outlet took these terms like duty distance more than the quantile interquantile distance, and this was to the point that, for example, one of you raised that in a prior place you would have lots of models make their prediction and whichever happened to make the best prediction on a sample data set, you keep the best and throw away the rest. The reason is sample is sample, sample from the the population another sample could have led to a different results you're far better off taking an ensemble of them, rather than just the. Best that worked for a sample right so, for example, the person who happened to have made the best guess is not likely to make the best guess next time. Remember, in everybody's prediction, there is information plus error the person may have gotten there just because of the error term right so you don't want to do that so anyway with that with that there hopefully that's a convincing that should convince you on the wisdom of crowds so the fact that you should always use ensemble methods if you can right Don't train one learner, train a whole ensemble of learners and then take the wisdom of that. Now when you take the wisdom of it, how do you take the wisdom of crowds basically? Oh the light, yes. How do you take the wisdom of crowds? We? In machine learning, there are many techniques that we use, and we are going to talk about a few techniques. We will talk about bagging, boosting, and stacking. and stacking. These are specific ways that you could sort of benefit from or aggregate or take every prediction and out of those predictions judiciously create one prediction, right? So let me start with the word bagging. What is bagging? Bagging? I'm reminded of something whenever I ask candidates in interviews, explain to me what bagging is. They always say that you you just take a sample or a bag of data points. That's not what bagging is. Bagging, and it has a clue that someone hasn't gotten it because the word bagging, it's a portmanteau of two different words, composition of two different words, bootstrap and aggregation. Bootstrap plus aggregation is bagging. So it's a bootstrap aggregation. That's what it stands for. What is bootstrap? Let me explain what bootstrap is. See, if you have the data, full population, and from the population, you could draw many samples. And for those samples, you take some value. You realize that if you were to, for example, just look at the mean of those, you would agree that the mean of the means would be pretty accurate. Isn't it? Each sample would have its mean, and then the mean of the mean would zero down to the real mean, right? This sort of the center limit theorem here. So it's a fact. You would imagine that some samples would have, you know, slight deviation towards high points. Some samples would have slight deviation towards low points. By and large, the average, averages, they will slightly fluctuate around the real average. So if you take sufficiently many samples and you average them, you'll get the right value. So that is the intuition behind bagging a bootstrap. So now in reality, you don't have a population. All you have is a dataset, right? Let's say that you have a dataset of size. Let's consider a dataset of size N, N points, N samples, right? Now, generally, N is not that big. We live in the world of big data, of course. In that case, you can go about taking samples. But if it is not so, in most situations, because of high dimensionality, et cetera, you don't have the luxury of throwing away data and just taking subsets of it and so forth. So one of the techniques you can do is you can pretend as though you have lots of samples. And what you do is you start. So imagine that you have a jar. Let's say you have this jar of about 100 odd points. I ask you to just put your hand in, find what it is. Take one instance of data. Now keep it. Except that, of course, you don't keep, you take note of it. But then you put the stone back. Remember, you have to put the stone back. With replacements, replace it back into the jar. Shake up the jar a little bit, dip into it again, take another data point out, write it down. And you keep doing it maybe n number of times. So what will happen is you will end up with a data set of n points, but they won't be the same as the original data set because some things would have repeated and some things wouldn't be lucky enough to the original data set because some things would have repeated and some things wouldn't be lucky enough to make it to that isn't it and it's a and i'll leave it as an exercise but basically uh if i remember the exact proportions you will get about two-thirds of the two-thirds of the data points will manage to show up one-third will not show up so if two thirds shows up then of course they're repetitions right the same data point might show up one third will not show up so if two third shows up then of course they're repetitions right the same data point might show up a couple of times but you just created another data set of size n from the original data set isn't it and now you can repeat this process to get to create many many many many bootstrapped samples isn't it this way of creating sampling is called bootstrapping so in other words bootstrapping is sampling with replacement right sampling with replacement so when you do that that is bootstrapping. So, so far, so good, guys. We understand bootstrapping. Now, what you do is, and this is the bootstrap, sorry, the bootstrap bagging is, you do one thing. Given a dataset D1, create bootstrapped datasets. Let's say that you create bootstraps data set B1 to B K K data sets you created and create a large number of them the more the merrier obviously the more that you put you will start losing out in performance I mean in computational performance. And generally, it stabilizes pretty quickly, right? For most machine learning algorithms, 100 or 200 bootstrap samples are good enough, right? So you create 100 or 200 bootstrap samples. And then what do you do? For each bootstrap sample, you build a model. And when you train a model, a model on each bootstrap sample, right? And when I say model, I don't necessarily mean a decision tree i mean any kind of model you could have picked regression linear regression you could have picked logistic regression you could have picked whatever you want so far so good guys so you will end up with all the way to mk models And then comes the aggregation part. The aggregation part. In the aggregation part, what you do is you make each of the model predict its y hat, y1 hat, all the way to yk hat. predict its y hat y one hat all the way to yk hat right for a given data point x they can all make predictions at that point what it should be let's say it's a classification problem you will have to tell whether it's a blueberry or a cherry or whether it's a cow or duck or whatever the problem is right or multi-class classification, what it is, horse. Then comes, how do you, and in the case of regression, for example, different models might predict the temperature as different in that field, in that land that we talked about. Now, aggregation part is for regression. What do you need to do? You can just take the average of all the predictions right i is equal to 1 to k of y i you can take the average isn't it simple average of the predictions so far so good guys right so you have the average of the predictions and so you aggregated all of them you added up all the predictions and you divided with the number of patients for classification you took the majority class among the predictions, if you got cows and ducks, cow, cow, duck, cow, duck. So suppose we have five points. So what is the majority here? Cow. So you would say your answer is cow. Majority in this committee committee in this crowd. Each learner is saying in the coward duck, and you have to pick the majority class majority class or the winner the most frequent the most frequently predicted class is that, by the way, when we mean majority it doesn't mean simple majority more than 50 of it for example if it is a cows ducks horses and goats nothing may have 50 of the words but whichever has whichever is the leading in terms of frequency or in terms of words the leading vote is your prediction are we together so? So interpret the word majority judiciously because it's the biggest one. The guy that gets the, the class that gets the biggest number of votes. You pick that as your answer. And so this particular approach is bagging. Are we together? Does bagging have anything specific to do, for example, with decision trees? No, right? you could use it for any algorithm and in fact you should you can bag with if you want linear regression go ahead please are quantifying effect we have n samples so this b1 will also create yeah you can make it have n samples right except that you did sampling with replacement when you do sampling with replacement approximately two-third of the points will show up one-third of the points will get missed out there will be repetitions right here that's fine and then you dedup it so then you set sizes it won't remain n it will become smaller than n right i missed that line around the bootstrap data set so essentially you're taking samples multiple of that the same data set right yeah yeah so these are samples so now so actually if I missed the portion where you talked about the replacement in bootstrapping, what do you replace in the bootstrapping? Yeah. So, all right, I'll repeat what I said. Sorry. Yeah. And if I take this, by the way, those of you who came late, none of you could beat the wisdom of crowds. There were 95 stones that folks here measured in this jar. The average was 93. None of you came close to it. 93 bit beat all of you. So your that just shows the wisdom of crowds that collectively your average beat any individual. The average was 95. No, the actual reality was 95. Average was 93.1. Right? So anyway, so suppose you have this jar of stones. You do one thing, you shake it up. Sample into it. What is this? So whatever it is, you keep this as your first data instance. Record it, but put it back. So what are you doing? You're replacing it back in the jar. Is it? Go shake up this jar quite a bit. Mix it up. Stir it. And then now pick up another data point. Are we together note it down and then put it back right and then shake it up again and take another data point or different right and it may be so that by accident you pick the same data point that you have seen in the past so repetitions are fine so now you can suppose there are n marbles here or n stones here you can still take n sample and a data set of size n in others by taking n trials each time picking a stone right and so you will end up with another a data set derived from the original data set derived from this original data set you will end up with another data set which will have endpoints isn't it but because if you have repetition its real size will be smaller then what happens that is called a bootstrapped sample okay that's a bootstrap sample okay now come the interesting fact i can repeat this many many. One second, I'll take your question. I can repeat this many, many times. Suppose I repeat K times. I will have K different data sets, bootstrap samples that I would have created. Isn't it? Yeah, so we'll stop there. Your question? Yes. So the imbalance data is a whole topic. So it's very pertinent, very relevant here and how it gets affected and so on and so forth. But for today's discussion, let's keep it out of bounds. I'll cover it properly later. We'll cover it when we talk about imbalance data as a whole session on it. Right. So, so at this moment, we ended up with K bootstrap samples, right. So from the original data set D, we ended up creating K, suppose we repeated this exercise k times. In practice, k should be 100 or 200 at least, but don't go crazy. Otherwise, don't make it 10,000. Because then what happens is you achieve stability pretty soon. Your prediction errors come down pretty soon. Two, three, 400 is pushing it. They're inexperienced. But don't go like only five samples. Then it's not right. Go heavy. And you'll see that so given the bootstrap sample you can build a model that is trained specifically on the bootstrap sample for each bootstrap sample you can train a model let us take so bootstrapping is commonly done for example a very common to do it with decision trees you can fit one decision tree to each bootstrap sample isn't it so you'll end up with a model m1 to mk or m trees k trees each of the k trees will now make a prediction at a given point in space a given point x let x be a point in the feature space actually i'll remove the arrow x bar is a point in the feature space in the you want to make a prediction at this y hat now each of these models will make a prediction about what it is at y hat if it is classification let's say it will tell you whether it's a cow or a duck if it is regression for example the case of a temperature it will tell you whether it's a cow or a duck. If it is regression, for example, the case of a temperature, it will tell you what the temperature is there at that point. So now we need to accumulate the wisdom of all of these learners. How do we do that? So that is the aggregation part. We have done the bootstrapping and model building. Now the aggregation part comes in. Well, aggregation is quite simple for regression. Just take the average of their answers. So if they are all predicting, ah, the temperature here is 72 degrees and they're saying, no, no, no, it's 74 or whatever it is, just go and take the average. And as we saw in a very live demonstration today each of you were a predictor right each of you were a machine learning model and you predicted how many stones are in this jar and your average i hope you agree that 93 is astoundingly good is astoundingly good, isn't it? It is. Your average of these predictions is a pretty good measure generally, right? And so for regression, go average it, average the predictions. For classification, find the class that got the most votes. So if you're looking at cows and ducks and goats and horses, if it turns out goats got the most votes. So if you're looking at cows and ducks and goats and horses, if it turns out goats got the most votes, it may not be more than 50%, the four classes, but if it got more votes than other ones, well, it's a goat. Make sense? That is it. It's as simple as that. So that is bagging, bootstrap aggregation, but in between is the model building and prediction part. So that's what you do. So when you train any bagging process, you train K models and you hold onto them. And so for example, if it is decision trees you you use k trees you hold on to k trees and at prediction time you make them all make predictions and then you aggregate the result all right so that is backing so one algorithm so now we are almost there so there is one uh so that is bagging. There is. So this is it actually. Then I'll bring another big idea here. It is called boosting. I always joke about it and say, the best way to learn is to learn from the mistakes of others, especially your parents. It's a joke, but it has the flavor of that. What you do is, suppose, so again, you go do your stuff. You take the whole dataset and see, you realize that bagging can be done in parallel. You can train all the K models in parallel. It is this virtue, it's highly parallelizable. That is why when you look at the distributed systems, like big data distributed computations like Spark, one of the first algorithms that came out were based on boosting. In fact, random forest we're going to talk about. It's not boosting, bagging, because it's highly parallelizable. You can distribute the computations. But this is different. Boosting is sort of more sequential. What you do is go build a model right so suppose your data is there you have x right so x 1 x 2 x p and then you have y actual y right so then this is Right. So then this is actual. You build a model, let's say you build a regression model of some kind, linear regression model or whatever it is that you build. So build a model build a model regression model of course let's say f let me call it f um one right uh i'm trying to align it to the notation of the book, maybe it's F0, but so forgive me for that. This is your X1 to XP. Basically, it's your X vector. Goes in, what comes out is Y hat, right? And I'll just put a superscript here, calling it is the first model that you build. In this case, you train it against y. In other words, train it against y. In other words, loss function should be as a function of y, y minus y hat, against y minus y hat square, right? Use this as your loss function which as you have been doing so far we have been doing this isn't it for regression model so what will you end up with you'll end up with y hat from the first model right the y had the predictions coming out of the first model that you built y minus y Then what happens? Those predictions will obviously not be perfect. So what will you end up with? Residuals, isn't it? You'll end up with the residuals are one, which is equal to y minus y hat from the first model. This is the residual. So you say, you know what? Residual residuals contain information do you remember that we used to see that if we built a simpler model we would literally see a pattern in the residuals you left a lot of information on the table so let's capture the information using one more model, right? So now what you do is your model number two, you build F2. You give the input is still the same XP, but your target variable, the target variable, so Y hat two is based on loss is based on R1 minus y hat y hat 2 right so in other words what is your what are you trying to capture or minimize you're trying to predict the residuals not the not the initial y right because you say that the residual has information. So if I could perfectly capture the residual, then the combination of F1 and F2 is the answer. Do you see how easy, how straightforward the thinking is? Right? So Y1 made a good progress, but it left some information on the table. It's there in the residuals. If I could fit, so in the perfect world, there is no noise. If I could fit model 2 and it perfectly fit to the residuals and all i need is model one plus model two right in some form reality is model two will be more susceptible to learning from the noise or overfitting isn't it because y has certain amount of noise but the but because information has been taken out the signal to noise ratio is lower in residual one isn't it and so what you do is you create a model you say that my model initially was f1 but then i go to a model from here which is f1 plus lambda f2 you shrink the model f2 little bit of model f2 you take you don't take the full contribution let's say half maybe or three fourth or whatever it is you shrink it down a little bit so this becomes your new model so in the beginning f is this right f1 f, the total effective F after that, let me just use a capital F for here, if that works. Capital F is this one. Capital F2 is this, right? And then you go and do capital, you now use this as your model and you again make predictions so what will you end up with when you feed f2 to x you will and then compare it you will again get residuals those residuals would be y hat minus r1 do you notice that r1 the residual how how much you fail to predict the residuals right so then so this is little f so this is little f uh three right f3 is trained on this so the loss of f3 let us say is see i'm forgetting the regularization terms and so on and so forth right i'm just skipping it, you can keep on going doing like this right, so this will be something like. f2 plus Lambda f3. a little bit. Why are you shrinking it? Because you know that it's less likely to contain information, more likely to contain or be susceptible to noise. So you don't want to give it as much weightage as the first model that you built or the second model that you built. So you progressively keep shrinking it, the contribution of the subsequent models. And ultimately, you stop somewhere after making certain number of models, models and ultimately you stop somewhere after making certain number of models because you know that it's just noise left after that so you can have a judicious stopping criteria you can just stop after n number of models so you can stop when you notice that your residuals have no patterns left or whatever it is you can you can have some process when you do this what are you doing you're boosting the efficiency of a base model by layering another model and boosting the efficiency of that composite model by layering one more model fit to the remaining information at each stage whatever the residual information is right you're trying to fit a model and layer on top of it and that is why this approach is called boosting right and thus bagging and boosting with bagging and boosting in place let me know it's simple ideas isn't it there is one more method that people often use, which is called stacking. Stacking is the idea. It's an easier to understand idea. What you do is you create the first layer of learners. Learners, right? And each of these learners could itself be composite ensembles right so for example you could have used bagging in each of these collections if you so wish or you could have it as just isolated learners but what you do let's say that it's a classification problem they will all make predictions y1 hat y yk hat, isn't it, on the data. And then what you do is you feed it, let us say, for example, into a logistic regression model. I'm giving an example. Or if it is is regression you would just fit it into a linear regression whatever it is put it into a support vector machine right and svms actually work pretty well as the last this endpoint model it is the last model but what it is learning from is not the data it never sees the data all it learns from is inputs. These y hats, these predictions are the input to that model, and it will produce an overall output, which is the composite y hat. And now you can do its loss with respect to the original y, right? So your loss function is still between y and y hat based on regression or classification, some squared error or cross entropy loss, and of course regularization and other things thrown in. Right? It's like almost built a little neural network. Yeah, except that you're not using a neuron. Right? That's it. So this approach is called stacking. You just stacked it. Asif sir, I have a question in the boosting algorithm um for the second model shouldn't the residual should also be input y2 no y2 or you mean f2 f2 trained on the residuals. But it says x2, xp only input. No, no, no. See, data, whenever you get data, right, you get the x vector and you have the target variable. All that happens is for f1, the target variable is y. For f2, the target variable is residual 1. For f3, the target variable is residual 1. For F3, the target variable is residual 2. Right? Input variable always remains X. Are we getting it? So, for example, if you're talking about ducks and cows, the weight of the duck and the volume of the duck, size of the duck is still the input, or the animal is still the input. But you're learning from the... if it is a regression problem suppose you're trying to predict how fast or how high it can fly then it is simply you're learning from the residuals but so learning is based on the input and then learning is never based on the input and then no no learning is never based on the input goes in see remember learning is a forward and backward journey okay let me let me let us go back to basics see what happens is so i will just write x is equal to x1 xp x vector i'll just use x here see learning is this step one you take some random weight so you have a function you do y hat is equal to function of x some function of x right correct then what happens step two compute the residuals, the error function. Let's say it's regression. You would do y minus y hat. So you remember, you notice that you are bringing in your target variable to find the loss. Right. Right. Do that. Step three, gradient. But usually, unless it is a deep learning method method we don't do the gradient descent right we always do gradient descent pradip unless you take the gradient let's say of the loss how would you update the weights right right you you need a gradient is the learning in fact when you say machine learning the learning is the gradient comes from the gradient gradient of the error because then what do you need to do whatever weight you have for example beta right some beta vector is beta next is equal to beta minus alpha, the learning rate times the error. to prime, which is that create a loss function out of it. Because loss function is not just the error squared, but it also contains some regularization terms or whatever extras that you have, right? But ultimately, you just take the gradient of the loss. And if you want to the first approximation, think of it as gradients of the sum squared error for regression, right? Now you compute your beaters like this. R. Vijay Mohanaraman, Ph.D.: If there is a whole lab pretty that I put in which we computed we did the instead of using psychic learn we did the entire gradient descent to re implement regression. R. Vijay Mohanaraman, Ph.D.: Right, the name of the lab notebook is literally a gradient descent. And you will see that we have implemented the regression model, linear regression on the old faithful geyser, geyser, geyser, geyser, geyser, old faithful geyser using from first friend say geyser geyser right using that so remember gradient is the heart of learning unless you can get to the error and take its gradient you pretty much have a hard time learning from that right so um yeah so now i understand i was little got little lost because this in the deep learning we do it explicitly so but in the other one it is the in library itself yeah see in deep learning it is more than that because what happens is that the output of one layer is the input to the next is input to the next one right so each layer will do this but their gradients updates have to back propagate. Right. There is also the back prop. Yeah, for that we only calculate and then do this type of the back prop. That's right. So remember the forward pass is just the F. At each level, you compute the sigmoids or whatever, the activation functions, A of output from the layer before it, right? Activation as the prediction. And that prediction becomes the input to the next layer. So you stack the layers basically. Can you scroll back to where we finished the stacking? Yeah. You can see the picture. That's right. Except that you have a whole yet another layer of uh nodes that in a deep neural network go ahead is actually a different of taking average or taking the math oh it doesn't do that what for example if you look at logistic regression right suppose it determines that the weight of w1 w2 wk it may determine that w2 is 20 times wk means the predictions coming from two second predictor is better than 10 times better than the prediction coming from the kth you see that so in a way it is waiting it is applying weightage to the predictions of the different models. So that's one way of doing it. If you do support vectors, then of course it gets even more complicated. But in a simple logistic or a simple regression model, that's what it is. You're doing a weights. So that means you can see weights are people who want that stacking. Yeah, stacking degenerates to just aggregation. So when people do stacking, one of the funny things that happens is the learners that come before it, sometimes they are not just individual learners, they themselves are ensembles. Right? So and one way that I do it quite often is I will create ensembles, or I will use heterogeneous learners. learners remember i said the no free lunch theorem no one algorithm is always good right or the best so in my my basic gut feeling says that it is always a good idea to use a committee of different algorithms models derived from different algorithms right so if you're doing classification use aistic regression, use a linear discriminant analysis, quadratic and so on and so forth, whatever you know, right? Throw in one of each or a few of each into your layers and then stack them together. So that is stacking. Now these are three big ideas and there is more to it, but there's more. We can go on to different ways on how you can take the wisdom from the crowd properly. But I will now start with actual implementations in the final hour. I will start with actual implementation. Perhaps the most used algorithm in this space is called random forest. It's a pretty complex reasoning when you first encounter it, but later on it begins to look very simple. So random forest is an algorithm that takes many, many ideas together. Right. And so in a way you have to line up quite a few ducks in a row. Right. Let's see what ducks you have to line up. First is you must do ensemble. Ensemble, right, of trees specifically. It happens to pick decision trees as its root. But when it picks a decision tree, let's see if I remember it right. Yes. So you don't bother about the pruning part. You know, trying to make one perfect tree. You don't do that. You just make a tree tree let it go deep doesn't matter so you take an ensemble of tree let's say k is equal to approximately let's say 100 to 200 something like that whatever you want pick then the second idea that you do is you do bootstrapping to generate k data sets to generate d1 all the way to dk or rather b let me use the convention used in your book b1 all the way to pk right out of the original dataset, out of dataset D. Then you use aggregation, aggregation to get results, to get the final prediction. So, so far so good, but it brings in one more, one more idea, which is actually very crucial in making of it to illustrate the idea i will speak to a weakness in the I will sit down for five minutes. Are you guys able to see me clearly? Remote? Yeah. Yes. There is a problem with decision trees. To illustrate that, see, it is the problem, what I call the problem of the bully predictor. Suppose you're building a model, and you take a bootstrap sample and you try to fit a decision tree to it. All the predictors are there. One predictor somehow is dominant so you will end up splitting along the dominant predictor isn't it because the data came in such a way that the best split that you could get was along that dominant predictor you end up splitting it then you might split again on another predictor that's there which is also pretty powerful so what happens is that two trees may differ only in minor details so some of the lesser little lesser influential regions isn't it but most of your trees will Do we get that? It is like real life. Have you seen that many times I joke about it. There are all kinds of leaders, but there is always the leader who goes into a room and first speaks what his views and then guess what happens after that what what does everybody else do in the room if he's a if he's a particular kind of a leader then everybody has no choice but to just get along with it and say give arguments in favor of that particular thought and obviously every time there's only one outcome that you expect and that outcome is predetermined by what the first guy spoke the leader spoke obviously you would agree that good decisions don't come out of such committees right so what you want to do is you want a way to kick out that leader from the room and then ask people to use their own intelligence and come up with different views, different decisions, and you take the majority or you aggregate those decisions, right? You actually have wisdom of crowd, but you wouldn't have any wisdom of crowd if this bully guy was there. Do you see what I mean? Is that the intuition? I'm just giving you a sort of intuition of it. The same happens with data if there is a dominant predictor all the trees will be perturbations of the same tree up to a first approximation right and that can be problematic right especially consider that there are two predictors which are highly correlated. Right, then, you know, one will start being the proxy for the other, and the other will look unimportant. It's basically you steal its thunder. So you don't want to do that. So you do something, and there is a lemma in this space, which is space, it says, and without getting into the mathematics of it, it basically says that whenever you're splitting, say in a tree, what happens is you do your splits, right? You'll do a region split. Whenever you do a region split, what do you do? You go and find the best split direction out of all the p predictors out of all p predictors find the predictor axis best for split the trouble is that it will always be the same for every tree, right? So what you do is at each split point, at each split point, you show the tree only a partial reality. You show a partial reality projection. And what you do is you take a feature subspace of only m m predictors where m is less than p predictors so suppose there are 10 predictors maybe you'll take only three of them right and say i will search along only these three directions. And you randomly pick those three out of 10, right? So you realize that if you look at 10C3, 10 combination 3, that's a pretty large number, isn't it? It's 10 times 9 times 8 right 720 divided by 6 right 3 times 2 times 1 right that is 12 there are at least 12 combinations possible so that right and any one of the 12 combinations may get picked up right so you will search for the best split direction only amongst the three and there's a lot of and some so you realize that when you pick just three predictors then at any split any one of the 12 combinations may may get looked at and only the best predictors of those three will be taken right this idea prevents the bully predictor why because the probability that that bully predictor will be there is only m over p. Right? Much smaller probability that it will even show up. So it will not dominate all the trees. All the trees will look different. Because some trees at different stages of splitting were not even seeing the dominant predictors, isn't it? So you will get a truly varied, a tree that truly is pointing is different. That is why often you'll see random forest pictures that people depict. They'll have trees looking differently and of different colors, right? Those trees are different, inherently different. And you say that the trees are uncorrelated. Uncorrelated trees is the closest thing you can have of independent thinking. Even the ranophores will never do some run with key predictors. It's always endless thinking. Welcome. Yeah. So, go ahead. So, when the intuition with which I'm asking this question is like this. When you work with M less than p predictors, there's a certain amount of information content you did not use. You did not use and that is the counterintuitive. So his question is, so let me, anyway, I'm going to talk about this. So, let me, anyway, I'm going to talk about this. So, let M less than P predictors. There's a certain amount of information content you did not use. You did not use, and that is the counterintuitive. So his question is, so let me, anyway, I'm going to talk about that itself. So see, when you take M less than P, one might get the uncomfortable feeling that are you not hiding information? Wouldn't it be a weaker model when you show less information? And the answer to that is yes, you deliberately want weak learners. The reason you want weak learners, and this is a counterintuitive fact, an ensemble of weak learners is way better than an ensemble of strong learners. Right? Listen to what I'm saying. is way better than an ensemble of strong learners, right? Listen to what I'm saying. An ensemble of weak learners always outperforms an ensemble of strong learners. If you, so for example, if you take M is equal to P, take all the predictors, then each of the decision tree is strong, isn't it? Because there was no subspacing while doing the splits. So the decision trees will fit hard to data. In fact, all the decision trees will show minor perturbations, but look rather similar. And so when you aggregate their views, it would be not vastly different from the decision of any one of the trees. But on the other hand, if you take weak learners, they will end up because they have seen partial reality. So they don't get dominated by bullies. Only a few of them get dominated by bully predictors. dominated by bully predictors right and many of them don't see it at crucial stages of the split don't see it or any one tree will see a particular predictor dominant predictor only at certain nodes of the splits right and so you end up with weak predictors weak learners the word is weak learners learners the word is weak learners and you want weak learners the best thing you want is ensemble of weak learners because it is the only way to guarantee is the only way to guarantee that guarantees that learners are, in this case trees, are uncorrelated. They are not all saying the same thing. Right? Uncorrelated. And that is important. Remember what we said even in human situation, it's wisdom of crowd so long as people are thinking independently. It is the hysteria of mobs when they all are thinking the same way isn't it right so that's the thing that you want to avoid and so once you bring it and so what are what is good value of M here is something i'll mention see scikit-learn doesn't help you decide the best value of m it will say that feature that split feature space that's a sub subspace of the feature space dimensionality of the subspace of the feature space it doesn't help you choose so people often think hey why not take all the predictors isn't it and they do in the random forest and that's they usually end up with bad mistakes and you will see that in bad models we'll see that in our labs here is an here is a rule to follow. And this rule is coming from a particular mathematical result. Square root of p. Follow this rule. m should never exceed the square root of p. So far so good? Should never exceed the square root of p. And in fact, here's a dirty little secret very often quite often m is equal to one just picking one each tree picks up its own predictor just one predictor and gives as good, almost as good, almost as, sorry, this thing should have been here, almost as the best one. Right? So this is a generalization of the one R rule. Remember I said that Holt's intuition that sometimes just one predictor is enough, find the best predictor. But when you take m is equal to one in a random forest, you're not just taking the best predictor. You're basically letting all the predictors have their say, but you're taking the average of their values. The best, the second best, all of them, they have the same, right? And you're aggregating their values. So remember this rule, important rule. And when you do that, so all of these principles, bagging plus this random subspaces, when you take at each split, then in building the decision tree, what you have is the random forest example, is the algorithm. Very, very popular, perhaps the most popular bagging algorithm. Its key virtues are that it's highly parallelizable, works very well. Its weakness is that despite the fact that it aggregates, remember I said that how when you take the aggregation, the decision boundary by being the average or being influenced by all the learners smooths out, right? But it turns out that even while it is smooth out, it may still have a certain degree of slight degree of overfitting, much much less than a single decision tree, but still it has some amount of and so they are they're all of these things what do they call extreme randomized trees or something like that uh scikit-learn has some many many adaptations of the random forest idea to improve upon it you could do that but now in the same spirit there is a the boosting the boosting is it what i just explained to you right but it also has many implementations for it there is gradient boosting so boosting boosting there is gradient boosting actually what i explained to you would be pretty much the gradient boosting thing now where does the word gradient come in there gradient comes from the fact that the residuals y minus y hat right is delta y right is sort of people often think of it again don't worry about it forget the get why why it is gradient let's not go there but it's gradient boosting then there is xg boost so the point is boosting is slow remember one model another model another model so somebody sat down and created a much faster version of the same thing ideas are the same more or less my variations but it just runs much faster so that is xg boost xg boost is not directly in scikit-learn but it if you do pip install xg boost you'll get it all right then somebody has created the cat boost and i'm still waiting for somebody to create the dog boost because i like dogs it isn't there yet anyway i'm joking so there are many boosting implementations out there and they're all based on the ideas that i explained to you okay so that is boosting So that is boosting. And that is it, ensembles. There is more to it. So there's one more topic, but I feel we are reaching the end of how much we can take in at this moment. There's something called BART. Let's keep it for Tuesday or something later. BART, Bayesian Additive Regression Trees. or something later like it's a but by shen addition by shen additive regression trees okay so there are many see guys here's the thing we can go on and on um shapiro and friedman i believe have written a landmark book. So they're the original creators of the boosting thing. And their book literally called boosting is a landmark book. If you read that, there's a lot of theory and work on this space keeps on going. But much of the work that I find in more recent terms is more about making it faster, making it a little bit more accurate and so on and so forth, the core ideas remain the same. There are many many adaptations. Like anything, it's become a subfield of study, ensembles, ensemble methods. So now let's go back and recap what we studied today. Since it was a long intellectual journey. We talked about dimensionality reduction, which we reviewed from last time. Dimensionality reduction has a lot of benefits. We talked about those. It becomes necessary to do that. We talked about those. It becomes necessary to do that. We talked about those. But the basic idea is that if truly there is a signal, data must be living on a, or either for regression, living on a submanifold embedded hypersurface or decision boundary is a hypersurface, right? Embedded manifold. So with that thing there, our job is to find it. We used the simplest techniques, principal component analysis for simple linear, approximately linear data. Works very well. Then we went today into ensemble methods. Now, before ensemble methods, we did tree, classification and regression trees, right, or decision trees, simply put. Decision trees have great intuitive power when they are simple. You can visualize it. It's virtue. You can see how decisions are being made. Many people consider it to be even more interpretable than linear or logistic regression models. And with that, we learned about how to use it. It is about successive or recursive, well, splitting of regions till a terminal point condition is reached. You keep on doing it. So there are measures of errors. For regression, of course, it's always sum squared error. But for classification, you could literally look at the error rate, three metrics we talked about, right? And of the three metrics, the error rate, the Gini purity index, though I look at it more as an impurity index, the bigger the value, the more the impurity, and then the entropy. What is the entropy? So bigger the entropy, the more the disorder, the more the mixing up of items of different classes in that region. So for example, if I were to look at this picture, where is it? This picture, then these would be areas of high entropy because you see a mixture of blues and blueberries and cherries but these would be regions of very low entropy isn't it so that is it so then using those measures you can build a tree a deep tree and then you can go about pruning it but while you're pruning it don't use the same method that you use to split use Use a different error metric. So if you use Gini or entropy typically used to go down, come back up using just the error measure, error rate, right, to prune it, right, and use alpha as a regularization parameter, t is the number of nodes. Basically control the number of nodes. Basically you control the number of nodes, you penalize the system for having too many nodes. And so then comes the idea, the big idea. So decision tree is yet another algorithm that will produce one learner. Now the big idea that we learned today is of ensemble, so the wisdom of crowds. If each algorithm's model is one learner. Now the big idea that we learned today is of ensembles or the wisdom of crowds. If each algorithm's model is one learner, you can take an ensemble, a crowd of learners, and then learn from it. It's the committee method some people call it, it's the ensemble method. The three main methods within this are bagging, boosting, and stacking. The three main methods within this are bagging, boosting, and stacking. Bagging is bootstrap plus aggregation. And so we talked about it. You take bootstrap samples. How we take bootstrap samples. I sort of explained it to you. You go on. It is sampling with replacement, basically. Now, in general, you sample by, when you sample and you take the same size samples as the original data set, you end up any bootstrap sample will contain approximately two-thirds of the points, because of duplicates. And then one-third of them will be called out of bag. They are not in that particular bag, or a bootstrap sample you aggregate regression is simple averaging classification is taking the most the mode the most frequent or the most highest most voted on right now there is one freebie that comes with bagging see whenever you do so you have the test data you have the test data, you have the training data, and you have the test data. One of the benefits is that while training itself, if two-third of the data is in one bag, one sample, bootstrap sample, guess what I can use it to test, isn't it? To quantify the error. And that is very nice because you can build cross-validation right into it. You know, the hyperparameters you can tune just using whatever hyperparameter, like the depth of the tree, the number of, et cetera. You can just tune, right, using the outer bag errors that's one lovely part of it the other important thing is with this with random forest you are and with xd boys basically you get feature importance and with decision tree also what is the most important feature the one that you split with first obviously you just saw it isn't it next important feature is what you split with first. Obviously, you just saw it, isn't it? Next important feature is what you split with next and so on and so forth. Random forest gets more complicated because you have lots of trees. So what you do is for each feature, you look at the prediction accuracy with that feature there and without that feature there. The difference is a measure of how important that feature was, how much it was contributing to accuracy or to the prediction power right so you can get feature importance with that and we saw that in the lab when we did california housing when we did auto if you remember oh no i haven't showed you the examples with random forest i will be showing it to you in the lab i'll release those labs and you'll see it so do you use use a package like SHAP or something to do that? No. Random forest will produce its own feature importance. Having said that, I must say that the feature importance produced by random forest is suspect because there are certain logical flaws in just using that approach. And so you shouldn't use that. You should use SHAP. But it is often considered a merit that you get some rough idea of feature importance from the model itself, though I would again say don't go with that go with sharp. yeah it's extreme random force that changes the split points to improve the performance right. improve the performance, right? Extremely randomized trees or changes, yeah. Extremely randomized trees basically says that when you are splitting, don't look for the perfect point, randomly pick a point. Now that looks counterintuitive. Wasn't it the whole point to find the best possible point along all the axis to split on? But extremely randomized trees basically says, hey, don't bother, just pick a point and split. Right, and while counterintuitive as it looks, what it does is, as you can imagine, it tries to create even weaker learners, right? And in creating even weaker learners, it creates learners that don't overfit at all, right? Or have less variance problems. But we won't cover that. So, I mean weaker learners, it creates learners that don't overfit at all, right, or have less variance problems. But we won't cover that. So I mean, yeah, it is true. But that's all that there is to it. Was it? Diversity based approaches. That is right. Actually, this field, right, is you want a PhD, go create one more implementation. Right? So, or you want to publish a paper just You want a PhD? Go create one more implementation. So you want to publish a paper? Just yeah, it's a machine learning. The beautiful thing is, you can be so creative with the algorithms. I remember that one venture capitalist when sent very sarcastically that in Silicon Valley you throw a stone, and it will hit a guy who has an algorithm that he invented right sort of devaluing the work of techies i suppose but and emphasizing that money is everything whatever it is but people do make a statement and this is actually one of the big VCs, so I won't take the name, stacking. So stacking is just taking the predictions as input to yet another model. And random forest is bagging plus this idea that you take random subspaces at each split point. So you deliberately create weak learners. And the big rule is, what is the dimensionality of the sub-feature space? It should never exceed square root of p, p being the total number of features, number of predictors. That is an important rule. And scikit-learn doesn't prevent you from that. You have to know it. You should remember it. Oftentimes, m is equal to 1 gives pretty good results. And it's computationally cheap. Likewise, based on boosting, there are quite a few implementations. Gradient boosting, XGBoost, CADBoost. And I'm really hoping somebody will create a dog boost. So, that is it. All right, folks, so today I will end here and take questions. Because it was a long theory day, we may be facing information overload. I hope not. Was it fun? Barrel of monkeys. Can you scroll back to where the random forest was discussed i missed some notes from there yeah all right guys so i i can see that everybody is quiet which probably means they have already gotten too tired and are falling asleep uh go ahead. So the whole idea regarding this ensemble team is they create the weak learners and we take the aggregation of the weak learners. What is the mathematical proof of it? Like more empirically, it is proven that this is the... Oh yeah, yeah, there is. So there are two things. First is that committee, ensemble's work. You'll see it in my lab actually give the right mathematical references there. The second is, in fact, is one of the quizzes. When you do the quizzes, you'll see that whole. But the other one, and the proof is straightforward, the other one is, see, this thing is deeply studied. The entire fields, what are they called? Social decision theory and so forth forth the fields of people who study how decisions should be made and in society so that it's optimal right and it's a well-known thing the second part is mathematically you can show that m should be less than equal to square root of p it is in fact the theorem's name is i keep forgetting the exact name it's called the lemma strass something's lemma the name is rather hard to pronounce german name somebody's lemma which rigorously shows that you should not do that see the point is you have to avoid basically bully predictors right and the right way to do that is to take square root or less that is it so these are all on very very solid mathematical foundations yeah and in this case how we define like how many how many How many learners we should do? It's a hyperparameter of the model. My general thing is that, see, OK, a very good question. See, actually, let me answer it. How many learners should we take? What is k? So what happens is, if you look at the test error or training error, what will happen is, as this is the k number of learners in the and actually great that you asked it i should have come at it the number of learners in the ensemble what happens is that the error rate right will will go like this, training error. And test error may go somewhat similar, but like, I'll just make it like that. So if you take too many of those, that also can be not so good. But generally, it doesn't happen. It also keeps falling along this. So what happens is computationally, you know that you have reached the answer here. It's not getting any better after that. Actually, I exaggerated it. It just flattens out, right? It just flattens out. And this also just flattens out, flattens out right it just flattens out and this also just flattens out flattens out so this is typically at 100 200 400 something like that you can just run it a few times and see when are you flattening out the moment you are flattening out stop right and Right. And also don't start with m is equal to I mean just start with 100. Right. And then go, the do I need to go further do I need to go to 200 do I need to go to 400. Once you notice you have saturated stop. For the sample creation. That is a learner, the number of learners is the number of bootstrap samples it will create internally but each learner needs a bootstrap sample so why would you create more than that at each stage that's it that is it what's that switching yes exactly yeah i just post the link to the to the this thing i should have remember it's a beautiful lemma it's a lovely lemma in geometric thinking i'll put it there it is it basically says it in very formal language that pretty much the properties of a higher dimensional manifold Raja Ayyanar?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi? All right guys, anything else? If not, I wish you guys a good weekend. Please pick your data set, form your teams. Next time I hope when we come here, we don't come here with blank faces, guys, please. It will defeat the purpose. Let's take it as a meaningful boot camp. Shall I stop the recording? Yes, let's do that.