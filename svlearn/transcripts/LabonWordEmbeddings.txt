 All right guys, so we are ready to start. Last week we did the bottleneck methods. When we talked about creating a neural network in which the input layer and the output layers are wider than the intermediate layers, in particular, if you have a latent representation layer of a much lower dimensionality, then the neural network is forced to learn a representation that is a summarized representation of the data. A more technical way we say it is that when you create a bottleneck you force an efficient representation to be learned. Now the uses of this are broader than just what we are going to talk about today or the lab of today word embeddings. Those are called auto encoders. Auto encoders essentially, for example, in the simplest form, you take an input and you try to reconstruct the input, except that the data passes through this bottleneck, this layer which is of much lower dimensionality. When you do that, you end up learning a representation. When that representation is stable and robust, it leads to a lot of benefits. For example, it is data compression in its simplest form, very efficient data compression. It can be also used to, for example, denoise data. You take data, you add deliberate noise to it, then you feed it in, but at the other end, you compare the output with not the noisy version of the data, not the noisy version of the image, for example, but with the clean initial original image, and you create a loss function based on the gap between x and x prime. So suppose you say X is the input, and let us say you add some form of noise, and then it becomes X tilde. X tilde goes into the autoencoder, and what comes out is Y. So you don't look for the gap between X tilde and Y, you look for the gap between X and Y, the original X and Y. And you try to do a gradient descent and back propagation to minimize a loss function that is the mean squared error between x and y. When you do that, you end up learning a very efficient representation that is stable. learning a very efficient representation that is stable. You come up with something that sort of learns to remove the noise from the data. That is a denoising autoencoder. Another example that we took is you can do a patching. You deliberately hide a piece of the image, you take an image and some little patch of the data, the image data, you sort of mask it and you feed it in. But then you again do a loss function with the original image output in the original image. And you basically force the autoencoder to reconstruct or fill in the blanks in the image. And that leads to also a very good reconstruction effort now this ability to not only reconstruct the original but also to fill in the gaps like to interpolate the gaps is very powerful and somewhat along the idea is one more thing which I didn't Sunder Rajanenan – SOMETHING ALONG THE IDEA IS ONE MORE THING, WHICH I DIDN'T ALLUDE TO LAST TIME IS THAT YOU CAN AMPLIFY THE RESOLUTION OF AN IMAGE Sunder Rajanenan – By using a technique called super resolution, you take an image, who's which is maybe coming from a smartphone of lower pixels. And then you create a super resolution version of it. Let's say that it is a five megapixel. You're trying to create a, let's say, a 20 megapixel picture of it. As you know, given pixels, it's very hard to create a super resolution version because that ends up introducing a lot of noise. And people have tried to interpolate various techniques to interpolate in between. And that leads to sort of a smoothing effect. One of the things autoencoders are very good at, as we are finding very recently, is that they can help you create super resolutions, versions of images in a very plausible way. So that is another thing you do. The next thing is you can do sparse representations. So given a data, if you build a neural net that has more, whose autoencoder in which the bottleneck layer encodes actually has more nodes than counter-intuitively than the input and the output, but you force most of the nodes to be silent on only some nodes to be fired that's your sparse autoencoder again it leads to sparse representations of the data so these are all ways of deriving efficient representations of the data now continuing the same idea but taking a slightly different approach to the loss function. When you bring it to NLP, you ask this question. Suppose I give the context of a word. Let us say the cow jumped over the moon. So jumped is the word, the cow over the, is the context, the two words before and after. Let's say two words before and after it's your three words before and after whatever you choose you take the surrounding words and you say this is the context and I want the system to predict the the the basic something like an autoencoder but I wanted to predict the word jumped right when you do that this is the continuous bag of words approach, you feed in the context and you expect the word to be the one whose probability is the highest. To do so, when you do so, you are creating a word embedding because the latent representation will be of a much lower dimension. For example, a typical vocabulary can be a hundred thousand word vocabulary in a corpus. Let's say the English language, if you take a sufficiently rich corpus, let's say the Wikipedia or something like that, or the Google news, you will end up with millions of words and or hundreds of thousands of words and that is exactly the lab we're going to do today. Then when you create a representation that is only in terms of 25, 50, 100, 300 dimensions, that's a very very efficient representation of the words. Remember, as I mentioned, in the original space, the words are all orthogonal. One-hot encoding means every two words are orthogonal to each other. The dot product between them is zero. The point of learning these representations is they capture somehow the semantics of the language. So we talked about the example like, for example, a banana and an orange should be near each other, but a banana and a tractor should be pretty far off. And that sort of a vector distance or distances in the vector space latent word embedding space should convey semantic distances. So the thing that got people very interested around 2014 and so was that when you use this kind of bottleneck architecture and you create a loss function the way we discussed last time, you learn these so-called word embeddings, lower dimensional representation of words, which are semantically rich. You can do things like a king minus man plus woman is equal to approximately queen and we'll see that today in the lab. Or you can say Italy minus Rome is approximately the same as France minus Paris. So you can see the relationship between country and capital. Or so you can do analogies, the sort of analogies that I suppose show up in standardized tests quite often. That Italy is to Rome, then what is to what is to Paris and then the answer should be France and so forth and we can see whether these analogies work or not and also if you encounter a word whose meaning you don't know you can ask for the nearest match and therefore you can infer the meaning of the word by looking at the sort of looking at the nearest match. Now all of this is impressive. It sort of works. I wouldn't say that it perfectly works. It has a few problems and today I'll mention those problems before I continue. One is the so-called alignment problem. There is a book that has recently come out. It is literally called the alignment problem. What happens with this sort of embeddings is you derive it out a corpus, corpus of text. A sufficiently large corpus of text is likely to have social prejudices, social biases, archaic beliefs and things like that. And in many ways, when you take a corpus like that, let's say, sufficiently large collection of web articles, websites, web pages, and then you build a word embedding, you train on your network to learn a word embedding, you sort of perpetuate and immortalize such biases, such archaic beliefs and stereotypes. So that is the alignment problem. And it's a pretty persistent problem and we should watch out for. We cannot get too excited about word embeddings. At best, it's an imperfect tool. At worst, actually, it can be pretty misguiding in its solutions. So with that being there, there are many techniques for doing word embeddings. The first one that I explained was the continuous bag of words. You feed in the context and you expect the word to come out. The second technique we learned is the opposite of that. You take this technique, turn it upside down. This time around, you're passing the word and you expect the architecture, this encoder-decoder architecture, to essentially predict all the context words. skip gram model, we built it by taking actual examples, positive examples of neighboring words for a given word, the context words, and negative examples, negative examples whereby just randomly picking words that quite likely are not going to exist next to this word. And those become your negative samples. Therefore, then train it using your standard cross entropy laws for a classifier. When we do so, you have the skip gram with negative sampling method of generating word embeddings. People usually just call it the skip gram model. A third way we learned, which was using the Co-occurrence matrix directly and that was the glove embedding. Now all of these are directly or indirectly related to the co-occurrence of words together or the co-occurrence matrix but this glove embedding tends to use it in a more direct way that we talked about last time. So I wouldn't go into glove. Glove has a more global representation of a word. In general, Skip Graham outperforms in most situations word to work. Word to work is the least performing in many ways. Glove does, and Skip Graham does, I mean, sorry, continuous bag of words is the worst performing, Graham does, I mean, sorry, continuous bag of words is the worst performing, generally. The Skip Graham does better than continuous bag of words and GloVe does in general much better than both of them. However, it's not always true and so you should try it out, play with it and see what works for you in your given situation. Now we'll come to the lab or the practical uses of it. See when you have word embeddings, you can either import a word embedding that has already been trained. So Google and so many Facebook etc, they have released fairly well trained word embeddings that has been trained on word embeddings that has been trained on massive corpus of data, different corpora of text. And these are general purpose corpora, they're not specific to our domain. Like for example, they're not specific to law and lawyering or medical and so forth. They're general purpose. And they capture the human language, the common human language very well. People have created these embeddings now in many, many languages and the number of languages in which these embeddings are available, pre-trained, is growing. So given the fact that you have these word embeddings, you could just adopt it as is and then you can do your work. Like you have a vector space representation of a word. Now like you have a vector space representation of a word now that you have a vector space representation of a word you can do just about everything you do in standard machine learning remember machine learning or er is essentially a box into which you feed in vectors vectors belonging to some euclidean space right some? Some are to the, you know, P dimensional vector, right? You feed into that a whole bunch of them. And so you feed in data and you can either do pattern recognition, for example, you can do clustering, dimensionality reduction, and so forth, topic modeling, and so on and so forth, we solve for text. Or you could be doing supervised learning, in which case you're doing classifiers and regressors and so forth. But the one basic requirement is everything finally must look, when it goes into our machine learning models, it must look like a vector. Every input must look like a vector belonging to a at the University of California, and he's been working on a lot of these things. So I'm going to give you a little bit of a summary of what we're doing. So, So having word vectors, especially of low dimensional representation and semantically rich, opens the door to a lot of machine learning, to classification, to this, to that, and many things. So today we're going to take some very simple examples. This is likely to be the easiest lab that you folks have done. Jay Shah, Dr. screen yes that is looking at my screen okay thank you so as you look at this screen uh you see that this is one file i put it there on your course web page the course web page it says where is it is it? Hang on, the course web page says, let me go here, labs and where we go. Yes, do you see word embeddings, the last item? It is this time, I've just given one file because it's a simple lab. I didn't see the need to upload the entire project and it also shows that word embeddings are very easy to use so let's come back and could you guys tell me if sorry the text is clearly visible is it legible on your screens very nice right all right so we importing. This is a very basic import. I don't want the notebook to get cluttered with all sorts of deprecation warnings and so on and so forth. So I removed that. Now one thing I had to do to run this, which was unexpected, is I had to install a library called PyEMD. Your mileage may vary, if you don't have to install all the better. If you have to then just go ahead and install this pip install PyEMD. Now we go and import spaCy. Just to recap, spaCy is our industrial strength high performance natural language processing library that we have standardized on. So let us review what we did. The line number four, it says spaCy load a model. If you notice this model is the English core of the large kind, it's a large model LG right so that means it comes with word embeddings and so on and so forth and just to wet your recollection if you do the small model which is quicker to download but that won't contain the word embeddings are we together guys so that is that and then next line. So what happens is when you load it, what do you get? NLP is what? NLP is a model. It is, think of it as a NLP, natural language processing model, spaces model. Now to this model, if you feed in a text, I put a text here, the cow jumped over the moon, the little dog laughed, see such fun, and the dish ran away with the spoon. You take a sentence like that and you feed it to the NLP model. What will it do? You get back a document. What is the document made up of? Document is a stream, is a sequence of tokens, isn't it? It parses that, NLP, spaCy will parse that and it will understand, it will go through the pipeline. Actually, a better way to think of NLP is not a model, it is the pipeline of things that you can do or the NLP operations that you can do. That includes, you know, cleaning up, cleaning up, checking whether it's a stop word, then lemmatization. Like you remember, lemmatization is taking every word and converting it to its standard form. Right. And thus we do that just to, do you guys remember what lemmatization was? Maybe I'll just add lemmatization here. Token. Somebody please say that again. It's the root word. So let's see just to better recollection, let us see what it means here. And yes, see that you notice that they have all become small letters and jumped has become jump, right? Most of the words remain the same, little dog. Laughed has become laugh. Ran has become run. So all the words are reduced to their root word, right? So they're standardized. That's the process of lemmatization. Then the other thing is we check whether the token has a word vector in this model or not. It does have. And what is the size of this? What is the norm, the vector norm, the size of it? They're all pretty much the same, but there it is. And sometimes a word can be out of vocabulary for example let's try the same word and I will now introduce a word the common large word that we tend to use hetero-ske-dasticity right suppose I add this word now what do we expect when we try to do this? Can you guess, guys, what will happen? Yeah, if you observe very carefully, the heteroscedasticity, is it an out-of-vocabulary word or the last token is out-of-vocabulary? So it is saying that this language model has never encountered the word heteroscedasticity. Are we together? And why is that? Certainly heteroscedasticity is a valid English word. It's a technical term. The point is, every vocabulary, if you remember, we're limited to more frequent words. If a word occurs only once or twice, we tend not to, or infrequently, we tend not to include it in our vocabulary, just to keep our TF idea vector of a reasonable size before we go into word embeddings. So I hope that just is a good refresher of what we did before. Now, if you want to know what is the dimensionality of these words, I just take this little bit of a thing. This, by default, spaCy will create a very large word vector of 300 dimensions. 300 dimensions has sort of become the standard ever since Google first released the word embeddings of 300 dimensions. But we're going to play around and see what other dimensionality is we can use so while it is good to be able to do that with spaecie in my experience a gensim is actually a little bit richer as a library when it comes to word embeddings it is good for both word embeddings and for topic modeling so if you remember last time we used as if spacey does what to work or what does it do yeah specie does work to work and glove book but you you didn't mention whether it was work to work or glove in that previous yes that is what in this particular case, actually, you can assume that by default space, if I'm right, is glove. Okay. It's glove, if I remember right. And you can check actually, it tells you some metadata information. So next we go to GenSim. GenSim and actually, so now we'll see in GenSim how you can play with different word embeddings. You can, for example, and in fact this is your lab assignment, I'm giving the example with Word2Vec. What happens if you change it to fast text? What happens if you change it to GloVe? Those are the things we'll try in this. So with GenSim, the first thing we'll do is, just to give you a taste of how we do things, I'll take a very small list of sentences. The sentences are broken up into words. Each line is sort of a sentence of sort. So you look at this so-called sentences, all the stop words have been removed. This is the toy data set. Common text is a toy data set that is there. And it is deliberately handcrafted by somebody with so many words. So how do you create a word embedding out of this? What you can do is you can take this vocabulary or sort of this corpus of sentences and you can feed it into the word to work constructor. The word to it comes in Jensen, do you see Jensen dot model. And by the way, import instead of word to it, you can do glove, you can do fast text and so forth. So should you do all of that, then you can start training it. The interesting thing with the constructor here is, this is a one shot, like it does everything. It builds the vocabulary. So let's parse it. Sentences is equal to common text. So what it means is, this is the array of sentences. Size is equal to 100. The size tells the dimensionality of the embedding space that you want, 100 dimension, which in this case is an overkill because we don't even have 100 words. But this is a toy example. A window, window is five. Could somebody remind me what is window? From last week's discussion. That sliding window around the... Yeah, exactly. And why do we keep it as odd number? So it's in the middle. Yeah, so you have symmetric, you know, so many elements to the right and the same number of elements to the left. if you have jumped and window size is 5 then the cow is on the left over there is on the right so you can pick up those symmetric amounts now minimum count is is a count like what is the minimum number of times a word has to occur in the vocabulary to be, I mean, has to occur in those sentences before you take it into the vocabulary. One means you always take it in, right? Workers is equal to four. That's just a way of how much you can, you want to parallelize and so on and so forth we'll ignore that and the thing is once you build a model now here obviously it's a very trivial corpus so it takes 17 milliseconds to build the vocabulary build a model word to weight model but in reality when you run on a massive corpus, it takes a very, very long time. And when it takes such a long time, you want to save it and you want to share it with your friends. So let us say that you are creating a corp, a special word embedding for some domain. Let us say you're creating a word embedding for legal for lawyers or you're creating it for doctors or you're creating it for the domain of human resources and learning and so forth. What you want to do is you want to pick, create a custom corpus which contains all the sentences that you do find in your domain and then train a custom word embedding. Now, what are the advantages and disadvantages of training a custom word embedding Jay Shah, Dr. Van Duzer of NIMS, The, the advantage is it may outperform a general purpose word embedding. is word embedding. The disadvantage is, as I said, it may, quite often it doesn't, because to train a word embedding takes actually a lot of data. And if you don't feed it a lot of data, it doesn't learn very well. And that is something I'm going to show you in a little bit in this lab. So be careful when it comes to these classical methods. Be careful and don't be assured that just because you have your own specific domain, you can create a better word embedding than one of these pre-trained word embeddings. And the reason is the pre-trained word embeddings has been trained on a truly gigantic amount of text. Whereas if you can't marshal that amount of text. Whereas if you can't marshal that amount of text from your sources, you might not be able to get as good a word embedding. Now, there's something else that you can do that sometimes people do. And that is you can take a word embedding, a general purpose word embedding, and then on top of that, you can augment the training with your custom domain, and that is effective. So what you're doing is you're doing what I would call sort of a transfer learning kind of a situation, a word embedding, adding more text to it that's specific to your domain. And so you amplify the relationships and customize it to your things. It's a last mile of fine tuning it. That is a good idea. So you can play with that and we will do that. So here, this is a simple, I wanted to show you how rich these libraries are. Just the constructor takes care of building the word embeddings. After that, you can save it and then you can load it and play around with it. Now, there is one thing, this word embedding model, it is a word embedding. I called it model, I could have just say word vectors. It contains in it a member variable called wv. These are the keyed embeddings, in it a member variable called wv these are the keyed embeddings uh keyed words would get vectors when you so there is a subtlety here this is specific to gen sim if you look into model.wv that is a much more sparse representation. It just contains the word and their vectors. It doesn't contain a lot of other things. So if you want to work with speed, it is a better idea to use the model.wv rather than model itself. You would get exactly the same semantics or the same results but model dot wv is a little bit faster much faster actually because you're working with things that load into memory much faster and more efficient when you're doing a lot of comparisons now comes a interesting thought one question before we move build for the common texts what did it contain what was the corpus in common text? Dr. Raghuram G. Literally what is staring you in the face here. Just that. Dr. Raghuram G. Just that. It's a toy dataset. Remember, it's a toy corpus. It is just to illustrate that you can do word embedding on just a few sentences if you so choose. Okay. Dr. Ragh choose. Okay. And also for it to run on those people's machines who are working with dinosaurs, like if you have, I don't know, a laptop that is five years, six years old, it will still work on that without GPU support and so forth. So if anyone of you have a laptop which has just something like eight gigs of RAM and a magnetic hard disk and a core i3 and so forth. So, you know, you always want to have some code that will work anyway. So this is that code. So now model that WV is a more sparse, is a more efficient sort of a high performance version embedded inside the model if you work with that that's the advantage it works faster there is however a downside to this the downside is while the model can be augmented remember i said that you can take a model and you can train it you can extend the learning with more sentences, right? You cannot do that with model.wv. That is an immutable object. The moment you call model.wv, what you get is an immutable representation, which cannot be further trained. Are we together, guys? That's the difference between model, the big object model, and this very compact and more memory efficient representation, which is model.wv. Wv stands, of course, for word vectors. Are we getting this subtle point? It is just a speciality of Jensen. There's nothing theoretical or deep about it. It's just a way to get your work done faster. But the subtlety is one is immutable while the other can be further trained with new sentences. So in this very simple word, if you notice the word computer appears here, it's next to human interface, computer is next to user and system and so forth. So if you go into this and you say, okay, give me the word computer, what does it look like? It turns out you asked for a hundred dimensional representation, so well, you get 25 rows. This is a hundred elements, a pretty big array of numbers. The only reason I'm showing you this is to show you that whether these numbers may or may not mean much to you just eyeballing it suffices to say that it does look like a single vector isn't it it looks like a vector in a hundred dimensional space all of these elements are floating points real numbers right so it is real to the to the hundred hundred dimension real space i see so quick observation here is if you look at the corpus the common texts that didn't even have 100 words so you literally actually expanded it yes yes you have expanded it which is why i mean in reality you would never do it uh it is just illustrative right and obviously the learning here is not very efficient at all in any way also you can't really learn from such a small corpus it's it's a toy exercise just as a warm-up so now let's go to something a bit more realistic and practical in life. So as I said, it is for your purposes in NLP and machine learning with text, it is usually wiser in the beginning to start with a pre-trained model. This again goes go or sort of goes to the point of transfer learning that I keep talking about. So go with a pre-trained model and all of these libraries come with pre-trained model. I think if there's one thing we have learned it is that most industrial strength libraries now give you a lot of pre-trained models that you can use in every domain whether it is image processing and we are talking about VGG and AlexNet and Inception and so forth. Or we are talking about transformers. We saw a lot of pre-trained transformers. Very hard to train, but train transformers, birds and all that. And here in the space of word embedding, so you have a lot of word embeddings that people have created. Now a small list that Genim comes with is this you can go to gensim and you can you can say give me a list so gensim downloader what it does is that it downloads from the internet from standard locations some pre-trained word embeddings right so it's actually a short list of what comes built in with it, but it doesn't prevent you from going and getting more word embeddings. And by the way, people keep coming up with new and new ways of doing word embeddings. In the class course portal, I've given an entire section on word embeddings and people have awesome word embedding papers and so on and so forth. Very active area of research. So anyway, so here it is. Given the models, you can get all the list of the models. So what I'm doing is I'm getting the list of the models and printing it. This code is nothing fancy. It just prints this table, right? In other words, this models object that is the list of models, it just prints it out, pretty prints it out. So this is what I'm doing here. And if you're used to the formatting, string formatting in Python, this should all look very, very straightforward to you. There's nothing extraordinary about it. And the only reason is this line is surprising is because there is at the end, there is one model, which is not a model. So I threw that out. I filtered that out. So now that we have all this model, you have a choice. Which one should I use? And so i'll tell you a little bit about what you can do you see here do you notice that i'm starting with download or that load glove wiki gigaword 50. now by the way this word download is smart if you have if that word embedding exists on your machine it won't go and and redownload it every time. It will just pick it up from your local directory, from your local host in a standard location, wherever it has stored. It will just go and read it back from there. So it is pretty efficient, but nonetheless, these models are huge. So for example, GloVe, Wiki, GigaWord, The way to interpret it is that, what is the algorithm? So let's go and look at the model. The first word is the algorithm. It is the fast text, one of the algorithms we talked about. Remember, fast text deals with word pieces, subwords, right? A character level of things. Word2vec, we know, concept net is another. Word2vec, glove, concept net is another. Word2Vec, GloVe, these are the big guys. These are the big word embeddings we talk about. Wiki, Wiki News sub words means Wiki News is, Wiki Foundation has many, many sites, Wikimedia, Wikipedia, Wikidisc, Wikinews, Wikibook and so forth. So it seems that they have picked up the wiki news here uh this one i'm not familiar with it so i will i'll be silent on that what to work uh russ corpora this is another there's google news wiki gigawatt and so on and so forth glove etc etc so the last digit the last is a number number tells you the dimensionality of the embedding. So you can see that the dimensionality seems to go from 25 all the way to 300. 300 is about the limit. You tend not to go beyond that. Sometimes rarely you see embeddings of 500 dimension also. But 300 has more or less become a de facto standard these days when you are using a pre-trained model. And most of the time people just end up taking the, let's say the Google News embedding or the glove, you know, this hundred dimensional gigaword embedding and so forth. So we'll play around with some of these. What I have done is in this notes I have commented these three out. I suggest you play around with each of these. A word of caution, the Google News 300 dimensional word embedding is 1.5 gigs. So in your home internet connection, it might take a few minutes to download and make sure you have that much of disk space. The other ones are smaller, but they're still big enough to download. So be aware that these are relatively big files, like all AI models that we have been encountering in this course. These are relatively big files that get downloaded. And each file, roughly speaking, logically is built of a word, its ID, its index number, let's say one, two, whatever it is, and then its vector representation. That's all it is. So with that, I've taken one particular example. Asif, is there a frequency at which these, like this is being made available by the creators of Gensim, right? Do they go back and keep updating these word embeddings? Actually, these are not made by Gensim people. These are trained by a lot of people in the open source community who have made it available. Gensim is just giving you a helper utility to download those. Okay. That's all. So the answer to that is, see, this is classical natural language processing. As you will realize, the state of the art has moved forward. Right? So we do things when transformers and attention comes in, you'll realize that in many situations we want to do things differently. People are still creating word embeddings of this kind. These are called like context-free, like basically context unaware agnostic word embeddings. There's still a body of research that is happening to create better versions of it. People keep training. The amount of activity is enormous, but all these word embeddings, they all look the same. So you'll notice that somebody has published a paper and created one more better word embedding. It doesn't take many lines of code to download and start using the word embedding. Okay. And generally what happens is that is why if you go and look at your course page, let me take you guys there. If you go to the word in NLP word embedding section, I want to pay attention to this. Guys, do you notice that i have the initial papers of glove etc deep contextual word representations and so forth nlp but these are the thing but if you go and resources on nlp nlp books and resources you will find a curated list of embeddings this curated list of embeddings of embeddings. This curated list of embeddings, do you notice that there is a lot of work that people have done on word embeddings? Do you see what a long list of research paper there is? Right? And it goes all the way down to, well, the last one seems to be on 2019, November 2019, spherical text embedding. So there you go. Very, very rich amount of literature. What does it take for you to go and get it? Nothing. You just have to go here. You see this training code, the code is is available pre-trained models so what do we need to do suppose you want this guy's embedding who say this one is sentence embedding you click here and you notice that if you notice carefully on my bottom left-hand corner there is already a download button frame did can you see that yeah yeah so I won't won't, I won't download it because I don't need it. So all of these embeddings are available. Go pick and choose. You don't have to go through Gensim. You can just go to the researchers page and they are all making it available. You can just use it from there. Premjit Sidhu, Okay. Thank you. That's that. And it's a very rich world actually. Like everything in research, any one topic, right, there's a tremendous amount of creativity and many PhDs have come out of it. And every once in a while somebody, just when you thought you can't do anything new, everything has been done. Somebody will do something quite innovative. So it's going on. So now, anyway, I'll take a word here. Consider the word vector. So let me actually run this. You notice that when you run it, it will load this, even though it has been downloaded on my machine, it takes a little bit of time to load, guys. And so be aware of that. when you run on your machine and you just see a star and nothing happening it doesn't mean that your code is not working it take in my machine it took 14.7 seconds on my laptop your mileage will vary but wait a bit patiently then a pretty large model word embedding has been loaded into memory. So I take a word as an example, morning, and I ask this word embedding, find me the most similar words to morning. Just to see if semantic, if it is semantically rich. Remember if you were using TF-IDF or Vectorizer, or we were using one hot encoding or things, I mean, if you're using one hot encoding, would you agree that no two words are similar at all? The similarity between any two words is zero. So you can't say, you can't ask this question in one hot encoding at the bag of words of presentation. What is the most similar word to morning? Well, every word is equally similar and not similar at all. But with word embeddings, you can ask these questions. So let us see when you ask for similarity, it will come up with some half a dozen or 10 or whatever it is, a small number of, and you can give the number how many words that you want in the output. So it has found some words. So the word most similar to morning are, afternoon, evening, day, night. Look at the first few words. Would you agree that these look similar? Intextually, they're related, they're very related. They're not synonyms, but they are related. Likewise, and by the way, you can play with any other, does anybody have a word suggestion? Let's say cat. It turns out that dogs, both of them are pets. Rabbit is the next thing that people like to keep as pets. And it shows up here. Or we could take, I don't know, what can I take? Hammer. Now let's see what is, I've never tried hammer. Let's try that. Arrow, stick, throwers, sticks, metal, bolt, nail is somewhere there. Nails. So there we go. So anybody else has a suggestion we can... Unicorn. Unicorn. It's not found in the vocabulary for me. Okay. Can you try happy? It kind of throws some interesting words there. Yes. So look at this unicorn. Yes, it seems to have thrown Medusa, dragon, beast, lion, mermaid. Yes, all imaginary creatures. Ninja, vampire. Or the heteros, radicaticity. That is, I doubt that would be in the vocabulary, but you want to give it a try? Let's try that. Word not found in the vocabulary. By the way, that is me putting a try catch block. If you don't, it'll throw an exception saying that word is not found in the vocabulary. So let's try it as it was debits airtime freebies etc etc so you can play with it I'll just leave it with morning and I unique also what the one time which one water vodka you seem to be fond of vodka ketchup beer cognac the first one I don't know why ketchup is similar to vodka but cognac tequila distilled champagne whiskey you see that these words are related they are all alcoholics except for we're not found in vocabulary just re-running that cell with a different word. That might be the poor man's Bloody Mary. Oh, ketchup may be the poor man's Bloody Mary. By the way, there are two kinds of similarity measures. The other one is cos matrix, cosine matrix, cosine similarity. Let's try a luck with that. Vodka has nothing found that I'm surprised with. Let's say unicorn. No, I don't know. Oh no, malt, course malt rather, multiplication. Medusa, dragon, bee, slide. So line so the couple of similarity measures you can do the Euclidean similarity you can do the cosine similarity and so on and so forth so those are the options that you have a limited unicorn let's find out the similarity between king and queen. Do you notice that it comes out to be 78% similar, right? The ordinary Euclidean similarity and what happens if I do the cos, cosine similarity, then it becomes, oh, what do I do? Actually, I'll have to look up the exact syntax for this. Let me leave it as that. Would I get word not found in vocabulary for unicorn when you get a nice list like Medusa, dragon, beast, etc? No, no, no, you don't find. Actually I made a mistake. Word most similar is coming out right word not found will happen only when you put something that is truly not there. You're asking why I got that at one time. Okay. Okay, what was? No, no, because I made a typo and that caused an exception. Was there? Yeah, I just made a typo. And caused an exception. Oh, was there? Yeah, I just made a typo. And so that made it go into the exception block. It isn't real. So unicorn is very much there in the vocabulary. So between man and woman, as you notice, 88 and 78% similarity between king and queen. Perhaps ordinary people are much more, should we say, having better married lives than kings and queens? I'm kidding. Yes. So and then I talked about this fact that with word vectors, you can do this vector arithmetic. So you can do king minus man plus woman to see what happens. So this is it. You're seeing the positive terms are, you can imagine the sentence to be plus king minus man plus woman. So the positives are king and woman and the negative is man. And then you ask, if I give this kind of a syntax, this is your basic analogy experiment, right? So a king is to man what is it to woman that sort of thing and see what what is the answer it throws up do you notice that it throws up queen as the first answer can i see your text before that has the unicorn for another moment please this one yes thank you let me know when you're done looking at it. Can you point out where the typo was? Oh, the typo that I had made was this. Let me cos mat rather than mul. It was a typo on my behalf. And so we ended up with this, a word not found in the vocabulary. So the thing is there are two distances, Euclidean and cosine distances. Cosine is given by cos mul. And then obviously if I give either cos mul or Euclidean, if I don't make the typo, I don't get the exception. Remember, except is causing this sentence to be printed, a I don't get the exception. Remember, except is causing this sentence to be printed. A word not found in the vocabulary. Did you get that? I'm still getting that word not found in the vocabulary. Where are you getting it? Are you looking at my screen? Is everybody seeing that? No. Where are you getting it? Are you looking at my screen? Is everybody seeing that? I think she is trying on her machine, sir. Oh, you're trying on your machine. And did you include this? Are you using this word vectors? Yes. I'm using that same first one. All right, let's take it offline guys. I'll help you Kate after the break, maybe I'll help you with that and see what in the world is going on. Should anybody else is able to reproduce the problem? It works for me. It works fine Kate, when I'm playing Unicorn. So Kate, we'll take care of it in your machine in the break. So guys, this is the thing that got people initially quite interested. A king minus man plus woman, as you can see, is the queen. It suggests the queen and likewise are comparing Rome and Paris what are they they are both capitals so here's a thing if you take Italy and you subtract Rome but you but you add Paris what do you get you get the country France you notice that the first match is France isn't it interesting a word to work or a representation is at the end of the day it is nothing but a rather compression you know summarized representation of word vectors based on the corpus that it has been trained on and yet it seems to be semantically rich right it's a quite an interesting thing that's's what got people very excited a few years ago. So you can, by the way, try a different similarity measure, cosine mult. So there's the Euclidean and the cosine distances you can play around with. And the same thing you can do with the cosine and you get pretty similar results. Then the other interesting thing you can do is you can find the odd man out. So if you guys look at this cow, horse, bird, giraffe, donkey, buffalo, ox. I just wrote down the names of some animals. Can you tell which is the odd man out? Bird. It's yes and lo and behold that's it has it has on the other hand come to the conclusion that it's Buffalo two-legged bird versus the four yes that's true so and this shows actually the limitation of this kind of learning just from a corpus. What happens is that a buffalo is not commonly used in a lot of the English literature. Actually it is mostly the Asian countries that talk of the buffalo. They would consider the cow and the buffalo together. In my childhood on my farm I used to have both cows and water buffalos so obviously if I have to look at this list based on your context I would consider giraffe to be out of place because on my farm I had everything I had a cow a hawk I had cows I had horses I had birds I had donkeys I had buffalos and I ox. But what I didn't have was giraffe. And so it depends upon how or what corpus it has been trained on. So suppose I now make it a hammer. Now let's see what happens if I make it a hammer. It will immediately identify the hammer because the hammer is really far off from those animals isn't it or suppose you take trump and see where he stands well it turns out that trump is an animal after all no i'm kidding No, I'm kidding. I'm joking. So, okay, here we go. That explains that sentence similarity. So you can also see the similarity between sentences. You give a sentence as a list of words, a list of tokens and two sentences as two list of tokens, and it will compare them. These are again things you can do with word embeddings. So here it is. This is the word mean distance. What is the distance between the two sentences? It turns out that you have to know how to interpret it. This is a much smaller distance. It was the best of times. It was the worst of times, the famous starting lines of Charles Dickens' Tale of Two Cities. So you get a much smaller distance. Then I take another sentence somewhat related. The first sentence is it was the best of times, it was the worst of times. And the second sentence I have is, which is not Charles Dickens, it is something that I wrote, once upon a time citizens could elect their president. And so, the word distance is much longer. And then you can take another two sentences. The first is, this is a very technical sentence, word embeddings are efficient latent representations. And the second sentence is, in a previous era, citizens could elect their president. And so you can see that the distance is much larger between these two things, clearly. So let's go through this. It is the best and worst of times. There's just one word difference. You can imagine that the mean distances would be close close and also best in words to the extent that they're superlatives, they're pretty close in the embedding space. So they are close. This one though, there's significant distance. We are talking about citizens and president and so forth. So the distances increase, the distance increases. And then I take two sentences. One has absolutely nothing to do Jay Shah, Dr. Anand Oswal, Ph.D.: One has absolutely nothing to do with the other. And you notice that the word the mean distance between the sentence increases. So do you see guys how you can compare. So if you had to pick. Let's say that this word. If I took two sentences, one from each subject, one from chemistry and one from politics, or two sentences from politics and one from chemistry, can you tell them apart? Can you group them together just by looking at their distances? right? Can you group them together just by looking at their distances? The distance of the chemistry sentence to both the politics sentence would be much larger than the distance between the two politics sentence, isn't it guys? So you would conclude that the two politics sentences are about the same subject. So that's how you can use word embeddings, quite useful. Now, sometimes what happens is, here's one thing that you can do. Suppose you want to tell if two articles are, like if you have three articles or something like that, and you want to see how similar are the articles to each other. What you can do is you can obviously tokenize, extract the keywords, get the article into a list of words. So I have baby articles. My article contains just two words, glowing bulb. The other is the shining sun and the next is the hammer nail. So when I compare the word embeddings for this, what does it say? Sorry, the word similarity, similarity between the words, we realize that, and the word, the method that you use is n similarity. And different libraries have different methods, all NLP libraries give you something. So bulb, the first sentence, which is about bulb and the sun, please give you something. So bulb, the first sentence which is about bulb and the sun, do you notice that their similarity is much higher, twice as much as the similarity between the sentence about the sun and hammer? Yeah, the bulb sun similarity would probably be even higher if bulb didn't also have a meaning of a plant bulb. That's true, right? What happens if we replace it with lamp, glowing lamp? Well, it's more or less remains the same. Well, there we go. So anyway, what I'm trying to show you is that word embeddings have a rich semantic structure that you can exploit for your tasks. One easy way to find out if two articles are related is just by looking at, extract the keywords and look at the similarity. One thing you could could do which is very useful suppose you have a lot of articles it's sort of a rough measure they all deal with the subject X what can you do you can create extract all the keywords that you find in those articles then create the average the the mean vector out of those keywords the centroid of those vectors, those word vectors, you can find the centroid, which would be the mean. Once you have found the centroid, what can you do? If you get an article and you want to determine, so suppose you have two subjects, subject A and you have found the centroid vector here and subject B and you have found the centroid vector here. And what you want to do is you get a probe, you get an article and you're proving, does it belong to subject A or subject B? So how would you do that? Would somebody suggest? One more time. What's that? Restate please. See, suppose you have already learned two subjects, let's say sports and let's say science. So you take all the articles that belong to sports, you extract the keywords out of them. Now each word will have a word vector you find the centroid of those word vectors what will you know you will get to the essence the central location of that subject sports now you go to this all the articles that belong to signs once again you extract all the keywords take all the nouns and belong to science. Once again, you extract all the keywords, take all the nouns and so on and so forth. Then what do you do? You find the word vector, take the word vectors and find the centroid of this subject to science. So now you have two vectors and one vector pointing to the centroid of sports, one vector pointing to the centroid of science. Suppose I give you a document, a new article, and I say, tell me whether it is science or sports. How would you do that? Anyone? Compare it to the centroid, like take the centroid of the new document and compare it if it is closer right what you do is you create this document vector you take the keywords you find the obviously the centroid or the the mean vector of these this one and compare it how do you compare it just using similarity right find is it more similar to this or more similar to that so whichever one it is more similar to you can classify as belonging to that subject am I right are we getting it guys it's a it's a very simple way of determining what a particular article is about. So, Asif, can we use the LDA? No, see, that is a different approach, isn't it? If you're doing latent-dirichlet allocation, you are doing a topic modeling, two-topic modeling, and so on and so forth. Oh, okay, got it. I'm giving you a much simpler, a different method, which is much simpler. Now, whether it is effective a much simpler a different method which is much simpler right now whether it it is effective or not is a different question but it certainly is a viable uh approach you just find the centroids of the subjects and then you allocate you determine whether a word vector an article belongs to this or that you see how straightforward it is now how do you extract the keywords a very simple brute force approach would be to use the nlp remember that part of speech so keywords are what parts of speech typically verbs now verbs and nouns right pick up the verbs and nouns right you build a knowledge graph out of a subject, verb, object. Right? So you can pick up the verbs and then out of the verbs you have to be careful. You don't want to take very common words. You have to filter out the stock words and then those verbs. Then you have, right? Then you have all the keywords of the subject. Once you have the keywords of the subject, all you have to do is create their mean vector or the centroid of all the words. It is a rough and ready way for you to tell whether an article belongs to it or not by just looking at the distance of that article to this centroid, to this mean vector you see how it is and so guys this has direct bearing on the project you are doing hint if you i hope you guys are making progress on the project because i'm going to start releasing uh pieces bits and pieces of solutions to the project soon. So use that guys. So that is word similarity. Now looking at a vector, by the way, now you can choose to just see what a vector looks like. So how do you see a vector? Actually, let me break it up into two parts. into two parts. Let me split it. Split this and X. What does it look like? Of course, it looks like this, you know, which we took as 50. I think we took a 50 dimensional embedding to play with here. Did I not? Yeah, 50 dimensional. So you would agree that this vector that I have come up with is 51, 2, 3, 4, 5, 6, 6 times 1, 2, 3, 4, 5, 6, 7, 8, 8, 6 here. 48 plus 250. Yes. And so one thing is there when you get a word vector, it is not normalized. Remember, these vectors are not unit vectors. Are we together? So in the latent factor space, one, two vectors can be collinear. But they may be of different sizes or almost collinear and they may be of different sizes. are almost collinear and they may be of different sizes in that case the euclidean distance would be pretty significant even though the cosine distance would be very low do you see that if two vectors are collinear one is let's say 10 times the other you you still have a significant euclidean distance to traverse but but the cosine distance, the angular distance between them is very, very small because they're collinear. They're close to zero. Am I making sense, guys? Yes, sir. Yes. Good. Anybody has doubts? Please stop me at this moment. So one of the things I wanted to show you is given a vector, if you do, how do you find a rough and ready way to find the magnitude, or rather the square of the magnitude of a vector, is simply to dot product it with itself. So if you dot product it, then you see that it is actually 26.3. It will be some arbitrary number. You do it for horse and it will be something else. Now, if you want a normalized version, you don't care for the Euclidean distance. You care only for the cosine similarity between words. One of the things you can do is you can use the norm. You can use a normalized version of the vector. When you take a normalized vector, what is the normalized version? What does it mean to normalize a vector, guys? Who would like to enlighten me? Is it like same scale? It adds up to one. All of you are seeing the right thing. I would like to hear it in a more mathematically precise manner. Magnitude is one. Magnitude is one. So what you do is you take the vector and you divide it by its magnitude. Then you sort of scale it down in such a way that it becomes a unit vector. So when you have a unit vector and you dot product it with itself, you will get one. So another way roughly to think about it is that it is a point on a sphere on a 300 dimensional unit sphere right all the words are little points on the sphere and the only thing you care about is the cosine distance between them that brings me to the question what the cosine distance between two vectors? The shadow of one vector onto other. The projection. A dot B by magnitude of A. The cosine similarity of two vectors A and B is A dot B divided by the magnitude of a and the magnitude of b because a dot b is a magnitude of a magnitude of b cosine theta angle between them so the similarity the cosine theta right is the measure of similarity the smaller the angle the more the similarity so the cosine similarity is a dot b divided by the magnitude of a and the magnitude of b let's scale that normalize basically the dot product between the normalized a and normalized b that is cosine theta that's a measure of similarity and distance when we speak of cosine distance it is one minus cosine theta right so one minus similarity is distance so that is the cosine distance so yeah there you go whether you speak of cosine similarity or cosine distance it's a matter of convention one is just one minus the other so if all the points are dots if all your points are dots, if all your words are dots, are points on a unit sphere or hypersphere, then you can, the only meaningful notion of distance you're left with is your cosine distance. A similarity, the angle between two points. Together. That's that. So the next thing, and this is the last thing that I'm going to talk about, because today is a small session. So you say, okay, all that is very fine. I have been using other people's embeddings and I've been using it. What if I want to build my own embedding from scratch? So I'll show you the code of how you build your embeddings from scratch. So what do you do? I just, so by the way, Gutenberg, how many of you know what Gutenberg is? Gutenberg Printing Press, of course, is the first printing press that started the whole revolution of books and learning. But there's a Project Gutenberg. What Project Gutenberg does is every single book that is beyond copyright, so a book that is more than a hundred years old by definition is beyond copyright. So it takes every book that has either been donated to open source or is beyond copyright because of being too old. They have collected it. Most of the great classics are there. They have collected it and they have cleaned out the text, tremendous amount of effort, cleaned out the text and made it available in text and EPUB for your e-reader and HTML format. And it's one of the most wonderful projects. Everything legal and it is the whole spirit of creating the commons for people, shared commons for people. So from that, we are going to take a few novels. These are all the great classics at one time for those of you who are quite young. Jane Austen was a British writer, very famous, she wrote lovely books. Emma, Persuasion, Pride and Prejudice, Sense and Sensibility and so on and so forth. Blake was a poet, right? Bible hardly needs a King James version, hardly needs an introduction. Byron was another great poet and writer. Then, Burgess I've never encountered, at least I'm not familiar with. Carol, Alice in Wonderland, of course, my favorite. And then you have all this Chester Senn humor and so forth. Then Merville is an American, of course, writer. Milton's Paradise Lost and Shakespeare's Hardly Needs an Introduction. And in US, of course, Walt Whitman's Leaves of Grass Hardly Needs Introduction. So these are the great classics, as you you know and they are freely available as text so therefore you can use it for your natural language processing use without worrying about copyright violation. So this is what I've done. What I have done is I've taken this and by the way this NLTK comes with it built in. You don't want this you can just go to the Gutenberg project and download the text file and then yourself break up the text file in play but I've started with that. What I did is I took three of the novels of Jane Austen, Emma, Persuasion and Sense and Sensibility, these three novels are these text files, and for each of these I got the sentences out of them and I collected all the sentences. So this code, I'll let you just eyeball it for a moment, which you'll realize that I'm not doing it. The only thing mysterious here is Gutenberg is a library that is part of NLTK, a module. And if you call the sense, which stands for sentences, it will take the file, the text of the file, break it up into sentences and give it to you as sentence. And each sentence, it will give you as an array of words are we together and so just for fun i printed out the first sentence and it says emma by jane austen 1816 as you can see it's a very old 200 year 200 year old story right and still a great favorite amongst people. Okay, so let's go process this thing. Remember guys, when you get text, a lot of the time in NLP goes in cleaning up the text. So what will the text have? It will have words and capital letters, it will have punctuations, it will have strange symbols and whatnot, Right? And the words need to be made into the standard form, lemmatized. You need to throw away the common stop words and so forth. So that is what I'm doing. This by now you should be familiar with. It turns out that if I want to use spaCy, I need to rejoin all the words into a common sentence and feed the sentence into NLP. Otherwise, it's very inefficient to feed it word by word. And just for efficiency, I've done it like this. So actually, I could have been even more. I could have just joined all the sentences also together and made one long but I didn't do that. I kept it separately a sentence for reasons that you'll see. So I take each sentence and I clean it up. I take this token, go to the lemmatized form for the token and keep it only if it is not a stop word, means common words. And if it is not in, you know, this square brackets and underscores and so forth, and it is not a punctuation, exclamation mark, full stop, comma, etc. And it is not a symbol. So I'm just doing basic cleanup, guys. And you'll realize that this weeds out a lot of the junk. And then I print out one of the clean sentences, Emma Jane Austen, 1816, right? So if you compare this with that, you'll realize that this unnecessary begin bracket, end bracket, these are not really words, disappears, by is a stop word, it disappears. So you take the clean text and then I randomly took one more text. 152 has no meaning. You can put whatever number you like. And here comes a sentence that seems to make sense. Emma think good probably means Emma thinks good or rejoin Mr. Woodhouse understanding and so forth. These are the words in the text. Then now comes, now that you have words separated out in the sentence, now you notice that I'm creative word, by the way, word to it. So your assignment is to do glove and fast text on it, homework. So if you can please note it down, take the same notebook, but instead of doing word to work replace it with glove these are just one word changes and see how different or how useful it is by do you get better results if you use a glove do you get better results if you use fast text are we clear on the homework guys yes yes by the different word embeddings try glove and fast text instead of word to beck. That's right. That's all. And it is as simple as just changing the construct and a few of the arguments, that's all. So I'm giving you the sentences. Now, when I'm saying size is equal to 25, I'm saying make it of 25 dimensions. Why? Why did I choose such a low dimensional representation? Because you know, I don't have enough text. The three novels of Austen is still, Jane Austen is still not enough text. You typically look for like a ginormous amounts of text in your corpus to train a proper word embedding of 300 dimensions and so forth in general. So take a small size. By the way, if you take it to be 100, you can play with it 200, 300, you can play with it and see how you get. The other thing I did is notice that minimum count I made it 10. And this is something you play with. Why did I take 10? I just said that if words don't occur, at least 10 times, they're not useful to me right because you I happen to know what Jane Austen's novel contain so rare words are likely to be typos it's literature it's not it is not some science or stem field or something in literature the the writers deal with they create the craft with simple words, straightforward common words. So there we go. So, and of course you can save it and then you can load it back. This is the point that I was making guys. Do you notice that word2vec, I'm not using the model directly. I'm using the model.wv, which is a more immutable, but faster version of the thing, a more memory efficient version of the thing. And so what is the difference, if you remember the story it's about Jane Austen's novel are all about young girls and their suitors, the people they fall in love with, are people who fall in love with them, right, and the whole journey of their getting married and so forth, all the tumultuous romantic years of young people. So, well, that is that. So I'm taking two words, mother and daughter, as you can imagine, in all of this mix, always a mother is involved. So the similarity between mother and daughter seems to be pretty high, 98%. On the other hand, if you take two random words, I just took injustice and Elliot, the similarity is low, it's 0.56. Actually, it was hard to find words which were even less similar. I couldn't, every two words seem to have reasonably high similarity. So from that I concluded, and if you look at the word mother and you say, what are the words most similar to it? You get encounter, husband, errand, acknowledge, et cetera, et cetera. Not very impressive. Anybody feels that these similarities are impressive? Not really, no. Not really, right? And so let's go and play with this suppose I change this to instead of that the common dimension that people take is 300 dimension and quite often they take minimum count is just one and let's see what happens if you take default values like that it takes a few vector well on my machine it just took 1.58 seconds to train. Why? Because the corpus is very small. Similarity between mother and daughter is 99%. Similarity between, it turns out injustice and Eliot, Eliot was not a paradigm of injustice by the way in their stories, but for whatever it is, it's 78%, not a paradigm of injustice, by the way, in their stories. But for whatever it is, it's 78 percent, not a good sign. And if you look at the words that are very common to mother, this time, it is actually reasonably good. It has come up with sister. Moment Mary is a character there. So you can play around with it. But what is disturbing is the word admiral has up to three significant digits, the same similarity as sister to mother. You notice how all the words seem to have more or less the same similarity. So it is surely not doing a very good learning. And this is where you play with it and if you do a different similarity measure the cosine would then again you come up with this sister moment very pretty was the same answer but all of them every single word seems to be similar to the main words if you want to see what are the words in the vocabulary of your embedding you can always of course call it vocabulary and check it out. And what does a vector look like? This is how a vector looks like it. This is your 300 dimensional vector, which is very, very hard for one to play with. And now let's try something different. Go back and change. I will change the minimum count to let us say at least Okay, maybe I'll leave it as that and let's see what happens if I make it something like, pick a number guys. Any suggestions? 20. 20. And I'll leave the min count there. Let's see what happens. Mother and daughter remain very similar, but Injustice and Elliot still are rather annoyingly similar. Mother is similar to sister. Well, mothers tend to deprive,rive I'm sure every daughter feels that mothers deprive them of something and during their growing years and there you go mother sister deprived what is the word vector it is this and by the way what other words are similar to with this sake I don't know why law mother-in-law I suppose comfort has been writing somewhat useful you notice that if you reduce the dimensionality things get better actually and obviously you can't make anything from the vector if you if you reduce the window to three then the Elliott just yes and this is another thing to plan maybe the size 200 yeah so let's see what happens so let's go back and see what happened uh obviously mother and daughter remains pretty similar but you need to increase the size 200 oh you mean you want me to increase this size 200 and minimum count to 10. yeah minimum counter 10. yeah so you can play around this is great that you're playing around with it and you notice that the word vector similarity has become this uh now this is good i moment etc these are all the words that tends to keep coming up and if you do it to, actually what I'm trying to do is reduce this value. If you were trying to reduce this value, let me take about 20 for what it is worth. This has reduced a little bit, but mother and daughter have come rather close. So these are the games you can play. I'll let you play with these. with that this is it. Today was a simple short day. This is your lab. So guys go play with it. Now what I will do, I'll take another 10-15 minutes. We haven't had a break today. I'll just summarize it. This is classical. Everything that we did in the last four weeks is, I would still say, the before transformer world or the classical NLP world. It is using deep learning, yes, but it is not using transformers or attention. In the weeks that will come after Thanksgiving, therefore in the month of December, I suppose, we will be doing natural language processing with attention and with transformers and those are game changers this current way of doing word embeddings has one limitation take a word like Apple Apple can be close to orange, it makes sense. But Apple can also be close to computer. Now the thing is that also makes sense. But in a world, in a sentence where Apple is close to orange, it shouldn't be close, computer shouldn't be nearby. Whereas in a sentence where Apple is close to computer, these fruits oranges and bananas should be pretty far away, isn't it? So in other words, apple is a homonym. It has multiple meanings. Same word with multiple meanings. What you really need is a differentiation between the two meanings of the word apple. And those two meanings should occupy different places in the embedding space. And the word, which word, one you pick, which meaning vector you pick has to be based on context. That context awareness is what you get when you use transformers and so forth. And so as we go from the next week onward, you'll notice that we will retrace much of what we have done in classical NLP, what I call classical NLP, by the way, it is my word, it is not a word that other people use. But this what I call classical NLP, now the pre transformer world, all these approaches will retrace it with transformers. And when we do it, you will see how or what is the value that transformers or attention bring to the table and how they change the game. Having said that, it doesn't mean that we have wasted four perfectly good weeks. Everything that you have done is widely used and should be used quite a bit. You should never abandon these classical approaches. It is, remember the no free lunch theorem, nothing guarantees that any one approach is always better than the other. Also the efficiency part, the computations are very very very cheap and fast with word vectors. So do use those and even though these are sort of universal word embeddings, not context specific word embeddings, they are still very useful. So with that, guys, I'll end today with the everything pre transformer. To summarize, in the four weeks of NLP, we covered the NLP fundamentals, we did topic modeling, we took a digression and we did the whole series of bottleneck methods. In particular, we learned about autoencoders, we learned all sorts of autoencoders, the standard autoencoders, which are the vanilla autoencoder, the denoising autoencoder, the patching autoencoder, the sparse autoencoder, the denoising autoencoder, the patching autoencoder, the sparse autoencoder. There's one more, the contractive autoencoders that I didn't cover, but it's there. What it does is that it comes up with a very stable lower dimensional representation of data. Then we did variational autoencoders, which are actually in many ways fundamentally different from other autoencoders, even though they share the same architecture of encoder-decoder. In particular, they are not discriminative models. There's a generative models. We talked about it and we used the notion of KL divergence of how you can approximate, you can take the notion of KL divergence of how you can approximate, you can take the probability of the Z given an input. And if you make that into a mixture of Gaussians instead, and you use that to approximate the postion, you'll be given an internal representation, what would be the output? Well, obviously I'm using words like prior and posterior. They all come from Bayesian inference. We covered that. I won't cover that in more detail. It gets a little bit technical, but we did go through the technical details. If you watch the prior video that is there on the website, we talk about the biotin inference that goes into the heart of variational autoencoders now the point of variational auto encoders is that they are generative models once you have a variational autoencoder you can create new faces new data new things, quite a bit. You can even create, you can use it for data augmentation. That's a strength of variational autoencoders. And then finally, we did the word embeddings as another example of bottleneck methods. We did in particular four methods, the continuous bag of words, we did the skip gram, we did the fast text, dancingly though, just mentioned that you deal with word pieces, and then we did glove. But that's not the limit of word embeddings. There are many, many word embeddings as we just saw in the list on that website. People keep coming up with different algorithms for word embeddings. And then there are two kinds of word embeddings, context agnostic and context aware. We have done context agnostic word embeddings so far. So guys, at this particular moment, to sum it up, you should feel comfortable in dealing with all the common tasks of natural language processing with this knowledge. We did transformers in the previous part, part one, and of course if you remember that and you still remember all of that, you are pretty much done, like pretty much you have understood all of NLP. In the coming two weeks of NLP transformers, I will just go, maybe I'll give one week or two weeks, depends upon the time we have, because if you're giving one week off, I might not. We're running a little bit out of time. But in any case, transformers change again. Yeah, we'll probably need two weeks of NLP in transformers. I will do it in greater detail. I'll explain the theory of the transformers more. The lab will do more language translations, more of this, and we'll also learn to train the transformer. So far, we have not trained a transformer. We have used pre-trained transformers in part one and applied to our use cases. Now in part two, we will learn to train the transformers and attention and so forth, and also learn to hybridize them with RNNs and do interesting things now transformers will carry forward with us to computer vision or to image process which will be the part after that so that is that guys we have finished ten weeks of this workshop. I'm done, guys. If you have any questions, you can ask me. I'm going to stop recording. Thank you.