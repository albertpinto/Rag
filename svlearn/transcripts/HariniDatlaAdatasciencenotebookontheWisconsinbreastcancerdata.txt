 Hi everyone, this is a notebook on detecting breast cancer. This data is taken from the Wisconsin dataset. data set. And so here, let's go through my notebook to see what's it. So the main aim is to diagnose whether a person has breast cancer or not. And the target variable here is diagnosis and it is two as malignant and Benin. It's a categorical variable. And the data set has 10 features that again has been given three more information, including mean, standard error and worst. Those are our features. And here we go. These are our imports to do our data modeling data. we go so these are our imports to do our data modeling data and this is how our the data set looks like okay the diagnosis is m or b and these are our features and then we've described the data we are doing the basic EDA in the beginning. That's exploratory data analysis. Here next, let's check about the missing values. Where are the missing values in our data set so we can handle them? Here, when you see the missing values are only in a column named unnamed 32, and it doesn't hold any much information and doesn't affect our modeling so i have taken this data this unnamed 32 out and id so we've deleted those two columns then we have this as our data set now this is how our data set looks now after we have done the how our data set looks now after we have done the missing value handling and then let's see how many benedent and uh malignant values we have we have 357 and 212 each these are our columns okay let's visualize our data and each feature has been visualized. You can see they are skewed, a lot of skewed data. And this is our pair plot describing our data. So, OK, here you go. This is how it looks. So we see some based on its color, based for the diagnosis column. Now we are going to check the correlation between each column to check if they are highly correlated or not. And here we see that a lot of columns features are highly correlated. For example, your radius mean, perimeter mean, area mean are highly correlated and they hold the same information basically. So we could use any one of the feature and to do it right, we'll be using PCA in the later of the notebook then to handle the correlation. Now we'll just go directly standardize our data because we have skewed data and then apply the various models. And here we are skewing using standard scalar. And then we're going to split the data into train and test. And next, let's apply the logistic regression directly on our data. And here, once we apply it, our accuracy is 97% and precision is 97 and 96. In this particular data set, we'll be focusing a lot on precision as well, because it's about a disease and we don't want to misclassify any person having a disease as not having and not having as having. So we're going to be careful and look not only to the accuracy but precision also based on this data set here you can see using logistic regression we have 97 accuracy precision is 97 and 96 this is our confusion matrix then we have it on the test data set as well now let's see the roc curve have it on the test data set as well now let's see the roc curve this model is fitting our data well and all our you know the points are here so it is good here this is our errors and the prediction errors which is very low we can see the predictions are very you know the misclassification is very low this is okay it's a good model. Next, well, let's go and check out decision tree and see how it does. And in decision tree, as you see, the accuracy is a bit less. The precision is 96 and 85. It's a bit not as good as our linear regression model. And here is our heat map for the test data as well. This is our ROC curve. Even this is good. Area under the curve is, you know, 91%, so which is good. And here you can see the misclassification is a little more. So let's try some other model and see, you know, how it works. And now let's go and see random forest classifiers. And in this model, we can even get our feature importance can be seen and how they affecting our diagnosis column. And here you see the precision is really good, 99% and 95. And then we have accuracy is also really high this is 97 accuracy and here this is our misclassification is also really low here area under the curve is also good our 99 is area under the curve. And here you can see the misclassification is less. Now let's see which features are influencing our target variable. And as we can see, the first four features, almost, I think they come up to 70% or 80%, almost influencing our target variable. So now I've used these four features to build a much simpler model using random forest to see how we fare. And here you go. So the precision is a bit less. Our accuracy is 95%. For a much simpler model, this is better than our linear regression. So this is good. So our first four features are helping us. ROC is good. Here, because the simpler model, there's a little bit more of misclassification, but that's okay. Here we are using svms i've used the same four features for doing svms as well even here the accuracy is much better than our random forest model so svms fare really well and here i've used the kernel svfm also from also to do it and here it is pretty good 96 and 96. you just have little misclassification and these are roc yeah okay so our threshold plot for swiss so we are looking at the precision. Recall is almost here. It's coming down. XG boost is the other algorithm I've used. Even here it's coming really well with the boosting algorithm. So it's 98% precision. This is by far the best for now misclassification is also very less one and two and roc is also good as we know now let's apply pca because we know it is there was high correlation to handle that i'm using pca in here and after applying pca i just got the first three principal components to see how you know they fare and here you go these are the first three pcas these are the variants and i've just plotted them to see the first two and the rest to see whether we can have a linearly separable data and then apply logistic regression on them here you see first principal component second principal component this is almost linearly separable so which is a good thing to use and here it's not good second and third is not giving us a lot of clarity to separate and here even this is good first and third is okay but i felt one and two pca one and two would give us better results so i have applied logistical regression using pca one and pca two again and here you go so we have 95 89 and 93 as you can see for a very simple model, we have a very good accuracy and this is a very explainable model. So it is very interpretable and we can because we have a clear demarcation And then I've tried a simple feed forward network and applied the logistic using the sigmoid function. And the loss I've used is BC loss. Given a training loop, a box of 500 and learning rate has 0.01. I've used just the basic things to do it. Here, if you can see our accuracy is 91% with 500 epochs. Actually, I think somewhere here from epoch number 350 and all, it's almost similar accuracy we are achieving and you have to do a little more tuning to do it and this is our you know precision score and recall i've done for this so based on this what i can conclude is for the breast cancer data the the first four features we can have a very good model and using pca we can build a very simple model reduce 300 30 features to two features and get a good model a interpretable model and our data can be represented using you know the target variables can be represented by you know doing a model on these PCA1 and PCA2. So that's it, sir. That's what I understood from this thing. Great job, Mrs. Winkler. Any questions, guys.