 you you you you yeah um oh okay All right, folks. One more. i thought that was love uh see yourself all right guys i hope you can hear me we are going to do a paper reading of the Lama paper, the Video Lama. This paper, the best way to understand it is, it is sort of a logical progression that takes the work rooted in clip and then blip and then blip two and builds upon those ideas and image bind and builds upon those ideas and applies it to video comprehension also and to do that i would remind you what clip was. Do I need to remind you guys what CLIP was? CLIP we all understand. So we leave CLIP alone. Do you feel that I should remember these are the notes of our paper reading from CLIP. If you remember when we did the blip paper reading, we talked about it. How did CLIP work work just scrape the web wherever you see an image with caption take the caption that becomes your label image is your target but how do you write a encoder a sentence and sentence encoder that will apply to images. Does anybody remember? How do you take an image and create a sentence embedding out of it? Yes, but before that, how do you even convert an image into a sentence? into a sentence. The alternate caption means that... No, no, no. That is what it should have, that's sort of the output. So just to recap, you remember visual transformer? What do you do? You take the image and you subdivide it into 16 by 16 pixel patches. Then you treat each patch as a word and the image as a sentence. You remember that? And therefore that becomes your input vector. Along with position encoding. And then for out like're doing is, you want to now train it through the usual sentence encoder technique across like a contrastive loss, and you want to make sure that its distance to its own caption is lower than, because now this image has become a sentence right and there is a pretty good insight there you realize that with language models we have taken in a way in hindsight it looks obvious but an image is a visual language isn't it it's a visual way of communicating just like text is a is a way of communicating. And so if you can do that somehow, then the rest of the technology comes in. What you did with sentence birds, you can do now with images. Now, if you remember contrastive loss, you needed a triplet. You needed the sentence, a similar sentence and a random sentence. So given the image, what would be a similar sentence? Caption if you're fine for that image. And what would be a random sentence? Just pick a random caption or something. Isn't it? So you've got your triplet. So you can use your same contrastive loss technique and therefore create a way of image embedding sentence embeddings for images that was the clip right i think the clip cli let me recall contrastive loss image what was p for i forgot right check it out right so that was that and then came the blip paper blip improved upon clip by using a three stage three loss functions do you recall that guys oh first was if you look at this image do notice that, and I'll move fast through this paper, because we have done this, and if anybody has forgotten this, let me know. But you look at this image, do you see that it has been broken up into patches? This patch goes through your standard self-attention and feed forward. This patch goes through your standard self-attention and feed forward. Now, this is going to, now there is a text here. Let's say that you have a text, a little girl holding a kitten next to a blue fence, right? That's sort of the text encoding. A text encoder goes to the standard encoder, encoder part of a transformer. That is good. Now, what would happen when it goes just through this it would come out with its own encoding encoding vector what can you do the standard clip idea is to do a contrastive loss that was your itc image text contrastive loss right so you can align the image and text. So let me call this as pillar one, pillar two, pillar three, and pillar four. One and two pillars are your classic, with this cross-attention sort of suppressed, is your classic clip. Then what they do so you have the this is one part of the loss function but it turns out there are two more parts of the loss function that really improves upon it the second part of the loss function that they do is something they call image grounded text encoding now what is that what is image grounded text encoding. Now, what is that? What is image grounded text encoding? To recap the pillar three, what you do is, you notice that in the standard encoder, you have sandwiched a layer called cross attention layer. In the cross attention layer, you're taking the image vector and you're feeding it in. Image encoding, you're feeding it in and you're hybrid the image vector and you're feeding it in image encoding you're feeding it in and you're hybridizing it with whatever the text encoding comes out from the first attention by by self-attention layer and then this is the cross attention so in these pictures whatever comes from the horizontal side typically represents the grounding point. It's almost like, I don't know, I don't know where they got the terminology, but one trick that I use is think of transistors. You have the base voltage, right? And so forth. So anyway, you're doing this. The signal then is informed by this image so it is very different from the text encoding here it is image grounded text encoding in the image grounded text encoding what do you do now you use a matching loss function image text matching loss function. Image text matching loss function. What does the image text matching loss function do? If an image and a text go together, the captions are true, you want to produce a high probability of a match. And when there's a random text, you want to produce a low probability of match. Right? That's it. So that will train this loss function. This is just a simple loss function that's just telling what is the probability of its being related. And then comes the fourth pillar. What was the fourth pillar? If you remember, it is basically your aut regressive decoder right token by token by token it produces is the decoder part of it except that you have modified this also to have the cross attention now you're doing grounding it with what again image grounded text decoder not text encoder decoder remember this is text encoder this is text decoder not text encoder, decoder. Remember, this is text encoder, this is text decoder. Right? Decoder means it will produce one word, then another word, you feed it back, all the token by token it starts producing. And then what is the loss function for a decoder? Comparing it to what you thought it should have produced. Right? The label, the label sentence, that's your decoder. So this was your classic blip that we covered a while ago are we recalling this guys does anybody want me to explain it more we recall this right this is your decoder i mean this is your blip so then what are the benefits of blip right one of the things it did is it helped you clean. That picture, instead of a 3x3 grid, if you had made a 4x4 or a 5x5, a smaller one, and the kitten was just broken into small pieces, then there wouldn't be a notion that there was a kitten there. That is right. So in other words, what is the right level of granularity? And by evidence, people are mostly stuck by 16 by 16 pixels. You can do 32, I mean bigger, but don't go too small. Otherwise, there is not enough context. So. See, what they're trying to do is each patch is like a word. Right. So a word has a meaning. So you want to keep a patch big enough to have a visual meaning. That's one way of looking at it. With this there, one of the side effects of this paper, if you remember, is you could literally use it to clear up the data so if you look at the clip raw data there was a lot of mislabeling and silliness people have you know most of the time when people write img src and then alt they use that more for the search engine optimization than being honest about what the picture is right so irrelevant things are often given and this example was given. Like here is a picture. Birds are flying in a sunset over a lake at sunset. This is what Blip generates, whereas the actual caption in the data was from the bridge near my house. Which is not something that the picture is saying. It gives an alternative reference point. And so you realize that when you look at this result, it's pretty impressive, isn't it? Likewise, when you see this, a potted plant sitting on top of a pile of rocks is more descriptive than in front of a house in Austria. And sometimes it fails also. For example, when it when it looks when a blip looks at this image, it says a large building with lots of windows on it. On the other hand, the actual caption was more specific. It was that current castle was built in blah, replacing blah. But if you think about it, that is a fact that is an addition to what you see in the image. Right. So I wouldn't call it too much of a negative. So that was it. And as you notice that when Blip came out, it right away did an amazing job with the benchmarks. I won't go over that. So now we go to the blip to paper. Oh, by the way, look at this. All of these. i'll just let you stare at the uh somebody said make it a little bit bigger touch on so i'll zoom out is it looking bigger guys ah yes right so it's that's how these things go. Sometimes it's right, sometimes it's wrong. For example, you notice that this one, it kind of thought they were moving, right? Yeah. So, it's a pretty good result. But there is a problem with this. Do you realize that this is a lot of computation? Like you look at blip and suppose it was a one week project for you guys. Now would be the time you would want to bail out. Isn't it? So this is a pretty like end result of a lot of hard work to put together something like this so now what do you do then came the blip2 paper and it said that see guys why are we using a decoder so basically the question it asked is if you look at the four columns here you can put a decoder the column four columns here. You can put a decoder, the column four, but these days they are excellent decoders or generative decoder generates. Why not replace the decoder with a full blown LLM? Are we together? What if you put Lama, Vicuna, Falcon, Mistralral whatever you want out there would be far more powerful but what what would be the catch the catch would be you can't train it at this moment all for you know you're training these pillars all four pillars right then you wouldn't be able to train it. But then you ask, is that really a loss? Likewise, you look at the first one, converting pictures to vectors. Why are we doing a contrastive loss, image text contrastive loss, when Clip has already done it and vit already knows how to create patches out of it right why are we doing that so what if we said that we froze the two sides and said that there are enough parameters in between for it to learn right the only weights and biases will change will be the ones in the center. We'll take a really powerful image encoder. Right. We'll take a really powerful large language model for decoding. And in the center, we'll put something. The bridge connecting all of them. Which is the only thing that you learn. Then what? And that is the idea of blip two. So let's go to blip two. And again guys I'm going over this a little quickly partly because we have a dearth of time. We have to have our presentations starting at 5.30 and also I want to give you guys time to get ready for it. Those of you who want to present. So then that brings us to the next paper. Is this visible guys? Is that readable? Yeah, yeah, my touch finger. So by the way, I hope you guys are not able to see my finger on the screen. Okay, good. So what this says is, it uses the concept of a frozen model, which basically means, take an image encoder, all its weights, make it non-learnable, freeze it. That's why you see that snowflake picture there, frozen. Likewise, take a large language model of your choice, Lama, Vipuna, whatever it is, freeze it. Then you say that all the learning has to be in between two pillars, except that you have one guy there. And you say that that is the only thing you can change. Now, large language models are getting scary big. Apparently I heard rumors that there is a 3 trillion parameter model about to come. And there's even some, I don't know how credible the rumor is, that while they'll release a 3 trillion parameter model there is a hundred trillion parameter model already in the skunk works right so things are getting crazier and crazier right so when such large and highly competent language generators are there decoders are there, you just use it. And at the same time, image encoders are getting better and better and better. Just use it and only have the in between thing trained. So the Q-former, to compare the size of a Q-former, compared to a trillion parameter model or a billion, you know hundreds of billions of parameters or whatever model a Q former what was the size of a few former guys do you remember. In blip to. tiny. By today's standards practically microscopic it was just a few hundred million parameters. Isn't it? And so you're saying that that Q-format just has to be the glue of binding two visual languages together, text, sorry, two languages together, text and image. Isn't it? It is just an alignment, learning how to align. Am I making sense? QFormer's only job is to learn alignment between the two. And aligning it also then thereafter to the language model feeding it to the language language model stays frozen image and photo stays frozen right so yeah so so obviously everything is a transformer in this world right everything so it's a baby transformer okay so now so here you said that write a romantic message that goes along with this photo right? Instead of along with this photo if you said that I am somewhere near the lake with the sunset write a romantic message about it, would it write the same poem? No, it would write a different poem. Why? Because I have seen it? No. Generally what happens is that even slight, and this is one of the things, usually perturbations of the input lead to small changes in the output. You may get a poem that is almost essentially the same but if i don't ask it to do any poem or anything what is the output over there it will just say it's a sun over go to sunset that's it it will come up with a description sunset yes and in fact as we saw in just the um flip itself blip itself it produces really good descriptions of images and so by the time you come to blip 2, it produces, as you will see in the results of this paper, even better descriptions of the image. In fact, the topic you say it is that it is, see one of the... So it's a summary of the image versus elaborate even more about this image. It can write more like this no no you can write it can answer specific questions you can ask okay so let me let's go through the examples and you'll see so it is so the topic that i'm the point that i'm trying to make is there is a big topic big field that has emerged visual language understanding value you are entering that. This is started with clip but visual language understanding as a field is maturing literally as we see as we make progressions with these papers. And visual language understanding is literally where your project is heading. So like for example and I will relate it to your the sort of MRI images that you look at and we'll talk about this. Right. So, guys, this picture when you look at figure two, is it dead obvious what it is happening what is that cross attention mode you remember when the text goes in it has to do image grounded text encoding right the output of that even in the previous blip was going to the image text matching you know the probability that the two are aligned exactly the same the output of that of this image text contrastive learning results you're feeding it into generation image grounded text generation because this whole data you're feeding into the the decoder or the large language model which is sensitive to the encoding coming from images and text, image-informed text or image-grounded text encoding. And what is the... In total, Qforma contains 188 million parameters. No, these days, when you talk of LLMs, where the fashion is to get bigger and bigger, no self-respecting outfit will release a model this small. But this is the magic. So what you have is a cube form is a magical thing. It just with a very, very efficient transformer, it manages to create alignment, just the right alignment, leveraging the work of others. Transformers for all different types of applications. Yes, and that's exactly why we are going to meet your llama. So this is a guys blip to second stage vision language, I won't go over the paper we have gone over this many, many times. language. So I won't go over the paper. We have gone over this many, many times. But let's just look at the results. You ask this image, explain the advantages of this product. And notice, guys, that this is not just captioning it. You're asking it a direct question. You're not even telling what car it is. If I had looked at it, I would have no clue. Frankly, I would not have a clue. And it comes back and says, the audio e-tron Quattro concept is a plugin hybrid, blah, blah, blah, that has a range and can accelerate. Tell me something about the history of this place. The Great Wall of China was built in blah, blah, blah. See, it cannot do that. You can say, hey, is the LLM doing that? But how did the LLM know what it is looking at? LLM just takes a prompt and then produces an output. The prompt has to be that Great Wall of China needs, right? What is the history of the Great Wall of China? So the rest of your blip too understood that and fed it into the LLM. Likewise, write down the facts that you know about this flour. And this I thought was really impressive. What are the ingredients I need to make this? And you see this and these two are really very, very interesting. Look at this picture. Right. And you ask, is this photo unusual? And it says, yes, it's a house that looks like it's upside down. How could someone get out of the house? Then it says, it has a slide on the side of the house. Isn't that nice? And for good measure, here's a funny one. What are shown in the photo? A man and a chicken. What does the man feel and why? He's scared of the chicken because he's looking at the blanket. In other words, the man is staring out. So Blip could look at like images and describe with images blip to get to the point of image recognition with an 11. let's say so instead of a decoder uses the full power of island. isn't some despite the same guys. guys by the way same guys yeah they're bringing in these capabilities yeah see one thing and i don't know how true it is it's anybody's guess because no they have never reviewed it the gpt4 from what i heard is not one thing in general it is a sort of a pool of different lots of different models to which uh agent delegates and so i wouldn't be surprised if this is sitting somewhere in the inside or some variant of this is. Just super. For all the examples that you've shown, and even the ones which were maybe did not capture the right sentiment, but still kind of described the picture. But none of them seem to like hallucinate in the way we were talking about. But do they experience visual language? Yeah. See, you're asking short questions and answers. Generally, hallucination tends to become pronounced. So the probability is always there, but it becomes much more pronounced with longer narrators, longer generation. Albert. So basically, you are getting a better caption for your image which is more meaningful yes because you have a great wall of china the previous thing would say some wall existing here you actually know the great one that's right and think about it calling you that's right and think about it if you all have images do you realize what it means for your personal image repository now you can just run through it and produce little narratives and you can create a search index so the search project that you are doing is beneficial to your own personal histories, family histories. And the vast amount of images, see what happens is we go clickety click and before you know it, you have 10,000 pictures. And people like me who are interested in photography and who now have carried this ginormous memories in our DSLRs. memories in our DSLRs. Sometimes you go shooting a picture, and before we know it, we come back with thousands of pictures in just one photo shoot. Do we ever get a chance to get to it? No. But now you can see it through the system. In fact, the system that you are building now in your project. So one of the ancillary benefits, French benefits of what you are building now in your project so one of the ancillary benefits of french benefits of what you're building is literally you'll be able to archive and better annotate your own images right you could ask questions like show me a picture of how did i When I lived in, let's say, Santa Monica, right? Then it will pick up a picture of Santa Monica and you there and show you. It's almost like they have this ask the PDF, similar to this, ask the question for a picture. So that is it. So and what is happening at the end of this movie, the Titanic's rather brief. You're right, but rather brief. Did Leonardo Carpri's character survive my doubt? As the story goes. I found that funny. It's like very laconic answers. And then here's the thing, the narrative quality, right? A conversation between the two animals. And it says, hey, cat says, cat says hey dog can i write on your back dog sure why not cat i'm tired of walking in this now no because it's a very precise answer the question is formulated in such a way that it admits yes and no as answer right so that was that that was blip 2. so i'm just wondering how they are feeding the input to this whole blip 2 here the input to this whole blip too. See here the input is not just an image, it's image along with the question. That's what you do. It takes two inputs. It takes a prompt and an image. I'm just wondering how you feed that to the LLM. So imagine a simple Python function. Imagine a simple python function. One, first part is an image, the p-i-l, an image object pointed to an image byte file. The other is just text. Both of them ultimately just become encoded vectors. They go through the v-i-t and the usual thing. If you look at v-i-t, it will take an image object. go through the vit and the usual thing if you look at vit it will take an image of it so and all of these increase away mature like if you look at pie torch it literally has five torch you know things like that processing vision the same would also be for sound as well right yes yes so we're coming now we're coming to sound and music. Right. So, let's go down. So you say well now that you have conquered Oh, by the way, this is good. Write a write a famous quote said by this person. And it says, recognizes this is Albert Einstein, and it says, the world is a book and those who do not travel read only one page. So interesting because you stated you are and pretty good quote. But you notice the word visual language understanding takes on color when you see examples like this, isn't it nice? And can I wear this on my trip to Canada in December? Right. He says, well, you can. It's a nice shirt and short, but it's a little too casual for a trip to Canada. Right. But it would be perhaps more precise if it said it would be rather cordial. And so now let's move to video llama that we covered. Why llama? Remember, one large language model. Right? So it's put in. So think about it, guys. If you want, can you do videos with Blic2? You can't. You can feed it images only. But the idea is there. All you need is a QFormer that will do alignment between a text and whatever other format you have. Isn't it, guys? between a text and whatever other format you have isn't it guys you just need q formers for alignment and q formers are amazing they're tiny easy to train amazing right you leverage the power of good encoders image encoders and you you use the power harness the power of the large language and you use the power, harness the power of the large language models. Right? So how in the world can we make this happen? These authors say explicitly that they are leveraging this and they are using image bind for sound. They'll explain what it is. So this picture says it all. If you remember, just to give you an idea. First, let me explain what this picture is. What is a video made up of, video clip made up of? Picture frames, images in a sequence of images and audio audio audio there two different artifacts isn't it so what you do is and at this moment you take it i did i cover image i recovered image so if you pass it through image buying what will you get you will get an encoding sound will become a vector that's okay or if you if you haven't done it, just assume that it can do it. Next, images. Each of the images will get chopped into patches. Each image gets chopped into patches. Those are your, you know, these vertical like 16 by 16 patch, for example. This is your this tower that you look at. Let me call this tower A, B, C, D. Right. I'll put A, B, C, D. c b does it do you see the alignment between the two this a went through the visual encoder and became this shouldn't be a b c d hi what do you think the order no no no the first one is yes oh you mean uh the way depends upon how you feed it it doesn't matter like you because you're feeding them all together okay you're not feeding them one after the other the sequence doesn't matter there is no sequence you feed all of them together okay just like uh see remember the whole transformer world's main thinking is forget sequence we'll just put them all together and do position encoding We'll just put them all together and do position encoding. They're there. Yeah, position encoding. So now all of these... But then you need a lot of vertical towers. Yeah. If you have, let's say, 50 frames, 50 vertical towers. But if you have like a 10 minute long video? So the limitation is at this moment, given the computational limit, it is not advisable to feed large videos. Let's just say that the hardware isn't there yet. Because what we're trying to do is give it a video and come back with embedding your map. That is right. But you could always advise. Clip, clip, clip. Featured clips. Minute clip, 30 second clip. You can transcribe it. What's that? You can transcribe the video to make sure, like, reduce the resolution. No, no, no. It's the number of frames. Number of frames. Yeah, you can also's the number of frames. Yeah, you can also reduce the number of frames. Yeah, you could do it. Also, you could do sampling, sub-sampling between that and so forth. So, guys, what happens? So suppose here we are looking at A, B, C, dot, dot, dot, and D, which means that there are lots of them there. But let's say, imagine that there are four, four frames. Each frame has let's say 10 patches. So how many vectors do I get? Encoding vectors do I get? 40. But for each image you also add its position encoding. It's called temporal. In the world of video you call it temporal information. Time position, right? Time position, the time, which come came first before the other. Temporal information you need to add just like the standard transformer. What do you do? Which word is it? First word, second word, third word, right? You did the position encoding. Here you do the temporal encoding. The same idea, same thing. You inject inject it so the first 10 frames the all the all the patches belonging to one frame will have the same position input timestamp you can think of it like that but they don't use timestamp they use actual vector but ideas are same right then what do you do you get all these vectors you you feed it through a q former right what does the q former do remember q formers are alignment devices and you ask the q former to given one picture right to take all the patches and somehow aggregate the patches and produce and the position embedding and everything produce one vector right so your a b c d is there so video qq former will produce some vector right now comes a interesting fact you you want to feed it to a llm but the llm you want to feed it to a LLM, but the LLM, its vector length may be different. Isn't it? Input vector lengths may be different. So what do you do? What you need to do is, you need to do a linear transformation from one dimension space to another dimension space. Vector of this size to vector of that size. When you're doing a linear transformation, Raja Ayyanar?nilavakot?inhu?vc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o?hc2o? layer. You see that, right? Might as well for good measure. And then you feed it in. But now think of the LLM. Now in the right hand column, right hand channel, audio goes in, exactly the same thing happens. Temporal sequencing, audio cue former is there. So what are the things that you're actually training in the system the video cue former and the audio cue form isn't it they're training for alignment and then you feed it into the llm and llm will produce some text now how do you back crop again what is it that you can train what are the parameters that you can train the Q form of parameters, how would you do that your labels, I mean, then you can use the standard decoder loss function, what is it that you wanted it to say versus what excited what it generated. And you can that use that for the loss and back propagate in there, and that is it, that is your. back propagate in there and that is it that is your in simple terms now the question is uh images yes you use the vit and q format to do that for the blip thing what about audio audio blip can't do audio ah okay no problem we'll use image right that's all it is where do they get the training data for this you know oh It is full of videos. No, video is there, but we need the corresponding text to describe it. Oh, it's a lot of manual work. They actually explain it that for the complete video data set, it's narrower, but they have a lot of data for the one. Yeah. But the painful part was the video and audio. Yes. They created it they explained some tools yeah see guys behind every good research paper they always explain to you the fun part the math part or the discovery part what is the the unsung heroes are the people who sit and annotate the data, who label the data. They are really the unsung heroes. I annotate it. Yes, there you go. All day long. For medical data, isn't it? Yeah. These are the unsung heroes who are making all this air. So how many frames are allowed in that video encoder? Theoretically there's no limit but in practice people have found that beyond 30 seconds or one minute frame it gets a bit tough. So what gets tough? The computations. It gets a lot. But it doesn't have to be a cascading way, right mean, see, my point is, I know that temporal, see, basically, what we did is we took a video, and we tried to flatten it out into a very wide vector, because I have 30 seconds worth of frames and temporal information. So that is some width of parameters, right? And that's how they kind of describe one object like the whole video must be put into that vector form. Yeah. See, think about it this way. If you put too many frames here right this video. Qforma is going to produce this tokens at the end of it. Each frame is a token. This LLMs, they have a token limit. This LLMs, they have a token limit. Right? Let's say that you have a token limit of 512. I'm just taking an example. And if you want to divide it equally between the audio and the video frames, then it becomes 256. Now 256 frames at the rate of 30 frames a second, gives you less than 10 seconds. Wouldn't it just become like a building block? Let's see, when we did the image, Yeah. we did cut it into smaller pieces yeah if i take a video and i say these are 30 second squares yeah and then i can build an even bigger model oh yes yes yes so people have done that so this is composition right people have tried compositions and so forth and they're very active area of work so guys this was your video you see how easy it was and we covered this but one thing that i want to show is you see how ideas in this field build on top of each other good ideas are building on top of each other right and this is very like a collective effort. Research is like that, you know, the best ideas build on other ideas and so forth. Then they look wonderful at the same time simple. So this was your Vigyo Lama paper. Sukhwal. We never used, even accessed data from But that is a very good point. Actually, I don't know. Yeah. Should we? We should have used that. Maybe somebody has used that. Check it out. You can see all of them. They have tons of data. Yeah, that could have been used. Yeah. So, guys, what's that? Are we understanding this paper? And by the way, did you guys actually read the paper? You thought, let's wait for the explanations very good nice right so i'm going to now pause the recording and i don't think can i leave the third paper to be uh not covered you guys have read it it's straightforward straightforward. Instruction is very straightforward. Right. What I would like to do is just cover the results of this. That will be and after that we'll go over to the demos. We are starting with the demos of the of that team. The Optimus Prime transformers. The Optimus Prime transformers are going to do the first demo. So guys, look at the level of sophistication of video understanding. In this video, you have to watch the video to say somebody is opening the door. Describe what you hear. It says, I hear sounds of footsteps on the floor in the background and there is also a dog barking in the house. Does the man wear glasses? Yes, he wears a pair of glasses in the background and there is also a dog barking in the house. Does the man wear glasses? Yes, he wears a pair of glasses in the video. This is the Lama segment. Now this is a different picture. It's a video clip in which somebody, some composer is performing something or playing something, I don't know. And how was the audience's response? And the audience was really enthusiastic and clapped loudly for the performer. So do you notice guys, it is not doing a literal, you could have said, hey, clapping, I have a clap detector. Right? It is not doing that. There is a lot more understanding of what's going on here. So this field is visual language understanding. What is this man doing? He's playing a saxophone on stage at the concert. Oh, I love this one. This looks like my dog. By the way, I have a golden retriever and she used to run like this. So, describe the image in detail. The image features a beautiful and playful golden retriever running on a bright green lawn. The dog is running with a tennis ball in its mouth and it is an active and lively animal that describes playing and having fun right the green grass in the background creates a vibrant uh this thing it is worth mentioning that the dog's focus and dedication to playing with the ball also showcases its natural instincts and its hunting abilities, this image captures a fun and heartwarming moment of bonding between the dog and its human companion. That is not visible in the image and the fact that it can say that but is implicit in the image. Say something. Because it's a retriever it's getting the ball to its owner yeah right as the dog's positive attitude towards the board reflects this enjoyment of interacting with its owner and its environment over overall the image conveys an inviting and joyful atmosphere and i can attest to that, living with a golden retriever. So if this was given to that stable diffusion or anything like this, would the picture that got generated have the same quality? Try it out. See, stable diffusion prompts though are different. They are unique and specific. They are never narrated. Right. But we are not too far from the other way around now yeah you could do that from that you can ask a language model to extract a stable diffusion friendly thing Patrick could you attest to that which market uh you can you can use the excel model which one yeah from this narrative you can extract a diffusion yeah from this narrative you can extract a diffusion diffusion friendly prompt isn't it from a text yeah. Yes, I can do it so there's there's a mother or the way we can such a train the output embeddings to be more instruction for this for okay. Dennis has mentioned something the acceptance and as the as as the as well yeah that's just one that's just one model, but the actual instructions to send diffusion from a model like this, but like a instruction to me. instruction tuning also. Patrick, Adobe Firefly will do it, right? You give the text and it will give you the modern regime. Is it? Adobe Firefly. No, but that is search, that is search. Or does it generate? Does it generate? It's generating. It's generating, yeah. It's literally using, yeah, some. Adobe Firefly. Interesting, guys, I have Adobe license, I'll try it out okay guys this image though has become sort of a benchmark image here is a guy and you ask what is unusual about this image and it says unusual about this image is that the man in a yellow shirt is standing on top of a parked car while holding an ironing board and pressing clothes you don't usually see that so it's comprehensive comprehend ability do you know the building in the picture it says yes building is the blah blah united states capital building do you know who the man on the right is? And since I have never watched Game of Thrones, I assume that this is a famous character in the Game of Thrones. Yeah. Okay. What was the Game of Thrones about? It was to capture a throne? Or what was it about? What's the core idea in that? Just killing each other that just killing each other oh boy okay yeah i think i uh would rather eventually it's a kingdom and the only other thing they had was the famous line is winter is coming so on one side they they assume there's like supernatural like or whatever. Okay. And that's pretty much it. Oh, okay. Who is the lady on the left? So the lady on the left is blah blah. What's the relationship between the two? It gives the relationship. I have no clue whether it's right or wrong. Describe this with you. So one of the... You're not on record, right? Yeah, it is being recorded. Maybe later. And describe this with you. What has happened is we kind of turned blind to most of this generated stuff, right? I mean, all the cartoons and things like that. But the young kids who are seeing, they're a lot fascinated by all this generated stuff and not the, we look at the sunset we get excited whatever i don't think they are that in the more video games and things like that and what over time will happen is the net will start getting filled up with all these generated images and things like that and the real pictures that are there will probably start taking the vaccine. Yeah, no, that's a good thing, Sachin. Let's discuss it right after this in the break, but I completely agree with you. Summarize this video in one sentence. The video shows a beautiful scenario. Cherry blossom. By the way, how many of you have seen the cherry blossoms along the Potomac? DC. Okay, so. And a cityscape with tall buildings in the background. I was going to ask, is it the Potomac? Is it the cherry blossoms? What direction is the ship going blah blah blah? That is it. And What direction is the ship going? Blah blah blah. That is it. And you can also generate hashtags. This was another paper on generating hashtags. So you can see, you know, you can continue along with that. So guys, that brings me to the project for today. You have to do, what do we have to do? In Iraq. Iraq, that is one. And the project for next week also I'll announce. I want you to do, and I'll announce it. Take an image. Create a UI in which you post an image. You can post an image. And you have to create a full narrative about that complete narrative right that must be a couple of thousand tokens long with sections in it with a with it and yeah And two aspects to it is, if it has to be, that image has to belong to your domain, it must also augment or benefit from your ARAN. So use your video lemma, take out some things, use that in conjunction with... Why video lemma? No, that's not video lemma, sorry, lib2. Lib2. Use it in conjunction with why would you love to live to live to use it in conjunction with something else, like compose use compositionally effect with different transformers and architectures created and come up with the best narrative. If it is about your knowledge, the domain, then obviously dip into your search take information from there stitch it all together into a nice narrator so this week next week blip and video easy though no i'm still not done with rags I'm still not done with rags. I have a lot of other people. No guys, infrastructure took a long time to set up. You guys have done that. Now you'll pick up soon. I do have one feedback though Azif. I think, you know, because I was very new to this domain, I had never done anything over here. But in hindsight, I think I would have been on top of the project if instead of setting up my own machine. See, you paid like 5000 bucks for it. I think for the duration of the bootcamp, if I had a 24-7 running A100 on an AWS, I would probably pay like 2000 dollars. And I understand that's a lot of money. And everybody has to do a trade off. But I'll tell you that the first two weeks, I just spend time getting the OS up, the driver up, the Kubernetes up, the Ray cluster up. I mean, there was a lot of infrastructure work. And that would have been a one click deployment or better yet just use any scale. That is right. That is true. So yes, Satyam has made a very good point. Let everyone know is that, see, we spend a lot of time in infrastructure and in cloud deployments, these are relatively straightforward. Actually, it was a thing we debated a lot in curriculum development. A lot. One day we would decide, OK, let's just not do this and focus on this, just the AI parts. And then the next day we would say that that is a crucial part of learning. And one reason that I did that, took this route. I knew that you guys will be frustrated. And for a lot of you, it will be the first time you're doing this, especially in the AI world. A lot of you are it will be the first time you're doing this, especially in the AI world. A lot of you are masters of Jupyter notebooks, right? And this was a new experience, isn't it? How many of you would agree with that? Most of you. And this was a learning experience. So I wanted it to be literally a struggle and a learning experience. But now you would agree that you wouldn't look at what cloud providers are doing as a black box anymore you would know exactly what they're doing that was the main point i don't know if it was the right decision or wrong decision but i think i think it should just have the pros and cons of the directory like i would have rather focused on three weeks on ml and deeper ml than the infrastructure because i knew going in the kubernetes is hard or some of these things are hard and i ran into problems like gpu drivers and stuff that was painstaking but i'm not saying bad things you're either of the games not saying bad things but you're either of the games and that's what i'm saying right see when you're trying to do a rag and you're trying to do like blip and stuff the folks who are behind i think they'll have a tough time so i have hope that i'll catch up by next week but i do understand yeah so just for that reason as most of you aware, we have released now the full solution. If you feel that you're too far behind or infrastructure is not the setup is not what you want to focus on. Start from this week with the solution. That's why I released the solution. See, this was the compromise I've come up with that we will let people learn this whole thing. But in due course of time, two, three, four weeks later, we'll give them the solution. And if people are behind, they can just start with the solution we can get you running in 15 minutes on the this rocky linux machines it will run in for 10 minutes 10 15 minutes nothing needs to be done it will run because all it takes is cpu and it runs fine you can actually run it on your laptop i mean assuming that you have a linux vl it will run inside that it doesn't need too much. So anyway, that was a compromise position. And the latest solution that you really use for which problems? It is only for project one, because project multimodal is actually you realize that it's very easy. start using it and hint hint some of the things are right here we're covering it isn't it and then comes the rag rag is even more straightforward what do you do you search you create your instruction right as a prompt you feed it to llm we can be honest so the rag part at least should have been given the fact that each team has six seven people, except for one team Oppenheimer team every other six seven people and five seven days. That should have been relatively easy. The hardest part was in the beginning, and it was a conscious decision to get you guys to that. So all right, guys, I'll just stop the recording with this. Anything else anybody has? Am I even recording? I don't know. So Asif, I have one question, which is a little out of this, what we are doing here. But as Satyam was asking about kind of launching something on, say, AWS Cloud or whatever the cloud of choice, right? Does any one of you have experiences with like the general websites, site hosting providers, such as, for example, I'm just throwing good at an example. example but do they allow for as much flexibility as cloud providers would in terms of um doing some proof of concept deployments and and then basically down the road I do totally understand that you would have to go down the complete round of uh deploying in the in the cloud and and going the Kubernetes route and maybe having your own servers and at some point see uh mage I'll give you my experience to my knowledge by the way there's godaddies etc right they give you very basic web hosting like using some all of the standard tools there is almost no ai support infrastructure support there but i may be wrong i'll tell you what i do i use godaddy simply as my domain register and i just it's for me it is just a dna domain name to ip address mapping right so long they just maintain my dns records I point my mail for that domain to Google Gmail and the Google switch I point my web website to the machine that I'm running it on ip address I'm running it on and that's where I leave it I never do heavy lifting or I never even thought it's possible on GoDaddy. So that is the story with GoDaddy. But there are other places. I'm not sufficiently well versed to know what the detailed answer is. I can share. These are my findings. OK, the best one, I think that you want like just get ready and get running and you you are committed to ray then it's any scale any scale will literally give you like a ray endpoint you write code and you say deploy a scale it just works but then if you want to go into like ray as a service or whatever you can deploy a ray cluster in AWS, GKE, and then there are the cheaper ones, which is like Lambda, the new Lambda labs and a couple more. And all that they promise you is, you know, we can give you a Ray cluster on a GPU, or we can, you can buy a GPU. But if you guys have experience with AWS or or gcp or whatever then you already know how you're doing kubernetes and things like that then it's just a matter of saying i also want a ray cluster next year yeah manish i'll repeat what sir tim said he said that see from his experience if you want a quick ramp up just go and you're committed to the ray framework then just go with any skill it makes it very very easy and if you want it as a service they can do your deployment on aws google and so forth and there's lambda labs also lambda perhaps is a little quite cheaper actually than that with lambda labs though i must interject with my personal experience sometimes it's hard to get resources because they're everybody wants it. New and cheap and reliability. It's hard to get. I think this was Praveen's experience. So it's a little hard to get. That's why number one pick is any scale. Yeah. They give you an endpoint and you can deploy after that. Yeah. So yeah, any scale guys, if you're doing great, that seems to be the recommendation here