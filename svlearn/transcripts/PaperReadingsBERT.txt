 So this is our Sunday paper reading. We are going to discuss this paper BERT, Pre-training of Deep Bidirectional Transformers for Language Understanding. BERT has become perhaps the most used model in natural language processing. There are many variations of it that have come up over time. And more work, more and more work is still being done. Now this is a large model. Quite often you can take this model and just apply it to your task. It has been designed in such a way that it's very very easy to apply it to a wide variety of natural language processing tasks. We saw that in the lab if you recall in the lab that we did on Wednesday. We used the Hugging Faces transformer library and quite a few of them were BERT-based transformers and we used it for quite a few tasks. So BERT represents actually quite an interesting point in the development of transformers or the use of transformers for natural language processing. If you recall in the development of transformers or the use of transformers for natural language processing. If you recall in the previous Sunday we talked about attention is all you need. When we talked about attention is all you need we talked about the classic transformer architecture that is mentioned in that paper which had the encoder and the decoder part. So I will start by just reviewing that in a very, very quick way. I won't get into the details. It's just keep it to a high level so that we can focus our time today to this paper. So what was attention about? Attention is all you need. If you recall, it talked about something called the attention heads, which could pay self-attention between words in the sentence, when a word could pay attention to other words in the sentence. So what was it made up of? An attention head was made up of a set of a few layers of attention or self-attention if you wish, self-attention. Then they were multi heads. The encoder Was built From many heads. And likewise the decoder also the decoder was made out of many heads. So they should become were made out of many heads. And this is how it looked the encoder so one encoder layer was like this basically what you had is you had a situation where you could take suppose this is your attention block you would get the words so suppose you get a word the cow jumped over the moon So, you have multiple words, I will call this W1, W2, W3, W4, W5, W6, and maybe full stop W and. And so all the seven words, they would go, they would go through a few transformations. What the first thing that would happen to each word is that it would go through the embedding. It would get converted to word embedding. So let me call it E1. And this word embedding is it's actually built out of two things. It was built out of the Little he won. Let me call it little he to little he three little he for A five or maybe I'll use capital doesn't matter. He six he seven of sorts of he and right and then what you do is you couple this layer with the position embedding of each of these words right you take the position embedding from just specifying where in the sentence it is, p4 and p5 and so on and so forth, p6, p7, right? And these together, you treat it as the, let me call it the big embedding vector, together, right? It's the complete encoding of the word and its context, its position in the sentence. Are we together guys? And here we go. E1, E2, E3, E4, E5, E6, E7. Now the interesting thing that this did is it would break up each of the words into a K, like think of it primarily as a K, how should I say it, three individual pieces, a K, V, and Q. Each of these words had become K, V, Q. K, V, Q. Three different vectors. If you remember, these were in the original, if I remember it right, it was 64 dimensional vectors. A K, Q, sorry, a K, Q, and V. Today I'm being particularly clumsy 64 dim vectors whereas the encoding started out with a fairly high dimensional even if you take a 512 dimensional encoding and 512 dimensions for the position encoding the word encoding in the word position encoding what you fed in was actually a pretty big vector 1 0 2 for them if I remember right from the original paper that is what you would feed into and then what the attention block would do is that it would generate code KQV. And now, so one, two, one, one, one, then this would be K2V2, sorry, Q2V2, K2, and so forth, right? So this is your Q7, V7, and K7, right? And now the point was that which word was paying attention to which other word? And the crux of that was we were saying that the attention. That a query word Q. I Has let's find out how much attention it is paying to each of the Other words. So let's mark each of the other words as K, J. You take the dot product of these two, and then scale it the standard decay, but we can ignore the scaling part. So then what happens is you will end up with a bunch of dot product numbers. And when you soft maxes, what will it do? It will convert these into pro so let's go step by step you compute this you get a bunch of this gives you a bunch of energies an array of of energies right for q i and then what do you do at that particular moment what you have is these energies you soft max remember we talked about soft max so when you soft max this q i k j d k what happens with this is now you get probabilities we get an array of probabilities probabilities and this is sort of roughly speaking the measure of measure of your attention and then when you multiply this with vj so the attention the full attention step after this if you if you say from here to here and then finally the attention was just going back to the definition of attention, it was quite straightforward attention, that attention that QJ, QI is paying to each of the other words in the system is this VJ. To the word J, it is paying this much attention. This is the intuition behind that. This is just a norm just to prevent the gradients from blowing up and so forth. So this is times this. Usually people write the VJ on the other side, but I wrote it here. So this is a recap of what an attention is. It just helps a word in a sentence decide or figure out which other words to pay attention to and to what extent. So it may say that, for example, it may It may so happen that let's say vi is equal to three, right? So then the attention that the third word pays is maybe, let us say that this thing may come out to be for the first word, second, third, fourth, fifth, sixth, seventh. So it may so turn out that this may be small. This may be even smaller, this may be this, then it may just so happen that this it is paying a lot of attention to. And maybe to this word it's paying a little bit of attention to and that's about it. attention distributes its attention over other words in the sentence in an intelligent manner and that is the gist of it if you know that these two words should pay attention to each other you can feed in fairly long sequences, relatively speaking, and not fear what problems that we had with record in neural networks, namely, you would You feed everything sequentially and the thought vector or the context vector begins to forget what is it that was there in the beginning of this sentence. So if you remember the the RNNs, they had the characteristics of the character Dory in Finding Nemo. It then tends to forget early stuff pretty quickly. So this is our attention and now what we have is the rest of it is straightforward. You have many, many attention layers. The classic transformer, if I may say, classic from, I don't know if it is appropriate to use the word classic, but I'll use it because it is all just two, three years old. It's all so new, all of these ideas. But this was, and I will gloss over a lot of the details it had an encoder decoder block right this is the encoder block which is encoder let me write it on the side encoder it was you feed in the word word or word embeddings the word and I will forget the word I'll just put it and you go through the whole process of creating the embedding vector that we just talked about e2 and so forth e in this case seven let's say the seven words so let me just say en words and that make up the sentence so they are going in here and then what happens is that you have not one so you have multi head so first thing is that there are multiple heads sitting there one head is looking at one aspect of the language and in the head is hopefully looking at something completely different in the language but each of the heads is made up of each head head has many layers many attention units layered on top of each other so the output of one goes into the input of the other so there was this whole business and of course there is one more I forgot there is a soft, there's sort of a ReLU layer to introduce the feed forward layer to introduce the nonlinearities into the transformation so that it can actually capture things. So, but I will just write it down. Now the classic transformer had about, if I am right, six layers, no particular reason it was six. And there were essentially eight multiheads, if you recall, right? But those are small six and there were essentially eight multi heads if you recall right but those are small details and there were some feed forward there was a layer norm then feed forwards and then finally some more weight multiplication and then the output of this and i'll now i'll change the diagram a little bit the output of this went into the decoder layer the output of this went into the decoder layer. And if you remember the decoder, as we talked about in the classic paper, is and that's important to remember. What will be given here? Let us say that this is E. And suppose you're translating from English to French. So let me just use the French one, French two. And let's say you have gone to three French words so far. The rest of them are masked. Isn't it? They become minus infinity values because these are all floating point numbers, right, ultimately. So what you do is suppose you have decoded three words. You have produced here f1, f2, f3, and your task is to figure out what is F4, what is the fourth French word, sequentially. So your decoding is happening here, like this. And when it happens here, it takes two kinds of things as an input. The F1, F2, F3, they first go through their own, the same cycle that they go through here, and the output comes, and it merges with this output coming here, and it goes finally to another decoding block, the real decoding block, a part of it, which is essentially the same as the encoder, but those are technical details, and then it produces these things. Do we remember this part, guys? This is your basic classic transformer if you review that. It is all right, it's a complicated architecture, it is possible that you may not, it will take a little bit of time to remember this. It is worth remembering this because in the NLP community it's a sort of a celebrated diagram of the classic transformer and it helps to know this it facilitates conversation for example this particular discussion on bird is informed by this classical transformer in many ways so we'll toss it yes please go ahead last time you mentioned earlier but it was a little bit short of uh which part of this in the transformer is uh parallelizable oh can you parallel oh that's the whole thing very very good see the parallelism comes from this first of all the entire sentence entire sentence goes in at once. In fact, it is necessary. You can't feed it sequentially. Unless you give it all the words, how will one word pay attention to another word? You see that, right? So you have to parallelize it. That is one aspect of the parallelization. The other aspect is you send in not one word, one sentence, but you can send sentences in parallel, a batch of sentences. A batch of sentences can go in parallel. Did I? I don't think there's a double in parallel come on correct me with my English is there a double L of why I'm supposed to and so word parallelization so words parallelization token parallelization this is so tokens are going in parallel token parallelization so in the first instance the entire sentence goes in all at once and then each word is is being processed like with respect to the other words in that sentence. So the parallel, the parallelization is happening from the word level, right? Yes, exactly. And the other aspect is if you think about it, you can actually feed this transformer different sentences, so long as you tell it that these are, this is one entire sentence, this is a a this is your sequence right you can feed in many sequences in parallel because what it does with one sequence is a whole lot of matrix multiplication and and activations and so forth and it can in parallel do it with many sentences right and that is a beautiful thing that's what makes this transformer so powerful. For example, you can feed in 2000 sentences at one go if you want. It's a big tensor, you better have good GPU or TPU or memory to be able to feed that much. Right? But at least you can feed it in small batches. As if in this context, when you say parallel, if I were to translate that to what we see there right now in terms of this W1 through to, let's say, you've written it for WN. So that would be W1 through to WI, which is one sentence. WI plus one through to WJ would be second sentence and so on. All those sentences are laid one next to each other. No, no, no. It's not like that. So think of one sequence, one sequence. As one sentence, one sentence, and I'll qualify this word in sentence. So sentence, maybe the English sentence or it could be any sentence. Definition is any logical unit, any logical set of sequential words. So if you so choose, maybe a paragraph is your sentence. Who knows? So now what you do is one. but let's stick to the common English definition of sentence to keep it obvious. So what happens is one sequence is like this. Let me call this E of the first sentence, E1, E2 of the second sentence, E3, oh sorry, of the first sentence, sorry, I shouldn't mix it. This is the first sentence, one, one, all the way E1 of the first sentence sorry i shouldn't mix it this is the first sentence one one all the way e one of the so suppose uh and in the what you do is you find the longest sequence of sentence the longest sentence that you have let's say your longest sentence that you have is uh 20. then what you do is you make sure that all the sequences are of length 20 and if needed you pad them with empty tokens in the end. So sentence one. So this is your sentence one and now suppose you have sentence two. So it will again be E2, 1, all the way to the EN. So now what happens is if you really think about it, you can feed this in one go into the transformer. Right? Isn't it? Especially the encoder. You can feed this entirely into the encoder. And then because this sentence is completely independent of this, right? You can feed this also into the encoder, right? Into the same encoder, but it will just take a parallel path. This will not influence this. In other words, the computation that's happening with the first sentence will not affect or be informed by the computations is happening in the second sentence. Right. It again can go in parallel blocks and so on and so forth. So suppose you have sentence. Okay, then you can have he K one. Now, why is this a big deal. Like why do we make such a big deal about this? And let me just use the word instead of encoder, let me use the word transformer here, the whole transformer, DR. So they can all be fed into the transformer in parallel. And that's what you do actually. You decide how much space you have, memory you have during inference time. Based on that, you don't just send one sentence at a time. You take a batch of sentences and you keep feeding it in. So this is what you have. So now to contrast this, think about the RNN. So in contrast, RNNs were what? They were basic units. You would feed in word one, H0 would produce H1, hidden state one would again need word two, H3, word three. And so you would finally get a context vector, right? Context or H final or the context or what I call the thought vector capturing the essence or the concept that has been, that is there in the sentence. So what happens is RNNs and RNNs and the gated RNNs, LSTMs also, which are just gated TMs, GRU. They are just things that, forget about feeding a lot of senses, they just work one word at a time, isn't it? So they are not massively parallelizable. That is one limitation. The other limitation with them is they are not conducive to transfer learning. They are're not conducive to transfer learning. They're not very conducive to transfer learning. So the whole idea is that if you look at the image processing world, remember in the very first lab we did, we wanted to do some image classification. We wanted to classify our own images, cats and dogs and whatnot. And we just took a pre-trained model, classification. We wanted to classify our own images, cats and dogs and whatnot. And we just took a pre-trained model, a VGG ResNet 106 or something like that, and inception and so forth. And we just directly applied it to our data. We did not even have to fine tune it. That is remarkable because these models take a very long time to train. And if you could just use and apply it, or just do the last mile, a little bit of fine tuning of the model in the very end, and get your work done, that is a gold standard. You ideally want that. You don't want to teach the neural network from scratch. It is something like that. When you hire a, if I were to take a more social analogy, if you hire an employee into your company, you know that the employee doesn't know the specific code base or the projects in your, or your specific domain to as much detail as the employees do, the existing employees do. But your hope from the candidate is that candidate is at least pre trained to the level of having an engineering degree. Right and knows most of the concepts of engineering or computer science so that you have to just do a last line training or fine tuning of the person To make the person productive in your workplace that is taking an analogy from the person productive in your workplace. That is taking an analogy from real life. Would you want to do that? You wouldn't want to start every time you want to hire an employee, you wouldn't want to say, all right, let me start from a kindergarten level education and put this person through another sort of 13, 17 years of education before he can start being productive in this team. That would be insanity. Nobody does that. So a very similar thing is there in deep learning that for many, many tasks, we ask, can we not just use transfer learning? Can we not just take a model and fine tune it if needed? It is like training the new hire, just putting them through an orientation process, teaching them a little bit, a little bit of training, and they're ready to go, they're ready to be productive. That's one way to look at it. So that is the gold standard. So those things with RNN, RNN based architectures, they are a little hard. So the two limitations are they're very sequential and the second is they are not conducive to transfer learning. And the third is, of course, sequentiality not only means that they are slow and not very parallelizable. The other thing, problem with sequentiality is that they have the characteristic of Dory in Finding Nemo. In other words, they tend to forget things. If you tell them a very long story, they'll forget the first half of the story pretty much. This sort of putting it based, sort of is oversimplified way, but that's that. So in contrast, if you think in this so when you contrast that with so let me write those limitations rnns limitations just as a review slow going owing to being sequential, sequential, the second limitation that they face is they are can't handle long sequences well. Forget the early parts, forget the earlier parts. So forget or fade the earlier parts, if I may use, not quite forget, but fade the earlier parts. That's may use, not quite forget, but fade the earlier parts. That's a limitation. You see that, right? And see, it's not conducive, not very conducive to transfer learning. And these are huge limitation. And in some sense, when BERT came about, this architecture came about here, and not BERT, the transfer architecture came about. So let's not contrast it. Will it be slow? No, you can feed all these sentences. First of all, you can feed an entire sentence at one go. And not only that, because sentences are independently processed, you can feed the sentences together in a mini batch or a batch or something like that. Now, in fact, it is the opposite. You can't feed it word by word. You have to feed it sort of the whole sentence so that it can do self-attention on it. So that is one great advantage like when you contrast that with a transformers you realize that transformers right have the advantage that they're very fast right because of parallelization. because of parallelization and so you can actually use the GPU with the hardware like utilize the hardware The second thing is B, it is, it handles long sequences better. There's still scope for improvement, sequences much better. And the C is a very conducive, quite conducive, rather conducive transfer learning. So this is the RNN limitations. So does this summarize it for you guys? Why transformers are quite a breakthrough actually? As a follow-up question. So when you said we put in a sentence, so it enters as a batch and it computes all the energies of the word in that sentence in parallel. No, no, a sentence is not a batch, Patrick. You don't use the word batch as a sentence. You send the whole sentence as an array of words, array of tokens. Right, right. Yes, I meant an array. So we send it in as an array, and it computes the energies of the words within that array, correct? That's right. It basically computes the self-attention within the sentence. Who is paying attention to what? Right. So my follow-up question is, if I put in a second array, like a sentence number two, does it also pay attention to the previous one? No, that is the whole point the classic transformer no the classic transformer has no recollection of the previous sentence at all it is completely independent so if it processes it in parallel it just but it just treats each sentence independently still yeah independently and that's that's the reason it's able to process sentences in parallel because suppose it had to do cross sentence context across sentence attention, then that would be completely different ballgame isn't it. Right, so it doesn't do that, at least in the original version. So you just just think of it that one entire sentence goes and it just so happens that the architecture is such that because everything is done at the level of a sentence all that self-attention business therefore the next sentence or the previous sentence it is completely unaware of what went before and what went after or what is going in parallel okay understood thank you uh so asif uh so what about uh random uh randomness so how do we achieve randomness here uh because we are saying that like not uh like in transfer not in the transformer, but in general, anytime we are like feeding the information, we want to randomize or we want to have the random state equal to true. So does that concept apply here? Yes, it does. When we get into bird, I'll specifically show what mischief we do in training the bird. Okay. Randomness will come in. So all right guys, so this is the context this is what we know so far are we together so the basic lesson is uh that people rnns are in general not as powerful so if you look at the prior work being mentioned in this paper so now let's start reading the paper from the beginning okay so just as again, I'll just scan through what we talked. Attention is all you need. It introduced the classic transformer and the concept of self-attention in which you would take the word encoded by position and embedding or feed it into the attention block and you would get the Q1, the QKV, the query value and key vectors, the 64 dimensional. And in a way you notice that they're smaller dimensional compared to the input 1024 dimensional input typically. Then we have the concept of attention. If you ask this word, a word QI, what is it paying attention to? So what you do is you take the QI and you do its dot product with all the keys, all the other keys, the other words keys. So it will be near some other word in the key space, the query vector. And when you see that this query vector is close to some key, some words key, you say that it is going to pay attention to that word. And so what you do is you softmax the energies that you get, how much attention, and then you multiply it by the actual value of that word that has also gotten computed in the process. So your net result is after this little bit of mathematics, simple mathematics, you get what is called the attention. And so you'll see that attention Q3 seems to be paying a lot of attention to one, two, three, four, five, six, seven, the three, sixth word and a little and quite a bit of attention to the sixth word and a little bit of attention to the fourth word and so forth and not much attention to the rest of the word. So that is the point about attention. It led to the classic transformer architecture, which is the well celebrated architecture. It has multiple heads of this encoder blocks or the sort of attention blocks, sorry, and each attention block has internally many layers of the attention, of the the attention each sort of attention head has about I think in the classic architecture six layers of attention blocks each attention block it does that it splits the word that is coming in into the kvq space vectors and then the rest of it is we talked about it in contrast so so if you think about it why is that such a greater thing because transformers can parallelize at the level of the word and and then because the sentences are independently processed then even there there to this parallelism in contrast and rN used to have the following limitations. They were slow because you had to feed in words sequentially. They couldn't handle long sequences well. They forgot the earlier parts. So the earlier parts, it begins to get muted or faded out in the thought vector, in the context vector. And it's not quite terribly conducive to transfer learning so you know it almost seemed relative to the breakthroughs that were rapidly happening in computer vision it seemed to be behind the times basically till the transformers came about so these are big things to be able to use transfer learning to be able to parallelize and do the computations fast. And so in contrast, the transformers are very fast. Their processing speed is just lightning fast. They handle long sequences much better. I wouldn't say that they handle it very perfectly. There are still limitations, but they do a much better job than the RNNs do and they are rather conducive to transfer learning. In fact, there's a whole today what most of the time you just take an existing transformer and utilize it for your purpose. So it brought about a sea change in the field and it sort of had the same people said that it was like the 2014 moment. In 2014, the CNNs or computer vision, deep learning for CNN, so sort of deep learning, it had a breakthrough. It was really when everybody began to pay attention to it. It was breaking all records, you know, the ResNet and the VGG and all of that. And they were like really showing astounding results. So in many ways, the Transformers in 2018, Transformer former paper original came out in December 2017 2018 was the similar moment in NLP world and so what we are going to do now is walk through a very very popular implementation of this but before I go into that we agree that our RNN based implementation is likely to be underperforming. So this thing talks about two architectures. One is ELMO which is a bi directional sort of RNN. It uses LSTM, the more modified, the gated, with the forget and the remember gates, if you remember, that version bi-directional using LSTMs. So it is RNN technology. So you can imagine that what it does is, if you have to predict some word here, and you have W1, W2, W3 is missing w five w six you have to predict this word. Vaidhyanathan Ramamurthy, W. And what you have to do is you could go in this direction one RNN would take this word. This word, this word, and then it will ask this question. Next word. Next word, right? And then you could do the other way around. We could take this word, then this word, the second word, it'll be this, this, this. And now you ask once again, what is the next in the backwards direction? And you use both of them independently. So you have a forward network. You have a backward network. network, you have a backward network, and somehow you train them together, backward network, you train them together to nicely be able to tell what word stands in between. So that was the state of the art. Then came an important sort of thing. it was it came the GPT to the outside not the GPT to the open AI come up with the GPT model what GPT model is it said something quite interesting about the transformer it says let's go back to the transformer and so now pay attention to this so it said let's pay attention only to this part if you really look at it as word after word after word f1 f2 f3 let's say the French words keep coming out they go back and become input to this decoder, right? And the thought vector has, I mean, the encoded message is already just sitting there being incorporated. But see what is happening in reality is this is predicting the next word. It's a forward direction from left to right. F1, right, leads to F2, leads to F3, leads to F4, prediction of the f4 right so suppose you have these three words in place you can predict what f4 will be right that is what the decoder does and the GPT architecture it basically said that let us a train let us do one thing what RNNs were doing, we will do it with the transformer, but we will forget about the encoder part. We'll only use the decoder part. So when you only use the decoder part, let me highlight this with a thicker line. This is when you, when you just use this and you say, forget the encoder part in some sense and just use this, we can get away with it. You treat it the way you would treat an RNN, sequentially produced, but make sure that we use attention, attention heads, not the traditional RNN heads. So then this is your GPT model. This focuses on that. So that was the other state of the art. The good thing with GPT was it was attention based. So let us realize it was transformer based, whereas ELMO was LSTM based. These two existed and they were doing quite well actually. The GLUE scores, so one of the benchmarks by which you can compare and a pretty important benchmark is something called GLUE. Just think of it as a measure of bragging rights so if you look at this and I'll try to highlight a few sentences yeah okay look yeah if you look at open GPT the scores the bird the the scores that you are getting is 75. Sorry, let me highlight it because Red, you kept for this. Do you see 75 here? That was what the GPT was coming up with, isn't it? And what was ELMO doing? ELMO was not doing particularly bad. It was at 71. Right. So I'll write those numbers down, 71, 75. And then came the BERT. And the sense of BERT is in this case, they make two models. One just to show that the fundamental architecture is superior. They used the same number of sort of the same size neural network as GPT. And they called it the BERT base, one version. Then they used another version, which had a lot more layers like in the, any one head, the attention head, it just put in many more layers, I believe 16 layers or something like that and or 12 layers we'll see what it is somewhere in here is the detail and then they also increase the number of heads the multi head had many more heads and then they went with that that was the bird large and you can clearly see that bird large is 82 points so this, if you look at it, I'll just maybe mark it in green. If you look at this number, 82.1. So it suddenly made, and this much point jump is a huge jump. So it made quite a huge jump from the state of the art. So you'll often see the word SOTA written in this papers, S-O-T-A. SOTA stands for state of the art. I'm mentioning it because someone asked me. So you clearly see that it begins to do that. So now what does it do and how is it implemented? Let's go and study the paper and then I'll explain that. But just to give you a preview what what they did is they said they took the opposite approach they said forget about the decoder we will only take the encoder this is what BERT is made up of it's a very surprising idea it is saying you can do pretty good transformer by taking only the encoder half of the transformer. See, when you look at the GPT part, it even, it sort of makes intuitive sense. You are saying the previous words that have been, let's say that you're doing machine translations, the previous words in French that have come so far, they form the context of the next word that you will predict, right? Along with obviously the original words, the original English words and so forth. So together you can feed it all into, forget about the encoding, you can sort of feed it all right here, the English also. So then you'll get your translation right away. So your vector, your input vector is sort of the words of English and the words of French and all in one go and forget about the encoder. Whereas BERT takes the opposite approach. It says, forget about the decoder. We'll just deal with the encoder. So how does it work? How in the world can it work? Let's go figure it out. So in this, I will start reading this paper from the, give me a moment. I'll start reading it from the beginning. The most important thing that they say here is in the very first line, they say that unlike recent representation models a bird is designed to a pre-trained deep bi-directional representations and so the two the most important things that it says I guess what should I do here more colors or yellow where is the highlighter I'm just looking for a highlighter here now it won't let me do that okay this tool has a issue maybe I can change one of them into a highlighter can I do that more colors no I like to is your first one, Asif. On the left here? Yeah. That's an eraser. Oh, okay. That's an eraser. It's all right. I will sort of let it be. I'll just use yellow to highlight it. So the sentence that we are paying attention to... Highlight is your fifth one. Oh, right, right, right. Thank you. I did set it up, yeah that's what I thought I had it. So there we go. So this is the crucial sentence, a bird is designed to pre-train deep bi-directional representation from by jointly conditioning on both the left and in all layers. So let's focus on that. A lot is packed in there. First it is saying that you can use pre-trained, like you can use it for transfer learning, right? You can pre-train a bird model and then use it in different contexts for other purposes. That is one. And the second thing that it manages to say in just one short sentence is it is conditioned on, or a very sort of an oversimplified version would be, it has learned from both right and left context. In other words, the words that are before that word, if you take a word in the sentence, imagine that you're trying to either predict the word or do something. So it has learned from the words that come before and the words that come after, right? Somewhat like ELMO, except that the ELMO, the bidirectional is using two effectively independent RNNs. All right, one trained to look, move from left to right, the other trained to walk backwards from right to left in English. So this one says that you can literally jointly condition on both left and right context in all theirs. That sounds quite interesting and we'll see how. So then they go on to say the consequence of it. As a result, the pre-trained BERT model can be fine-tuned with just one additional input there. So suppose you're writing a classifier. Imagine that you're doing sentiment analysis. So what they're saying is once you have a pre-trained BERT, all you need to do is to make it into a classifier. What do you do? Roughly speaking, you realize that you take some form of output from BERT, a hidden state from BERT that it produces, and then feed it into a softmax layer. Isn't it? Or a sigmoid or whatever you want. And now you've got a sentiment classifier. Am I making sense guys? Asif, when you said you take a hidden layer, are we introducing one or are we just... Whatever. So think of the hidden layers, whatever comes out of the transformer. Oh, okay. So the basic idea is this, suppose you want to take a transformer. Let's say that this bird transformer, you give it an input. And the reason you call it hidden is because whatever bird produces is not what the final state is. All you need to do is put it through a softmax, a classic logistic regression. You remember your basic logistic regression, right? And then right away it is producing the sentiment and the output would be the output of this would be positive or negative sentiment. So to the extent that the hidden layer, we always use the word when it is not the final output. So the birds output, so therefore by definition is the hidden layer when you use it for some other purpose. Are you understanding that now? Yeah. So the output of the bird is basically feeding into something downstream. And people use the word downstream, whatever task you want to do, downstream task. And that speaks to the generality of it. Actually, one of the most amazing things is that bird, when this paper came out and in the very first version of it, a single bird, same architecture trained with all the weights there and needing just some fine tuning. What does fine tuning mean? Once you have trained the bird using, so you have to pre-train the bird. The steps are, you go, one, sorry, one, number one, pre-train. This is the hard part. Go pre-train on a large corpus of documents. And so once you pre-train, you get something, you get a BERT which is ready to be used. Number two, what you do is now BERT will have gazillions of parameters, right? The base model, I believe has 140 million parameters or something like that. The BERT large, I believe has close to half a billion parameters, right? But but those parameters what you do is they almost for whatever task you do you don't have to shake them up a lot they just need to be fine-tuned to your problem so what happens is when you jointly train this bird with where the weights are already close to the right answer with respect to your problem what is happening is that some weights here are getting trained right the standard logistic regression weights are getting trained. And when you back prop, maybe the parameters that are there in the BERT model, they will go through some tweaking, some fine tuning, some little changes with the gradient descent, but very quickly you'll be able to train it. And in this paper, they make it a point they say that that if you train use it for all these tasks and a whole variety of tasks they speak of actually let's jump ahead and look at the bragging rights here so if you look at this here this table these are all different things. Sentiment analysis, question answer, the question answering, one, two, three, four, five, six, seven, eight. So eight different tasks. If you look at eight different kinds of tasks, one of them is basic things, ablation. So if I give you two sentence, can you tell if the second sentence should actually follow the first? Right? There is a relationship between that is sort of the entailment, sorry, not ablation, I apologize, entailment, that it is really should be the one following it, or is it some random sentence, things like that. So that is it. There are eight different tasks. And if you look carefully in all eight different tasks this seems to have like if you pay attention to this guy here you notice that this seems to be beating all the previous if you just compare it to this the compare it to the existing state-of-the-art but do you see how much improvement there is everywhere? And what they are saying is that once you have trained, pre-trained bird, then from a pre-trained bird to go to these tasks, you know, putting a head layer, either a classifier layer or something like that and training it, you can train these all within one hour. Right? So that is the power of transfer learning that I was talking about. Once you have a fully trained bird, you just apply it to your specific domain, do a last fine tuning on your hardware, and on any reasonable hardware, like a single workstation hardware, you can finish the entire training within an hour. And so this has become the, this is the power of it. Like, that's why it's so practical and it just took the world by storm in our little corner of NLP world. Does that answer your question? So now, So now, this, so it goes on to say this point very well, as a result, the pre-trained BERT model can be fine tuned with just one additional output layer for whatever task you're doing. To create state of the art models for a wide range of tasks, so it's surprising how versatile transformers are. In fact, just about every area of NLP seems to be benefiting from the transformers. And so with that thing there, we have a situation where you have the same level of legibility that you had for like in the world of computer vision. The other important thing that it says, and so it says all sorts of tasks, for a wide variety of tasks such as question, language inference, et cetera, et cetera. And the more important thing is for each of these tasks before BERT came about, people had custom architectures for each task. So if you were doing question answering, you had an architecture that worked best for question answering. You couldn't apply the same architecture to sentiment analysis, right, and things like that. So there was a literally you need one solution fine-tuned and specially trained for one purpose. Right, but BERT came and as I said, its crown achievement was that in one fell swoop it gave you an architecture that pretty much became the state of the art in many many tasks or eight tasks at least. So we'll learn about that in a moment. So now this language pretty much I've explained it to you. So here is one word in this paper that is crucial. So you can read this and you will realize that the rest of the things is what I have been talking about. And so one of the nice things is you can pre-train all the parameters you know when you're doing the fine tuning. You can actually fine tune all the parameters not just the last few layers. It works just as well you know because there's only a little bit of a tweaking. So in a few epochs, you're done. Now, in this paper, they introduce this BERT. This is where they define the word BERT. And it goes on to say that it does it. And the trick that it does is it uses an old idea. It is called the closed task. Looking at the date, 1953, it is probably a concept that has existed in natural language processing for almost 70 years. Right? Closed task. what is the closed task it is basically this roughly speaking that if if you take a sentence uh a b c d e f g let's just say this is uh these are these are the words in the sentence so suppose i go and hide this d i mask it and i ask you to tell me what word it was that I that I hid from you right so the ability the models that can do that they are masked language models and we talk about that masked language model so they coined this term a lot of the literature is interchangeably people use the closed task or MLM when people talk about it. And we will use this as one of the two techniques. So let's see what is the technique that they actually use. Now I talked about the prior work before BERT, ELMO and GPT. before but elmo and gpt i won't talk more about it so let's go and see what yeah bird does so look at the bird architecture it first of all it says that it uses the famous attention is all you need remember this paper waswani is the attention is all you need paper right and it says go read it is all so it says that the implementation is almost identical to the to the original right the transformer almost with the difference that you don't need all aspects of it so this is it and we denote the number of layers as this this this we primarily report results with two models. So look at the bird large, pretty big. Isn't it? So the number of hidden layers, sort of the number of layers in a transformer block, just one block of transformer is, or one head is 12 or 14 or 24 my goodness 24. the in each of these the number of uh what is it attention so how many attention heads are there you're using 16 attention heads so it is like attention heads have practically doubled and the number of those attentions inside each head is almost of those attentions inside each head is almost become six times four, four times, pretty large. So it leads to the total number of parameters, which is 340 million. And the hidden state that this bird produces is a 1024, a vector of a 1024 dimension. So that's a pretty large vector that it produces. The base version of BERT produces 768. So when you do the lab, what will happen is you try to train or even fine-tune BERT-LARGE. It takes a little bit of time. It can take an hour. So people, when they're just experimenting, they start with the BERT-BASE. They see good positive results, and then they move on to BERT large. That's one way of looking at it. So this is that. Now comes the input-output representation, and this needs some bit of explanation. The way it deals with is the inputs are actually not words. And the reason for that is there was an interesting finding a few years before this paper, which was the work of the word piece embeddings. People found, so let me just put it this way, that words, suppose you take the word, let me think of a word. Let's take the word transformer itself. Transformer. you realize that actually the building blocks of these two words can be thought of as, where am I? This. One is trans, the other is forma. Because I can have, just to illustrate this point, I can have words like transfer. Transfer. Transfer. Does it have S? Transfer, yes, of course it does. A transfer. A transistor. And you can think of so many words that begin with T-r-a-n-s isn't it trans you can think of a lot of words right and likewise former you can think of reformer instead of transformer right deformer and god knows what are the words you can cook up, but I'm sure you can cook up many other words that all have that. Are we together guys? Right? So, and then you can further break the word former and you can say it has a piece inside it, which is actually form is the real building block because form is there reform transform just form uniform right so on and so so many pieces that you create so people realize that it is better to take as building blocks for For tokens, when you convert words to tokens, take word pieces. And there's a bit of technology to it. See what happens is you first start with character level things. Let's say that you start with A all the way to Z. They have certain frequency. You see what their frequencies are. And suppose you create a frequency table, sorted it. Then you realize that if you have A and if you do a combination of a and uh well okay let me take this word form suppose you look at the word f o right soon you realize that f o r m f so suppose f is here but it's in relative frequency sorted by decreasing frequency means most frequent characters at the top soon you realize that the word piece, F-O-R-M, or the word form, or just for, is far more frequent in text than just a F floating around. F floating around is a little hard to see in literature. So what happens is the form comes up, for comes up, right? And so on and so forth. There's a little bit more to it. And so what happens is that little word pieces, they begin to bubble up, right? In this, and you take them more seriously, right? And you take them as tokens. So suppose you want to pick the first 30,000 very popular tokens, right? Or 10,000 very popular word pieces. It is better to take the 10,000, not the 10,000 most popular words, but 10,000 most popular word pieces in the sense that each piece is in some sense information dense, right? And so you use some notation. You are saying, for example, if you say form, former, you can say, I break it up into two tokens, form, and then you use a special thing to say, and then er is the suffix, and er is the suffix to gazillions of things and you treat this as a suffix notation. You're saying this is a suffix. So there's a little bit of machinery here, a little bit of technical details, but those are not very important. But the important thing to realize is that it's a better idea to deal with word pieces as tokens. So the gist of all this discussion is to say that use word pieces as tokens. Okay It leads to much better results. So that is what they mean when they talk about using word piece. Where do they talk about that? Ablation studies and so forth. Fine- i lose the place pre-training but yes we use word embed word piece embedding so now you have word pieces you can do the embedding of the word pieces right you use that then comes the embedding question itself what form of embedding do we use question itself. What form of embedding do we use? So far, we have said that you can remember we learned about word embedding. Now, when you use word embedding like word to back or glove, they're a wonderful idea. I don't know if you were impressed or you are happy at the whole concept of word embeddings, the way it was done. I certainly was when I learned about it. But it has one limitation. It is context unaware. Because you have learned from the entire corpus of human language and you have just looked at the word vectors. Yes, you can do king minus queen is man minus woman and all sorts of beautiful relationship. But there is a problem a problem homonyms your words that in different contexts have different meaning right so for example a transformer in uh ai means what we are talking about isn't it it's architecture neural architecture but when you talk to an electrical engineer a transformer is what hangs on the pole or in your neighborhood and does the voltage change, isn't it? From high tension, a very high voltage that is used for long distance transmission down to more residential area voltages. Are we getting the point, guys? Sometimes the word has many meanings. If you open the dictionary you will see that a word can have many, many meanings and each meaning is captured by the context of the context where it appears. Isn't it? For example, if you say the utility company had to purchase a lot of copper wires to go and repair the transformer. There, the meaning of transformer is entirely different than saying we used a transformer for sentiment analysis. We certainly didn't use that big mechanical, big electrical device for sentiment analysis. So I hope that is an obvious thing that I'm saying. So then the question came, can we do a different embedding of words, which is context aware right so that work was happening along the way and these people benefited they said yes let us have context aware embedding of words right so I'll not dwell into that very much just to mention the fact that that's what we eventually will use and do. So that is that. Now next comes the actual embedding part. So I want to go over the architecture. Did they mention it here? I seem to be going. Okay, this paper is written in a interesting way. It sort of talks about the results and its differentiation from other architectures before quite a bit before it goes into the details but I'll go to this picture are you guys able to see this picture here yes on the left so this needs a little bit of a description you remember that in the classic, you would take the word embedding plus the position embedding to make the final joint embedding of a word, of a token. Do we remember that, guys? You take the token embedding, the word token, then you take the position embedding. But here we use one more thing, which makes it interesting. And to illustrate what is meant here, what you do is typically think of two sentences. You don't feed it one sentence at a time. That is for reasons that we will understand in a little a let me take a color it is taking two sentences s1 s2 together right so then the question is how will it look so traditionally it would have looked word one word two of the first sentence right of the first sentence all the way to word let's say n of the first sentence, right, of the first sentence all the way to word, let's say N of the first sentence, then you would put a demarcator, something called a separator, full stop, you know, something that stands for the fact that you encountered a different, you're starting a new sentence, and then you would get W of the two, so forth, W, W of the two, so forth, W, N of the two, of the second one. So now there is one extra thing to do. So first of all, we need to distinguish whether you are in the first sentence or the second sentence, isn't it? It's a very basic thing. So that information is this layer, right? So for example, for the first sentence, it would say, okay, we are in the first sentence, we are in sentence A. Actually, since they use A and B, let me use A, B. Sorry, I should have noticed that they use A, A, A and B, sentence B. I don't know if little bit, you can see what I'm writing this quickly little thing. B. B. Two sentences. They're separated. So that's the first thing you do. So is position embedding easy to understand? This comes straight from your classic transformer that we did this week. The additional thing is this segment embedding. Segment, one easy way to think of segment is which sentence are you in? And the third is of course your classic word embedding. You take all of these three to create your complete embedding of this joint thing, but there is one extra thing you do. What you do is you prefix all of that with a new thing. You notice that. So this is like the beginning. This is saying it's just the beginning. EA is not really a word. If you look at this sentence, my dog is cute. He likes playing. But before that, there is a pillar. There is a token to say it's the beginning of the word. And when it gets encoded, you deliberately call that particular token the CLS. Now, why CLS? That word looks rather cryptic. The reason for that will become evident in a few minutes. Right? But just say that you put in the beginning, you put a token to specify that you're about to feed in this two sentences together. You just take two for the timing, but two sentences together. And then between the two sentences, you have the end separator, isn't it? And in the end also, so let me highlight the ones that you should pay attention to. This, this, and this. Do these make sense, guys? You basically get the encoding, whatever the embedding is, or the total full encoding of the word, my dog is cute is cute of course those would by now be numbers you know there will be an array of numbers typically one zero two four or whatever it is long array of numbers and for each of these words or word pieces rather let's not use the word word do you notice a word piece here if you observe this one you see the word word. Do you notice a word piece here? If you observe this one, you see the word piece coming into play here. Isn't it guys? With the two hash marks saying it is a suffix. So that explains the hash marks, right? Actually, I should have given it another color so I could leave notes here. Let me call, what color am I left with? Maybe, let's take a little bit darker blue, different color blue. Perfect as a word piece, right? So these are word pieces, word piece, pieces, encoded, encoded, fully encoded. So this is what will go into our transformer. Are we together guys? So far? You let it go into the transformer. If you have understood till this far, actually you have understood everything. There isn't much more to this paper. Ultimately, you'll produce a hidden state. This transformer architecture will, at the end of it, produce a hidden state, right? The output. And what you do is, let me actually write it here in this. So this thing goes in, let me say CLS, right? And the cow, by the way, is this blue visible or is it hard to read at the bottom that I'm writing? Okay. Visible, right? The cow jumped over the moon. And then let us say there is a separator and there is another part that is going in actually, I should keep it horizontally there, But let me decrease the font size so I can stick more in here. The cow jumped over the moon. And then after that we have another important thing. What else do cows do right after jumping over the moon. Mood with delight. Then she mood with delight. Mood with delight. Right? And then you have the separator, the end separator. You have the end separator here. So this is your separator here. So what happens is that all of these word blocks, these, and obviously these, I've written them as words, forgive me, but they're word pieces. So this would probably be a jump and then hash, hash, hash, hash, ed, or something like that. So I'll just leave it there. Just being sloppy here, but we'll leave it there. So this goes in. Then what happens is you feed this into the transformer, the whole transformer architecture then transformer architecture then what do you get at the end of it you get some output the output that you get and I believe they use the output of a hundred and two four dimensional this is your transformer but transformer but transformer and then a hidden dimension dimensional output for a large and I believe is 768 dimensional output for for base which is comparable to the in in terms of parameters to GPT so they wanted to show that it's better. And also it's easier to use if you're working on modest hardware. So then comes the interesting thing. What do you do? So suppose you want to send it to a classifier. Say your task is sentiment analysis. And let us say that this has been pre-trained. Imagine a pre-trained bird somehow magically, and we'll talk about how you have trained it, but imagine that this bird has been just pre-trained. All the 340 million parameters in the large have some values and those are sensible values for for most stars so suppose you want to do sentiment analysis what would you do let's say you put it through a sigmoid either a sigmoid or a softmax right whatever it is you put it through and it will come back with positive or negative. Are we together. And now what do you do you take a corpus of you take an input data set. Take for example the SSD, whatever take our, you know, movie review data set, take the movie review. Review data set, for example example or whatever you want this is a classification task classification or whatever your data is what will it contain it will contain sentences and for each sentence reviews and it will contain positive or negative isn't it so now remember that the definition of sentences change the whole logical unit is they're treating it as a sentence but that's all right what you can do now is you can use this to fine-tune or do final training fine-tune training of work in your particular problem you put it together all you have done is you have attached a soft max layer let's say and then yeah you're Jay Shah, Dr. And that kind of training when you do transfer learning is called fine tuning. So you have the fine tuning training of BERT and you're done. And then you can use it for many, many different tasks, but we'll come to that in a moment. But now let's ask yourself, that is all very lovely that most of the parameters come almost near the correct answer for a large number of tasks. And all we have to do is screw in the last layer for whatever task we want to do right but how in the world did we pre-train the bird at all so the pre-training of the bird happens using two tricks uh the task is and that's what this paper talks about and now we'll go into that how do we how is the bird free trade this is a crucial question isn't it guys we are saying that bird is so clever that um most of its values uh weights are already close to perfection for most whatever task that you want to do and all you have to do is a last mile training a fine-tuning training right then it begs the question that what is it that you do to make those magical weights or parameters to happen would you agree guys that that becomes a crucial question yes right so now we will go to that question. So let's go to that question. How do you pre-train it? So the way you do so, so fine tuning, of course we understood. Let us go to the pre-training. So how do you pre-train it? You use two things and this is just two particular approaches you use and these are task one oh sorry this guy masked lms and the other is next sentence prediction so let me talk about both what you do is you give it in the task so we'll talk about the master LM task one must LM you you basically say go go become good at you say go become good at these two tasks. And what are those tasks? Task one, masked LLM, masked language model. Masked language model. What you do is something pretty interesting you take a sentence right let's say the sentence is the cow jumped over the moon in great delight right i will take this and you just go and nuke out some of the words. So one easy way to nuke out would be what? You just go and you say, well, suppose I remove this. So it becomes Mars over the moon and maybe I'll remove the word this also. But for the time being, let us just keep it simple and take one word that we nuked out. So I'll remove the word this also, but for the time being, let us just keep it simple and take one word that we nuked out. So I'll bring back the grade for just for simplicity. But generally they're saying that empirically, they found that masking 15% of the words was quite effective. Quite effective in training this, in the training. Well, I'm writing in almost a curved way. All right. So you could mask 15% of these words in a in a given sentence in the sentences that you fed to the machine and that would be good enough but we will say again I'll just give our example now our sentence becomes the cow and then it becomes the token mask right and then the rest of the thing jumped over there and remember the first token is your CLS right jumped over there and then you say your end and so forth separ separate, etc. Right? And then what you do is you can feed it and then you can feed another sentence with masks and so forth. Now you ask this bird that go and produce. So what happens is that the final state, so I'll write it very simply, input. You give it the input with masked you give it the input with masked masks and you you will get an output what you do is see now here is the thing each of these words will produce an output a hidden state output right because they are all going through the system separately do we remember they will all have their KV attention and all of that. They are all going through this transformer in parallel, right? Except that, of course, there's a lot of self-attention happening between the words. They're looking at each other and asking, are you interesting? Right? So at the end of it, they will all come up with some hidden state. Are we together? So this is the hidden state of CLS. Output state. This will be the hidden state of the word hidden state of cow and so on and so forth and hidden state of separated And then what you do is you pretty much throw away all the hidden states and this is the crucial thing and a very very interesting idea you only keep the hidden state that the beginning of the sequence has and what happens is the beginning of the sequence in some sense has has also looked over all the words has been paying attention to all the words in the sequence. Isn't it guys? Because every word is paying attention to all the other words. So as a convention they wanted to pick one and they picked up this special one, the first one. The beginning of that, the encoding, the hidden state produced by the first word. And what they do is that is your hidden state vector, hidden CLS vector, and that they feed into, into let's say a softmax. Softmax layer, what will it contain? Your entire vocabulary, right? So suppose your vocabulary has 30k, so the word pieces has 30k to the word the word pieces as 30k tokens of word pieces right unique tokens so what happens is that this softmax how many nodes will it have guys. 30k notes. This is a softmax classifier. Am I making sense, guys? All I'm saying is you take the sentence and you mask the one word and you feed it in and whatever the output comes for the first word, you just feed it into a Softmax classifier. And then you ask, which word is it favoring? Whichever words, the Softmax will end up favoring one word, the other words will be smaller, smaller, smaller. So some word will be here. And you hope that the word that it favors is jumped. That once you train it well in the beginning, it will favor all sorts of random words, right? But gradually you compute the cross entropy error. So softmax, if you just as a reminder, the basic machine learning, when you do softmax or classification, you use what? So, Asif? Yeah. So let's say the mask word that you put in your input is a noun. Wouldn't this mean that in the context of your sentence, it could also infer that the mask output would be somewhere else where there's another noun. You see what happens is by noun I probably mean you don't mean the cow horse like a horse a bird or something like that you probably mean a proper noun like the name of an organization or something some a word that is relatively rare is that what you meant? a word that is relatively rare is that what you meant no i mean if the cow was the one that was masked can can it put the moon and in the sentence and then the cow would be the one that would be in the end as masked okay let me try to understand what you said suppose the sentence is the mask rather since i'm using yellow for mask the the mask rather since I'm using yellow for mask the the mask mask jumped over the mast jumped separator now not now ask your question I've of the cow is the noun. What is the question now? So won't there be instances when it's paying self-attention that it would predict it as the moon jumped over the Indian mask? No, it can't. All you have to do is fill in this word, and the position, the word moon is already there in the sentence. So it could predict the moon jumped over the moon, right? But once you train it properly, hopefully it shouldn't predict that. You know, once it has been properly trained, it should probably pick cow or whatever it is, other things that habitually jump over the moon. cow or whatever it is, other things that habitually jump over the moon. Oh, so the issue is only just a matter of training to be able to do the right thing. That is the whole point, right? See, what happens is that how do you train? You make it produce random. So suppose this is totally untrained. So there are random weights sitting here, right? And then what happens is, so when you feed it to a softmax classifier the first time around it will point to some absolutely absurd for example it may say the castle jump over the moon right well there would be a huge cross entropy loss coming from that mistake no because you find that the word cow which should have been there its probability is extremely low so very low probability so let's take this example let's make it very literal right suppose you take that cow a probability of a cow turns out to be 10 to the minus 6 as an example right so what is the cross entropy loss? You remember the cross entropy losses? ln p of the cow. This was the right answer. What is the log of this? Well, you use natural log, but let me cheat a little bit and use base cheating. If it were base 10 then so i'm cheating a little bit you would be something like uh minus six isn't it but your definition is minus so minus of minus six is six so what you're saying is you're accumulating the loss over every of the data point data point being one sentence going in a couple of one sort of a sequence going in. So then what will happen? This has suddenly contributed a loss, lot of positive loss to it. You need to minimize this loss. How will you minimize this loss? You will do the gradient descent, the back prop. Remember everything that we learned, right? The standard machinery of machine learning will kick in. So as you train it epoch after epoch, these weights will, let me not use the word weights because there are biases also, parameters will adjust to better values through back propagation and gradient descent step. Are we good? Yes, Asif. I think my question would be, how does the mask affect that whole process? Let's say your mask was a noun in this context um how does how does having a mask like because i is that supposed to mean the mask supposed to mean it's a specific word that you just do that you were just hiding or is it this a word piece you're just hiding a word piece and so your label data is literally this this is your label data and what is the word that you hit? Cow was the word that you hit. You ask the sentence, the classifier, to take this sequence, which is this entire sequence here, and the output it should predict cow. So whichever word you hit, it should be able to predict that word that's all oh okay i see so the mask is not like a placeholder for for a noun in general it's actually something you've already introduced before you just go in you just go and pick a random word and now that is the one that you're hiding mask means to hide that is the one that you hide and that that is now the answer you you ask this it's like a game you play with kids right you um in school you must have gone through all these exams they'll give you a fill in the blank and in the blank they'll give you choices abcde right and they will see whether you can pick up the actually maybe i should introduce this mask business in the quiz this week i'll give you guys a lot of fill in the blanks and see if you can fill in the right for it. That is literally training the classifier. Making BERT predict what was it that was hidden? What should have been there in the blank? Like the hidden word is the label data? It becomes the label data. So suppose you take a sentence, the cow jumped over the moon and I take cow out randomly you can pick any word you want so suppose i pick cow so what does it become cow becomes the target variable isn't it you expect bird to read the ma the masked sentence and somehow be smart enough to say this mask should be this fill in the blank, the blank has to be replaced with the word count. Okay, I see. So in a way this acts like how we do dropouts in neural networks where we try to let the system catch up. That's sort of the same intuition. The dropout is a little bit similar. Dropout is for regularization. This one is just basically saying, I'm playing a game with you. Jay Shah, Dr. Anand Oswal, He or She instead of cow if it said the horse jumped over the moon it would still be sensible isn't it but if it said the uh something very absurd like for example the hammer jumped over the moon hammers don't tend to jump on their own right or something like that then you would attach a penalty you would say that well you know this this transformer hasn't quite understood the english language after reading so many books in other words being trained in all these sentences it has still not figured out that the most likely thing should be a cow and not a hammer so then you train it some more are we getting the sense guys yes so this helps specify the language even into deeper detail by understanding. Exactly. Yeah. And so this is one of the ways, actually there is a little subtlety, there are some more things you do, but I don't want to go into that. When you mask this thing, where am I? No, it has again jumped. Okay, because I'm writing down here. So when you do this, you can, you know, you mask it about 80% of the, so 15% of the tokens are masked. But even when you mask, quite often you mask it. Sometimes what you do is you replace it with a random token, right? And then, so anyway, actually don't go into it. Let's not go into that for the time being. Let's stick to the big ideas. The second thing you do the second task is a pair of this. One second, let me just write it out for us number two sentences. Go ahead. I'm trying to understand this one where the masking, right? So we already have pre-trained model. The last player- No, no, you don't have pre-trained, you know, that's the point. We are talking about how to pre-train BERT. How to pre-train. See, let me go back. I think you guys missed the whole point. I said that, see, if you have a pre-trained model, then fine tuning it for a specific task is easy. But the big question that remains is how do you magically pre-train BERT in such a nice way that it can be used for so many tasks. Yeah, I forgot about that one. So if here when we are pre-training it for say in this case the level data says cow but next time when there is blank I mean there is a gap like this the the word is missing will it understand the noun to be kept there or like what will the modern English the beautiful thing is you don't even have to ask about the part of speech this in tuning this right because so many sentences have gone in you don't even bother it will figure out whatever word it points to will be a noun okay right but you don't have to just say a pick from one of the nouns or something like that you don't have to do any extra effort it is smart enough to know at that moment yeah so that's my question it the the word which it learns will be now yes it will be it will pick up the cow right so the second thing you do is you do this you say that and so let us read this because this uh needs a little this may be worth reading it aloud the second task next sentence prediction so suppose you take a long text and take a sentence in the next sentence and now the two of them together becomes your input the the important question is should that sentence follow the the previous sentence so in other words should sentence to follow sentence one does it make sense are we together So I'll take an example. What you do is, and obviously here it makes sense. People found that it is better to pick these data from actual books where there is a narrative, where there is a very good cohesion between the sentences. Don't just take a random collection of words or sentences from somewhere. And that would not be the right way so read books books and wikipedia's in fact that's what they do google has a huge collection of books that it has read right a massive amount of books and then it also has read wikipedia so you do the basic cleanup you know you throw away the table of contents and headers and this and that from Wikipedia, you get to the core content where something is being described. Right? So now suppose you have something like you see, let's go back to our sentence, the cow jumped over the moon and then what was the next sentence we wrote it was in great delight or something to that we said the cow jumped over the moon in great delight well did we follow it up with some other sentence I forget then she moved with delight forget then she moved with delight okay then she moved with delight these are two sentences we can say that this one is sentence s1 and this one is sentence s2 right so this is valid isn't it hopefully there is some book somebody has written where this sequence exists so this is valid so what you do is s1 s2 you say true you have data like this s1 followed by s2 you give it a score of one let's say right or true or whatever it is or maybe true it chooses just binary classification and then what you do is while Suppose I put S3 here is equal to something like that. And the derivative, or maybe something like that, the derivative of X cube is 3X squared. Let's say that you find a sentence like that. So now this is a non sequester, isn't it? S3 simply doesn't follow from S1, does it? So what would you say that it is very unlikely that you'll find anywhere in the world a sentence like this. A combined sentence like this. A combined sentence like this. Would you agree guys? Yes. So guys, make sure you understand this. This is a very basic thing I'm saying. I'm saying that you take valid pairs of sequences of sentences, A by b s1 followed by s2 and you just pick up you go through all the literature and you make some valid pairs all the valid pairs and then what you do is for each valid pair also do this game you take the s1 and replace it with s3 randomly chosen from somewhere else randomly randomlyly chosen sentence. And let us hope that very, very rarely will it ever make sense after S1. So you give it a value zero. This is not what it is. Are we together now? And so what you do is you create a lot of data. 50% are valid and 50% is invalid. So you end up really with a nice balanced data. And then once again, you can train this bird. Are we together at this task, guys? And the weights will adjust. You ask it to somehow always tell the right answer. Does S2 follow S1 or not? Can it be trained guys? It's just a classification task, right? A binary classification task. Right. Very simple. So when you train this, once again, the weights will go and get adjusted. All the weights and parameters, biases, they will all go and get adjusted all the rates and parameters biases they will all go and get adjusted everywhere so at the end that is it that is all the pre-training of bird and that finishes a long journey and it turns out that and that's a sheer simplicity actually they have not done very custom hacks for each of these tasks they They have done something fairly universal. They have trained it on mask sentences and they have trained it on the next sentence prediction. Task one, on task number one mask language model in other words be able to predict the mask and sentence prediction and once you do that it turns out that all your weights of a pre-trained and almost magically now you can apply a general purpose transformer for various tasks. And once again, for each task, all that you have to do is do the fine tuning, right? So for each task all that you have to do is do the fine tuning right so for each task right it's all natural language processing tasks yes so for each now what you do is you for each specific task for each specific task take the output so the output of the bird is always called the hidden the hidden output just because you know in the great grand scheme of things when you attach it to something else birds output becomes hidden because it becomes the input to whatever it is that you're putting there. Take the hidden of the only one token, the CLS token output. The hidden, it's really the word people use, the word is hidden, but think of it as the output of BERT and feed it into your your classifier, etc whatever the task is classifier etc etc whatever your task is that is it right and that is what this paper is saying that's all there is to this paper right it's a it's a wonderful paper actually at the end of it it's remarkable that they've come up with this idea and you can imagine how many experiments or how many things they must have tried before something like this worked. Because it's not an idea that would easily occur on its own. A lot of, you know, trying this and trying that must have gone into it before they nailed down this fairly elegant general purpose architecture. So this is the architecture for bird guys. Asif sir, can this be used as like a chatbot kind of thing? Chatbot, question answering, of course. Last layer tuning will be, let me, I mean, training will be... It's very easy. All you do is you take the text. So you take the input text, question. So whatever the user has asked the text, right? Let's say in the chat, you are the customer service agent whatever the user is put here you feed it into your bird right and it will point to the right answer right it will generate the right answer so it will produce the answer the appropriate answer that's all right after after like little bit of training. So that is it. All these things that you think, eight different tasks you see in this paper mentioned. This is a question answering is in that. You could train it now. So what it means is that you can take the pre-trained bird and feed it into your specific problem just by adding a logistic regression or whatever it is. Why just logistic regression? If it chooses your your fancy throw in a sme or whatever you want and then jointly just train it so for training how much data should be should suffice for you know remember there are two trainings bird comes pre-trained that needs such enormous amount of data like so that is already trained one need not to worry about that part right is it not worry about that and then the second part the fine tuning that's the beauty of it because you're just shaking the weights a little bit you need very little data i see so sir in the like little data like when you say can can we quantify like a Like when you say, can we quantify like a problem? It depends on the problem. Let's say you're doing sentiment analysis. I mean, surely you would like to feed in a few thousand or tens of thousands of sentiments or something like that. So in deep learning world, right? Numbers are a little bit large. The datasets are never like 30 data points or something like that to train i'll assume uh as if to pradeep's question if i were to train a chat bot that's going to answer questions typically from an faq collection that i already have that faq corpus would be what i'm training on right exactly yes or fine tuning or tuning, fine tuning on is the word you would use in the transformer language. You'll do both those things. One, you're kind of masking individual words and helping that. No, no, no, no, no, no. See those guys, the creator, the pre-training of BERT has already done that. All the weights are there. When you do it, all you do is you put a soft max on top of it pointing to the right answer and but you just do the fine tuning of it which means that you don't play that game of task 1 and task 2 mask language model or the next sentence prediction you don't do that you solve your real problem see here's the thing suppose a question is asked think about it this way a question is asked question there are answers a1 to a n possible think of it is this not a simple classification task right forget transformers and everything completely this is a classification task isn't it right in question goes in and it has to point to some right answer AJ isn't it that's all it is and so what we are saying is this classification task you do with two steps you put you put the pre-trained bird you don't touch it you just put the bird there except that you leave the weights to be fine-tuned and then you put your normal classifier put your softmax right as an example put your softmax so the output of this goes into the softmax and that output points to the right answer and then what you do is you do a little bit you take a bunch of table of data q1 a1, A2, you must be having some repository. Let's take an example for Oracle customer support. You have maintained a large database of questions and what is the correct answer to those questions or text? You have that Q and A. So what is this? You're just doing a prediction here, right? You can use this to fine tune, fine tune the model. So your model is not just bird model is all of these things together because here are the weights that go into the softmax. You're multiplying the bird output with the weights, right? Fine tune the model, which right fine tune the model which means fine tune the bird weights and fine tune and tune the soft max weights okay so i think but then in this understanding the implicit assumption here would be you pick a bird model that's already been done let's say for the english language and you use an English FAQ to complete this process exactly so whatever specific task you have you fine-tune bird for that task that's all in practice so that is the whole value proposition see guys I said I started out by saying that transformers take care of parallel processing all right but we needed a framework to have a highly shareable model now pre-trained models that you would apply to your problem and just do last mile fine tuning therefore bert is one very popular and powerful model like that so as if can you spell out a few constraints? Like when I tried to summarize it, I said an English but and then I'm using English FAQ. That's not necessary. You can feed an English but and ask it to produce English questions and ask it to produce Hindi answers, whatever you can do. I mean, see, those are not restrictions. Those things it's pretty clever at. So, in essence, what I'm hearing then is the fine-tuning, that amount of training is sufficient for Bert to be able to figure out the necessary learnings to go from one language to the other as well. All those things, translations and all of that, those are very easy. Like, that's the whole point that it's surprising actually how powerful these transformers are. These have taken over the world practically in AI and natural language processing. They are the dominant team. See, they are not perfect. Even today, if you go and try to translate something from Hindi to English, sometimes the translations are completely hilarious. I'll give you an example. And then the English will come out, he ate circles and fell. You see that, right? So these things are not perfect. There are still a lot of limitations. Some of the translations can be still quite hilarious. But in a large number of situations, the translations are pretty good. Like for example, let me put it this way. A friend of mine, he for some reason, many years ago, he got a job in Huawei. Well, these days it's not a good idea to take job in Huawei. Huawei used to have a center in Silicon Valley. He became one of the chief architects in one of the divisions. So he went there. Actually, you know him, Prem. Alex, right? No, not Alex, Ram Veluri. Yeah, Ram, okay. So then everything, whenever he would be asked to go to China, all the symbols, you know, the signs and everything was in Chinese. Occasionally you would see English and everything was in Chinese occasionally you would see English and it was very hard to figure out what was there just how do you go from place A to place B and so he had this Android app it would just do a translation all he had to do is point it at a sign and right away he saw the translation so you can imagine that in practical life, it is very, very useful. What I'm trying to wrap my head around as I'm listening to all this is, if one has to think about an application context where one is looking at what is available commercially right now or in open source in terms of BERT, and then try to productize something, how much amount of effort is involved in that additional effort? So generally, like when these things came out and people don't realize that whatever effort you were putting in and training your systems before these transformers came out, you're putting literally one percent of the effort now. People finish projects and go to market in many times right at least do a prototype presentation before clients in weeks now okay and people are not used to the fact that this this this big transformer models are so versatile and they can actually do all sorts of things so uh read the latest transformer that has come out is GPT-3. If you Google up, you will see there's so many videos. It just came out a couple of months ago, I believe in July it came out or something like that. Since then, people are still marveling at how many things it can do. Apparently you tell it, one example somebody was showing, I hope I don't get it right. So let me see, what was he saying? They said that you can speak to it, one example somebody was showing, I hope I don't get it right. So let me see, what was he saying? They said that you can speak to it, you can give it a natural language description of a UI, that create a user interface where a user can search through your inventory of books, let's say, or whatever items, and then be able to do this or that. And literally GPT-3 came out with a fully formed user interface written in React.js apparently, or JavaScript and HTML or something. Can you imagine that? We are there. I mean, the state of the art is so far ahead. Most people haven't woken up to the fact that AI has actually advanced that far. Nobody thinks that the AI can generate code for you. It is today. So when you think of data visualization, for example, it is very tedious. You have data, you know what to do. You would sit down and write a ton of code and then there'll be bugs and this and that and so on and so forth. One of the things people are seriously looking at and with great success, with some success actually early success I would say, but still fascinating results is you talk to these transformers and as you talk to it, it is literally getting into your data and drawing the visualizations in front of your eyes on the screen can you imagine that so imagine what it does I mean when that new world comes about what it does to the old old historic reporting tools right it will completely put those on steroids isn't it isn't it uh yes uh instead of training the words and sentences uh with it's like physically adjacent uh peers uh can we um try to run like an attention model or like the whole thing on a summary and then? Yes, yes, you can do a lot of that. See here's the thing, that's a separate discussion. Today I wanted to focus just on this paper, but okay, do one thing guys, look at this, look at the sort of things you have, the things that they have dealt with you know where they're getting the state-of-the-art model so what are they trying it on let's look at this very detailed experimental setup so some of them a quora question pairs is a binary classification task it will determine if two questions asked on quora are semantically equivalent right literally search through it and then tell whether they're equivalent. It does it. A multi-genre natural language inference is a large-scale crowdsource entailment classification task. If one sentence actually follows the other. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment contradiction or neutral with respect to the first statement. Next is a QNLI, question natural language inferences, the version of the Stanford QA database, which has been converted into a binary classification task. The positive examples are question sentence pairs, which do contain the correct answer. And the negative examples of question sentence for the same paragraph do not contain the correct answer and the negative examples of question sentence for the same paragraph which do not contain the correct answer so you can do all of this ssts is the standard sentiment uh of course you're familiar with a cola is a is the binary single sentence classification where the goal is to predict whether an english english sentence is linguistically acceptable or not right it makes sense or sense or not, right? STB2, they're all of these MRCP. Just read the wide variety of the data sets on which it was benchmarked. And just to think that in one fell swoop, a pre-trained bird goes in, just think about it. What a breakthrough it is. in research you you get breakthrough improvements very, very slowly. You don't get this in one fell swoop. But look at this table. Asif Nadavattu Ramaswamy, Ph.D.: It seems to have in one fell swoop gone and done very well and beating the state of the art and all these tasks very different tasks. Asif Nadavattu Ramaswamy, Ph.D.: And that is why everybody woke up and said, my goodness, this is really a moment. Asif, do you have any comment on the current software that's in the market for plagiarism detection? Do they work on similar models? Yes, yes. You have even the traditional string matching is pretty good. And then you put AI into the mix and it becomes the whole thing runs on steroids. Very good. Actually, plagiarism is very fast, scalable, and reasonable, quite good at detecting. The question is the definition of plagiarism. That is the human element. Suppose two of my sentences match your book. At that moment, is it plagiarism or is it that we independently had the same thought? There's nothing new under the sun. If I'm thinking of something, somebody else probably is also thinking something like that. And we might end up writing very common sentences. So where is the demarcation boundary of plagiarism? Does plagiarism start when you have at least a sufficiently large number of sentences which are essentially the same? Or what is the definition? So you have to be a little careful with that. If you're overaggressive, like for example, some universities universities were students were getting very undergraduate students were getting pretty hurt they were using some of these softwares and they would submit a term paper and immediately it would get flagged as plagiarism and the students would swear that they had not plagiarized and they wouldn't have known about those obscure references where something similar like that was written. So it is always there. Those tools have to be used with caution, but the machine can do it if you give a sufficiently good parameters. So for example, what's the likelihood that all a sequence of 100 sentences are exactly the same between you and somebody else. That probability is extremely low, isn't it? And you let the machine look into that, and then you train them. See, you bring a whole world in itself. So let's put it simply, Okay. But by the way, I'm not an expert in plagiarism. I'm just saying off the top of my head, but I just happen to know broadly that they are getting pretty good results. I don't know all the details. So take it with a grain of salt on that front. All right, guys. So if that is it, if there are no more questions i'll stop the recording you