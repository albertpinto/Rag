 One of the requests I received here is just to refresh what I mean by the parameter space. So this is a review of the, I call it the hypothesis space. Or the parameter, more conventionally, it is called the parameter space. So let's do this review. See, suppose you have data. And let's go back and create data. Absolutely white, more colours. Okay, let's say you have data like this once again. And I'll just go back to this data. Now, this is your data. You're trying to predict Y as a function of X. Let's say X is the temperature and Y is the amount of ice cream. Remember the problem that we do for a young entrepreneur? He's trying to, she's trying to predict how much ice cream would get sold at a given temperature. So she builds a hypothesis. She says that there is a linear relationship, beta naught plus beta one X. In the language of deep learning, of course course we may use w naught plus w1 x but i will just stick to beta because in the lab and in the previous course i use beta so now you can say let's see there are many hypotheses you can hypothesize a solution that says this is the relationship this is hypothesis a this line it says that the more that the higher the temperature the lower the ice cream you would sell right another hypothesis could be like this that That no relationship. It's called a null hypothesis. B. Null hypothesis. Null hypothesis says that the amount of ice cream that you sell is invariant of the temperature. Temperature has nothing to do with it. Can that happen? It depends on the the temperature. Temperature has nothing to do with it. Can that happen? It depends on the predictor. So for example, the number of the amount of ice cream that you sell doesn't depend on, let's say, how many, what is the ratio of consonants to now, to, to vowels that some politician used in his speech on that day? Quite unlikely, right? So that is a feature that is not a predictor that is at all related to the amount of ice cream sold. Hopefully, God forbid, the day when ice cream sale depends upon the fickle speeches of politicians. Let's say that you have another hypothesis that goes like this. Oh, sorry. Let me use a different color. That goes like this pretty steep hypothesis and let me call it c and then let's have a hypothesis let's call it d now which of these hypothesis looks to be the more likely that that intuition tells you is the best of these hypothesis so you would say that hypothesis D is better than, I'll just use the symbol greater than, the hypothesis, what is the next best hypothesis? Hypothesis C is better than hypothesis B is better than hypothesis A, roughly speaking, right? And so you would imagine that the loss for D, the errors for D will be less than the errors for hypothesis C would be less than the errors for the hypothesis B would be less than the errors for the hypothesis A, isn't losses so far so good this is common sense isn't it if you say hypothesis d then it should have the minimum error now observe one thing for each of these hypotheses there's a different value of beta naught and beta one. So if you look at the space of, no, sorry, let me use the white color. Now look at this. This is beta naught, the intercept. This is beta one. It's a completely different space, which I call the hypothesis space. And for reasons that you will see why I tend to call it the hypothesis space, more traditionally called the parameter space in textbooks. You realize that if every line here, whether it's the the red the blue the this thing they all have they all have a value here in this space for example the green line has a negative slope and a positive inters intercept so what is it it is a green one is a point, let's say here, isn't it? With a negative slope and a positive intercept. Do you see this guys? This green line has a positive intercept and a negative slope, it's sloping downwards. Now the red one has a negative. If you look at the red line, where is the red line going? It is negative intercept and positive slope, right? So the red line is probably somewhere here. Negative intercept and positive slope, right? The peach colored line has zero slope, but a finite intercept. So where is the peach coloredcolored line going to fall? It's a hypothesis somewhere here, isn't it? What is it saying? Slope is zero, but intercept is present, right? This is the intercept. This is your intercept here. And what about the yellow line? Yellow line seems to imply a slightly negative intercept and a positive slope. So a yellow line is putting it here, a slightly negative and a positive intercept, right? Do you see that, guys? So this is your A, guys so this is your a hypothesis a this is your hypothesis b this is your hypothesis c and this is your hypothesis d right and you can see the relationship here let me Let me do it. You see how what a line in real space, x, y space maps to a hypothesis, a point in the hypothesis space, isn't it? In the beta 1, beta 2 space, it's a point in the hypothesis space. Likewise, this red point, red line maps to a point in this hypothesis space. The yellow line maps to, sorry, yellow line maps to this point in the hypothesis space. And this peach-colored line maps to this point in the hypothesis space so what is a line in data space becomes a point in the hypothesis space so now not only a point in the hypothesis space but if i were to draw this hypothesis space or parameter space let me now make it three-dimensional because for each point, beta naught, beta 1, and in this floor, let me just say, this floor, for every point that is here, some value of beta naught, beta 1, what do you have? For every value of beta naught, beta 1, you can compute the loss, the amount of error and this loss surface looks like this which has a minima somewhere so it means the contour lines okay let me draw the contour lines in another color the contour lines are like this right yeah things like that and so you're searching for the point this point loss surface this is the loss axis this is it so that is why the surfaces that you see loss surface exists in the parameter space. Parameter plus loss space plus loss space, because the vertical axis is the loss. The parameter space is just the floor. Now, is it obvious guys, what we mean by that? And that beautiful surface that you saw was that. The hypothesis space. All right, so now we are going to change track. And by the way, is it already information overload or we can move forward?