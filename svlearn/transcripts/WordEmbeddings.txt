 So we realize that a bottleneck principle gives you, or a method, gives you good summarizations. And in a way it is semantically, it's very rich with information, with knowledge, right? It learns when you summarize it, when you summarize something, you actually end up learning. So today what I will do is apply the same concept, the bottleneck method, to word vectors, you know, a representation of words as vector spaces, and see what we can do in creating more meaningful representations of words rather than the one-hot encoding. Now just to recap, if you have words, so let's recap of terminology. Let the vocabulary be word so let's let's recap of terminology let the vocabulary be v of size v right this is the size of your vocabulary right then one hot encoding of a word is what? So suppose you have a word cat. What you do is you first create an index for every word in the dictionary. So hard work all the way to Z, Z, Z, something. So now you're somewhere in there is a place for cat you'll put zero everywhere and put a one there and zero everywhere then likewise you have dog and then or zebra let me just make it like that so it's a little bit far off zebra and that will be and here is zebra and And he zero here. Now this would be zero everywhere and zebra location one. Right. And cat location all 000000 everywhere. This is a now what are the problems with this representation. This is a very high dimensional space representation. So something like 100k words, even if you take a modest vocabulary of 100k words, you're looking at this vector, let me call the one-hot vector representation. I will use the same notation that I used with the bottleneck method. So remember the initial raw vectors we used to call x so x belongs to word for a given word belongs to r to the power of 100 k so imagine a euclidean space that is hundred thousand dimensions wide it's a huge vector space now there is another problem space now there is another problem if you take two words any two words xi word one word i and word j any two words the dot product what will it be what is the just what is the dot product or the distance between these two words? Anybody? If you do a dot product between these two vectors for cat and zebra, X cat and X zebra. Always zero. Always zero, right? It is always zero, why? Because every two, in other words, X W I is perpendicular always to XWJ for all I not equal to J. Isn't it? So you have no sense if every word is orthogonal to every other word then there is no way that you can talk about the similarity of words and yet words don't exist in random orders the words are pieces of communication and the building blocks of a natural language they are parts of sentences certain sentences make sense and certain sentences don't make sense. Right? So, and certain kinds of sentences occur quite often and certain don't occur that often. So we may search for a latent space, a latent representation, once again, a z, right? Somehow, once again a z right somehow let's use a bottleneck method and build a theory of discovering that so suppose you have this this is your x vector it goes down somehow through many things whatever things you want to do but actually in the in this world it's quite simple you don't do anything very complicated. Z, that's the beauty of it. So suppose there's a Z vector for each of the words, there is a ZI vector here, right? That you can represent it as, right? Now the bottleneck principle is this goes into what it goes. And this is where it differs from the basic autoencoder. Something else comes out. So let me just call it yi. yi comes out. And we'll talk a little bit about yi. But what we want, what are the desirable, so what are the goals that we want to set? Goals we want to set that if words are related, they should be close to each other, isn't it? So let me just for simplification say that all of these z vectors are unit vectors for the time being. Then, in other words, they are just points in a sphere. This is z1, this is z2 in some hidden sphere, right? Z2. And basically you would agree that two words which are close by, they should have a smaller angle between them. or the similarity, similarity is easily captured with just the dot product, right? Two words and what you want is and for reasons I mean that if two words are similar then their dot product should be big. The dot product will go let's say from zero to one right in the interval. It should be closer to one right so for example the word cow or i don't know the word cow and duck should be closer to each other both being animals that you find in a meadow versus cow and something completely different. Let's say a politician, because you don't tend to find politicians in a meadow or something like that. Or, I don't know, some rocket, right? A rocket spaceship. You don't tend to find rocket spaceships in a meadow unless you believe in aliens. So one quick question here as if uh for our understanding right so when we say in in terms of the intuition for understanding similarity here yeah it could be anything right in in if it's the language synonyms are similar antonyms are also similar because the relationship is opposite yes exactly right so synonyms are similar antonyms are also similar because the relationship is obvious exactly right so synonyms and antonyms are not that different but unrelated word like for example you're talking of zebra and you bring in a hammer right then they're unrelated they should be far off okay that's how it is so the notion of similarity is very general here. Very general, but now we will give it a specific meaning now. Right. So there is a statement, which was back to the 1920s, I think 1926, by a famous person I'm forgetting, he made an interesting statement, which all the new things are based on. The statement is, a word can be defined by, or maybe the word is something else, represented by or something. This word, I'm a little foggy about the exact word, by the company it keeps. This is a famous statement. I wish I could remember the author if anyone of you have it. If you Google it up, it'll show up in a moment, the exact statement. A word can be, is defined or is understood by the company it keeps with other words, where, company where, in the corpus. So let's go back to our statement. See, we assume in the background that there is a corpus, corpus, which is a matrix of what? Documents, D1, Dn, and each document is a vector, right? Some vector. I will just leave it as X, right? X1, Xn. At this moment, you can take the vector in the initial space, one hot encoding space. So these things. Now what happens is that in this thing, there are lots of words. Each document has words that do or do not occur. So for example, cow and duck are present in this document. In another document, the i, dj, there may be some other words that are present. So for example, the words would be auto encoder right and then you can use it for let us say denoising denoise just by looking at the fact that they tend to occur what words a thing tends to occur with. So suppose you had never heard of a cow, right? You just scan through all the documents of your corpus, and in each document, see what are the words proximal to the word. So suppose you take a word cow. Take a word cow. Take a word cow. Would say cow. You want to get a sense of let's let's say represented by X cow. How would you get to know what should it be similar to. So you go and look at the document and you take a certain window. Right. Or you may not take a certain window depends upon it you just say what does it co-occur with what company is it keeping and now we'll define the company that it keeps the company it does it keep it so you realize that if you read a lot of documents you have never encountered a cow the word cow but you read a lot of documents, you have never encountered a cow, the word cow, but you read a lot of documents where there are words which you do understand and you find the word cow next to those words, gradually will you get a sense of the meaning of cow? Yes. You'll get at least a pretty good approximation to what is meant by cow. right you'll get at least a pretty good approximation to what is meant by cow so that is meant and so we need to the crucial thing is the company and so there are a class of algorithms uh they they they are bottlenecks like this they help you create a latent representation of words and they all are based on the concept of finding the company that a word keeps right but how do you define company the word company in a little bit more mathematical way is co-occurrence. Co-occurrence. Now co-occurrence itself is co-occurrence means two things exist in a sentence. So suppose I give you a sentence, they co-occur. You say that they're co-occurring in a sentence. So now the co-occurrence can be measured in many ways. For example, if two words are present in a document together, even though they may be at the beginning and end of a document, you may call it as co-occurrence. Or you may say that, no, no, no, words, a cow, it must exist within a certain window. So Kao, look at the word before and the word after. So you take a window, sliding window. So literally you look at the immediate company of the words, isn't it? When you look at the immediate company of a word, W, I, N, w i are we together so suppose you take a window size 5. you look at the two words the happy cow the happy cow jumped over jumped over the moon right i tend to keep taking this sentence so So the word cow, happy is there, jumped is there. If nothing else, you realize that whatever this thing is, this cow thing is, it can have happiness associated with it. It can have the jump verb, action associated with it. You begin to get a sense and then you read more and more sentences am i making sense guys this is a very obvious thing right so far let's pick a sliding window pick a word so given a word x cow find the couple of words before and a couple of words after, and you can choose. Take an odd number, like pick an even size here, two, two. So the total size would be five. Or take it four, four, then it will be nine or seven. People play around with different window sizes. Depending upon what you define the window size, what do you consider company, right? The company of a word, how far are you willing to go away from the word? It's a matter of choice. Think of it as a hyper parameter of your model, but you can ultimately translate this famous sentence in many, many ways. So some of the classic ways that it has been interpreted, and we'll go and talk about it, are first is three very related ways. They are all in the family of word to vec. So those are the SIBO, continuous bag of words. So when this thing is coming up, I'll explain what this means, guys. We'll come to it in a moment. The bag of words. The other one is skipgram. Right, skipgram is literally skipgram. People don't shorten it as sg or something. And the last one is called fast text, which is in the same family as word2veg, though sometimes people consider it a different algorithm and it uses the interesting thing that it does is it doesn't use word but it uses sort of word components parts uh you know interestingly we'll talk about it but they're all very related algorithms right then there is another algorithm which is called yellowOVE with a capital V. So think of it as a global vector, GLOVE. GLOVE and you think of it as a global vector. In other words it's a vector derived from a little bit more broad study of documents. So what I'm going to do now is give you the intuition. All of these intuitions are based upon a co-occurrence within a window. So how do we deal with what are these algorithms? Remember, these are just different implementations of the basic idea that a word is pretty much understood or can be understood in terms of the company that it keeps with other words in the document. And you can define company with a sliding window. Pick your sliding window size and look into the sliding window size, right? The rest are implementation details, right? And now how it is implemented using the bottleneck, let's connect the dots. let's connect the dots. So what you say is that, let us say that for, let's take a simple way, right? You take a word like the happy cow, cow jumped over the moon in joy. Something like this. Let's take a sliding window of size five. Let's say that the word that you care about is cow. So one thing you can do is you can create a table, sort of an input vector, input data space. So method one. You could create basically the happy jumped over so this is your x1 x2 x3 x4 and this is your feature space and a target your your Y is the word cow right then you can move your sliding window one step forward and now sliding window number one sliding window one step forward. And now sliding window number one, sliding window number two. Now, what can you do? You can again go back and say, all right, what is the crucial word now for sliding window to what is the middle word, guys? Cow. You slid it, it's jumped. Jumped, it's jumped, right, so in the second one, you now will say again, the happy cow, the is gone, happy cow over the, and the word here is jumped. Are we together guys? Cow jumped, right? right so and you can keep sliding the window one easy way to do that is from the third word keep computing forward jumped over the over the moon and the word is you think of this, this word, the word, the word that you're studying. And this you call the context. In some sense, it is the context. It is the company that the word is keeping. Are we together? So now let's take this example, the third one. Could you guys tell me what should be the for over what should be the words what should be the context I'll jump the moon cow jumped great jumped the now look at this problem. Now let's try to, and so you can keep going on. And so what you have is context and the word. And we are saying, if I give you the context, can you tell me what word it is? some sense. I give you the context and you have to tell me the y, the output, what word should be here. How do we do that? You can do that in a few ways. Now let's into our bottleneck because we want to use our lovely bottleneck here. Right, so there is an input, x goes in and y comes out, right? And there is an internal representation of this. So what you do is something interesting. You take the first, you see that this is actually made up of, it's a four dimensional vector here, right? The first row of data. Why is it a four dimensional vector? I mean, at least four words are there. And each word is of course course, a 100,000-dimensional vector. So what you can do is you take the vector, the x of the plus x of happy plus x of jumped. You know the one-hot encoding? The input vectors plus x over, you add them. You can just sum it, or some people prefer to average it, average of the four. So what will you end up with? The amount will still be, let me just call it, it will be the input to the word cow, right? It is the overall input in this sentence to the word cow. Do you understand it? This is the context. If you add all the context vector, you're basically saying that this is the company that it represents the context of cow cow context so far so good guys i'm just adding those vectors anybody who's confused please speak up uh this is siebel right yeah i'll come to that i'll come to the names but are we understanding the concept all i'm doing is adding up their raw of one hot encoded vectors of the neighboring data neighboring words that is what goes in here right this thing this is your x that goes in here. And what you do is you force it to learn the weights in such a way, nw and w prime, you force it to learn these weights in such a way that y that comes out is cow. You know the x for the cow, the vector, one-hot encoding, output is one-hot encoding for for cow let's think about it this is what what is the space of this vector this vector belongs to what dimensional space if your vocabulary is 100k what is the size of this space in what space does this x vector exist guys 100k 100k right and likewise what you expect the word cow is encoded as uh as again a one hot vector it will belong to again a 100k space and do you notice that this is very reminiscent of auto encoders except that you're getting something else out you're putting in x which is the average of the neighboring words and you're expecting a y right your y hat that you are expecting or let me just say y hat that you're expecting uh is again that but what you're doing is your your architecture is such that you have a single you have a single hidden layer here and you're forcing it to go through this bottleneck right but how do you how do you train these weights the question is that is all fancy but i can just spill out some random weights it would still work then the question is what is the preferred weight of these objects so the preferred weight that you have, and that is the learning part, where is the learning, deep learning part. The deep learning part is, if two, so given two words, xi, zi, zj, and remember I use this for dot product should be closer to it should be high high for similar words at the end of it all the words that are sort of similar they should ultimately come closer but you don't try for that your loss function is that you expected cow and what came out. Now, what came out? So this is your ultimate goal. This is your ultimate goal. You somehow want to learn, but in this architecture, how do you learn it? You got the word, you expect the word cow, but let us say, what will you get here? Let's think through this. What you will get is because, let's think through this. You have the W matrix. These are the weights. They hit the input. So you get x.w. Now you get z is equal to w.x. Actually, there's a transpose, but let me gloss over the transpose. What's a transpose here or there? So we take the z. And then what happens is that on top of it we apply this but then there is a node here it's a value we need to figure out which animal each word corresponds to this one corresponds to rd work this one corresponds to zzz somewhere in there is a cow somewhere in there is a zebra right what you get are certain numbers these are called energies when you multiply this with this you get something some numbers our energies vector which is again belonging to our 100k you'll get some numbers come out what you want to do is you want to somehow convert these energies to probability and that probability, and then you want to check what was the probability of the cow. Now, if you expected a cow and you didn't get a cow, what is the cross entropy? It is your log p cow that comes out. the way how do you convert these energies to this we covered this in a previous topic you soft max it right so if you have a bunch of numbers x1 like we literally not use x because x is for input a1 a2 a n one easy way to do that is first convert it to proportions. That is A1 over total, A2 over total. Right. You can do that. That is one way of doing it. But you can do actually. But the problem is, suppose if some of them are negative and some are positive, then it begins to get problematic. them are negative and some are positive, then it begins to get problematic. Probabilities are never negative. So now you get a bit screwed. And so you say, okay, what can I do now better? So you say, no, I won't do this. I would instead take a clever trick. I will exponentiate each of these things, A2. And if I exponentiate, I get two possible benefits actually mathematically and then divided by the total of the x so this is it and then I can convert them into proportions by first you exponentiate it and the next step you do is you sort of scale it down by the total so each number so let us say each value e to the a i divided by sum of all of these things a j all all of these numbers you add up total right total the new total t prime is equal to e to the a1 plus e to the a2 all of these numbers e to the a n this is your t prime and all you're seeing is that the probability because now i have all positive numbers and if i take a positive number and let's say probability of the event a i of energy i or the cow or whatever class it is c i is this divided by t prime in other words it is this divided by p prime. In other words, it is this divided by equal to e to the ai, the energy produced here by sum of all of these energies e to the ai energies, aj actually. So this is the probability that you'll see whatever it is, hard work or cow or zebra or whatever it is. So you can take any bunch of numbers, so long as it is finite, and you can convert it by going through the softmax process, exponentiate it, and then just take the proportions. You will get probabilities. So now comes interesting thing. We talked about cross entropy and all of those things now now what can i do i wanted a probability one for cow zero for everything else but suppose i got p cow outside i got a number something like 0.2 or very low likelihood zero point actually it will be something like 10 because there are so many words 10 to the minus 4 and it seems to be smaller let's say that the p of of hammer p of hammer is more let's say that it is equal to 3.1 into 10 to the minus 2, or something like that. Hammer seems the most likely, right? So these are small numbers, and let's say that the hammer seems more likely. Then you realize that, oh gosh, this is not what you expected, because you'd never hear that the happy hammer jumped over the moon in joy. That doesn't make sense. So you don't want hammer. You want cow. So what can you do? Cross entropy laws. You can do your cross entropy laws, guys. And then you can back propagate from there. The standard deep learning is simple back propagations. You shake up the weights, W and W prime, to make it a little bit better so that in the next iteration, next mini batch when it goes through, the cross entropy loss is a little bit less. This is standard thing that you learn for classification and so on and so forth. Right. Are we together, guys? I'm not talking about anything extraordinary here. Am I making sense? Yes, sir. Yeah, this is very easy, right? And so what you do, you do epochs after epochs with batches of, mini batches of data, and all of your data is coming, fulfilling the condition that take a word and take its company. The company is the input, either averaged or summed, and the output is the word. And you make sure that this somewhat like an autoencoder, this bottleneck architecture takes the input, context as input and produces the word as the highest probability thing, the softmax. It's pointing to the word, right? So one of the nice things that exponentiation does, by the way, in softmax is it's also exaggerates the biggest value because the if you think about exponentiation exponentiation is a curve like this so big values get extraordinarily amplified and so their probabilities tend to become much larger right so that is that right so so that is that now you can train it when you do that this particular method one is called a CBAO. CBAO. Continuous bag of words. Asif, can you take a moment to go back to the what is the. What was back propagated? What was back property loss. It is your loss, you're trying to minimize this loss right the cross entropy loss, which is what is it why log y hat you expected it to be cow. What you're getting is Mostly you seem to be getting you the the y hat of cow is not equal to one you You want the probability of, you want to maximize this. So you have a loss. And this loss you can back propagate. You can try to minimize by taking its gradient and back propagating so that the next iteration, hopefully next time when you give it, when you expect a cow, cow is a bit more likely. Your network has shaken up its weights improved upon the values and now you get a higher probability the next time around okay so in this case it will be only a single when the expectation is that it will be only one item a cow right yeah so that is the loss yeah exactly it will be the word cow for that one input cow is the output and then if you pass a mini batch of data of course if you do a stochastic gradient descent of course you can learn one word by one word right or one data one row by one row of data or if you do mini batch then you can add up all your loss functions across the different words you're doing but those are minor details think that you're just doing stochastic one word at a time you get the intuition more easily and you just back propagate you go fudge your weights in such a way that thinks now next time around a cow looks a little bit more like a cow right so it may jump from a car from a hammer to let's say next it may start pointing to a zebra, which is closer, then to a horse, and then maybe finally to a cow with luck. Right? In a few iterations. So that is it. So when you are done that, once you have a network that at least this network, at least for most words, most context, it can appoint to the word that should be there, what has happened? What has this network done? It is saying, if you give me the context, this box, if you give me the context, I'll tell you the word you're probably referring to. Isn't it? So then when you think of that think about this internal part this network is fully trained look at the z representation there is a there is a x vector for car for whatever it is context x context or whatever now here is a cow word cow and here is a z and you know that this is produced forget about the input in the the thing once it is trained somehow whatever the z z the hidden vector is of the cow that when inflated up back into the 100 100 k dimensional space using the w prime matrix and going through the basic softmax and the machinery of it, it will start pointing to X cow. It will begin to look very similar to X cow, isn't it? And that is, I mean, it produces energies pointing to X cow, right, probabilities to that. So then what happens is this, therefore, has learned the representation of cow. But now, suppose you didn't put only one word. One word is easy. You put the entire corpus and you kept on looking at the sliding windows of all the sentences and documents in the corpus. Eventually, what will happen is if it is getting all the words essentially right, given a context of a word, it is more or less pointing to the word, what have you learned? You have learned a good hidden representation of the entire vocabulary. And so that is the essence of continuous bag of words. So this was the oldest way, B-O-W, of words. Now there is an alternative which is the opposite, which does something quite different. It is called skipgram. Now let me perhaps use another color. It is called the skipgram model. It is called a skip gram model. Skip gram, but the full name should be a skip gram with a negative sampling. And you'll see what now that looks quite a mouthful and a scary term, technical jargon. But yes, before we move forward. So where do we use the z representation? This is your word embedding. So hereafter what happens, given a word, given any word represented with a bag of, you know, this one hot encoding. What can you do? You can pass it through this part, only this part, w, right? You can do w dot x, forget the transpose, there's a transpose sitting in there, but ignore it. You get z is equal to z, right? And z therefore, what is the dimensionality of z? Depends on the bottleneck, how many, you pinched it down to how many. Typically, what you do is Z typically is very common to make it 300 dimensional space, right? Or 500 dimensional space, that also is acceptable. Sometimes if you have a limited vocabulary, you can even play with 50 or 100 or something like that. You can play with these numbers, 50, for really small representation. And if you just think about what we did, we took 100,000 dimensional space and we reduced it down, let's just say 100 dimensional space. So you realize that you have compressed it by a factor of thousand, isn't it guys? You have a much smaller vector space which is only 100 dimensional compared to 100,000 dimensions. So I understood that. What I don't understand is like initially we fed the context, right? And now you're saying we just feed one word to the Z. Oh, yeah, yeah, yeah. Actually, sorry, I take that back. In this particular case, what happens is Now you're saying we just feed one word to the z? Oh, yeah, yeah, yeah. Actually, sorry. I take that back. In this particular case, what happens is this is the relationship between z and the word. This is the relationship here. You have these matrices. And with these matrices, what you can do is you can take a word and you can come up with one-to-one mapping between every word and its's z prime z z vector so when we uh input a word what will be the output in that case so you put cow and you'll get the z cow so we jumped over the moon as the output no see that is the context jumped over the moon is here. I'm saying if you if you take the word cow, and you just look at this part of the situation. Do you realize that there's just a matrix linear matrix multiplication, you can use this to go back and find the latent representation Z. z. Suppose y is equal to w x. Would it be fair to say that, okay, let me put it this way, x is equal to w z, right, w prime z. Would it be fair, therefore, to say that this is cow, this is cow. Would it, therefore, be reasonable to say that you can have Z cow is equal to W prime inverse X so that I understood but what is Z what exactly is Z isn't it the context no it is not see at this particular moment right you you throw away the left hand side of the whole table that matrix because all you care is if i want cow as the output what is the input what is z what is at the hidden layer so suppose this part did not exist this part did not exist now once it is fully trained you realize that for every word that is produced there is an internal z representation connected by just a matrix as simple as that right so so the use of this model is whatever word be feed in the output would be the next word or the output would be... No, no, no, no. See, while training, you use the context to predict the word, to produce the word. But once it is fully trained, so Nisarg, please differentiate between training and inference. Once you are fully trained, your weights are frozen permanently. Now look at this situation. What have you ended up with you have created a relationship between a word and its embedding and its hidden representation what i don't understand what is the use like how will be how is this going to be used so then you have to wait that's exactly what i'm coming to oh dimensionality reduction is what i'm seeing yeah no it has two main properties you guys remember we started out this journey by saying that we want this representation and the beautiful thing with this representation is z i z j this is not only is first of all z belongs to a much lower dimensional space right it belongs to just 300 let let's say, dimensional space, much smaller dimensional space. Right. Let me just put the word P here. P, where P is much, much less than original 100K, let's say. Right. And the other thing that you start noticing, and this is a this is an interesting thing that people started noticing, that you have a vector algebra or vector arithmetic here this is what people started finding and that is why this got a tremendous amount of attention because things that followed from that is that you take two words which are really related they they are closer to one the closer to to one, let's just say. I won't put the exact value. Closer to, or bigger. Let me just say that bigger, if the words are related, bigger, if the words are similar. So the Z for those two words, the latent representation, or the word now I'll use is embedding. Embedding. The word embedding. The word embedding is Z. The Z that comes out for each of the words, it shows a very interesting relationship. two words that are close to each other will will have a good dot product and two words which are unrelated will have close to zero dot product that is one the other interesting relationship people began to find is if you do minus z of man and plus z of woman, you start getting approximately the value of z for queen. And when people saw this sort of interesting vector arithmetic relationships, they began to get really excited. They thought they are onto something interesting and they were, of course. The other thing that began to be like Z, let's say India minus Z, let's say, Delhi, New Delhi is equal to Z of, let's say, UK minus Z of London. Do you see? So what it means is that in the vector space, somehow, you have some words like India, right, and New Delhi. And then you have two words, which are UK and London. And it turns out that the gap between these two words, the vector space, is exactly the same. So not only are similar words close together, but they seem to have a very interesting vector, word vector arithmetic to it. And this got people very, very excited and people began to take the word embeddings very seriously. So guys, are we getting the main point? You can find a low dimensional representation of a word using this bottleneck method. Just think of it this way, that you can come up with a lower dimensional representation, which is a linear transformation. There's no activation function in the Z layer, hidden layer involved, just that. And you'll come back with the embedding. And all of these things are based on some form of co-occurrence or company, words company within a sliding window. Anybody who feels he understood, please speak up. Yeah, got it. Got it, right? All of us. Anybody else got it? Got it. Yeah, got it, right? It's straightforward. And now comes this other algorithm, which is not a variation of it which is actually far more effective than continuous bag of wood and it puts the the thing on its head it is skip gram with negative sampling it just takes the continuous bag of wood and turns it upside down. So let's go over the sentence, once again, let's go with a sliding window, pick an odd number sliding, even number on the left and the right. Right. So let me take this again, the sliding window here. So this time, what we do is we put the context, the real word on the left hand side. We make it the predictor. It your x we put the cow here and then biology can you please mute yourself yeah the cow cow happy cow Cow jumped, cow over. So what we did is, we are basically saying that the word cow, these are the company, we have evidence that cow is keeping a company with these words. Are we together? And then what we do is we do negative sampling. We deliberately take, let's say, cow and randomly pick a word from the vocabulary. All right. So, for example, the let us say that we take a word, a DC motor. Cow and cow. And what should I say? A paper towel. Cow paper towel. If it were a word, actually, these are two words, so I'm a bit hesitant, but OK, the notion is the same cow and cow and what should I say hammer words that are completely unrelated you throw into the mix and then what you do is you create a table you say that my prediction should be this cow does keep the company of the word the so this is true one one one one zero zero zero and all the you throw in a lot of cow with random values from the vocabulary. So for example, I don't know, take off a very random word. For example, autoencoder. Very unlikely that these two words would be in the same sentence except here. So 0. You're saying the problem, it is i'm creating a data set where the y is whether or not there is evidence that these two words coexisted or co-occurred so cow and the did co-occur cow happy co-occurred cow dc motor didn't co-occur cow auto encoder did not quokka right so this is your x1 x2 are we together now what can you do guys what you can do is go back to your autoencoder and now this time i'll be brief what's the time time check nine nine o'clock okay uh so so it's time almost for a little bit of a break. So what you do is you feed in only the word cow. And then you let the results come out. You'll get this probabilities, right? Softmax. And what you want to do is you want to make sure that the probabilities of the probability probability of happy, probability of jumped, probability of over, these are high. And at the same time, you train the network, the neural net, or your weights again, ww prime in such a way that the probabilities of, in this particular situation with cow as input, probability of DC motor, of paper towel, etc., of autoencoder is low. You want these probabilities to come out low. Are we together once again you write your cross the same cross entropy laws and then you start training the network so visually what you're doing is yes see in the first case in the bag of words you took four words of context let us say four words of context right this and you did some sort of a aggregate operation, maybe some average or whatever to create a vector, you feed it into your, into this, and you had one output. The word is the output, right? Context. In the case of the skip gram, you turn the tables around and what you have is a C bar. of a skip gram, you turn the tables around and what you have is a C bar. And in skip gram, turn the tables and what you see is word is the input. What you want is the four words should start shining up. The probabilities of those, the context words light up. If you think of each probability value as how much that bulb is lighting up, you want those four words to light up and not other words to light up. Are we understanding it, guys? So this is an implementation detail. It just so turns out that when you use the skip gram model you get much better results there's nothing about it yeah go ahead uh i mean does it mean the probability should be close to 0.25 for those four words is that the expected probability you know this is only one sentence no that word cow may have uh in other sentences other probably other words in its company but see the bottom line is your idea is correct if you take the sum total of all the words that tend to be near that tend to form the context of the word cow in all in the corpus of the documents, only their probability should be high, the probability of other things should be low. Okay, that is it. Yes. Oh, so let's say that we take the search engine for Google, when when you type in your search search key and it starts to show you suggestions? Is that a skip ground? The answer to that is the Google search engine uses more than a thousand algorithms and approaches in a single query and it returns in a couple of milliseconds. So one of the things, do they use word embeddings? Of course they use word embeddings. It's there. See, all machine learning is in a vector space, right? You need a vector space representation of words. Otherwise, it's meaningless. You can't do machine learning, right? So first state is you need a vector space representation of word. Second thing is, do they use word in the this particular skip gram or not um i would say that you know the state of the art is more forward i would call these still um what i'm teaching you it's a necessary thing to learn because it's very very commonly used uh word to vec algorithm both its implementation implementations, CBOW and SCIPGRAM, and the third one, FASTX, very commonly used. But we have moved ahead. The world of transformers has changed everything. So remember, we are going to get into transformers now. Right, Patrick? Okay, I see. So I shouldn't disappoint you too soon that, you know, when when transformers come a lot of the things you don't use as much as with transformers but the fact of the life is these word embeddings are massively used there's a reason why uh spacey comes with word vectors so you know when you take a token just do the vec underscore you'll get the vector representation of the word right and so they all come up with vector because similarity of words, you can do in one line. You can do document similarity in one line in spacing just because of this, because you have a very nice vector space representation of word, which is semantically rich, which contains that nearness and this nice vector algebra, the vector arithmetic to it. There is a yet another implementation called fast text. Yes. This pairing with the unrelated word of cow, right? There are some, are those like infinite or? Infinite, so what you do is you don't do that what you do is suppose you're taking a window size of uh so people that's a hyper parameter of your model let's say i used to remember the number like suppose you find a word cow and throughout the document you find let's say 100 words that it keeps company with and And then you might as well throw a hundred words that it doesn't keep company with. Okay, some random hundred, okay. Because it's always good to learn from balanced datasets. Okay, that's why probably is then the performance of the skip gram better compared to... Yes, actually the skip gram performs better in many ways because you not only teach what the word should be, what is the company it keeps, but you're also teaching the network, the company that it does not keep in some sense, there's sort of an intuition on why it's slightly better. Actually not slightly better, distinctly better. Right. Okay. And then there is a fast text implementation. so what fast text does is it takes the word so let's take the word um let's take a long word hetero one second hold your question in mind let's take a long word heteroscedasticity. We encountered that in machine learning, right? You remember it was the first big word we learned in our workshops. This word is very unlikely to be present in the corpus or its probability will be very very low. Are we together, guys? But this word hetero is quite common. Elasticity contains this. Skadastic is a word that can exist. So you start looking at word parts, or rather trigrams. What are trigrams? You break it up into little pieces that you know how to do that. And then you start looking at little combinations of characters. So trigrams of characters, for example. So you may say het, right? Or you may look at ete or things like that. And you start e. So hang on, I won't go into the machinery of it just think of it as that you look at specific try word parts as biographs trigrams of or some grams of characters in the word and the same analysis that you did you don't do with words themselves but you do with these pieces or trigrams or pieces character level combinations, character level n-grams within the words, right. And so, as you can imagine, this was by the way proposed by Facebook, I believe in 2016. What it means is now you have a giant vocabulary. And so the training will be slower, but generally, it will now outperform in many ways the previous algorithms. It's an improvement about over just using the words. So far so good guys. So all of these come under the word2vec family. Any questions guys so as if yeah if a word has multiple meanings would that mean it would have different energies that is the demerit of this so for example an apple computer versus an apple for the farmer have entirely different meanings, isn't it? So it is not context aware. That's a limitation of the classical methods that we are learning today. So the context aware things will get better when we get into the transformers. Right? Thank you. Awesome. Those are homonyms,onyms, a word that has many meanings. Not context aware. Please say that again. It's not context aware yet, right? It is not picking up its meaning contextually. What will happen is because you're making co-occurrences, the word apple will be proximal to computers. Apple will also be proximal to orange. And so what happens is that in your vector space representation, you'll have the peculiarity that apple, and apple is here, computer is here, orange is here, right? Whereas this computer should actually not be near in one sense near apple and another sense when see the two things should be that either apple and computer are near or apple and orange are near right but the two both of the two shouldn't be true isn't it and that is the problem in creating static word embeddings like this, that they don't pick up meanings based on how it is used in a sentence. So ideally what we would want is that the same word, apple, the vector, the Z vector for this, in doc D1 should look like this. And the same z vector, apple, in doc D2. So let's say this is about farming. Should look like this. And in this D2, it should look like this, something completely different in the world of computers. Computer world or IT world, it should look something different, right? And that is what you get when we get to the transformers later. But hold that thought in your mind. Today we are not getting there. We have one last one to do, GloVe. And by now you must be realizing that these are all very clever ideas on the co-occurrence matrix so glove takes a very explicit approach it creates a co-occurrence matrix so you take a big giant matrix you have the word cow and again you take a sliding window whatever choice you have if the word cow co-occurs with jumped, you go to the word jumped and then you put a one. And maybe you'll end up with seven throughout the corpus, a cow jumped and then the apple orange is eight and some other things are zero and some is one. So you realize this is called the co-occurrence matrix. And here, I shouldn't use the word. Actually, in the glove literature, they tend to use the word x for co-occurrence. So let me just use something else. Let me use m for co-occurrence. m for co-occurrence matrix. So guys, is the meaning of co-occurrence clear? Just slide over your entire corpus and every time two words show up in the same window, just go increment the count at the intersection of those two words. Right. So in this matrix, this square matrix. And this square matrix will be symmetric and positive, of course. All values will be. So in other words, the cow and jumped value here and jumped in cow value would be the same. Do you agree with that? It's a very simple statement I'm making. It's just a co-occurrence matrix. Yes. Straightforward. Now comes the interesting thought. What you do is you can convert this co-occurrence matrix to, so let me just take a much simpler matrix now. I'll make it not so big. I'll just say, suppose they're just four words, A, B, C, D. A, B, C, D. Right? And let us say, actually, in the literature, they use the word X. So I will use the word X itself, unfortunately, because just in the previous section, I used X for input. But the co-occurrence literature, I mean, this glove literature tends to use the word. So now if I say, what is the probability of seeing uh d d show up next to a right so what you can do is you can say how often did a show up right this is a which is some all the you know how many times the word a occurred doesn't matter in the company of which other word would you agree that if i add up the columns i'll get how how often the word a occurred yes right if it occurred a occurred with a itself so cow cow right and three one nine so you can just add it up right and so let us do that bit of arithmetic x a here is equal to nine ten fifteen right and x a d is the value here. What is that? 9 over 15. Are we together, guys? Am I making sense what XAD is? How many times did I see A and D together? Nine times. And how many times does A occur in this vocabulary at all? 15 times, right? There are 15 occurrences spread over A, B, C, D. So guys, I hope I'm saying something very simple and obvious. If not, then I haven't explained you properly. Somebody would like to speak up. So this is the probability of This word a and D occurring. I saw that the paper uses the word IJ would use a and D, the probability that ad will occur. will co-occur is this, right guys? Does it makes very basic sense. The probability of seeing A and D together in this is this. Given A, fine. So let me state it different. Given A, finding D in the company of A, the probability is this. Right? Now it turns out that we want to use this somehow to go back and use in our matrix. We want to use this network again. Now I'll be a little bit fast because we seem to be running out of time so so let me just take z a z b right what we want to do is we want to have this relationship z a and and i won't give you the full mathematical details because i think we are reaching a point of maybe reaching a point of mental saturation z a z b we want the dot product or or in this particular case we took b we wanted to be somehow dependent on how close these two words are should somehow be related to the probability of their co-occurring would you agree with this guys Would you agree with this, guys? Yes, sir. Yes. Right. And now probabilities are positive, whereas this dot product might end up becoming negative. So what is one easy way not to ever be embarrassed by a negative number? We just dealt with it a bit ago. What are we? Exponentiated. and so one rough way of saying it is that if i exponentiate the dot product i will get this right and so if you work it so if you take the log of both the sides you will get e i mean z a and this part you can forget, guys. This is just giving you the intuition. D, that product is log PAD, which can be broken up in terms of log XAD minus log XA. But you could have asked this question to keep it symmetric like ZDZ a if you want to keep it like that to be the same thing. What happens is These, these things you split it up into you call it so called bias terms a bias of a and bias of D. So I leave it as that Plus bias of D. You just break it up symmetrically into two pieces. It's just a technical mathematical up symmetrically into two pieces it's just a technical mathematical thing and so you come up with this lovely relationship which basically says that the log of the co-occurrence you don't even now need to do probabilities just go to your co-occurrence table look at this number nine log of nine should be the vector z a vector d, you know, this negative comes to the other side becomes plus, B, A plus B, D, the bias terms, is equal to log of X, A, D. So ignore the bias terms. Basically, what we are saying is that the dot product between these two vectors should be like this. And so what is your last function? You can, you want this thing to be true right the co-occurrence but you feel you shake up your net networks ww prime in such a way says that this thing is beginning to gets respected right you again the same thing so uh create a loss and so on and so forth so what will be the loss it will literally be the gap between these two this should be this if it is not this so suppose you take some vector it is not yet fully learned then what happens your loss is literally the square of z a z d minus b a minus b d and just square it might as well, sum squared error, you know, the typical sum squared error loss you're doing, and there is actually one more rarity factor, but let me just put, there's a factor here for rarity, decay factor, which you can put gloss over, let's say, for there's a reason, technical reason for putting a decay factor, but ignore it. So this is it, this is your loss function. It is basically the sum squared error between what you expect and what you get when your model is not fully trained. When you have some values that are not perfect, there'll be a gap between these two. And so all you need to do is square it and then move forward with it so if you look at glove it's very interesting glove takes a more regression like approach you know it takes a some squared error kind of approach where do you use some squared error guys i mean squared error and so forth you tend to use it in regression whereas the word to word kind of thing, we tend to go more with a cross entropy. So this is the glove thing. And here I've glossed over a few things, guys. So in order to explain it and not get lost into the details, I've glossed over a little bit. But this is the main argument. Do you see this, guys? A simple argument that you do. And I went over it a little bit fast so you might have to watch the video again let me recap what i said you look at the co-occurrence matrix and if two things co-occur a and d co-occur they must be similar it means that those word vectors must be similar you apply that constraint right you basically say the dot product of x a and d must be somehow related to the probability of seeing them together but probabilities are positive and these vectors can be positive and negative one easy way is just go exponentiate it you'll always have positive numbers right and then then what you do is you grind through the mathematics and then you really you come up with some relationships and there it is. And you also end up with the loss function. This is GloVe. Now, generally, GloVe tends to, in my experience, quite often outperform Word2Vec. All the mutations of Word2Vec, CBOW, SkipGram, bow skip gram fast text glove tends to quite often outperform but not always consider the word embedding as a hyper parameter of your more of your modeling process sometimes one can do better but uh in i tend to prefer the glove by and large as a word embedding so all right guys today was a long session on theory, but I finished all world embeddings. So next week, we will do entire week on bottleneck methods. And just for the last five minutes, I'll throw in a sort of a bonus thing that you may or may not get. See, guys, if you go back and look at the bottleneck method, OK, let's say that you have a lot of data. You have good data, good data, right? And you feed it through your some sort of an auto encoder, right? And you reproduce it, right? You create a reproducing encoder. So what will happen is this thing will learn there'll be a abstract encoding that the network would have learned you only show it good data so and then suddenly you show it an anomaly right you give it an anomaly guess what will happen to the z of the anomaly Guess what will happen to the Z of the anomaly? It will begin to stand out. Right? So we also use autoencoders in a very simple way for anomaly detection. But the important thing is don't train with anomaly data present. Train with good data and then feed in anomalous data and then your autoencoder will start observing the and marking it as the anomaly because that data point will stand out from the from the other data points in your latent space okay so therefore you use this this bottleneck methods this autoencoders they also help you with anomaly detection actually they're a powerful way of anomaly detection next week we'll actually do anomaly detection also along with um along with all the other things we do with the bottlenecks the word embeddings you know reproducing images and generating images and all of those exercises i think somebody was very interested in anomaly detection in this workshop who was it i was you were okay nice so there it is we will come to that all right guys so that is all i have to say let's go back and review what we learned today the new thing one one one quick thing before we move to the reviewers so in all the intuition that you presented today the like with that um the diagram that we were using essentially the uh what's that um the choke and the the squeeze right bottleneck yeah yeah the bottleneck you you were always referring to them with the notion of a word, right? So the Z we were trying to bring up was essentially a lower dimensional encoding of a word. If you use word in the NLP case, but remember when we, the last on Monday, when we talked about it, we're talking in more general cases, images and other things. It will find a lower dimensional representation for anything for you. other things it it will find a lower dimensional representation for anything for you so what i the way where i was leading to with my question was i was looking for moving from that concept of a word to that of a document how would you generalize this to a document very good thought so one of them is you take once you have the word embedding a document is nothing but this a total of the words you know add up the word vectors in the document and you get the document vector that is one and that is that is called document to work and people have been using it but there is a further way that you can do it which is we'll do when we come to transformers and then we'll talk about such things as universal sentence encoders right okay we'll have more to say about it see the moment you start taking documents to mean a whole sentence or paragraphs and so forth then there are many interesting things come out see guys today i'm finishing what i would call the natural language processing with deep neural networks before the transformer case right then the next two weeks today is the fourth week i think third third week this is the third week is it okay uh all right so if this is the third week, is it? Okay. All right. So if this is the third week, it's a good point because half the rest half, we will give to, well, next week again, fourth week, we'll go just to the lab of this and fifth and sixth week, we'll just go to transformers. Now I've introduced you to transformers in a practical way, but we'll go deeper into the transformers in the last two weeks. but we'll go deeper into the transformers in the last two weeks. And we will also do transformers all over again when we do image processing, because now transformers are not just NLP, but they are in image processing also. So the transformers will end up getting a lot of time. They'll get time during the NLP part and they'll get time during the image processing part. Part three. I have one question about that last topic, the GLOBE VEC. So where did the BA and BD terms come from? So, sorry, I missed that. Literally two words. No, two. So, okay, let's go over the reasoning again for glove see suppose you have a doc you you have a corpus of only four words right this is your co-occurrence matrix isn't it right there'll be a certain in a sliding window there'll be certain co-occurrence of a b c d etc that is it. Imagine a vocabulary of just A, B, C, D. So if you scroll down a little bit. Yes. So I'm talking about that minus B, A minus B, D. So what does... This is a technicality. Why log A is broken up into these two parts. There's a cute little argument. Ignore it. When you see B, A, B, D, imagine you're not seeing it at all. The whole argument is far more intuitive if you don't see it. And you look at ZA, ZB, right? Plus something, it doesn't matter what, up to a term, up to a constant. See, this is a constant, right? It's a bias term. Is basically log XAD. This part is intuitive, isn't it? If you ignore the constant part, which is BABD. Ignore the BABD because there's a little bit of a reason you have to bring in arguments of symmetry and so forth. To just take a bit of time and also add to the confusion. At this moment, I want to make sure that we stay with the highlights, you know, we understand the big ideas. Okay, thank you. But you see that this is very intuitive, isn't it? It makes total sense. The dot product should be somehow related to the co-occurrence. Yes. And that's that. And so you come up with the word embeddings, which is the glove word embedding. That's all it is. So the last function should be z a z d minus log x a d right oh my goodness yes yes yes my you absolutely are right thank you for catching that g minus log x a d like my goodness i would have lost that square right is the gap between these two absolutely correct and can we say it's a constant, we can ignore the BABD for... Generally what happens is those constants get learned in the learning process, right? Very quickly the conversion they're done to the learning process. It doesn't matter basically. That's why you call them the bias terms. You just learn it as part of the whole game. But do you notice how interesting it is? The loss function, the whole argument is very elegant. There's nothing very mysterious about the argument. It is ultimately using a sum squared loss. There is a little, see there's always a little technicality to make things work well and take care of corner cases. So there's a decay factor involved and obviously not to forget the summation symbol to sum over all data instances and so on and so forth. So but at the end of the day, this is it. It's a it's a sum squared loss. And that is GloVe. So, guys, what did we learn today? Let me summarize. We learned, we are out of time, we learned that the bottleneck methods when applied to NLP give us word embeddings. There are four methods of word embeddings, three belonging to the word2vec category. There is continuous bag of words method, method one, in which you give the context as input and the word as output, expect from this whole bottleneck architecture. Then the opposite of it, skip-gram, in which you give the word as input and you basically expect that all the context words will light up in the probabilities, the opposite of that. And remember, skip-gram with negative sampling, right? Negative sampling is important. You have to also tell what is not the context. And then there's the glove, which directly uses the co-occurrence matrix value and it basically says that the exponentiation of the dot product is equal to or is equal to the probability that a and d will co-occur that is it now that's simple statement exponential takes care of your positivity and so on and so forth right and the dot product with unit vectors max it will achieve is one right so there is obviously a little bit of normalization here and there, I won't go into that. Take that constants and so forth. Awesome. Yes. For the last function, should it be plus BA and BD? Oh my goodness, yes. Because I put plus here, right? Yes, okay. I made too many mistakes. Good, good. Since you guys are catching it, it shows that you're understanding it, very yeah because i put plus here right yes okay i made too many mistakes good good since you guys are catching it it shows that you're understanding it very good so remember guys when i explain things i'm rather careless with my proportionality constants and pluses and minuses so be be on the watch out incorrectly does the word is uh skip gram has any meaning so the word meaning as such no i didn't get the question and the skip skipgram, does the word skipgram have any wordly meaning? Oh, quite literally. In an n-gram, 5-gram, you skip the word, the central word, and all the other words are the context. See, the cow jumped over the. So what happens? If you take this this take taken out is your input this is your x and then the output is all of the rest of the from the skip this is a five gram right right you're skipping one the rest of them all up here. It's sort of a way of putting it is a skip gram. Right. The fact that you hide one, basically one of the words, the center word. Okay. I was just getting that. You can look it up. By the way, this is my interpretation. Maybe in the literature, they have given even some better or deeper explanation of why it's called skip gram i never questioned it to me the fact that there's one word being skipped always stayed in my mind but there may be another reason look at that guys i may be wrong thank you sir like uh is it called like a five and gram, like five skip gram or like skip five gram? Maybe skip five gram, but nobody actually, you just call it the sliding window size. People don't use the word five gram or they don't hyphenate it like that. At least I haven't seen it. Okay. And the window size, by the way, is the hyperparameter of your model. By changing it, you can get different results or different performances. Right. By the way, Pradeep, why don't you find that out? What is the root of the word skipgram? Let us all know next time. Yes. I'll learn something then. I learned something then. Alright guys, so I am done. Was it clear? Was the explanations clear guys? Yes. The big picture is that you can use the bottleneck and the co-occurrence of words in Windows. And they're clever implementations of how to derive the latent representations. And these latent representations in the NLP will be called the word embeddings to come up with word embeddings. And we saw four implementation, three from the Word2Vec family, which was continuous bag of words, skip gram, fast text, and then GloVe, which is a different one. All right, guys. And so with that, we will do the lab of all of this next time, of all the bottlenecks next time, next week. And the week after that, we'll get into Transformers. And Transformers are big. Part 3 will, like, Part 2 will have'll get into Transformers. And Transformers are big. Part two will have two weeks of Transformers, quite likely part three will have two weeks of Transformers. So we'll have a whole lot of Transformers everywhere. Thank you.