 So this is a Jupyter mood book again. What I have done is I've added this extensions and perhaps if you guys want or if you are not able to install the extension, I'll help you do that. Just to recap what extensions gives you is a lot of extras that makes that facilitates work in the Jupyter notebook. So you get all the snippets, you get, I have integration with YLab. So you can actually write an application in the Jupyter notebook itself and run it straight using YLab, which is very, very convenient. But here, but let me go through it slowly. If you are creating a title, like for example the heading of this, what I can do is I can use the markdown method one simple way is markdown is what you are trying to tell the reader whereas code is what you are trying to tell the Python kernel you can see your screen oh you can't go back and forth all right hopefully this time any luck now we can see it now today's I'll just mention this there is a language called markdown markdown is as close to plain text as possible it has a very simple syntax in plain text if you start a sentence with a hash mark it is considered a title if you start with two hash marks, it is a subheading. Three hash marks are sub subheading and so forth. And then there is a way to put a list and so on and so forth. So, for example, suppose I want to insert things here. Here is a simple illustration illustration of the use of kind of refiling report to get automated exploratory analysis of the data set. So this is it. Now right here, suppose I want to, do you see that the markdown is as a language, there are all these helpers. Suppose I want to itemize something here. What has it done it has told me that I can have sub lists and so on and so forth. So let me I just write it just for the sake of writing it is in the next two libraries, numpy, cpy, then another one is, let's say, DataFrame. And then, and then there's profiling. DataFrame. These are the two things. Then we can have visualized plotting. We can have plotting through C1, not plot lib. So just for the sake of illustration, I am putting statements here. And the third is some overall configuration of locking. PLT looks fine. Then PLT with RC commands. And do you see how simple this language is? It's not like HTML. Relative to this, HTML looks complex. And so what I do is, if it is a markdown, I run it. When I run it, do you notice that this has become nice, well formatted text? So this is the part about markdown it is as simple as that a very very simple language and if you have the snippets installed then it is extremely easy you just go and pick whatever it is that you want to do and this is also a very latex aware like for example if you were to insert, just double click on it. It will go back into the insert mode. And let's say that in that you want to insert an equation. And those of you who are familiar with that, remember that. So here it is. It given an example of of an equation now i don't know if i've installed the math jacks but hopefully so now when i run this thing now you notice that there is a equation here that has showed up in the documentation so it's very convenient obviously it implies that you need a little bit of latex and not everybody is familiar with latex it is something that has a learning curve unfortunately there is no better way to write equations or syntax mathematical syntax than latex for example the notes that I give you I don't know if you realize it has been written in latex, and the formatting is superior to what you would get if you were doing either Microsoft Word doc or Google doc and so forth. So this is the advantage of using Markdown and having the convenience of all of these things quite literally in this, and then again you have a lot of these quite literally in this and then again you have a lot of these things also you can you can embed widgets for example if you want to put a button or a picture or anything you can just directly insert widgets into this but we'll do that later and then let me start with this the the basic inputs. I'm just saying here we are. These are the basic input. So I'll compile this code. This is again a markdown. So I'll change it to markdown and I'll run it. Asif, can you write that particular code in another editor and bring it into whatever you like and copy paste it sometimes you don't even have to copy paste if you want Jupiter Jupyter is very powerful. There are all these hidden powers. You can do all sorts of includes and whatnot. Today I'll stay with the basics. Okay, thank you. So when you do things like this, here we have the input. We just insert a cell below. And here I'll give it another heading, let us say. Two hash marks is obviously the heavy load and load and describe the data load and describe Columns are not mentioned in the original data set of sklearn. We add it manually. Let me just say we add it manually. It's just a description. Do you notice that this is a heading and this is a description? If I change it to Markdown, once again, I can run it. It becomes section 1.2. You describe the data. Then now suppose here I say insert cell about. Finally, let's do automatic data exploration exploration analysis using the finders profiling. So this is So, here we go. We have put it here. Now, this all of, so this notebook now now it's a very basic notebook it is made out of two elements text element or markdown element where the target audience is the reader so this is is this this element where my or this cell where my mouse is or highlighted is it a markdown or is it code markdown it's marked down and any markdown element or cell like if you want to convert it back into edit mode all you have to do is just double click on it and if you want to run it just control enter will run it get you back into execution mode and to execute it you can just keep hitting run run like this right and something or the other here oh I data frame I think I data is iris columns columns. So this code, let me just explain a little bit about this code. In your lab notes, I've given you the IRIS dataset already created. I didn't use, I believe this, I used the one built into Skicketland. But there are many data sets which are sort of of academic interest. Iris is, of course, the most famous of them. It is heavily referenced. It's perhaps the most referenced data set in the machine learning literature. It's also the most basic that you can think of. There are four predictors, the FLAGO's attributes. And then we are trying to determine the species of the most basic that you can think of. There are four predictors, the Flago's attributes, and then we are trying to determine the species of the Flago from it. So a lot of these data sets, they come built into these libraries, the Skicket library. Speaking of libraries, let me give you a walk through the library. The ability to do matrix multiplications, you know, arrays and matrix in a high performance way that is done using the numpy library numpy sits upon uh underlying fortran library called blast basically in your lgb system you may wonder why sitting up on fortran why not implement it in python the reason for that is the performance difference between multiple reasons one is the performance difference between multiple reasons one is the performance difference between Python and Fortran is roughly speaking a factor of thousand fortune is the fastest language that we know faster than see faster than anything else that is one the other thing is fortune has had 40 years almost of investment. People have been writing in the scientific community. Predominantly they write code in Fortran. Even today a lot of scientific computing code is written in Fortran and those libraries in 40 years have been perfected. They have been perfected with respect to bugs. Very, very few bugs. It's hard to find any bugs in them. They have been perfected with respect to performance. It's almost impossible to beat the performance of quote unquote. So what Python does is it sits upon those libraries. It has bindings to those underlying very, very mature scientific computing library. And those bindings are written underlying very very mature scientific computing library and those bindings have are written as Python libraries numpy is the Python library that brings us arrays and matrices and these matrices are very high performance one question that people often ask is why not use the list of Python that is just as good treat it as an array and so for the reason for that is when you let's say that you want to multiply all the elements of a list of numbers by something let's say 10 you'll have to in a do multiplication element wise multiplication even though Python may give you a syntactic sugar that makes it look as though you're just one shot doing it it doesn't what is called broadcast whereas numpy what it does is even if you have an array that is massively large because that array is actually a fortune array when you do a multiplication it happens at the 14 level and it happens usually if you are in a good hardware in a massively parallel way under the covers those 14 codes will utilize everything that you have in your hardware. The next library here is the Pandas. Pandas is what brings you the data frame. Think of it as a spreadsheet with rows and columns. That support is not built into Python. This library, Pandas, brings it. Recall that in R, this comes built into the language. But in Python, you need to use this library. Pandas profiling is what we are using. A very very convenient tool as you will see. It's a lot of exploratory analysis you would have done by hand. You're actually doing it with one line of code. It's a recent addition to the data science toolkit and very popular now. These are plotting libraries, these three, Matplotlib and Seaborn. Matplotlib is the oldest library, everything is based on top of that. Seaborn is a layer on top of that. Seaborn is very tightly or sort of much more compatible with DataFrame, the Pandas DataFrame. So between Pandas DataFrame and Seaborn you can have very good are much more compatible with data frame the ponderous data frame so between ponderous data frame and seaborn you can have very good plots and visualizations quickly the rest of these are just parameters we said like we say this is just giving a keen seaborn dark red it's a theme of tables and visualizations this is then we are setting some figure size when we draw the figures it should be 20 I suppose units by 10 units and so forth some colors blue green etc these are the colors to use but I don't think I'm using it here but yes then import warnings one of the things it does is when you run the code sometimes copious amounts of warnings come out and so the notebook gets littered with a lot of warnings and things like that so one way to suppress it is this now one thing worth mentioning is do you notice that when you run a cell it tells you how long it took to learn, to run? There's 980 milliseconds and it finished. So that information is useful because suppose something takes three hours to run. In machine learning, that's very common. Some of the tasks can run for 12 hours. So you will be hesitant in rerunning it knowing that it's a huge computation you would look at the execution time and say do I want to wait for that to finish do I really need to run that this this helps you determine that it also tells you when was this last run so that's the explanation of the basic imports one more library is this SK learn SK learn is the famous skicket learn library it is the de facto machine learning library in Python it has support for everything except except that when you do deep learning or deep neural networks you tend not to use Skicket learn even though it has some support for it you tend to use tensorflow Keras and Python now as we make progress in this workshop itself and this is the first time I'm doing it I don't know if it doesn't become too much for you guys one of the things I was planning to do is that introduce you guys to deep the things i was planning to do is that introduce you guys to deep neural networks pretty early on those libraries that is early on in the lectures in fact as we do regression one of the things i wanted to do is do regression using skicket learn and which is the sk learn library here and also do it using tensorflow, Keras, PyTorch etc so that you have early familiarity with those for the whole toolkit the whole tool chain that we have in machine learning so we will do that in due course of time but this in the base we are still in the very early stages of this workshop series so that explains that this is what this does is it describes the data it tells you basic descriptive statistics about each of the features the features are obviously the four features include all says that don't skip the categorical second it's not really necessary because all of them are numerical now guys do we know the distinction between categorical and numerical would anyone like to remind us what is the difference between categorical and numerical features features even good category is where we have a finite set of elements in that particular set or in that that category which can be represented by just find its. That's categorical. Anything that can be measured is numerical. Excellent. So excellent, Suresh. Thanks for chiming in. So in simple terms, when you have a finite set and you have to identify one of them, so for example, you have a finite set of true false Boolean right so you have to tell whether something is true or not false or whether or suppose you have three animals cat dogs and cows and you have to identify which of the three animal or data point is if I tell you the features of that so that that set or or the values when they come from a finite set you say that it is a categorical feature of categorical variable and everything that's quantifiable that's measurable is numerical that's measurable is numerical it takes on real values rational irrational fractions small numbers negative numbers whatever but those are all real value they're measurable quantities that's the difference between them why did I call transpose the reason is if I see what happens in this particular case if I remove include is equal to all the reason I put in Q is equal to all is I wanted you guys to develop a habit of using it but see suppose I don't do it I do a very basic X describe you see this is the standard form in which it shows this for this data set simple data set is okay but But if my input feature space had 30 columns, 30 features, you realize that there would be 30 columns here, and there would be a scroll bar at the bottom, which is a bit of a nuisance. So it is more convenient to transpose. If I do transpose now and hit enter, you notice that the whole table has been rotated 90 degrees. And now even if I have 30 features it will just be 30 rows in that this table I won't have to scroll horizontally which is why this is preferred and include is equal to all in this case will not make any difference will not make any difference because none of the variables are categorical. Suppose you want to find how many, this target variable, how many unique values are there? You can do that. NP-unique, it tells you that there are three values, 0, 1, 2. Now, moving forward, you have this lovely, lovely tool. You just import Pandas profiling and then say x.profileReport. When you do the profile report, it produces something quite lovely, actually. And let me see if I can open it in some external view frame. you know you can't load it outside but okay here we go so what this pandas profiling does is in one shot it looks at your data and gives you a lot of information about it so in the overview do you notice that it gives you the statistics there are four variables number of observations is 150 it tells you a lot about it duplicate rows it turns out that there is a duplicate row somewhere here are there missing values you know for example is it a situation that sometimes a separate with is missing or petal length value is missing or something it seems that the data seems very clean there are no missing cells zero percent missing data and then you it does systematically explore the data you're looking at univariate statistics sepal length is a statistic and it tells you all about it you know mean median everything pretty much you calculated that above here but now you see everything coming out automatically for you you also see a frequency plot frequency plots are called histograms you see the histograms here and if you probably detail you see more of that in statistics you can see the quantiles you see you can see a lot more detail right you see the standard uh deviation the coefficient of variance a lot of these things that are says skewness and variance and so forth what these things are at some point i'll be explaining to you but if you're curious you have you may have studied it in your high school i may not have most people have not or even if they have studied they forget about it these are called the moment first second third fourth moment the first moment is the mean the second moment is called the variance the third movement is called the skewness and the fourth moment is called the kurtosis so they are descriptive of the data let me give you an idea if you look at the data do you feel that it's does this graph look to you symmetric? Does it look symmetric or does it look slightly skewed? Very slightly skewed. Very slightly skewed to the right. Right? Right is considered positive and you can see it here. This Q value is a little bit positive does it have a long tail does it not have a long tail well it doesn't seem to have much of a tail at all kurtosis is somewhat light minus 0.5 kurtosis is a measure of how heavy the tail is what it means is for example when you purchase things from Amazon if Amazon were to look at a table of all the items and how much they sold you would realize that there would be a few best-seller items which sell a lot and most of their items sell maybe what hundred hundred items a month so they have a long tail of millions or hundreds of millions of items which have small amounts of sale but it is still profitable so in those case the Curtis's value would have been high so that's the meaning of Curtis's let's look at a histogram as you can see in the same histogram has been made like this it is also a dynamic histogram, winning strategy, buy-shin blocks, and so forth. So a lot of this is too much detail at this particular moment, but it is still nice to know that they exist. How often are certain common, what are the most common values in the data? It turns out that is a sepal length tends to very often be 5 the next most frequent value seems to be 6.3 and so forth so it just gives you some things extreme values like what are the values at the extreme ends these all seem to be values at the extreme end they seem to be the low values I suppose or is it the high values and statistics minimum is 4.3 and maxes to be 95 disease so this seems to be on all the small values three small values extreme values for maximum minimum five values and maximum five minimum five values and maximum five minutes this is all just for seven so you realize that this tool has given you a lot of facts which would have taken you a long time to compute and write code and come up and the same thing it does with settle with I won't go into it you can toggle the details and you can go to petal length what it is saying is that petal length has high correlation it observed something it says that there is a strong correlation between petal length and if you click on this petal bit here also it says these two are highly correlated something interesting and we'll come to that in a moment once again for each of these you can look at the graphs common values and so forth so I wouldn't go too much into that let's look at the interaction so is there a relationships between these so it is getting into too much detail at this particular moment, but let me not get too much into it. The most important thing is correlation. This correlation plot, you see that if you talk about description, it will tell you what it is. I have taught you only Pearson correlation. At this moment you can stay with that but if you are adventurous you may want to learn about Spearman's correlations and rank correlation then Kendall's which is also a rank correlation in five so there are multiple correlations that are used in the industry by far for numerical variables the most significant and most often used is the Pearson correlation. So let us look at the Pearson correlation. We realize that if it is dark red or dark blue it means strong correlation. Let us see what we see here. Petal length, do you see it strongly correlated with something? Petal width and petal length? Yes. This is dark red. What does that mean? It's on the scale, the sidebar, if you look at the color bar, it means very high correlation. Isn't it, guys? Likewise, there's a very strong correlation between petal length and sepal length these are all high correlation areas what are the negative correlation it turns out that petal length and sepal width opposite correlation the bigger the petal length the smaller the settlement in other words in iris flower if if it is long and slender petals the sepals will not be white which if you know iris makes sense so it's a way to see correlations here this tells you are there any values missing now in this very very clean data set none of the values are missing. All of them are present. But this plot is of great value when you get large, complex data sets. For example, the California data set that you have, one of the things to study there is what is the missing value analysis there? And you'll be surprised what a rich graph you'll get there, the plot you'll'll get there and you'll have to have some strategies to deal with that so it checks for missing value and then of course this is just some sample of the data this is your overall and this is what you call filing very rich I would highly recommend that you use this wherever possible but don't become totally dependent on it in other words you should know how to do it by hand how to draw the histogram and so forth so you can listen the notes that I gave you I have deliberately used both syntax for some data set I have used automatic automatic automated exploratory analysis for some I have given the whole analysis the code for how to do the whole analysis likewise in the notebook I've deliberately switched between R and Python so that you become familiar with both. Asif just one one question here. On this tool that's not directly related to what we are doing but let's say the data set that we are analyzing to do the EDA is like a billion rows and I know it's going to just hang when I try to give these comments. How do I approach those kinds of data? Is there a different way I should approach those see here's the thing we are not able to grasp a billion even if the team could load a billion rows you realize that your eyes will simply glaze over what you do is first define your problem and then take a sample sampling is easy okay let's take a sample. Sampling is easy. Just take a sample and be careful that it is a representative sample. And there are some strategies, but let's start with simple one. Just start by taking a random sample. Now, random sampling gets into a bit of trouble when there is a huge skew of data. Let's say you're looking for the blood test results for a rare disease then most of the data sets will be for healthy people and they may be four data points out of million for that rare disease so if you randomly sample you may not catch the rare disease the data instances for the rare disease so in such situations you have to be a little bit more careful but in in general, at the very basic level that we are starting, just start, take a random sample and start. Generally, the law of large number holds, which essentially says that once you have a sufficiently large sample, it is representative enough of the larger reality. Okay. So the lesson is the sampling would work best if you if the rows are limited and you can you can understand exactly the ins and outs of the data that's what it makes sense and here is a rule of thumb that I use I'll have to think why I use this but I've used it now for almost 25 30 years and it has worked for me when you have a lot of data, see big data in the industry, in the commercial and the public world is just coming out. But in my world, as some of you know after IIT I was doing a PhD in theoretical physics, so in particle physics we used to get massive amounts of data. We knew like sort of big data on a giant scale so basic rule that I used to follow is suppose your data set size is like you took a billion right what root of a billion into the nine isn't it it is three into ten to the. That is approximately 30,000 rows. So easily handle 30,000 rows. And generally, I find that unless there are class imbalances and other considerations in the data, generally taking the square root of the value for very large data sets is quite representative. OK. OK. I follow. I'm sure there's a mathematical way I can justify it though at this particular moment it sort of is escaping my mind many years but follow it I mean just take it as one practitioner to another that it works I will trust that yeah I'll take that thank you so deal with that so this is it so now guys I want to show you something this is a very simple notebook I just have given you guys the notebook suppose in this notebook I want to generate a table of contents what do I do I click on this is that on the side table of contents are coming does it make it very easy to scroll and understand this data or this notebook? Do you see this, guys, where my mouse is going? Yeah, on the left. Right? So it is quite useful to be able to do that. Very convenient, actually, to be able to do that. And you can do more things. You can insert a table of contents etc sector the very top of this I wouldn't go into it it becomes a little bit littered after a little while and you see that as you get more and more familiar with things you will see that there is a lot going on those of you who are used to emacs for example my fingers are used to emacs so I switch I can switch over to emacs keyboard example my fingers are used to emacs so i switch i can switch over to emacs keyboard bindings sublime text is very popular with people you can switch over to sublime key binding win is again a vi editor if you're used to those key bindings by all means go here then you can do you notice that there are many, many things you can do. There are toggles for all of them, show labeled anchors, etc. Many things, equations, bibliography, latex environments, and there's a, and you can keep on adding things to it. One other lovely thing that you can do is the theme ability there are some Jupiter things anyway this is just starting so I'm mentioning these things that many people ask that they don't like the white color late at night when you're looking at the screen white is very jarring so you don't have to stay with that people like that piece well there are these king themes you can go and look at this do you see this dark means you get there many GP so all you have to do is install the Jupiter themes that's pip install this command all you need in fact I don't think on this Linux machine I've installed Jupyter themes yet why not let me go and do that so I'm going and sure enough Jupyter theme has gotten reinstalled and so if I were to go and do kernel restart hopefully somewhere I will develop the ability to now go and change the key actually no I can't I have to restart the whole Jupiter with the with the key there's a way to configure the key and the documentation is here JT and you can change this how do you change the theme and there's many many configurable things for example um obviously i have not done it here but i tend to use more a textbook font you know then the book the old style text font something that looks better in textbook on paper than in the notebooks because quite often i tend to print my notebooks on paper and then read it in bed, for example. So for that, you can change the fonts. You can specify what your font choices are, and everything is highly configurable. You can do what you want with this. So I invite you to play around, guys. The Peter is something you're going to spend a lot of time with in your career might as well get used to it. So this was one data set. Now this thing, suppose you want to export to HTML, you can do that actually. See download as, you can download it as PDF, you can download it as HTML as later. The link isn't I wasn't too impressed with it. It's rather noisy, but it's there at least. You can make it into slides. Actually, I've never used the slides, maybe I should. Maybe we'll do it together. All sorts of formats you can export it to. So, for example, if I do do HTML which is the simple with table of content let's see what happens it has generated that and you notice that this by the way is a HTML file with a table of contents on the side and now you can just publish it on any web page that you want now it is not a jupiter notebook it looks like it but it is actually an HTML export of that you can do other things you can do let's try another thing you can do yeah for PDF you better have latex on your machine but if you have latex or text which I talk about in my notes do you see this guy is does it look like a PDF now you guys see that yes you can see that the fonts are very paper-friendly this is the standard Roman form, nice forms, and this entire thing has become a PDF. Likewise, you can do, let's try something that I've never tried myself. It could be an experimentation for me. I don't know how I can do this slides. That should be fun oh Irish slides look at this this has now become the outline of this has become slides I suppose so quite a bit of functionality here in Jupiter and you can keep on adding it and tab nine and many more things can be added. You can highlight things. I wouldn't go into that. Navigate. Actually, I have not. This is a new machine, so I've only added a couple of plugins. There's more to add. Now I'd like to go on and take the next easy data set systematically the weather data set. By the way this one should I also do the Python the R version of it? I believe this is the R markdown version. Yeah so the same iris doing it in R doesn't make it harder but in R I chose to do it by hand not to do it using the automated data exploration tool like the pandas profiling in R that is called the data Explorer if I use the data Explorer Library it will have a similar effect as using a pandas profiling in Python but here I chose to do it quite literally by hand and when you do it by hand you see the results here and I believe I mentioned this in your notebooks desire there in your notebook somewhere so one thing is that how do you load the data it's very simple what in Python was described is summary, reduces the same median min, et cetera, et cetera, looking at the first few rows of the data. Now there is a library that is very, very useful. It's a collection of libraries called tidyverse. And it sort of centers around the concept of tidy data. Both pandas and has a good support for it. And in R, of course, this is where it was created. The whole movement for tidy data was created in the R community. And what it does is this library makes dealing with data look almost like SQL, very simple data manipulation language. Do you notice that you take Iris, you pipe it. This sort of the Unix just as in Unix those of you who are familiar with Linux know or are familiar with pipes know what I'm talking about this is the pipe symbol means you take this data and you give it to this from this you select these to this. From this you select, deselect a column called ID. Why? Because this ID column here looks useless. So you deselect the ID column and then you rename some of the columns. You notice that select. So now these columns remain. These names look pretty bad. So you rename the columns to be better. Better names and then you will see me use tidy data format and this dplyr syntax, this pipe syntax, all over the place. This is a way to pretty print a table. I could have just printed and shown it. There are many libraries, two libraries in particular. One of them is the Kabel and the other other one of them is called the table extras and grid extracting they're very good libraries to create beautiful tables especially for publication now you notice that i managed to highlight this particular column why do you think i highlighted the species because it's the target variable it sort of makes sense to show that this is this is separate these four columns are together as feature space and this is the target space and that's what I was trying to do and this way of dealing with data is very powerful so those of you who are used to see Java etc et cetera, would realize that, suppose in this data frame, I want to quickly find the median of each of the feature variables. It would be quite a thing to do. You'll have to write a bit of Java code, a C code, and so on and so forth. But in R, and obviously also in Python, in these data science languages, these are all one-liners. First, what you do is you group the data by species and then for for each species the group of data points for that particular species you just find the median you see me doing the median so this syntax guys I hope you will remark at how powerful and how very simple it is how much you can achieve in just a couple of lines of code. In fact, this would be considered one line, even though I've broken it up into many lines. It is quite literally one statement. In one single statement or assignment, you were able to find the entire summary of the data. And in one single shot, I'm able to also represent it yeah then plotting it nothing the iris data again is a straightforward thing now here I am using a yet another library so I used iris to actually just introduce you guys to the various libraries one library for visualization that is heavily used is ggplot I talked about it in the book which is a grammar of graphics thing it systematically builds up any visualization however complex so you look at this visualization does it look complex guys first of all there it's a grid lots of things are happening diagonals look differently diagonals are something called density plots the lower lower cells here parts are all they are all scatter plots do you see the scatter plots not only that there is a best-fit line a linear regression line going through them and there's regression line going through them. And there's a line going through for each species of the flower. These are the contour lines of KDE. They give you the contour map of the data. So a lot is happening in this particular graph. If you were to do it without using a systematic method. For example, if you were not using ggplot, just using normal plotting library, this would be reams and reams of code. And if you search in the internet, quite often you find people have written the reams of code etc. But actually it is a very simple, nice way of doing it. One thing that I use is gg-galley which is a lovely thing for representing things like this but here I'll go it gives you this pair plots it is an extension of gg plots and then there are themes I'm just introducing these concepts here gg-galley helps you make pair plots in Python I mean in R and the other libraries also it is just prettier than ours native function outcomes built in with a function called pairs the output of that is pretty rudimentary not as rich as that so let me explain what all things are happening first of all this is a grid plot secondly you notice that I've teamed it. And I'm using a team called Tufts Team. Edward Tufts is a Stanford professor. And he started a very influential movement where he said that let us stop being silly when it comes to data visualization. For example, when we make a bar chart, there is no reason why a bar chart should look three-dimensional. It should have a color gradient and you know it shouldn't look like eye candy use the ink to convey information to just prettify or do things most of the ink in the plot should be good should go to showing the data itself and so he is not too much in favor of having a lot of grid marks, three dimensions and all of that, but simplicity and that is quite influential. He has written books like the the visual display of quantitative quantitative data which is a very very influential book in data science communities. And obviously, he also gives this very popular workshops. You guys are welcome to go attend it. He's written many books now. So influenced by that, increasingly, people are adopting the Tuft style in their plotting. And in fact, the notes that I hand out to you, the notes themselves are adhering now the to the tough book style and more and more textbooks now in the community in the scientific community are being patterned after tough book recommendations so it matters so you notice that in one line here we are able to do so much we are able to give it a theme we are able to specify various thematic elements like what is the size of the font and the left size of the font these numbers and so forth and let me walk you through first is what is the data first layer is the data then you specify what is the what is it that the data then you specify what is that what is it that you want to see in the specialization upper ones should be density plots it says lower one should be these scatter plots and the diagonal should be obviously done mentioning but by default it becomes these density plots so that is it what is alpha so color by species use species to color so you can see there were three species so there are three colors in each of these graphs and alpha is equal to 0.3 just having a little bit of a pastel color makes a graph look good otherwise it is too much in there you know too bright primary colors perhaps doesn't look so nice it is common these days to mute it down a little bit by adding a certain degree of transparency here look at the graphics to the graphics, which is what I have done. Let's look at the next one. Now it should begin to look very familiar to you. What have I done? I've created the main item, which is the correlation plug. What is the correlation between each of these features? Clearly, we can see that there's high correlation between sepal width and negative correlation, sepal width and petal length. We talked about that before. At the same time, there is a positive correlation between petal length and petal width. You remember that the Python, Pandas profiling mentioned about that. We can see it here. This is the code for that. Now, box plots, this makes the box plots. So guys, what happens is that the first time you come into this field, there is a choice that I had. Usually, I would start the workshop by giving one-liner codes, you know, one-line, two-line codes, and then making progress with it. it. What happens though is that as you make progress through the workshops and you go into a more advanced workshop, I noticed that people used to struggle with basic exploratory analysis. They were not very good at it in the sense that they could not do sophisticated analysis. I just recently held a boot camp for this data. A pretty well attended boot camp. And when was it, Nisarg? Around September, October, November, isn't it? So in that boot camp, and it was remarkable actually to see that people were doing very well with the algorithms, but where they were struggling with or not able to do is do a systematic exploration of the data and that's when I realized that first I need to bring people into doing good quality exploratory analysis so you guys I am I lead a large data science team and this is sort of a side but in the team it's heavily dominated by PhDs and people with masters as I lead this research team and you can see there are people who are very very good with the algorithmic stuff and they'll do this but sometimes and it's quite odd actually sometimes they'll come up with something in the analysis after doing a lot of computation and sure it is there and when you say well it wasn't this obvious in the exploratory data aspect itself then they'll say oh my goodness we didn't do that we didn't look at it why didn't they look at it because two reasons people are in a hurry to get into the algorithms and secondly people are actually not well trained in data exploration whereas most of the you know what I would say intuition about that data comes from the data exploration I always say that if you have spent a lot of time doing data exploration, when you go to machine learning you're not flying blind. It's not that you're taking your airplane into a fog and you don't know which way you're going. You have a very directed approach to machine learning. You already have some intuition of what you're going to find, how you're going to make your predictive model, so what pattern recognition things you're looking for. You already work with strong intuition with that. So spend a lot of time in doing exploratory analysis. Now looking at these things, I'm giving you guys this code samples and you can just directly use it pretty much. At the early stage when you are picking up things, there's a little bit of a way, I suppose in every field. I learned programming by using the monkey see monkey do principle. I learned from the textbook, then I realized that all I had learned are toy examples and homework exercises in college. Those are not long code bases. The solutions used to be one or two pages of code, simple code in undergraduate. And so after a little while, this was the first second year of engineering, I realized that the right way to learn coding is to take a large code base and literally read through it, put breakpoints, debug through it. And then as you do that make small improvements to it and do it by seeing how it is done so I call it the monkey see monkey do way of learning and that's how I learned it gradually you pick up all the things that you don't find in books essentially a pick a guy who is very good with code who writes very clean code and then follow that person pick two three such people and you will find gems in their code that you wouldn't usually find in the textbooks. So, to do that, these are very simple code of course, it's not reams of code, but at this moment because you're new to it, if you are having a bit of struggle, just follow this, see that you understand it and then whenever you need to, just refer back to it and you are having a bit of struggle just follow this see that you understand it and then whenever you need to just refer back to it and use this code in your work. Asif, for the the correlation plot I'm having a little difficulty in reading it. I mean I need to read through little carefully what's the best way to read a correlation plot? the best way is it is a pair plot so between two features let's say therefore there are four features here petal length petal width sepal length sepal width we are correlated only three I don't know why but I you see the mistake I made I should have said one to five it's my mistake I'll fix that actually good catch in a hurry I didn't include all four columns I included only three that is certainly a error here but let us say that you click at the intersection of sepal length and petal length section is here in the center isn't it this color looks like what on the color scale you would say that it is like that right yeah highly correlated yeah okay now if you look at this guy sepal width versus petal length what does it look like it looks like at the bottom of the scale rather strongly negatively correlated. So you fix on one feature and then just read through others to understand how it's related with others. Yes, yes, yeah. And it's nice if you're working through this because I just realized that I had a bug here. I'll fix that. And is it common to omit the same thing, like for example, separate length versus separate length is white, because we don't want to fill with that. Obviously, it will be highly correlated, because it's the same feature. You have just made it white here in this graph. The thing is that it is redundant to give. See, here's the thing. This is petal length. I have to sepal width. but the reason it doesn't exist here is because it will be redundant because petal length and sepal width what is again here so having it here does not add any value to it right because we already have it here see what happens is I have taken pains to remove it you know you notice that yeah yeah if you just do a normal correlated broad I think it gives you everything yeah exactly and the reason you remove it is quite interesting actually people believe that you should not have redundant information see in programming we say right do not repeat yourself isn't it somewhat like that in visualization do not waste ink the reason is any plot should be succinct and convey the maximum information so it doesn't help And this is the pair plot. Actually here the bug is I have said n is equal to 3. I should have said n is equal to 4. Number of colors. That's why one of them got it wrong. I think I'll fix this. I think that's what it is. This is the histogram and again what I show is how can you put, suppose you have four plots, they all are related like for example of four different features. How do I make a grid of them? Right, so you do that and here I use something, yeah so once again plot grid. It's basics. Guys, any questions here? All of this code code guys I know it is new but are you fee are you feeling that with this looking at it you get a sense of how it is done and we gradually will become familiar and start using it yes yeah it's very intuitive to follow right along. And this comes like that. I see most of the things I see textbooks are great for the theory, but the coding part, you have to see the code. That's why I'm opening up all this. Learn by copying, you know, first of all, literally monkey see monkey do. But when you're doing it, you'll already think of example this mistake of mine so one obvious thing that you will do is obviously try to fix this mistake because you know that it was a typo it was a silly mistake and so you fix it and then you suddenly realize that well you did something new and then in small steps and before you know it you are taking big steps and after a little while you have your own style of writing code and it is just like that this field is a lot of artistry or artisan work a craft mansion and each craftsman ultimately has his own style of doing things there is something that is mathematical in this field but there is also a lot of things that is individual I can often look at people's analysis just read their notebook and I can tell in my team who did it because I can sense the person's thinking style and a way of approaching problems and it becomes obvious pretty much in five minutes whose notebook I'm looking at I don't have to see the author. So we all develop a unique style in due course of time, and it's a pleasure actually to have your own style. Like, for example, when you see my style of code, this is very elementary code, but you must already be getting a sense that it is slightly different from other people's style. sense that it is slightly different from other people's time we look at the weather data set because that's where I emphasize the ID data stop now and I'll use that and stop here today with that but let's take better should we go with our or Python or Python we can go with our without okay so without if you have some plans to show some things in Python that equally good so actually if we start with our why not what just happened okay yeah whether tidy data Python whether tidy data whether dot HTML I think this is R so if you look at R and you notice that what do I do here can you without reading the text tell what am I doing here reading reading files of data and this is real life guys whenever data comes you cannot expect all data to be nicely formatted in a single place. Data will come to you from many many places. Right here itself, even if the same place is scattered across multiple files, because the intention of the data generator was different from your intention. It made sense to the guy who created the data, it doesn't make sense for the problem that you are trying to solve. So the first thing you have to do is you have to wrangle with the data. You have to manipulate it and bring it to the format that you want. And once again as I said, if you think of it as a grammar, as a systematic process, no matter what the data is, in very few steps you'll be able to handle it. Now I took this data, weather data comes in many formats, most of them are already pre-formatted, but I deliberately took one in which the formatting would be very different from what we want. First of all they are in different file, the other thing is they are in wide format, so we need to make it the way we want to make it. So this is all just looking at a preview. The city is fine, city has this county latitude longitude very nice when you look at temperature data you get a bit of a shock do you notice that I can't even you know it horizontally keeps on scrolling you see the scrolling guys in this table your temperature table this is a classic signature that data is probably in what is called a wide format, not too many rows, maybe there are too many rows, but too many columns. And when you look at the columns, you get a surprise. The columns are not features. What we want the features to be when we are looking at weather, we want the features to be when we are looking at weather we want the features to be temperature pressure wind speed humidity wind direction things like that but instead it is actually the sort of what we call dimensions of the data the context because for us when we talk of weather we talk of weather in fremont today so the context is a pair city and time so we would have ideally like to see the first two columns be the time in the city but instead city is sitting there in the heading in the columns and the cells are temperature which is nice with the the temperature, but no pressure, no nothing. So that is why the spreadsheet, or this data is scattered across so many files. So the first thing we will do is, we will bring it into the tidy data format. We will make it into a so-called long table, a table that looks the way we want it to look. And how do we do it? Once again, we bring in the tidy words library this is it we have to pivot this is a concept of pivoting those of you who are who are used to either sequel or Excel who do a lot of that they probably know about permitting but permitting means is rotating then rooting is columns into just all the column headers into a single column. So you pivot and make it into this format. You leave the date time alone because that is why I've subtracted it. I don't include this in the pivot. And then exchange like the, create a column called city and the values that are sitting there they should belong to a column called temperature this is reverting a sort of sort of a way of rotating the data in some sense and also while I met it let me change the name from date time which looks rather odd you know this is in small letters and we want our other titles to be in capital letters so we might as well rename it for consistency. So you realize that this in one line of code or one statement we are able to achieve quite a lot of stuff. And so I repeat the same process for each of the things, temperature, pressure, and all of that. And after that, what do I have to do? I can see whether it is in the right format. So I just sample into the temperature table. Does it look right, guys? The context is at some city at some time, this was the temperature. Does it look like the right format? It looks like the right format is the way we would like to think of temperature. Except that we want not just temperature, we want all the other things. One question. What if there are more than one names to or values to in the data set? Can we do multiple times names to and value to? If say borrows within a city and hierarchical right yeah there are ways to pivot that but it's a slightly advanced topic is the you know the level levels how do you deal with levels and so forth that's it simple answer to your question is yes can be done and those of you who are with sequel how many of you are SQL gurus and realize that SQL also has a form of pivoting? So this is it and so here we go. Then what do we do? We join. Join what is common to all these tables? Date, time and city. Those are the two columns that are common to all of these data. So we can do a join on date, time and city. Join is a concept that comes from SQL. What it means is you're collating two tables where the context is the same and making a bigger table in both the tables, like the keys, let's say San Francisco and a particular date. If it exists in both the tables, then only that row will be kept and the data from the two tables will be joined into the final table. Any context that's missing, a key that's missing from one table or the other table, they are, those rows are excluded. So that is inner join and sometimes you don't want to exclude, you want to keep it. So then you do outer join and sometimes you want to keep on the left table but not the right. Right outer join, left outer join and so on and so forth so there are subtleties there obviously from sequel world may be familiar to many of you but once you do that you get this table now the beauty of having data in a tidy format is doing analysis of this becomes very easy so for example remember for iris data set we found the median value the right? The same thing. How do I find? I just took median. Maybe I should have taken some other metric, max or mean or variance or whatnot. But it happens that I took median. In just one, again, pretty much the same code. Do you see that, guys? Weather data, drop NA. Actually, a few rows may have NA. Why are NA is problematic because you can't you can take the median of one two three four but you can't take the median of one two three and a missing value it gets a little bit harder so often good to our average actually median is still positive averages how do you deal with the missing value so things like that that I will just how do you deal with the missing value so things like that so well here it is and arrange means to sort it by this and why did I choose to sort it by temperature just for fun right so for example if I sort by temperature I come to know which is in this data set which is the coldest city Minneapolis has anybody lived in Minneapolis or Daniel it's a gear beamer it's cold isn't it it is really likewise Denver and Seattle of course it's if it is not snowing it's raining and so let's see Seattle does it have a lot of humidity oh yeah indeed you see that the humidity of Seattle is more than the humidity of the others cities in the sample and so this is doing it with but how is it to do with Python let us look at the Python so when we have a lot of missing values we have to substitute is that part of the sampling technique or what's what's it called value how to handle missing value is actually an important topic when you come to the bootcamp it's a topic in itself so I'll give you a preview of it see the values are missing if you click with data missing values are not too many you say oh I don't care let me drop the rows suppose you know you're talking about let's say that disease problem I told you. Most people are healthy in a million rows. You have just four or five data sets, which refer to some rare disease. But in those four or five data sets, you realize that some columns are missing. You can't throw it away. What do you do? Well, you impute a value. You can either take some sorting and, for know, for example, copy the value of the nearest neighbor. Over below, you can put an average value of that column there. Or you can even interpolate the value. You can do an exercise to guess what the value should have been. Or you can literally do a machine learning exercise to predict the value so there are many many techniques that you use to fill missing values and again now you're getting into the sort of things that you'll face in the real world when you face this situation and a lot of those real world things will come to see guys here's the thing at this moment in the workshop we are focusing on the four concepts of machine learning because it's the intro we want to lay the foundations exploratory foundations is done regression is there we'll do classification and regression but it's just the start a lot of the practice aspect of it we you will do when you assuming that you do come to the bootcamp there it is a pretty good grinding you'll be here in the morning till midnight practicing you know trying out doing analysis on datasets what is the bootcamp called it's literally called the bootcamp okay it's called okay the two versions of it one is with deeply on deep learning and one on uh everything but deep learning the structured data right so it splits it up there's one for structured data one for uh like actually split it out like that structured data image processing you know congulation neural nets then natural language processing time series so the last three are mostly deep learning related the first one is a core one is everything that you the broad spectrum of machine learning getting the practice of the industrial experience in that will take a lot of data sets and morning to evening you'll just practice and you'll present you learn other things like how do you tell a story with data how do you present it to management right how do you when do you know that you have a result when do you know you have been fooled so you'll get a lot of practice on that but again let's learn to walk before we run kind of thing so at this moment we are the only come to that so here's how do we do it in Python by now you know that this is going to be exactly the same reading the data this almost looks the same isn't it if I remove the PD and say read dot CSV instead of read underscore CSV it would become our you see that guys is that close very small cosmetic difference in syntax here you do describe there you do summary data manipulation is exactly the same so So we pivoted there. You can use pivot, but melt, I used melt here, which is very useful. So, Here you see me do that. Why am I doing it twice over? I just showed that you can do it in two ways. You see the two different syntaxes? They both do the same. You can call the melt on the data frame itself or you can do pandas.melt and give it the data frame. Once again, you have to give the columns city and the temperature, the attribute column and the value column. After that, data becomes becomes like this the rest of it is very simple what will we do we'll do the same uh pivoting on the rest of the table and after that we'll do the join does this join look almost the same the only difference is that there in r we say by and in python we say on in partners we say on other than these minor cosmetic differences they all look the same here we get data by all of that now that we have in that join or out by default by default it's always a in a joint I think I'm pretty sure it is just a in a joint but in case I'm wrong you see when you do outer joins you have to explicitly say so. So there we go. Let's clean up this. Now what do we want to do? We want to generate the summaries. Let us generate the summaries. How would we do that? Oh boy, here is data. If you want to look at data. So here's the thing. Once data is in a tidy format, see how easy it just becomes to do that. Suppose I want to filter the data only to San Francisco. Here it is. If I want to sample a few rows. Suppose I want to find the median. Again, group by and median. As simple as that. The syntax in Python is the same number of lines of code, more or less, as in R. Almost similar syntax. You do that. And this is deliberate, guys. If you follow a specific grammar, like a tidy language, tidy data format, then whatever language you choose, the code will look more or less the same. But the interesting thing is that you can systematically manipulate data or wrangle with data and not feel that, oh, my goodness, I'm lost. How will I deal with it? Just follow this process. And this process, by the way, the book that you have amongst the textbooks, the practical book that you have, Funders for Everyone talks about it. And the R book talks about it a lot. Actually, the R book does an exemplary job of talking about it. And I would suggest read both. You will get a good review of the exploratory data analysis. So guys, it's 8.40. We have 20 minutes. So I'll just run through this code a little bit faster, perhaps. Where did my? Let's go. So we can go from here. The rest of it, I don't think I would like to. Oh, if you guys want to see something that I just added to your, I don't know if you saw the California data set. So let me show you the California housing data it is there in your notebook once again by now this should have become cliche you're already good at doing it how do you draw maps it's very easy to draw maps in both R and Python. They both use something called the leaflet library and Python's wrapper is called Folium. You can write code in Python and in one line of code, as you can imagine, your maps will show up as you can see here. So then this is a scatter plot of the data. Areas which are expensive are heavily marked. Median house price is high, is heavily marked, like marked in dark red. And less expensive houses are, as you can see, light-colored, yellow-colored. And the size of the bubble is the size of the population there. So obviously, one thing you can see from this graph is population centers tend to have the most expensive houses which makes sense are the most expensive houses are there in the cities San Francisco for example it's in prison primates a prime case where housing is very expensive now I have put this this two side by side, scatter plots and so forth. How do you do the histograms? This is it, of each of the variables. Now when you see variables like this, you look at data whether it is symmetric or not, and you observe what is going on. This data is bimodal. Do you notice that housing median age? You can literally see housing booms, isn't it? Phases in which a lot of houses were constructed. So this data is from 1995. So you can see that 10, 15 years ago, in the 1980s, there was a massive house construction boom in California. And then 35 years before that that makes it 1960 there was a huge house construction boom at that time so you get a sense of what's going on the history and so on and so forth and if you look at latitude why do you think there are 2 bumps here? Can anybody explain why? Why is it by module like this? And latitude anyone can guess. What do these 2 things represent? The of the houses in a particular area look at this look at this latitudes guys Los Angeles is one latitude low down and at a much lesser latitude at latitude of 6 300 is San Francisco so you do expect two bumps and in between is the empty Central Valley isn't it much less populated Central Valley region in the coastal regions so you do expect this becomes even more clear here see one population center is here another population center is here at latitudes 300 and approximately a little more than 600 see is that true so well I think the latitudes have been what is it 34 and 38 I don't know what just happened but yeah you see two bumps here so I think they have subtracted some amount or something like that. Likewise, longitude, right? You see two bumps because if you look at the cities, the longitude of this, San Francisco is here and LA is here. So you expect bimodality. So it helps to just look at the data and see is it making sense because if it is not You will it means that you have a bug in your code You need to do something better You notice that there is a skew in the income What does it show? This is what you would say sort of like a log normal distribution Or something like that skewed right skewed distribution. I don't know which one would fit best. It's right skewed. This is also right skewed. But you notice an anomaly. There is suddenly data seems to have a problem. At this value, there seems to be a lot of houses. So either they are mansions at that house price point, or something is off with the data and so you need to deal with that you need to find out what is going on with the data and why am I seeing this pillars or is it a mistake is it an error do I need to exclude this data or do I need to keep it those are things to do this is another very dense this is the pair plot what we in R, we are doing it here in Python, because literally that scatter plot, histograms and there's density plots. It gives you a sense. So, for example, what is the relationship? You can't read it in Okay, household and total rooms. As the households increase in a block, so if there are more and more people living in a block, then the total number of rooms will increase. Is that something logical? It seems logical because quite likely, if in a single block, there are lots of households, then typically each household needs about a two bedroom three bedroom four bedroom so the total number of rooms in that block will be proportional to the number of households or maybe there's a high-rise building with lots of households in that and so forth so you do expect a linear relationship a linear positive correlation here also total bedrooms and total rooms of course bedrooms and rooms are highly correlated people who have a six bedroom mansion tend to have a lot of extra rooms also but sometimes you don't see any very strong correlation. For example, the housing median age and longitude, maybe a little bit of a relationship, but not much. So it is good to just think about what this data means at each of the places. And by the way, I've colored the data by how far it is from the sea or it is in the sea. It is not visible here but if I were to increase the font size then the color could I don't know even then if the color would show up. Yeah now. would show up. Yeah, now. So that is that. And then just how do you make a single box plot or a thing, how do you make this violin plots? What violin plots show you is the densities, sort of smooth variation by household median income. So most people, for example, in this particular, people who stay within an hour of the ocean, they tend to have the median age seems to be in the late 30s, a little bit less than 40s, which is quite common. See if it relates to your personal life these are the box plots and so forth so this is that guys and read it you have it in your do you guys all have access to the notes I've shared it with all of you The other thing that may interest you is I'm adding these notes here. You'll see it in a bit. See we have been talking about tabular data but data these days have three or four more formats. One is fast emerging space in machine learning is called complex network analysis see the social graph the Facebook graph our connections are the emails we sent and who into and whatnot there is a tremendous amount of information you can gather by looking at the network structure or you know the power grid one from electricity is connected from here to there to there, and so forth. And so one of the emerging areas of machine learning, rapidly growing, is called network science. Very hot, and actually it's the first time that I'll be introducing it in this workshop, simply realizing that it is a future. It will be more and more important going into the future. So one of the notebooks that I will be releasing in a little bit has to do with network visualization. So first is that what are networks like? And I don't know, can you guys see my screen? Yes, so it's a little bit of code. I would say that ignore the code at this particular moment. These are some utilities. I wrote, but the code at the end of it is very simple. This example, let's take it. It's a famous karate club example, a textbook example. It's like the iris of. Network signs, so. network signs so this is friendships in the karate club what do you notice you notice that there are two pivotal guys these two who have a lot of links to other people and other people have just like this guy only has two links and so forth so this kind of a network is very likely to partition into two. And in fact, that's what's happened. This character network at some point is split into two, into two factions. So you can actually study that. You can see what is really going on. Or in a company, which is relevant is, in any company, there is a stated hierarchy, you know, somebody is the vice president. For example, I'm the vice president of AI and big data in my company. And then under me are directors and architects and so on and so forth and layers and layers of people. So, you may consider that, okay, this looks like an important figure but actually it isn't. In a company if you really want to see where the influence flows you have to see the communication networks and when you see the communication networks you'll find out who the real influencers are in any social body whether it's a company or social group or something like that so there is an interesting story I like to say later on when we do network science and talk about that story again it's a textbook case in one company management said that see whatever we try to tell the company they don't believe us instead we try very hard to be transparent but there are all sorts of rumors flying around and misinformation flying around in the company so one of the people in fact Barabasi the guy who made the fundamental breakthrough in this field he looked into the company's data and guess what he found he found that the hub these people are called the hubs there was a hub that there was a person who was more central in influencing the opinions in the company you would imagine he must be some very powerful guy director VP or something like that can you guess who he was? Anybody? Just throw a guess. You guys there? Yes. Come again. Like a janitor. Pretty close. Actually, he was the printer and computer repair guy. So, what he would do is, there would be problems, IT guy, there would be problems in different departments, and he would be going everywhere fixing problems, you know, hardware problems. But when he was doing, he was picking up gossip from one place and passing it to the next. So, he became the propagation channel for gossips and everybody reached out to him to know what they thought they thought that he had the real information of what's going on so now it's a very interesting question once you find out in your company in this company who the influencer is what do you do do you fire him what do you do do you do do you fire him what do you do you use it suicide here is if you want to communicate information you make sure that this guy understands and understands it correctly so wherever he goes he gives the right information so that is the value of networks. Asif, how did they get the data? Just look at who is interacting with whom. You can get the metadata of emails. The repair guy won't have any metadata, right? Yes, because everybody was emailing him. Wow, okay. This is it. So here's the thing, guys yeah there are many networks so here I wanted to illustrate the kinds of networks you have this is what was considered the classic network that you take n people and there is a certain probability that the two of them will link it's a five percent probability that any two people in the network will become friends, let's say. So how will the network look like? This work was actually done by a mathematician named Erdos. Erdos is famous, one of the revered demigods of the field, so that everybody has a so-called Erdos number. You have worked with somebody who has worked with Erdos, your Erdos number is 2. Things like that. So most people in this area, they know what the Erdos number is actually. So this is the Erdos of Erdos fame and he wrote a paper which for the longest time was perhaps the most quoted paper in the mathematical literature. And that paper was about this sort of graph, in the mathematical literature. And that paper was about this sort of graph, how the relationship forms. So you may say, oh my goodness, I never heard of Aldous and I never heard about graphs being so important, but this is it, you know, as we enter machine learning, you will find all sorts of gems sitting here in many areas. So this network is, if you notice, what what i have done is i'm looking for which nodes are influential or in other words which have lots of links to it edges to it so what i've done is i've colored the same network here could you try again i'm sorry guys my cell phone siri is picking up okay so if you look at this uh this graph what what I have done is I've color coded. Do you see this color bar on the side guys? Light yellow means the point has very few edges out of it, links out of it. Very few people are linked to it. So you notice that these light yellows are all around the periphery. Then as it goes darker and darker, those are notes with a lot of links to them out here as you can see this used to be the draft theory believed for the longest amount of time around the turn of the century there was a breakthrough and so you can see that this is the distribution of degree degree is the number of edges in a degree distribution is there then at some point influential breakthrough was made by Barabasi and Albert Albert was a student of Barabasi they together realized that real-world networks wherever you look they tend to have a property called scale freefree property. And physicists make a big deal out of scale-free properties. It so happened that he was a physicist looking at it. And so when you look at this graph, if you look at it carefully, you will realize that this graph is different from the previous graph. If you look at this graph, maybe it's not so apparent at this moment, but it takes some time to observe it. Look at this graph. You notice that no one point is sort of the hub, a strong hub from which many points, and it shows in this picture, in this, but when I color it, but when you look at this thing, the Bayavasi graph graph what do you notice you can see the hubs you know the strong influence points from which all other like which have excessive number of lines or edges going out of it and when you do its degree distribution you see remarkably that there are a few hubs which have as many as 80 degrees you know 80 links coming out of them and this is how the world is you know for example some websites are very popular some research papers are very popular everybody refers to those research papers in their research paper and this is the citation network you You have the web page network. Some web pages or some sites are referred to by a lot of other sites. And most sites are, you know, most web pages or most research papers are, but they have hardly any references. So this is realistically, and even this, if you look at the LinkedIn network, for example, it also shows a similar behavior. If you look at the number of followers of Bill Gates or something it will be in millions innovation which he did what was the change he made to the earlier graph like in his model which made it more it basically starts with one assumption it says that the probability of two nodes joining is not constant it depends networks grow evolve over time so when a new node comes into a network then it has a preferential attachment to a node that already has a lot of edges. A node to which other nodes are linked. Which is common right? If you in any social group you notice that the person who is the influencer everybody first links to that. So when you bring that you automatically the mathematics when you grind through it you will have a scale-free behavior and you can see that I have plotted it on a shell map and you see the scale-free behavior of this graph. So anyway this notebook I'll give it to you guys at some point and add it to the notes. It is actually already there in the notes. Where is it? Hang on. And with that, I would like to end. It's 9 o'clock. Where am I? Share. I'm giving you an idea of what is coming so that you watch out for it when I update this note application. Ask if I access the notebook through slack demo how do I know it is there as a Google share so you if you go and look at your emails you will have received a Google share for the order you will receive a Google share for the PDF this PDF that you're seeing the notebooks are there on this okay let me show you so first of all let's go through the PDF so you're seeing the notebooks are there on this. Okay. Let me show you. So, first of all, let's go through the PDF. So, you see that I have this section on networks now. So, there I've given a lot of mathematical explanation. They said that you were asking for, right? So, for example, this, do you notice that the scale free dynamics and so forth? like the scale-free dynamics and so forth. So because it needs explaining, a little bit of reading all these things. So background theory I've explained. And then I've gone into some very interesting networks. For example, how do diseases spread? These days that's actually of paramount importance. We all are hiding in our caves because there's an epidemic in progress. How do viruses fail? For example example if you have to what are the vulnerabilities in a power grid structure you guys probably know that at one time in northeast pretty much the eastern seaboard went dark because of certain critical grid failures substation figures but how did this happen in a technologically advanced world how did we end up with cascading failures? So you look at it, you can do analysis of a network and tell their vulnerabilities in the network. That's a very interesting emerging area of machine learning. And I thought we'll start slowly here. We'll just do exploratory analysis and I'll give you a background at this moment. We stopped at exploratory, but later on we'll do. Much more deeper machine learning today. I didn't get a chance for the rest of the linear regression, et cetera. Perhaps the next help session will do that. I would like to show you now. where do you see the notes so let me stop sharing and share again the slack channel share slack application window show the screen i said no my screen is a high resolution screen then you won't see okay we can see now so you're seeing the slack so guys are if you notice on your details of example here this is how you see your screen isn't it click on details at the top you see the top right there's something called details you may fact okay open your own slack because I don't think it is showing the whole window and check if you see details link you can open up details of your channel you see these details and you see it your man house is going and then you go to the pinned items what I have done is all my nodes are there as pinned items so somebody asked about the notebooks here they are do you see this all these notebooks itemized are you getting the Who is asking this question? Are you getting the answer? You see that all of these things are itemized. Even my hand scribbling lecture notes are here. For example, the last, here it is. The network thing is not there. The network thing is not there at this moment. I have not uploaded it yet so that data explorations note site is a evolving thing as you will on the course I'll keep adding material to it and hopefully if we come out of this whole corona thing properly then I'll send that to the printer and you guys can get that book as a printed hardbound book in color hardbound book at whatever the printer charges which should be $30 $40 I think we wrote down the names the bootcamp no so I made a deal with Lulu online printer called Lulu and so they have agreed to print it's an on-demand printing so let's say 15 people I would imagine that about in our network about 300 people will want that textbook so specifically we can order a print of only 300 books so print on demand service so this is it guys all the notes are here you see that this this I everything is here right the only thing not here is the this thing the PDF notes and those are a Google share I've already shared it with you the YouTube videos they are here so for example if you click on this you can go straight to the YouTube. Are you by the way? Are you guys actually using the YouTube videos to review or it is just a waste of. Because it takes me quite some effort to upload all of that. Are you guys actually using those videos? Anybody? Yes, I use, I go back and create isn't some of the stuff on the second of the not not from entry and I know certain areas that I need to focus on so I go through that the videos are very useful awesome all right guys it's 9 o'clock we are being having this session for two hours do you guys have any questions otherwise we'll close and we'll meet on Monday if I have a question the lab notes right is it similar to the one that you shared in the previous kind of instance of this class the regression note I have not shared with you guys yet right let me do that tonight yes they're exactly identical and you are talking a lot about Python so does this have a little Python because the previous one had the last time I gave it it was both are in Python your batch did it with both are in fighting elders okay let me go back see it again okay then then it's then I starts with R and then repeats the example in fighting now this time I'm going to do a different strategy because repetition makes it rather done I may alternate or one problem I'll solve in our, one problem I'll solve in Python. I might do something like that. But anyway I'm not changing the nodes. It is as it is. Thank you. Longer term I'll absorb it in this new format, the new data explorations book that I'm writing. But at this moment I don't have time those notes exactly as it so did you verify it is there in both question for me yeah we can verify it is there in both okay I'm also going there and looking at it anybody is missing basic material guys let me know how to reach out to me I'll take care of it. Lab one solution, and lab two solution. Yeah. So 29 cases. Uh. Uh. I'm looking at this. Maybe to your batch I didn't get the one with both R and Python, but now it is with both R and Python. I'll add it. i'll upload it let me upload all okay thank you by the way guys a general feedback are you guys finding this course useful like do you feel like you're able to follow along is the textbook uh understandable When you said the textbook, the ISLR, right? ISLR, yes. Actually, it takes a couple of readings. I'm not able to grasp everything at one print, but definitely after the way you explain it, it's always better. Otherwise, I'm not able to grasp the concepts there. That would be true in fact that's a general feeling people feel that that book feels very dense till they have been through the workshop trust me by the time people go finish the intermediate they begin to find that book elementary I've had people come to me and say don't you think you should up the level and start with a more deeper textbook whereas people who are doing ml 100 the first one for them even this is a tough beginning yeah all of you will reach a level when you'll find that book too easy so keep at it okay all right guys so with that I'll end it thank you guys so good night thank you