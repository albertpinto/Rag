 Hello guys, welcome to today's session. It is lab session number two. Today we will cover three topics. The first topic would be, we'll look at the center limit theorem that we talked about. I'll just quickly review what that was and then we will, after doing that, we are going to look at the gradient descent which we studied and finally we'll get started with doing a couple of labs on regression. So with that being there, let's get started. We mentioned a few basics of statistics. We understood without going into the mathematics of it. We knew what correlation is from common understanding. And today we won't talk so much about it, but we talked about one fact we said that whenever you get data any kind of data then you can be either finding statistics on the entire population or you could be finding statistics on samples of the population quite often it is not feasible to compute the statistics on the entire population. It's just not viable. For example, if I have to ask you, what is the temperature of water in a lake? You can't go and take the temperature of the lake, essentially of the water at every point in the lake. What you can do is you can sample here and there and then say, okay, now I know the temperature. So sort of like that. And this is perhaps not the best example. You can imagine even more examples. If I were to ask you what's the height, what's the average height of, let's say zebras, you wouldn't be able to go to Africa and herd together all the zebras and measure their heights and then take an average. The sensible thing would be to get a reasonable or representative sample. And once you get a representative sample, you can take the heights. The trouble with taking sampling is sampling, we don't know whether samples represent the whole faithfully there is always a bias in the data there is a there are statements that how big a sample should we take even if you take a diverse sample if you just take two people so for example if you want to find out very basic things how many what's the I don't know, how many genders are there in the human being? If you just pick two people, they may both be male, or they may both be female, right? So the sample size is way too small. Even if you take a sample size, sometimes even if you take a sufficiently large sample size, let's say you take a sample size of 20 or 30, you may come to conclusions that may not be meaningful. For example, if you happen to see some soldiers and you take a sample size of 30, and you look at their gender distribution, you might come to the conclusion that humanity is predominantly male. So you have to watch out for bias in the data. It goes even further. Even if you try very hard to take sample sizes, sometimes you may not be able to. So you may say, all right, I know that there is a gender bias amongst soldiers or so forth. So you try very hard, you get a sample size of 30 or so people. You might come to the conclusion that human beings come in only two genders, male and female. You might not know that there are other genders possible between male and female, other than male and female in humanity. So the lesson there is sometimes if there are things that are relatively rare, you may not notice them in your samples. So that is one of the issues. For example, today in the ocean, all the time we are discovering fairly relatively rare and almost extinct species of animals in the sea. It's on a continual basis. Why is it that we didn't discover? Why are we still being surprised by new species? Partly because we haven't explored the ocean fully yet. It's vast, it's deep. Partly also because by their very definition, these things are going extinct. There are not that many of them available. So if you take just a sample of animals in the sea you might not find them right so that those are always issues that are specific to sampling now you can measure so the next point we said that you could measure facts about a sample and the sample could be the entire population but we'll use the word sample now facts about sample like for example if I just give you a set of values the height of uh 200 people then you would not be able to remember them and perhaps you don't those are just numbers um being able to remember the 200 values is not a value. What you want to do is extract something that gives you an understanding of the data. Things that help you understand the data, facts about the data, that in some sense give you a sense or understanding of the data, of the data set itself, of the sample itself. These things are called statistic. Examples of statistics are mean, median mode, min, max range, standard deviation, skew and kurtosis. Skew and kurtosis are probably new things that we came to realize. We talked about the moments. Now before we talked about the moments, we introduced the concept that when you have data, data can have high variability. So for example, let's say that you have a herd of elephants and you're measuring the temperature. The temperatures would show a fairly small variability. They would range from, I don't know how much temperature, what's the temperature of an elephant, let's say it's 100 degrees, I'll take 98 degrees or something like that, whatever the temperature of elephant is supposed to be so you'll see variation around that. But if you take the weight in pounds that will have a pretty large variation between the baby elephant and the giant elephant. spread sometimes and some values have a much smaller spread and it may be just an artifact of the units of measurement sometimes or at other times right so just the units are you measuring it for example the same elephant instead of measuring it in kilograms if you measured it in tons then it would have much shorter variability range range of values. If you take the same elephant, same herd of elephants, and you measured them in grams, now those values would have a much larger range of possibilities. So but you're measuring the same herd of elephants. So in a way, to remove the impact of the units of measurement, that's one way of looking at it. What you can do is you can standardize it. The one very common standardization or normalization is you look at each elephant's weight relative to the mean. Relative to the mean, how far, so you can say, oh, this is a heavy elephant elephant or this is a light elephant a relatively lighter elephant and so forth and then you scale it by if you scale it by standard deviation then what happens is it becomes a dimensionless quantity so it doesn't matter whether you're doing it in pounds kilograms tons or grams at the end of it, the normalized value associated with the elephant's weight would be invariant of the units of measurement, which makes it very valuable, right? In a way, you are not hostage to what particular units of measurement you took. Not only that, in data what happens is when you're doing, let's say, a scatterplot between the temperature of an elephant or its weight, I don't know, the two are unrelated, but let's say suppose you decided to do. Your graph would actually look meaningful. Otherwise, what would happen is the small range would all cluster one axis. All the values would cluster around the origin and the other value would spread out. So it's just more intractable if you deal with raw data. It's always or most, a good idea to normalize the data. So like I said, it's a good wisdom. You wake up in the morning, like data scientists wake up in the morning, brush their teeth, sit down, and standardize the data. It should be a basic mantra. All right, then you talked about the expectation value, which has an expectation value or the expectation operator. It is a very interesting thing. It looks like an integral of values. You take suppose X takes values, any value weight, for example, can take any value from zero to infinity. Well, there are no infinitely heavy elephants and no zero weight elephants, but whatever it is, range of values. If you and then for each value there's a certain probability of that value showing through so it is the weighted average of values right so at that moment it is basically what you expect an elephant to weigh that's why as you call the expectation value now it is represented as this integral and more intuitively think of it as just the average value that you expect for the random variable. So the average value of the random variable is its expectation value intuitively. Then what we have is expectation value of a particular quantity of z is called the moment z to the power k is called the moment so we've been through a little derivation which i'll skip but the most important point is expectation do you notice that this this is z so expectation value the the skewness the third z to the power three if you take its expectation value its skewness of the third z to the power three, if you take its expectation value, its skewness of four, it is the kurtosis. These are measures of kurtosis and skewness. Now what in the world is skewness? We realize that distributions, whenever we think of a distribution, it looks like this. It's a normal distribution distribution so let me put this way this is often called the and if it is a bell curve it looks symmetric normal is an example of it normal is an example remember it's only an example there are many symmetric distributions you can have log normals that are symmetric. You can basically use beta functions, Laplace, and many, many distributions that can look sort of normal. So somebody, somebody I saw a very lovely joke. It says this is normal distribution and this distribution is the paranormal distribution. I don't know, casper the friendly ghost anyway leaving that aside jokes aside we'll move forward uh we talked about skewness we'll learn we'll see those values and what is kurtosis kurtosis oh yeah well okay i didn't finish skewness skewness is how skewed the distribution is is it looking right is the? If you think of the distribution as a duck, is the duck looking right or the duck looking left? If it is looking in the right or positive direction, it's positive skewness. If it is looking left or in the negative direction, it's negative skewness. So that's lovely and then um likewise there's the concept of kurtosis which measures how much outliers there are how much far off values there are from the from the mean right so that is the kurtosis and um that was about the moments of it and we'll see the center limit theorem is a remarkable theorem that says um it has many forms, we'll go over it, but basically it makes a startling statement. It says that no matter what your true population distribution is, one of the questions that you have is, you don't know what the population is. Remember, I talked about all the problems in taking samples, samples may be biased. So you might say that, isn't it hopeless? If I really want to measure zebras, shouldn't I line up all the zebras and measure them? Because if I take samples, who knows, they may be all short zebras or tall zebras, or there may be some other bias in the zebras. It may not be the right answer. And then comes the center limit theorem to your rescue. It says that no, there is actually a way out of it, which is that you just take sufficiently many samples of zebras. Now, two things. First is take reasonable sample size. Now, what is reasonable? In the center limit theorem, you say, can I take a sample size of one? That wouldn't be meaningful. Two, it doesn't seem. So use common sense. But common sense says, you dip into it and take a bunch of, like if you were to look at this jar of stones, the idea is taking out one and measuring its value is meaningless. It's an average of one. So you would want to take out a handful. Dip your hand into the sample's jar and take out just enough stones so that you can weigh them and you can find their average weight. And when you do that, you can weigh them and you can find their average weight and when you do that so first is you have to take sample sizes of reasonable size and at this moment actually i'll get to the lab itself let me bring up the lab so the statement is very simple and let me state it in a more formal way. Suppose there is a population of size capital N. So let's say that on Earth, there are zebras, which are, how many zebras are there in the world? Let's say 10 million zebras are there, or 100 million zebras are there, I don't know. That's the real zebras. And let us say that you are in some God mode. You happen to know the, let's say, weight of each of the zebras. It is, and the weight is the random variable. Random variables, it can take on values in a range. And so here we are, X1, X2, XI, XN. XI is the ith zebra. And you have capital N zebras. The N zebras being whatever you're let's let's take 10 million for example 10 million zebras are there let us say that in the god mode you happen to know that their real average weight is would this be the average weight guys this notation are we um are we all familiar with this notation from high school? Sum up all the zebra weights and divide it by n. Isn't it? So we can do that. So then that is fine. And the idea is you can only do it if you're God. And the trouble is many people may say, well, we can't talk to God, and for all you know, he might not exist, right? And so forth. So, well, I'm joking, guys. So now what can we do? In real terms, what can we do? We can draw samples of zebras, which means we can run ahead, herd a few zebras, that's one sample. Now, it's no use just going and herding one zebra and looking at the weight, well, two zebras or something. Take sufficiently many zebras. So we will say that you need to herd, whenever you look at the weight of the zebras, go collect at least S zebras, right? Sufficiently large sample size. So now when you herd a few zebras, it's your sample. The word you use is you have a sample of zebras of size S. I'm using the letter S here of size S. So then you do it and then you take their weights and you find the average of their weight. So that is, let's say mu one. So remember, this is a sample average, isn't it? Sample mean. Then you go and herd another, you again go corral a few more zebras from the herd, right? You go and measure their weight and and take their average and let's say that's mu 2 and you do it k times right k is a k could be i don't know in simple terms k should be bazillion right just do it a gazillion times right it's because you are going to different areas oh yeah keep going to different areas finding ze. Oh yeah, keep going to different areas, finding zebras, or do whatever you want. But just don't keep sampling the same zebras, right? Their weights might not change. So you keep on going around and finding more and more zebras and corralling a few, and then just take their weights. So are you covering all the areas in that K-REIT thing that key read thing yeah the whole idea is you should take a sufficiently diverse if you happen to know that zebras in one area are taller than zebras another right you don't know so what you do is you know that in this area zebras exist so go and take samples from wherever you can right or random samples one from here one from there whatever it is take a lot of random samples. When you take random samples like this, you will end up with a lot of mu k, right? Averages of there. The statement that is made is the mean of the means. Now, if you take the average of these mu's, little mu's, mu 1, mu 2, mu 3, all the way to mu k, surprisingly, that average will tend towards the real population average. So you never know the population average, but this theorem guarantees that you are asymptotically tending towards the right answer for a very large number of k's. So the more k's you take, the closer you come to the right answer. So don't be happy with one sample. Why? Because that sample will have bias. And what happens is when you keep on sampling, these biases cancel out. But it happens only for the mean, remember. It doesn't happen for variance. Remember we said that a variance variance is always underestimated. So average of all the variances will still be underestimating it. So we have to do a correction there. So in other words, mean is an unbiased estimator. And you can come to the right value through this. But variance isn't. The other thing that we have is, which is quite remarkable, is that if you plot out all these mu's, plot out these mu's, you will be surprised that the histogram of the mu's is a bell curve, irrespective of the original distribution. That's a pretty remarkable fact. bell curve, irrespective of the original distribution. Right. That's a pretty remarkable fact. So now you can say, what should be a good K? How many samples should we get? The answer to that is how accurately, how important is it for you to get to the real population mean? If you need to be the more precise, the more closer you want to be, the more samples, the more number of if you need to be the more precise the more closer you want to be the more samples the more number of times you should sample the more the case should be are we are we together so if you go and take a gazillion samples then you're pretty much done it will turn very very soon towards the real mean in practical terms you can't do that right zebras will kick you i don't know they're very nervous animals so it's scary being around a zebra a few times and forever doing it it doesn't it's not practical so well good news is in practical terms reasonably reasonable number of sample sample like the sets of samples, let's say that you took about 10, 15, 20 sets of samples, you're pretty much getting to the real deal, that you're pretty close to the right answer. So unless you need to be absolutely right about the population mean, you don't have to do gazillions of experiments, gazillions of trials, one more time. The second question that remains is, what should be the sample size? What is a sufficiently large sample size? So there is, in the literature, people have a lot of things in the wild. They'll tell you, oh, it must be 30. And sometimes, I mean, if you're taking a statistics class or something like that, people will tell you that 30 is the bare minimum you should take. Actually, there is absolutely no mathematical evidence or limit that makes 30 a magic number. It is not true. This will happen, actually, even if your sample sizes are very small. Let's say four, four three whatever it is the only thing is the smaller you make the sample size the bell curve that you will get and i'll give you an example at the end of it the bell curve that you get it tends to be more uh smoothed out in other other words, let me draw it out and say what happens. What happens is if you take small s, so we talked about, let's talk about the notation that I used. Now I use this notation, k samples, right, s size of each sample. Isn't it? So if S is small, you get this. You get a curve that is very, sorry, very gently. You see, you get a bell curve. By the way the the lower thing is not a curve is supposed to be the x-axis i apologize is that a reasonable approximation to a straight line right yep looks good x-axis right this is zero so what happens is that um you will notice that mu this is the actual mu the mu it will tend to and it will be like this when if s is reasonable more reasonable is slightly bigger is a bit Raja Ayyanar?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2?s iPhone 2 then you tend to get to right so what do you what do you see right in the asymptotic limit the entire population size of course there will be only one answer what will be the answer s will be what will s look like the photosynthesis because every time because every time you'll get the same value, isn't it? And the population mean. Because if your size, every time you're sampling the same, you're taking the same population and counting it over and over again. Right? So somewhere in there are other approximations, very large. So let's say S is very large. S is truly big, truly huge. So let's mark that one also. So do you see, guys, what is happening? The bigger the sample size you take, the more peaked the mu's are, right? The mu's are, the individual mu values are, mean values are. And the individual new values are mean values are so what it means is that if you pick reasonably big sample sizes, because very quickly, you can zero into the right answer. Right if you take a small sample sizes, it will take you far longer to 00 into the right answer right, so. right answer right so uh the reason i mentioned this is uh it is worth knowing that this is uh this is how it happens and also knowing that don't don't there are no magic numbers like uh some will say 20 some will say 30. it isn't like that right you can go with smaller numbers but smaller you take the more sampling you have to do the more number of trials that you have to do The more sampling you have to do, the more number of trials that you have to do. So that is one aspect to remember. Any questions? Sir, I have two questions. Yes, please go ahead. So in this case, the variance goes on decreasing and the kurtosis also goes on decreasing. Oh no, kurtosis will be zero. Remember, these are symmetric distributions even if the tail is flat we will say that the photos is oh right right right yeah from that actually for a bell curve that's a very good point actually why don't you do that and see let's make it a homework. In which you play with the sample size. Actually let's make it, you can do it in the class itself and see what happens. You'll find something interesting. Right? Okay. Second question is, you said mean of the variance would be underestimated. What about the variance of the variance? Like you take 30 samples, you calculate variance of each, they will be underestimated. But what about you take variance of those 30 samples to get to the overall variance? I'll have to think about it. We think and answer that. We can figure it out. It will just take a little bit of fiddling. But off the top of my head i can't answer that huh it should be very easy to figure out but good questions very good questions go ahead oh you mean the jupiter notebook that's right and we are coming to that itself now the jupiter notebook so guys i'll show you from the beginning so these are the two statements you have to remember by the way one of you mentioned that number 30 who was it kyle wasn't was it you yes was it another statistics course that drill that Yes. Was it another statistics course that drilled that in? Yeah. Yeah. So people say that that's not true. Yeah. Generally, there is a there is a truism in mathematics. Unless a theorem says that it's not true. Because all that is true can be stated as a theorem or at least can be stated as a hypothesis for example a Riemann hypothesis or something like that may be unproven it's called anzads you know that it's a hypothesis that likely is true you don't find counter evidence to it but for 30 and so things like that it's very easy to find counter examples go ahead Albert so that you want me to yeah okay yeah so you have to summarize i mean the sigma that right one by n of sigma right yes will be the real population absolutely absolutely so see this theorem historically it there are many variants of it. So I am stating it in a way that is, let us say, engineer common sense, making it down to earth. But it is actually surprising how many variants of this theorem exist. So you can literally say about center limit theorems, because there are many variants of it, all essentially saying very similar things, but with some subtleties. Now there is a catch to it. There is a fine print, right? So this theorem has a gotcha to it. Can you guess what that gotcha is? What it could be? Negative. Negative values are fine. There is no absolute value concept yet like mean square mean square new or something no no no it's it's a simpler see I said that if you go on drawing samples their means will tend to the true mean, true population mean. Isn't it? But what if the population mean didn't exist at all? Can you have distributions in mathematics for which there is no population mean? It turns out that there are some very ill-behaved probability density functions. There are certain probability densities which are really ill-behaved. The classic of them is the so-called Cauchy distribution. Cauchy distribution is literally there to be the counter example. It is a very interesting and weird distribution for which you can't find the mean at all right because you can't find the mean well center limit theorem won't hold because you don't know what the population mean is it doesn't exist so taking samples is not going to help you do you see that right what would that be an example what creature or what oh uh culture distribution show up for example in nuclear decays in late stage nuclear decays and many things uh there are there are biological systems that exhibit coaching distribution so it is not just a distribution mathematicians concocted simply to disprove their own theorems or be the exception to their theorems it actually has real world existence right so uh so so far guys so what i will do and we will do this exercise so repeat after me guys whatever i whatever i show you repeat after me so that you are not just watching it uh and I mean as if this is not uploaded on the code no it is not uh Kyle and just wait it is there in our GitHub support workers get okay the comprehensive oh shall we grab it and post it please don't yet okay because if you post it people won't repeat after me the idea is we all repeat after me and if you repeat after me uh you will actually do it but if you post the notebook people will download and lazily just stare at it so it's a lab repeat after me do it in your notebook open a new notebook okay exactly open a new notebook and let's start typing the following and i'll explain each line of code so a first line says from scipy import stats now i think we should give a two minutes people to open their notebooks. Yeah. All right. We'll do that. Oops, sorry. All of you guys, please do as we go along. so i'll explain while you're doing it. See, remember I said that Python, it is a very slow language. The paradox is data science, a so-called data science needs numerical computations, pretty powerful machine learning needs horrendously powerful computations to be done, for which people are all the time making bigger and bigger hardware, and still it's not enough. The runtime or the training time of these algorithms can go for years sometimes, or days and months. So how do you reconcile the fact that we do Python as a very popular language for data science, while the computations that it needs to do are horrendously expensive and should be done at blazingly fast level you should ask shouldn't they be done in fortran because after all fortran is a king of the hill for fast computations right it's been there for a long time also most of the numerical computations have been done years and years ago. Those libraries have been created years and years ago. So C is a fast language. C++ is fast enough, let's say, not in the same league as Fortran and C perhaps. But Python is like, while these are comparing a snail to a Concord's. So when you're comparing Python to, let's say, the performance of Fortran, et cetera, you're literally looking at different orders of magnitude. It's a very slow language, but it's a beautiful, well-organized language, and easy to learn. So what people did is that you don't want to reinvent the wheel. Performance is one reason. The other is numerical computations are horrendously difficult to get right. Those libraries to make them bug free. Nothing is as simple as it seems. I gave you an example. If you want to find the average of two numbers, X and Y, doing X plus Y divided by two is actually the wrong way of doing it it doesn't work so your intuition fails you can't write code the way you would write typical code a lot of thinking has to go into how numerical libraries do it something as simple as finding the mean of two numbers right because they can be overflow and so on and so forth. So that points to the subtlety, even in the most trivial things. So these libraries, they have been written in Fortran in C. These are the LINPACK, LAPACK, BLAS, right? BLAS is the basic linear algebraic system, all software. These have been written 40, 50 years ago, right? For decades, they have 50 years, maybe more actually, of 50, 60 years of maturity. Are we together? Ever since there were computers, there was Fortran essentially. Very soon right after was Fortran. And they have gone through 50, 60 years of maturity. All the bugs have been weeded out, their performances have been optimized. So what these Python has done, or Python does, is it sits upon those libraries, those fast, almost bug-free libraries. You don't reinvent the wheel. Written in Fortran, C++, even today, any state-of-the-art library, the new fan, TensorFlow, PyTorch, these are not written in Python. You write your code in Python. You use those, but underneath they are written in C++ and C, right? And they leverage a lot of the assets that have grown over the years, right? So NumPy that we got introduced to last time sits upon BLAS. The SciPy sits upon libraries like Atlas and Linpack, LaPack, and so forth. And you can actually dip into those. When you install these, you'll see that those Fortran libraries are getting installed on your system. So Sypy is for scientific computing. Statistics is considered part of scientific computing. So Sypy library is there. Matplotlib, Pyplot, is a plotting library. Now, Matplotlib is a library that has two layers to it for plotting, making graphs. The Matplotlib core itself is a low-level library that uses such primitive as artists. Underneath, there are things called artists. Each artist can draw one thing. It can draw a line, draw a circle, area, draw something. These are artists. And it's very low level. Unless you're trained in computer graphics, you usually don't go down to the level of matplotlib. But on top of it, there is a higher level library, more user-friendly library called PyPlot. And most of us use PyPlot most of the time. Occasionally, for power users, you dip into the underlying Matplotlib, but most of the time you use, when people say they use Matplotlib, what they're saying is they're using PyPlot of Matplotlib. They're not directly using the artists underlying there. Right, so that is it. NumPy, as I just say, what it gives you is multidimensional array and functions on multidimensional arrays. If you remember, this was your homework to look up and study, review NumPy, which I hope you did. Now, PyPlot, this is just a little configuration setting. This is an old language that just says that, okay, whenever you draw a plot, make it 20 by 10. The ratio is 2 is to 1 ratio, 20 by 10, right? So this is it. Now, gazillion. Gazillion is just a number I defined to be lots. Now, I define lots to be a million. If you look carefully, it's just a value I called a million. Are we together? Now, let us look at a normal distribution. What's a normal distribution? Bell curve. In simple terms, a bell curve. So suppose you happen to know that people's heights or zebra's heights, they have a bell curve distribution, which probably does, right? Suppose it has a bell curve distribution, which probably does. Suppose it has a bell curve distribution. Then if you sample from them, then let's see what happens. So a norm, by the way, the word norm is quite often used. Should I prefix it with the word bell curve? Let me do that. So have you guys typed this much from here to here? Have you typed? Have you typed this much from here to here? Have you typed? Yes. Okay. Is anyone facing any trouble with starting your Jupyter notebook? Okay. You guys, please, if you need help, take it. This is not the time to fall behind. We are just starting the course. So the bell curve, right? We call it normal. So how would we get the bell curve? Well, from the stats library, we import the norm object. And the norm has all of these values. So the first thing is, if you get the norm, instantiate the norm, but you don't give it the mean and variance, you know, the center of the bell curve and how fat the bell curve is, it will assume the mean is zero and the variance is one. Right? So now let us print out, type this thing, guys, and I'll explain what it means. What we are saying is print out from the norm of four statistics. What are the four statistics we are asking for? Mean, variance, skew, kurtosis, which is cryptically written as MVSK. So you can imagine that M for mean, V for variance, S for skew, K for kurtosis. And we just print it out. So please do that and confirm for yourself that a bell curve has zero mean, zero variance, zero skew, one variance, zero skew and zero kurtosis. Skew is symmetric. Kurtosis means, well, the tails are not heavier, right? Heavier. One way of thinking of tails is to ask this question, are the tails heavier than the bell curve? If they are heavier than the bell curve, it will be positive curtosis, right? That's one intuition that I carry. So by definition, bell curve is the standard, right? If you look at the math that we just did. So type this, guys, and run it. And make sure that you get this output. So norm here is the probability distribution and not taking any samples yet. We haven't taken any samples. We're just asking the norm function itself, but internally go take sample, tell us the variance Q and kurtosis. Okay. And it is producing those four moments. Remember I taught you the four moments. So guys, here's the thing. Mean and variance is something you learn in schools, but actually it is a cardinal sin. Remember, I taught you that for a moment. So guys, here's the thing. Mean and variance is something you learn in schools. But actually, it is a cardinal sin if you get a distribution of data not to look at the skew and kurtosis. Let me make it very real for you. If I were to ask you in the companies that you work with, what is the average salary you would be, and somebody told you what the average salary in the company is, you would all get depressed because it would turn out you're all earning less than the average salary. Can you tell why? Because the CEO and higher management, there are very few people, but they're paid a lot relative to everybody else. Exactly. Pretty much the C circle gets paid in some places ridiculously more than half the salary. The more than half the total compensation pool goes just to the C circle. The rest of us then go with our begging bowls, right? So the average is determined by that circle, right? And you are pretty poorly off. but what is a much better so you should look at the median therefore but how would you know that such a mischief is happening what would be the clue that such a mischief is happening what would you look for later excuse outliers you would look for skew you would look for kurtosis and so forth. Right? You would look for these things that will give you a, this is the importance of looking at skew. So a very basic question, should you look at the median? Have you noticed that if you look at, for example, any disease, cancer especially when people when you have a cancer at a different stage people will tell you the median like median survival time that on average how many people survive why don't they tell you the why don't they tell you the average survival time can you guess why people don't tell you the average survival time? There are outliers that affect the mean. Right, because some people have a spontaneous remission or because of meditation, remission of cancer, and then they live forever. They will die at 100. So looking at that data doesn't mean anything. They are outliers now. So you'll have a long tail in that distribution. So you don't look at, you look at the median from that perspective. So what you should look at matters, but it doesn't mean that mean is useless. If you're budgeting, as a CFO, if you're budgeting, that this is the salaries portion, compensation portion of the expenditure, this is on hardware, this is on whatever, marketing and so on and so forth. But then average is more important because you'll multiply the number of employees by the average to get the total bracket, compensation bracket. Are we all here, guys? Can I make progress now? Yes. Observe that Belker, this no skew, no kurtosis. Now let's go and take gazillion samples from it. So by the way, this is me just plotting the bell curve. I just wanted a simple way to plot the bell curve. This is about as good as I did. I'll leave that. Now comes the real thing. What we do is, let us go, and I'm taking a sample size of how much? Let me make it. Actually, while along with you, no, by the way, you have to do this. Type it in, type it in. Asif, why is the gazillion a string? Why didn't you use like as a, right? It's a constant. I define it at the top. Yeah, but it's a string, right? There's an underscore there. No, no, no. A Python allows you to put underscores and numbers just for clarity. I did not know that. Thank you. So it still means a million, even if. All these languages, Python, java all the modern languages they allow you to put underscores for clarity oh i didn't know that thank you otherwise you can imagine right you you have a string of zeros and you sit there keep counting the zeros so that's that. So guys, let's do this and plot it out. And while you're plotting it out, write it. So what in the world is the RVS? RVS is psi pi speak for random variates. So what is a random variate? One value drawn from the distribution. Right? One value drawn from the distribution. Like in other words, the weight of one zebra is a random variate. Are we together? Random variates is a sample, like you know, certain is a collection of zebra weights. Is that making sense? I hope. Yeah. So that is that. Now here, what we're doing is we're making a population. This is the whole population, Right? And the whole population, you plot out the entire population itself. X is the population itself. But I got the numbers, not the graph. Okay. You'll have to scroll down a little bit. Let me run it. This is happening for all of us. Oh, yeah, yeah. Got it, yeah. now you got the now you got this huh i think he missed out the semicolon because if you don't put the semicolon that's right get junk here you'll get oh yes i did so remember one things, in Python, you don't put semicolon at the end of your statements, unlike C, C++, and Java. So that's what people tell you. But in Jupyter Notebook, whenever you plot, if you put a semicolon at the end, it will just prevent Jan from appearing. Otherwise, it gives you the data, and then it makes a plot. It's not Jan, but it's sort of clutter. So are we all here guys? Can I make a move forward? So remember, X is the population. Now in this population, where is the mean? The mean is at zero. So now let's do one thing. Guys, is there anybody who still needs time to type this in? Are we all here? So Asif, what does bins over here mean? Oh, bin. Whenever you do histogram, in how many little bins you put the data? Like, you know, I'll show you what it means. It's a good question. Actually, might as well change the... First, let me do one more thing. If it is a population, let me just call it the population. One moment. I will change the name of the variable also uh popular popular this is the population and let me use the word population so when i host the notebook it is clear oh sorry this this population so what it means is at this moment, we are binning all the numbers into a thousand bins. Oh, now the color has changed. But suppose I did only 10 bins. You will see. Oh, do you notice that there are 10 bins? 1, 2, 3, 4, 5, 6, 7, 8, well 9 and 10 have disappeared. They're very too thin to see. Do you see that the data has been counted into or you create 10 bins of range whatever and you put data there 1, three two like if you just count the bins you'll see that there are 10 bins from minus five to five okay right so they're 10 bins so instead of 10 bins as you can see you the shape of the bell curve doesn't quite show up i'll make it when you make it a thousand uh you know that you're looking at a bell curve. So this is the true population distribution. Now let's go further. Now what we are going to do in our language, because I want to match it to our language, let me do this. Our K is sample S size. How many? The sample size. The sample size. Sample S size. How many? The sample size. The sample size. Sample size. S is equal to. Here we are taking a sample size of 100. So we are going to corral 100 zebras together and go take their weight. And then keep repeating that. How often will it be repeated? We'll repeat it a gazillion times. Remember the rule of center limit is that you have to keep doing it in the asymptotic limit you'll get a bell curve and we'll see whether we get a bell curve or not i will just say sample size s right let me run this so guys write this down and run it also. When you write it down, also run the notebook. Oh, by the way, should I explain this code? This is called list comprehension in Python. Right. This syntax is called list comprehension. If you're not familiar with it, please sit down with our TAs and we will explain to you what list comprehension is. It's one of the powerful things that you don't have an equivalent syntax in C, C++, Java. But at the end of it, what you have done is you have created lots of samples. How many samples have you created? You have created a gazillion samples not actually practical in real life the sample has um hundred hundred yes hundred hundred values So the sample size is 100. So please run this and we are going to plot the means. We are not going to plot the data itself, the population itself. We are plotting the mean and let us see what the mean looks like right sorry again i'm not getting the histogram the thing runs but oh why don't you share your screen and it's just slow are you getting an asterisk it's running slow for me so guys one thing you can do if you don't have a powerful machine, you can reduce the size of your gazillion. Maybe 10,000 is enough for your gazillion. Go to the top, change the value of gazillion to just 10,000. Make it smaller, then it might work faster. Then make sure to reduce your bin size as well. Yeah, and then reduce your bin size. Good. Guys, remember, this will be our only lab on statistics. After that, we'll go full steam into core machine learning algorithms. Can I share my screen? Yeah, definitely. share my screen yeah definitely please do well i can't share it because somebody else is sharing no i am sharing i can stop sharing um but yeah share your screen okay Oh, you got a beautiful curve. Oh, it just took. No, no. Did I? Oh, yes. It has come through. It just took time. It just takes time. That's all. Well, you know, see, you're gazillion. It depends on your definition. I actually reduced it, as you said, to 10,000. Yeah. Reduce the bins to 100. This also will now run faster. it depends on your definition i actually reduced it as you said to ten thousand yeah that reduce the bins to a hundred and then this also will now run faster and bin sizes also you should decrease you have too many bins thousand is too many wins yeah no no you'll have to wait now oh okay all right so basically one thing raj if you are in the field of data science, right? One of the first things you do is you go make Apple rich or Dell rich, whichever you go and buy yourself a more recent laptop. Oh no, I have a recent laptop it's it's just that it traveled doing a lot of things in the back okay all right yeah yeah so basically if the sampling process over here is random then the samples might be overlapping right the sample values oh yes of course there will be repeats it is it is uh in other words it is quite literally uh bernoulli the technical word you use is bernoulli sampling in other words you so basically what it means is look at this jar right i take a sample yeah i find its value i put it back replace this but these are floating points right let me say so you take out stones just pay attention forget floating points in all of them from this sample if i take values out i have a handful of stones there are two possibilities the next sample i keep these stones away and the next sample is again taken from the jar so there is no overlap. So that sampling is not what we are doing. That is called sampling without replacement. In statistics, there's a name for it, sampling without replacement. I think it's also probably called Poisson sampling or something. But when you sample and put it back, and you take one sample out, you measure the weight of each of the stones, and then you put it back. You shake it a little bit, you take another sample out. There is a possibility that the same value will show up in multiple samples. That is your question, Siddharth, isn't it? Yeah. Okay, that is called question said that isn't it yeah yeah okay that is called sampling with replacement okay now to the point that uh kyle is making see we are talking about real numbers real numbers there are infinitely many real numbers obviously so when you sample on the real nine there is a chance that sometimes you'll end up with the same value well computers can't do with real numbers. What they deal with is so-called floating point numbers. So it's still a pretty large set. So when you sample from the large set, there is a reasonable probability that two samples may have common values or a few samples may have the same value showing up. That is perfectly okay. The center limit theorem has no, nothing is silent on that topic it says it doesn't matter okay so guys are we all here so far right so you notice that what happens What happens? This bell curve is centered around zero. Would it be fair to say that the mean of means is zero? Where is the bell curve centered? At zero, guys? Some response? Somebody? Yeah. So what we are seeing is, and what was the true mean? Let's go and look at the true mean. What was the true mean? It was 0 of the population mean was 0 isn't it right? So you can say, Well, i'm not impressed. You took a bell curve, and then you took samples from it, and then you're telling me that the the sample means show a bell curve distribution, right? That sounds like cheating. Let's try something else. We are now going to do another distribution, which we will call the two distributions we'll take, uniform and beta. Uniform distribution says there is equal probability of any of the values. So basically, and beta, beta is one of the lovely, lovely distributions that shows up in survival analysis and everything, just about anything. It's one of those magical transcendental functions that a simple word to say it is, it's truly magic. It's one of those unicorn-like creatures in data science, the beta distribution. We'll come to that, but before that let's take the uniform, the most boring distribution. So what is the probability of something versus something else? They are both the same. What's the probability of seeing a three? What's the probability of seeing seven? Well they're both the same. So that is a uniform distribution from this uniform distribution if we do a histogram it is more or less flat because we didn't take enough numbers it looks a little minor variation but as you can see more or less flat distribution isn't it so this distribution is flat now from this so please type this much in and then we'll continue. It says uniform is not defined. From. You have to import it. Did you do this statement? Oh, okay. Oh, I thought all the imports would be together okay So RV is a very cryptic way of saying random variates. Remember, extract some random values from that distribution. By the way, if you don't want to see junk at the end, this is the more formal way of doing it instead of putting a semicolon. If you say plot.tight layout, it will just prettify your plot, make it more condensed. This is how you give a title to a plot. So you notice that I'm adding a little bit of a visualization things to your code now, maybe steps towards visualization. Baby steps towards visualization. Let me do this. Everything you can see in one shot. Is my screen reasonably readable, guys? Are you able to read it? Yes. Okay. guys are you able to read it yes okay and so folks you're creating your first data science notebook today. Are we done? Hi, Asif, I have a question. Go ahead. ask if i have a question go ahead and in a plot title they are there what is that ignore that uh that is some of me me doing something for this there's a language called latex and people like me who start introducing mathematical notation they use it but you don't have to you shouldn't okay thank you okay Now that you're done, now let's sample, right? The same code, but we are sampling from the uniform distribution. And what you notice is when you sample from the uniform distribution, surprisingly, those means show a bell curve distribution. Now, this should come as a surprise, unless you know the center limit theorem, isn't it? distribution. Now this should come as a surprise, unless you know the center limit theorem, isn't it? The mean of value is drawn from a uniform distribution. All those means show a bell curve distribution. And I believe at this moment you should be, if you really think about it, it's quite a remarkable fact. This is something you probably didn't expect. Normally you wouldn't expect unless you noticed here. I think it's Vince. So in the subsequent labs, guys, I'll be moving a little bit faster. So gradually ramp up your Python speed. So, anybody who has been able to reproduce this so far, speak up please. Ravneesh, are you there? My gazillion is 10,000, so I made the bins 100. Is that correct? Yeah, you can make it a bit smaller, yeah. Okay, I'm done. Does it look anything like this, remotely like this? Yeah, it's just chunkier, more pixelated. That's right. And so, guys, what you notice is you have a uniform distribution. So if you have a uniform distribution which goes from minus infinity to plus infinity, what is the mean? Would you agree that the mean would be zero? Yeah. The uniform is zero. And this, again, is centered around, actually, for whatever reason, it's centered around 0.5. That's a good question. Why did it do that? It's a uniform distribution. Oh, my goodness. Sorry. By default, the distribution that we took is between 0 and 1. Do you notice that this uniform distribution doesn't go from minus infinity to plus infinity? The data that we selected went only from zero to one. So what is the mean of this data? A half, isn't it? This data's mean would be half. Would you agree? Right? So this is what you see here. The mean of this, the mean of means is peaking at half, or less you can say well these are easy ones what about something hard so there is a distribution called a beta distribution these distributions by the way are the workhorses of machine learning they are lovely distributions these are shape shifters right so you know shape shif, in kids' stories, you always have this ghost or something that can take the shape of whatever. It can become a fish, it can become a horse, it can become whatever it is that you want. Usually they are evil characters. But the beta distribution is actually a benign character. But there are two knobs, A and B. And as you turn A and B, the shape of the distribution changes. So I'll show you for A, B, both A equal to 2, B equal to 10, it looks like this. So I'll let you write this code. Hang on, let me tighten this code a little bit, remove extra spaces here. So remember A and B are the knobs. If you turn these knobs, different values of A and B can play with it. You'll see different curves emerge. So a beta distribution is essentially a shape shifter. Now if you look at this distribution and you think of it as a duck, which way is the duck looking? To the right. To the right. Right side. What do you expect the skew to be? Positive. Positive. Positive. Compared to a normal distribution, does it have more on the outlier side is or is the mass much more concentrated towards the center outliers there are outliers but is it relatively more to the bell curve or less than the bell curve more outliers than the bell codes. More outliers. Okay, let's see if it agrees. We will get it. So anyway, guys, have we written all these four things? I've done nothing the same code. I've just replaced it with beta, the beta distribution. So this it says that yes, the kurtosis is, well I should change this to this. The kurtosis here mean is 0.166. Where is 0.166? Sorry, 0.166 is somewhere here. You see where my mouse is? If this is 20, 16 should be somewhere here, right? Here is the mean. What is the mean? What is the mode? It's here. Right? Mean median mode. The highest point is less than the mean and the median will be somewhere in between. It has a positive skew or negative skew. The value here, 0.9 is a positive number, isn't it? And does it show up in the picture? The duck is looking in a positive direction towards the right. And likewise for kurtosis, it turns out that it does have certain kurtosis and so your intuition is borne out now we will sample from this distribution exactly the same code except that you replace the previous code with the word beta and now i'll go a little bit faster. I leave this as a homework. I mean you guys have to do it right now. Do this replace the whole code. But now, by the way, guys, you're done? Yes. Ravi? I think Ravi has gone to sleep. Can you show that last line? Which one? This one? Yeah. The last one that is print the mean variance oh okay so that comes afterwards so guys are we all done here i will now scroll down this is exactly what we did before i know i'm not seeing for these two values of a and b i know but i'm not seeing it for these two values of a and b Okay. um Are we all done? Should there be that comment after the curly brace around the skew? I'm sorry. Should there be? Oh, it looks like you're fine. Okay. Yeah. All right. All right, guys. So we do this. And now we sample from this distribution. I assume that you have done this. I'll post this notebook shortly in a moment. Let us now look at this distribution of the sample means we do exactly the same code. We go and draw a lot of samples from this distribution. A gazillion samples we take out of it, and we repeat the same code. And what do you notice? We get the thing. Now, if you were to look at the mean, what would you guess the mean to be approximately is between point 1, 6 and point 1, 8 point 1, 6. Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam agree with each other guys and isn't that magical the two distributions look so different you know this is the distribution but in irrespective of that the mean of mean shows this i mean the the means showed this distribution you can take another variation of this one reason i like the beta function is as i told you it is a shape shifter what you can do is the code that you wrote in which all that you have to do is change the value of A and B. Make both of them 0.5. When you make A and B both 0.5, exactly the same code will produce a distribution that looks like this. Now, when you look at this distribution, does it look symmetric? It does sort of look symmetric. By the way, when you look at this distribution, does it look symmetric? It does sort of look symmetric. By the way, it's called the bathtub distribution. This sort of distributions are common. For example, in survival analysis, look at human beings, what happens? Just when you're born, there's, very high risk. There is a considerable risk in the first few months, first couple of years, because your immunity system is not developed, isn't it? So the risk of death, after that the risk of death is pretty uniform. You're healthy, more or less you're plateaued off, you're in the bathtub. And then as you get older, what happens? The risk of death, I think that's increased. Right? So this is the famous bathtub way of looking at the beta function. Now, if you repeat the same thing, you notice that the mean is half. Why? Because between zero and one, you would expect the mean to be 1. Variance, there's considerable variance. There's a lot of data moving around. Now, what about kurtosis? Because it is symmetric, there is no skew. Kurtosis, minus 0.5. What does it say? Most of the mass is not near the center it has been pushed out right it has been pushed out to the peripheries isn't it most of the mass has been pushed out right on the other hand here what is the kurtosis here is positive means most of the mass has been squooshed into the middle and the number of outliers are fewer. Right? So that's the positive cartosissance, more of the masses pulled in. So okay, you do the same thing and you do you sample from this distribution which looks so wildly different, right? And still you get the same bell curve, right? And by now you should convince yourself that this is true for all distributions. You can do it for discrete distributions, it will still be true for all situations, it will be true. I'm going to post this notebook right now so that you guys can take it and put it there. Let me put it on Slack. Kyle, could you please also post it on the course webpage? Okay, I'm posting here. Now, And this was the center limit here. All right, guys, take this notebook and you will have it. We'll take a 10-minute break, guys. And after that, we'll pick up pace. The first notebook we went through slowly, but now we'll pick up pace. We'll take a 10-minute break. Please import this Jupyter notebook into your desktops. Play around with it. If you missed out on some code, catch up with it. If you found this coding hard, it means you need to pick up your Python. And in particular all right guys so let's start again we have finished a demonstration of the center limit theorem i hope you are suitably impressed. It's one of the crown jewels of not just statistics, but all of mathematics. The more you think about it, you can take the craziest distribution and you'll see that it is true. The only thing where it wouldn't be true is, and it doesn't claim to be true, of course, is when there is no population mean. Like, for example, for a culture distribution. So we'll close the topic. We will find it useful later. Guys, please mute yourself, all of you. So today, so that is something to remember. Why is that relevant? Because it means, it gives us hope that we can do statistics using samples. We can actually do machine learning using the sample and we don't have to go to the entire population or wait for the complete data. The opposite of center limit theorem would be that any sample would be misleading. You're right. Sorry, it's not true. Now, we are going to change topics. And we'll talk about something else that we taught in the lectures, in the sessions we covered. One of the things we covered was gradient descent. Remember, I said that the lost surface, the error surface, you can do a gradient descent of. In particular, I took the sum squared error and the mean squared error and mean absolute error. So today we are going to do gradient descent from force principles without using any machine learning library. Because just so that we see how these things work is good to implement it from scratch. And, and to do this, we will, we will take a data set that comes from one of the most amazing things in nature. Many of you are familiar with geysers, probably you have a geyser in your neighborhood. I don't know. In the US, which is the West Coast and the central, certain areas of the US, they're seismically very unstable. They're geologically very dynamic areas. West Coast, of course, belongs to the ring of Fire. So there's forever an earthquake. There's an earthquake. I mean, today, by now, it's close to noon. And I wouldn't be surprised if more than 100 earthquakes have already happened. There are websites to plot it. There are apps, iPhone apps, to track it. Most of them are small. You don't see it. Now, in the yellow, there is a wonderful place called the yellowstone national park and the national parks in u.s of course since there are people from outside u.s i'll give a little bit of understanding u.s is one of the in fact the the country whose greatest invention perhaps in a thousand years will be remembered not any one of the things that it discovered let's say I don't know the transistor or something like that it would be fair to say or at least I would think that the greatest and the most remarkable discovery of the United States was the concept of National Parks as human beings are proliferating everywhere and destroying the land it was happening all over europe it was happening everywhere in the world in the u.s thanks to many visionaries they decided that human beings should be limited to only a portion of the land and vast tracts of land should be left free of human habitation and where human beings are at best guests. We don't belong there, we are guests there. These places which are preserved constitute, if I'm right, about a third of the United States. A third of the land is devoted purely to national parks, which is why when you come to this country the first thing that strikes you is the utter greenery and vegetation that you find, natural beauty that you find here. So one of the national parks that's amazing is called the Yellowstone National Park. It is absolutely breathtaking. It's geologically in a very dynamic area. geologically in a very dynamic area. The earth's crust there is pretty thin apparently and there's a lot of volcanic activity and things happening. One of the things that keeps happening, well, volcanic activity hasn't erupted recently, recently as in a few thousand years, but that is a rather short time in geological time frames. But there are some things that happen called the geysers and there are these geysers that periodically almost by the clock they keep erupting. One of them is called the old faithful geyser because every once in a while it erupts. So what happens is under underground the water pressure builds up because of the heat and at some point it all shoots up through a fissure and when it shoots up through the geyser it lasts for a few seconds now the the longer it lasts the more I suppose the energy is dissipated and so the longer it takes for the next one to show up you know for the water and they need to develop enough pressure to again come shooting up. And when you look at this picture, it looks quite beautiful. In real life, it's even more breathtaking. Now, those of you who are in California, I don't know if you know, you don't have to go all the way to the Yellowstone National Park. We have one in our backyard. There is one in Napa. Well, actually, close to Napa. Sonoma. One is close to Sonoma or Calistoga, is it? Somewhere near there. It's wonderful. It's smaller, on a smaller scale, but it's there. You can go visit it. So we'll take the data here. The data here is... So what is the goal of this? We will do something remarkable. We'll take this data and we'll implement the, well, in your case today, this one you won't implement because it's a little bit harder, but I'll walk you through the code and show how you can do the machine learning that we learned from scratch. So we take this data set. I will walk through the lines of code. This is the data set, by the way, it's there on the internet on our website, you can directly pull it from the website. When you look at the data, this is the source of the data is the URL. And you can read the data when you read the data, it looks like that. So what is it eruptions? How long the eruptions lasted and how long you had to wait after the eruption. So as you can see, if the eruption lasts longer, the wait time is longer. If the eruption2 rows, two columns, eruptions and wait time. These are the only two things. And your goal is to predict wait time. As a function of the time that of the previous eruption, how long did the previous eruption last? So this is very analogous, quite analogous to your ice cream. The entrepreneur who is trying to sell ice cream on the beach in his shack right so there also you had to the target variable was or the outcome was how many buckets of ice cream will you sell and or how much ice cream would you sell in quantity and the input was the temperature of the day here the input is eruptions and the output or the target variable is the rate time. So you can take this data. Now I told you that a good data scientist would wake up, brush their teeth and standardize the data. So that's what we do. It turns out that obviously in the library, this is the scikit library, there's a lovely function called standard scalar. It will take your data and transform it into scaled data, transformed data. the library, this is the scikit library, there's a lovely function called standard scaler. It will take your data and transform it into scaled data, transformed data, right? So when you look at transformed data, it's, as you can see, well, okay, these are, this is, you have the data, you look at its statistics, and you will notice that, what is the mean? 5.5 into 10 to the minus 16, which is close to zero, right? So it has been centered, right? The data has gotten centered, both the eruptions and wait time has gotten centered. The standard deviation has been made to one because that's what standardization does. It makes it zero centered and unit standard deviation that close to zero it wouldn't be exact but it is 5.1 anything into 10 to the minus 16 is basically computer's definition of zero it's it's as close to zero as it would come you have a value from min to max min and max you can see it goes from minus 1.6 to 1.4 as i told you most of the there are no outliers in the data you can see a sample of the data now standardized data you can see those values are small they're not huge numbers so this is by the way these are the things or this notebook will post you you should get as you become familiar with these functions scalar dot fit transform these are literally the code equivalents of the steps that i taught you so this is what the scale does is z is equal to scaled is the z z is equal to x minus mu over sigma that is done by this line of code are we together right it is doing it. Describe, by the way, produces descriptive statistics. Count, mean, standard deviation, minimum, the 25th quantile, 50th, 75th quantile, and so forth. So what does the first line mean? mean scalar equals to standard scalar parenthesis oh it is a so let me explain that there is a library that we will use it is called uh well i haven't shown you here it is called the scikit-learn sk learn right uh in python that library is used as a main machine learning library. I mean, we together data science library. Unless you're doing deep learning. And there are other libraries that you will use. That library has a class called standard scalar. And we are instantiating the standard scalar. This thing is called a constructor. So let me put a comment here. This class comes from the scalar and library. Are we together? And I'll go over it in a moment. Actually, why don't I go over it right now, since you asked that question. So you notice that in this, I have, just so that I don't keep repeating, I have run support vectors common. That IPython notebook. So we will go into support vectors common and see what have I put there. Support vectors common. Yeah, so there are certain stylistic elements beautifying the notebook, which I would strongly suggest you completely forget about unless you are into HTML and React and so forth. Ignore that. Now I do some imports. So do you see that I'm importing? Let's go over the imports. Import numpy as np. Is this easy for us to understand guys numpy is the what what does numpy give us amongst other things give me tell me one thing that numpy gives it gives us high performance multi-dimensional and operations on arrays that looks like math simple math right what does pandas give us it basically gives us excel-like capabilities it gives us a data frame that looks like rectangles rows and columns and operations on them panda's profiling is actually something i'll explain later it is just a tool that will automatically do exploratory data analysis on a dataset. Makes it very easy. Now, when you do machine learning, you do many things. But the one thing that we should focus on here that you asked for is, do you notice that Scikit-learn, there is a library called Scikit-learn. It has pre-processing of data. The name is very apt. What did I say? Before you do anything, before you do any processing of the data, do some pre-processing. One of the basic pre-processing operations you should do is standardize the data. Z is equal to X minus mu over sigma. So this is the class standard scalar is the class in this library, this module that does that. So once again, guys, at this particular moment, the one thing to know is you should become very familiar with Python, and in particular, gradually start looking up the documentation of scikit-learn okay and pandas and python so in this context i'd like one second i'd like to recommend you two two books let me recommend those two books it's a good time to do so one book that I found very useful. Do you see this book called Pandas for Everyone? It's an old book now. By old, it means four years old. But it's an excellent book and very easy. Each chapter is such that it takes about an hour to do. And so you can pick up Pandas in a set of about 20 chapters or so, 20 or so chapters. Let's see how many chapters there are. Three, oh goodness, how many? 18, yeah, about 18 chapters. Each of those chapters are very small, right? And they just get you started with, think of it as that data science Excel spreadsheet is the Pandas data frame. This book is the best investment of a weekend that you can do. Take this book, read through the whole of it. And when you read through the whole of it, surprise me, let's see, you will notice that it's very easy to learn. Do it one little chapter at a time, the 18 chapters, right? Spread it over one weekend, two weekends. But by the time you're done with one third of the book, that is six hours, you're already very familiar with what we want to do. So I'm going to put this link in our, Kyle, could you please add this as one of the textbooks for the lab? Yeah, sure. Everyone, let's do that. The other one that I would recommend is Sebastian's book on machine learning. Let's see. Python Machine Learning, Sebastian Raskin. This is the book. And this is another very good book. I would highly recommend you, it's in its third edition. Focus, at this moment, ignore the scikit-learn part. It's a, don't be, it's a little bit of a plump book. Right? It has 770 pages, but those are very easy pages, actually. It's a very well-written book. I would suggest either book, Python Machine Learning, is a very good book. Let's put a link to this one also, Kyle. And another book that is very good. Now, take only one of them and now I was going to say this book the next edition of this book that's coming out I would say wait for that there is a third edition coming out this book is by Aurel Girond this is actually Girond this is actually a wonderful wonderful book and the only reason I'm saying hang in there is because the next edition of the book is imminent. Until it comes out, I would say don't buy it. Given both of these, it's hard to compare which of these two to have. You might want to have both of these, or you might want to have one of these. This book also. So it's second edition. edition if you can't wait you can go for the second edition of this book which is uh this as you can see i purchased it a while ago our second edition as as you can see these are best this book is also a bestseller. And if these are considered, oh, hands on, obviously, the next edition has not come out. These books have been translated guys in dozens of languages, I believe some 20 languages or so. It just gives you a sense of how popular these books are in the community. The three books that I mentioned Of the three books that I mentioned, three, four books that I mentioned, these are classics in the fold. The textbooks that I gave you, the main theory textbook, Introduction to Statistical Learning, is a bestseller in the field. This is a bestseller, as it says. And the other book that I asked, Sebastian's book, also is a bestseller. Pandas for Everyone, again, is a great book. So if you can afford to, please go get the four books or three books, actually. Between Giroux's book and Sebastian's book, you can pick which one you want. Sebastian's book is available now. So the advantage is you can finish it. Oh, it is going to be available July 24th how did it oh this is a second edition sorry we are talking about the third edition third edition is still far off so I would say that since its release date is indeterminate at this moment go and get the Sebastian's book. Sebastian Raskin's book. It's a very good book. So anyway, coming back to our imports. You know, does that explain the sentence for you? Hira, does that explain the sentence for you? Hira, you're in mute. Now you've got the context, right? So this is that. So the nature of this subject. In the beginning, you have a lot of not so much machine learning but the mechanics the tools you're getting acquainted with new tools new hammers and you know screwdrivers and things like that so get used to it it takes time it takes almost more time to pick up the tools than to pick up machine learning in the early stages but become familiar with the tools as soon as you can, because we are going to start ramping up the pace at which we learn machine learning. We won't be going at this comfortable pace all the time. So we have this data. This is the standardization of the data. Now, you can visualize the data. When you visualize the data in a scatter plot, this is how it looks. So as you can see waiting time against eruption right it looks like this now how did i create the plot there's a little bit of a mat plot lip syntax that i do uh title label while other than that it's just a one-liner you can plot it like this right so from this it is pretty obvious the longer the eruptions the more the waiting time for the next event guys please mute yourself yeah so the more the waiting time for the next eruption so remember that now what i'm going to do is take the same data this data which we have visualized now we'll do some histograms of this data. You see that there are two histograms, which is obvious. There's two clusters in the data. So along both these axes, there'll be two humps or bimodal distribution, two humps, two peaks. Then there's some more visualizations you can do. I will leave it for you to, I don't focus on the code so much. It just, just introducing you to more visualizations, but ignore it. But now I'll come to the main thing that I taught you. and the main equation of gradient descent was beta minus alpha gradient of the loss, right? The error function. The error function we defined as, people often call it the loss function. It's a generalization of the error function with a few extra things added, but we won't add extra things. The loss function we took as a sum squared error. You see that sum of the yi minus yi hat. Do you remember the notes? Between prediction and reality, square it, we take the sum squared error, which if you fit in the equation of a line, beta naught minus beta one, it becomes this. Now, there's a little bit of mathematics. And guys, this one lab is optional. If you don't follow it fully, keep it in your back pocket, keep revisiting it, sit with our TAs, we'll explain it to you. But in the beginning, it might look a little bit intimidating because I put some mathematical notations there. But this is nothing, there is nothing new here that I didn't cover in the theory session. I'm writing the expression here in its component form and then figuring it out. What does it mean when I do the differentiation of this? It becomes this. And when it becomes this and I write out the beta naught beta one next, it becomes this. So suppose you do, the important thing is that you can actually write the entire machine learning the regression without needing any libraries you can do it by hand so here i take alpha is the learning rate oh by the way uh just be aware that i use a lot of greek symbols when do i use greek symbols in code greek symbols are for concepts things like that and the roman symbols are for data whenever you see me use roman it means i'm talking of data whenever you see me use greek symbols i'm talking of basically conceptual things right so and if you're wondering python perfectly allows you to use Greek. You can mix your Greek and your Romans in Python. So this is it. Alpha is my... Do you remember what alpha was in our equation? What is the alpha here? The learning rate. That's right. Learning rate. Beta naught, beta one are the intercept and the slope of the line, isn't it? And we just took the, randomly, we took the learning rate to be 10 to the power minus 4. 0.0001. Very tiny learning rate. Then we take this data and then what we do? This is a loop. We'll take some random values of beta naught, beta 1, and then we will just make those steps. How do we make those steps? At each step, we can first compute the gradient of the function, the gradient at that point. At that value of beta naught, beta 1, we can compute the gradient. that point at that value of beta naught beta one we can compute the gradient remember given an error surface in the hypothesis space where hypothesis space is the space of beta naught beta one you take any one point there'll be a gradient you just have to find the gradient along gradient is derivative along the one axis and then against another axis you take the gradients then using the gradients you can take the step do you see this beta the next value of beta naught is the previous value of beta naught plus minus alpha times d beta naught is the gradient along beta isn't it partial so this is it this this is it this is literally the gradient step and then what do we do once we make the gradient step we need to recompute the loss here is the loss that we have we'll print it out and then you you keep repeating this step you keep coming up with better and better beta knots ultimately you stop after certain number of epochs you just stop you hope you're done right and then this is a table of intermediate values so as you can see the loss in the beginning the mistakes that you make in the beginning if you just randomly take some values of beta dot beta one is pretty high isn't it six thousand but as you as your machine learns and this is it this is. Guys, if you want to know what machine learning is at its core, it is these few lines of code, right? It is the gradient descent. Are we together? In a simplest form, this is the core idea. Obviously, we are doing it for a simpler problem, linear regression, but this idea will generalize with us. Now, the good thing is you won't be writing this code in real life. In practice, there are libraries that do it for you. But in my view, it is always a good idea to do it once by hand. Are we together? Just to know that you understand it. So take this code, guys, and study it and make sure that you understand it carefully, which is why I've written it like this. Try to reproduce it in your own language. First, understand this code, reproduce it in your own language. And there's one more, I believe. Okay, we'll come to that so then let's see how as the product as the loss how does the loss beta naught the value of beta naught goes from 4.0 to this so you can see that it has stabilized after 200 epochs 200 times after 200 epochs, 200 times, after 200 steps, the value has become pretty stable around this value, zero, right? What about beta one? Similar behavior happens. It stabilizes around 0.7 or 8 or something like that. Look at the loss function. What do you see loss? So loss is how much error remains after learning for each epoch. So what are you doing in the error surface? You're coming down. But then when you reach the valley, what will happen to the error? There is an irreducible error that you can't get rid of. So your value will stabilize. You're sitting in the valley of errors, isn't it? So you can see that here. Are you sitting in the valley of errors now after a certain number of steps so guys do you see this is machine learning for you literally writ writ out large it's worth doing it like that now fortunately you won't do it in such detail because libraries will take care of it now here is a little bit of a code that sort of projects and shows the lost surface contour, and it shows how, what really happened. So let me show you what happens. What happened is that we started at this point. Do you see where my mouse is, guys? This. At that moment, how much was the error? You can see that the error was pretty high, isn't it? Pretty high. And you also notice that the error surface is a nice convex surface. Is it a nice convex surface? Parabolic surface. Looks like a bowl, beautiful bowl. And so what happens is, as you do the gradient descent, remember, these look like concentric circles kind of thing, almost, right? Ellipse. In this, if you remember, the gradient was pointing outwards. So if you walk against the gradient, you're literally making a direct path towards the best beta naught, beta 1, isn't it? The best parameters, right? And in the error bowl, you're taking this path that you see here is that is that obvious to you guys that you're taking this path so you can animate it and i've created a little animation here uh made it into a video and you can have sometimes people ask me how did you how do you do videos and so forth uh using matpllib. So here is the video of it. So pay attention, guys. Remember, pay attention to where my mouse is. Do you see where my mouse is, guys? Right? Because we'll start learning from here in the hypothesis space, parameter space. And here, you will see what is happening. You are basically here at the error will be at the top. Now, I'll play this video. It's just a 40 second video so watch it do you see that you are moving closer and closer to the optimal solution here you are doing your gradient descent in the bowl. I'll play it again after a little while. It's inching its way forward because our learning rate was very small. It's practically found the right answer. Remember, it's in the valley of the bowl, isn't it? The valley of the error surface or loss surface. It has found the answer. I'll just zoom in a little bit more so this is more obvious see see this do you see this happen guys yes yes the whole code yeah the whole code in fact i'll do it again for you and the code that's why i've given you the code uh c I'll do it again for you. And the code, that's why I've given you the code. C. Do you see gradient descent happening here in the valley, in the bowl? So you're literally descending down this slope. Once again, pay attention to the, I'll go back here and I'll stop here. So pay attention to this point. Do You see that it is falling down? Down, down. You're descending, descending, descending. Are you descending down in the bowl? This is it. So this is your gradient descent. You see it actually happen. And the reason I wrote it out is it's worth seeing the inner workings of these libraries or how machine learning works. Any questions, guys? So to your question, how did I create this animation? Here it is. It's a little more complicated code. This code, by the way, you may not hear. I mean, guys, if you're new to Python, it will take you a little while. But Albert, you'll figure it out. This is it. Yeah. new to python it will take you a little while but albert you you'll figure it out this is it yeah any questions so i'll review this code again from the beginning guys first of all we realize that this is the data First of all, we realize that this is the data. We want to find a linear regression line here. That line will have a slope and an intercept, a beta naught being the intercept, beta one being the slope. So those are the two parameters of our hypothesis, the best fit line. How do we do that? What we do is we randomly pick some value. So a little bit of mathematics, just an obvious one. This is our gradient descent equation, straight from our notes, if you remember, guys. Right? You go against the gradient. Whatever your value of beta is, beta vector is, the next value of the beta vector is, the next location is, that location minus a little bit of the little tiny step opposite to the gradient alpha b alpha means tiny step learning rate are we together guys this very this calligraphic l is a common uh notation that people use in machine learning they call it the loss function at this moment when we are beginning machine learning, I introduce it to you as the error function. So remember, loss function, error function, in this situation is exactly the same thing. But I use the notation, which will carry you through the rest of your life. You remember that it was some squared error. Do you remember, guys? We took some squared error. And what was the Gauss-Markov theorem saying? That for most situations, this is a good idea. Now, obviously, there's a homework I'm about to release in which you will see that sometimes the mean absolute error is a better idea. So data decides which one works best. Now I'm writing it out in a vector notation. So the vectors is typically written as a column, one stacked over the other. Why? Otherwise you may run out of indices if you have too many dimensions. In high school, you write it as ij, x, i vector, i unit vector, and the j. So you don't use it. You implicitly assume that. And so let us work out each of the component derivatives. So when you work out this equation, those of you who are familiar with calculus would immediately know that if you take the derivative of this with respect to beta naught, this is what you'll get. And with beta 1, this is what you'll get. And so when you plug these two things back into this equation, what happens? You get these two parts. This is how you update beta naught. Here is how you update beta 1. So now life is easy because you start with some random value of beta naught beta 1 and then what do you need to do at that point just compute the gradient gradient is your google map immediately it's saying go this way right because opposite to the gradient is where you want to go if the gradient is pointing this way you just take a little step on the opposite direction that is the direction home isn't it and this is what you do you take tiny steps home and all those steps and now the same thing literally what we wrote here as math i've realized as as code alpha is the learning rate beta naught is the starting point just randomly start somewhere. Then this is your data. You need the data because you need to find the loss, the errors. Now, these are your values, alpha, beta naught. In the beginning, learning rate is a tiny one. This is your starting point. And you have just decided arbitrarily to take 200 steps. Why 200? Nothing magical about it it's just 200 right so why did we take four and four is beta not initial value of beta one and beta two because i happen to know that that is not the right answer i mean that those are not the optimal value so i just wanted to take something that is far from the best value and so and that would help me creating this visualization for you like to show you that there is actual learning taking place if i took a value very close to the right answer you wouldn't be able to visualize it right so i randomly took a value which was far from the right answer you can try out whatever value you like doesn't matter the in reality what will happen is libraries will start with random values completely random values okay and also because i knew i was going to make this visualization so do you notice that from this point the gradient descent is a lot easier to see right you see it happen. So I deliberately, for the sake of visualization, picked a fixed number. But in reality, pick a random point. So then now, this is just a little table of intermediate values. At each epoch, what is the value of beta? What is the loss, et cetera? And so what do you do? You take 200 steps. For each step, what do you do? What do I need to compute first? The gradient at this point. Whatever value of beta is in the beginning, this will be your value of beta. Beta naught, beta one, right? Just start with that and just update it. You say right just start with that and just update it you say you know what i'm going to add the first find the gradient once i find the gradient i will take this step which is the gradient descent step and that is it that's all i have to do and i have to keep taking this step this is just a little bit for our own benefit these two steps are just for us to create a table to see what is happening, which we do here, right? You see that the loss is decreasing. And you can see that the values are converging towards the right answer, slowly. So that is it. And you can see how the beta, these are just graphs, just showing the um betas are converging slowly moving towards the right answer descending towards the right answer and this is your loss you can see that in the beginning rapidly you move but after a little while you are in the valley of the ara bowl right the last bowl lost surface so then you you're creeping forward because your learning rate is very small. If the gradient is small and the learning rate is small, then a product of the gradient and the learning rate will also be small. And so you're just creeping forward. And the rest of it, by the way, this part of the code, visualization code can be quite messy and hard to know unless you're familiar with it do not try to understand this code in the beginning right if you're an expert with python then by all means do that so guys remember that you should stop your review of all of this at this point beyond this just look at the visualization. Because this is a little bit tricky code, and we will come to it as we develop more muscles in matplotlib visualization. Plotting, as we develop familiarity with it, we'll do it. So ignore the rest of the code. You're not supposed to understand it but i i do expect you to understand up to this section um this part is the crucial bit of code that i expect you to understand so we'll post all of these notebooks um kyle we can make these notebooks visible and this okay yeah so all right guys So this is a half an hour left. What I would show you now is if this code looks intimidating and you're thinking, oh, we just started the course and it's already getting to be hairy. The good news is in real life, you don't have to do that. So we are going to see regression, how you do it in real life. you do it in real life. In real life, what you do is, we will again take, actually ignore all of this. These are just the imports. We load a data set. Here is a data set. So let's see how you do it in real life using the very powerful very powerful data science libraries now you may ask why did i not start with that the thing is it is important to know the foundations you should know how library how things work because libraries come and go right so but now in practical terms we'll use libraries you this data, we'll do the same thing. We'll read this data. We will sample into it. We'll do descriptive statistics of it. You do null values. Are there null values? There is something called Pandas profiling. It will tell you. It will generate some report. I won't go into that at this moment. Visualize it. So let's do a simple visualization of the data this is x this is y looking at this data what do you conclude do you think a line is a good hypothesis for this data a linear relationship so we can use a regression which says there's a linear relationship in the data right so let's do that by the way we improve this visualization a little bit do you notice that something is lacking in this visualization you can't tell which the labels are missing the title is missing so one of the things you should do and here we are learning a little bit more of matplotlib is you can actually prettify graphs. Graphs in your data science notebook need not be ugly. So let me show you between this one, which is minimalistic, and this one, I hope you would agree that this is a little bit aesthetically more pleasing. If nothing else is informative, it says this is the x-axis, this is the y-axis, isn't it? And it's a scatterplot between the data x and y, right? And I've added a little bit of transparency and so forth. So you can do all of that. You would also notice that this title has been formatted. Do you notice that these titles are formatted? And this right, these arrows are there. This comes from something called latex or latex right latex is a mathematical language most of the papers in data science are written in latex right they are not written in microsoft word or google doc they are actually written in latex right so this might be a great opportunity for you to start picking up the elementary aspects of LaTeX. So please take this as a homework. Pick up the elementary aspects of LaTeX. If you don't pick up, no harm done. But if you do pick up, it will be good for you. So as you can see, guys, paradoxically, when you enter a field, there's a lot of tools you have to pick up in the beginning. We are not learning as many concepts as we are being told to learn tools. Today, I said, go make sure. Numpy, you're strong that last time I said. This time I'm saying, make sure you're good with pandas. Make sure you're good with your Python and so on and so forth. Scikit-learn and make sure you're good with matplotlib. Now I'm saying it is a good idea to pick up LaTeX. LaTeX is optional. If you pick it up, it's fine. Learn only the elementary aspects of LaTeX. Nothing complicated. Don't go into the esoteric aspects. We are not going to make tables in LaTeX. We're just going to write some elementary mathematics in LaTeX. So here we go. This is it. Now, we can do it. There's another visualization library in Python. So there's actually so many visualization libraries that after a little while, your head will start spinning. Now, there's one called Seaborn, and it claims to be better than Matplotlib. So, well, here is the same thing done in Seaborn, and I'll let you decide whether you consider it better or not. Then there is yet another library called Altair, which you can run when you get this code, but let's go to the real one. And then there's Plotly, and then there's VegaLite. I mean, there's no end to it, right? In the Python world, people have gone crazy. Each library claiming is better than all others, right? So if you have spare time, do take the initiative to create one more library, and definitely claim is better than all the rest. I'm kidding. So one of the things you do is in data science, and definitely claim is better than all the rest. I'm kidding. So one of the things you do is in data science, see, when you train your model, how would you know that your model is valid? So go back to the example of children. If you take children to the meadow and you show them a cow and a duck and you show a few cows and ducks how would you know that your kids have not just memorized the the cows and ducks photographically what you have to do is show a cow or a duck the child has not seen the children have not seen and ask what is that because if they can identify that correctly there is evidence of generalization isn't it so you have to look at the error rate by showing them samples examples that they have not seen and then asking what is it likewise in regression you have to show them a data, certain temperature values that your model has not seen. You're saying, make a prediction. How much ice cream will I sell on this day at this temperature? Isn't it? So for that, what you do now, how do you do that? How do you get new data? Well, what you do is you take your data set and then you chop it into two parts right one part you hide under your pillow and the other part you give it to the model and say this is all there is go learn from it and you never ever let the model know that you have hidden some data under your pillow are we together right uh somewhat similar to you guys are all software engineers. You know what happens when the product team says you have to build a product. When do you think you can deliver it by? So they will always say that, oh, we absolutely want it by June. Customers will otherwise be very unhappy. It must somehow get done by June. What can you do by June? So then there's negotiation and then you'll agree to something let's say by december say and then whatever you agree to you will try very hard to deliver by june what they didn't tell you is there was a buffer customer was actually expecting it in october right so anyway i'm joking i'm joking so so the whole idea is that you don't show all the data. You should hide some under the pillow. Actually, this is not a good example to bring up. It's an irrelevant example. But anyway, I don't know why it came to my mind. You hide some data under the pillow. So that's what we do. You split the data set into the test and train set. A test you hide under the pillow for the time being. under the pillar for the time being once the model is trained then you ask it to make predictions on the test data are we together and see how good a prediction is making on the test data now the proof of the pudding is the error rate on the test data because if it is giving perfect predictions on the learning data it might have just memorized it internally. You don't know. That's a general concept. That's a general idea. So what you do is you break the data into test and train. Now, let's think about it. What happens is that, remember, we did all of those things by hand. In real life, you don't do that. All you do is you take your train data, you create class called instantiate a class called linear regression the names are very descriptive in scikit-learn linear regression can you guess what it does this mark it's the name of a model what do you think this model will do yeah it will make a linear relationship through your data. Try to find a linear relationship. In two dimensions, it will be a line. In higher dimensions, it will be a hyperplane. In three dimensions, it will be a plane. In higher dimensions, it will be a hyperplane. So it will find a linear relationship. So you instantiate that and you ask the model to fit to the data, training data, remember, only the training data. When you do that, the model will learn. Are we together? Just one line of code and the model has learned. Then you can ask the model to tell you the intercept and the coefficient, beta naught and beta on the slope. And it tells you that. Do you see that it is telling you that? That this is the intercept? This is the slope, minus 0.1. Does it look reasonable? Let's look at the data. Would you say that the slope is about minus 0.1? Just a little bit below zero. Sorry, intercept is a little bit below zero. Does this look reasonable, guys? Yeah. And how much is the unit increase for one? Let's say from zero to two, if you go two units, it goes to about 20. So the slope should be roughly speaking around 10, isn't it? Let's see what it comes up with. Slope is 8.3. Does that look reasonable? Right, yeah, this is it. So you see that in real life, one line of code built your model, and then you can ask your model to tell you what did you learn, and you can use the model to make predictions. Predictions are often remember I said predictions wear a hat. Why hat is this? And you can now the rest of the part of this I won't cover today because now the question is, well, you made prediction, but how good are the predictions? What are the ways to tell if the predictions are good? Right. That brings us to the topic of residual analysis and this concept is called the coefficients of determination and so on and so forth. So these are the things we will deal with later. But one of the things we can do is we can draw the prediction over the data. We can just draw the line that this model is predicting. You look at this line. Do you think it agrees with the data? Would you say this line agrees with the data? It's a good model for the data. It's pretty faithful to the data. So this is good. We are going to release this notebook, but when you review this notebook, ignore the residual analysis and the model quality, the coefficient of determination part, right? Because those are the things we'll cover on Thursday. When we do it on Thursday, it will make sense. Review the rest of it, right? But do you see that actually in real life, using this powerful libraries of today, all you have to do is three lines. Split the data into test and train, build the model, and then predict from the model. y hat is the prediction from the model. Just three, four lines of code. So this part, we will actually hand do next time, because I'll give you fill in the blank for another problem and you will do it on your own. But for now, just we'll release this notebook. It's called Univariate 1. Please go and familiarize yourself with this. I tried to study this code. Now we have about 20 minutes, which I'll keep for QA. Any questions? So guys, did you just, was this useful today or was it overwhelming? It was good. It was good. Good. You guys followed most of it, isn't it? Review it. And if you don't follow Speak Up, guys, I would be happy to repeat over as many times as needed. I'll repeat it. So one thing which is unfortunate when people are remote is I can't look into your faces and tell whether you're grasping or not. Otherwise, I can be look into your faces and tell whether you're grasping or not otherwise i can i can be looking into your face i have a pretty good clue whether you're getting it otherwise you have a glazed look on your face so we'll stop here let's talk