 yeah so that is for numpy now likewise for we have been using matplotlib plotting so i will just take this example data set By now, you're all familiar with the plotting, but I wanted to tell you some stylistic aspects of it. So suppose you have a data. If you read the data, data frame, data set 2, you're all familiar with. The simplest one-liner that will plot the data is this. And when you look at it, it's a pretty respectable plot. It looks pretty good, right? Now, obviously, part of the reason it looks so good is because we have done a lot of stylistic things in our base notebook, support vectors common. We have themed it a little bit, but leaving that aside, it's still one liners will always produce a nice plot for you. You can do it directly by saying plt matplotlib this, or there is a convenience actually. Panda's data frame now supports matplotlib directly. So instead of doing it like that, you could actually do it in a perhaps more intuitive manner. You can say dataframe.plot.scatter. It is exactly the same thing as doing plt.scatter df.plot is the matplotlib plot then you can call the same functions except that you don't have to do df.x df.y you can directly give the x and y here and you will get the same plot and you will get the same plot, right? Or you could have done it even more, dataframe.plot, you can say kind scatter. You could have said line plot, scatter plot, whatever it is, and it would still produce the same results. You could get histogram of the variables if you do dataform.hist, very convenient method. It'll automatically draw the matplotlib plots for all the variables in the data frame all the features in the data frame right so that is that now or you could call it directly on the matplotlib plt.hist it will draw it out slightly differently now pandas has a convenience feature. A lot of people feel that matplotlib is not the best, it's the default, but it's not good. I find it good enough, but a lot of people feel that, no, it would be better if it was different. So I haven't shown you the bokeh plot here because this thing runs on my Unix Linux server, but here it's on a Windows laptop and this environment here is not properly set up. So I have no idea whether this will work properly or not. I wouldn't run it, but you can run it on your own and see that a bouquet a more pretty plot comes up some people consider it pretty i'm not necessarily so enamored of it but if you find it pretty then it is and there are multiple back ends you can use a bouquet as a back end you can use plotly as a back end and all you have to do to change the back end is you have to tell which back end it is and when you do that it shows up here it won't show up because i didn't do it but see this is but we'll focus on mat plotlet see when you look at a plot like this from the data visualization perspective there are many objectionable problems to it first is it doesn't have a title right and unless you're careful it won't even have legends and so on and so forth like look at this plot when you do just plt dot scatter do you know what the x-axis is and what the y-axis is it's not you don't know what is plotted against what and what exactly are you looking at so the very first thing is in matplotlib you would like to add some basic information so relative to that is this better you have x-axis you have y-axis and you have the title now here i should mention that when you use matplotlib you have the convenience it has lovely integration with LaTeX so you can use LaTeX syntax to produce much more professional looking graph. Do you see that the labels are much more professional publication quality labels right and so so this is the thing, what did you do, you entered some logic on by the way, if you don't know logic, it doesn't matter, just leave it as x, that's all x and y. And that is enough. We can also modify things. You can use change the size, you can add lots of extra things. So for example, here, these dots look rather blue little dots, you may not be happy when you say use circles, use this color, which is a salmon color, put in some transparency, so on and so forth. So the same graph that you see here with a little bit more styling now begins to look like this. Is it beginning to look a little bit better, guys? Hopefully, right? It looks a little bit, I don't know, beauty is in the eyes of the beholder. Some people may disagree, but to me it looked prettier. So I did that, but then you can go further. You can add annotations to a graph. So what are annotations? Like, look at this. You notice that I've said at this point, this is happening at this point, right, points of curvature in this graph. So you can put explanations or embed explanations directly into the graph. It is very easy to do that. You have to use the annotation function. And with this annotation and notice that I also made this bold. This seems to be a sine wave function, blah. Right? Because you can embed LaTeX in your graph, it also means that you can use the Greek alphabet, which typically we use for concepts. How would we create subplots? Sometimes you don't want a plot, you need a lot of plot side by side so subplots is actually not hard or tough you you need again so by the way all of these things like subtitle title subplot dot adjust spacing between the horizontal and vertical i've commented it so that it's self-descriptive what we are talking about. And then I go to the same datasets that you're used to, dataset two, three, four, five, and we plot it out. Do you notice that now what I'm doing? I am saying that we are going to create a subplot in which at this particular location, it's a two by two grid, but at the first second third fourth whatever location go and draw a plot the plots are very simple but this is the way subplotting works in matplotlib you basically say uh p by q like three by. So now how many slots are possible? Six. And you go left to right, top to bottom. The way you would read a sentence. So the index would be, the top left would be one, bottom right would be six. So that's how it is. You would lay it out. And so you get a plot that looks hopefully pretty clear now you notice that here what i have done for clarity i have adjusted these two features horizontal space and vertical space right horizontal and vertical spacing between the plots it's one of those tricks that people forget and sometimes their little plots are overlapping with each other the subplots and it looks ugly so whenever you do subplotting pay attention to the aesthetics it's important to do that right and likewise if you want to do histograms of all the data sets as subplot it's the same thing the code is not very different except that i'm not plotting it. I'm plotting the histograms. Remember, hist is the function to do it. Right? So this is matplotlib. That's a basic tutorial. What I would suggest is keep this notebook as a reference. Most of the time, if you, most of the plots that you will make in your data science notebooks, you can refer to this code and you can refer to this notebook to quickly draw from it and know how to do it. Are we together? That's why I created that and people have found it sort of useful as a reference to quickly dip into and get what they want for their notebooks any questions here from matt blotlib folks do you all feel pretty comfortable at plotlib now generally but you have been using it for some time you mentioned the library at the top that did all the styling you know where is that oh this this support vectors are common uh i i did that i put it on the website for you of course portal here it is supportvectors.com it is the thing see what happens is that you don't need to put it in a python node a jupyter notebook Typically what you do is in your specific Jupyter machine, you go into the Jupyter directory and you look for custom.css and you actually put these things there so that it's automatically applicable to all of your notebooks. But because I'm sharing this with you, obviously I can't go and make changes to your custom.js. And it's a little hard to know where it is and so forth for some people. So I instead put it in this. And we invoke this. And also, it contains the standard imports. Because otherwise, your whole code starts with a long list of imports. It just looks messy. So it is separated out. So that was that. Now, a little bit about object-oriented Python. This is a topic. I've just given an example. What does object-oriented do? It says that you can have classes, bundles of functionality, of properties and methods that it can do. So for example, descriptive statistics is mean, median, whatever, min, max, standard deviation, etc. It's a logical group of things to compute. So one thing you can do, you can create a class called descriptive statistics. And do you notice that you have added all of these methods inside the class? Inside means you see the indentation. This is the class declaration. Now most of you are familiar with object-oriented programming. So in some language or the other, C++ or Java or whatever other language, C Sharp. So this is illustrative. You can just map your notion of classes and objects, see how it is done. In Python, there's something unique. You notice that every method must take self as argument, first argument. This is what tells that it is a member method of a class that's how you know it right so that is how you do this and so then if you want to use it for descriptive statistics you notice how easy it is you can go through for loop over multiple data sets and just say call summarize one liner you instantiate it and then you use it, makes it very, very convenient to be able to use this functionality and when you use linear regression and all of these things you have been using, you have actually been using classes that have been created by people. created by people. Next, I'll look at pipeline. So, what is the init? The underscore init? Oh, that is the constructor. Remember, you have to be plus plus etc. Very good question. Is the constructor. Okay. And how do we pass the self like if I'm calling them methods, how we are passing this, what are the self? Self will automatically be passed. You don't pass it. Okay, it's implicit. Okay. In fact, the presence of the self says that this is a member, it is a part of the class, part of some class. So it's a very Pythonic syntax, right? You may say why was that needed? If it is a class, it is implicit method, it is implicit. But in Python, for whatever historic reasons, it has always been there that self is passed in the first argument. And if you want to pass the other class itself then the other class so very much like c plus plus uh sanjay there is no difference you just syntax is slightly different ideas are exactly the same also also what i've done is also it's very hard to break the functions onto yeah i have a way by which you can stitch different functions and bring them as members of the class itself it requires some special code in the beginning okay but then you can separate it out otherwise this gets very messy to figure out but you have to have a right order of calling in then each of those things can be separated by themselves yeah i'd love to see that one of these days hold a session and i'll attend your session just use support vectors zoom and you can i would love to see it then other students also may want to see that we'd love to see that also may want to see that. We'd love to see that. So that brings us to a nice topic guys, share your knowledge. If you have something like Sachin has been, he has been very proactive actually in sharing the things he knows. Please do that. I mean the learning need not be from me, we should be sort of a web of learning from each other, from all of us. And I have a lot to learn from you guys. I do, in fact, every time I run the workshop, there is something new to learn. So we did NumPy. Now the next thing I'll do is actually very useful. It is called the concept of a pipeline. Let me run this. Yeah. What is a pipeline? You'll see it is best explained by showing you how it works so go back to your data set two which was the sine wave data set sine wave light what happens is there suppose you load the data the usual things load separate it into predictor so i've not explained anymore because by now no explanations are needed you have become pretty good at doing all of these things and then you scatter plot it so here is the scatter plot of the data so now now let us think about it we want to build uh let's say polynomial regression model first thing we notice that the y the x-axis is on a completely different scale. It is not zero centered, right? You may want to scale the Y axis also, but I just took only X as an example. X is not standardized. Why should we standardize the data? For all the reasons we talked about. They become comparable. Features become comparable, number one. And the second reason is gradient descent is much better because otherwise what happens is that that you know this contour that bowl the loss function uh bowl kind of a shape it becomes really pinched and it begins to look more like a ravine than a than a bowl right in one direction it gets, it gets very, very squished. In other words, the contour plot, when you draw it, it has huge eccentricities. In one direction, it will be very long and stretched out. In other direction, it will be just tiny little bit. You can imagine because the variables are not properly scaled. So scaling the variable is a de-rigger. You must always do it. So one exercise I will do now, I haven't been scaling the variables because I hadn't introduced this topic in the labs. I want you to go back in all the labs that we have done and even the solutions that I've posted, go and add the standardization step, scaling step, right? Go back and add it. It'll'll be something fun a small exercise to do and repeat so you can do that so this is a standardization of the data so how do we standardize data it's very simple yes this particular notebook that they have loaded yes probably is not working on the photo so if anybody has downloaded i got a json error too trying to load the pipeline notebook really make sure that we're not struggling with it together okay i'll look into that and fix it uh kyle please remind me tonight right so uh whatever whatever the little issues is i'll fix it see you guys uh at this moment i'm working on the laptop windows laptop and i am not an expert with videos most likely it's a small oh it's a corruption of the file itself i'll upload the new version of the file i will do that okay so i apologize for that so just watch and go along for the time being. I'll upload the latest version. I wish I could reproduce it, but on this Windows I can't. Envarmage is not set up on this. I have to get to my Linux server. OK, so the standard scalar by now, as you would imagine, you bring in a class called standard scalar. Now there are multiple scalars. There is the minmax scalar. Minmax scalar does what? it will subtract the mean from a value and divide it by the range of values so then everything goes from 0 to 1 right that is a min max scalar then standard scalar does what who would like to tell me what does standard scalar do the minus mu by sigma yeah minus mu by sigma excellent and so what happens is when you do it to any feature then the mean of the feature is zero and the and the standard deviation or the variance of the feature is one exactly that will standardize it the shape doesn't change the look of it doesn't change the magnitudes will look differently right the shape becomes like this? The shape doesn't change, the look of it doesn't change, the magnitudes will look differently. The shape will remain the same. Remember none of this scaling will alter the fundamental shape. In other words, if it is symmetric, it will remain symmetric. If it is skewed, it will remain skewed. But what it will do is, it will just shift it and standardize it make the sizes better for example we go back to the example of our elephant the elephant's weight isn't if you measure it in grams it will spawn it will just spread over a vast number of grams some elephant may be like 600 kilos and some elephant may be like, I don't know, 600 pounds. Some elephant may be like 1500 pounds or some huge number, right? So there's a huge variation, but when you standardize it, most of the weights will be between minus three and three because three standard deviations after that, everything is outlier right so the values get standardization most values get between three and minus three in fact most values remain between two and minus two right they are all zero centered that is what it does and we'll see that so so the first order of businesses we should scale it how do we scale it very easy we first scale the x data and then we scale the y data the i mean sorry the training data and then we scale the test data remember we are doing it only for x so then this is where you would do like log transformation and we will come to that too but not here at this moment we're just learning about standardization right so here is a use case for standardizing. So suppose I standardize. See, has the shape of the sine wave changed? No. This looks exactly identical to this one, except that if you look carefully at the bottom, you see that the x scale has changed, isn't it? It is zero centered, and it is not so obvious, but the variance is one. If you compute the variance, you'll see that it is one. So now that is good. That's what I mentioned. Not so obvious in the picture, but the variance is one. So this is how you scale a feature, guys. Standard scalar, min-max scalar, there is robust scalar and so forth. And in a separate notebook, I will show you the effect of different scaling when you're comparing data. So subtle differences are there, but standard scalar is pretty much the workhorse of machine learning, partly because like having data centered around zero to like between three to minus three essentially leads to very good behavior during the learning process now then the next thing yes one question so during standardization are you randomizing it also no no no this is very not right whenever you do the data in the beginning when you load the data it is a very good idea to randomize it and in our case we do it implicitly because what we do is let's look at the splitting of the data somewhere we split the data right do you see that we have a random state okay so we have randomized the data at that. Thank you. But it's always a good idea to shuffle the data just to be very safe. I haven't done that. So there are little little things that I'm not doing, which in a production in a very formal notebook, you will sort of tend to do, but we will go do you notice that every lab I introduce more machinery, more things to do. Right? In the beginning, first lab, we started with just a basic linear regression. I introduced more machinery, more things to do, right? In the beginning, first lab, we started with just a basic linear regression and everything goes, forget about everything else. Now we are bringing in more and more things. So today it's the standard scaler or the scaling. The next is we need to add, what was our intention? We know that for dataset two, polynomial regression is the thing, right? It worked. So with that in mind, let us try polynomial regression. Now, we can't do it on any data. We have to do it on the scale data. We have scaled the data. So we have to do polynomial expansion on the scale data. So it is this step. And then finally, you do this and you go through the rest of the analysis, you get the coefficients and so forth, right? See one good thing when you have scaled the data is one advantage, like amongst the many advantages. You can see the importance of the various features. So for example, how much does, so this is what, beta naught? Zero is beta naught intercept. You can tell from this picture that the intercept is zero, because if you look at it, zero, zero, it's meeting there. Right, sort of any kind of a linear behavior that you plot. So it makes sense. What about the first coefficient? It is minus two. The second coefficient is very small, and the third coefficient is 0.8 approximately right does it agree with your intuition guys mathematically what is the Taylor expansion of sine wave if anyone of you remember sine function? It is x minus x cubed over 3 factorial, isn't it? So what does it mean? Intercept is 0, right? 0. Then the first term, the x, the coefficient of x, which happens to be this, what is the coefficient of x squared there is no x squared term isn't it so do you see that this is very small not exactly zero but very small and then you do expect a substantial coefficient for the x cubed term which is here do you see that right and you do know that the coefficient of the x cubed term is much smaller than the coefficient of the x term. And it shows through, right? It is approximately one third or something like that of the x term. So it all makes sense. That's the beauty of it. But the important thing to know is that once your data is scaled and you look at the coefficients, you see the relative importance of each of the coefficients. So looking at this, which is the most dominant coefficient? X, the coefficient of X. And which is the second most dominant coefficient? Yeah. So X and X cube are the dominant features, isn't it? You say that the most important features is first x, then x cube. And of course, it completely agrees with your mathematical intuition because sine x is x minus x cube over 3 and so on and so forth. And that's the beauty of standardizing the data here. So now you did all of that and you do the rest of the exercise which I've mentioned only briefly, it is no wonder that when you look at it, you find the R square of 97% 96.7. Right pretty good R square, you know that you have hit the nail on the head, maybe you could have added an x5 term or something like that, but we'll leave it as that. You look at the residuals, there is no pattern you're done, but the one thing is that it was a long journey to take the data and go from standardization and this tedious right you couldn't just do that so that is where the value of pipelines come in it turns out that in general this is just the beginning when you do a data science experiment you'll get data that is noisy there will be you'll have to deal with the null values you know drop the null values or whatever it is or interpolate the value inject something so that's called null value treatment right you may have to do some transformations like such as you just mentioned about the log transfer the power transform or something of some of the features then you may have to standardize the things then you may have to do polynomial expansion so you may be having successive things to do and all of it then becomes tedious from this do this then do this then do that what if magically this, do this, then do this, then do that. What if magically we could pretend as though the entire pipeline is like one algorithm and all we had to do is call fit transform. Remember linear regression, what do you do? You say .fit, isn't it? .fit, .predict. What if we could do that? And that's the beauty of a pipeline. It helps you do that. See, look here, please pay attention to this code maybe i'll expand it a little bit more you make a pipeline what are the three things we did we scaled it we did polynomial expansion and we did linear regression isn't it did we do all these three things guys yeah and so i have just listed them comma separated list and i say make pipeline and that pipeline now becomes sort of like a model that you can train just like in a model you would say you know model.fit now you can say that and i call this literally regressor regressor.fit do you see that thereafter once you create the pipeline nothing else is required everything automatically happens under the covers do you find this convenient easy so that is the power of pipelines it makes things very convenient and possible so from this you can have y hat and so so on and so forth y test hat and so forth and you can compute the r square. So look at this, we achieved this in just one, two, three, four lines of code, what previously, we had to go through so many lines of code from here all the way down to achieve, right? So that is the beauty of pipelines, use pipeline. So I have one question in couple of the, the lab, we have seen that when we increase the N the value of R square is keep going up. Right? So, so let's say I tried for a couple of them. So how long it will increase at some and when do we stop right so we got 0.95 and then I increase let's say then maybe it's 0.9 very insignificant no no no no so two things first of all if you increase the number of features R square will always go up which is why people often you know they get tempted to go on increasing the number of features or the number of polynomials, making models more. The more complex you make the model, the more, the better your training R square is. But when you look at the test R square, it's a different story. Or when you look at the test RSS, the sum squared residual, it will be a different story, right? You will notice that that doesn't improve so much. In fact, it starts getting worse. There is one more thing I wanted to mention. See what happens, guys, that we keep saying r squared, r squared. It's okay. It's one sort of a number we started out with. Gradually, as you make progress, there are many situations. In fact, we'll encounter one right now where you don't have the notion of a null hypothesis. It's hard to craft the notion of a null hypothesis. So what you do is you work directly with either RSS or mean squared error, MSE, right? Total sum squared error or mean squared error. And you look at which algorithm gives me less mean squared error right so anyway that is just an aside but in your question you have to look at these values mean squared error of on the test data set not the training remember training data set the more complex you make the better the better the r squared the mean squared error will become. But that is misleading. It is the test data that speaks the truth. Okay, thank you. Next slide, Avash. So when you're in real commercial problems, so standard scalar, polynimal features, all these, what you have there are essentially what was written as part of speak- learn the package that is true do you overwrite these typically you write them yourself to extend to take care of nuances in the data no these are pretty robust see first thing is this the okay okay how should i say see here's the thing scikit-learn is a very useful library Okay, how should I say it? See, here's the thing, Scikit-learn is a very useful library. Increasingly, it's becoming more and more robust, but it is still not 30-year-old library. Like we are not talking the BLAS or LAPAC or LINPAC, right? Which are like Atlas, which are like battle tested. You use it for nuclear simulations, thermonuclear simulations. You use it to do ocean currents. You use it to predict weather. Even today, those libraries are there. And when you do it, you send a probe to the Mars with this. Those are completely battle-tested and hardened libraries. Very, very mature. This is work in progress. It's the best we have. It's rapidly improving. It has a lot of functionality you can use in production literally off the bat right but it also has areas of weakness at this moment and it's like a swiss cheese they're gaps so sometimes for example these are the scalers you may need other kinds of scalers right of your own which are not there so that is it it has a little bit of everything but it's not in many areas it's not very deep so the way I was thinking in my mind that's the connection I'm looking for so say standard scale you're not happy with it yeah you would still be able to work with the rest of what is provided as main pipeline yes by improving upon standard scaler and yes yes yeah in fact that is exactly what i'm going to say in the next sentence see i say observe the sheer brevity of the syntax which i hope i've convinced you of it also makes for easy and interchangeability. For example, you could substitute the standard scalar with min-max scalar, right? Change it and see its effect, if any, on the results. So this is a work for you. Now, suppose you have written your own scalar. All you have to do, the whole of the code can remain the same. You just plug in your own scalar here. That's it, right? right likewise you may not like linear regression you just learned about two more regression techniques what are those rich and lasso or elastic net throw elastic net also into the bag but apply that guys see what happens what happens when you apply rich what happens when you apply lasso to this problem no does it change the value so how is it like because we learned that under the context of regularization so you might say rich here so is it like a no just replace the word linear regression with rich here suppose instead of linear regression i put the word rich that's it internally it will do whatever is required yes the parameters and that is why now you know one of the things you notice that in machine learning most people who claim they know machine learning only actually know the psychic or some library they don't really know machine learning why because these libraries do so much lifting under the covers heavy lifting under the covers right but people don't really know machine learning. Why? Because these libraries do so much lifting under the covers, heavy lifting under the covers, right? But people don't ever learn the theory. What is the advantage of learning the theory? Because sooner or later will comes a day when you have to write something from scratch. Are we together? Especially when you are doing deep neural networks. Sooner or later comes a day where you have to do things from scratch, right? Because you're in uncharted territory. And that is when you need to know the theory of doing it. Otherwise, you live in the cookie cutter land. If the cookie cutter tools can solve your problem well and good, if they can't, you're completely fixated. You can't make progress anymore. That's the point why I make it a point to teach you about the theory and the labs. But the lab unit is very simple. It is, change the word linear regression with rich, lo and behold, it got regularized, right? How easy can it be? Yeah, so I'll leave that, those as exercises for you. Now you might have a sneaky suspicion that yes, when you do all this standardization and polynomial, et cetera, at the end of it, are my results any different? What if I screw up? So you can verify that actually it doesn't make a difference. It is just more efficient to scale and to do things. Because do you notice that your prediction line still does very well over the data? It continues to do very well. you notice that your prediction line still does very well over the data right it continues to do very well so this is it this is the concept of pipelines uh i noticed that we have been going on for an hour would you guys like to take a 10 minutes break before we do the next thing all right folks we will take a 10-minute break in my watch it is 10 past four pacific standard time okay we will all right so we learned about creating a data pipelines and that is a very efficient way to do a lot of the transformations that go into a data science experiment. Now I'm going to change topics and talk about a different question. And I believe that you will find it useful. There is one more topic about data manipulation that I'm not covering. It is called the topic of tidy data. Tidy data is a beautiful topic, but I think today we might be reaching a saturation point of the amount of information we can absorb. So I will keep it for next time. Remind me that I do cover tidy data. In any case, I put the notebook online. If you read it, it's more or less self-explanatory. Raja Ayyanar?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre?nilre For example, if you look at the weather, this is a temperature data for cities. So the columns are cities, right? The rows are on a given day, what was the temperature in those cities like this. It turns out that while it is illustrative, it is perfectly good data. We don't consider it to be what is called tidy data. The data needs to be in a slightly different form and if it is in that form, it is very useful. We'll talk about it next time. This tidy data notebook also illustrates how do you join different disparate bits of data, you know, one in one file, one in another file, how do you properly join it? So that topic, again notebook I have uploaded but we will do that. One of you said I need to upload the pipeline. Let me do that before I forget right now. And in case it doesn't work, it means that what I have on the laptop is corrupted. Then I'll go home and look from the Linux machine. I will upload it properly. Pipeline needs updating. Okay. And let's try a look. Where is this running from upload a file? On this machine, it is where? Downloads. Compreg lab for. Oh, goodness. I don't even know. Okay. It is where downloads, Compreq Lab 4. Oh goodness, I don't even know. Okay, comprehensive. Okay, one of you, could you please confirm if the problem is solved or not? I will try it right now. No, yeah, just pause a minute, pause a minute. Force download, yes, this is correct. Kate, you can try it now. Please see if it works better. Okay, will do. All right, so next topic that we'll go to is nonlinear regression. It's one of my favorite topics. So guys, regression regression any one topic in machine learning is a vast ocean of knowledge there are many it's a treasure trove of knowledge and techniques and things there are certain things we don't have time to cover for example generalized linear regression is another broad topic i will bring it in at some point, but I'll bring it up in the extras. It is working now. Nice. Thank you. So, but today I will do something that is a cute drill that people don't mention often, and it's sort of a step in the direction of what I'm going to do later on with generalized linear regression. This topic is called nonlinear least squares. So I'll tell you a story. This is a very interesting story, and I've given a link to it. What happened is that there's a link to it that you can go to, to learn more about the history of it. And actually, this is one of the links. I read it in the Great Men of Mathematics book, It's a, I read it in the, it's a great men of mathematics book. Very old book, I think by Bell or something. So long ago, here's the thing. There were these mathematician brothers called the Bernoulli brothers. There was the Johan Bernoulli, I believe was one of them. And yeah, Johan. And then his brother was Jacob Bernoulli, if I got that right. And to make matters worse, there was a nephew who was also a Bernoulli. So when most people say Bernoulli's equation or Bernoulli is that, it is hard to tell which of the Bernoullis you're talking about, because each of them were great mathematicians in their own right. Now, there was quite a rivalry between Johann and Jacob, between the two brothers, and they would throw mathematical puzzles at each other. So there was one puzzle they threw, which is the famous puzzle of the Bacchus' crone. I won't go into the detail, but just let's say that given two points, what is the fastest way to go from one into the detail, but just let's say that given two points, what is the fastest way to go from one to the other, assuming that gravitational effect is enforced or something to that effect, it was there. Now the details are here, classic problem. And the challenge was thrown and I think a few months were given to solve it. Now these Bernoullis were obviously French, and people submitted their solutions and tried. Then Newton was asked, did you try that challenge, that grand challenge about this problem? And Newton dismissed his answer, I don't care for these nonsensical European mathematicians. He sort of dismissed the whole thing, but quietly he couldn't resist solving the problem. And he actually solved the problem, which is submitted anonymously to the competition. So when Johann saw that solution, he made this famous code that I, where is my non-linear least square, at the top I have put here, he says that you can, he recognized Newton as the lion by its claws. You could tell this is Newton's work, just from the sheer grandeur of the, or the elegance of the solution, who could tell that nobody but Newton would have done it. Now, what is that as a relevance to this? I use this in a metaphorical, a very loose sense. See what happens, guys? We have been doing linear regression, polynomial regression. And yet, in our heart of hearts, we know when we look at this that what is this? It's a sine wave. So isn't it the most normal thing to ask why are we not trying to fit a sine wave to this equation, to this data? At least we should try it, right? And so what is a sine wave defined by? Amplitude, phase shift from the origin, and the wavelength, the wave number, right? Wavelength, amplitude, and phase. these are the three things and this is going back to your basic if you studied sine wave in school it is if you didn't just take it as a fact that it is true that sine waves are defined by that so why not learn the amplitude the wave number the wavelength and phase, instead of learning beta naught, beta one, beta two, and all of those things, isn't the most intuitive thing to do to learn that, right? So, and this brings me to the fact that whenever you see data, if you are good with your transcendental, well, sine wave is an early transcendental, or one of the transcendental you introduce school kids to when they learn calculus, but there are many transcendental functions. At some point I told you that transcendentals are beautiful, beautiful functions. The physical reality is written quite literally in the alphabet of transcendental functions, right? Or whether we are talking about the electron cloud around the atom around the nucleus right whether you're whether you're talking about the drums the waves of a drum the pebble dropped in a pond the waves are sinusoidal the dampening is if i'm right a a Legendre function, right? So transcendental functions are all around us. Nature is written in the language of transcendental function. Therefore it stands to reason, and of course, engineering and physical sciences, many things. So when you see the data, quite often you should suspect transcendental functions. When you think like that, and you know your transcendentals very well, right, then wouldn't you just want to fit it directly to one of the functions that you know about? Isn't it? So, well, that reminds me of something. You know, as we grow older, we make these silly jokes. Most of our jokes are puns or something silly to that. So first, they used to be, my children tell me that there is actually a concept, it's called dad jokes, right? Which is very close to bad jokes. So in the same way, one of my dead jokes that I make is that instead of being too deep into transcendental meditation, meditate on transcendental functions. It's far more fruitful in this field of machine learning or data science. So we are going to do that. Here we are, just as a recap, a nonlinear least square. So suppose we do a sine wave. It's a recap that the sine wave is written by this equation isn't it so our job is to find the amplitude wave number in the phase shift in this particular data that can we do that let's try that so for that there is a lovely function in scikit called curve fit. You see, curve fit. Fit a function to a curve. Fit a curve to the data. Curve being whatever function that you have in mind. So all you need to do is define that function. Define the function and then you're done. It will do the fitting part for you so here we go I define a sign way, what is a sign given X input X feature X. These are the two three parameters of the model amplitude K and wave and then it will return this, by the way, in case you notice, do you notice that I'm directly using Greek letters in the code? Python allows you that. Python and Julia, et cetera, these modern languages, they allow you to directly have variable names as Greek letters or Greek words, which is not possible, for example, in C or C++ or Java, at least so far, right? So if you come from that world and it surprises you, this is actual code, this is not just a logical syntax, right, this is real code that works. And so you can see- This is only in the notebook, right? Yeah, see, that is a, no, it works in notebook, it works in Visual Studio Code. The one place it doesn't work is by charm. They haven't still caught up to it, but the language fundamentally supports it. As if can you zoom in a little bit. Is that better. Yeah, much better. Yeah. So you notice that I have defined this function. But see, when you use the Greek letters, I don't know, but to me, it just feels more clear, more sort of intuitive and closer to the math. So that is Greek. I suppose mathematicians keep speaking Greek. That's why people often joke, what he's saying is Greek. So anyway, time for another bad joke is the afternoon. So I'll crack one more and hopefully the last one. This is a real story. There's a great physicist named, was a great physicist named Richard P. Feynman. So Feynman one day had to go to a conference, I believe last week or so, whatever, some city. go to a conference i believe last week or whatever some city and he landed at the airport and he asked people that can you take me to this hotel let's say myriad with the conferences and the taxi driver then said well there are two big marriotts in the city or two big whatever hilton or whatever is in the city which one do you mean and now he's completely fox because he hadn't actually read his paperwork carefully or brought any with him so he thought for some time and then he had an inspired idea he asked the taxi driver see there must have been people actually another texture with the taxi office so there must have been people coming here who would be talking things to each other that sounded very greek right where did they want to go? And sure enough, the taxi office knew that it is this hotel that you have to go to because they were talking some Greek stuff, right? And sure enough, he went to the right place. Well, so much for these Greek letters. That book is pretty good. Surely you must be joking. Surely you must be joking. I think the story is from there, isn't yeah very nice book yeah another little thing i want to say see guys when you print something and it comes with high number of decimal points or something you can actually decide how many points you want to keep how many points you want to throw away it shows up in this do you notice that this is only till five decimal places in this matrix the covariance matrix now i'll explain what this is but it is controlled but in this a i did not control it right so it is to god knows any number of points so anyway uh that is that how did you print that five symbol into this thing literally uh ph oh you how did i okay i'll show you how see uh let's say that i'm typing it you say phi write it in latex okay okay and then hit tab see what i do i have fire written here and what you can't see is that I'm going to hit tab on my keyboard. What happens? That's it. It's just beautiful. Most people don't know that you can do that. Whereas I find it very useful. Actually, one of the reasons I love Julia, absolutely love Julia is because you can use code that is so close to the math that you write on paper or the math that you find in the textbooks literally that very very nice and so there is something called compute the confidence interval i haven't explained what confidence interval is so but let me give one minute to explain it i won't go into the technical details but simply put it is this suppose somebody told you that the temperature you you want to decide whether to wear a coat or not while going out it's beginning to get chilly so you ask your friend what okay the temperature is uh well 70. right and then he says plus minus two three degrees right a plus minus five degrees you have a pretty good idea whether to wear a sweater or not or wear a jacket or not but you ask another friend of yours what's the temperature and the person says yeah 70 degrees plus minus 30 degrees do you have any idea what you should wear when going out because 100 degrees you should be in t-shirt and shorts and most probably not even go out right and 40 degrees you better wear overcoat and really be well wrapped and once again you better not go out something like that do you see the point right so the it is not enough to estimate the value it is also necessary to give the band of uncertainty so you can say that with 95 percent confidence uh the temperature is between 70 plus minus five. You're being far more precise, and there is far more information in your statement, actionable information, than to say it is 70 degrees plus minus 30 degrees or plus minus 40 degrees. Would you agree with that statement, guys? Yes, it's also very important for hypothesis testing. Yes, it's also very, so we will keep that aside. But this is sort of the basic intuition. Hypothesis testing we won't cover in this class, Kate. But yeah, it goes to that territory. So the intuition is that. So whenever you make an estimate, it is good to have confidence intervals. Now there is a little bit of a debate within the statistical community. The Bayesians think differently and they sometimes disagree. So we won't talk about that, but confidence interval is broadly considered a useful concept and certainly the authors of this book do their textbook do so here is a bit of computation which don't bother about it this part just ignore it just remember that it is able to compute the confidence interval because it needs to know the t value and so on and so forth and compare it to a student t distribution of which again we i haven't taught you yet but consider this as some form of abracadabra that you can ignore but the end result is you can come out with a confidence interval so the is the confidence interval of a pretty strong would you trust the values is trustable around 1.32? It is reasonably good, right? As opposed to one confidence interval that I received once. Once I ran a Spark job, Spark big data job, and it told me that the value of something is something, I think it said zero or one or something. And then it gave the confidence interval. It said with 95% confidence or some x percent confidence it can assure me that the value is between minus infinity and plus infinity was that useful no you could say that with 100 percent confident yeah absolutely so it actually happened to me i felt really bad because i'd waited for that computation to finish for hours I feel really bad because I'd waited for that computation to finish for hours. So that is it. Anyway, now what is the equivalent? So we build a model, we come up with estimates. How do we know that the model is good? One of the signs of models being good when you do curve fitting is that the parameters, they should not be correlated in any way. When you do repeated experiments, you don't see their values or their estimates are in any way correlated. Shouldn't be. And the other way that you do is you don't have the notion of R squared because what is a null hypothesis? You don't have a clear definition of that. So the best that you can do is you can look at the correlation between the prediction and the reality. You can always do that right it is the are the two correlated and you can plot it so when we plot the correlation between prediction and reality we get this result that the correlation is pretty high isn't it would you agree 98.5 98.6 correlation is very high correlation. So it gives you a hint that you may be in the right direction. When you plot the core, when you plot the prediction versus reality, and notice the y hat on the y axis and y here. Do you notice that they are very close to the 45 degree axis? They go together, they have close correlation. This is rather encouraging. What about residuals? Don't forget residuals. You look at the residuals, what would you say? Is there a distinct pattern or you see homoscedasticity? No pattern. No pattern, right? And you have essentially homoscedasticity of the residuals. And so you move forward, things are looking pretty encouraging. Final step, let's plot the predictions on the data model predictions and when you do this you get this would you say this seems to be a good model the sine wave was a good hypothesis to begin with isn't it and so this is the way you do curve fitting you have a model which is this you can now given the that a k phi you know you can write your equation you can say that y is equal to this a k phi as a sine wave this is example now you may say all right this was too simple let's try the other one the data set three when you look at the data set three what did it look to you like you remember data set three uh at some point let me open the data set three for what it's worth you remember the data set three when we applied polynomial regression to it got us into a bit of trouble what did it do got us into a bit of trouble. What did it do? Oscillation of the peripheries. Oscillations of the peripheries. Runge's phenomena kicked in. So we weren't very happy. If you really look at this thing, it's not very satisfactory fit. Keep this in mind and let's try and see if we have a better luck. If we build a hypothesis that this is a normal curve. Right. So where is that? God, yeah, here it is. So we are going to hypothesize. By the way, the mathematics look scary, right? But actually it's one of the most intuitive and beautiful equations in all of machine learning. When we do this workshop, I don't know when we'll do that these are covered times when we do the workshop the the mathematics of data science then I will take you through a journey such that these sort of equations begin to look uh as familiar as your dining table right at some point uh by the way uh that's a good time to take a poll maybe we should how many of you are interested in that workshop mathematics of data science you guys are and how and have you taken it with me you two of you have taken it with me but a few of you have not yes it's a that is usually well pandemic times before that it used to be houseful people used to really love that. We give geometric intuitive explanation to most of these equations. But anyway, that is right. So I'm going to go through the whole exercise. What are they looking at this? Guys, what are the hyper? What are the parameters of the model amplitude here? And then mu and sigma bell curve is defined by mu and sigma, and then scaling. A is just the scaling of the curve, right? So this equation, mu and sigma, let's do that. Once again, I'm using Greek letters, mu sigma directly into the function, and you play around with it and so forth. And I won't repeat that, what I did. I'll just repeat it the same code, this around curve fit but look at this what do you consider this fit compare this fit versus this look at these two guys this versus this do you think that the non-linear least square is a far, far superior model? And it has essentially hit the nail on its head. Right? Yes. And that is the beauty of this. Then look at the last one. We had a function, we had one more data set, which was data set five, we tried to do with polynomial regression, we away see you notice this, this was how it looked. Satish Penmetsa, Ph.D.: We tried to do it with polynomial but what we noticed is there were some patterns in the residual small pattern they were there distinct patterns in the residual linear doesn't work. Satish Penmetsa, Ph.D.: polynomial sort of works, but oh my goodness scary pattern in the residual. Right? Small patterns. These are very small. Remember, this is 0.015. Right? Very, very small. But still a little bit of a pattern. And you look at this, this by no means looks like a bell curve, normal distribution of errors. So now let's try to deal with it by asking, can we do it using nonlinear least square? so you have this this function well of course the title is a giveaway i shouldn't have done that should have kept the title something different when you look at this plot you ask yourself what does it remind you of right and it turns out it can remind you of many many things the log normal I mean of course in case you're familiar with your transcendentals it can remind you of log normal it can remind you somewhat like Poisson distribution it can remind you of gamma function it can remind you of the beta function and a whole variety of other things there's more Weibull gumperds large class class of transcendentals that I could think maps to this. Are we together? Now where did I pull all the scary names from? Well, it turns out that if you read a book on transcendentals or just look up some table of transcendentals and their shapes, it's very, very useful. And with me, because over the years i'm actually worked with these functions it's it's familiar so now let's go to our non-linear thing here i will make a hypothesis that it's a gamma distribution like function gamma distributions look like this i will go through the same in gamma distribution the hyper parameters of the model are they are a case okay i just use the amplitude you know the scale part i'll leave that i mean actually i shouldn't use the word scale because literally the word scale is for alpha and beta there are two particular predict parameters alpha is called the shape how will the curve look right by changing alpha you end up changing the shape and there's another parameter called beta which is the scale of the shape right so and k i will just consider as an amplitude so you do that you define the function and then you need to know a little bit where is this gamma function coming from it turns out that scikit-learn statistics has a gamma function you can use. So we are saying that it's a, I made it gamma-like, not exactly gamma, but there's yet another amplitude, K added to this. And so it will compute the value and try to fit it to a gamma. When you do this, you see that this code is exactly the same as before. I just defined a new function and passed it here what is the result this looks surprisingly good 2 and 2 right and when you do your y hat and you do y hats correlation with y you that's when you start smiling. Who wouldn't like this? Absolutely perfect correlation, isn't it? Which shows up in this curve, perfect correlation. So when you have a situation like this, two things are evident. The data had no noise, which is true. I told you I forgot to put noise into the data. And you probably hit the nail on the head which is also true because it so happens a secret that i shouldn't tell you but it was that i actually derived the data from a gamma function gamma distribution right it was a gamma so it hit it straight on the head and when you look at the prediction do you notice that the predictions are so good the upper and lower bounds are practically sitting on the line itself in fact directly on the prediction line itself it's a near perfected so i have given the exam homework for you guys do this homework i did it for gamma now replace gamma with log normal what if it is a log normal like distribution try it out with log normal distribution try it with beta distribution and poinsettia like distribution what if it is points are like three things to test out with nonlinear so these are all transcendental functions all you need to do is look up the expressions. Like beta, what do I need to do? Just this expression, isn't it, guys? You just need to feed this expression instead of the other expression into this code and see how well the beta distribution works, see how well the log normal works, and see how well the Poisson works. Are we together? Right? And so with that, I'll end our long journey of understanding the foundations of regression, linear regression at least, guys. What we have done is we have covered a vast territory, more than most books cover. We have looked at simple linear regression we did it with didactic or sort of synthetic data sets because i wanted to teach you certain techniques we have learned power transform we have learned polynomial regression we have learned non-linear least squares we have learned. We have learned scaling of the data. Right? So there is a whole slew of things we learned. Now here is the deal. Going from univariate to multivariate is actually very straightforward. In the coming workshop, the next week, we will do a lot of examples of multivariate regression, right, with lots of datasets. But as a preview, today, Harini will do one analysis. Just watch her analysis. You may not understand all of it, because then next time I'll explain it to the California dataset and more datasets. But my belief is that you'll get most of it, you'll understand most of it, as she explains. But now, most of the data set in regression that we'll encounter will be real world case studies. We'll do a lot of real world case studies and real world data. And you will find that you have built enough muscles now, enough power in your intellectual sort of cerebral horsepower you have to tackle those real world problems with ease, right? With fluency and with ease and at a slightly higher level of maturity than just applying a library to a data and then smiling over the results. You'll be able to do a much more nuanced understanding of what is going on here. Right. So that is the conclusion of this lab. Yes. Yes, those all come into multivariate. Remember we have a multivariate lab waiting. Right. So we'll do that. But today, I would like to end, because it's going to be 5 o'clock now, we'd like to end with two things. One is I'll yield to Kyle, and she will show you a sort of interactive plot I'd built in which you can see the effect of regularization from more data. Remember I said that more data regularizes it? You will see a very direct way I created a small simulation. Unfortunately, on the laptop, it won't run. Kyle will show it to you. Kyle, would you like to take over and show? Hello, Kyle, you're on mute. Look, Kyle, you're on mute. Just a minute, Asif. I'm loading it. Okay. And it's regularization with more data? Yes. And you can run it locally, guys. If you have downloaded that, just do streamlit run main.py. The readme file gives the instructions and you can run it locally on your machine. Please do that okay uh i'm having an issue with my internet connection is there anybody here who's able to have it up so if kyle wants to direct me like i'm running a slideshow. Let me share my screen if that's all right. All you have to do is. Okay let's try kid a kid just streamlit to run. yeah let me just share my screen I got it up already because kyle knew it might be an issue let's see so desktop two excellent okay do you see it uh i do let me bring it to my main screen to share with everyone so please pause for a moment i need to exit. Yes, is it showing on the big screen here? Okay, so yes, there we go. So Kate, if you can just take the sample size very small, increase the degree of the degree of the polynomial. Okay. From one to let's go to 10. What do you see guys when you look at this curve? In fact, would you mind making the sample size down to five? Yeah. Oh, yes. So what do you see, folks? Do you notice that the, what do you see at the end between five and six? Half required to, sorry, to do extra formulation. No, no, six is valid. It is around the periphery. What is that oscillation called? Rung phenomenon. Rung, Rung is phenomenon. And six is valid. It is around the periphery. What is that oscillation called? Runge's phenomenon. And it is huge. It's pretty much like what happens is there is a lot of oscillation in the normal values. But compared to that, this last big swing wipes out everything else, isn't it? You can see the dramatic effect. now we can stay at five and increase the sample i mean sorry degree 10 but increase the sample size to 10 or 15 15 is fine do you notice that more data is doing what the the longest phenomenon those oscillations are suppressed at least the graph looks reasonable do you see that now let's go to let's say sample size 20 or 15 i mean kate just play around with it keep on increasing slightly is it beginning to gradually take shape you still see a lot of pickles. Go more, more, more. Yes. So do you see the regularization kicking in? Isn't it? And you all know what the real answer here is. And it is moving towards that. And we can keep going. So in this code, you guys have the code also, but try to increase from not just 200, but to 1000. And then see what happens. Here, we go only up to 200. But by the time you reach 200, it's already beginning to get to the right place, isn't it? And by the time you go to 1000, it will be. Now let's increase the degree to 20. Do you notice that now 200 doesn't seem sufficient? There are a lot of wiggles in between that have come up. Right? And if you go to just 100 points, like somewhere around 100, yeah, let's see what happens. Ah, Runge's phenomenon has come back. Those oscillations have come back. You see, the more complex the model, the more data point you need to smooth it out. And what happens when we do just 20 data points or somewhere close to that region? Oh, this huge oscillation here. So this is only at the origin and the extremities? It's on the peripheries, yes. On the peripheries? Yes. Why? it all in the extremities it's on the peripheries yes yes why it can also be in sparse regions in between if you have very sparse regions it will be so so the y of it is a bit mathematical i can explain that is it right so that is it guys and so i play around with this i wanted you to see regularization by sample size now today we did regularization by minkowski norms right the penalty terms the constraint surface and so forth so most probably i'll put a interactive graph so that you can see the effect of that but see guys play with this this is how you develop intuition right play with it yes all right so now we will end with one thing we'll jump ahead and get a preview of things we'll do the next time i would like to take a five minute break and then we will start i just need to