 In this case, I have this. All right, guys. So let me then I'll have to share a different screen, which I can do by doing what? One second. Are you seeing this picture? Yes. So what happens is when you don't regularize in the parameter space on the plane, the contour lines are here, the minima is here, the optimal beta is far from the origin. And you say that is fine. Unregularized means you take this as a solution. Regularize means you put a constrained surface, a unit circle of some norm, and then you look for this point. So this is regularized optimal point. This is the unregularized, just a sum squared error for regression, for example, point. And for different algorithms, it could be different. For example, in a deep neural network the the error surface is not so simple it is not just a bowl shape it could be highly non-trivial but nonetheless an unregularized solution would be you take the parameters wherever the wherever the minimum value of the error surface falls uh asif can you explain again like for some reason I'm not able to see your mouse. So yeah, I don't get that intuition. See, alright, let me try this. See my mouse is not visible. Okay, do you see this point here? Okay, yeah, okay, yeah. This is far, wherever it may fall, right? The error surface will achieve a minima somewhere in the parameter space. Yes. fall right the error surface will achieve a minima somewhere in the parameter space oh yes it may form a minima pretty far from the far from the origin far from the origin means large values of the parameters right so this is true not just of linear polynomial regression etc it's true of all um all modeling unless you regular, your error surface may find a minima very, very far from the origin. So origin is here. You don't want that. The whole point of regularization is you want to make sure that your parameters are all small. To do that, you put a budget, you put a unit circle, a circle of certain diameter, and you say that this is my budget. I'm not willing to go out of it right so it's sort of like the lexman raker around the punch button you go here now you find the minima here this is the regularized minima sorry exactly this is the regularized minima this is the unregularized regularized minima this is the unregulated the degree that's all of us it's all this is it this is the theory of regularization in simple terms that we are talking about let's go to the other one now oh where am i combined quiz right so that's why this this is explained what is Ginny we have done through that impurity index. Now look at this curve guys, this is concave. What will it represent? A norm of L is equal to half. Go back and see the norms that I drew. I don't know where I drew the norms, different shapes of the norms in the notes. If you go back and look at the videos you'll see that this is for on the other hand this is the opposite of it this is as bulged out as it possibly can be yeah so I say for this one my only thing was you said Manhattan distance so or is it like a minskoski distance Oh Manhattan distance norm by the way I made a mistake it should not be it should be Minkowski distance norm guys I apologize everywhere I wanted to actually tell you that like because some for other questions also it is Manhattan yes yes no you're right I have to go and fix all this question stuff. I apologize guys. And I was just sitting today morning and I didn't get a time to review this question. So all right, there we go. It should be Minkowski distance norm. This is the diamond obviously it should be. Oh my goodness. Everything that I use. Yeah, yeah. At my goodness. I think I've used it. Yeah, yeah. Yeah. At one place, it is correct, because you said Euclidean distance, L equal to 2. So there it is correct. And here also. Yeah, this is also correct, like Manhattan's. The next circle one. Circle one is correct. One is also correct, yeah. I apologize. But surprisingly, most of you understood what I was trying to say. So most of you got it right. As if like in a few questions, the circle is not visible. Like I couldn't see the picture. Like when the figure basically. That is odd. Yeah, I couldn't see that. So I skipped that question actually. Oh, so that might be a browser issue. Maybe you have to delete your cache or history and then write again. Okay, okay. So what the information gate, of course, it's from the last time I would do. Large hyper parameter values lead to what? They'll over dampen. Large lambda, small budget. So your parameters have become too small bias right yeah you end up with a lot of bias that's true okay there are two correct say right then this question is just pure depth what is the definition of it remember the diamond shift the L is equal to one norm is here absolutely here I was saying as if A and B are same I think A and no there's a plus and there's a minus okay plus lambda minus. Yeah, sorry. So Lasse regularization is superior, of course, this is the no free lunch theorem. No one is superior to the other always. By the way, there was a, I still have to fix it. It should be a plus sign, but most of you got that right. It's a review question. Oh, here I use Minkowski, which of the following is true? These two answers are true I use Minkowski correctly. Which of the following is true? These two answers are true. The Minkowski is L is equal to, the ridge is L is equal to two and the lasso is L is equal to one. Random voice we won't go into. The polynomial regression without ridge array, the model tends to overfit the data. And we can see how, because the parameters become very large and absolute next for polynomial regression without regularization the error surface achieves is absolutely unacceptable far from the origin we just talked about this geometrically in rich or lesser regularization, we do a constraint optimization where we search for the minima. Within the use, so what is the constraint that you cannot, you have to find the minima, minimum error within a unit circle. So you'll typically found it at the edge of the unit circle for linear for linear linear regression but for others it can be different that's how it is okay rich literally the definition of which you use a circle definition you know x y plus x I think they won't go into a bagging, bagging, bagging. Sir, 27, can you check the 27, sir? 27. In a decision tree, one can get a measure of the relative feature importance for each of the features in predicting. The decision tree gives you a feature. This is false, right? Because we said for random forest, we get the more feature importance and not for decision tree. No, no, no, you get for both. because we said for random forest we get the more feature importance and not for decision tree no no you get for both see the one that gives you the biggest uh information gain is your most important feature if you split along whichever feature right so you take the average gain the information gain from each of the features you can get your answer oh related feature yeah okay yes so okay now this is a regularization question elastic net is the use of both l1 and l2 remember this is what we've ended with it takes it tries to take the virtues of both the demerit is now you have two parameters lambda 1 lambda 2 so you have to do more hyperparameter search but if you have the computing power go with elastic points the l is equal to one manhattan distance constraint leads to dimensionality reduction why because you'll puncture the error surface along one of the axes you know the pointy diamond the error surface along one of the axes, the pointy diamond. The L2 regularization leads to dimensionality reduction. Actually it doesn't. Why? Because all it does is it will make certain values small but never zero. You don't tend to touch the error surface along the axes, rarely. LASSO has the additional benefit of dimensionality reduction. Remember that was the point, right? If you punch along the axis, you get dimensionality reduction. R. Vijay Mohanaraman, And this is literally the definition of Manhattan distance Manhattan distances, but taxi drivers distance the shortest taxi drivers distance in Manhattan between two intersections. S. G R Narsimha Rao, Ph.D.: So I said, like in future the definition may change right because in Dubai they are doing the flying taxi. Oh that's right. Yeah. And we can't talk of the taxi. They're testing the flying taxi yeah. Wow very nice. Dubai has always been very progressive. I don't know how they can afford to be so I guess it's the oil dollars right. But this is manhattan now i always say manhattan right yeah yeah that is true yeah manhattan distance is another of euclidean distances no the other way around euclidean distance is one of the minkowski distance regularization is a powerful tool that we should use aggressively in model building and set the hyper parameter lambda to high so this thing i deliberately worded it in such a way that it would be false regularization is a good idea you should always try it but it leaves you a problem you have to you have to tune the hyper parameter and high values of hyper parameter are not necessarily good the next regularization is the process intended for the separation of bias, errors and predictive model. No, it is to reduce variant errors, overfitting. And that is it. That was the quiz today, guys. Very good questions. Very good refresher, Rasef. Thanks. Yeah, welcome guys. So today, just as a reminder, today we are not doing the lab. The attendance becomes too low on the weekend. So it would be, it's an important lab I want to do when all of you are present. So we'll do it on the weekday. We'll do it on Tuesday. Asif, I have a question for discernt trees. Yes. So in the discerntry, we say that it can do linear regression also. Yeah. So how it will create the rules? Like for every change in x, it will create an estimate value of y? Something like that? Or like x between 1 and 2, y is 3? Yes. See, given an input vector, it will predict a y how will it do that that point will belong to some sub region of the decision tree right decision tree breaks the whole feature space into sub regions isn't it so your point will fall into one of the rectangular regions all you have to do is take the y average y value in that rectangular region that is your prediction for that point. Are we getting that? Let me draw it out for you. This may be worth looking into. But did I get your question correctly there? Yeah, we can take a two a two feature example and uh z axis has like uh no no you don't need a z axis for okay for regression yeah we can take the z axis for regression so here's the thing let me just draw it out in two dimensions suppose you have a so i'll just randomly split this regions here uh i don't know, something like this. Take this to be a decision tree. Let us take an arbitrary point. Consider an arbitrary point, let's say X, right? X happens to fall in this region. So what will you do? This region contains a lot of points let us say that it contains points like let's add some points associated with each of these points y i in the region where x i y i belong to this region r subspace yeah yeah so this region belong to the region so all the y i will be same one one value for one region no no that's the point so if you want to make a prediction your y hat right of this point x is equal to just the average 1 over n of the y i belonging to the region k that's it you just take the average of the y's in that region so yes all the predictions in the region will be the same the y-hacks will be the same not the y but the y-hats will all be the same be the same the y-hats will be the same not the y but the y-hats will all be the same so this will each region let me just say each region will have the same y-hat prediction that's it that's how regression works and the way it works is that because of locality there, you expect that the Ys to not be very different. So the average Y to be fairly representative. Okay. Yeah. All right, guys. So with that, if there are no more questions i'll let you guys go tomorrow we have the seminar remember um we're getting that iot professor he's a very interesting person um he'll be giving a talk i hope the talk will also be interesting he's a very uh simple person unassuming and it may be it will be fun i believe to listen to him if you have time do please come to the talk. In the afternoon time permitting, we'll also do the project reviews. People who need help with the projects, welcome. We'll talk a little bit about it. Let's see if I can help you. Monday, we will do boosting. It's the last topic. And Tuesday, we'll do the lab. Wednesday, we'll do the lab Wednesday we'll do the lab and then we'll follow it up Saturday with the quiz so we are coming towards the end of the workshop guys are you are you guys having fun with the workshop so far is it proving useful yes yes yes learning so get ready for the continuation the next thing would be the math and then after that the deep learning boot camp i've designed your way that it is a very logical continuation of where we end here i don't know how many of you have registered i have to go and see but it will help you guys deep learning is the hottest topic these days when will the math classes start? I want to start right away. I don't want to keep a gap. So we are ending on, let's say 22nd, by including the Saturday 25th. Pretty good idea to start. Let's leave 27th aside aside maybe 27th or third maybe third third yeah give a week's gap and let's start on that have we started the uh what is it the back class registration no unfortunately i still have to create the registration form guys uh bear with me i'll maybe do it tonight in an arm set registration form guys, bear with me, I'll maybe do it tonight in an arms setting. So and it will be the same fee structure, but the hours I may squeeze the hours guys I may make it actually I don't know maybe the boot camp I should push by two weeks and keep this to six weeks again. Or are you guys open to coming here like formally three days a week for the math? We can move faster, but the risk is it may be overwhelming. So I think two days a week is reasonable. It's intense enough. Okay. Yeah. Because we need time to digest that information. That's right. So then we'll push the bootcamp by two weeks and do this in six weeks, the regular six weeks. Yeah. As you said, you have, you're changing the book this time, right? Bootcamp. I may push it by a week. No, you're changing the book for the math this time. Yeah, I'm keeping the bootcamp here so that the math class can be squeezed in between. that's yeah in the boot camp yeah so that the math class can be squeezed in between hello i said his question is which book uh are you going to use the same book or a different book no the book is different yeah the book now is i forgot i put it in the slack you guys remember okay yeah i put it on the. It's a unfortunately it's at support vectors. I'm at home. So I don't see it. Can one of you please re post the book title? I think it's like mathematics of machine learning or something. Mathematics of machine learning. Okay, why don't we explore it on the sort of... It's on the slack already. It is on the slack. is it a pinned item on slack it's not pinned but uh could you report pdf link to the amazon both of them okay just for fun take you to the amazon once again mathematics for machine learning yeah you like this book. It is simpler. It's easier than the previous one that we are used to. Wow. Yeah, this book. You see this book? Oh yeah, yeah, yeah yeah this is the book the people yes how important is this book? Well, all books are important. Right. You can get a free PDF of it. Yeah, the free PDF is really good. You can browse through it. See, here's the thing. It's a very elementary book. But the math that you need for machine learning is not deep unfortunately if you don't know it machine learning as a subject feels very intimidating which is the which is the reason that once in your data science career you should do the math of machine learning or you should do it obviously as you notice it starts very very simply It talks about basic concepts. What is a vector? It really starts from there and goes from simultaneous equations that you did in maybe middle school or high school, and it develops matrices and so on and so forth. So in a way, if you know your math, you will breeze through this book in no time. You may even feel it to be too slow. If you't know math fortunately it's not intimidating it is still just the right level that if you have forgotten your math you can if you remember if you have been through high school you'll clear this book easily and it has a lot of colored pictures to you know explain the intuition about vectors and spends a lot of time on basic things amazingly that's all you need machine learning that's all you need and you will get a grinding and all of these things like for example what is a like for example in support vector machines separating let's see if it takes us there yeah look at this classification with the concept of separating hyperplane well I use the notion of a river these people talk like this and this is it and it goes into the kind of math that I would like to go into with you. I haven't gone and explained the SVM. I give you the intuition, but not the math. So you will get the math also here. That's it. You do this book or you do this course. After that, you will never get intimidated by deep learning or machine learning. Any of the research papers, they'll all feel very easy that's the purpose all right guys so i need to run my family is waiting for lunch it was uh i said you were recommending some screen uh energy screen can you oh my goodness yeah i have not sent that to you right why don't we look at it because i it is in my purchase history yes this is the one okay and uh lg is pretty good remember you want a computer monitor you never want a tv to act as a computer monitor yes yes yes so that is important let me put this on the slack i think all students should have one it It's really very good, guys. Once you get used to big monitors, and especially in dual monitor format, it just does wonders to your productivity and concentration because your entire field of view is taken over by the computer monitor. So you don't have distractions. You don't see distractions. You're completely absorbed in what you're doing. How is this different from a TV? See, the way, if I remember, obviously I'm not an expert, but the way they do it is completely different. These are designed for static images. TV is designed for dynamic, moving know, moving images. Computer monitors have round pixels, high resolution picture, very sharp pixels. TV on the other hand has square pixels. The other thing is, it is called 4 is to 4 is to 4. There's a technical term, means the signal that the computer sends is 100% projected onto the screen. In a TV, you don't do that. The TVs are cheap, you know, they're commodity items. So you achieve that by under sampling. There is a lot of under sampling happening. That is why those are called four is to one is to one or four is to two is to one. They all under sample. And because they need to show you motion quickly and with a cheap processor because the tv is a commodity item you know people are very price sensitive so that is one and the other is the color gamut a good monitor will give you a srgb color gamut close to 100 percent even more whereas the tv will give you a color gamut that essentially sucks so that's why you pay more for a computer monitor than for a tv some people are fine with tv also i mean it's not the tv is too bad 4k tvs are pretty good if you don't have the budget to buy a tv see here's the thing a computer monitor is 700 700 can buy you a a giant here it buys you a rather tiny monitor 43 inch monitor 43 is pretty good big for computers but small for TV you don't get all the connections either you don't get HDMI or that's right you don't you don't get my USB so these are just more sophisticated beasts. There's no compromise in these things. Thank you very much. Thank you very much. I'm stopping now.