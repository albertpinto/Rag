 Okay, so this paper is a very interesting paper. It came out recently, as in a couple of months ago. And from a computational perspective for transformers, it addresses one of the big problems that attention and transformers have. So let me give you a little bit of a background. We did transformers, if you remember, in part one of this workshop, we covered the concept of attention, that when you take an input, you feed it in, you get three we covered the concept of attention, that when you take an input, you feed it in, you get three vectors, the Q, K, and V, the query, key, and value vectors. Now, the crucial part of the paper was this attention, which was a softmax function over the key and the query vector. Now at that moment we didn't emphasize it too much but today as we work out the details we'll see that it's a quadratically expensive computation. It's been one of the reasons why training Transformer has been or even during inference in the transformer, it is a little bit, it's been prohibitively expensive. You can't have very long sequences because the computational cost, as you will see, rises quadratically. And because it's quadratically, it slows down quadratically. So it's always been a problem that attention has faced or transformers have faced. And there is a whole body of research on how to not have that problem, how to solve for it. So I will talk about, or maybe I'll highlight in the paper itself, some things that they talk about. in the paper itself some things that they talk about. So this is the point, this is the main point that they scale, that this is it. And so the scaling of this is very, very computationally expensive, right? So this is the main idea that this paper talks about. Now, given the fact that we have this problem, there are many efforts that people have made. Let me slowly go here. If you look at it, there are many, many things that people have done. And I'll talk a little bit about what people have done to do this. They have said that attention, if you have a whole sequence and every word in the sequence need to pay attention to every other word in the sequence, don't do that. Pay attention only to some neighborhood of words. Neighborhood. So that is the concept of the local neighborhoods. Now it's sort of an ad hoc or a priori neighborhoods. Now it's a sort of an ad hoc or a priory belief, you bring it as a prior belief that a word needs to pay attention only to nearby words. That's not always true, as you know. Sometimes, it depends on the way the sentences are constructed, a word may need to pay attention to a word, another word which is quite far off. So that is one limitation if you try to superimpose a belief or prior that words need to pay attention only to neighboring words. Another thing you can do is you can say, every word pays attention to only a few of the other words. So suppose you have n words, and you have, therefore, each word pays attention. So suppose word I, instead of paying attention to each of the W1 to Wn to different degrees and computing all of that, don't do that. Instead, compute this WI, the attention between two words, only a few of them, assume that most of these, most attention values are zero. A word is not paying attention to most of the other words, just to a few of the other words. So if you remember, this is a little bit like the autoencoder sparsity argument. You force it into a sparse representation. Now you can do that, but that is again a belief, a prior that you have imposed upon it. When you impose a prior like this or a belief like this, sometimes it works and sometimes it may not work. Sometimes a word genuinely pays attention to more words than you thought it should pay attention to. And so that becomes a limitation of a sparsity-based approach. The next approach that people do is pooling. They say that pay attention to a small cluster or a group of words. And how do you find those small group of words that a word needs to pay attention to? What you do is you cluster them. You apply a k-means clustering or some sort of a binning. You put all the words into small bins, three or four bins, and you say that one word can pay attention to only one of the bins or one of the clusters but not to all of them right so that is uh another approach it is again a prior in the sense that you say let's try if this works simply put that's what it comes to let's try if this will work and if this will work uh this is great right and so sometimes it works and sometimes it doesn't work now with all of these ad hoc approaches the the thing is is it good to try them of course that's how you have to a lot of ai is experimentation and trying things out and then seeing how often it works worst case situation it won't work at all best case situation it should always work and mostly work and then reality can be somewhere in between sometimes it works sometimes it doesn't now when you take this sort of ad hoc approaches the problem that is there is that there is no rigorous proof there is no mathematical deep basis it's a very empirical a very experimental approach to somehow simplify your computation and not be stuck with the curse of a quadratic computation. That is the thing. With that being there, there are other things people have tried. For example, there is something called locality sensitive hashing, which again is to group the tokens into some form. If you don't think of if you're not doing clustering, you're doing something else. And so you each word pays attention to its own group of words because of the hashing. So people have tried many things. The other thing people have tried is, and as you will see, attention is a matrix. So people invariably say, can we do something akin to singular value decomposition? Like, for example, in the recommender or principal components analysis and so forth. What we do is we do some form of eigenvalue or singular value decomposition. If we can do that decomposition, extract a truncated matrix, diagonal matrix, we get a low rank representation of the data. We talked about some of this in the auto-enquiry example, how, for example, principal component analysis helps you do that. One quick question, just to get a little context here, right? little context here right so what we are right now reading is theoretically the problem statement can become quite complex in terms of the way it scales right and techniques for reducing the complexity but from a practical perspective yes what's your experience in here what i'm sensing is this is all about the length of a sentence and how far do you want to go in terms of attention right that is true how does it break down right now what's the number okay let me let me talk about that actually i'm standing let me sit down i realize that uh you guys are not getting one to see my face clearly because of the lighting. I better sit down. Give me a second. I'm trying to get a sense for at what point is this problem really an industrial-scale problem? And where is it still a toy size? Yes. You see, the biggest problem with transformers, either training the transformers or using it for inference, has been performance. It's a huge problem to such an extent that you can say that with transformers becoming so popular and all of these things, there's a pretty good reason to buy those expensive, you know, video cards, graphic cards. Today, if we can accelerate transformers, it really directly translates into reducing the hardware cost. Everybody loves transformers and everybody hates the fact that they're so, so slow in the sense that they're so computationally intensive. So what happens is you overcome that by moving most of your computation, all of your computation onto the video card. Even on a video card it is it takes a long time to train an industrial strength model. Long time as in a very very long time So for example, the GPT-3 was trained on some 10 or 20,000 node machine, right? Machine, a cluster. And yet it took, from what I understand, a very, very long time to train, right? It is something today, a training a large transformer is something that you and I can't even do. It is so computationally expensive. So obviously, within the world of transformer, there's a tremendous trust to solve that quadratic problem. Think of search, you know, when you do search in ordinary computer programming. You don't want to do bubble sort because we are told bubble sort is quadratic. Right. As you know, you know, it will be very, very slow. But if you can reduce it to n log n, it becomes very manageable. Right. So in the bigger notation, if you remember, this is this is the definition of terrible. Right. Very expensive. Computationally, if you can reduce it down from there to, let us say, order n log n, you say this is good. And of course, if you can reduce it down to order n it is like green isn't it so as you know when you go for a typical these days it's very common when you go for interviews they will the only question they will ask you in search is after they've pressed it you out and how do you do pivoting and this and that in quick sort quick sort then sorry i should have used the word sort not search i thought then the first question they'll ask you is okay these are n log n methods do you know of linear methods something that can sort in linear time and of course you need to know something special about the data to be able to pull that off. So the way in algorithmic world, the scaling goes is order n squared or anything more than that is terrible. Order n log n is reasonably good. Order n is a dream. Now your transformers are worse than order n log n. It is, as we will see, it is order n square times the dimension of the hidden representation. So it is pretty bad actually, very bad. In terms of the sequence length, it is quadratic. Then you multiply that of course, further with the dimensionality of the Hagen representation if you have a big dimensionality typically 64 64 size vector the duration vector now you 64 times the L square is the order of magnitude of your computation in the big orientation this is a very real very very real problem in the transformer world. That's why people are trying to solve it. So I was trying to relate to the context of a problem. So like what I would assume is for, if we're trying to use a transformer today, you're most likely gonna take a built model and then try to extend it. Now, if I'm extending it in the context of let's say an FAQ document in which I want to allow for some kind of a chatbot to work right and if the FAQ is typically consisting of only sentences with around 10-11 words making a sentence is that already complex enough that I shouldn't even attempt it? Or is that still within the range of toy size problems that one can attempt with reasonable compute? See, here's the thing. Once a transformer is trained and you're just using it to your problem, then it is the inferential cost. Now, why is inferential cost relevant? When you deploy, inferential cost is runtime cost inferential cost relevant when you deploy inferential cost is runtime cost you know your application and today we live in a world of web right so most of these inferences we make at web scale let's take one example microsoft they have a tool called the visual studio code right it is pretty much a JavaScript written studio. It's an ID written in JavaScript and it is web hosted. You can just visit it and so on and so forth. Now, every time you type something, it comes up with recommendations of what do you, auto completions. The auto completions that you get are actually being served from the Microsoft server. On the Microsoft server are transformer models that are doing those inferences for you that what is the best auto completion, what are the best recommendations you can make for auto completion of this code. of this code. Now if you think how many people are concurrently using Visual Studio Code at any given moment, you would realize that we are looking at a million people at least are concurrently using that tool. I'm just throwing in a number, plus minus a couple of orders of magnitude, maybe 10 million or something like that, anywhere between a million or 10 million people are simultaneously using that tool at any given moment. So what is happening is that those transformers are giving you inferences coming from millions of users at pretty much the same time. So what happens is obviously you have to pipeline it, queue those requests and then quickly rush through it and do a massively parallel distributed deployments, and so on and so forth. But at the end of it, see, if you could reduce the computational cost, if you could make it very linear, what it would mean is that with lesser amount of hardware, you can sustain more amount of parallel inferences or concurrent inferences. You see that, right? So that's an example of optimization you really need, performance you really need at inference time. I am myself actually at this moment facing a bottleneck. We are in the space of learning and development. I have a differential model that makes some very interesting predictions. For example, it detects, sort of differentiates between the opinionatedness or the informativeness of any content. And because people are all the time are trying to feed it content and ask, should I read it? Is it informative or is it just an opinion and things like that? If you're taking a massive amount of load, we have 70 million users, a massive amount of load. We have 70 million users, a pretty massive amount of load. Now, this is even in the very early alpha stage when all of these users are not actually even touching it. This is still in the lab. Now what happens is that running it, we initially ran it on a box, started out of course as always on a powerful box, but one box then we realize it doesn't work we scaled it out more and more and more and so we have scaled it out in google now and what happens is that the sheer amount of google cost we pay cloud cost we pay is depressing to run the service well it sort of comes, the benefits outweigh the costs, so we still use it. But we would love for the cost to go down. Then this model, we had to actually train more or less from scratch. We had to take some part of pre-training, but a large part of it also we had to retrain. There's a lot of fine tuning, so-called, or retraining that we had to do. a bit right and a lot of fine tuning so-called or retraining that we had to do the retraining of these models is to take like typically they would run for 48 72 hours what happens is if there is a bug in your code or if you want to change something you have to wait two to three days before you can do it you have to wait for the computation to converge to then you run it on the test data, see how well the test, how it is doing on the test data. Sometimes it is, you have to eyeball the results and see does it make sense or not, right? Afterwards, there's a lot of human judgment involved. And then you go back and start all over again. Now, do we wish that this computation finished in two hours rather than two two three days? We definitely wish that. So transformers are lovely but they are slow as hell. And we need to solve that problem. That is where these things come in. Does that answer your question? Thanks, Asif. Yeah that gives a context. I was trying to see the the magnitude of the problem that we are kind of reading about. It is the problem in transformers today. So that is why this paper, I chose it, because it is a very interesting approach, and it does seem to address the performance issue. So we'll go through this. Now, as you look at this paper, what we're seeing is anyway, so the bottom line is many, many approaches were tried. You can do a binning, you can do clustering, you can do some matrix decomposition, low-rank matrix SVD kind of argument. You can apply all sorts of arguments to that. But the local attention on so on and so forth, at the end of the day, sparse attention, all of these are heuristic arguments, right? They come with a prior belief that this, let's try this. And in very simple terms, it comes to the things that, hey, let's try this and see if it works for our situation or for reasonably large number of situations. And so people have come up with a lot of these arguments. The point of this paper is that it says that they have an approach which makes rigorous guarantees. This is the thing. Other approaches do not provide a rigorous guarantee for the representative, such a method. And this sort of what people and what mathematicians have often said about the deep learning community. There is a lot of post-Hoff hand-waving arguments after something works, right? You're trying to explain why it works and sometimes those explanations do make sense, sometimes they leave gaps. So what the main idea in this particular paper is that this thing is this, this part. They have created... This is the important word to pay attention to in this, is this not relying on any price, which basically says don't come with any beliefs that this is how a word should pay attention to other words. Just come up with a mathematically rigorous approach originating from the math, from the theory, not from empirical hand-waving. So that is what it is. This is the politest way of saying, hey, guys, we work the math rather than make some vague hand waving arguments. In simple terms, that's what they're saying. So what is it that they're doing? What they're doing is a little bit mathematical. So today, because I want to keep this one hour i will introduce the ideas uh once we have done the transformers because from next week we're going to do the transformers once we have done the transformers then at some point for those of you who are who have taken mathematics of data science with me, the engineering math workshop with me, them or somebody who is brave enough to sit through the mathematics, I'll invite you to come and we will walk through this paper line by line. Are we together? Because this paper is fairly dense. It has a lot of mathematics and it's a little intimidating for people without that bugger. I don't want to do that. I don't want us to feel. So it goes into the mathematics here. Let me do one thing. Let me recap it in simpler terms because this is a very condensed representation of the mathematics that is given here. Let me explain in terms which is, in fact, in terms that I used to explain it to myself. Let me do that. Give me one second, guys. So are you seeing my writing screen, guys? Yes. Yeah. So suppose you take, let's say that you have word one, you have a sequence word two. And you have word, actually, why am I doing it like this? Let me do the traditional way we did it in the attention paper. So imagine that here you're feeding in word one to word l l is the length of the sequence now word two word i right word now if you remember what we do is we come up with a representation of it some sort of an embedding representation which is embedding of a word i which is made up of the word embedding so first you take some sort of a word embedding word embedding wi and then you attach the positional embedding of the word. So this is, let's say that this is 512 and this is a 500 or 500, let's say 500 and this is approximately 500. Dimension, right, r to the 500 and r to the 500. You remember guys attention paper the dimension the the the dimension and I mean sorry another dimension the position encoding has to be given because otherwise you wouldn't know where the word is with respect to other words in the sequence. Okay so this embedding each of these words becomes e1 through this process of this process. And this is just a recap. I hope, guys, you remember all of this. EI all the way to EL. This goes. Now, what happens to each of these vectors, these embedding vectors, what you do is it goes through the first attention layer. Let's say that this is an attention layer. Well, it is a multi-head attention. So there are many heads to it. So one head, let me just call it one head. And there are, of course, many heads. And there'll be many heads of attention. But then let's look at this so what this attention thing does is it has a matrix wq wk and weight matrix and wv and what it does is let's say for each of the embedding you have sorry you you get q the word q let me just say q i for the word ei a word i so here's a sequence that we will do i hope this explains it wi becomes the the embedding the word embedding and then so let me just call it the journey is that it goes through the wi plus position encoding becomes the embedded vector the input vector this input vector goes through these matrices right to become the q i v i and k i right the words we use is key vector then a value vector and this is the oh sorry not the q query vector and this is the key vector And this is the key vector. This is just a recap of what we did. And so, sorry, this is just straightforward matrix multiplication. What did I just do? Yeah, this is just straightforward matrix multiplication so in other words q i for example is w the weight q times e i right if you multiply the input vector this so you will get with the w matrix you will get the query vector it's a query vector for example do you guys remember this part we did this right in attention any questions so far so we need to remember this part before we can continue anybody would like to give a feedback? Do you remember this, guys? So anyway, if you don't, just remember that it does get broken up into three parts, three output things. And the whole idea is that a key is in some sense the coordinates of a point of a word in a vector space. Now, all of these vectors, they exist. Q, the dimensionality of Q vector, or a V vector and K vector, they are all some fixed number. Let me call it D because this paper uses D, typically 64 dimension. Right, this is what you use. You can pick a dimensionality but this is 64 dimensions. So this is it and just keep this number that e dimensionality of e is e vector is approximately thousand. is approximately thousand. Next comes, what is attention? So if you want to quantify what an attention looks like, with this there, see what happens is that given a word, given a query, Q term, QI, word given a query q term q i how much attention q i pays to word j k j right remember i said that you take the dot product people also write it as q transpose k and so on and so forth i'll sort of avoid the transpose notation think of it as dot product K and so on and so forth. I'll sort of avoid the transpose notation. Think of it as dot product because transposes with matrices because you a patch in a whole batch of data and so forth and this paper tends to use that but because that's how you do the computation in batches but mathematically it's more elegant to think just in terms of dot product. So think of this, the attention that query vector i pays to, pays is basically, first thing is you have this matrix, a i j is equal to this. It is softmax of and then you you if you remember there was a normalization factor softmax. It is the softmax actually looking expanded out in a clean way. Attention is equal to softmax of this quantity the dot product and the intuition is that how much attention this query this word qi appears to the word j is the dot product between the query vector and the kj so in other words in that hidden representation space let us say kj is here and kj is here and the KJ. So in other words, in that hidden representation space, let us say KJ is here. And KJ is here. And the query vector is here, right? This is a QI. So these two are pretty close. They should pay a lot of attention to each other as opposed to a query vector, let us say, like this, a QI prime. QI prime will not pay much attention to QJ because they are orthogonal to each other. They sort of don't make, they don't need to pay much attention, they're independent words, right? Orthogonal is like independent or unrelated words, right? Whereas if these two words are genuinely related in the sentence, then they should pay a lot of attention to each other. Now, if you remember, what is a softmax? Softmax of anything. Softmax, just to just to remind you that the softmax of x of any vector or anything quantity was of like let's say x1 xn if you remember we did the softmax thing it is the way we do that is we take each of these elements e to the xj over all j, right? So this thing, if you normalize it, this is your softmax. This is just reminding you of the softmax function that we did that. Now, this is a normalizing constant, right? For example, if d is 64, this will become 8. That's all. So as far as reasoning about the theory is concerned, without loss of generality, there's a term that mathematicians use often, and you see this very common term used in mathematics, is to say that something doesn't matter when it comes to reasoning or making the main argument or theorem proving. And the one thing that something doesn't matter when it comes to reasoning or making the main argument or theorem proving and the one thing that here doesn't matter except from a computational perspective is this denominator right so uh if you can absorb the denominator and the qis itself if you are a little bit sloppy enough sloppy but you can just sort of absorb it so you can come up with a new definition of AI, which is the softmax of qi kj, right? This is it. Now, this softmax, considering that the denominator is, again, a fixed thing, what these things add up to is sort of a sorry why is this repeatedly touching okay let me remove the touch okay touch off much better so what i'll do is i'll sort of not pay attention even to this so what i'm doing by now you realize that i'm beginning to reduce a function to its logical core by eliminating some things, being a little bit imprecise. And this is a thing that physicists and mathematicians do very commonly. So I'll give you an example. Like in the theory of relativity, you would say E is equal to MC squared. This is a cliche in our society. Energy is any mass can disappear into energy. And the amount of energy produced will be mass times the square of the velocity of light. Are you guys familiar with this expression? This is expression that you learn in high school or college or something like that. But if you look at theoretical physicists, they won't write this expression like this. They will just say E is equal to m what happened to the c square so you know that c is equal to 3 into 10 to the 8 meters per second but to what what the theoreticians will do is they will just say c is equal to 1 they'll set the constant equal to one because they say that in order to argue or reason a theory, you don't need to worry about constants and non-essentials. So they will just say that the essence of the theory of relativity is that mass is energy, that is it. And so in a similar strain, I will just sort of even forget about the underlying denominator which is essentially a trace of sort of a of the matrix but we'll leave that right we will just say that this is sort of in a way exponentiation of q i k j right because that if you look at the definition of this, only the numerator matters, denominator doesn't matter. So we will pay attention only to this part. So you can say that the attention, the essence of the attention is the exponentiation. This paper uses this notation. Q, I think, what does this, how it uses all this transpose business, which I won't. I don't think that, is this. So far, so good, guys. Is this looking meaningful? So the attention is essentially the exponentiation of the dot product between the query and the key. Dot product is similarity. You exponentiate the similarity between this. of the dot product between the query and the key. Dot product is similarity. You exponentiate the similarity between this. Now comes this problem. How many values of i are there? The number of i values are number of words, isn't it? Number of words in the notation that we created, the length of the sequence, right? So we said that the length of the sequence is L. So we are saying that this computation, therefore, of the A matrix is L cross L matrix, isn't it? Because I can take values, I goes from zero to, sorry, one, one all the way to L, J goes from one all the way to L. Isn't it? And so this computation, each element of this, each cell of this matrix needs to be computed. So therefore, this computation is order L square. Would you agree guys, that in terms of the input sequence, it's a fairly massive computation. Yes. computation. And then comes the other part. The full attention is this. The attention vector for the word I is actually equal to aij times vj. This term, if you go back to this, this is equal to softmax written out in full form, softmax qi kj square root of d. I'm just writing it the way it is written in the paper, it is written in the paper times vj right these are all vectors so that is the full expression as in the original attention is all you need and that's the expression so you realize that now what is the dimensionality of this guy the dimensionality of this guy is a d value vector all of this kqv and so the computational cost is of computational cost of computing this attention vector is order l square to compute the aij part and then d times d order D, order L squared times D. And this is the main problem, to find how much attention each word pays to other words. You need to compute this weird, this huge, this attention for each of the words, attention vector. You need to do a very expensive computation. And this is why transformers are so slow. So far so good, guys. So guys, I'll take a break because now I'm getting into the crux of the argument. But before I do that. Excuse me, I want to see how many of you have followed through with this. Give me some feedback, guys. Are you sorry. Are you following through? I hope you guys are following through with what you're seeing. Is it looking reminiscent of something we have studied guys? Do we remember this or do I need to go slower? slower. So you need this much of preliminaries to understand the crux of what this paper is saying. And now what is this paper trying to say? It's a very, very nice argument, actually. It says this. It says that, remember kernel methods? Do you guys remember we did kernel methods? Basically, what we said is that the application we use of kernel methods is you can use a kernel to linearize a problem. So suppose a decision boundary is crooked, you can go to some other domain. So suppose now that me use the notation of x y vector so suppose you have a vector space in which x y are that i'm you i'm trying to use the notation that this paper uses but uh just bringing back memories that suppose you have uh bad memories that suppose you have a peculiar decision boundary like this like this side is positive this side is negative and you need to find a decision boundary what we said is something quite interesting we said that you can actually go to a different space and that space is you take X and you transform X using a function to phi of X. This is called the kernel function, the kernel mapper. And likewise, Y will go to phi y and we say that the kernel function between x and y is by definition it is the dot product of say, remind you that this is the definition of a kernel function. Now, these kernel functions are by definition symmetric. They are also, we tend to sometimes require them to be positive definite and this and that. So, but this is it. There's a Merced theorem that sets certain degrees, certain conditions to these. But this is it. Right. Now, the lesson we learned is that what you could do is you could map, use this function to map to a space in which a decision boundary, like a decision boundary like this, gets transformed to a let's say, a dimensional space, input dimensional space n, phi will take you to a dimensional space, to phi x, where phi x is a vector that belongs to a much higher dimensional space. Let me call it d, and d is usually greater than, usually greater than equal to, greater than equal to n. But when you do this, the problem gets solved. So that was the theory of the kernels, right? It turns out kernels can be used for a lot of interesting things. You look at this fact, and I told you that kernels all have to do with dot products. Now here, if you, the clue that you get that kernels may be applicable is the fact that do you see that at the heart of this matrix is this dot product do you see that guys exponentiation of the dot of this dot product the fact that the dot products in many ways you can say kernel methods the main argument is whenever you see dot products you can do clever things with it dot products are pretty powerful you can apply the kernel trick and so what is the kernel trick that they apply i'm going to talk about it a little bit they they have a function and what the function does is something like this and this is the crux of the argument they're making see what we have is when we do a word q i k j you are doing a computation whose size is l cross l isn't it guys when you do this dot product would you agree then you multiply it with the v vector then you multiply it with the v vector the vj vector to get the full attention and the size of this is the the size of this is well this is l l cross d d being the d is let's say 64 dimension and l is equal to uh you know whatever the the number of the words are and so on and so forth right so this is an expensive computation what they are saying is that this we have a way to make an equivalent computation whose nature using this kernel trick is that you do a computation of this initial L number of words at sequence length times a much lower dimensional vector r plus l right this is your uh so let me put names to it this is your a i j one part this is your v j vector kind of thing oh yeah i have put it at the top so let me not confuse it it. It is there at the top, A, I, J. And what you're basically saying is you create a transformed vector. Let me call it K transformed. All right. This and. This part here is your V vector, right? R cross L, L cross D rather. This is R cross L, one part. This is your K, actually, I'm mixing. That is a Q prime. Yeah, yeah, Q, because it is q k v so yes this is q this is k and this is v of course so v remains v l cross d but what happens is that the q k the q k you because you bring in a q gets replaced by uh the, because of the five notation, it becomes, actually let me use this, Q prime and K because of the five transformation, this kernel mapping transformation becomes a K prime. K prime. But when you do this, the way, so this becomes this Q prime, K prime. And so this is the crux of the argument. What they are saying is that you can do this computation first, K times V computations first and then bring in this. And when you do that, you end up for reasons that we'll get into you end up doing a much more efficient computation you end up doing a much more efficient computation by how much efficient it turns out that this becomes a this times d a computation and this computation this times D, a computation. And this computation, I think this is a plus D or is it D? Something, it may be the times D or plus D. I think as we move to the paper, we'll see it. This is far more efficient. There is no quadratic term in here. Now the whole question is, how in the world did that magic happen? Just by, it is almost like you are doing this QKV. Initially you were doing this times this, but now somehow if you put, you find a way and you do this. Very logically speaking, you're doing this. So from this, you're instead moving to a computation like this. And in doing that, somehow the computation becomes much smaller. In fact, the reason it becomes much smaller is that this R dimension that you choose, R is much, much smaller than L. And so you get to do computations which are much more fast because of that. So what does it mean? How in the world do you do that? And the entire magic of this has to do with the fact that you need to find this representation now if you remember that in support vector machines we used a kernel trick and we decided that what phi is may not matter very much all it matters is what kx is so long as you can express this in terms of dot products, you're fine. So you took all sorts of kernels, the Gaussian kernel, the polynomial, this, that kernel. Now here, what this paper does is it does something that is reminiscent of spectral decomposition or Fourier kind of expansion, almost like that. It says that, see, it first makes a small argument. It says that if you look at this part, just take any two vectors, forget kq, just for the sake of argument, you look at this exponentiation of it, which is essentially, roughly speaking, your softmax xy, right, because you can forget the denominator and other parts. This part can be represented as essentially two, like three parts. You can represent it as exponentiation of x, size of x over two, then a kernel of x, y, some kernel function of x, y, e to the y, the size of y, square over 2. This is a very interesting result. It's saying that if you want to do the softmax, if you could break it up into three parts like this, this thing of two vectors, exponentiation of the dot product of these two. You could write it in this form. Then suppose you could do that. Now, the question is, how do we know that we can do that? So this is actually very easy to prove. In this paper, it is your lemma one. Easy to prove. And the proof is in the appendix. We'll go into the mathematics later. What you're basically saying that the exponentiation of a dot product, you can write it in some form like this. Well, you say, well, that is fine. But there is still the the the tedious matter of the kernel. Now, why in the world would we do that? One reason you do that is see finding a vector's size, is it a quadratic computation? It's not, right? It's a very linear computation. You go to each element wise square and add it up. Isn't it guys? Suppose I give you a vector x is equal to 1, 2. Right? What is x squared? 1 squared plus 2 squared. Would you agree? This is a very simple linear computation. Right? So the important thing to realize is that this computation is easy to do. This computation is easy to do. But there is still this pesky matter that what you have done is you have replaced the exponentiation which used to be a very expensive L cross L computation of quadratic computation with three parts. One part is easy, linear, linear, linear. And this kernel now comes the crux of what the paper is trying to say, the beautiful thing. So by the way, there's a word in probability theory literally called a beautiful function. You will see it referred to in this paper when it talks about beautiful function, it doesn't mean in a poetic sense, it actually means it's a technical term that you use in statistics and probability. So we'll go into that later on when we do the technical data and notice that we are out of an hour. So I'll just tell you what it is. What they do is they represent this with some function, a hidden representation function with a normalization. So I'll leave the normalization. And then what it does is it breaks up this into components. Each component is some sort of beautiful function of the input multiplied by some weight, W1 weight sum 2. Sorry, F1, F1, F2. And you say this all looks very strange, all this mathematical gymnastics. What is all this? W transpose 3x and so on and so forth, right? So all the way to fL, W transpose L times x, right? So what you're doing is you're taking all of these w vectors, multiplying it with your x vector, and then taking this so-called beautiful functions out of it, some sort of a function out of it. There is a result. Or you can say that it can be represented like this. Now, what in the world is this the simple answer is quite often let's take a simple situation suppose this was not there this was just one right and checks is equal to 1. So the only thing you look at is f is equal to some function, 1 of course, of w1 transpose x. That is it. Now what kind of a function can it be that you can use into which you can expand this kernel mapping function. It turns out that if you take a very simple situation like this and you take only the first term, this sort of kernels have been very well understood. These are called PNG kernels. But forget that. The beautiful thing is that you can use simple trigonometric functions. Cosine, sine, cos, etc. Any one of these you can use. And when you do that, it should remind you of, you know, Fourier expansion or something like that, right? So what you're basically saying is that you can write this function, this mapper, in terms of a whole set of trigonometric function. And you know that this is true. Suppose I give you a thing like this. What is, if you remember from your engineering, you tend to do our fx is equal to some sort of a series expansion, Fourier expansion. That is roughly the intuition behind it, very rough intuition that you can write things like this. And here, they explicitly take a representation in terms of trigonometric functions. And when they take it in terms of trigonometric functions, what about multiplying trigonometric functions? Very, very cheap, isn't it, guys? Multiplying any two trigonometric functions is very, very cheap. And that is the gist of what this paper is saying. That what they're saying is that any, this complicated and expensive expression, you can actually break it up into two linear terms, which are very easy, and then a kernel function, but the kernel function can be explicitly mapped to sort of a Fourier like expansion, Fourier like terms and or trigonometric based terms and gaussians and taking some gaussians let's say for h and so forth and so what happens is that when you do things like this you end up solving this also into a linear problem and so you have linear linear linear your computations go very far very fast now the mathematics of this is quite interesting actually uh it goes into the mathematics in great detail in the appendix not in the paper itself but in the appendix let me show that to you so this is the performance results just to show that it works. But actually, the main thing of this paper, the beauty of this paper is in the appendix here. And so it is the so-called favor plus algorithm. And this is the, by the way, this expansion of phi, it is the random feature map, that concept or the trick is called the random feature maps. And that is the heart of this algorithm, the use of random feature maps. So a lot of proofs showing that this actually strictly converges empirical proof, but at the end of it, the section F of the appendix is where the beautiful thing starts. If you look at it here is where it is making an argument the sort of argument i was talking about that you can write any dot product like this exponentiation as this thing and you can work out the algebra very simply you can work this out and you'll see that this is the the first part is obvious right and then from that you can step by step you can gradually bring it down to an expansion in terms of this omega function omega times unit factors and so on and so forth it basically does some form of a spectral decomposition and it gets a little bit techy but just take it as a fact that you can write it in this space where you're representing it yeah just look at this equation equation number 13 right what you're seeing is that mean squared error of this can be represented as this quadratic basic simple linear terms that you can find out times the variation of the cosine of omega times delta delta being x minus y like k minus q the difference between the two query vectors and so those things all are very tractable problems and well okay it goes into more and more details when you do that at some point we should do that. I'll cover this in detail. This paper is really doing well and it needs a little bit of mathematics. You, for example, should remember some of the trigonometric identities of integration and so forth. So this paper does get into the proofs in great detail and those proofs are worth studying. It took a while for anyone to sit and carefully work through the proofs. that you can linearize this kernel, the main attention computation. And when you do that, it is, you can prove that it is universally true, that it is, and it works all the time. That is the gist of the paper, guys. So any questions, guys? So there's a lot of heavy mathematics here, but the gist of what it says is pretty much along this. Where am I going? It is along this fact, this main equation, that you can write it as two linear terms and then a kernel. And the kernel can be, in some sense, do a sort of a spectral or a, to me, it appears like almost like a Fourier, reminiscent of a Fourier decomposition, a frequency domain decomposition, almost like that. And make it into a linear problem. And that is the gist of this paper. But the implications are huge. Implications are that, first of all, this is drop-in compatible with the original transformer attention computation, you know, the softmax computation. So if you have trained a network with that, you can actually put this as a drop-in replacement. And so even a transformer which have been painstakingly that you have trained using the older approach, nonetheless for inference you can drop this in place of that. And when you drop this in place of that with the same weights and so forth, what you do is you make the computations even at inference time happen much, much faster. And that is that guys. That is it. Any questions? By the way, was the math too heavy, what I was just saying? Or is it straightforward? I tried to only put the main idea here, not go too into the mathematics. It's helpful. So let me ask people who have taken ML 200. To you guys, let me randomly pick somebody. Who should I pick? How about Balaji? You took ML 200, isn't it? Yeah, yeah, I took and I'm still finding this to be a little deep. Okay. So the kernel kernel function a lot of questions in my mind how someone would come up with this kernel function and you oh it's a little bit they didn't they didn't actually the theory of random feature maps has existed for a long time see the way they have done is they have connected the dots the random feature maps as an expansion for kernel functions it has been there there's a rich body of work what the crucial thing that they did is they took the softmax and they broke it up into these parts and they showed that actually using the kernel feature maps you can do it now how did how did those scientists come up with the kernel feature map in the first place those researchers well you know every research is incremental that people have been trying to come up with explicit representation of the kernel functions right and they have created all sorts of reasonings and if you if you know a lot of this has a flavor of four-year series and things like that you guys remember four years series that any complicated function can be represented as a four-year series in the frequency domain and so forth uh terms in the frequency domain so that intuition goes back many couple of hundred years from which those this has generated. So piece by piece, you know, a lot of the time, a big breakthrough is not doing everything from scratch, but being able to put different pieces together. So I'll ask it, so okay. The Fourier series is like taking the transcendental functions and making, you know, stitching the whole, I mean, decomposing the whole function using a lot of transcendental functions, right? So let me write it down. So suppose you have, let there be an arbitrary bn rb arbitrary function Rb, Rb, Rb, Rb, Rb, Rb, Rb, Rb, Rb, Rb, Rb, function. Usually, let's say, of t, right? Time. Think of time. The intuition is think of it as a time varying. So let fx, fx, look like this uh ft rather look like this uh t and this is fd i'm reminding you of your basic engineering so suppose there is a function that goes like this is a time series what you think of is that it is actually made up of certain sine waves and so on and so forth, multiple sine waves of different frequencies. So what you do is you assume that Ft is equal to 1 to infinity let's say n is equal to 1 to infinity right there are infinitely many terms into which you can expand this. This is a Fourier expansion. I hope I got this right. I'm writing this equation after almost a decade or two decades. So what you're doing is you're saying that technically infinite, but quite often you're lucky and only a few terms matter. So for example, maybe A1, A10, A, something like that may matter. And there are these frequency terms. There are two, three frequency terms, A1, W3, or something like that, three, four frequency terms, a small number that matter. But nonetheless, in the general case, you can write a function like this. Now, how do you find the frequencies? What you do is you take f t, you integrate it with a function minus omega t, right? When you do this, you'll end up with another function which is the Fourier transform of this function and this Fourier transform is quite interesting actually do you notice this omega t function here you think of it as a convulation between a known frequency term here and a probe here and the function and what it does is that it extracts the frequencies the actual frequency profile of this particular function what are the frequencies present are we together so let's take an example suppose you have just a sine wave right a sine p let me just take this a sine t and then you try to do the Fourier trans or maybe cosine t let me just make it as a cosine t it doesn't matter what is this this is sine t and then you try to do the fourier trans or maybe cosine t let me just make it as a cosine t it doesn't matter what is this this is sine t this one is sine t the shape looks sine t okay i'll leave it as the sine t now when you try to do it and this is a very elementary mathematics if you try to do minus i by the way i omega t i omega t let's say sine t dt now the first thing to remember is sine t is the imaginary part of e to the it isn't it because e to the it is equal to cosine t plus i sine t so you would realize that sine is the imaginary part of this full expression. So you can also write this expression. And I'm just doing a very quick word, imaginary part of e to the minus i omega t e to the t dt. Now this expression looks, you would agree quite quite solvable because this becomes imaginary part of e to the one plus i omega t dt right and this of course you can do that when you integrate this this will be equal to what one plus i omega e to the one plus i like basically let me just call it e to the one plus i uh like basically let me just call it e to the z one one minus actually i should take it back one minus one minus and one minus it doesn't matter a minus one plus z a dz right a minus And basically you're looking at the imaginary part of this. And what you have just done is you have come up with a Fourier transform of it. This is by the way, the kind of math you may be remembering from the past, right? In fact, I myself am doing it, God knows there must be a simpler trick to quickly extract this. So this is your Fourier transform, but the generalization of this idea is obviously you can do sometimes even more. You can do the Laplace transform, which is also called the Heaviside transform, which basically says that e to the minus st where s can be a plus i omega right need not just be imaginary part it can have an attenuation factor let me just call it an attenuation factor lambda ft dt this is your laplace so there's a whole bunch of theory and beautiful results. In fact, a lot of control systems analysis, those of you linear systems analysis, those of you who come from electrical background would remember that entire semester course is quite often reduced to just the application of this function to your circuits, right? If your, if you want to understand, there's a resistor, there's a capacitor, there's a solenoid and whatnot, all of these combined. And how do you look at the response of all of this? All you have to do is a simple way is that, that you just compute the Laplace transform of it and you do your systems analysis. So these are very powerful ideas that are almost 100, 200 years old. And then even bigger generalization now from this is that you can imagine that any function f t can be represented as a summation, not just in terms of sine and cosine, but any orthogonal function, a n, some orthogonal function of t let me just use fx here now because then i'll be coming back to our notation to the more like kernel like notation any function can be represented as n is equal to zero to infinity as a decomposition in any orthogonal basis. You know this result is the heart, it is the mathematical result and it says something that any function, if you have a good enough orthogonal basis, you can approximate a function with it, any function with it. This is now, if this thing is a node, right, this is your activation function phi. If you consider this phi as the activation function, what you're basically seeing is that phi 1, phi 2, phi 1, phi two. What you're looking at is nothing but your universal approximation theorem of neural networks. And it's a very powerful way of looking at it. What you're basically seeing is an equation like this is the sense of a universal approximator, a neural net, because you can think of a node of a neural net as nothing but this, a function that is applied to all of this. And so it gets quite interesting actually. You can think of all sorts of generalizations of neural networks that can do things, fancy stuff for you. So there's a lot that you can think through on a piece of paper and reason and have fun with. Thank you.