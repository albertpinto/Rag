 All right, let's take care of it in the course of time. That will, oh sorry, where is it? Somewhere. Oh goodness. I have a boom. I'm having, I have to somehow make it quite mute much better. Too many windows open. Are you recording at this point, Haseeb? Yes, the recording I believe has started but it usually announces if it hasn't, yes it is being recorded. Yeah. So, Today's topic is something called support vector machines. But before I go into it a couple of announcements. I think I have not given clear enough instructions on how to get started on the project. given clear enough instructions on how to get started on the project. To some extent I thought giving a lot of independence and flexibility on how you do it, do it in R, do it in Python, do it in cloud, do it on your laptop machines, wherever, would help you if you guys form a team. But if you guys have formed a team, maybe you need a starter code. So what I will do is I will take maybe one of them say archive data and create starter code with it so that you guys can get started with your project. Would that help you? I can make, please set up the early stages. I can make it as fill in the blanks and then you can start by filling in the blanks, yeah, the early stages. I can make it as fill in the blanks, and then you can start by filling in the blanks, and that will give you the momentum. And then the idea is that you take it in your own creative direction, find things. The reason for giving minimal instructions, just getting started instructions was to give it the nature of a hackathon. Hack the data as a data scientist and see what you can make of it. The goal is not to deliver something that I have set expectations for, that you must deliver specifically this. The idea is that you guys get started and see where your creativity takes you. There are many, many directions to pursue. If you guys do need guidance, any of your teams need guidance that can be pursued that or is this interesting do please reach out to me this weekend i will do a session in which we will do a bit of starter code hopefully i'll get the time to get that ready and we will also work on to all these technical modalities modalities of the uh can you publish the data set uh even before you could give the starter code as a uh step you know as a step zero so we can get started people who think those data sets are published so to recapitulate maybe i should i trade this the two data sets are the archive data set on kaggle and the kovit data set which is also data sets which the archive data set on Kaggle and the COVID data set which is also data sets which are available also in Kaggle okay so we can pick any one of them no you said archive data set you're going to give the starter code so that that particular data set that you're going to select I just want to be for uniformity's sake I want to work on the same data set sure so let me do that and so yes there is a gap i need to fill the gap i will do that yeah until you get time to give the starter code like tomorrow or day after if we just publish the data set so we can at least get started we may not make much progress but at least we'll get started with the eda and stuff right and remember guys when i'm giving you this thing you can do start with descriptive statistics but i'm giving you this thing, you can do start with descriptive statistics, but I'm giving this project as a creative area to learn new things, things that I couldn't teach you during the sessions because we have limited number of sessions. We are halfway through this, you know, more than halfway, we are halfway through this workshop or more and we have a, I want to cover a lot of topics. I'm fairly sort of this time was ambitious. We have to do the all of the kernel methods today which will include a nearest neighborhood methods. I don't think we can cover all in a day. It will spill over to the next time. I want to do regularization and I want to do the recommender system. So if I have to cover these important topics, which are really worth covering, then some other topics we can do only through the medium of these projects. For example, in the ML 100, I introduce you to graphs and to NetworkX. Obviously, I know that you guys didn't get time to reach that far. Maybe it was a stretch at that time to do that. But I want to use this project as sort of the vehicle through which I give you some starter code, some way to get started with graphs and graph data. And that is why I gave that archive problem. It is a straightforward graph problem, and you can actually extract a lot of meaning out of it. Some of the Kaggle networks may be very good, but a couple of random ones that I looked at on the web, they were not very impressive. You can easily do much more impressive notebooks as a project, and it would be a pleasure for you guys to go post it as a team and to be upvoted and become popular and so on and so forth, which is my ulterior motive. I want to see you guys lead on some data set with the best notebooks there eventually. Do it in your CoLab, do it in your GCP, but eventually do put it there. And if you do, see, whenever you teach, it's sort of a personal thing, you always hope that the audience that you're carrying forward, the participants, they meet with resounding success, of course. Then reality is somewhere in between because people are busy, they can give only limited time and so forth. But the fact does remain that if you guys do excellent notebooks, put it up there, and that gets a lot of upvoting and a lot of appreciation, you do a great job at it. gets a lot of upvoting and a lot of appreciation you do a great job at it it will be a good self affirmation for you you will get affirmation from external third party people that yes you're doing a good job and for me that will be really a moment of pride you know a moment of happiness that these efforts were worth it right so please do focus on your project and showcase it. With that, that is what the project. The second thing I'll talk about today is, there's one part about ensembles that I didn't do, which is boosting. I did bagging. Random forest is an example of bagging. Boosting we skipped. I was going to finish it today, but then I just took stock of how many classes we have left, how much territory we have to cover. And I realized that we wouldn't have time. So if you guys are all fine with it, I will give an extra session somewhere in the next, before the next Monday, of course, somewhere in this week or the weekend on boosting. Would that be okay with you? Does your schedule permit it, guys? We will have an extra theory session yeah yeah i think that will be done you also sound very quiet today i sound very quiet i i have my value maxed in us oh a little quiet uh i i can i can hear uh I can hear. Yeah, it is normal for me as well. You know, Kate, sometimes there are two volumes in computer, the one the zoom and then the other one, the computer volume. I don't know if one of them is turned down in your case. I'll check. I'll check. Yeah, so already guys, one more thing. I haven't sent out the comprehension survey how well we understood the ensembles, I'll be sending it out. Please do fill it out. But in that survey, there will be a couple of extra questions because this is a midway through the workshop in academic environments it is customary to give a mid term sort of a survey to see how the the workshop the course is going and so there will be a few questions which are not about random forest but which will be about overall satisfaction and that and if you want the course to change in any way, do write about that. So just to summarize all that we said, we will take the projects more seriously now, but it'll be in the second half of the workshop. Number two, we will do an extra session on boosting, where we'll talk about AdaBoost and Gradient, all this Gradient boosting and XGBoost, CADBoost. I may not be able to go into every specific implementation, but I'll give you the overall idea of what is the concept behind boosting. Boosting came with the work of Freund and Shapiro, I believe the year was 1998, if I'm not mistaken, when this, when the first big paper on boosting came out. And certainly after that, as they always say in machine learning, the world was not the same after that in the sense that if you go to Cagle today, for tabular data, you see the random forest and boosting. They often are ruling the roost in the leaderboard. They're often present as usual culprits, helping the people get into the leaderboard there. So these are worth learning. And I hope the lesson we learned, we haven't finished with that, but the lesson we should learn is ensembles are a great thing. Wisdom of crowds is a great thing. Always try to harness it. Lastly, there is, so that. Always try to harness it. Lastly, there is so that would cover bagging and boosting. But there is one more thing called stacking. The stacking is doing something very unusual. It is taking a bunch of learners or even ensemble of learners taking their predictions and instead of either averaging them or boosting their predictions together what you do is you feed it into another layer another let's say classifier for example if you're trying to classify data regressor quite often you make it a just a few simple layers of a neural network you can can make it just simple support vector machines are very common, something that we're going to learn today. So you take the input, the outputs, or the predictions of those algorithms to be the inputs of the new layer, the new classifier, let us say, to make the final classification. That method is called stacking. It hasn't gotten a lot of attention, I suppose, and it deserves a lot more attention. It's sort of heights and obscurity. I don't see many textbooks talking about it and so forth, though in practical terms, it is very, very useful. So that is also something we'll cover. The problem is when will we cover? Most likely, I'll try to sneak that stuff into one of our lab sessions. So we have an example of stacking also. That is that. And that will round up our understanding of ensembles. It's, well, again, as I said, I apologize, we couldn't squeeze everything in 12 sessions. As you realize that the sessions are pretty crowded, isn't it? I hope you do feel that we are covering a lot of material every session. So today is support vector machines. Now, after that long preamble, let us get into support vector machines. These support vector machines, there are two things I would recommend, and I will post it in Slack. There is a lovely lecture given at MIT by Professor Patrick Winston. I like it very much, you know, he's one of the great legends, not just of machine learning. I think he was the director of AI at MIT. Obviously he's one of the great legends. He was also an exceptionally good teacher who got many, many awards for excellence in teaching and not just in MIT, but at a much larger national scale. He has contributed ideas on how to teach and so on and so forth. And he was obviously a pretty deep researcher in artificial intelligence. He has a lecture. His lectures are there on YouTube and this MIT's site and so forth. I will post the links to that. His lecture on machine support vector machines is particularly engaging. And one reason I like it very much is, by the way, Patrick Winston has passed away now. He died fairly recently. It was, I think last year he died or the year before. I can't imagine time flies so fast, but anyway. So he's passed away, but it's, whenever I think of support victim she not teaching it I wish I could do it with the same flair, which he taught. The thing I like about him is that he makes things alive. All this algorithms all this subject matter. It is not all cut and dry as we all know, they just didn't come out of the head of some very dull and dry people. These were created by human beings, you know, very alive, flesh and bone human beings. And he had a tendency to talk about how those algorithms were created, who imagined it first and how did he go about doing it and so forth. Particularly so in that particular lecture on support vector machines, which is why I'm recommending it. So the story goes as it's quite an interesting story. It's worth hearing. Now I'm just saying it from my recollections of how Patrick said, because I watched that lecture many, many years ago. So this is how he says it. There was a guy and this is one of the principal people. Obviously, anyone research is many researchers come to it at about similar times, they contribute ideas and so on and so forth. But here we are talking about one of the key ones, or perhaps the key one. So there was in Russia a mathematician, his name was Vladimir Vapnik. The Vladimir part of course is like Ram in India, half the people are Ram or Krishna, so it's somewhat like that, or John in the US I suppose. So Vladimir is a very common first name, but Vapnik, I don't know. So his name was Vladimir Vapnik, a mathematician, is rather, let me say his name is. So he, in the 1960s, had an interesting idea. So he, in the winter, in the cold Russian winter, he was at his dacha apparently and then he started having the germs of a great idea. He wrote down, he began to think and I will literally take you through the journey of intellectual discovery. He thought about it and thought about it and then he started like beats in a garland, putting all these ideas together, till the whole thing began to sing. It was just beautiful. And he knew that he had hit upon something quite remarkable, and you can imagine that he was doing it over his cold, over different, you can imagine in cold Russian winter in a dacha, somebody sitting with a samovar and having pots of tea, and then going out for a walk in the snow and coming back and having another idea a few days later, another idea. And through the months of the winter, he puts all of these pearls together, these little remarkable insights together, till it all begins to come together in a remarkable way. When he put it all together, he tried to publish it in a mathematical journal. As is no surprise, quite often when you have something truly revolutionary, you tend to get rejected right away in academia. It's sort of the nuisance. People are very afraid, they don't know what to make out of it. Everybody approves, the reviewers easily approve an incremental research, something that moves the needle a little bit forward, right, because they know that you're adding grains to a known hill of sand. But when you come up with something radically different, it feels odd, strange, you don't know what to do with it. So his papers were rejected in the Russian journals, mathematics journals. He then thought maybe I'll try neuroscience because neuroscience people are not that mathematical and they'll be more appreciative of what he has done. It has applications there. And to his bitter surprise even they rejected his paper and this was in the mid 60s he sat upon that paper for many years and and he sort of like any man would he grumbled about it to his friend all the time so then in the 90s so the story goes i don't know who did it i think uh ibm labs did it or Google or somebody did it. They had an open competition in the West to say. I think it was the Amnesty competition itself. But I may have my facts a little bit wrong here. So that was there. And the idea is that who can classify or do this the best? It was a worldwide competition. The news of it reached remote Russia and Vapnik was sitting with his friend and saying that I bet I can do much better than anybody can on this because my algorithm would work. So his friend apparently, the way I recollect, I think that's what Patrick said or somebody else said, where did I hear this story from? He said that well you know what, this is not academia. If you can produce better results, you are the winner. Why don't you apply? Everybody can submit results and compete and so forth. And so apparently he sent his work over to the United States. And as they say, the rest is history. When people saw that work, when they realized what he had done and how remarkably he had improved and moved the needle way forward, people knew that he wasn't just playing in the same playground. He was in a playground of his own. So there was a revolution. He started literally a revolution. And that revolution is called the Kernel revolution. Kernel revolution is what we are going to, the kernel methods is what we are going to talk about. Very powerful idea, very, very powerful idea. That idea sort of reverberated through many, many aspects of applied machine learning. Medical imaging was radically transformed, radiology and handwriting recognition and many many things went through radical improvements. So for a few years, it was just this kernel machines everywhere. They had a pretty dominant time. And at some point, then came the ensemble methods and stole some of the thunder, but still they were very relevant. And they're pretty on my bookshelf. If you physically come to support vectors, you'll remember I have a whole bookshelf dedicated to kernel methods and support vector machines and all of these things. So it took the world by storm and we learn about these ideas that are so elegant, so powerful that they did it. Then later came deep neural networks. These days, as you know, deep neural networks are the king of the hill. So you wonder, and did this kernel methods disappear? Actually, they have not. They have just gone deeper and they're everywhere. So for example, just recently I was reading a paper, if I remember right, I've just read the abstract. So all bets are off whether it would still hold when I read the rest of the paper. But from the abstract, it seems somebody has shown very remarkably that the reason transformers, you remember I gave a talk on transformers, transformers outdo recurrent neural networks, and there was this whole concept that attention is all you need. Now somebody has shown that transformers can be thought of actually as kernels, right, and their kernels apply to recurrent neural networks. So very interesting and deep connections people are finding and these kernels are everywhere. So what are these kernels? That is the topic that we will do. So to get into the topic, I will use a particular analogy. The analogy that I'll use, and it sort of comes from my hometown. So I'll draw the picture and we'll take it from there. Let's see if I can get that set up the framework for us to talk on that so let me do this this is my kids Are you guys able to see a blackboard? On your screen, do you see a blackboard? Okay, so we're going to talk about kinematics. So the way I'll pose this problem is, I come from an ancient city, which is variously called kashi's are not what nsi but now. I cannot see the blackboard I can see only you. Know we can see the blackboard. Oh, is it. yeah yeah it's just. Okay, I accidentally twisted to yeah I did switch to sharing content then I can see now. Sorry. But if you see on the blackboard, your portrait. It's lovely. Okay. No, I can see. I can see. I can see. Okay. You look like me, sir. Oh, thank you, sir. Yes. So I come from this ancient city, actually. I'm a bit proud to say that it is the oldest living city in the world. Mesopotamia, you know, the cities of ancient Egypt got destroyed many times over. And so there was a discontinuity in civilization in those places. But the city I come from, Banaras or Varanasi, Kashi or Sarnath, is in the east it is considered to be of fairly, with great fondness people think about it for a variety of reasons. It's the oldest living city of continuous civilization. It was never completely destroyed. You go and walk through the streets of the city and you find that it is practically a museum and a museum of history. You find people in the city who are carrying mobile cell phones and talking high-tech in the university campuses and you see at the same time people who are still practicing things of before the iron age and and so forth so they will not touch anything made up of metal. They insist on living the way life used to be in the pre, in the old ages, right? So there is a huge amount of history there. You find people preserving history and different stages of mankind's progression through history in their customs and traditions, somewhere or the other. It's a very remarkable city and they all coexist there. It is also a city that is sort of the, in the Eastern philosophy, it's sort of the, you can think of it as the Cambridge or the Harvard or the MIT of sorts. Every single great philosopher who has been there in the East, in India at least, has come there, visited that place, discussed his views and so forth. It's a very philosophically alive city. Lots of debates today, even now. The great Buddha, what people in the West call the Buddha, and in India weburg. But he, for example, gave his first teachings in a place called Sarnath, actually in Benares, in my city, about five, six kilometers from where I was born. And I used to go and play in the so-called Deer Park, a park, sort of a nice park area, where he gave the first teaching. He used to treat it as a picnic area and as a child I used to play there. And Kabir, there was another poet, interesting poet saint, poet philosopher named Kabir, who literally came from exactly where that little township in the city where I was born. So I always be, I always get a little chauvinistic and say, I must have some dust in me. Anyway, so that's about bragging about the city and talking. But I'll use that as a context to explain support vector machines. Through the river, through this ancient city goes a river. Like all great civilizations, it's remarkable that most civilizations are founded next to large water bodies, to rivers, to oceans, to lakes and so forth. Chicago is next to Lake Michigan, I suppose, and so on and so forth. We all live next to the sea. So this ancient city is next to a famous river that in the West we call the Ganges and in India we just call it Ganga. And many of you obviously have visited it or had a dip in it and so forth. It's considered of great relevance. So this particular city great relevance. So this particular city has houses, let us say, houses, I'll just mark the houses with a triangle, on one side of the river, on the riverbank. In fact, if you Google up any picture of Benares, you'll see something quite remarkable. One side is absolutely densely populated with houses. They are called the ghats. The one side, this side is absolutely densely populated with houses they are called the ghats the one side this side is the heart right and densely populated with lots of buildings and even back down it is all buildings buildings buildings buildings buildings and more and more buildings so it's just this area you can say roughly speaking is like a non-agricultural, like they are housing of some form, housing or commercial or something like that. Everything is here, people live here. In other words, this site is the, let me use a more appropriate term. This site is the population center. On the other hand, remarkably, as you can imagine, population center tend to be expensive real estate. That's the lesson we learned from the California housing in our last lab. And of course, we know it from practical experience. On the other side, you find trees and just plain land and agriculture. So I'll just draw some trees, but actually not even trees, they're shrubs or just fields, agriculture. Re-culture. This side is agriculture. Culture and terrible spelling, but okay, agriculture. And this forms the context of our setting the stage for understanding SVMs. So the problem statement is this. A geographical point. X. So this is your x1 and x2, let us say. You pick some point and let us say that you pick a point either here, here, here, you need to determine in what is it is it a population center or is it agriculture agriculture land or are you in the middle of the river right so that is the so does it look like a classification problem guys we'll treat it as a binary classifier we'll say can I build a house here or do I grow food here? The question is, build or find a house here, a house here, or is it agricultural agricultural land. Land. That is a question. So it's a binary question. So we are going to classify between house and agriculture. This is our target space, isn't it? Let me write it in the way that we write. The categorical variable, the space is made up of house, agri, these two things. And we have to create a classifier that will take, given x, we have to do a classifier fx that will be, that will take you to either a house or an agricultural land. So does this look like a typical classification problem, guys? Yes. Right? Yeah. Yes. So by the way, there's a little bit of a curiosity as a physicist, this may invite you, that it is a remarkable fact that if you look at rivers throughout history, you always find civilization or population centers only on one side of the river, not on the other. Well, things have changed. For example, if you go to the river Thames in London, you do notice that there are embankments now and there's equal population centers on both sides. So the river seems to be going through a population center. But without those human engineering efforts, if you look traditionally as how rivers have been and civilizations have related to it, you always find population centers on one side and agriculture on the other, or you plant land on the other. Now, I'll pose you guys a a puzzle why do you think this is true it has nothing to do with machine learning by the way but i don't know if you ever observed while you're traveling to throughout the world that population centers tend to be on only one side of the river unless i say if i think i think one of the reasons is the floods. Because I live on the other side, on the flood side. Yes, but why are the floods there on one side but not the other? I think it's because of the gradient or the way the water flows. Gradient is there. So one side is steep, the side of the population center is steep. On the side of the agriculture, it will be gentle slope, right? And the water easily rises up the slope. But why do rivers behave this way? One bank of a river is steep, other bank has a gradient, slow slope, and so tends to get flooded more. The rotation of the earth erosion over time come again erosion over time erosion over time sort of yes it is part of the answer okay rotation what direction it erodes yeah rotation of what they're getting very close rotation of the earth so why does it erode in one direction actually so i'll give you the answer it's very beautiful actually there is a force in the force that is created a form of a second force it is called coriolis force it's uh the coriolis forces cause the rivers always to drift one way in the northern hemisphere have you noticed that motor most rivers tend to go from north north west to southeast in the northern hemisphere quite often right or that as they move the rivers also drift they never remain at the same place they move over the hundreds of years now the way they drift they drift in such a way that they have actually drifted in this direction so when they drift in this direction ultimately you have the embankment but the place that they have left is used to be the bottom of the river. And that is first very fertile. And secondly, has a gentle slope. Which is why one side is just alluvian soil. For example, on this river, it's fertile alluvian soil. And you can do a lot of agriculture. So anyway, for the physicists amongst you, this was a fun so today i'm taking obviously a long-winded way back to our main topic lots of digressions that's what happens when your instructor is fond of the topic lots of digressions so all right we'll come to that can i add one great example of coriolis effect? Yes. So in Australia, in toilet, water rotates in other direction. Yes, yes. I thought that was a myth. No, that is actually true. What is a myth is that from the equator, right at the equator, five feet away, you can see it happening this way, and five feet away, you can see it happen the other way. The effect is not so pronounced so that you'll see, literally you can see it happening this way and five feet away you can see happen the other way around the the effect is not so pronounced so that you'll see literally you can stand on the two sides of the equator and see it happen but if you are sufficiently far from the equator and you have a sufficiently large body and you create a sinkhole in the body but then you'll see the Coriolis forces at work. So, but, or at least that's the way I remember that. It is a Coriolis force. Are you saying it's a myth? It's not true, Kate? Well, probably at the equator, it's like you say that it's overstated, but my son was in Ecuador and they had some demo thing that looked pretty convincing in the video but my husband said that they have had some way to rig it but i don't know yeah demos can research people want to picture their demos succeed so they always regret it no no the gorilla effect is correct the force is there because the hurricane and all those things of course is there i mean my own research was looking at coriolis force in the nucleus in the structure of the proton okay it's at the equator then okay thank you so all right so now the question is once again let's get back to our river suppose your river runs like this and i'll pose the problem like this imagine that it is pitch black because that's how a machine learning algorithm sees it x1 x2 right you have to find a decision boundary so that you can say one side is our houses and another side is agricultural it's trees Let's use the word trees even though it wouldn't be trees, it would be shrubs or agriculture. How would you do that? You would say that you know I am looking for the median of the river, the middle path in the river, right? A river is here. This is what I'm searching for. So now let's try to do that. Suppose you have houses like this and a tree like this, something like this. So think in how many ways you can flow a river, you can flow a river, for example, like this, and you see that it still fulfills your criteria? The Green River? Isn't it guys? You could flow a Red River because you can't see the river. You could also have flown a river like this. Now between the White River, let me give these names to the river. This is A, let's say this is B river, and the White River is the C river. From a data perspective, which of these seems more convincing to you. If you had a... You would say, why? Because you suspect that if you have seen this data, there is likely to be a tree here that is not there in the data, isn't it? And are likely to be a house here. You can sort of extrapolate and convince yourself that these narrow rivers that you can flow probably are unstable solutions. You take a different sample of the data, and you would be able to flow a different river through it. Isn't it? So if you had to choose which river is the best to pick, which is the best river you can flow through your data between the houses and the agriculture? Your best river would be, what characteristic would it have? What does C have compared to A and B? Broad. It's wide, isn't it? Is the broadest, is the widest river that you can flow through your data? Are we together? The basic intuition is if you want to be robust, take the widest river that you can flow through the data. So that is our first intuition. And this is how these ideas did it. So let us write it in formal terms we want to flow the river through the data. isn't it this is our intuition intuition to have a stable or good solution is that right guys is this intuitively obvious what i saying? So ponder over it for a moment and see would you agree with that intuition? Any questions? Is it more or less obvious? I hope so. So this, well you don't in machine learning, you wouldn't be, so these banks of the river are, you are searching for the maximal margin, actually let me write it in capital because it's a term that you should remember, margin actually let me write it in capital because it's a term that you should remember maximal margin hyperplane hyperplanes so this river this bank of the river let me just call it bank of the river one. Bank one of the river. And this would be, this thing would be bank two of the river. A river has two banks. So these are the two banks of the river. So you call them a bank is a hyperplane. Well, in our case, it is just a line. A line is a hyperplane using more formal language because in higher dimensions, that's what it would be. So far so good, guys. So do you see how this translates to this you are searching for those river banks that gives you the widest reverse right a widest river to flow through the any questions so far and then and then then the decision boundary the decision boundary is simply the median hyperplane, median part or hyperplane between the two riverbanks or if you want to use formal language is basically the path so if you think of yourself as a boatman a little boat so one intuition that i can think of is imagine that you are this little guy on the boat and you are rowing so you want to hit neither the tree nor the houses. Late at night, you don't want to get stuck either on this side or this side. What path will you take? The path that the boatman will row is the decision boundary. Is it looking simple and intuitive guys so far? So when you do that, what we have done is we have taken the first great milestone on our journey of understanding support vector machines. This particular algorithm is called literally by its name, maximal margin classifier. I hope the name is suggestive of what it tries to do. What does the maximal margin hyperclassifier try to do? It searches for the two hyperplanes that are as, that enclose a river as wide as possible through the data clear intuition guys right now here is the thing sometimes people confuse and say these are support this algorithm is support vector machines no no no it is the first step in the journey towards support vector machines but it is a little gets a little bit more complicated these houses that help you know that are hugging the and the trees that are hugging the hyper plane so the river banks the houses and trees banks are called what are they called they are called support vectors are called support vectors are together. I like to imagine, think of them as lighthouses. Houses for helping the boatman, the boat, navigate. boatman, the boat navigate. So in the sea we have these lighthouses along the coast that give you a sense of how to navigate, get closer to land. In the same way, think of these as lighthouses, except that you will find that there are no real lautaises. It would be the light shining in people's homes. What people do in this city is something actually pretty poetic. Every evening in the water you find these beautiful little earthen lamps, made in terracotta, baked clay lamps with lights on them. And this practice of putting these little lamps into the water goes back thousands of years. It is really evocative. Whenever I think of my hometown, that the picture of these beautiful lamps in the water comes. So, well, here we go. Think of these as lighthouses or lamps in the water, or lamps in the water, in the water edge. Are we together? So do you see that so far the intuition is very clear? If you look at this picture, you realize that if you are in pitch dark and you are a boatsman, and you see, let us say that you see one light here and you see let us say that you see another light here and you see a light here on the other bank of the river right you know that this side is agriculture agree maybe the the agriculture lights are green in color and these are more like more like amber in color or something like that yellowish in color I'm just tricking distinguishing marks right so they stands for houses house house you know that this is a house and you know that this is a tree or a shrub can you as imagine that you're in pitch darkness what is the river that you can infer as a boatsman would you agree that the river that you would infer would be something like that we take the color of, the river that you would envisage would be essentially this, isn't it? Where this data point and this data point and this data point are sitting. These are your support vectors. How many support vectors? You realize that you just need three points to draw your river. And therefore, you need only three data points to draw out your decision boundary. Your decision boundary is simply the path that the boatsman takes flying through the river. So ponder over it for a moment, guys. Make sure you get the intuition, the geometric intuition here. Is it looking easy so far? Imagine you're in pitch dark and you see this, maybe I'll color these differently, a yellow light. These are yellow lights and these are green lights, do you think that this is enough for you to be able to draw your maximal margin hyperplanes or simply put, discover the banks of the river and then ply your way through the center? A little bit of thought, anybody who disagrees or thinks this needs clarification any thoughts guys all right so if it is clear to all of everything go ahead were you asking something yeah I was saying like if there is a bend in the reward then no so we are going there it's a very good thought hold that in your mind at this moment we are looking at a very simple river a segment of the river and this is it a straight part of the river okay so uh then but we'll come to the more complicated situation that's why i said it's a first milestone right but in a straight river this is enough to do that and so that brings us to some very uh remarkable conclusions this algorithm which color should i use to emphasize the beauty maybe i'll use this This algorithm is robust and this is something that startles you when you see that it's obvious. Robust against outliers. Isn't it? So, for example, suppose there are houses way out here and there are green things way out here. Do you think they matter, all of these things far away from the decision boundary? Do they really matter? They don't matter, isn't it? This house and this tree, they do not influence these outliers, right? As we can see, outliers, as we can see, outliers are almost irrelevant, are practically irrelevant, relevant, terrible. went to the discovery, to the learning of the river path. In fact, it is the inline data that matters most. That is one startling observation. The other startling observation that you have is the model after that becomes so simple, do you realize it is a shockingly simple model? This shocking model, just a list of support vectors. So let's explain this word, support vectors. See, this is a point of data. This is a point in the feature space, isn't it, guys? So suppose I put a coordinate system here, x1, x2, you would agree that this represents a point, too, you would agree that this represents a point, this represents point x, let me just call it xa, a vector, this represents a point xb vector, and this represents a point xc vector, right? Every point in a Cartesian space is just a vector, isn't it? It can be written as vectors. So the word vector is therefore understood in this context. The next question is, what about support? You're saying that you need only three vectors to support your decision to flow the river like this. Isn't it? Your decision boundary is supported or proven with just these three vectors, just a small, just a very small list of support vectors. So you may have a data set that has thousands and thousands or tens of thousands of points, So you may have a data set that has thousands and thousands or tens of thousands of points, but your support vectors may be a very small list of a handful of points along the riverbank. Asif, if your green support vector was an actual outlier? You mean this one? No, no, no, you're not an outlier. So the assumption is that around the green point, there are lots of other green points. Right. But if it's very far out, let's say. Yeah, it is. See, outlier is invariant of the classifier you use. Outliers, outliers. the classifier you use it outliers outliers and you can discover outliers by the usual manner of outlier detection so detecting outlier is easy the hard part is is is that when you have outliers in your data how do you make your algorithm not be hijacked by outliers now if you remember for learn for linear methods outliers were pretty nasty, right? They would screw up the model and they would do all this outlier search and so forth, make sure outliers are not there. But one nice thing about support vector machines is they are very robust against outliers. They couldn't care less because they are searching or not being influenced by outliers. They're rejecting the outliers they are actually searching for those high value inline points a b c right that will help them support their decision boundary and points are vectors in cartesian space and thus the word support vectors. You see that guys? So what we just said, go ahead. I'll say just one question. So that the boatman rowing there, so that's the decision boundary, right? Now, if let us say pick a point between that line which the boatman is rowing and that other blue line, you know, the one on the right. Yes. That is still not, you know, the what is that called the green area. It is in the river. So how do we avoid that problem? No. So let me try to parse what you said. So there are two riverbanks, right? This is the bank, let me do this here, to the left and to the right. And the boat has discovered what the two is. So what we will do is it will first draw this line. To draw a line, you need two points in a Cartesian space, isn't it? Yeah. On a hyperplane, you need these two points. You draw the first line. Now you have, okay, yeah, this is an important thing I should mention. The other plane has to be parallel to this. So what will you do? You will start with drawing little planes parallel to this and see how far you can go till you hit a green tree. And then you'll stop, right? So you will stop here. The first green tree you make, you stop expanding your river and you say now i need to stop because now i've hit land isn't it and so the boat can see that and therefore it will draw these two big blue lines and it will take a path midway through this which is the boundary. So but to decide whether it is a house or a tree, it has to take that whole margin into consideration, not just that line, right? Yes, it has to take the whole margin. So what it does is, this is given data to you, this is training data. The light, the houses are burning yellow, let's say the trees are burning green, the light in the trees is green, let us say. And so they are well there, you're trying to learn from the data. So you're trying to flow the widest river you can through this data. So how do you do that? Learning is always iterative process. You flow a river through the data and then you try to make the river optimal, the widest possible. So that is the implementation of the support vector machines. And support vector machines, when they came out, they were when you try to implement it, it gets rather intensive computation. Because in a plane, you can imagine so many rivers that you can flow through it. For example, you could have one that's from this river. Obviously, that wouldn't have been a good river. It's not the maximum margin hyperplane river. The widest river that you can flow through the data. Are we together guys? Abhijit, you got that? Is your question answered Abhijit? You're on mute. So guys, let us recap what we learned because this is a foundation. First of all, the intuition is, we want to flow the widest river we can through the data because that would be a good solution it would be it would have low variance in other words if i take a different sample of the data i shouldn't end up with a different river and for that try to find the voiders so more technically so in my in my mind whenever i think of support vector machines i always think of river and the widest river you can flow. But that's not that that is a language unique to me. So don't open a textbook and expect words like river and riverbanks. They will use more formal terms. They'll talk about maximal margin hyperplanes and the decision boundary will be the path of the boatsman. That's how they call it. So this classifier is called maximal margin classifier. It is not the support vector machines algorithm, which is, we're going towards it. But confusingly, these points that help us make the model, so the model is just the collection of points on the riverbank. Those are called support vectors. they are vectors in a cartesian space and they support the decision boundary right so that is that now there are two problems uh that i have a question come again uh i have a question so they support vectors like three support vectors here right so like three support vectors here right so but they belong to two different categories of data yes right and the finding the widest river is basically finding the maximum gap maximum separator which separates the data of two different categories that is true okay and that's why it is computationally intensive because we can start with any river but finding the my widest one is the computation yeah actually the linear part is not that hard we can quickly get to it but when i get to the full support vector machines it gets hard harder even this takes some effort but as you move forward to the more complicated one the full support vector machines then it begins to get hairy and you'll see why sir but what if there are you know there are multiple categories like basically multi-class class for data. For the time being. So that is one limitation of what I've just explained. I will tell you how support vector machines overcome that. But fundamentally, the theory of support vector machines, very much like logistic regression, is inherently a binary classifier. Blueberries versus grapes and so forth. That kind of a thing. So here, let us think as a binary classifier. So it's good that you mentioned that. And we will see how these things are generalized or harnessed. Or any binary classifier can be harnessed to do multi-class classification. Got it. Thank you, sir. So far, so good, guys. Now what we do is, if we look at the limitations of it. And there are many limitations. One is that you can only tell between houses and trees. So binary is one aspect of it. But that can be easily addressed. Let's look at more complicated aspect of it. If you have lived next to the river and seen that rivers flood during rainy season, especially in the tropics, and nowadays, well, in the temperate regions also, we know about the floods in Michigan and so forth. So rivers flood, they overflow their banks and complicated things happen. So then let us say what happens when rivers flood. You find that they drag in. And again, I'm giving this narrative to just explain the concepts so they will they will wash away a few trees isn't it guys you will see a few trees washed away into the river so then the question comes suppose you just see two trees here but most of your trees are here most of the things are here what should you do would it make sense and this is a question i'll pose to you to redraw the decision boundary and say well you know uh fact of the matter is i'll use a different color here which color should i use Let's use this color. It's a pretty multicolored thing. And should this become your new river? Do you see this thing? Should this become the new river? I'll pose this question here. And let me ask this question. Oh, this is really multicolored. Should this be the new river bank? Bank basic intuition says that no, you know, sometimes you might find a tree or two in the river that is perfectly normal. The river is wider, its boundary shouldn't be determined by the first tree that you hit upon. In other words, you should allow for mistakes. And this becomes even more apparent if for example you realize that this particular green tree could easily have washed its way and come here right gone to towards the other side but then you get into even more problem because if you try to draw your river where is that i draw your river but now your river is beginning to look rather narrow do you see you're just left with this little bit and your basic intuition says that this one point this one tree in the that has washed into the river has essentially hijacked your model isn't it you somehow don't want it to make the decision of where the of where the bank of the river is. Would you like to agree with that? Is that intuition clear? Would you like to discuss or talk about it? Yes, sir. It is clear, right? If you come to, for example, Benares, you see it happens in the rainy season, Anil, isn't it? isn't it? Yes. And actually there's more complication if you are there. In cities like in Benares, there is such a thing as a houseboat. What it means is there will be little houses like structures, but they are actually sitting upon a boat. So they are triangles sitting in the water. Once again, and this triangle may, you know, this guy would have gone to the other side of the, well now this picture is getting rather busy, this, a few of these house boats are floating around on this or that side of the river, and once again, you don't want just a few of these to make big decisions. Ultimately, this is the population center, and just a few isolated points shouldn't make the decision. So what you do, and by the way, I'm explaining all this without mathematics. We'll go into the mathematics now. So one thing you would agree that perhaps, perhaps did I spell that right? I think so. Perhaps we should, we should allow for a few, a few contradictions or mistakes. Would you agree? Don't make your decision boundary. Don't agree on your riverbanks just because you met one or two points meet sufficiently many points before you are sure that you have hit the hit the riverbank with the population on it likewise on the other side meet enough trees before you decide that okay and now we are talking of the other river bank on the agricultural side are we are we together guys we are being more permissible otherwise our river would not be wide enough or would not even exist for example with this tree here and this houseboat and this houseboat here you realize that it's practically impossible to find any decent decision boundary, linear decision boundary, right? You will end up with a no solution situation. So you need to be a bit permissive. So we need permissiveness here, permissive. But then how permissive? It can go a bit too far, right? You keep meeting house after house after house and you can't just keep saying, no, I'm still not in the bank, I'm still not in the bank. You realize that that becomes rather counterproductive. At some point you have to accept the reality and say, well, now we have hit the riverbank is that common sense so we need a certain threshold right and for to do that what you do is you set up a cost you you set up a penalty so the two steps you do is for each mistake, you set up a penalty, epsilon i. Suppose it is the ith point, you set up the penalty. And then you put a constraint saying that sum of all penalties has to be the sum okay i use epsilon for let me you because the symbols are beginning to look rather surprisingly similar epsilon i the summation over i of the epsilon i in other words the sum total of all the penalty c is equal to this and constraint that want to, this is sort of like a cost. Total cost of penalties you can afford, right? Cost of penalties. And then you have a budget. Then budgeted penalties then have a budget say that you can afford only c greater than or less than equal to some particular value value. Let me just say c, well let me just say this is a typically the word c itself is used. So let me use little c here, little cost c. This is common. So you can say that your little c has to be the budget. Why don't I use the word budget? People use the word see in the literature. So I was using see. Let me use the word some, some of all penalties, right? Some of all penalties. So then the sum of all penalties must be little less than your budget. So the language actually in the support vector literature is a bit at divergence with what i say they will call this c as the cost right but intuitively it is more the budget that your budget for mistakes that you're allowing so that the sum total of your um mistakes or the penalties do not exceed the budget you have. Right? So I suppose this is literature. I mean, it's one way of writing it. And I used to find it a little worrying in the beginning when I was trying to teach myself this thing years ago. So we came across our first constraint. We should do this. Cost of penalties is this. And this is in the language of mathematics, you call it a constraint this is a constraint right and what we are going to do in the let's take a little break and then we are going to learn that this entire problem this is this entire problem problem is a classic problem is a classic constrained optimization problem. Constrained optimization problem. By the way, guys, do not be scared with these big scary words. There's nothing to it. These are very simple, very obvious words. And when we develop the theory, you will realize that actually this is just common sense written out in symbols. And we'll do that. So, so far so good. Before we take the break, can we review and make sure that we understand maximal margin hyperplanes? Can we review and make sure that we understand maximal margin hyperplanes? Maximal margin hyperplanes is the search for the widest river that you can flow through the data or search for those two hyperplanes that will give you maximal separation between the two classes, houses and, let's say, trees. The decision boundary will be the median going between the two hyperplanes and so is itself a hyperplane one one good quality about the support vector this maximal margin hyperplane says it is really this class of algorithms is very robust against outliers outliers become irrelevant actually very very nice the other thing is that's nice is the model is so simple you just have to tell which are the support vectors that help you that make the decision boundary or you know which vectors are points points are vectors which points list of points basically that specify uniquely the decision boundary and the hyperplanes, the riverbanks. You can say it in whichever manner that you want to, but that's about it. That's just the gist of it. Now, the trouble with this kind of a thing is real life situation is data has noise, it has mistakes. And so you don't want to be hijacked by all these mistakes and noises. So you want to define a penalty term for each mistake and then sum all the mistakes and then say that you have a certain budget for mistakes. You should not exceed that budget. Otherwise, your solution will become either very hard to find or very unstable. You take another sample of the data and your river will look a different way. In other words, it wouldn't be wide enough in simple intuition terms. Are we together? So therefore, we have a constraint. All of machine learning problem is an optimization problem in the sense that you take a loss function, you decrease it and so forth. That's a mathematical language in which we encounter all this intuition. So let's take a little break and after that we'll come to that, by developing the mathematical language. We have been talking for an hour. Take the time to think over it, digest it. I will go down to drink water but you guys continue discussing with each other if you want to see if you understood it and then when I come back I'll field questions this moment I'm going to pause the recording So give me a second. Here we go. So pause record. I'll see you guys in 10 minutes. 10 minutes is enough for 10-15 minutes. Today we started a bit late. Should we keep it short to just 10-15 minutes? See you at 8.45? Yes. Okay. Yeah. like should we keep it short to just 10-15 minutes see you at 8 45 yes okay yeah Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Are we all back? Should we start now? I'm ready. Excellent. Let's get started. So we will now take the simple maximal margin hyperplane or maximal margin classifier, given the intuition is here with houses and trees and write it in a bit more formal language. So first thing, of course, we already talked about that points are vectors in a Cartesian space in our, in this case, our square space here, the plane in this case, our square space here, the plane of a river. Every point is a vector, so long as you set an origin, you set up a coordinate system. And we are searching for those vectors that help support or discover the decision boundary by helping you draw the maximal margin hyperplanes in a more formal language, or more intuitively help you flow the widest river that you can through the data set. The second thing we realize is that sometimes it's not so easy to flow a river through the data set because there are points that you can think of as encodes mistakes. A river can wash away a tree and it can be floating in the middle of the river the river might have house boats right both of these are actually true in the narrows and by the way how many of you here have actually been to my hometown is there anybody I did of course you have. Anybody else who has been? Pradeep, you have been. Who else? I've seen it. So you must have seen firsthand what I'm referring to, isn't it? So these mistakes, you set up a budget of how much mistakes you're willing to tolerate. And the sum total of all the mistakes, you attach a penalty to it. And you say that the sum of all the penalties must be less than a budget for penalties. That's your constraint. And so this problem we will realize is a problem in constraint optimization. Now, may I just advise you that as a background, you should go watch a video of mine Now, may I just advise you that as a background, you should go watch a video of mine on YouTube about what is called a distance function. I will just today briefly talk about it because we have to cover a lot of territory. But there is, if you go to support vectors, let me mention that video. Yeah. And share a different screen for a moment. Share the main screen. So guys, if you are seeing this YouTube channel, you will find the recordings for the introduction to data science. Do you see that? Interestingly, the very first video, yeah, the first video that is here, linear classifiers the distance function. It's a very short video of 16 minutes, and it will help establish or help you understand something that today I will just state as a fact. I'm reading it as a fact because we won't get the time to derive that but I'll just mention it as a fact. We need that. What it basically says, let me show you the beginning and end of this video from that and let me share the sign. When you are in a situation like this, look at this. You have the blue points and the, what is, what color would be it? Orange-ish, not quite orange-ish, but okay. This pinkish color. All right. Let's go with pink. You have this blue, and let me cook up some silly words. Let's say that these are blueberries and these are pink berries or cherries these are cherries right and x1 is the weight x2 is the size cherries are a little bit bigger and heavier and then blueberries are a little bit smaller and lighter and so you want to distinguish you you want to try to classify between blueberries and blueberries and cherries. What would you do? You can intuitively see from this visualization that there would be a decision boundary that goes sort of diagonally like this as I'm moving my mouse. So how do you discover the decision boundary? We go into the theory of it. Then we say that what is your basic intuition? Suppose you have a decision boundary. How do you decide a point to be blueberry or cherry? You look at the distance from the decision boundary and you take one distance to be positive. Let's say distance above to be positive from this, distance below to be negative. And so the more positive the distance is, the more it is a cherry. The more negative the distance is, the more likely it is that it's a blueberry. And if the distances are small, it becomes uncertain because you see a little bit of a noise mixing up along the decision boundary. The decision boundary is where it is unsure. Like you don't know whether it's a blueberry or a cherry. 50-50 chance. So that's your decision boundary. So then I go through the derivation. So if you take an arbitrary point, I'll go straight to the end. What we say in this video is that, let me say here. Yes, so the net point that I say is that if you take a unit vector perpendicular to this plane, which is w, so any point x in this feature space, the distance of that point to the decision boundary, which you need to determine, you know, whether it's a cherry or a blueberry, is given by, through a mathematical derivation, is given by the dot product with the unit vector w, the orthonormal vector, and x bar, the x vector, and a bias term added. The bias term is actually negative of the distance of the hyperplane to the origin. So from the origin, the shortest part to the hyperplane is minus w naught. So that's part of the shortest part to the hyperplane is minus w naught. So that's part of the derivation here. That distance is minus w naught. And so whenever you see an equation like that w naught plus w hat, etc., what you're talking about is the distance from the decision boundary. So this particular result, let me restate in our note, and we will use this to make progress so with that so here we are so let us start with that. I am seeing... Has somebody recording? Oh my goodness, yes. I did not start the recording. Oh goodness, should I restate everything I said? Would you like me to just summarize it again? I will just go back to the previous screen for one second and repeat what I said. Let me go there. So what I said is that if you have a decision boundary between the blueberries and the cherries, here you see it with this greenish line, then how do you decide if a certain point is representing a cherry or a blueberry? The intuition is that if you take this orthonormal vector w, the blue hat, and you say that the further along you are in this direction, based on your distance from the decision boundary, this distance being positive, this distance being negative, based on how far you are from the decision boundary, you can tell whether it's a cherry or a blueberry. A cherry, if you are far, a huge positive distance, very sure there's a blueberry. If it is a huge negative distance on this side. And for small distance, there's a certain degree of uncertainty. So that brings up the usefulness of a distance function. How distant is a point from a decision boundary? So we go through a derivation and we find that the distance function can be represented as this term. It is a dot product of the unit orthonormal vector, w, with the point itself, along with a bias term, w naught. The w naught is actually the negative of the distance from the origin to the hyperplane. So minor distance needs to be moved to go through the origin. So you have to move minus w naught distance, in other words in this direction, for the hyperplane to now become, still remain parallel to itself, but go through the origin. That's when we are thinking. But the net result is this. The distance from the hyperplane of a point is given by this term. Two factors are affected. The orthonormal vector of the hyperplane and how far the hyperplane is from the origin. Now the orthonormal vector, why do we give it in terms of orthonormal vector, hyperplane's definition, and its distance from the origin? Because it is something that works for higher dimension. For a line you can go with the slope, but what do you talk about when you have a hyperplane in high dimension? It turns out that you could actually use every hyperplane in whatever dimension has an orthonormal vector, and every hyperplane will be a certain distance from the origin. And these two things uniquely define a hyperplane. So we will take these results, bring it back to our current pursuit, and see how we can quantify all this intuition of flowing a river through the data into a more mathematical language. Are we together, guys? So here we go. So let me bring back that intuition. I'm saying that, let me draw in the same way that it was drawn here. Suppose this is a decision boundary. Boundary, we are seeing that a given point X, X, and let's say that this is the origin. And from the origin, you have a the shortest path from the origin to this is this will be of course 90 degrees. And this is minus unit vector here is w hat unit vector. Actually, I will use the notation that is in your book in the book, textbook, they tend to use the word beta not beta rather than w. So I'll just omega rather than. So I'll use beta here, unit vector. a unit vector unit vector is a vector whose size is equal to what is the magnitude of a unit vector one right so that brings us to our first constraint it is a constraint on the B the constraint on the betas because beta is nothing but beta one, beta two, depending upon how many dimensions there are. Suppose there are d dimensions, let me just say n dimensions, n dimension, then there'll be an orthonormal vector in the n dimension space where we are talking about, let us say, r to the pi, the r dimension, Cartesian space of n dimensions. And so we came to the conclusion that if you are actually, let me take a point a little bit here. More x is here. You're seeing that from this point, if you want this distance, this is the distance of x from the decision boundary, this is given by the result that we had, distance of a point from the decision boundary is beta naught plus beta Isn't it? The distance function. And if you understand that we have gone most of the journey in now formulating this problem as a river problem, flowing the widest river through the data, let us say that let me bring back your yellow points, a few yellow points, and a green point here. Where's my green point? I'm going to get rid of this green point here. So suppose we flow a river through this. This is our decision boundary. Are we together? Now you ask this question, this vector or this vector or this vector, you realize that they are is the same distance, let's say m, let me call this distance m. Would you agree that all of them would be the same distance m from the decision boundary? So this is your boat. Reminds me of a Greek mythology in which you are between, what is it, a rock and a hard place. It used to be Psyche or something like that. Anybody remembers the Greek mythology, those two monsters in the sea who were waiting to devour you and your ship? Scylla and Charipnus. That is right. Thank you. So it's like that, you know, you want to silence that is right thank you so it's like that you know you want to avoid both the banks otherwise you'll capsize your boat you want to go through this so you'll be equally distant from both of them so you agree that all this so from this we derive that the support vectors in the simple case the margin the points on the margin case the margin the points on the margin margin are given by D effects is equal to M are we together? This is very obvious, isn't it? Because this distance is, after all, d of x here, distance from the hyperplane. This looks obvious, isn't it, guys? Please tell me that it is. Yeah. Yes. Let me say that the direction of this river, which direction that you're flowing it, is given again by an orthonormal vector of beta hat. So our other constraint is beta hat size is equal to one, which means that beta one squared plus beta two squared, beta n squared is equal to one, right? This is our constraint number two right and now let us be a little bit more clever let us say that we call these points the negative points houses are you know as a target variable when you give it you can say y is equal to minus 1 and here for trees y is equal to plus 1 right as a y minus 1 plus 1 why not 0 1 why not just house entry you'll see it's a mathematical convenience you'll see what it is and you'll see it shortly so you realize that for trees for all tree points let me mention this as a crucial observation, for all tree points, points, what would be the constraint? dx. They will be at a positive distance. Let's say that this distance is positive, this distance is negative. dx for a a tree for any tree point x tree is greater than equal to m isn't it positive yeah and at the same time distance of any house point is less than equal to m would you agree guys right they would be minus m sorry less than equal to minus this crucial minus is important right so now if i write x t as a x minus or x house as the positive distance or we don't need to do that now these are two different constraints isn isn't it guys? You would say that find me a river says that this constraint is fulfilled for tree points and for house points. It's like this. So there's a clever mathematical trick. If you multiply this with the Y for tree, what is the Y for tree plus 1 Y tree DX tree let us multiply now this happens to be 1 plus 1 and so this constraint will remain if you multiply this equation with plus 1 it will remain this on the other hand Y house is equal to minus 1. If you multiply it by dx house, what happens? When you multiply an inequality with minus 1, what happens to it? The sign reverses. And so this will also become like this. Do you see this, guys? And so it's just a clever mathematical trick. It's just being clever with the symbols and so you can write it as a simpler equation which says that for all points in the data you want to find that decision boundary such that irrespective of whether it is a house point or a tree point, this constraint is fulfilled. Do you realize that these two become exactly the same equation? So this is our next constraint. This is without penalties. Without penalties. So if I were to write it down, let us write all the three constraints. So summarizing for, and I will use the word hard margin classifier. For that maximal margin, or maximal margin classifiers when we don't allow for mistakes when there is no allowance or budget for mistakes. So we are talking about the hard margin. For hard margin classifiers, that is for the definition of hard margin classifier is these are those maximum margin classifiers where there's no allowance for mistake the opposite of that is sidebar Soft margin classifiers. This is terrible. I'll write it in the next sentence. Classifiers. Budget forest. Yes, are the maximal margin classifiers. By the way, is my handwriting fairly legible guys when you later on review your classifier where the budget C for mistakes is greater than zero. Zero. Right? It's not zero. It is some value. It is some value that you set. Typically the values could be 0.1, 0.01, one, two, four, a certain amount of. Typically you do it in powers of 10, this budget. So we're talking about hard margin. For hard margin. For hard margin, margin classifier, we realize that our constraints are that y i distance x i is greater than equal to m some m isn't it guys at the same time you you also say that the beta the beta vector beta had the unit vector is equal to one because that goes into the definition of d in other words i.e beta one square plus beta 2 square beta n square is equal to 1 right this is the second constraint right let me just say equation 1 this is one, this is equation two. Are we together? So any kind of river that you flow where these, and by definition, as we saw, dx is equal to beta naught plus beta one x1 plus beta n xn, right? So I'm just trying to map it to the notation of your book, right? And so you could rewrite this whole constraint equation as saying that this thing therefore can be written as y i d x i can be written as greater than m is the same as writing yi beta naught plus beta 1 x1 plus all the way to beta n xn is greater than equal to m, right? This is obvious, isn't it, guys? It just comes from the definition of this. So in your textbook, actually I've mapped it now, this intuition to your textbook. So your textbook, this is by the way, chapter nine of your textbook. The reason I mapped it to that notation so that it becomes easy for you to go and review this stuff. Actually, this derivation is not mentioned there. And it is my sort of a crazy way of deriving it. you to go and review the stuff. Actually this derivation is not mentioned there and it is my sort of a crazy way of deriving it or looking at it from a more intuitive perspective but I hope that once you have understood it you can read more formal books and get the intuition. So this thing you find innumerable videos on support vector machines and on maximum margin classifiers and so forth. They all talk of the orthonormal and they sort of go a little bit fast because I suppose this is understood usually by people who really care about it and they have a lot more mathematical training sometimes. But I thought I will go through the entire derivation for you here. Now comes the next question, the soft margin classifier. By the way, can I ask you guys one thing? It's 9.15. In 15 minutes, it would be 9.30. Can I take some more time today? We started a bit late. Is that okay with all of you? Can we stretch? I'm fine till 10. Yes. So 10 is a hard constraint. All right, let me try to respect that. So I love to drop off at 9. So now the only difference is yi, you remember the times the distance, the distance from the decision boundary must be greater than equal to m, but in some cases for some points you allow mistakes. Are we together? So for every point, data point you have a certain penalty factor. You hope that most of the penalty factors are zero and some of them are non-zero. Are we together? But for those terms you allow a decision boundary smaller than m. In other words, suppose there is a river here. This is a point, this is a point, this is a point, this is m. You allow for this point. This is less than m distance. Let me call this mistake q. You would agree that let q be a mistake point, isn't it? A mistake point. Then for q, you would agree that q is the q the q dist the q point a distance of q is less than m isn't it it is not as far this is m this is some small distance a distance that is less than m so that is why m minus some amount right right? So you define whatever it is. The epsilon Q is m minus dQ. This is the mistake that this guy made. Do you see that, guys? How much penalty or how severe is the mistake is based on this. So far, it's a good thing. Can you please repeat how you wrote this one? EQ is equal to... So the q, this point is q, let us say. Yes. So the distance of q from the, this line, whatever it is, it is not as much as the distance of the river banks from the center, from the decision boundary, isn't it? So what's the dq is? It is less than M. Right? And how much is the mistake there is? A Q should have been here. It is instead here. It should have been ideally here, but it is here. So this is the mistake EQ or the penalty that you're allowing for, the softness that you're allowing for. Got it. Thank you. That's what I'm saying here and so that is essentially the point that we are making here and so you allow for some mistakes and then the rest of the equation so again to come back to it what are the basic rules you want to say that for all points dxi is greater than equal to m up to some tiny mistakes that each of the points can make most points should make no mistakes some point can make but then there is also the constraint the summation of all the epsilon actually let me use the other epsilon symbol so it doesn't look like summation symbol the sum of all the mistakes should be less than equal to a constraint isn't it guys this is and finally the one obvious statement is a beta hat is equal to one i.e beta one square plus beta two square plus beta three square beta n squared is equal to one, right? So these three, so your root is that this is just a small adaptation over the hard margin classifier, right? This is it. So these are, so what you're saying is give me a decision boundary that respects these. In the hard margin classifier you're saying give me a decision boundary that respects these in the hard margin classifier you're saying give me a decision boundary that respects this uh by definition where are we okay that respects this right now the machine learning algorithm has to just go creep over the feature space using some clever means and gradually a flow to find the best river that it can flow which respects those conditions for those two different algorithms. Are we together? So now we took care of mistakes and that soft margin classifier is it effective? Very effective guys. But there is yet another problem with reverse in our model. While for the simple case it works, in real life, rivers tend not to run straight, do they? Right? Rivers tend to meander, right? So let's use that intuition. Now let us say, but rivers like this right more formally Oftentimes, the decision boundary is non-linear, is a non-linear surface, hyperaces, isn't it? I'm using fancy language. But I hope, guys, whenever you understand things, remember that if you bring intuition to mathematics, you'll realize that all of mathematics, and certainly all of machine learning, is saying something very obvious, right? You're just writing it more precisely, the obvious statements more precisely. That's why I always say that math is the easiest subject in the world. Coding is much harder than math. Everything, as I have spent my life, as you know, I went through IIT, engineering, electronics engineering, and I did theoretical physics, mathematical physics. I did a lot of math and a lot of physics, did computer science, right? And all of that, hardware, software, everything. But from my experience, I've never found a subject so absolutely obvious and easy as mathematics. The trouble with mathematics is how do you see it the right way so one picture that somebody you know in the social media people keep forwarding pictures one recent picture somebody forwarded I don't know who be interesting it just showed that there was a picture of a big stone that mysteriously was floating in the air big rock it was floating in the air and the rock. It was floating in the air. And the thing is, you're puzzled. What is going on? Why is it not falling to the ground? It just is levitating on its way. And you stay puzzled for quite some time, till you invert the picture. And when you invert the picture, you realize that, well, actually, the picture is in the water, the picture is in the water the rock is in the water and what you thought of as sky was actually the water and what you thought of as the ground was actually the sky and so on and so forth so a lot depends on perspective and mathematics is always all of machine learning theory goes boils down to if i may say so and go on a tangent for a moment, it is all the search for that perspective which makes the problem look trivial. In mathematics, that's why mathematicians often use the, will say statements like that, this and then this and from this we prove this and from that, as you can see, this happens and therefore it becomes trivial. So when they use the word trivial, they mean it. What it meant was not trivial in a derogatory sense, but through a chain of reasoning, they made the problem equivalent to something that is self-evident. And that is the pursuit of machine learning. When you do that, think of it like that, that every algorithm you have to look at it in a way that makes it look obvious. Support vector machines are notoriously hard for people to understand. Most people in fact do not understand support vector machines. You can try it if you have data scientists in your workplace, walk up to one of them and say explain to me support vector machines. Explain how they work, explain the theory of it and you'll get blank stare quite often. Some one or two geniuses might be around but people find it hard. But in my view, it is hard because you don't look at it in an intuitive way. And always try to relate things to stories like for example, this river metaphor that I am using to carry this learning forward. Use something like that from your real life. It helps. Then it becomes very real and easy to get the intuition out. So we'll do that. So suppose you have a nonlinear decision boundary. What do we bring to our rescue? We bring something that we learned in ML100. Remember, we learned that every, we recall that every, we recall that every curve can be linearized in some higher dimensional space. So I have a silly analogy since I'm giving so many analogies. You know, even the most wicked person is absolutely perfect in the eyes of his or her mother. I'm pretty sure that Hitler's mother must have been very fond of him and so forth and you can think of all sorts of villains in history but in the eyes of the mother the child is always perfect so I always say in a very silly analogy that every curve and therefore is generalization a hyper surface there exists a higher dimensional mother space in which that curve becomes linear and I'll sort of illustrate it with an example that you have seen so suppose you have a parabolic curve like this right so this is some you can write it as an equation as some beta naught plus beta 1 x plus beta 2 x square right there is one bend in this curve so you realize that maybe a quadratic equation would be good enough right and your high school algebra tells that indeed it should suffice if it is truly a parabolic thing now how do this is two-dimensional right this is r2 how do i linearize this in codes you go to a new space, which is three-dimensional, right? So this is y, x, which is three-dimensional, in which the axes are x, and this axis, let me just call it x1 is equal to x, x2 is equal to the square of x, and the third dimension is y and you will realize that if i write it as in that form this equation becomes y is equal to beta naught plus beta 1 x1 plus the beta 2 x2 isn't it in r3 in r3 and this is the this is a linear equation this is a equation is as trivial as that it's a linear equation a hyperplane a equation. It's as trivial as that. It's a linear equation, a hyperplane. In fact, it's a plane. It's not even hyper, but I'll use the word a plane. It's a plane. Actually, I meant to put the word hyper here, but here. Actually, let me just leave the word plane. It's a plane. It's a plane. It is a plane. A plane is flat, isn't it? It's some plane. I don't know. Some plane here. It's an equation of a plane in a higher dimensional space, which is three dimensional space. And this intuition carries forward. And this intuition carries forward. There are two kinds of curves. Curves that can be represented with a finite dimensional polynomial. And curves that cannot be represented by a finite dimensional polynomial. So they will need an infinite dimensional polynomial space. But polynomials are not the only basis functions. You can use sine x, you know, the Gaussians and so forth. But these are the things you will learn in the engineering math class that is coming up. So we won't go into it but just to give you an intuition that you can build your axes out of any kind of functions and then expand or write your curve or imagine that your curve or your surface, your decision boundary is a finite dimensional expression in that basis function, you can get away with it. Or sometimes you literally want infinite dimensional spaces. So you can take your data, and this is hard to imagine, how do I draw a line in an infinite dimensional space? It seems rather impractical. There is no pencil that will help you do that, of course, but in mathematics you tend to generalize with abandon and say, well, I can still imagine that. So we'll do that. Imagine a higher dimension space. So now what has that to do with this? Well, the statement is very Since any curved decision boundary, well, let me use the more formal term, non-linear. What is non-linear is therefore curved, right? What is this? Non-linear, or in simple terms curved decision boundary boundary any non-linear decision boundary is straight in some higher dimensional space or let me call d, which is a high dimension space. Now think of the river. So what it means is that our meandering river, this river that sort of goes like this, this in r2 becomes what? It becomes a nice, simple, straight river. Do you see that guys? It becomes a straight river in some higher dimensional space. And so if that is true, then all we need to do is go to that higher dimension and use everything that we just used, the maximal margin classifier technology, find the support vectors and come back. Am I making sense, guys? You're getting the intuition. So if in some higher dimensional space, the river is truly straight, then all I have to do is have a magic manner to go to that higher dimensional space and then find, solve the problem there, just the way we learned to solve the problem. This two-step process, right, A, two steps. A, go to a linear high dimension space B, then bring back your maximal margin classifiers. back your maximal margin classifiers. These two together is essentially the gist of the algorithm support vector machines. This together is the algorithm support vector machines. But it leaves a fundamental problem unanswered. How in the world do we discover this space? Well, a given dataset, isn't it so it is very easy to say that go to that other mother space in higher dimension where the river straightens out but how do you get there how do you discover that space that problem remains there right do you guys understand the situation yeah right so the answer to that is actually very very elegant and it is in fact the last insight that vladimir vapnik got so i will state it for you in simple terms i'll just sort of give you the intuition what it says is that a lot of problems in machine learning can be formulated as dot products. Right. Dot products. This dot product. A dot B. Right. So imagine that there is a vector. You remember that a vector is invariant of what reference frame you look at. Right. And if you have two points, how close they are is given by a dot b. Now, it turns out that the nearness is preserved as you go to higher dimensions. Close points, I mean close, right? Now, I won't go into the full mathematical specification but i will just say that a point can be defined using its dot product with its neighbors so if i say that there are these three points and the dot product with one and two and three i give you you would have no choice but to place the point here because you have written a set of equations of dot products of that x.a is equal to something, x.b is equal to something, x.c is equal to something. When you solve for this equation, you will end up solving for x. So you can actually specify a lot of things, reformulate a lot of things in dot products. Now, why are dot products interesting? It is the so-called kernel trick. And I'll give you the intuition of it. What it basically says is that if you want to solve a problem, like find the decision boundary and you can specify the decision boundary as a dot product, then you have to do a dot product, then you have to do a dot product in the higher dimensional space. So there are two steps involved. A, go to higher dimensional space. B, do the dot product. And C, kernel of the dot product, let's say kernel of the dot product. Kernel means any function, any function of dot product is a kernel. Are we together guys? Kernel is the word that is reserved for that, functions of dot products. And now here comes the magic kernel dot product come back with results and the kernel trick is this sometimes you can do the dot product in the higher dimension without actually finding all the details of how to go from here to there. It's a very interesting trick. You know there's a higher dimensional space. In that higher dimensional space, you need the dot product. You can actually get the dot product and your functions of dot products without necessarily finding explicit mapping or translation rules from this space to that space. Now that's a little bit mathsy. I'll give an extra session for this beautiful math actually. So it's a very elegant math and some dual space and so forth. But we'll do that at some point, right? It's an extra session, certainly not beyond the scope of this class. And when you do engineering math, we will go through this carefully. And so the support vector machines became powerful and Vapnik realized that he had to hit upon something when finally the last piece of the puzzle fell in place and he realized that well every problem can be linearized you can use you can flow a river through the data and what is more the process of going to that linear, you don't need to solve the whole problem. You need to solve only a small part of the problem, so long as you deal with dark products. And thus the technology of support vector machines. Support vector machines is one of the simpler kernel machines, right? The scope of kernel machines is huge. Next time we will use kernels to do nearest neighbors, kernel nearest neighbors. We'll talk about dimensionality reduction. Let's keep those things for next time because I noticed that we are running out of time today. But this was guys, the theory of support vector machines. Did you understand it guys? Or did you understand most of it anybody found it interesting yes yes mostly got it yes so i said this is a widget so just one point here there so here the what is the kernel here so kernel is basically sort of a a function which translates from the lower space to higher space kernel of just think of some things that x i x j the dot product function roughly speaking they are functions of dot products okay but actually in an intuition form it is basically translate helping translate uh the those points in the lower dimension to higher dimensions they are functions mathematical functions kernels are mathematical functions that help you give something for example distances the beautiful thing is if you can formulate your problem as a kernel problem then the translation rules from this to high dimensions, which may be bizarrely complicated, you don't even need to worry about it. You can sort of get to the kernel faster than you can get to solving all the rules of translating from here to there. Okay. And that shortcut actually saves you a lot of computation and makes this whole thing very very practical and relevant see the first time you see the kernel theory come together it almost feels like magic wow you can actually do that okay and you can imagine the tremendous uh insight that is embedded here and how vapnik must have felt in 1960s when he came up with this theory he sat upon it for almost 1960s, when he came up with this theory, he sat upon it for almost, what was it, 20, 25 years before it finally got recognition. But when it got recognition, people knew right off the bat that this is fundamental, this is a breakthrough. The time for recognizing that algorithm had come. It had been discovered way ahead of its time. We use kernels everywhere, even in deep neural networks and everywhere. So we live in a world where kernels are taken for granted. We will delve into kernels much more in the audience at this moment. This is a practical methods workshop. So we won't delve more into the kernels, but certainly in the engineering math, the math behind data science class, this will be a hot topic. We'll do it in depth. And we'll do the whole derivation of it in depth. When is that happening? Pasif, sorry. That is something I want you guys to make a predictive model for, predict the day. Okay. I don't know. See, we are living in the coveted world it's a new it's an ensemble we have to do an ensemble of all these people voting machines you know yeah you guys can all guess and see where the guest lands if i have to guess it will be see it depends upon when i have enough people maybe i should open the registration and say we'll start when we have enough people yeah let me do it like that it is actually it's a surprising thing one would think it's an abstract thing but i think of all my courses the boot camp and the math class the math of data science are the most popular they usually go housework some of you have how many is there anybody who has attended my math class? The math of data science? I have. Did you like it? Did you guys find it relevant to what we are doing and talking? Yes, that is actually the ABC of machine learning. Yes, it's the heart of it. So as of here, we are finding dot product and coming back and what are we doing again they can find the dot product see i did a lot of hand waving arguments i hit a lot of mathematics but what i'm basically saying is you can find the decision boundary using some clever mathematical tricks in higher dimension and solve your problem. Right. Without actually knowing explicitly how you went from here to there. Yeah. Okay. Can you scroll up a little bit here? Yeah. Uh, what? Uh, yes. So here we are trying to find, uh, what a and B okay so a is b or this one okay any three points suppose i take yeah x and some reference point x and xi they will have a dot product and any function of the dot product is called a kernel roughly speaking any symmetric i mean obviously dot products are symmetric so any function of the dot product is considered a kernel. So in support vector machines, we typically use three kernels. One is the linear kernel. What is the linear kernel? The linear kernel is literally the dot product between XI, XJ. The kernel is kernel XI, XJ is quite literally the dot product. i'm using this is the dot product also written as xi dot xj right like that but because dot is sort of gets lost in writing so i write it with this angela bracket then there is a polynomial kernel. The polynomial kernel is, polynomial kernel is Xi, Xj, is quite literally that, a polynomial of the dot product. You allow for, you know, the basic term also, XI, XJ, to the power d, some power, polynomial power d. And then there is a more powerful and quite ubiquitously used kernel. It is called the RBF kernel, RBF. Now, what in the world is the cryptic word? Radial basis functions. radial basis functions. Yeah, well that too is cryptians a lot of you can write it as a bunch of gaussians so your kernel would be written as the kernel xi xj is equal to e to the minus gamma this is a suppression considered a suppression factor xi dot xj right i believe square right there's a square and you can put in d2 or something like that the typical gaussian the two factor divided by two is subsumed in this gamma it's a suppression factor what it means is if you plot this e to the i you will realize that it can if it is gamma is small it will be like this it will be stronger and stronger gamma will suppress it you see that this curve is being suppressed down as gamma increases what is gamma by the way, this is a Greek symbol gamma. And so what is the intuition behind it? The intuition, the way I would suggest think of the intuition is, see a polynomial is simple. I showed you that a curve can become straightened in a polynomially high space, right? Polynomial of degree 3, for example, three-dimensional, four-dimensional space, and so forth. That is the polynomial kernel. The radial basis function kernel is interesting. It's sort of an infinite dimensional kernel of sorts. You take there. But yeah, so we'll come to that. Let me not go there. What it basically means is that, suppose you have a complicated river. Let me make this river like this. Like this. Remember that these points are like lighthouses shining. So if you can find lighthouses, enough lighthouses, support vectors, right, enough lighthouses shining. And maybe a couple of mistakes in here, which, by the way, those mistakes are also called support vectors because they help support the decision boundary. You realize that you can draw your, support the decision boundary you realize that you can draw your you can navigate this river using these lighthouses isn't it all you need is lighthouses along your path does this make sense guys am i uh yes Am I? Yes. Oh, sorry. This yellow thing should stay. As a boatsman, you just need that the riverbank should be lit. The metaphor is you just want, as a boatsman, just want the river banks and mistakes to be lit with enough lighthouses along the way. Is this intuition obvious, guys? Isn't it? As a boatsman at night, you just want the riverbanks and the mistakes to be lit with enough lighthouses along the way. If it is, you can navigate your path. Isn't it? And support vector machines is just a means to find this light houses support vector machines M is an algorithm that cleverly finds these lighthouses for you. Of course, needless to say, these lighthouses are called support vectors. Well I hope that this workshop series in my little training place truly represents lighthouses or support vectors to guide you guys along your learning journey. Yes, and that's the name of my company. With that, we'll stop, so if say the width of river is say the river is narrow right in that case that also means that the classes are overlapping so you allow for mistakes to pretty because you allow for mistakes it may be true that a river may have washed to the other bank right or one of these uh boat houses is sitting in on the bank of the agricultural site because you allow for mistakes in between the overlapping regions are the mistakes isn't it you can still flow a reasonable river through it so long as you have a you have a good budget it is you know it depends on individuals to decide how much budget they want yes or more formally see ultimately the proof of the pudding is in the eating when you are building a support vector a machine classifier let us say how do you choose the budget you choose that budget which gives you best accuracy of performance on the test so if say it is binary problem sir and let me finish that so uh sorry to interrupt you so in other words that budget is a hyper parameter of the model, isn't it? So, in a support vector machine, there are two hyper, so for example, budget is one hyper parameter. Let me just write it down, parameters, so that is part of the nodes. Hyper parameters are C, the budget. Actually, most people don't call it the budget. They call it the cost, which I find non-intuitive. In the literature, you will see that the way scikit-learn and your textbooks call it, they call it the cost. Actually, it's the opposite. It's the budget for mistakes. And the other is each of these for gamma is the gamma for for RBF dimension degree of the polynomial for poly in polynomial polynomial kernel you still need to decide what your d is right what's your optimal d is and you'll figure it out you can tell it just go figure it out it will do that and so forth so if you have certain degree of actually I would say that these days the library is just to figure it out on their own but there are certain hyper parameters built in typically you use RBF quite often if your problem is complex so these two become at least two of the hyper parameters in the model okay so for the nonlinear problems like where the river is not straight the RBF can help finding the lighthouses either the polynomial or the RBF see remember linear is out of the window isn't it all right so you'll have to try polynomial or RPF one of the two and so if it is binary classification problem yes so in that case i'm just wondering where the linear regression will be more efficient just for the accuracy point of the linear logistic regression draws a straight decision boundary in the feature space so you would get screwed your river is non nonlinear. Decision boundary is nonlinear. So it won't work. So the burden to go to higher dimensional space to linearize the problem is on you. And remember, we have been doing this. If you go back to your data set for regression, what did we do? It was a sine wave like structure. We went ourselves to polynomial degree. In the language of support vector machines, you would say that you have used a polynomial kernel. So let me say it one more way. See, you realize that finding a linear decision boundary is solving a linear problem. So all of machine learning problems ultimately go through two phases, whether it is support vector machines or it is deep neural networks or so forth. The first phase is somehow to linearize the problem. All of these things, the deep neural nets, in many ways, they're clever ways, roughly speaking, they're clever ways to linearize a problem, to find that representation space in which the problem is linear. And then solving a linear problem is easy. So those of you who have played with neural networks, remember, what is the last layer of a neural net? It is just a simple linear regressor node, right, activation function, one simple, or it is just a logistic function or a softmax function, a generalization of logistic. Both of those are linear classifiers. So what happened? You solved a whole big complex nonlinear problem by taking the input and adjusting the weights of the layers of the neural network in such a way that it found a true representation, says that the problem becomes linear. And then in the last stage, you solve the linear problem. This is pretty much the way complex problems are solved. You find ways. You can do feature engineering, as we did last week. You can use clever tricks. You can use these powerful algorithms. They all just help you linearize the problem. Once your problem is linearized, then you use a last step is a linear algorithm to solve it thank you sir can you again explain a little bit about rbf sir how the rbf as a function how does it work ah pradeep i would love to that's a little bit outside the scope we will why don't we do it either's do an extra session or do it in the engineering math because you know it can be a bit intimidating. In the audience, a lot of people who want to delve on, stay with the practical side for them, that math can be a bit overwhelming. That's why I sort of glossed over the at the last stage some of the kernel level math also for that reason it is there we will learn it but we learn it in the other workshop carefully thank you i know in this one like in support vector we don't have to find that higher order space and we just did this trick right dot products yes but in in so why why why didn't we play that same trick in earlier problems with why we have to find polynomials in our other algorithms so we are learning so what happens is that see when we can do by hand it's the gold standard it's an open model i see the machines are black box models it's an open model i see the machines are black box models right so it will it is very difficult for you to find out what is the space that this machine discovered yeah there's but there's no need for ability is a is a huge casualty here right at the end of the day support vectors is still remarkably interpretable because through the data set what what did it do? It says, don't tell me how, but I discovered these lighthouses. And once you know the lighthouses, the support vectors, you can literally see the intuition. You can see how the support vector machine is making its decisions. It becomes evident. So in that sense, it remains interpretable. But you lose this fact that how in the world did you pick these as the lighthouses yeah what was the reason behind it and then it says well welcome to the land of mathematics and let's talk constrained optimizations and dual spaces the business guy will run away but you still get that function at the end right because you got those lighthouses yeah exactly that's that so it's all beautiful math we'll cover it guys at some point uh but okay we have three minutes left i said somebody said 10 is a hard cut off any other questions was it fun guys i find support vector machines to be one of the most beautiful gems in machine learning yes sir so even if we you know after a limit and then it will gonna start overfitting right doesn't matter we cannot increase the budget unlimited but it will gonna start overfitting then right no what happened is if your budget is too small it starts overfitting because it tries to find the narrowest river that may still exist or it may not find it at all. To make your budget too relaxed, you have bias wide error. So imagine that your budget is very wide, right? But what will happen? A river, a huge river will come up that is basically flooding the houses and the trees. You can make out. And there'll be so many mistakes, bias errors, isn't it? It's a simple, more or less straight, simple river, hugely wide flowing through it. So a large budget leads to bias errors. A small budget leads to variance errors. That's sort of the bias we're interested in this situation. Thank you, sir. I think just one thing that helps me is an The way to visualize is functions is after all also a vector. It's like infinite number of Connected Functions of vectors and And once it's better than you do linear transformation and preserve the distance between the dimensions, right? And that's what I meant when I said radial basis function. They're basis unit vectors in an infinite dimensional space whose axes are this bell curves, there's basis functions. But that's getting a little messy. Let's do it in the relevant workshop. All right guys, I hope you enjoyed it. In the end, I did gloss over some of the higher level mathematics for a reason guys. From a practical perspective, you need to know the intuition. And I hope today you learned the main intuition. It's a beautifully argued thing. And to think that one man in one Russian winter sat down and talked through it is just amazing. And it turns out that he was not the only one. There were other people who were thinking along similar ideas a few years later and so forth. Today, when you look at the support vectors, say you see a reference to a lot of researchers, but at least one of them, and and in fact he's considered the leading light of the topic he did discover or independently and perhaps before other people a lot of this years and years ago quite remarkable isn't it all right so with those words i'll stop the recording All right, so with those words, I'll stop the recording and I'll let you guys go. If you want to stay back for something, you're welcome to, but I'll stop the recording. And I'll also stop.