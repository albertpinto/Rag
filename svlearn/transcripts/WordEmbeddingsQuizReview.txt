 This is Saturday, December 19. We have gathered here to cover the quizzes. There are two quizzes, word embedding and the other is on anomaly detection the second thing on the agenda is project review so our situation currently is that word embedding has a majority of the people who have taken the quiz so it makes sense to cover that i am a little hesitant to cover the anomaly detection quiz. If by the time we get done with the first part of the quiz, the first quiz review, we reach a majority, then we'll cover that also. Otherwise, let's wait a little bit and we'll certainly cover the project reviews. That's our plan today. So I'm going to share my screen. Balaji, are you here? Balaji, can I please use your solutions as the review point? Hey, Arthas, for the embedding, right? Embedding? Yes, yes, please. You know very well what I'm asking you. Yeah, I mean, it's just the embedding quiz, right? This one I just started taking, sorry. Just finish it, then I'll use this of yours too. Alright guys, so this is the word embeddings quiz. All right, guys, so this is the word embeddings quiz. This quiz was staggered in such a way to help you decide that are you, do you have clarity in word embeddings? Word embeddings, we think of it as a simple topic, you know, ultimately what is it? Find a lower dimensional latent representation and more efficient representation of the words in the corpus. So on the face of it, it looks very simple, but there are subtleties and there are trade-offs between one kind of word embedding versus the other and so forth. So in other words, it's a subfield in its own right. So having given the fact that we all know what word embeddings are, then the only purpose of a quiz is to see whether you have enough clarity on the fundamentals of it. And so I made the quiz in such a way that there was negative marking. The purpose is if you get zero or below, you should then come to the conclusion, not that you don't know word embeddings, but that your understanding is a little bit blurry. The optics is not very sharp at this moment. So what you need to do is go back and review your understanding and make sure that you have caught the finer details also. And those finer details sort of matter. So as I work our way through this quiz, do pay attention to the questions and the solutions. But more more than that look at your score obviously you know your own scores and make a judgment call it is uh if you're getting a lower score it doesn't mean that you don't know word embeddings it means that you're you lack clarity at this moment and clarity is important to this thing all right guys so the first question was word embeddings create a lower dimensional representation of the words in the vocabulary by studying their co-occurrences and proximities in the inner corpus of documents. So remember that all of NLP is done with reference to a, with a corpus. The corpus is sort of the universal set of documents that you use to train a neural network on, an NLP model on. And a document is a more abstract thing. It could be a sentence, it could be a paragraph, it could be truly a file, the whole text in a file. So we use the word document in NLP in a more specific sense. And it sort of differs from the colloquial use of the word document in popular language. Then we also use the word sentence. Now, once again, when we use sentence in NLP, we don't quite literally mean the English grammatical sentence or grammatically full sentence in other languages. A sentence could be many things in this space. You could, for example, define a paragraph as a sentence. You could define part of a clause, one clause as a sentence and so on and so forth. It is some logical unit of words, combination of words is a sentence. So that is a distinction to bear in mind to distinguish what we the words that we use in NLP and how they are subtly different from the use of the same words in colloquial usage. So now is this statement true the word embeddings create a lower dimensional representation? Yes, it is true, as you remember. We often start with, let's say, one hot encoding. And a one hot encoding of words can lead to a vector space that is 300,000 or 100,000. Or you can pick your number, sometimes a million or three million, big. You can pick how big a corpus and how many unique words you have when hot encoding would create very large vocabulary for you. So then you're reducing from this ginormous vector space, which doesn't contain any relationship because every word is orthogonal to every other word in that original input vector space. The point is to find another vector space, a latent representation, which is sort of more semantically rich, where the relationships of words are more reasonably well captured as vector arithmetic. To be in other words, two words that are reasonably similar or imply similar things or adjacent things, they should be represented with vectors that are fairly adjacent in the vector space, right? They shouldn't be orthogonal to each other. So when you consider it like that, and then you go about creating all sorts of algorithms to build the word embeddings. Now in all of these word embeddings, we use specific methods. We'll talk about that in the next questions and so forth. But in general, this statement is true. How do you find the relationship between words? You study their co-occurrences. So you see how often one word occurs closer, close to another, given a certain window, a context window. Let's say you take a context window of 10 words or you know, 11 words or three words, whatever it is, three to the right, three to the left, and so forth. And you see what words sort of tend to be in the neighborhood of a word. The implication being that the meaning of a word can be essentially captured by studying its neighbors, by studying the company that a word keeps in the entire corpus. So from that, this statement is true. I believe most of you got this one right. Second question was, to create lower dimensional word embeddings, GloVe, GloVe words, global vector, uses, this is one of the algorithms of word embedding we covered, uses the global word-word co-occurrence matrix and its factorization to generate lower dimensional approximate representation. Unlike Word2Vec, it does not utilize a local sliding context window around the words. a local sliding context window around the words. So. Obviously, the way I worded it, deceptively, I was sort of tempting you to. Fall for true, and most of you did, actually, that's not true. The GLURV is unique because it forms a bridge between two different kinds of approaches. There are approaches that use the context window, let's say word2vec, and there were approaches that simply used global co-occurrence matrix factorization. In a way, GloVe bridges the gap and it says that we will look at co-occurrences within a certain context window. But that co-occurrence, now we will look at the at sort of a more global structure or you will do something slightly similar to matrix factorization but not quite that. Remember we worked out the probability ratios, right? xij over x sum over xik over all possible values of k where xi's are the probabilities of co-occurrences and so forth. So I wouldn't go into the GloVe detail, but it does utilize context window. If you look at the original paper that is there for GloVe, in the very first paragraph, it makes this strong point. First few paragraph, it makes this grub, in the very first paragraph it makes this strong point. First few paragraph it makes this, actually in the abstract it makes a point. So moving forward, which of these algorithms use word pieces instead of words in generating the word embeddings? So this is a matter of knowing. Remember, this is just going back to the lecture notes and seeing what I said. A fast text, the way it differs from a word to work is, it's nothing but word to work, except that you don't use word, you use word pieces. And that is as a distinguishing features. In a from abroad, essential perspective is a big idea. Next question, did I already cover three? I believe so. Yeah. The fourth question is, which of these can be used to create context-sensitive word embeddings? Sesame Street, Word2Vec using Skip Grams, Glove, Elmo, Word2Vec using continuous bag of words. Elmo, word to work using continuous bag of words. It seems that Sesame Street had no takers. So none of you fell for that. But almost, quite a few, almost most of you got this answer right. You got it as Elmo. Elmo was the first, the research paper of ELMO specifically says that we can create context aware word embeddings or contextualized word embeddings. And those contextualized word embeddings are superior. The word apple to the farmer is different from the word apple to computer people. I think Kate used the word stand. So is it to stand somebody, tolerate? Is it to have a music stand? Or is it that you stand up? So words take on meanings contextually and ELMO used, if you remember, bidirectional LSTM to achieve that. We did not go into the details of ELMO. We just left it at saying, we did go a little bit, very little bit, but when we were covering the BERT paper, then we did that. So those of you who attended the Sunday research paper readings, so I took you folks through the whole thing of how BERT relates to ELMO and so forth. So this is it. It is ELMO. And what does ELMO internally comprise of? So this was something that obviously only people who attended the Sunday research paper reading would be able to answer. Or if you have read the paper on your own or something. So it was essentially an invitation to go read about ELMO on your own if you haven't. And so ELMO, the important thing is that ELMO does not use attention. It predates attention. The breakthrough and all of that came afterwards. I mean, this may not actually it doesn't predate attention, but it doesn't use attention. Actually, I take it back. It doesn't. Attention was done in 2014. But yes, it wasn't used in this. It uses a bidirectional LSTM followed by feedforward. And then all the feedforwards culminate into the softmax and so forth, the usual stuff. So that's what it was. Bidirectionality is important. Remember the context to the right and the left? You guys are all quiet. Is there any confusion so far? Then we'll move forward. So I say what does the single directional mean? That option there? A simple LSTM is one directional. So unless you lay two LSTMs together, one going, one reading the sentences, the sequence forward, and one reading the sequence backward, you won't get a bidirectional LSTM. A typical LSTM takes a sequence of something, right? So generally the implication is that you would, if you are feeding it a sentence, you would feed it first word first, then second, the third and fourth, and the last word last. But when you do a bid first word first, then second, the third and fourth and the last word last. But when you do a bidirectional LM, what you are doing is when you give it, it will read the whole sequence backward and forward. It will read it from first to last and then from last to first. OK. That's bid by direction LSTM. Next question. Did I cover this question? Which of these algorithms for generating word embeddings does not employ a classifier in the training? So for this, of course, you had to know a little bit about each of them. Do you remember that in Word2Vec we have a softmax, right? At the end you do have a softmax if you remember. And what do you do? You do a dot product of that softmax and so on and so forth. So you very much have a classifier at the end of it. have a classifier at the end of it. That you don't quite use it like that, but okay for what it is worth. Glove does not have that. Glove just works of the probabilities, global probabilities. Glove would, and I hope if you go back and see the details of my explaining the glove, it'll become clear. Glove, Word, and I hope if you go back and see the details of my explaining the glove, it will become clear. Glove, Word2Vec, and FastText are trained with? Well, this is a question with supervised learning, statistical methods alone. Well, that's not true. You do use a neural net. With semi-supervised learning, with reinforcement learning, with unsupervised. So the answer is unsupervised. Now, why is it unsupervised so the answer is unsupervised now why is it unsupervised since there is there is no training no label data and we are simply searching for lower dimensional embedding this is a classic example of unsupervised learning right so the correct answer is with unsupervised learning so this brings me to one more point see when people talk about this supervised learning. So this brings me to one more point. See, when people talk about this, the whole thing is not watertight. Sometimes you find some other researcher will reinterpret it and say, oh, well, you know, word embeddings are self-supervised learning, right? Or something like that, right? Creating a word embedding, you create a word and then you predict its context and so on and so forth so you often see that there's a little bit of a fuzziness and sometimes people will with great confidence say it is this and then then somebody else would say well no maybe it's that but the majority will more or less say it is unsupervised learning but the point that i made long long ago that these methods in the on the face of it look very simple. If you have labels, it is supervised learning. If you don't have labels, it's unsupervised learning. Now think about word embeddings. Do we have labels? Well, the context of a word, maybe the label, if you're doing the skip gram or the context, maybe the input in the word itself is the label. If you're doing the continuous bag of words approach. But these are all words within the context window. You have no real honest label data. So it gets a little bit more nuanced. But people broadly still call it unsupervised learning go ahead yeah this one is i'm not very clear crystal clear i mean there's some clarity but it's not very crystal clear at least for me and this is not the only question i made this mistake here i think in the other quiz also where you have similar question i made the same mistake i noticed i noticed that and my point here the way I was thinking here is you know I know the output in the in the learning in a training part right when I'm training the model I know the output and I'm comparing with the output so it if it's not still you know technically labeled I'm still knowing the output that I should do and you hit you you are arguing it correctly but see if you really think about it you are predicting something for example in the skip gram you're taking a word and hoping that all the context words will light up in the softmax and you know those words you know they are in effect the labels for that input. But, and this is the whole part, and you train it as a classifier. I mean, when you do that whole thing, you create the data and you use negative sampling and all that. You use a classifier. A classifier is essentially a supervised learning algorithm. But those are the, people say, oh, those are the internal details. From a business perspective, you didn't ask me for label data. You just asked me for data. You just asked me for corpus. Therefore it is unsupervised. You internally happen to chop it up into two pieces, a context window and do it. But externally, you didn't ask me for label data. So broadly speaking, people still call it unsupervised learning. But it again goes to the point of making that sometimes, you know, as the world gets as we get into more interesting stuff, this whole thing gets a little blurry. Which one to take? So generally, one thing I would say is I to take. So generally one thing I would say is take the answer which makes sense from an outside functional perspective, not from the inner detail implementation perspective. Generally people tend to converge around such answers. Even in that case, Asif, in the training part, right, isn't that, you take the data, but essentially don't you label what is the outcome here that you're comparing with? No, you don't label. No label. There are no labels there. All you have is a corpus of lots of documents. Thanks for the clarification. That's a good question. In other words, you have a giant collection of Now, which of these uses a single one directions? Go ahead. Yeah. So can you explain the difference between like self supervised and like supervised learning? In supervised learning, you need a label. Yeah. kernel. In supervised learning, you need a label. Yeah, as in self supervised learning, the input is also the label. Why the ground truth y is the same as x, the input. Asif, that's applicable for the autoencoders and so on. That is true, autoencoders. Does that clarify? No, still, yeah, I'm not getting that intuition. Okay, let me draw it out. Also Asif, if you can just just when you say the output is labeled if you can clarify that also let me do that oh this is our paper from last week let me create anomalies this let's create one more all right so are you guys seeing my writing boat yes see what happens is when you get data you may get data as x vector right and xi and now the question is y i right so the distinctions are this unsupervised from this perspective you have just xi right which is equivalent to saying you have xi and you have null and you have null, no labels. Each of the data items comes with no label whatsoever. Then you have supervised, am I writing it right? Supervised, you truly have XIYI, right? XIYI where explicitly YI is not equal to null. Are we together? And very explicitly, this is a must ip label must exist and be specified in training data Right. On this point, then the novelty detection, would you clearly classify that as supervised then? No, no, hold your thought. We're coming to that. So let me itemize that and we'll think through this. All right. Okay. Then comes an interesting case, self-supervised. And so see, these two, there is broadly a consensus. Everybody agrees what these two words mean. All right. Things get fuzzy in the next two things, self-supervised and semi-supervised. That's where the vocabulary or this dichotomy begins to lose that watertight clarity that one is supposed to have, self-supervised, in my view. So is it like an entanglement kind of a thing? XI and the output is XI. This is very peculiar. In other words, which is the same as saying xi yi, right, where yi is equal to xi. When the input and the output are exactly the same, the label is the input, then you have self-supervised. So in other words, you're trying to predict yourself. But what is non-trivial about it? You can just transfer the input to the output. The reason the non-triviality comes because you don't give it enough memory. You create a bottleneck and the system is forced to discover a latent representation, an efficient latent representation, right? And then obviously we're talking of autoencoders and then you look at the reconstruction loss. And then there is semi-supervised. In semi-supervised what you do is, there are multiple meanings that I have seen of semi-supervised, what you do is, there are multiple meanings that I have seen of semi-supervised. And by the way, I am speaking from experience. If somebody has authoritative definitions somewhere, and I have seen conflicting authoritative definitions. But just in case somebody believes that there is truly one correct way, just let me know. correct way. Just let me know. In semi-supervised, many fuzzy meanings. For example, it may include any one of the following, including A, just the self-supervised itself some people don't use the word self-supervised and they will just use the word semi-supervised a b only some data is labeled right So you have a situation where you have XI, YI, where YI can belong to, let us say that it can belong to, maybe null. You see that some values are labeled and some are not. This is one another way of interpreting it. And the third way that people interpret is the training. Uses only one class of the target. Why? Category of values. So for example, in the case of anomaly detection, it was very straightforward. What are the two categories for For anomaly detection, category is normal. So you can say that, or let me write it in a more intuitive manner. Why, whatever the value of why is, Y must belong to this. This is the category or the set, right? This is your, usually you represent it with a G. This is your set of possible values. is in here you only train the novelty detector with normal. The set of such data where this condition is fulfilled, right? IE, which is equivalent to saying that XI, YI, set of all training data where YI is equal to normal. It has only specifically this value. So are we getting a sense, guys? So now let's review what we are saying. For unsupervised, do we have a clarity guys? When we review, can you include example also of the Of unsupervised? So for example, okay, hang on. Our clustering, dimensionality reduction. These are these things, if you remember in ML 100 we did, specifically if you did ML 100, this is chapter 10 of your textbook. The ISLR textbook we did. Examples of this are classification classification actually I'm handled up the spelling completely classification and regression. These are examples of whenever you're doing classification or regression, you are doing supervised learning. What is an example of self-supervised learning? You just encountered that. It is? Auto-encoded. Variational auto-encoded. Yes. Variational autoencoder. Yeah, variational autoencoders is one classic one. And autoencoders in general. Then what is semi-supervised learning? Many examples. So for example, novelty detector would be like, I just took the example, novelty detector is at that. But now there are other meanings also. So some authors some writers they will call self-supervised as semi-supervised and also they'll be fuzzy about this then sometimes if they feed if they have only some label data not all data is labeled and they are training on that they would would still call it semi-supervised learning, right? So there is a little bit of a fluidity on what people mean by semi-supervised learning. They can mean any one of these three things. Do we have clarity now guys? Yeah, Asif can you go to the bottom, to the last row? Or can you go to the bottom uh to the last row or can you scroll here what specifically are you looking for i was looking at the what's the criteria so the input is the normal data right is the normal data. Right. So that's that. Should we move forward now guys? Any other questions? Okay, let's move forward. Thanks, sir. See guys, that's the value of quizzes and reviews of the quizzes. And if you do the quizzes and then we review it, your understanding sharpens. Likewise, guys, that's the biggest value from this workshop, maybe at the end, when we review the projects, because you learn a lot in seeing how you did it and how it could have been done, alternate ways that it could have been done. So do the project guys. So we did this. Word2vec utilizes the factorization of the global word-word co-occurrence matrix to create a lower dimensional approximation as word embeddings. It is false. In fact, word2vec explicitly uses just a local approach. Remember a context window. It does not use any global information at all. So the moment you see the word global, you should have run away from it. Given, and of course, there is no matrix factorization happening here. Given a context window around any word in the corpus, in skip gram word to where the input is the word itself and expects the output preferentially point to the other words in the context window. Or more colloquially speaking, when you give it a word, you expect that the softmax lights up the bulbs around or lights up the nodes around all the words that form the context of this word in the context window, all other words in the context window, right? That's how Skip-Gram works. In which of these algorithms is negative sampling used to train the word embeddings? This is again Skip-Gram word to reckon, because you have to train a classifier. So you know that the word x existed with word y but it did not exist with the word pizza or something like that, right? So you have to create negative sampling. That was there in our lectures. Please do review that. Given a context window around any word in the corpus, in CBOE, continuous bag of words, word to bag, the input to the neural network is obviously here. It is the context. In other words, the context window, all the words in the context window. And what is the output? Of course, the word itself. Given these words in the input, adjetively, you want to find the output word, the actual word that they surround. So the way to look at continuous bag of words is that which word do these words surround? That is that. And that is it. A very simple quiz of what is it, 10 questions? Or 12 questions.