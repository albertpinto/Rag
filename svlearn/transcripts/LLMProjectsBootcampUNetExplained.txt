 you All right folks, we are live on YouTube so please be aware of that. I will get started shortly now. We are doing the paper reading for the UNED paper. Remember today's theme is the generative art or generating images from various text prompts. So how many of you got time to read the paper? Almost all of you got time to read the paper. Akira, how would you explain it? I do. Okay. All right, so we will do this. It's a very simple paper. If you guys remember what an encoder-decoder is, you guys remember what an encoder-decoder is? An encoder-decoder follows this architecture. You give it an input. Let's say you give it the input, some image, x goes in. It gets encoded into a hidden state, z, through this process. And then the z gets decoded into X prime. Isn't it like it gets decoded, the prediction, the white hat is basically an image. Let me just write it as why hat, why hat is X prime is basically some variant of the input. That's your basic encoder decoder architecture. Now the point of our encoder is, encoder, decoder, encoder is z is some function of x, right? Is some function of x. It is a non-linear, lower dimensional representation. Asif, if you intended to share your writing on the... Oh my goodness, I have not shared my screen. I apologize. Oh my goodness, I have not shared my screen, I apologize. Is it now visible for those of you who are remote? Yes. Yes. Repeat. I repeat, this is your auto encoder if you recall the auto encoder was made up of an encoder and a decoder the encoder is a function encoding that takes an input x and creates a latent latent representation. In some sense you can think of it as a very meaningful low dimensional representation of the data. Now, and therefore it's a very powerful technique for dimensional reduction. It's a nonlinear technique. Why? Because in an encoder typically even if you do it as a fast forward, a feed forward network fully connected layers. So the simplest situation, feed forward layers. What are we doing? You're doing a layer of feed forward layers. What are we doing? We are doing a layer of linear transformation. What is that? A linear transformation. Actually, let me not use z because I use z for something else. Let me write it as a hidden state. You realize that at any given moment if you are doing this you have z and activation of z to produce the output z is w dot x input plus a bias term do we remember this guys this is going back to the basics of a neuron this is what a is and the its activation the output is equal to the activation of z. So what happens in each layer? You have a linear transform because this is linear and activation of z is non-linear. The linear transform is literally composing or putting it together, superimposition of the signals with different weightages, and the nonlinear transform brings about the deformation that makes a neural network into a universal approximator. That's a basic intuition. Now, that was your encoder-decoder in its simplest form. Now, the easiest encoder-decoder that you can think of is quite literally, suppose you have an input like this, you reduce it down in a single shot to, let's say, a lower dimensional presentation. This is your encoder. And then you have a decoder. And then in reality, you don't have just a single layer encoder. You have multi-layer encoder. This is your decoder. And let's say that your decoder, you write it as another function, g of x. g of h so you say that x prime is equal to g of x so then you realize that if x prime is g of h that is the same as g of f of input right and suppose you say like the simplest form of the encoder is that you're trying to look for a low dimensional representation. And so you would want X prime to be as close to X as possible. The input as possible. And you say that, well, isn't that silly? I could have just transferred the copied information from input to output. So what possible use is the autoencoder the use of the autoencoder comes from the fact that you have a bottleneck this is your bottleneck the bottleneck principle and as you know there is such a thing as a bottleneck principle which is a more general principle beyond just autoencoders but its main thesis is that whenever you introduce a bottleneck you force some level of understanding or extraction of a higher higher order representation right in in very simple terms see if somebody has read a story. Let's say you have read Anne of the Green Gables. And now I say, write the story, the essence of the story in two pages, right? A thousand tokens, thousand words, such a thousand words or something. You realize that you can't talk about every detail, every emotion, everything, but you need to compress it down to the essence of the book. I wish you knew that. And in doing so, you can do that, if and only if you have a higher level understanding of the plot structure, the author's intent, the message and so on and so forth. Making sense, guys? So that is the bottleneck principle that whenever you introduce a bottleneck, you force a higher order representation to get formed. You cause something meaningful to be understood from the data. And because you save the data to a bottleneck and then re-expand it, it will be able to re-expand your summary back to a story, if and only if the compression, the compressed version is sufficiently informative, like out of which you can re-derive much of the story. You may not derive every single detail of the story right, but you'll be able to re-derive much of the plot, much of the story. Am I making sense, guys? So that is the gist of the encoder, autoencoder. So it is made up of the encoding function f and a decoding function g. Right? Or one compresses. Now you need not just use a feed forward network. If for images it turns out that it's far more efficient to use forward network. For images, it turns out that it's far more efficient to use use? CLS, convolutional neural nets. Why are convolutional neural nets useful? Because convolutional neural nets try to understand the local structure. Two pixels are just in, they tend not to be dramatically different. It is the local neighborhoods that convey something. Isn't it? So that locality or the convolution means fuzziness, borrowing from the neighborhood. If you look at the nature of the convulation function, the convulation, for example, and again, this is taking a quick deep dive into the mathematics. What is a convulation function? So suppose you have a function fx. And you convolve, convulate it with another function. Generally, there is a subtle nuance. The way mathematicians think about it and the way computer scientists think about it is slightly different. The science people say, forget about this, the t minus x, et cetera, just fx convol, and just put a notation, g of x, right? In which they don't do t minus x, they just do x or something like that. This is a convolution operation. So what does that really mean? What it means is, suppose you're sitting, imagine that there's a very dark spot here and these spots are not so dark suppose you're convolving over the whole patch right giving and you say okay from this patch extract let's say min pool max i mean just extract something right or find the total value or multiply it with suppose you have a mask here weight w1 w2 w3 you multiply it so suppose you put a lot suppose it's like a bell curve you're pulling in information from the center more and a little less from the neighbors. That could be one way of looking at it, right? If you're convolving into the bell curve or something like that. But in image processing you have discrete values, you don't have bell curve. You'll have some weights, w1, w2, w3, all the way to w9. So this is a three by three filter that you convolve. It means every input value you multiply. Now, based on the way you set the values, you can make them, and again, I'm rehashing the thing. You can have an edge detector, you can have a line detector. I mean, you could have a point, things like that. But more generally speaking, a convolution has the ability to pick up local structures. So you realize that just one pixel, multiplying it by very big, is not as informative as getting a little bit more sort of locally global, more spread out feature like an edge that's a little more informative and that is the key idea between convolutional filters and therefore convolutional neural networks right so whenever you do image processing it is common not to have feed forward networks but to have convolution so what you do is you pass in an image image has how many channels red blue green channel maybe an alpha channel right three channels four channels whatever it is and then the image may be let's say five uh one two cross five one two pixels or i believe they take 572 so why not take 572 572. So why not take 572 pixels times four. So each pixel would be either red, blue, green value or alpha value, and it would have its coordinates in the grid, in the picture grid. So suppose you have this house. You divide this picture into pixels and each pixel has a coordinate. It is first row, last column, things like that. So you therefore, any picture can be represented essentially as a tensor three-dimensional matrix isn't it so that is that so you say that each of these are channels so there are four channels in the beginning and typically what you do is when you convolute it with a filter and then you do max pooling or average pooling and so forth, the size of the picture from 5 cross to 5 to it will reduce to a lower dimensional picture. But because each filter is a feature extractor, It is extracting some features. What will happen is instead of just four channels, you will end up with, let's say, 16 channels. You get many more features out of it. Asif, what do you mean by channels here? See, whenever you look at a picture, picture, a colored picture, colored picture they're made up of four channels red, blue, green, means it has a red value, any pixel has a RGB value, red value, green value, blue value, right on from 0 to 55. So any one pixel, let's say this pixel here, will have a value like 15, 2, 1, 7, 18, and then alpha value from 0 to 1, let's say 0.5, half transparency. So this is how one pixel is represented. So these things things these coordinates or these axes are called channels does that explain that for you yes it does thank you yeah so that is all there is to chance a lot of this wording that comes from image processing they use the word channels whereas more mathematically you'd say okay those are just axes. When you say 16 channels, it's not clear like how does the RGB8 converts to C? What happens is that you use many filters. Suppose you use 16 filters. The shape filters. Yeah, shape filters. So when you apply, you will now get 16 different features out of it isn't it but traditionally whenever you apply a convulation and you pull it the size reduces so you get less smaller image but many more features about them so that is a typical tendency when you start convoluting that. So typically people represent it as again, like they will show it as a big picture, then it becomes a smaller picture, it becomes a smaller picture. But the number of channels keeps on increasing. And this will be huge number of channels like typically 100 or something like that 100 features have been extracted from this but tiny features right so that layers you go through come again the more layers you go through it's like more channels getting created yeah that is right so you say that every so typically a convolution layer is made up of the true filter convulation and some sort of a min pooling, max pooling, average pooling, something like that, followed by that. And the end result of one full layer is you'll end up with more channels or more features and smaller images in the convolution process. and smaller images in the convolutional process. So lots of smaller and smaller things. That is why it totally fits the metaphor of an encoder. It's a channel sentries. So now imagine that you're doing it like that. The first thing would be, let me, why am I not able to, oh, it's frozen. I'll give me one second, guys. The application seems to not like it. And let's go back. Okay. So if you think about the picture of our encoder and our decoder, it seems to fit into that paradigm isn't it. one note and reopen it something went off with it all right with some luck yeah this thing is scrolling so do you notice that even with convolutional layers you can still have the notion that a very big picture gets reduced to many layers of a smaller picture to many more features of an even smaller picture to many of an even smaller picture and then if you so choose you can keep this representation and then you can reverse it these are called people call it deconvolution actually this is not decondition these are called, people call it deconvolution, actually this is not deconvolution, this is called transpose of a conf, transpose convolutions, but people colloquially often tend to use the word decon. So what you can do is this part is encoder, think of it like that, encoding, somewhat like this is your decoding. And you may have given a picture of a cat or a house, that encoding somewhat like this is your decoding. And you may have given a picture of a cat or a house. And what do you expect at the other end to pop out? Something that looks like a house. Now you may say, well, what does that have to do with stable diffusion or you know diffusion models and so forth. We'll come to that. That's a journey for today. But so far so good guys. Now the unit architecture turns the whole thing, generalizes it, and does something interesting. This is an old old paper now, almost seven, eight years old, and yet very very sort of a powerful one. In fact one of the bedrocks of the diffusion models. This algorithm is both simple. When I explain it to you, you will almost say, what's the big deal? It makes sense because we have known encoder-decoders for so long now. But when it came out and it immediately broke the state of the art and has become a workhorse of the industry for many, many reasons. So this was discovered by a German and his colleagues. The German gentleman, I never get his name quite right, which is a shame, Olaf Ronberger. And from what I understand now, he's not in Germany anymore. He's moved to Netherlands or somewhere like that, you can check it out. He and his colleagues wrote the paper. And I just talked a little bit before going into the model and its things. So how would you train such a such a thing? Or what can we use it for? First, you could use for something called image segmentation. Do we remember image segmentation, guys? What is image segmentation? Who would like to remind me? Finding different objects from inside the image? That is right. Image segmentation is looking at a picture of the street and saying that part of the image is a tree. It's called an image mask. That part of the image from like you put a pencil mark around it and color it inside and say that's a car this is a pedestrian this is a road you realize that that is image segmentation i hope there's a example of it somewhere in the paper or maybe not uh the original paper yes so look at this as in the beginning they also mentioned in the abstract how they won the competition for the cell segmentation and things like that. Right. In the abstract itself, they say that if you go even up above, in the abstract, they say that this won the first method on the ISBI challenge for segmentation. That is right. So let's look at this image. The cell image is a bit more abstract for engineers to grasp, but I'm sure we'll get it. The pictures are illustrated. So look at this cell, guys. Look at it. This is a image. It's called a differential inference contrast microscopy image. Basically, it looks at different tissue structures will have different sort of a contrast. When you look at this image, it's a little hard to tell what is what, isn't it? But when you look at B, it is somebody has gone and segmented the cells. They said, okay, this is one cell, this is another. Could we also do that? For example, we can go here. Would you agree that we can do this? Oh, sorry, this is too blunt an instrument. Let me do this. This. this was one cell approximately isn't it likewise this is another one each of these is a mask you see it here in b it is the ground truth. Right? And then you look at, you ask yourself, can I go from this to this if I have the ground truth? Right? And the whole question is, how would you do that? And I will spare you the math. It's quite straightforward, but I'm going to, it's just a bell curve and so on and so forth. But I'll spare you that and explain it to you in a more intuitive way. Here, by the way, it's much more obvious. Look at this image. The two cells, these are the two masks. And segmentation results with manual crop truth. So we will continue this. But this paper actually is a very brief paper and with not as much explanation. So I'd like to explain what is really going on here in this paper. So what you do is first part you realize that you there is an encoder like there's a funneling down right you use this three by three three cross three filter layers with pooling filters pooling filters if you're used to convolution you know exactly what is doing, followed by decon. So that would look like your classical convolutional encoder and decoder, right? Or transpose convolutions produce a decoder. They blow up a small image into a bigger, bigger, and they reduce the number of features the opposite the only new thing that they do which is quite interesting is and which which explains the u shape see look at this picture this part is normal right this journey makes sense you go to a smaller smaller image. This is the bottom. But what they also do is, see, look at any one stage. We are going to look at this is the hidden representation. From the hidden representation, you're trying to produce the slightly bigger representation unpack right you're trying to deconvolute it isn't it but when they do it interestingly they have what is called residual connections when in machine deep learning literature you call it the residual connection, you take not just the hidden state and deconvoluted, but you take the initial thing out of which the hidden state was produced. What was it produced out of? It was produced out of this, isn't it? Are you seeing it guys? And you feed it into this to get the next stage. Isn't that interesting? So in other words, this, this as well as this goes to produce this. So at each stage, you deconvulate by having, if you were to look into the picture as like this, what you're doing is you're also doing a direct connection here. Right. So to when you are decongulating, let's say you're decongulating at this layer, what are you giving? It is not only taking the input from the previous layer in the decoder, but it is also taking its corresponding encoder input. That is the skip connection or the residual connection. Right. And just to show that they nicely and beautifully in a very visual way, put it in a U shape. I suppose they wanted to use the word U. It does look like a very intuitive U shape now, right? That's what they're doing. Now, why in the world would you do that? Make a guess. Why would you do that? Say that again loudly, please. Anyone? Why are they doing it? Context is what the feature extraction does. What does the initial image give you? Positional information. Positional information. In a way, your context is also right, I suppose. So see what is happening is when you decongulate, is when you decongulate you are compre you're trying to decode some features and in a way where those features are located is captured in the original or the equivalent original you can't have the original but of the same size the encoder produced version contains a better representation of its context or its location and so you get both the semantic information and the location or context information. Are we together? And you therefore get both. Because one of the limitations of, I mean, comms have traditionally had a bit of a difficulty dealing with the localization of information. Like, for example, ConNet, the joke is that whether you put the eyes above the nose or below the nose to a ConNet, it may not make much difference. Right? But with the residuals, it does make a difference when you decongulate it. Make sense, guys? Right? So that is that. And so what is the use for it? Let's look at image segmentation. What can you do? I can take data set in which I have, let's say, an image of a house. Right. And a tree growing. Growing like this. I can take a ground truth. Let's take a ground truth picture. So this is my x vector. I can take a label y or the ground truth label which will do this. Oh, this is too fast. Give me a moment. or let me pick another color here does it look reasonably like masks the house mask and the tree mask i can give it a ground truth and i can basically say that my unit, I'll pass it through my unit, but what I expected to produce is not the original picture, but I expected to produce, try to recreate y, not y, not the original x. So it will produce some y hat in the beginning for example let us say that the original y hat may be pretty bad it may look like i'll just make a guess let me say that it may say It may produce a mask like this, and it may produce a mask like this in the early stages. Oh well, how do we go from this to this? How do we go from this to this how do we go how do we take this journey through the last function in the gradient percent in other words i can just look at the loss function which is y minus y hat square the standard loss function and go about minimizing it throwing some vector minimizing it throwing some vector and go about minimizing it does that make sense right and so gradually it will learn to predict the mask right i could do something else now this looks like a pretty good use case for that but notice one thing i'm deconvoluting the image. Isn't it? In deconvolution what happens? In some sense I have, you know, there's lots of features that I get edges, this, that. From those I'm reconstructing the picture. So can I do this? Can I take a blurry image and reconstruct a higher resolution image? Why not? If from the blurry image I extract a description or sort of features, then I can re-create an image with exactly those features but for much higher resolution. Right? So in other words, not only can I do image segmentation can I do image segmentation that you just saw, but I can also do super resolution. In other words, I can take a blurrier image and create a much more sharper, well-defined, lots of feature images, isn't it? I may have a blurry picture of a flower and I can have a detailed picture where I can see every pollen and petals and sepals clearly. Am I making sense, guys? I can choose to do that. So that's another thing I can do. And I can. It is also very useful in where we are leading towards namely the diffusion models. In diffusion models what you do is one of the games you play actually if you get this idea of a unit you're not that far from understanding how diffusion models work. I can take a small picture right and this picture is a Farhan drinking water. Right. And I can go to a bigger picture of Farhan drinking water and an even bigger picture of a drinking water. Can I do that? Because once I have done the feature extraction, I can progressively keep on adding more and more resolution, so what I will do is i'll put a unit here and i'll get this result, I will use this as a input put a unit here for super resolution. R. Vijay Mohanaraman, Ph.D.: Do you see how I can play this game, and I can put another unit, and I can do cascades of this super resolutions now we are just putting the building blocks together for doing diffusion models so this is the gist of this paper and what should i say this is it there isn't anything else but the amazing thing why is this so remarkable compared to prior approaches this is this the virtue of this is it runs very fast because it's just convolutions and decons are what mathematical operations transpose they're just multiplications by and large and you you as often you do for image processing tasks, obviously you're not just doing dot, I mean linear transformation. So remember, it is always linear and a distortion. And a linear and a distortion. The distortion function here is typically ReLU or one of its variants. It doesn't matter. But ReLU is quite commonly used because it's very fast. So far so good, and that was unit. Right, so it is one piece of the puzzle in the latent diffusion models. Can I assume that glide is so easy that you guys have already done it and I can jump straight to diffusion models or we should do glide votes how many want me to do glide paper please please do that okay so we'll do the glide paper but guys uh at this moment I get the impression not many of you got time to read the paper go read the glide paper I'll give you guys half an hour read the glide paper and come back properly don't glide through it i think so one question go ahead so when you're saying uh increase the resolution will it intelligent intelligently increase the resolution in the sense let's say i'm kept i'm trying to increase the resolution of a human face yeah so actually the hair will be just one big curve but then going forward will the individual hair strands will get yes yes that is its magic why because it is learning not from one data but from thousands of pictures of human spaces so it knows that statistically speaking what is the most probable look if i have to do it is a form of infilling when it does that or high resolution uh got it what's the most statistically probable attributes and another question i had is when you said convolution, is it like the filter convolution or it's more of a Same thing. What you understand is the combination. That's what Okay. Thank you. So does this paper look very easy. It's a short and easy paper guys but very influential ever since it came, it has become one of the industry workhorses. So read this paper guys. Please do read this paper. It's a simple paper. You'll get it.