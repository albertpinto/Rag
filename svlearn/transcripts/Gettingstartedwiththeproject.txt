 being broadcast to YouTube live, which means that this video will be available to you. You can watch it right now and you can have the video immediately afterwards. At the same Okay, so in this workshop, we have two projects that I asked you to do. One was prescriptive. I said go and analyze the archive data set. This data set is of researchers who have submitted their works, preprints of their work, pre-publication versions of their work to the archive site. Archive is perhaps the largest network or site of preprints. Whenever people come up with some discoveries, at least in the fields that I belong to, which are theoretical physics and computer science, you don't go to a journal and ask them to publish. Your first reaction is you summarize your research and you submit a preprint to archive. Once you submit it, it already becomes available to all the researchers in your community and they can discuss, they can think about it. And usually by the time something gets peer reviewed and published, many, many months have gone by. These days, especially in fast moving fields like artificial intelligence or data science areas, you don't want to wait till something gets published, which may take up to a year or more. You want to know about the breakthroughs as they happen. So it has become very common for us to every day treat archive as some sort of a new site. One of the things I'd recommend it to all of you, if you remember, was this website called Archive Sanity. You can go there and some people curate the best articles or what they thought are the best articles, best publications, or what they thought are the best articles, best publications, or research work. And they make, they sort of annotated with comments and this and that or something or the other. And they make it available. There are a couple of YouTube channels which do the same thing. They'll take the breakthrough papers or some important papers is there on Kaggle. Somebody has collected it. It's basically only the metadata of articles published there on the preprint site and is limited to the domain of data science. So it is AI, it is that machine learning and so on and so forth are these areas so it is not all of the areas that archive covers archive is vast but it limits itself to machine learning because we know this field so when you do the analysis you will have some intuition so for example if you find that if you if you're looking for the most published author and a name pops up, you can decide whether this analysis looks right or not, because you probably know who the most published authors are. Right, who are the most referenced authors or cited authors and so forth. So that is why I gave you the archive metadata data set. data set. It is also something that is fairly manageable in the cloud and on your powerful laptop if you have one, actually even on ordinary laptops, though I highly encourage you to do this in the cloud, not on your, if you don't have a very powerful machine, don't do it at home, do it in the cloud. There in this sort of analysis and more and more in the modern world, when we analyze data, there are three different axes, even for just exploratory analysis. The first is looking at the tabular aspect of the data, you know, a data frame. You have data as a data frame that you can do your basic bar charts and count lots and a pair of plots and all the things that we have been doing. Then a data like archives data, there's a networking aspect to it. Researchers collaborate, they network. Research papers refer to, or preprints refer to each other. They cite each other. So you can observe the collaboration network between these researchers. You can also create a network of citations from paper to paper, or a bipartite graph of, you know, researchers citing certain other researchers, that is a citation network of researchers, citation network of papers, or a bipartite graph in which authors cite some of the papers and so forth. You can do all this very straightforward way. These days, networking libraries have become pretty powerful. We will use a library that is quite popular in Python. It is called NetworkX. For R, we'll use a library called iGraph. iGraph is based, is implemented fully in C. It's much faster. Obviously, there is an effort to put iGraph to Python, but I don't think it is as popular as NetworkX. NetworkX is written entirely in Python. For that reason, it's a bit slow, but the syntax is very Pythonic, very natural to Python, which explains its popularity. We'll use these two to begin with. Then we can do text analysis. After all, these articles, these research papers are about some subject matter. They contain abstracts if needed. We can programmatically go and get the content of those. We can get a PDF versions of those papers. And from the PDF, we can extract text. There are all these programmatic means to do that. Apache, for example, gives you lots of open source tools to do all this manipulation. For text manipulation or text processing, you should use the NLTK library to begin with. It's pretty popular. Let's start with this. Later on, you can use the Gensame and other things to do semantic analysis. And we can use that as a starting point. Later on, I mean, as you grow this project and go into the deep learning, you can carry forward this project because you can do apply your transformers and other things that you will learn in deep learning and the same data set. So this starter project that I gave you deliberately was designed in such a way that you could carry forward to the next workshop. So to summarize, we'll do the archive dataset today. I'll give you only the starter code. I won't do much. The idea is that you should do and learn. At subsequent time periods, I may release a larger and larger part of an analysis, but it would be far better if you took it in your own creative direction but the three axes on which you must address this analysis is first as a data frame you know as a tabular data the tabular data analysis the second thing you must do is a natural language processing use basic stuff at this moment since you are not experts in NLP, use the natural language toolkit. Do basic things, see if you can create a word cloud of the most popular authors, word frequencies. Some of these things I will do right now. You do that. And the third is networks are becoming increasingly a central topic and one of the central topics in contemporary data science, everywhere we have networks, our friendships are networks, we have professional networks, researchers like paper, they have collaboration networks and you have citation networks and so forth. So you must also learn network analysis. Now those of you who took ML 100 would remember that we went through a couple of sessions of network analysis before. So have network analysis also as a dimension of your analysis or of this project. So with those words let me start sharing my screen now and let's go with this. So here we go. This is a notebook, which I hope all of you can see. I'll increase the font a little bit. Let you know if I need to increase the font even more. Is this fairly legible to all of you? Yes, it is. So by now you must be familiar with the Jupyter Notebook and the way we write it. This is your standard table of con content as you can see it's a very short table of contents unlike the other notebooks that have released the point being that this is just a starter code this is your basic preamble you know themes and other things and this just ensures that all the libraries that we will need are installed actually we won't use torch deep learning is for the next workshop but as a motivation if you can it would be even better. So we're going to do exploratory data analysis on the archive data set. By the way go beyond exploratory if you can build predictive models and so forth great that's what I hope you do but let's start with exploration of the data. So the basic imports are by now, these are all the usual suspects and you should be quite familiar with them. NumPy, what does NumPy do? It gives you support for high performance areas and matrices and operations on them. Pandas provides you a support for the data frame construct and its manipulation. Pandas profiling is an automated tool to give you a preliminary exploratory data analysis. Right, like now when it comes to handling networks and the investigations, NetworkX is a very popular library. Again, as I mentioned, it is in Python is very popular and for those of you who are doing it in R, use iGraph. It is a bit slow. iGraph is lightning fast, fairly, but NetworkX can be a bit slow for large graphs and you will see that when you actually run it. So once you're done, you know, once you reach the limits of NetworkX, then graduate to a bit more powerful library, for example, NetworkKit and so forth. There's a very close parallel between NetworkX and NetworkKit, and you can make things much faster, better. Because today is the first time we are doing the lab in networks, I will make the code dead simple. I will not focus on making it prettier or more beautiful. So just as a recap, some of you may remember that we did in ML 100, I had looked at the Facebook graph in one of the sessions. I said that look at the friendship networks for a small group of people in Facebook. And when we did that, we created the graph, those of you who remember some of this analysis, and we created this beautiful visualization of the network. When you look at these real world networks, you realize that they have structure. Do you notice how people or their relationships are clubbed together, friendships are clubbed together into communities? Right? And this is sort of an ego network of a person. And when you look at this friendship, so you know, your friends themselves form groups of friendships. And you know that you have a professional, some friends are from your professional life, some friends are from your social life, some friends are probably from your tennis playing club or your photography club. Then within this you can also look at the degree distribution, like how connected is each person to other people in the network. So these are called degree distributions. It's one of the things that you should do if you can in this. It's really helpful to do degree distributions. And then to detect the communities, you know, when you look at that there are all of these communities, can you detect the presence of communities? It's very analogous to clustering in data in Cartesian space. When you have data in Cartesian space, it's a real valid space. You do look for clusters, K-means clusters, density clusters and so forth. In the world of graphs, the more appropriate term is communities because you don't have a notion of, a very obvious notion of distance. You can create a notion of distance, but it's not a Cartesian space. You still can create notion of a very obvious notion of distance. You can create a notion of distance, but it's not a Cartesian space. You still can create notion of distance as in how many hops does it take to go from one person to another and so forth, but it's not a Cartesian space. So what you look for are communities. These are the communities that you find. And sort of, we went through that. I will leave that aside. Today, we are not expecting that level of sophistication. If you can get there, as you can imagine, these are all like four or five lines of code. But since you're getting started, some of you with networking code, I will skip that. You can now move forward. So that would be the networking library. There's another library called PyTorch Geometric. For now, skip it because we'll need it in the next workshop when we do deep neural networks. For plotting, of course, we use Matplotlib and CBOM. No brainer, by now you should be familiar with. For machine learning, we'll use scikit-learn. Once again, it's a ubiquitous Python library for machine learning and we are all familiar with it. PyTorch, a high performance GPU based or GPU optimized library for deep learning. And finally we'll use Folium for visualization of geospatial data. Now where you will use it, I will leave it quiet, you can figure it out how you will represent, how you can do any geospatial representation of the archive data. How do we load the data? So where is this data from? I've given you guys the link to it. So somebody has put the just the metadata of it. There are multiple versions, there are a couple of versions, five gigs and nine gigs of data, which complete data across many many communities in archive those are huge data sets it's much better to play with data sets that you can handle in your notebooks when you're just starting out you don't want to run a computation that goes on for like two days so the smaller subset of data is better. It's about 24,000 authors or so or entries. So let's look at this data. When you look at this data, oh yeah, 41,000 entries. So how do you load data? This comes in JSON format. Pandas of course can load it just fine. So when you do it, change this line, change it to wherever you have downloaded the JSON file for this data. This should look familiar. What are we doing? We are describing this data. So when you describe the data, we notice that there are all of these fields. These fields are described, by the way, if you go here and you click on this, you will notice it described. So you can. These are all taken from the. Oh, I'm sorry. This data is based on. About this file. Okay, now we need to get to the data. Yeah, this is the data and its descriptions are given somewhere. Where are the descriptions given? No, the description seems not to be given. But if you look at this, this is actually a subset of other archive data and other people have given more of a, let's say we go to this two gig distribution, yes. So when you look here, what you have is a subset of this. So what I will do is I will just copy this or maybe not. I will make a link to this for the documentation. Archive, university archive. Let me add it to this to understand this data is taken from Keter. So, if you go here, you can see the descriptions there. So what are the things? ID is a unique identifier. Submitter, who submitted the paper? Not of terrible importance. Typically a paper has multiple authors. One of the questions is, in general, how many authors does it take to write a paper on the average? And we're going to find that out. Title, comments, number of pages and figures, journal rev, information about the journal this paper was published in. That is if the paper finally gets published, you put journal revs. This is a unique thing. Nowadays, people are making an effort to give a unique document identifier to all CS documents, public documents. So in that spirit, there is a DOI. Abstract is essentially a summary of the paper. It gives you what the paper is about, typically at the top of a paper, even before the introduction and right below the title, etc. You put the abstract. Categories speaks about which are the keywords or categories to which it belongs, which is self descriptive. Versions, well, sometimes papers can have different versions associated with them. People keep resubmitting it after fixing typos or corrections to their calculations or graphs and so forth. So that is that. One good thing is that any one of these papers you feel like accessing, you can always directly go access it. It's a very publicly available dataset. A typical data looks like this, as you can imagine. A typical data looks like this, as you can imagine. Entry in the data looks like this. So we'll go and explore it ourselves. We take this data, we load it into an archive. When you look at it as a data frame, you notice that these are the columns. By now, they look familiar. There is only one catch. If you look at the first few rows, you begin to realize that, see, this paper has Gordon and somebody else. So author is actually a string containing many authors, all the authors of a paper. So the title author is a bit of a misnomer. The rest of it is standard, ID is linked, month, month means from 0 to 1 to 12. Year of publication. Title of the paper. Tags. So once again you have these stacked terms. You should give it the same treatment. So this is a point I was making. If you look at the first row of the data, some row, and you look at the author, it turns out that the first author, because it starts with A, is somebody named Mr. Osman and Mr. Vashinch something. Well, we need to break it up into two different authors. How would we do that into the number of authors? This is a little bit of regular expressions. Do you guys know string matching, regular expressions? When you get a complicated string and you want to extract only what you want, there is a very powerful technology. Regex? Regex, yes. That's what it is, regular expressions. It's a technology that helps you extract what you want in a very, very powerful way from the text. And so if you look at this, let me give you an idea of what it is. Look at the names. What we want to extract is this and this, isn't it? But you get this long, complicated string. How can you do that? You notice that semicolon, space, and single quote. This is a unique identifier that helps you find names, isn't it? And it ends with a single code. So that's what I did. I'm saying start with a semicolon, a space and a code. And after that, as many characters as they are present, till you hit the first end of single code. When will you hit the first end of single code here? will you hit the first end of single code? Here. So if you look at this text, you would get this kind of match twice on the two authors. So you compile the string, you compile the pattern that you're searching for, and now you can give it any string and it will return you the authors. You'll introduce the string match. Now when it returns, we don't want the first, second, third character. You want to start from the fourth character and you don't want the last character either. That is why you take the substring 3 to minus 2. So there's a lot happening here. Play around with it and you'll realize Go ahead, please. No, sorry, nothing. Go ahead, please. No, sorry, nothing. Yeah. So when you go around, you'll realize that a lot is achieved in a single line here. We have managed to find all the authors as a list from text. This is the power of regular expressions. They help you do a lot in just a couple of lines. You notice that a function that is just one line has achieved a lot. Now, you may also notice that I tend to give hints. You notice that there are hints here. One of the things I didn't optimize is to say that whether str is null. What happens if this is null? Actually, this will take it fairly gracefully and return you nothing, I think. But you can try what happens with that. Maybe it will barf. Who knows? I didn't check that. I should have. So all right. So you get this. Now what do I do? I have a function and I can apply it to every in the archive data set. You see that this data frame I called archive, isn't it? In this archives author column, I transform every value of the author column by this function to authors. So what will happen is the author column will now be replaced by a list of authors, isn't it? And we can see that. If you notice, we now have authors and then once we have them, we don't need the author column anymore we can just delete it drop it and then we have this let's look at a few rows of data when you look at this rows of data do you notice that it comes out very well all the authors are coming out very well much better isn't it as a list of people okay now let us do some very basic analysis on this data. Let us look at, for example, how many submissions were made in a given year. That is easy. You know, some of these fields are numerical. For example, a month is numerical, isn't it? A year is numerical. numerical, isn't it? A year is numerical. So you can do sort of group buys and counts on that. So you're doing a count on the number of submissions every year. So as you notice, research has been rising up exponentially. It's sort of one of the first things that surprise people. People think that research grows linearly, but actually, more research has been done in the last two years than in, I think that statement is true, more research has been done in the last two, three years than in the entire history of mankind. So the pace of research is accelerating, completely accelerating. And you can see that effects of example, well, archive is not a really reliable measure of how much research is done. But as a very rough and ready thing, you can see, if you take, well, 2018 is not complete here. So we should ignore it, actually. This and this, if you put together these two, you realize that they will add up to a number. Let's see, do they add up to a number or not? This is 12,000 plus 18 000 more than 18 000 if you add up all of these let's see what it comes to little more than 4 000 3 000 approximately 4 7 and another maybe 3 000 10 000 maybe a little more than 10 let's round it up to 11 000,000 and a little more than 2, 2500, 13 and a half thousand. You get the idea, isn't it? Maybe there's two couple of 14, 15,000 and so forth. So you realize that if you added the last two years, you'll end up with the same number as up till then. And it just shows you how fast at least this field of artificial intelligence and machine learning is growing. More work is done in the last two years than seems to have been done in all of the history of the subject. That is a remarkable insight, isn't it? And this is the point, you know, when it is one thing to pick up the technology, but gradually you should start thinking what is this plot trying to tell me we may say do people submit more research papers or finish more research in any particular month of the year when you try to plot it you get a plot like this so my observation was that there's nothing really remarkable about it these small changes i wouldn't read too much into it. Next year, it may be different. Going forward, it may be different or something. These are fairly small variations from 2,700 to about 4,000 something, 200 or something. So not much, a little bit, March and November seem to stand out a little bit, but not much. When you look at researchers, one of the basic, so now you can start posing your questions in experiment. In a field, one of the first questions you ask is, who is the most published author, number one? Who is the most prestigious author that everybody is citing or referring to? Who who is the person people have done who is the one author who has done who has the maximum number of collaborators in other words has collaborated with a whole lot of other people right you can actually start answering those questions all we do is little bits of code you take all the authors. First from that archive, you extract all of the authors. Do you notice that from the authors column, what am I doing? I'm adding all the authors to an existing list and from there I'm saying we found these many authors. Once you find those authors, you notice that there will be duplicates. The same author has published two papers. He'll show up in the list twice. Folks, I hope this is obvious to you, isn't it? If I just go and take the authors for each of the papers and add it to a master list of authors, suppose you have published three papers, your name will show up twice in this list, isn't it? It shows up, and we do a distinct. You see that there are 132,679 published authors, but because they are duplicates, the unique number of authors is 60,000 approximately. So guys, think about it. There are 60,000 people who have done some form of non-trivial research in this field and contributed to archive. This contains great luminaries, as you will see, and it contains, well, everybody else like you and me. Hope this is an encouragement for you to also do a good project and write your own archive, pre-print, and submit it. Discover something new and submit it. It doesn't have to be great. It just has to be original work of relevance to the field and of value to the field. So there are only two things needed to publish. A, it must be original and it must have some use. So for example, you cannot go to an obscure library and count how many books of poetry are there and how many books of this and that or whatever. If you do that, you may be the first person who has counted that, so you have produced something original. But perhaps that data that you produced or the article that you wrote may not be of much value at all. So that is something to know. Moving forward, we can ask who are the most published authors. So we get this authors. Who are the most published authors? How would you do that? That is very easy. You could just do a box plot. Once you create, by the way, this library is very useful. I often see people use dictionaries to create, you know, word frequencies and so forth. Here the word being the author. You can do that of course by all means, but it is more elegant to just use this package. This library in Python does the job and all in one line. And so then in from that, you can ask for the, for example, I happen to pick a 50 most popular or most published authors and I created a data frame for it and then I visualized it. When you do that, do you notice that who's the most published author? Joshua Bengi. Anybody knows who that is? Has heard of Joshua Bengi? No. Okay. Who's the author of your textbook for the next workshop? And he's a luminary, huge luminary, big guy. I think after Geoffrey Hinton, the next name you hear is his. But do you notice something quite remarkable? Geoffrey Hinton is nowhere there in the first, at least I didn't find him, in the top 50 published authors. So it just alludes to the fact that just because you write a lot of papers doesn't make you great. Sometimes great authors, they write sparingly. But great researchers, they write sparingly. But when they write, it's immediately considered a breakthrough. They come out with something very, very non-trivial and advance the field. So, but Joshua is a very prolific writer and he's also a very, very high quality researcher. So obviously you see him here. These names, as you do deep learning, they will gradually become familiar to you. Quite a few of these names will become familiar to you. The next thing that you may ask is authors collaborate. So now you may collaborate with 10 different people, they may collaborate with other people and so forth. So you can imagine that there is a network of collaborators. Isn't it? So for example, you may ask this question between me and that other researchers, how many links of collaborators do I need? So I may have written a paper with somebody, who may have written a paper with somebody else, who may have written a paper with the person, the target person that I'm thinking about, right? So then you would like to, for example, pass a message through and try to do a direct collaboration with these intermediaries as your reference. And so forth. So it is quite common to look at collaboration networks. I should say collaborate collaboration networks rather collaboration. So guys, you notice that I'm posing the questions things from common sense that seems relevant and doing it. So this code, I've tried to keep this code as simple as possible. All it does is from the author list, if two authors exist in the same paper together, I add them to a set of pairs, you know, tuples, A, B, if A and B happen to be in a paper at the same time. So suppose A, B, C are in the paper, how many pairs can I extract? I can get A, B, B, C and A, C, three pairs, right? So we do that. So this is what I do for every author list in the authors column, every value. For every, the value is a list, so therefore for every author in the list and every other author in the list, x greater than y is just to make sure that we don't have duplicates, like we don't have a comma b and then b comma a. So this is a string comparison. Make sure that by alphabetical order it is there. And you add the pair of those people. And at the end end of it you find that there's actually a surprising amount of collaboration in this field research is a collaborative effort massively collaborative effort very rarely people relatively rarely people write papers completely alone right so generally people collaborate and write this for example you guys have formed a team and you're collaborating and writing. So you form all those pairs and then now comes the graph part of it. Graph is very easy to get started with NetworkX. What do you do? You create an empty graph and to this empty graph you add all these pairs. You say these are the edges. So here i'd like to explain what i mean let me go to the drawing board i'm now going to the drawing boat oh why did i not get to the drawing board yes i did so um one note is here. So, and now where did I keep my digital pen? Absent-minded that I am. Give me a moment guys. I might have managed to lose my digital pen here. I keep it. Yeah, here we go. So, see guys, suppose we have a pairs of people, let's say A, B, right? These two have collaborated. Then we have C, D, then we have B, C, right? And let's say A, D. Let me take this example. Asif, are you sharing the screen? Oh my goodness, I am not indeed. I am not. So that needs another little switch back and forth. Give me a moment guys. And where is Zoom? Zoom is where? Zoom meeting. I apologize. Thanks for pointing this out. And which one do we want to share? Any better guys? Are we seeing my screen? Yes. Yes. Let the researchers be. So suppose you find these collaborations present. You made this list of pairs. Now what can you do? You can start with A or any researcher as a node. You can say B, you can say, let's say C, and you can say B. Now A and B have collaborated. So you can make a link from A to B. C and D have collaborated. So you can make a link from C and D. And these are undirectional links. So if one has collaborated with the other other has also collaborated with one so these are undirected links then c and d have collaborated now b and c have collaborated and a and d have collaborated oh goodness this makes a perfect rectangle let me make it something else. A and C have collaborated, let's say. A and C have collaborated. And maybe C and E have collaborated. So do you notice that you have a collaboration network? So if A wants to collaborate with E, all he needs is a reference from C, for example. Isn't it? And one more thing you notice that C seems to have collaborated with everybody else. So what happens is when you get a network, well, obviously, this is a very trivial network. But generally, when you get a network, you start seeing some very interesting patterns in the network. But the study of complex networks and the structure, not just like drawing a huge network out and wondering what's there, but they are mathematical tools, they are abstractions. You look for patterns and structure and clustering and the degree distributions and so on and so forth. So that is there in my other book, the book that I posted online, the network book, which is still in the works, that chapter. So yeah, it is there. And of course, you should read the NetworkX documentation. So read the NetworkX documentation. NetworkX documentation. Also, Barabasi's book, The Network Science, by the great researcher Barabasi, is completely online, is online in a beautiful format. The book is also available in print. Now, needless to say, the print is one of the most beautifully printed books. It's an amazing book. If you guys get a chance, or if you can, especially if your employer is part of your employment, you have a budget for books, you should certainly buy it. It's just, it's not expensive, I believe it's under a hundred dollars, 50, 60 dollars or something like that or 70 dollars. So anyway so we can draw the graph. Now the question is can we draw this graph and see structures in it? So I will go back now to sharing the previous Where am I? Okay, so I'll share the screen now. There we go. So here, you notice that, how did I create this network? I created an empty graph and I added all the edges from where? From this pair that I created of edges, isn't it? Then when I print information about this network, it tells me something. There are 57,000 authors. Amongst them, there are 178,000 edges. Interestingly, the average degree of a node is six. What do you think the average degree means? The number of collaborations each author has. Yes. How many collaborators on average does an author have? How many authors? They may not all collaborate on the same paper, but typically an author has collaborated with about six other authors, six other researchers in general, six to seven. Now, I started to draw this number. Asif, I have one question here. So if its average degree is like the number of collaborations, so shouldn't it be a whole number or like or no no it's a decimal because you know it's a fact it is fractioned over the whole it's just a number imagine that suppose you come up with um each one author has three collaborators there has four collaborators. And suppose these are the only two, I mean, suppose you're taking the average of these two, you'll end up with 3.5 collaborators. Okay. So, well, that is it. Now I wish this nx.draw had finished by the time I was showing you guys. I'll post this code. Hopefully you run it, go to sleep, wake up in the morning, the graph will be drawn. Generally this graph of, NetworkX is a bit slow in drawing graphs, really slow actually. For what it has worked, it takes a long time. It didn't finish and it should finish, God knows when. It was queued at 12.01. It's already 50 minutes and it hasn't finished. At some point it'll finish. So in other words, NetworkX is very easy to learn. It doesn't scale very much to large networks. So one easy thing you can do instead of just sleeping over it is instead of taking the entire graph, you can take the graph of the thousand most or maybe a hundred most prolific writers. So you can say, I will take only 100 writers and I will look at their graphs, the collaboration network amongst them. So that will be easy to draw. So you can take that as a challenge. How would you draw the collaboration? So let me write it down. How can you So this would be something worth. Actually, this sentence is too long. It is just collaborations. Collaboration between. This is better. I'll leave this as an exercise for you. You can do that. Instead of drawing this big graph, you can draw that out. Now what else can we do? This is just a starter code, guys. You can get the citation of each paper. Each paper cites some other papers and it is cited or referred to by some paper. So you can create citation networks. Do you see that guys? A cites B. So those could be directed edges. You could create that network and see. Then you will find that who are the authors or which are the research papers which are seminal in the field, which have extremely high degree of citations associated with them. You can find in terms and do a page rank of the graph. Let's say page rank algorithm is nothing but a graph algorithm. You realize that it is a graph of web pages referring to each other. And I invite you to look up the page rank and apply it to this graph and see what comes out. Are we together guys? To find the most influential authors or research papers and so forth. So that is your project. This is a project that I'm hoping that you guys will do. So do you guys feel that this is a good enough starter? Yes. Ah, yes. Can we go line 19? Line 19. 14, 15, 16, 17, 18, 19. There we go. You have collaboration is equal, can you explain this code or you're calling the oh i'm calling the constructor of the graph okay that is the and that's in uh in the nx library right that is right that's a network okay yeah sorry that's new to me way. I also one question like on this line 18. Yeah, so I didn't quite follow that logic. So for x in author list and for y in author list if x. Let me draw an experiment to you one second. Yeah, thank you. Sure. Let me now go back to sharing this. I wish this Cintiq writing pad worked on Linux. Somebody knows how to make it work. I have never been able to do it for some strange Hang on, where is zoom? Yes, oops, not this one. Where is zoom? Oh, there it is. So, share. Are you guys able to see my writing pad now? Yes. See, here's the thing. There's an archive data. Archive. So we created a data frame, like Pandas data frame. Let me start with scratch. In this code we have created data frame called archive, isn't it? Archive data frame contains amongst other things, a column called, so there's a ID column, there is a title column and there are other columns and then it has authors column, isn't it? Now authors column for any given entry, let's say to or something like that it will contain a list of let us say a a B let me let me take this so it turns out authors ABCD may have collaborated or C may have actually because it can confuse with the diagram above, I don't want to. Let me use some other language here. PQRST. So let us say this paper was written by five authors. These are all strings. I'm not putting quotes in there. Maybe I should. This is the value sitting here in the authors column. And there are more columns. We won't care about that. In this column, it is it. Now, what do you want to do? You want to do this for this entire data frame. So how would you take the entire data frame archive a exciting that authors. If you do this, what do you get, you get this column. You get this column. This is. Yes. What's a good thing? Right. What is it? It is essentially a series. Basically an array. Has an array. array of and each of these is itself a list, isn't it? A list of authors. So array means each of the row is an array, array of rows. Let me just call it author rows it is basically that now what do you want to do you want to take for each row that is for each of these so you would say for author list in this This part is now easy to understand. In other words, you want to take each of these things, items and break it up into pairs. You want to know s t p t you want to create all of these pairs am i making sense right so how would you do that you would say for every element for x in this author list this author list. So you take x and you take y, some other element y in also the same list. What do you want to do? You want to make a tuple. You want to make a tuple x, y, right? So that you will get get p q and so on and so forth are we together guys yes yeah so x and y in the list but except that you don't make a you don't want to make a pair of an element with itself right you don't want to create a pp isn't it and if you have created pq you don't want to create q p right so you want to avoid this and avoid this unique isn't it so one easy way to do that is if you put a condition if a p is greater than oh sorry x is greater than y what happens is x and y are strings, right? Because x and y are strings, isn't it? These are P, Q, R, S, T, right? These strings, these are autonyms. So when will x be greater than y? It will have to fulfill both this condition. A P, it is not itself. And secondly, in terms of alphabetical order, right? So for example, a Peter will come after adam is greater than adam why because a b c d the alphabet alphabetic order for strings this is one of the tricks in a lang in programming both java and python support that that you can compare strings when you compare strings the one that comes later the whose first letter comes later in the dictionary is greater than the previous one so you get the connection guys isn't it yes yes so if x is greater than y then what do you do suppose you have the pairs pairs is equal to let us say a list of maybe set of pairs what did i do i don't remember if it was a set i think it was a set of pairs right so then you would say pairs dot add what do you want to add the x y so do you realize how these three four loops came through and this f condition came through now yes yes thank you so much for explaining yes so all of these things now first think it through in your head and after that when you write code you will realize that if you have thought it through carefully the code will be very very simple you know now when you go back and look at the code it is actually pretty neat and simple you could do it in a more complicated way actually there are more elegant ways of doing it i did not do it necessarily in the most elegant way there are other ways also but it is certainly uh hopefully one of the simpler ways of doing it. I think this is basically an upper triangular matrix with off diagonal elements. Absolutely Shankar, you got it. Right, and so after that, I have all the pairs. What are those pairs? Those are the edges in my graph. Authors are the nodes and the pairs, the collaborations are the edges. Therefore I can draw a graph, which is what I'm doing in the next line and that is it guys so I hope I this was useful I'm going to post this to our slack channel in fact let me not say I will but let me do it right now otherwise I have one more question. The thing is that once I get out of this meeting, I've been forgetful. There is no guarantee that I haven't entirely forget it. Workshop kernels, kernels, networks, notebooks, where am I? This is the archive network. Why am I not seeing it here? Oh yeah, it's the very first one, of course. It's already highlighted. we go and so yes please go ahead with your question yeah uh this is regarding like the original author list that you got like was it a dictionary or was it a list i sorry i mean i couldn't understand that original data see okay let's go through this the original data. Let's go through this, how the data comes. So let's review what we did. So besides this preamble and including the libraries, we just loaded it as a JSON file. When we did that, if you look at this description, there is only one field author. It's a misnomer because the author field contains actually a string of all the authors. So when I sample into it, you realize that it's a mess. Do you see how this is a messy string? But it is not a dictionary, right? Because I see like a name colon. Yes, yes, yes. Initially, in fact, my first thought was that it is a dictionary. I inspected it and realized that unfortunately, it's not a dictionary, it's just a string. Okay. Colon separated string. See, I could have read it as pure JSON, but I read it as Pandas. But when you read it as Pandas, it doesn't go any deeper. So maybe if I had read it as pandas it doesn't go any deeper so maybe if i had read it as a json directly i may have been may have found it easier to get to the structure so there are many ways in which you can address it i addressed it this way then what i did is i just extracted the names so now you notice me doing an extraction and creating a new field called authors where the value is a list and creating a new field called authors where the value is a list is a proper list of names do you see here list of names isn't it actually let me do one thing just to illustrate this this is very So we can see it in all its glory. There we go. Let's try this. Oh, it can't run because this other project is running. Okay, let me go stop the other one. Oh goodness. All right, let me kill the kernel. Restart. So I can show you. Is it restarting? Yes. So let's go from the beginning and run it. This doesn't need to be run. This is straightforward. I'll keep running it. Yeah, we, where was I? Basic author. Okay, so we'll continue to run. So here we are dropping now we have it in the right way that we wanted. Look at this. Karma separated list of authors. Are you seeing that guys? The authors field is comprised of a karma separated list. Yes, yes. You elaborate. That is neat. And it is a list, not a string. It's just a list, proper data structure. So what do I need to do now is we can derive some basic statistics from it. We can do some fun stuff. We can do this. We can find them typically this will run fairly fast 50 most published authors are here. Right now comes the finding the collaboration networks. And what you can do is insert above. Let's see. You could print it out, you know, pairs. Let us see what it looks like. What does it look like guys? It's just a couple between two authors, right? Yeah. They are the edges. If you treat an author as a node, these are the edges in the node. Make sense? That is it. I will delete this node. And then comes creating this graph. Creating the graph also is reasonably fast, but drawing this graph will take a long time, so I won't go into it at this moment. Actually. Actually, the node types are not as easy as I leave the node types as strings. So this is better. And then this on my machine will take some time to run. So guys, that's it. Get started with your project. Make some progress. I hope this is enough to get started. What I haven't done are very easy NLP things. Which tags show up most often? What are the most common tags? What words show up often? Do basic NLTK. Do that. So do three kinds of analysis, tabular data analysis, network analysis, do that. So do three kinds of analysis, tabular data analysis, network analysis, and textual analysis, three different dimensions. And then you can build sort of all sorts of models and so forth out of it. There is ID tags, you know the tag terms. You can also look at the tag terms. we can also look at the tag terms all right guys so with that i will stop stop today does anybody have a question otherwise i'll stop for today and i will also... That is it guys. So with those words, I'll finish today's session. Thank you. Thank you. Thank you. Thank you. Bye. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Gracias. All right guys, nobody seems to have any more questions that I see. Asif, one quick question. So are drill downs possible on the graphs? Yes, of course. I mean, it depends upon how interactive or what you want to do with the graph. OK. Different dimensions you can do that. Yes. Graphs are very powerful structures. You'll see that more and more people are not appreciating the value of graphs. It seems to be a specialized topic, but you'll find that everywhere, the graph and graph network analysis will be there and machine learning on graph will be there. That's why I keep emphasizing to people that get started with graphs early, not just graphs, but graph machine learning. Thank you, Asif. All right then. So do you recommend any books for that? See, first learn graph. This Barabasi's book is the one I would recommend. It's freely available. It's also available in print and a lovely book to buy in print. And then once you're done with that on graph neural networks, there is no book at that moment, unfortunately. It's all made researchy. So Barabasi's tabular data. Barabasi's for graph data. I will post the link in the chat. Thank you. All right, guys. All right, that's what I'm finally ending it. Thank you, bye. Bye, guys.