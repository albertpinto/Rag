 So welcome back. We were discussing this week to a class of algorithms in neural network or an approach that I call the bottleneck method. This word is not called like that. But to me, it seems to all be based on the fact that if you pinch down a neural network and its flow in a hidden layer, it is forced to somehow summarize the information or extract the essence of the information. So in that spirit, we talked about a few things. We talked about, for example, principal component analysis, and we realized that it's a special case of doing something like this. I'll review what we did. We said that all of these neural networks, they have a certain structure. The structure is that you can compress things down to a much lower dimensional representation. I use the word z for the lower dimensional representation and x or the initial vector space as x, which is a much higher dimensional space. One example that we took, which is the singular value decomposition, would be that. So in singular value decomposition, you just do SVD, LSI, latent semantic indexing, you do singular value decomposition. The same idea carries forward to this bottleneck algorithm. The only difference is singular value decomposition is inherently a linear method. Whereas we are generalizing this now to a more complicated situations. It need not be like that. It may be more complicated than that. So, and sometimes linear is good enough. For example, today, we are going to talk about something called word embeddings. It turns out that they are mostly linear. The sort of bottleneck you create is through a linear transform, a matrix multiplication. And you don't apply in the hidden layer much. You don't apply an activation function at all. So it's a hidden layer, the input layer, the hidden layer, the output layer, the input layer is pinched, it's a bottleneck and usually you do or do not apply activation based on situation but for word embeddings, things that we're talking, going to talk about, you don't apply an activation. You just do a straightforward matrix kind of a multiplication. So now we'll learn a little bit more about it. When you create an architecture like this, you give it an input and you give it an output. These things are, for example, examples of these are autoencoders. That is one example we did. And we did of these are autoencoders. That is one example we did. And we did the variational autoencoders. Principal component analysis, it turns out, is also another example of it. If, for example, you do take the data in, try to do a lower dimensional representation of it, and then you project it back to the original data, what will happen is, so long as you don't apply activation function, or another way of saying it is, so long as you apply an identity activation function, you will essentially recover PCA. Because a PCA is literally that. You take the input and you multiply it by a matrix that reduces the dimensionality, and then you get back to it, right? So that is that. Now, I will just go back and talk about this. In the context of, let's say, so anyway, this is the terminology we use for input. We use x and we'll use z for the r squared right now in auto encoders z is often used also in GANs z is often used for the latent the latent variable in for example but but you have to know that that z is actually, it's an activated Z. It is the weight times the input with bias, the weights and bias applied, and then a nonlinear activation applied to that, which is unusual. If you remember in the deep learning literature, sorry, the feedforward literature, we would say that z is just a weight w dot x plus b. And then we would apply the activation to get the y hat or the output of that particular layer. So there's a slight difference in terminology in these two literatures that you should be aware of. I'm adhering to the terminology that is common here. So how do we do that? Suppose you have input and you have output and you expect the same to go there and you're representing it with the lower dimensional thing. So one sort of a very hand waving intuition I gave is think of it as milk, lots of milk. You have to transport it across the seas. You wouldn't want to transport it, all those gallons of milk. What you do instead is you dehydrate the milk into milk powder, right? And that is your sort of it captures the essence of the milk, you transport that over to the other place and then you sort of rehydrate it back into milk, right? Without much loss, hopefully of nutrients and so forth. It's a pretty successful approach. So the same thing is the basic concept of autoencoders in many ways. Now the principal component analysis is a special case in which there is no activation function, identity activation function. So you apply a weight matrix here, you get the hidden variable, and the hidden variable, if you apply a different weight matrix, which is the transpose of it, you would end up getting back the original thing. Now the thing is, in an autoencoder, in the simplest form, whatever you're given, you expect to come out. And so your loss function is the gap between what you fed in and what you got out. So that's your basic autoencoder. You can write it by saying, if this is your loss function, you're looking for the argument of WW prime says that the loss function gets minimized. So how would you do that? The normal technology of gradient descents, back prop and everything kicks in. Nothing unusual here. Remember at the end of the day it is just a neural network. In fact this is a dense net with single hidden layer. So it does work. When you attach a complicated latent or activation function, of course now you have non-linear distortions non-linear transforms and you can do a much more complicated things so for example one interesting game you can play and that is very effective you take data and you add noise to it you feed that in and now you look at the response what happens is when you look at the response and you look at the response. What happens is when you The autoencoder learns to abstract out the epsilon, the noise from the picture. It learns to see behind the noise, right, at the actual picture. And that is quite a lovely thing because now you have created a system that can denoise an image. Now, once you have trained it, you can give images that have a certain degree of noise, pixelation, etc. It can fill it. Not only can you do that, for example, you can give it a letter with text on it, printed text or whatever, handwritten text, and it has creases and a coffee stain and so forth. And you can pass it through a denoising autoencoder and it will remove the noise, it will remove the clutter in the background and you expect a pristine page with writing to come out or text to come out and it does so come out. That is a huge value of a denoising autoencoder. Then you can use it for something else for example what you do is you take a image x and then you apply a mask so you have x plus mask you deliberately put a random mask m somewhere in the picture but when you feed it through while in the training process you ask it to recover the actual image, fill in the gaps, right? So your loss function, you still do between x minus y. So suppose y came out, y hat came out. Let me just write y because in this literature, people just call it y or something. Let's say y came out. You still do this. So now what happens? you're basically asking it to get rid of that mask somehow and fill in the gaps now the the the beautiful thing is it's able to do that quite often with a pretty good degree of success just goes to show the power of auto encoders now we take this one idea forward and say that, see, let us not have just one hidden layer. Let us have a symmetric situation such that there may be many layers before that bottleneck. They're converging to the bottleneck and then they're radiating out from the bottleneck. And you can have many layers so that you have the full power of a deep neural network and the if you remember a neural network is nothing but a function at the end of the day it is a function approximator so you can still think of your deep layers before the latent the latent layer right as some function f and you think of g as another function now that takes a latent representation and produces g right so in other words x goes to z the latent representation using some function f which is represented by the west part of the neural network deep neural net and it goes through g to produce y right which is the second part of the neural network so this is f and this is g right f of x and g of z yeah are we together and so y comes out and x goes in out and x goes in. So far so good guys. So now we put dense layers, many dense layers, but then you ask this question, can we do something better? For example, for images, why not use convulations? We learned that convulations are very useful. Let's put a lot of convulation layers instead. And so be it. Between the input and the latent layer you can put deep layers you can put convulations you can mix and match you can do all sorts of crazy things and so those are those are called convolational autoencoders then there was a simple question to reflect on we tend to create bottlenecks what if we did? What if we instead had the latent layer wider than the input and the output layer? So then you have to be a little bit careful. If you're not careful, what will happen is, if there is no noise added or something like that, it will just learn to memorize the input and just pass it forward to the output, right? So which is why it is less common. But actually, there are situations where you do this. One situation, which I didn't cover in detail, is that somewhat counterintuitively, it tends to, you can come up with a sparse higher dimensional representation of an input using this method, but you have to be a little bit careful there's some implementation details and so forth so those were auto encoders the traditional auto encoders they have a problem though what they tend to do two problems actually is that they're the the classes right they represent certain spaces in the latent factor space right certain regions of the latent factor space right and those regions sometimes overlap and when they overlap it's a little hard to tell if you pick a point from here are you looking at a one three nine what are you looking at right or what your digit you're looking at or what thing you're looking at which class you're looking at so that is one problem the other thing that So that is one problem. The other thing that it doesn't quite deal with very well is if you randomly take a point in the latent factor space, right, and say, go move forward and just apply the GZ of it. This is Z point and apply GZ to see why you will actually end up in trouble. You'll get some garbage because this point was never defined. There is no, there's nothing really here to move forward to. So in other words, the latent factor space has some blank holes to it. It's a very rough and ready way of saying it. So then there's this technology of variational encoders, which is a little bit more complicated. It uses a variational inference argument. What it says is that if you look at this, you're asking that given an input what's the probability of seeing z here. This is your quest. Given 8, what will the latent representation look like approximately? And when you use the bias theorem then you get this expression in which the denominator and this is only for those people who have taken engineering math with me the denominator is intractable remember this denominator when we did bias and learning was the source of trouble one approach to deal with it was Monte Carlo, the other is variational inference so. Vaidhyanathan Ramamurthy, A very short inference approach is not very much is very is very straightforward actually once you get the hang of it what you're basically saying is. Right? X given Z. So what you do is you just approximate the distribution in the latent space, the distribution using some Gaussian. So what does it mean in very practical terms? Suppose there are only two classes. Just in the latent factor space, pick some points. It will end up being like this. There'll be some Gaussians in the Z, like a P dimensional space, Gaussian hills that represent, let's say, suppose you have two things. Well, let's say the digit one and the digit zero, if that's the only thing you have, they will separate themselves and then more of them. And the only question therefore is, if you make the hypothesis, what you're doing is you're imposing, you're projecting a hypothesis on the data. You're saying, what if this were true? Right? Then what sort of Gaussians would represent this, would approximate this very complicated distribution that is truly there, but that I can't see, that is there, a good latent representation, but you have a problem to figure out what it is. Now, Gaussian's, as you know, they're pretty nice things. I mean, they can approximate a lot of things and they also belong to a class of explanations that are themselves universal approximators. so they're very good actually in adapting to many many functions very well actually so especially when you see natural things like digits you do expect that all the zeros will look essentially alike so they should expect or they should have a cluster of some form in the latent factor space. So in that case, you just assume that what is the Q, a distribution of Gaussians, a mixture of Gaussians, that I can presume or I can say that it is a good proxy for the reality. Now if you think of the underlying latent factor space filled with Gaussians, right, one for each of those classes or a mixture of Gaussians, next comes a very interesting problem. All you have to do is, Gaussian is uniformly, is completely described by the mu and the sigma, the mean and the covariance matrix. Sigma is the covariance matrix. Now, covariance matrix is something we talked about in ML100, but you may have forgotten. But if you remember covariance matrix. Sigma is the covariance matrix. Now, covariance matrix is something we talked about in ML100, but you may have forgotten. But if you remember, covariance matrix is fine and good. In this situation, we'll simplify it even further. We assume nice, well-shaped bell hills such that the only thing we care about is the diagonal. We just expect that the latent space correlations are approximately zero and the off diagonals and it is just a diagonal matrix of different variances along the different latent axes. So if you do that then you have a very nice way to figure it out. Now to figure it out there's a little bit of mathematics. We talked about information gain. What is the information gain? And the intuition we used is if somebody tells you something that's very likely, right? So let's take an example. What is a likely, I mean, I give the example that the sun will come out in the east. Now most of us don't doubt that. So suppose you're telling a friend that sun will come out in the east tomorrow, you're telling something that's pretty obvious. But if you're, let's say that a highway tends to go under repair and 10% of the time it's under repair, you tell your friend that, that oh there's a repair on the highway and there's congestion. So that is more information because now your friend needs to hit the highway earlier in the day for commute. Right and suppose you give information about an even more unlikely event. Let's say that you say that there is a bridge that has gone under repair for some reason and the likelihood of that was one in a thousand right once in three years you have a situation in which maybe on the bridge one of the rivets or something or struts somehow gets into into a bad state and needs to be urgently or immediately fixed and they closed part of the bridge. Bridges are fundamentally bottlenecks and then you do some repair on the bridge, you know what happens, we all have been in the Bay Area. So that is far more information, isn't it? The value of that information is high. So therefore the concept of information gain, how much information did your friend gain? it is essentially the inversely proportional to the probability of a event in particular you take the log of it right you give it a lot now log to what base it depends which field you are in if you are in information processing or information theory you tend to take log base 2 because there most of the information you look at is bits and bytes uh bits zeros and ones doesn't matter machine learning people and physicists they tend to take the log to the base e the natural log the two are related it doesn't matter just by a constant so which one you pick doesn't matter very much so suppose you get information then the next question that comes is let's say that you keep informing your friend because you wake up early in the morning, you listen to the radio, and then you tell him one statement about things. What is the average information that your friend is getting? Average is called expectation value of the information. How much information gain there is averaged over a number of days days so that is the proportion of the events it happens p times in general and times the information from that which is log p so p log p summed over all values of the outcomes becomes a quantity called averaged information gain average information gain has a name and the name is entropy. So that is something. So in other words, this is your entropy. And it is. And obviously here, when I write it as this, I should also say that I over all values of probability. Obviously, I skipped over some of the notation XI and probability of the outcome xi of all possible outcomes, right? Maybe I'll write it in big letters. Sigma, let me write it in big bold so that we tend not to forget it. Sum over all possible outcomes. So suppose you have our outcome Xi and its probability is P Xi, so log P Xi. Where X is whatever random variable, for example, whether tomorrow the road is free, the road is undergoing repair, or the road is undergoing drastic repair on the bridge itself. So maybe three outcomes and so forth. You can play with those numbers. So that is entropy. And you say the word that people use is that, so this is the formula. Like if you remove the indexing and the summations, think of this as the entropy. You assume like the sigma, sometimes people forget about putting the sigmas there, the summation and so on and so forth there. And there are situations, like for example, you may have a continuous variable. So what is the probability of some random variable which is continuous? Temperature tomorrow being something. And so what is the information gain if somebody told you what tomorrow noon temperature is going to be? Temperature is continuous, continuous the infinity many options, so this summation will become an integral at that particular point. Gaurab Rajanthamishra, So things like that, but the one thing to remember is entropy which is usually represented with the edge is essentially the expectation value of the information game and we together that's all it is now what happens is. Gaurab Rajantham Now what happens is, suppose you work with a certain amount of information so that when you draw from the data, when events happen, what happens is that you assume a distribution P, but let's say for anomalous reasons, the events are happening not the way you want it. They're happening with a different distribution Q. And then there is a divergence between what you expected and the information gain that is happening. So that is called the cross entropy. And this again again, is something more details we keep for engineering math class to give you a good simple intuition for it. We drew some pictures, geometric pictures, if you recall. But I won't go into the geometric way of looking at it, but it is. It's like if you have p in mind, but q shows up, then what is the cross entropy? The cross entropy is the P log Q. Q is the information gain with the understanding that P times it was supposed to happen. So that's your cross entropy. Now, when you have the entropy of an event P, and of course P is happening, things are happening as you expected. And when you have a P and Q, there is a gap between the two entropies, cross entropy and the entropy of P itself. And that is called the KL divergence, that gap. That gap is not symmetric because your expectation was P and then you started seeing Q. So it becomes a reference point that you're comparing it to so that is your cross entropy I wouldn't go into it now what is that related to in our variation and autoencoders the idea is that if you don't know this distribution and you just say that actually I don't know but I'll just assume it's a bunch of gaussians actually i don't know but i'll just assume it's a bunch of gaussians right a mixture of gaussians then then comes an interesting thing you want to decrease the divergence between q and p in other words you want q to imitate p as much as possible and how would you do that well quite literally kl divergence becomes your loss function you you, you make the autoencoder learn in such a way that it minimizes the KL divergence. And then whatever parameters come out, what the parameters will come out? The mu and the sigma vectors, you know, of the bell curves, what is the mean and what is the essentially the sigma vector if you assume diagonal and no off diagonal elements of the covariance is zero so this is what you get right so that is a summary i hope of what we did the last time right just as a recap a bell curve is a fun question go ahead what is the difference between kl divergence and cross entropy loss what is the difference between that see the two are different cross entropy is uh so anyway let me do that i think i did that at some point let's just do that let us say that i'll give you a classic example let us say that you're trying to you're writing a classifier. Classifier, some X vector goes in, and what comes out is a cow or a duck. Cow, duck. So pick one of these. Let us say that probability of cow, right? So I will take the random variable y to be so y hat is the prop suppose this machine because at the last day if you remember and you said we put a soft max or a either soft max for multiple variables or a sigmoid you remember logistic regression right logistic unit activation unit so what you end up is the probability let's choose the word for cow probability of cow right so this is the probability of the cow right this is what you're getting now what is it that you were expecting when you're training the data you know that this is either a cow or a duck let us say that you you take instance x1 and it was actually a cow and you produced a probability y hat is equal to zero point pick a number between zero and one is like point eight point eight and let us say x2 was actually a duck and now pick a probability of it's being a cow that your machine predicts well it is pointed no no for a duck oh forthy, yeah if it is a duck, what is the probability that it's a car let's say that you say that the your machine is not yet well trained i'll just put it like this 0.5. Vaidhyanathan Ramamurthy, problem. Cow. So it is saying a little bit, a number smaller than eight. But see now the cross entropy. Let us think. We associated cow, two, one. What should have come out? The answer that we expected for cow is what? One, right? Complete, what is the problem? What should have been there? The label is saying cow. When the label says cow, what does it mean? Probability of cow is equal to 1, isn't it? That is the ground truth. Look at this situation. Would you agree that here the ground truth was the probability of cow was one, right? Which is the Y, the ground truth Y, the label. And it is, you are predicting a probability of 0.8. So do you see two different probabilities here? You can think of this as P and this as Q. The probability that you predict is Q. Are we together? And now think of this as p and this as q the the probability that you predict is q are we together and now think of cross entropy see what are you doing when you write a term like this minus p1 which is x1 is that a px1 so you can say p1 which is the cow log you can say P1, which is the cow, log Y hat for the first. Let us just think about this. This is nothing but, in terms of the language, if we consider this log Y hat to be Q, in the language that we are using, it log q1 are we together if you identify as the other probability this is what you are seeing right q is that y hat is what comes out of your machine and uh p is the ground truth that these are two probability ground truth is the probability of cow should have been one and the probability of a duck should have been zero. But what you are seeing is Q. So now what happens is this, why do you consider a loss function with this? Why is the cross entropy a loss function? Let's take this. And likewise, let's go to the duck. What is the probability of a duck? probability of a duck it is zero right so you can say that one minus the probability of a duck right is uh one minus p two times which is the probability of a duck one minus p one no p2 p2 the second data point is duck. This one is actually a cow. This one is actually a duck. Isn't it? So if you think P2, cow, is equal to zero, isn't it? The ground truth is that it's a duck. So what is the ground truth probability that it's a cow? Zero. Right? So it is one minus p2 that is equal to 1, which is the probability of it being a duck, times the log 1 minus q2, which is essentially the same as 1 minus y hat 2. So at this moment, ignore the second term because just look at this term, first term. What is it telling you? This number, cross entropy, what do you really want it to be? Zero. No, you want it to have the smallest possible value, right? The smallest possible value would be minus p1 log p1. Right. At this point, right, the cross. So in other words, let's go back to it. Cross of KL is equal to cross minus entropy. So think about this. Cross is equal to KL. Right. The KL divergence, if you if you think about it this way, cross plus entropy. Right. So what is the KL divergence is always a positive quantity. It's a measure of how different they are. It's never negative. Right. So what is the smallest value the cross entropy can have? It can have the value of entropy itself, entropy p, the expected thing. Do you see that? If you go back to the definition of KL divergence, it is this. So the point is that the cross entropy is always more than the entropy. So the point is that the cross entropy is always more than the entropy, right? It can, as your learning takes place better and better, you want Q1 to tend to P1, Q2 to tend to P2, you know, you want these matchings to happen. You see that, right? And that is what you are pursuing but um but kl divisions itself is minus p log q right yes yes it is it is see kl is equal to minus p log q what is uh no no it is not minus p log q this is cross this minus p log q is the cross right and the kl is cross minus cross entropy minus entropy but the formula we saw earlier was just p minus p log q for p no i did not no no i did not if you look up look at this look at this this is called cross entropy you see that right you got mixed up you got mixed up between the two are we together now that is it guys and so so the point is, why do we use this thing? And now comes an interesting fact. Is it, in some sense, KL divergence? Are we minimizing KL divergence in even a binary classification problem? Yes, we are, because think about it this way. KL is cross minus entropy. So if you minimize KL, now entropy is going to be what? When the ground truth, the entropy of the ground truth is what? Zero. Like if the probability of a cow is one, what is the entropy? There is no information conveyed, right? If I tell you that a cow is a cow. So it's zero. So in a way, when you do the, in classification, when you do the cross entropy, it is also equivalent to, equivalent to minimizing the KL divergence. Minimizing the KL. Can we call KL divergence as kind of a regularized cross entropy? You don't use the word regularize. See, regularize is to dampen down the parameters of the model. They're absolutely equivalent, whether you minimize cross entropy or minimize divergence, it's exactly the same loss function. But a KL divergence has a more general meaning when you look at a situation like this, when you look at an autoencoder and for many large class of problems so i'll give you a very basic example see suppose you take the height of 100 children right a thousand kids that you randomly picked up what will their heights look like it will look like this right a bell curve curve. All of people it will look something like this. So this is your P but suppose you went to an island and you notice that the island this is the you know the story Gulliver's Travel and the Land of the Lilliputs. Do you guys remember that story? Lilliputians? Yeah, Lilliputians, yes. So we have, so suppose your distribution now of height is now, like, let's say something like this. Like, for whatever reason, Lily, with just a few people people very rare people being tall enough so are you surprised by seeing this if you were to do that it would represent an element of surprise isn't it that's like yeah right and what kale divergence maps is you realize that what you expect and this is your Q and KL divergences therefore right it is basically the gap between P and Q actually here it should be Q given P but okay Q given P right how much surprised are you when you encounter Q but you were expecting P a Q, but you were expecting P. And there are many variations of it. So for example, you notice that if you take the, you know, we all go through performance reviews in companies, it's the bane of it. Somebody, some manager sits and judges us and throws a number and generally most people are some people feel happy some people feel dejected for at least a month and then they leave the company actually performance reviews can often cause is a causation of a lot of attrition is the bane of corporate life right so there you again expect that the average score, so suppose there's an average score of, I don't know, on a scale of five, most people should get three. Underperformer should get two and overperformer should get four and achiever should get, super achiever should get five. But you go to a team and you suddenly find something like this, a distribution like the yellow one. What is that manager trying to say about the team? He's basically saying that this team is made up of intellectual lilliputs. These are intellectual dwarfs, and he has no respect for the team. You should be surprised. How come the perception is so bad? And so things like that. So KL divergence is a very useful measure that you use to compare two distributions. It's not the only measure by the way, we will talk about as we move into this course and when we do networks and graphs and so forth, we'll talk about other divergences. So here, the entropy is calculated based on what is expected or what we have talked about? Yes, and you remember, so Nisarg, I went through quite fast. And so let us write it down. The entropy of an event associated with this is this, of a, let's say that entropy associated with a probability of some random random variable right is essentially this p xi log p xi probability of that event are we together? It is by definition. In other words, it's not that entropy is equal to and somebody has computed this formula. This is the definition of entropy by definition. By definition or another way to say that is entropy of PX is equal to the expectation value expectation value statisticians write it with this unnecessary extra line through it and so whenever you see it you should know that you are doing expectation value of the information information associated with p px which is the same as expectation value of log px so this is literally by definition you're defining it you're not deriving it or something like that you're just defining it it would be minus right uh oh yes uh the yeah minus yes you're just defining it it would be minus right in front of the oh yes the yeah minus yes you're right expectation of the minus of the log of px are great no in front of the summation i think here also and here also yes you're right yeah so because information is information of px remember is again by definition minus log px this identity holds yeah forgive me for skipping a minus here and there so i guess in in uh the best loss function you can achieve for k l divergences twice the expectation of the expected value negative. No, the KL divergence has this property KL divergence div, right? So KL, every KL has this property that it will be greater than equal to zero. Because remember, it is the gap between again to stated grass between cross entropy and entropy because this is always more than entropy see the the clutter or the extra information that comes when you're surprised is more than than the information when you're not surprised isn't it are we together? So let us take a probability distribution that on Jan 5th, or let's say Jan 15th, of every post-election year, you find a new president, that is your probability. But some year you find that Jan 15th has come or Jan 20th has come, but there is no new president, is the same old president. That's a huge surprise, isn't it? It's a nightmare. Yeah, so that is your cross, that's sense of your cross entropy. Why? Because that which happened, happened quite different from what you expected. That's it. When things happen, it's a measure of surprise, Nisarg. You know, there's nothing fancy about it. It is just saying, what is the gap between what you expected, which was p and q that you're seeing, what your data is showing you. Yeah, it's just like it's both the expectations are distributed between cross entropy and entropy. That's right. Entropy also has the P and cross entropy also has the P. Yes. So P is your expectation, because if you're surprised, you're surprised with respect to some things that you expected to happen. So the positiveness of this calculation comes from entropy, right? Because entropy is basically becoming high as a result of this. That is right. There is more information. Entropy is a measure of information. There is more than you thought happening. What you thought happening was p. Now there's more than you thought happening. Right? So you have one look at me and you expect me to go, suppose I were to go jogging, you expect half a mile should be it. One fine day I come back with a 20 mile behind me of running. Would you be surprised? Would you feel that there's something really big going on? Secret marathon trainer. Yes, exactly. So you would want to know that. That's cross entropy. Expectation is at most the probability that I'll reach 20 miles is zero, close to zero. But then suddenly I do that. Then I come back a changed man. And so now you have to worry that the everyday distances I run are different from the distances I used to run. And between the two, there's a cross entropy. So in other words, you have to recalibrate your expectations then. So that's the idea of cross entropy. Guys, are we together now? Or do we have more questions on entropy, cross entropy and KL divergence? KL divergence is the high level. I think I understood at the high level that I need to look at the references. Yes, you have to read the book here. So one thing that we have to do, I'm giving you an entryway into the concept, but it will solidify only when you go back and start reviewing the book. By the way, the deep learning book that you have, the one with flowers on it, does devote time to talking about entropy, cross entropy and KL divergence in one of the earliest chapters, I believe chapter two. So please do go and review that. in one of the earliest chapters, I believe chapter two. So please do go and review that. So with that, is that a good summary, guys? Or are we still feeling completely lost? See, here's the deal. The mathematics of variational encoders, if you didn't understand, remember it was an advanced topic. Ignore it. Just remember that there is a smarter way of doing, one rather powerful way of doing autoencoders is through using these generative methods by assuming a distribution. And when you assume a distribution and you figure it out, now you can sample from the distribution to create all sorts of output value. So what it means is you can, it becomes a generative model and you can produce all sorts of shapes, right? Example is you take a face of 100 people, in the lab you'll do that, we'll take the faces of celebrities and what we will do is we'll create completely new faces And what we will do is we'll create completely new faces that are hybrids of those celebrities that are not real people, but they are somewhere in between. And it's very interesting actually. So we'll see that, and that's the power of a variational model. See variational models and the generated models of this quality. Once you can train a generated model. Now you can cook up data and do you see the similarity between this and GANs in GANs also. What do you do, there is a data distribution P, the actual data distribution. Right, px. And what you're trying to do is you're trying to fake it with some qx, the generator, generator, right? And this is trying to catch up and infer the p and sooner or later it figures it out in the GAN. And something here is similar except that it's not a GAN architecture, it's an autoencoder architecture. In both of these cases you have a generative model. architecture. In both of these cases you have a generative model. And generative models are great because you can create new data once you have trained them, very realistic new data. New data with it. So far so good guys. So I've given the better part of how much time uh to okay what's the time now i should keep the clock eight o'clock we have given about uh 45 minutes to reviewing i would like to take a very quick break of five minutes and we'll consider the review of last time over right see guys if you understood the auto encoders but didn't understand the variational part you're still doing very good variation will take a bit of time you have to go review the book and you'll get it right and then if you still don't understand please come and sit with me right over the weekend and i'll re-explain it to you Thank you.