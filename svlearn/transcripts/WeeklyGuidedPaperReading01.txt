 All right folks, today we have gathered for the Sunday paper reading session. The topic today is interesting breakthrough work that was done by Open AI group. This work is called the Dal-E. Dal-E is a sort of portmanteau, the combination of two words. One is the great artist, Dali, and I believe his full name is Salvador Dali, is that right? I think so. Kate, is it? That's right. Salvador Dali, is that right? I think so. Kate? That's right. Salvador Dali. Yes. And the lovely Pixar movie, WALL-E. So, Dali the great artist and WALL-E, the movie, together make Dali. Dali is one of the emerging body of research. This one is very, very interesting. What it does is you give it a prompt, a textual prompt, or maybe a little bit of an image combination, a typical thing that you would do to a transformer. So think of a standard transformer model that we covered in our previous sessions. If you go back and remember all of the BERT and the attention is all you need and all the things that we covered and the visual transformers. The fundamental thing is you give it a prompt, you give it some input, and then you have an output. In particular here, what we are talking about is using GPT-3. Now, just to remind you, remember we have the full transformer as an attention is all you need kind of a diagram with its encoder and decoder. Now, at some point we talked about BERT, but more or less uses the encoder part of it only, architecture, and it evolves from there to do its work. And the GPT family, they started with only the decoder part. And what's the difference between the decoder part and the encoder? Encoder, you have input, and it sort of does all these attention layers and a few feed forward layers. And then it, remember all of this K, V, Q and all of that, we won't go into that, but it produces a vector, those final vectors of K, Q, V. And then that's the encoded data that finally goes into the decoder in the full attention model. What does the decoder in the full attention model. What does the decoder do? Decoder takes the output, the encoded output, or the encoded information. Then it takes a bit of the, again, the input. And quite often the input is the previous output. So there's an autoregressive cycle to it. It takes it back and then combines the two together. aggressive cycle to it it takes it back and then combines the two together and what it does is it produces uh encodes and decodes the abstraction except that when it decodes the classic language model is you encode english into a sort of abstraction and going english sentence into an abstraction into those vectors and then what comes out word by word, token by token, is let's say French comes out, a French translation comes out. And the whole point of attention was that words can pay attention to each other because they, in some sense, understand where to look, how to do it. So the French sentence comes out looking the right way. And we won't cover all of those things. We have covered it many times. Now, that's how this thing goes. Now, again, GPT is only the decoder part of it. So you feed it to something that you have encoded, and then you also take the output sum of it, your feedback and so forth. So that is the decoder and that's the GPT. Now, GPTs have been, and BERT also, but are dominant implementations of transformers. There's a whole family of BERT, all sorts of BERTs are there, LBERT and this and that and ExcelBERT and so forth. So ExcelNet. So, and veryBERT and so forth. So ExcelNet. So, so very successful for their purposes. At the same time, the GPT's have also been very successful. When GPT-2 came out, it was supposed to be remarkably good. Lot of people were using it for a lot of things. Then came GPT-3, which is ginormous. I believe the exact numbers escape me, but it is in hundreds of billions of things. Then came GPT-3, which is ginormous. I believe the exact numbers escape me, but it is in hundreds of billions of parameters. The sheer thing is mind-boggling that you can make a model with so high, in such a high dimensional space, like so many parameter space. You're talking billions now, hundreds of billions. So GPT-3 has been remarkable, as you know. So the basic idea that you should remember with this transformers is you give it some input and in some sense it continues that. It's sort of, for example, language translation, you give it English and it will, from the representation, it will produce a french or you give it a sentence english sentence and you say what comes next what is the next sentence that should come out of it right and things like that so now these transformers have been very effective obviously in the nlp domain and we covered it in our nlp workshop quite extensively. We also talked about the visual transformers, the fact that computer vision has been completely dominated by CNNs and the transformers weren't quite there. But then came the papers and we have covered them, the 16 by 16 and so forth, in which you slice up the picture into little pieces, into little tiles, and you treat those tiles as tokens, effectively, as words. 16 by 16 pixels becomes equivalent to a word, and if you treat it as a word, you can, the whole machinery of natural language processing and and transformers that we had could be sort of reused. I mean, that's the basic high level idea. So when you do that, the visual transformers, it began to appear that quite well, they would achieve the state of the art or beat the state of the art, right? And today we are now seeing quite a revolution and it's happening all around us. Computer vision is now heavily, heavily influenced by the so-called visual transformers. And obviously CNNs are not gone. There is still a mixture of CNNs and attention in the mix, but certainly it isn't all CNNs anymore. It is transformers in CNNs put together. So one of the prior work has been, so for example, if I show you a face, let's say, if I give you a face like this to the computer and ask the computer what comes next, can the computer extrapolate and fill in the rest of the things and come up with this. Now, those problems are interesting and one of the image, this thing here, work GPT does exactly that. You give it some image and it will complete the image. You give it a cat, it will do it and the results are quite remarkable. It sort of forms a background. You show drops of water and you just show a tiny bit of ripple and the transformer is able to infer that, oh, there are ripples. And it fills in the picture with beautiful, realistic, very physically realistic water drops falling into and creating ripples in water. And when you see that, and maybe we should look at that also today a little bit, it is absolutely stunning that it can understand, it understands the physical world so well. It gets the lighting right, it gets the shadows, it gets the 3D effects. It seems to get very realistic, including imperfections. And if you have a clock, sometimes the needles are crooked, It seems to get very realistic, including imperfections. And if you have a clock, sometimes the needles are crooked, the hands are little crooked, so sometimes it'll do all of that. So it makes very, very realistic things out of that. But the one thing that was there, that can we have produced images from textual description? So there has been a body of work. People obviously first start out by thinking gans remember generative adversarial networks that we covered in the deep learning workshop i apologize to those of you who are who haven't been part of that workshop asif are you sharing something or just the github stream yeah i'm not sharing i'm just talking okay or just the GitHub screen? Yeah, I'm not sharing, I'm just talking. Okay. So anyway, I just apologize that some of the things I'm talking, it has background in the workshops that keep continuing here. So anyway, generative adversarial networks, if you remember, can generate fakes, deep fakes, and so on and so forth. So initial effort to have somehow a text, give it a text input and produce images started out like that, the work of Reed, and then there were variational autoencoders in the picture, so forth. And then this recent effort is remarkable. It's just absolutely remarkable. And we won't do the paper per se. What we will do is the basic idea of the paper is the same. We are just taking the GPT part of it, a simplified GPT model, just a 12 billion, well, it's strange to say just a 12 billion parameter model. That itself is a ginormous model, but not as big as the full GPT. When you take that and you play with it, I want to just walk through some of the results that you come up with. The first result they lead to. So what it is, is you give it text and you expect an output, a contextually relevant output from the transformer. And the fact that it can do things and it can do things that don't even exist in the real world. That is what you expect the illustration industry to do. And very odd things like for example, a daikon, I believe in India, we call it the mooli, right, a vegetable. Can you expect to illustrate a baby daikon radish in a tutu? This tutu, I believe, is this funny dress, walking a dog. And you would not be able to find such an image on the internet. It's not an illustration anybody has a thought of creating and yet from this sentence look at the remarkable thing it does it creates that and it gets very interesting from here and look at this here we can change the word a baby daikon to avocado. So now all of a sudden, avocado is wearing this in a tutu dress or in a leather jacket. Do you see how it is able to produce realistic images? Now, see, given a description, there are infinitely many images one can produce. Technically, you say, and the paper uses a sentence like, any textual description is an under specification for the image. In other words, it can produce lots and lots of images that are all in coherence with the textual specification. And you see that, I invite you to play with this, folks. Go go to the blog just play and think about how utterly remarkable it is now in their website they have given on the things like this in which you can play with a finite set of options okay now you may wonder that maybe has this transformer just learned the images so what was it fed It was fed pairs of text and image. Apparently they went to the, they didn't use any of the existing database. They scraped the web to find combinations of text and relevant images, contextually relevant images, and fed that into the transformer to train it, into the DALI to train it. But then what it is doing is truly generative. It is not like just remembering from memory, oh, I remember this image on the internet and I'm serving that. It is truly learning and generating from that. So at some level, it has a comprehension of a visual comprehension from the text it has achieved. And that is the most remarkable thing, what it has built in its mind and understanding, which is in many ways, it is multimodal. It is not just pure actual understanding, which would be thinking in terms of word tokens and grammatic and structure linguistic structure and so forth it seems to have transcended that it seems to have at some level understood the the inner structure of images too right but the semantic structure of the images too and it has created a unified understanding of the two which is what is most remarkable about this particular breakthrough so you can play with this you can say leather jacket with a mustache. Well, here is a bunny with a mustache. And you notice how fairly photorealistic it looks. It's an illustration here. Asif, I want to chime in with a comment here. Just this capability, if available at an industrial scale will wipe out the entire business of illustrators, the marketing illustrators, they're probably not even aware of these. But their entire business will get wiped out. Because now if you're a marketing person, and you have a script and narrative in your mind, you can literally play your narrative out in text and see an image come up and you can literally use that for your marketing. Exactly exactly and it has this profound consequences. See when these authors when they write the paper very interestingly they mention at the top they are aware that this will have profound societal impact. At this moment they are sort of withholding the real source code, full source code of the paper, claiming that once the full societal impact is understood and its potential for harm, because it can also be abused, right? So for example, we can say, a Premjit stealing the horse. And all of a sudden, we have a picture that you present in court saying see look here preemjit is stealing a horse right so uh because it can do things like that uh they're studying the impact potentially i'm just cooking it up i don't know if it can actually i think forensic science needs to keep up so that's where i think is going to go so it just puts a yet another twist to this whole deep fake stuff right but this is it and people are worrying they're already questioning see most of the time when we ask for illustration at least for commercial and advertising people are not expecting a picasso kind of art they just need illustrative art and to your point that illustration industry will have to reckon with Dali now. And the sheer creativity of Dali, just look at it. You can search for these images, reverse search these images on Google and you won't find them anywhere. Right? The sheer like... But artists have a style. So does this produce a sequence of images in a particular style? Very much so. So, Kyle, the style part actually is a very old salt problem. This is the whole. Anyway, so you were in the deep learning workshop. We go through the whole thing. You give an artist's style, any artist, Picasso or Monet or something, and you give it any photograph and you basically say, apply this style to this picture and lo and behold, it will make, quite literally, you will get a Picasso or whatever version, whichever artist style you want, perfect style copied into your photograph, and you will have a painting. In fact, that has become a big thing these days. Neural style transfer. Exactly, neural style transfer using GANs, Generative Adversarial Networks. So we did that exercise actually, and this was part of our project yeah yeah i just want to get it deployed somewhere where it won't break my bank so i'm waiting for support factors right so yes go ahead is there a certain level of specificity that like um dahlia will go to like i'm seeing you know like a like a monkey or like a lemur is like a certain level of uh specificity specificity in terms of species that it can like go to see here's the thing they have trained it in a lot of images and when they wrote this blog and they advertised this obviously they're bringing their best foot forward. They have mentioned that there are some limitations we'll talk to, but in principle, it is limited by how much training data you feed it with. If you really want to focus on species of monkeys, then so long as you have trained it with those species, it will learn to distinguish between them and use them. But what we don't have from them is an open prompt in which you can add anything we want. People have created a mini... There's a DALL-E mini project GitHub whose code is available. And I highly encourage you in your particular case, I think that's what I expect you to do. Get started with the DAL-E mini source code GitHub. The point with these things now has become that while they are breakthroughs, quite often the code is, once you understand the theory, it looks straightforward. The larger burden becomes training it with the right data. Can you find that level of data and train it? If you have access to it, surely it will do it, at least in principle. I see, thank you. So we'll go a little bit below. It says a professional high quality illustration of a giraffe. So you notice that a giraffe imitating a turtle, that would be the most absurd thing and I don't think anybody would have tried it. So now a giraffe trying to look like a panda. Right? Well, this one looks pretty interesting and all of these are giraffe looking like a kangaroo. And if you look at it, what you get a sense is if you ask a person to do it, they have to really understand giraffes and they have to really understand the structure of a kangaroo, right? Or the quintessence of a kangaroo. And it somehow seems to have blended these together in a fairly realistic illustration and these are still illustrations but then you say well these are illustrations these are easy what about let's go a bit more and we'll start it starts out with illustrations. So you can see how it is able to generate emojis, literally emojis of any kind, creative emojis. And you have a laughing dragon fruit. Or a scared dragon fruit and so on and so forth. So you can essentially now create emojis out of anything, right? And you call this kind of learning is called zero shot because you really didn't train it to do these things. You had just given it word pairs, contextual word and image pairs to learn from. But what it has learned to do is something beyond what it was trained to do. Which is the gist of the word. The word zero-shot is sort of an extension or somehow almost a joke on one-shot learning. So zero-shot is we don't even need to do that. Have they exposed a grammar for this, like in terms of what is supported what are the verbs and so on no it's pretty open actually the the the statement is you can use much of the common language if you use an esoteric word they have not encountered that's different but from what i understand because they have fed so much of the human language into it and certainly images it is fairly encyclopedic okay so you can talk in natural language and refer to reasonable amount of content in terms of like animals and i'm seeing there are also a red cube on top of a green cube it's able to understand positional on top of a green cube it's able to understand positional yes and the 3d aspect of it so and that's what i want to go through like if you really think how it does that you get the understanding or the inkling that it is truly able to get a comprehension of real world objects right so in a way real world objects can be described by a picture and by text but somehow it has derived an understanding of it which is the most remarkable thing it understands 3d stereoscopic views of things and i just want to go to go a little bit through this so you can say this exact same cat on the top as a sketch on the bottom so you notice that everywhere these these are images of cats and here well not exactly cat but dogs and what it does is it says make their sketches so isn't it remarkable that it can sketch it was never taught to do sketching remember it wasn't taught to make sketches of anything at all but from the textual language, it knows what sketching is. It has an understanding of sketching. And so it is able to translate a cat, this cat, into all sorts of sketches. Now, I hope you would agree that to me, they look pretty photorealistic. Some of them look odd. For example, this one looks rather odd. But this looks pretty realistic. This one looks pretty realistic to me. And you can do, I actually like this, I have a golden retriever, so I am inclined to go with this one, right? So do you see it is able to do these dark sketches? But instead of sketches, here's something that I found more interesting. You ask it to do a close-up somehow. Where is it? Photo, photo reflection. Look at this animal in extreme close-up view. And it is able to sort of get a sense of how this animal would look in a very close up if you had just zoomed your lens and seen it. Isn't that remarkable? I found that very remarkable, that it could do something like this. Look at this where my mouse is this picture. I don't know this having a golden retriever and basically this isn't a golden retriever, but just some other't a golden retriever, but it is some other dog, but still to me, it looks very, very realistic closeup. Isn't it? It even has a twinkle in its eye. I heard a dog kind of approving, dog bark. Who is that? I said a dog is barking in the background, so somebody's approving. Yeah, that's my golden retriever. Giving her approval. And so anyway, this is illustrated, we'll talk about it and why it is very very important i suppose these days that when you do machine learning in the deep learning one right the transformers applied to computer vision now is a huge thing so now see image completions we already did computer visions classification the remarkable breakthrough is that it can combine text and images tasks together in such a remarkable way, such a remarkably accurate way. This teapot was another interesting illustration. You have a teapot in the top with GPT written on it. So well, it goes, do you see how, so this is the point of saying goes do you see how so this is the point of saying do you see how photorealistic it is it even draws shadows and lighting sorry not shadows here but even yeah here in this one you see shadows it is so realistic the lighting and everything it seems to be able to render and if you just change it to this it still gets it pretty realistic any just change it to this, it still gets it pretty realistic. And you, so do play with this. And even if you give it a strange teapot, it seems to manage it. And you can sort of, it gets more interesting as you go down. So one of the biggest puzzles, Raven's progressive matrix, as you know, is an IQ test. Like, if you're in the army, they'll test you with this and so on and so forth. If you want to be promoted in the army at one time, it used to be necessary. And it is supposed to be a quintessential human test of intelligence. And while people are dazzled by the fact that it can do all of this, what I found very interesting, very interesting actually, is that it can do all of this. What I found very interesting, very interesting actually, is that it manages to fulfill this test, this Rabin test. It can tell you the completions of what belongs where. So set B is here. So I don't know, example image prompt, it will ask you what should be here. Right? So if you look at this picture, there is this, there is this, there is this, right? And with a circle, with a square, with a circle, with a square square with a plus. So it comes up with a square with a plus, right? It's trying its best to do the completions. And that is interesting that it can do, it can actually do well in IQ tests, which is quite remarkable. You give it a word, give me a photo of the food of China, we can say, let's try India for a change. And would you say that this sort of is the very realistic rendering of food of India? Anyone? Yes. rendering of food of India? Anyone? Yes. It's pretty, right? It's quite remarkable that it understands this. Food of India and it is able to do, you can say, museums in India, and all of a sudden it has the right images. So this one in a way you can say well it may have memorized because it could have done. The remarkable thing is that it has these things don't actually exist. It has synthesized from whatever images or understanding of museums of India it has in desert. But the point is that do you see that there are so many combinations possible wildlife of india deer is pretty common so it has done that and then you can say uh let's say uh what time is close to where i live so i'll try that oh amazing do you so what you may not know is bhutan is a mountainous country Bhutan is a mountainous country, lots of mountains and extreme greenery, very mountainous, very, very green and with terrace farming. And the remarkable thing is, do you notice that there is a hint of greenery and hills everywhere in this picture, except maybe this picture. And all of these are synthetic images. And truly remarkable is it sort of knows this is how probably it looks. Now, I don't know the animals of Bhutan so well, but I know that some of these are goats and these things are pretty common there. So anyway, we'll talk about it when we do the, at this moment, this is just having fun with it. A Lama Square, San Francisco from a street at night. So it seems to understand lighting and composition. Golden Gate Bridge with, and now, so now this is different. Remember that in the transformer, you give the inputs and sometimes you give two sentences and so forth. So here you give a text prompt and you give it an image. So you say that the produced results where the top looks like this. So do you notice that all the tops look like this and it has rendered the golden gate in all sorts of scenarios. To me, this one actually looks very realistic because I know this is the naval base from which I will take pictures. I've taken a picture similar to this. So obviously these are all very realistic images of the Golden Gate. But what happens if I choose something like this. And there's something actually I found this remarkable because my building was on the East Bay in Emeryville, a place I mean, most of you who are here in Silicon Valley know Emeryville, the tall Siebel building, the green building. I was lucky in the early days to have an office with a literally million dollar view I would look out of the office and the golden the sun would set on the golden gate very much like this and the interesting thing of course is that the sun moves because in different seasons so I would see it set on one side of the golden gate and then over the seasons change and set at the other side of the golden gate so this picture when i looked at it it was very reminiscent of what i used to what i used to see from my window office window for a long time very very remarkable so i would say that let's go play with it guys let's appreciate the beauty of this and then uh we we won't be able to see the the we'll look at this code a little bit but we'll try to do something with it in our well not in this particular course but in the deep learning course when we do and giving you a heads up eventually we are going to do the deep learning course right now but we now have deferred it to post january we will actually be writing our own little generators text to text prompt and image generators but we'll do it for very specific domains this is all very good the question is how do you make it useful for something real right how do you productize a functionality like this so i throw this as a challenge to you to come up with ideas, right? And think about it. One easy way is that suppose you have lots of Lego projects, Lego pieces or something. And can you say that, show me this, putting it together, not Lego as in kids play, but as in machinery parts. Like could you design something like this? Could you build something like this? So you can can think about that and there are many such ideas it also seems to know history and this was a remarkable fact a photo of the phone from and you give it a date and the most remarkable thing is it seems to have figured out what the phones will look like. The amazing thing is it can even tell you the phone of the future and the distant future. These are its guess of the phones of the distant future, right? Which probably are wrong, but I'll let you guess. But I found it interesting that it could do phones of the past quite well right and you can do with different lightings and so on and so forth right so anyway uh how does it do it there are two aspects to it see when you take a picture the picture would be the pictures they took is 256 by 256. i believe 256 by 256 which is too big a picture, as you know, from our deep learning workshop. We don't have the computational research. So what did they do? You remember variational autoencoders? So it uses variational autoencoders to come up with a much shorter representation of the image, 32 by 32. Once it gets a 32 by 32 image representation, that is an encoding. That effectively limits your vocabulary now to 32 by 32, which is about 8000 something, right? Or 8000, 9000 something. So effectively, that becomes your new codebook. Effectively that becomes your new code book. If the word code book doesn't mean something to you, think of it just like words are dictionaries. The whole vocabulary, a word belongs in a vocabulary. In some sense, when you do computer vision or anything you do or any picture that you see, you sort of try to relate it to in quotes a word in the vocabulary or sort of a entry in your codebook. Right. So that's the basic idea. Then the rest of it is somewhat straightforward. So the work of Reed, etc. use Gantt. So here, what they do is, once you have created a compressed representation, you have this, you create a single vector. Remember what goes into a transformer, a single token vector, right? And so they give 256 tokens for text and the rest, 1024 token for the image form. And so they send in a token, a text and an image together into the Transformers learning process. And that is all, that's it. I think that eventually should they open up the whole source code, open source code, we'll see a lot of optimizations and a lot of details and so on and so forth. But fundamentally what we'll see is, I don't think it would be something you would not have expected, but they have certainly thrown a lot of data at it to train it. Excuse me one second. So, there are quite a few, I would like to say, as we finish it, a few paper or things that do. One thing that I liked very much is Janik. It's a Janik culture. I've encouraged you folks to listen to his videos in the past also. He tends to write or create videos. He created this even before the details of the paper came up but now the paper is there at some point so but i think he did an incredibly good job in pretty much guessing the paper before it came out and in giving an illustration or he thinks how it was built and i think i agree with that i'll post these links uh on our and in many ways this explanation is far more detailed than what you find in the paper. The paper has details, but this is even more detailed. So I'll put this there. And then there is MiniDally. MiniDally is... Now, this is available. By the way, don't try to run it on the website. It will always error out. They say you can play the same game on using the mini DALI. But what you can do, you guys remember hugging faces? Hugging faces was a repository of transformers, transformers, exactly. All sorts of transformer based models are here, since this is also transformer. So inevitably, now, there's an implementation of Dali in a mini form in hugging faces to be used. It's pretty good. At least the good thing with this is you can actually go and read the source code. You can go and read into it and get a sense of how he has implemented it. And so it's pretty good, actually. It takes some time to read through it. So if you want to read through it so if you want to get a jump start you can do it and especially if you're doing the project with it you certainly do that right go through the source code so very well actually it was going to the source code just now uh fairly well written it's very neat and well written there's no comment there but when i look at the source code the all the variables and everything is very well described the naming condition is good so my feeling is that if you sit down for one day morning to evening you will get the whole idea assuming of course you understand your transformers and you understand your pie notch right so this is that and anyway so that is that uh there's a paper obviously i see one quick question, so that is that. There's a paper, obviously. Asif, one quick question there. So what is the purpose of the source code that they are making available? Because nobody can practically train this. So what does the source code essentially mean? See, they haven't revealed the source code. People have implemented DALI Mini on their own, based on the main theoretical ideas, but opening eye is still very hesitant. See, they were very hesitant in giving the GPT-3 source code, even here. And this claim is that this has a potential for massive social impact, that there should be, people should be given time to understand what the social impact would be for something that looks so breakthrough. And they have withheld it. But should they release it? The question still remains. In this world, when to train these ginormous transformer model techs, All you need is, like, you know, only the big companies can do it. The likes of Google, OpenAI, which is a consortium of Microsoft and Facebook and all of these people. They all pour their hardware into it. Then only can you do it because the amount of data that will go itself is ginormous to train this massive, massive transformers. So you and I can't do it. And therefore, the transfer learning part, right? Remember if they released this valley publicly, they could do that. But then the beautiful thing is that we can do transfer learning. Remember in deep learning, we did transfer learning. Means we took this and we applied it to ours, the tree pilgrim project, all the other projects. So the good thing is once a transformer understands something so general, usually applying it to specific commercial things is a very short run. It's just fine tuning the transformer. For example, in my workplace at this moment, my team, we do a lot of things trying to understand from the text, especially learning material, which subject, which topic are they about. So we don't use, what do we do? We take families of birds, we fine tune them to our problem. We can't train bird from scratch. It's not possible. It just, it will break up a ginormous bill in the cloud. So we are in a peculiar world. This AI is becoming more and more powerful. But the sheer hardware that is needed is the cost is exponentially high. We are talking now of tens of millions of dollars or hundreds of millions of dollars to train this AI. Somebody told me that not just money, but the sheer environmental impact, the electricity consumed to train one of these AI models is such that it will shock any environmentalists. It's an environmental disaster, actually, to train one of these transformer models. So obviously, we need fundamental breakthroughs. See, AI at this moment is a very young subject, especially this transformers and this ginormous deep neural network models. People are playing with things and being awe-inspired, awestruck by how amazing they are, what they create. But we don't have a full theoretical understanding of these things so that we can create a simpler one so in another paper we'll do soon remember we talked about the lottery system right that deep neural networks are fundamentally they have a internal lottery based on the random initializations when you train a neural network what happens is that the entire network doesn't get trained according to this hypothesis the lottery hypothesis essentially says that the entire network doesn't get trained according to this hypothesis the lottery hypothesis essentially says that a sub network preferentially based on the random initialization becomes the one that does most of the learning and it leads you to one specific minima okay one specific solution if you think in terms of minima like for, only yesterday we are talking about the minima of the last surface. Now for linear regression is beautiful. It's a convex last surface. There's only one global minima and you can get there. In deep learning, you don't have a convex situation. So you have hills and valleys. If you remember the last landscape paper, lost landscape work that we did in deep learning, what it showed us is that there are huge thousands and thousands of minimas, right? But then the remarkable thing is with all of this, that those, you don't have to ask for the absolute minima. Most minimas are close to the true minima and not only that there seems to be a path there's to be ravines you know riverbed ravines connecting some of these minimas a very beautiful landscape uh emerges in that in that and we did some lovely visualizations if you remember in fact we started the deep learning with that right so so that is I mean, at this moment, we are still beginning to understand it. So one question is that if only a subset of the network gets trained, then why are we spending electricity and so much of compute resources training the entire genomics network? Is there a way we know beforehand that this is the short network we have to train and so forth? So there's a lot of work. It's a very active area of research. In fact, so active that while I'm speaking, it almost gets a feeling that you say that something needs to be done and next day you hear that somebody has already been doing it and he has a paper on that. So that's the fun part, I suppose, of deep learning and of machine learning in particular. The field is rapidly, rapidly moving forward. So that is it for today, seven minutes. Any other questions, guys? Fun romp. Fun romp. Nice. So do play with it. It was a simple because we also have in the audience people coming from the comprehensive data science workshop background who don't have a full background in deep learning. I thought I'll start simply by simply taking a blog where you can go and play around with the state of the art rather than dig deep into the technical aspects of it for at this moment. Sorry guys. Yeah, bye.