 This is January 4th, 2021. The topic for today is explainable machine learning. In particular, because this is a deep learning workshop, we'll pay a lot of attention to interpretability and explainability of deep learning models. Now, when we build machine learning models, whether it's deep learning or not, people have often believed that there is a trade-off between explainability or interpretability and the performance of a model. So you often find a graph that sort of looks like this. And this is the performance of a model and this is its interpretability. Let's say human interpretability. How easily can you explain it to a person what those models are? So if you think about it, some models are very, very interpretable. For example, a decision tree, a linear classifier, for example, if you look at logistic classifier, it is reasonably performing, but highly interpretable. So this would be logistic. Let me just look at classifiers for a moment. Regression are very, very interpretable. Somewhere here is a decision tree, let's say. It has a slightly better performance and not just one decision boundary, but it has quite a few splits and whatnot. So I would say that it belongs somewhere here, a decision tree. And then once you move away from the simple models and you go to, for example, either kernel methods or to ensemble methods. So examples of the ensemble learning would be, would somebody like to give me an example? Random, please. be, would somebody like to give me an example? Random please. Random forest, yes, and gradient boosting methods, XGBoost and whatnot, CatBoost, etc, etc. All of these, they use an ensemble or a collection of models. If you take them in random forest as decision trees. So when you take a whole collection of these trees and you average their prediction for example in in bagging a random forest then it is very hard to explain what is happening at a given point and why that point was considered let's say if you're trying to distinguish between a cat and a dog why this thing this particular data point is attributed to a cat for example right let's say that you have the weight and size of a cat interpretability becomes hard so what happens is you see a general trend like this, you get the ensemble methods, random forest, boosting, xgboost, et cetera. There are many of these. And so all of these, you realize that what you lose is interpretability. Along the same line you have a similar situation you have for kernel methods methods sorry kernel methods sorry kernel methods, SVM, etc. What happens is they create actually very simple models but they make a very straight hyperplane as a decision boundary but they do it in a very high dimensional space. And that dimensional space is sometimes infinite dimension. For example, if you take the Gaussian kernel, you're projecting data into a Hilbertian space, Hilbert space, which inherently is infinite dimension. So to say that I have a linear decision boundary in an infinite dimensional space is not terribly useful to a common man on the street in being able to understand what exactly is the implication of each of these factors in let's say you take a real life situation if you're trying to determine whether or not in five years you'll develop diabetes based on your weight and your habits and so on and so forth and your hereditary. If you just come up with a prediction, the question that will come is, how did you come up with the prediction? What should I do? Will I get more effect or benefit by exercising? Will I get more benefit by cutting down on portion sizes? Would I get more benefit? I mean, what are the factors that will have the most benefit? Models like these, they have a difficult time giving you an explanation of why it's said whether or not you'll have diabetes, let's say, in five years. Inter interpretability is important. And they could support vector machines to say that in an infinite dimensional Hilbert space, there is a decision boundary. If you are on one side of it, then you will have, and the other side not. Means nothing to a common person. Likewise, for ensemble methods, let's say random forest, NXT boost, et cetera, there's always this thing that, how did you come up with this understanding? What is the real dynamics between the factors and the response? So for example, if you think of machine learning as the way that we have been thinking about it, X vector goes in, there is some function such that you get the y hat. So in other words, y hat is some function of the x vector up to an error term. If you think of machine learning like this, the question is, what is f? How easy is it to understand f, right? That is a crucial problem. When you are talking about logistic regression, let's say logistic regression, f is very easy to understand. fx is simply is equal to one one plus e to the uh this is minus I hope I got this one right yeah so this is your logistic equation right another way to another so this is the way you make a prediction on whether it's a cat or a dog as a probability this is an understandable equation it draws a very straight decision boundary let's say that this region is cat this region is dog right and this is your decision boundary while this equation looks complicated it actually the real thing is just a line, which is w.x. This line is the equation of the line, a straight linear equation for the decision boundary. And so it's highly interpretable. Linear regression is, of course, very directly just w.x. Linear regression is of course very directly just w.x, right? With a biased term, which I've included into the weight, into the x vector itself. So those are highly interpretable models, but once you do a random forest or any of these ensembles, now the question is, what is x? What is f rather, right? And did you even explicitly analytically compute it is there an analytical expression for it and the answer there becomes that no it is we didn't even try to we just went and subdivided the regions in a decision tree based on whether there are more cats in this region or there are more dogs in this region and so forth. So you remember the algorithms for decision tree and what we were up to. So it becomes hard to answer this question, what is F? And that is the heart of the explanation, right? So as you move to deep learning, the situation becomes even worse because what happens in deep learning is, so suppose you have a deep learning model, let's say, deep learning. Whatever input goes in here, it goes through, let's say, arbitrary many nodes. Each of the nodes applies its weight. Now, this part is Z1. Let me just call it Z1, Z2, Z3. They will produce Z4, Zn. And they will all do their weight 1.x, weight 2.x, and so forth. They will all do weight n.x x the weight associated with this vector here but that is not enough because this thing each node also does the second part that it does is it applies an activation function to it right a non-linear this goes into a non-linear activation function do you remember guys a sigmoid function? So sigma is the activation function. Yeah, let me write it here. Sigma is the activation function and it could be your ReLU, the simplest. It could be your sigmoid. It could be your logit, logistic. It could be your sigmoid, it could be your logit, logistic, it could be your tanh, and so on and so forth. So each of these are nonlinear functions. They create a nonlinear distortion. And if you remember, early on in this course, we talked about it, but the whole point is you need these distortions to be able to create a universal approximator. A neural network has a universal approximator that can approximate any arbitrary function f. You do need distortions. So you have this distortion. Now this distortion goes into the second layer and it gets further distorted. And you can imagine that as you go and keep mashing it up, mashing the input, as it goes from layer to layer and goes to more and more nonlinear distortion, you can quite a bit see that it's very hard to keep track of what's happening to X. Are we falling, guys? All we remember at the end is that there are certain scores that come out and if it is a classification problem you do a soft max of it and you pick out that it happens to be a cat. The most activated, the bug that lit up the most was the cat rather than the dog. Okay? So in other words, deep learning leads to highly nonlinear models. You can't explain it. So the problem with this is you can't say that because in deep learning, let us say that it's not always true, but quite often if you're dealing with unstructured data like audio and video, it turns out that quite often the deep learning model would be like this. or accuracy and come and opaque opaque these are opaque models uh sorry i apologize for this thing opaque these are opaque models in In other words, low interpretability. Does this make sense, guys? Very low interpretability. But then that has a potential for problem because of many, many things like, for example, yesterday we talked about it fundamental questions of bias come into this if if the data has bias your model will learn biases and you won't know that it has biases so one story that sort of circulates in the ai community quite a bit and i've never really found out whether it's true or not, but it certainly has a message to it. The story goes as follows, that at some point the military approached Silicon Valley outfit, AI outfit, and said, can you distinguish between civilian vehicles and military vehicles? If I were to pose this problem to you, you would say well that seems like a simple problem of image classification. Sure enough, we can do it if you if we get enough data, so the military will oblige you brought boxes and boxes of pictures. Now, how did it get pictures it asked us soldiers to go take lots of snaps of civilian vehicles and military vehicles, The whole point is that if you target a vehicle, you don't want to target a civilian vehicle. Obviously, if you suspect enemy combatants, you probably want to target a military vehicle or some such justification. So the company trained the neural net to that problem and as you can imagine today neural nets are extraordinarily smart they will train almost easily very easily to be able to tell those things apart with very high accuracy so in the lab the accuracy was very high when the model was actually deployed in the field though, for mysterious reasons, it was a complete disaster. Now people began to ask, why is that? Was there overfitting? But then it couldn't be because the military tested it out with new pictures that the startup didn't have access to or the company didn't have access to. It seemed to work just fine in the lab even with new even on the test data set and yet in the field it failed and as the story goes the reason for that was an inherent bias in the data which of course the deep learning model couldn't explain because you know it's a black box. And the inherent bias was that soldiers see military vehicles typically in their base camp in the mornings and evenings before they go out for their tour of duty and after they come back from their tour of DT. So if you ask them to take lots of pictures of military vehicles, they'll just take it early in the morning before going on their tour or before going out or late in the day when they get back all the vehicles. And when do you find civilian vehicles? You'll find it in broad daylight, right? So you're on a beach, you're touring a certain route and all around you are civilian vehicles. So you keep lots of pictures and it is broad daylight and so what the neural network presumably picked up on was ambient light conditions the daylight pictures all of them it was marking as civilian and the pictures that were in the sort of low light conditions, it was marking as military, but you wouldn't know that about the bias. So that sort of that story speaks to how, or sort of is a cautionary tale about the dangers of using black box markets. Now, well, that's a story. We do know that there are some very real situations that have happened. We know that, for example, people tried to create algorithmic hiring. Many companies did that. One of them, Amazon, actually got sued because it was discriminating, I believe, against women and against races or blacks or something like that. So U.S US protected classes. And when people looked into it, why it was doing it, the model was very accurate, apparently. It did as well as what the human interviewers were doing. But when you look back at the data, it turns out that the human interviewers were biased, presumably, and that bias was perpetuated in the algorithm, except that in the human case, I suppose nobody questioned it. In the case of AI, people were suspicious and they questioned it. And because they questioned it, it turned out that the flaw was in the data itself. And so the bias of the data gets perpetuated in the models you build. That's a problem. The other problem is just the sheer interpretability. For example, if a model is going to, let's take the example of there was a model that claimed it could predict how often you would be, how likely you were as a criminal to recommit a crime or something like that, or whether or not you should be let out on bail. The idea being that they will let you out on bail if you're a good citizen now, a reformed citizen. On the other hand, if there's a high probability that you'll commit a crime again, then people are hesitant to let you out on bail. From what I understand, one company created a software which people started using. Now Now there was a lot of confidence in that black box, except that till somebody pointed out that this model is not even accurate. It's a black box and it's not even accurate. The predictions that it makes are not better than you pick random people off the street and ask them to make a prediction about a person, whether he's likely to commit the crime or not. And it has about the same accuracy. So that also speaks of the danger that we might get caught up in the hype and start believing that machine learning or AI models are somehow magical, and they can solve the problems when they really can't. So the question therefore comes you really need to look into the box and see what really is going on and such stories go on and on. There was a facial recognition system created I believe by I think yeah this Amazon itself again so they wanted to give it to law enforcement and it would tell what a person is or whatever. And people were using it. And I suppose the company was promoting it quite a bit. Till some researchers thankfully noticed and did a simple experiment. They fretted the faces of all the US congressmen and senators, the Black Caucus. And it turned out that for all these senators and congressmen who were Black, quite often it would identify them not as the sort of highly law abiding citizens that they were, but instead would mark them as rapists and serial killers and whatnot, murderers and all sorts of nasty things it would identify them as. And that set the alarms going because it seemed to do extremely poorly with Afro American faces. When you look at these systems, the explanations are very easy. You can quite imagine people like us, engineers sitting in our offices, and we need some data to train the AI. What do you do? We start taking pictures of each person in the office. And when you do that, the demographics that you are able to work well with is the demographic of people on these campuses. It is an unfortunate thing that on those campuses, there is a scarcity of African-American people and other minorities. And because of that, well, your model is simply not trained to deal with it. And it seems to do a terrible job of it. So maybe an explanation. I don't know. So these things do go on in the field. Again, it speaks to the fact that you can't trust an AI model. You have to ask it to explain itself. But then the question arises, how do you make a model explain itself? Like when you have a black box model, how do you get it to explain itself right that is a fundamental question you have a you have a deep neural network many layers deep nowadays people are going like hundreds of layers deep right at that level what does it mean to have interpretation in the model or asking the model to explain itself. If you look from a technical perspective you may say well the cause is lost and that is what a lot of people believed and even today believe they seem to see a sort of a rough inverse relationship something like this they seem to see a relationship that goes like this that says that the more the power of a model is in predicting, the worse it will be, the more likely it is to be a black box. And a lot of people said that, you know, it's a fact of life, just accept it. When you're driving a car, for example, and it's a self-drivendriven car perhaps you don't care so much for interpretability there though one can argue there too it matters but certainly there are so there are a few situations where it may not matter the only thing that matters is it drives correctly but in most situations you do want an explanation of why the model is making the predictions it is making. And so a lot of inquiry has gone into this field and this has become now a very active area of research. It is being investigated along many completely independent directions or orthogonal axes. And it is hard to keep track, the amount of research here is so much that it is rather hard to keep track of it. So I will be posting for you a couple of review papers from Archive, where some researchers have created what they called a status or a field guide through all the landscape of research that is being done on model interpretability or explanations explainable AI now it's a broad area and most people are probably not aware that this area of research has already been bearing fruits to the extent that today, just as when you build a model, you validate the model, you do its predictions, you make plots, in the same way, you should actually, one more step you should add, you should make the model explain itself. And that is the topic for today. How does a model go about explaining its details, even if it is a black box model? So we'll learn a few techniques. I will start with two techniques, which are actually three different directions. two of them are agnostic. In other words, they don't care whether it's a deep neural network or whether it is random forest or whatever it is, but they will give you an answer no matter what. Then we'll do another set of approaches, three, four approaches, which are very, very particular to deep neural networks. So before I start, would anybody like to ask a question? You guys are all very quiet. So I'll ask a quick question. What is the state of art? Is it agnostic models that are rolling or particular models that are explainable models that are tuned to those particular architectures that are? What is the direction of the research, the current state of the art? At this moment, research is rapidly moving forward in all directions. Some researchers are very focused on creating model agnostic explanation algorithms and many researchers are looking at it algorithm by algorithm. So for example, there is a huge body of research that only focuses on deep neural networks. but there is a huge body of research that only focuses on deep neural networks. And it is a little hard to say, like I can tell you what I do from personal experience. Suppose I'm doing tabular data, right? For me, it is a given that I would do three things. I would do the Shapely, I would do LINE, and I would do something called PDP, Partial Dependency Plots. It is almost second nature, it's part of my routine to do things like that because to me that is part of just like you have uh you know you do i tell you that when you get data if you remember in this workshops you you need to become friends with the data right even before you build a model understand the data what is it right in the same way, I feel, I've always felt, that once you build a model, it is all right that your model is making good predictions. But now you need to make friends with the model, because that model is going to be deployed to production. It will affect the real world in tangible ways. So it is important that something that you are going to unleash on the world, you understand it. And so having these things ready and explained, give you intuition into it is important. One more reason that is important is you can catch mistakes. If your model is latching onto wrong things and giving importance to the wrong features or wrong combinations, you will catch it. You will know that something is wrong here in the model. And so it is important to do that. Also, just seeing how you OK, so we'll talk about this, the line, shapely and uh so forth and the pdps today when it is deep learning if i'm doing deep if i'm doing image processing and so forth where pretty much deep learning and rains what do i do there a lot of people believe actually that you can't use shape your line but actually you can. I do use it there also. But and PDP is also partial dependency. But what I do, and it's very interesting actually, they are able to show in a picture what to pay attention to. But more than that, we use deep learning techniques as its own special things. And those techniques I will talk about today. There's a pretty powerful techniques, one by one I'll introduce them to you. And- And Asif, there is one more thing that I, again, I can come back to it, I couldn't understand is, additive nature of these explainable models. It depends on the additive nature of the features and some claim that may not be the case and that's why Shapely is dependent on additive nature to explain its feature importance. So that what they mean by additive nature and I don't know if I'm explaining it rightly but something I couldn't get past. I would really appreciate that. That is a good question. So there has always been this question that, see, whenever you make generalized additive models, right, that can, how valid it is. It turns out that actually you can make additive models with considerable validity. It goes to the basic fact of mathematics that you can do simple Taylor's expansion of any function. So there is truth truth to it the only thing is you can't make a globally additive model those are those additivities are local right yeah and it the the reason I mean I have heard this debates and I almost feel like explaining to them basic mathematics or calculus. So within a local region, you do have additivity because in a local, we'll talk about it actually. Why don't you hold that question in your mind? I really have this very deep, I mean, this is, I don't understand it deeply, but I would really, and of course I'm using it in my work. I need to explain it in terms I would really appreciate your help course I'm using it in my work, I need to explain it in terms, I would really appreciate your help. Yes, I'll do that. And in fact, the way I'll explain it is non-standard. See, most of the literature in this is written by, I don't know, programmers and people who write it in, or mathematicians who have couched it in very abstract language. I'm going to talk in terms of geometry, just as much of machine learning we learned as geometry. So I'll give you a very simple geometric intuition and I believe it will remove all such questions. It will make it easy to answer all such questions that you have. Are we together? All right. So, so with those words, I would like to get started. Give me a moment. Where is my water? Guys, give me a moment guys. I need to go get myself a glass of water. Yes, so all right. That is one question. Is there anything else that you guys want me to focus on and talk about? I do know that some of you have already taken my workshop at the boot camp last year so i took you through some of these exercises that we will do on wednesday some of these not all of these because we did not cover deep learning now uh how many of you have here been through shapely and line and partial dependency plots uh let me see play and line and partial dependency plots uh let me see anil praveen you too and nisarg you also isn't it and pradeep yes sir and kate yeah also quite a few of you and so here you too were you there in the bootcamp? Yeah, 100% yes. Yes, yes, you were there. It's a pretty sizable part of the audience, at least one third of you have been through. So did I go into the theory of Shapley and Lyme in those time? Or did we just do the practical? You went through the theory. I did go through the theory. It's good. So for you guys, it may be a review. If you want, you can take a walk and come back in an hour. So with that in place, let me... I have a whole set of topics that I wanted to discuss. Okay. Let me start with a very simple geometric explanation of things. And pay attention to the geometry that is it. Geometric perspective. See, remember I said that in a way when you write y is some function of x, right? What are we saying? If it is regression, let's take the case of regression. Let me generalize these ideas step by step so you see it. For if in one day, in one dimension, y, what you're seeing is y hat is some curve. If this is x, this is y. You just don't know what curve it is. Are we together guys? A function is nothing but a curve. Are we together? Do give me some feedback so I know. Yeah. This is very simple, right? Right now furthermore let us just impose because if you make smooth smooth and continuous curves of this thing then this is regression. Right in 1D what is classification? It could be again this this curve except that this is a decision boundary. this is the regression curve and the same thing for classification this happens to be the decision boundary right but whether it's a whether it is and so these are yellow points let's say on this side and let's say that these are some other class right the the pink plus or red class whatever it is red triangles class let's say are sitting here in this part of the feature space. And the way you would look at classification is that you have two features, x1, x2, here. And this is the decision boundary that you are searching for. Now, let us generalize it to higher dimensions so in higher dimensions in higher dimensions what happens what happens is that this becomes let's say that you have this if you just give the intuition in two dimensions for regression give the intuition in two dimensions for regression what will happen this will become a surface right some form of a surface are we together your uh regression surface it's a regression surface, right? So for any point, x1, x2, let's say here, this is x1. Let's say this is x2. What you have is, you have some value here, which is your y hat. This is y hat x1, x2, for x1, x2 in the base manifold. So this surfaces mathematicians often call manifold. And remember, I've been using this word manifold. Now manifold has a more formal definition, but simply put, if you remember what I said is a manifold is a bed sheet. A bed sheet. A bed sheet, right, essentially. And a bed sheet can be crumpled and whatnot, and it can take all sorts of peculiar shape. Sort of like that. But it has the following properties. It is, obviously, let's have a bed sheet without tears. No tears. obviously let's have a bed sheet without tears, no tears, or discontinuities. It is smooth, right? So it is not an iron bed sheet with creases or something, edges it is smooth everywhere the word people use is differentiable it's differentiable so what we are looking and and it has one property which is very crucial to manifolds which is that locally in the eyes of an ant in the eyes of an ant, ant or god forbid bed bug, bed bug, crawling somewhere on the sheet. What happens? The world looks how it's very tiny the bug is very very tiny or a bed bug is so tiny you can barely see it right a dust mite or whatever it is now for that that bed sheet it will look flat locally it looks flat isn't it so imagine any even just imagine a sphere if you take only a local patch you realize that if you make this patch small very small patch it will look flat. Would you agree? If on a sphere, you take a very, very small patch of the sphere, then to a little bird, to a little ant, it will look flat. Yes. And in fact, this is history. We are the ants living on the big sphere of Earth, a tiny little creatures. And for the longest time, what did we believe? That the world is flat, the Earth is flat. Isn't it? We wouldn't believe that the Earth could be a sphere or spherical in shape. And that again speaks to the kind of surfaces we are looking at. The surface of the earth is a perfectly good manifold in some sense. Concept of a manifold. So locally it looks flat. Now, so what does it mean if it locally looks flat? You can put i.e. is Euclidean. Euclidean means you can just put Euclidean axis, x1, x2, locally. You can make, let's say that it's a two-dimensional sphere. The surface of the Earth is an example. But you can put a little, this thing, coordinates there, and measure things with respect to any arbitrary origin. You can say, for example, this is a very real fact. I don't know if it is true in the u.s or not but in india there is a concept of zero mile have you heard of that concept every city has a zero mile it is the reference point or the origin it's i believe it's a very british phase perhaps origin and the coordinates of any anything in the city is with respect to the zero mile or with that particular origin. And you presume an Euclidean space. And that is what is meant. And now what has that to do with what we are coming to? So what we are seeing is however complicated a function may be right the relationship between input and output may be ultimately it is some hyper surface in the feature space are we together either it is the actual prediction surface or it is the decision boundary based on whether it's regression or classification but so do we agree to that point, guys? Simple point of making. So now, if you further impose this notion that it is smooth and differentiable and smooth and continuous and so forth. And by the way, algorithms like deep neural networks, they always produce such functions. That's what you will get by enlarge. So now comes the question as universal approximators that sort of do things like that now comes the interesting question if you look at a so if if you are at a point let's say there's a very complicated relationship and this now goes to the point of yours uh who is asking you biology was it your shiva about the additivity so let us take any point so suppose i take this point in one dimension do you see that the local neighborhood of this point is a straight line isn't it would it be fair to say that the local neighborhood is linear isn't it now generalize it to higher dimensions so if it is a surface in in high dimensions So if it is a surface in high dimensions, in Rn feature space or Rd feature space, d dimensional feature space, the local, again, the same thing happens, the local neighborhood of a point, let me take the point X naught, some point, right, x naught, and you create a local neighborhood of it, walk around it, local neighborhood will be a sort of like a tangent plane, right? It's a plane, linear plane, linear linear surface a plane is by definition a linear surface would you agree i'm just generalizing to this so for so far so good now if if that is true let me ask this question. Suppose it is a problem of regression. What is y of x in relationship to y of, if you know this and you take some arbitrary point x in the neighborhood of x naught, what will it be? If you do remember your Taylor's expansion, this would be in one dimension. It would be dy dx at x naught, isn't it? Times delta x. Delta x being x minus x naught. Isn't it? So this is the Taylor plus higher order terms. Higher order terms. This, by the way, is called the Taylor series. Taylor series expansion. It's something that you must have learned and have perhaps forgotten. Now I can write this equation in a form that you'll remember much more, minus y x naught. If you look at this equation divided by x minus x naught is equal to dy dx at x naught. This is practically the definition of the derivative, in the limit of x tending to, so in the limit x tends to x naught, you know that in the limit of extending to, tending to, so in the limit, X tends to X naught, you know that in calculus, this is practically the definition of a derivative, isn't it? And this is your Taylor's expansion. So now this is in one dimension too, isn't it? I hope I'm saying something very obvious. You have done this mathematics some point. Otherwise, take it as a fact that in the neighborhood you are. So y of x is just the value here, plus a little bit. You know, you take the displacement and multiply it by the gradient by the slope but it's a linear uh slope with a sample it's a linear slope and now comes a question that hopefully will solve some of the riddles so now let's generalize it to higher dimensions but what it means is y of x is equal to y of x naught as a vector plus gradient of y times delta x right this is a generalization and let me expand it y x naught plus a gradient of y this is dy dx1 times x minus x1 minus x naught 1 right plus dy suppose there are two axes x2 minus x2 0 2 isn't it guys this is what it is because this is nothing but x sorry why did i yeah x minus x1 minus x not one i plus if you're looking at two dimensions if you uh x2 minus x not two uh j in terms of basic vector you would write it like this isn't it and so you're just taking the dot product of the derivatives this is the basic expansion plus higher order terms plus higher so far so good guys sticking it so what basically it says is that if you want to know how much y has changed, look at the change along each of the axis and add it up. Are we together? Change along each of the features and add it up. The slope along each of the features and add up the contribution from each of the features where x1 x2 are your features in your model tell me guys if i need to repeat it or is this clear this is i'm just writing out basic things here math here i can repeat it another way let me know if I should, otherwise I'll move ahead. So why is the decision bounty and x are the features that goes on, I mean can you explain this, I'm a little trying to wrap my head around this. What is the question? So the x is the, what is x here? It's a feature. It's a point in the feature space. Okay. So you're trying to define the curve at that particular point and it turns out to be this way. Okay. So I'm saying that in the neighborhood, if you know the value here, in the neighborhood, the value would be the value at that point, plus these terms. terms now do you notice that what am i doing these slopes are additive right these are the slopes derivative with respect to each of the features and this answers your question about uh additivity so what does it mean in the neighborhood of a point locally models are always additive. It's linear in the hyper plane in that particular scenario. Yes. So to the extent that all functions are some manifolds, some manifolds, those manifolds are smooth, I mean, assuming that they are smooth and continuous. At any point, you can do a Euclidean expansion. You can, I mean, you can do a Taylor expansion, right? And because you can do a Taylor expansion, what will happen? Each of the, the gradient of each of the features will contribute in an additive manner. You see that? There are higher order terms. So now the problem is if you go too far from it, let's say that you have a curve like this, you know, like this, you can't go from here to here. This is too bad. It is no more aligned. So the point is the caution that you have to take is stay local. But within a local, you are essentially guaranteed a linear model isn't it by by construction by the very fact of elementary calculus you're guaranteed a linear additive model with respect to the features so that that hopefully answers your question biology why that that hopefully answers your question biology why does it i see so the n-dimensional creature space i mean each dimension we can just assume it to be a linear so anything happens within that small region it's a linear uh yes it's a plane this is a plane or in this particular case you notice that it is quite literally a straight line. Isn't it? Right. Right. And so in higher dimension generalize it, it will be a plane. And a plane of course, a model that represents a plane is by definition an additive model. And so that when you look at the contribution and so I'm jumping ahead a little bit for your sake, but when you move ahead, a cat isn't fair to have an additive model? The answer to that is yes, so long as you stay local. Right? Okay. And what is the, I mean, sorry, I'm trying to understand it from different perspective. When you say additive model, you are considering all the features independently and then working with it independently. That's what it means. That's all it means. Okay. And if that's not the case, if it's not additive, what's the problem and when would it be a problem? and when would it be a problem yeah so so for example if you if you say that give me global explainability right give me a global explanation then your local arguments go away right see what we are explaining here see what are these slopes so now let's go back to something that is at the heart of today see what is a derivative? We think of derivative geometrically as slope, as the steepness, as the gradient, isn't it? And you must have remembered that deliberately I planted ideas in your head along the way by saying that derivative is a measure of sensitivity. So let me go back to that idea and I'll explain what I mean by that. See, when you say that you have a function y is equal to fx and you make a small change in x. So x goes to x plus delta x. What happens to y? then y would be function of x plus delta x and that is equal to fx plus df dx. I mean, this is complete derivative. I'm used to writing it in partial form, but let me be exact here. This is in one dimension, it is this, delta x. In other words, the delta y, so this is your y of x. So this is y of x plus delta x. And this is nothing but y of x. So you can say that the change of y, delta y, which is y x plus delta x minus y of x, right, by definition. This is equal to df dx delta x. Now, this is very basic calculus. We'll respond over it and see what it means, actually. So, for a small change of the input, how much the output changes depends on this slope, on the derivative, isn't it? slope on the derivative isn't it if the derivative is small the response won't change much right so for unit change in x the response completely depends on the derivative in fact it is equal to the derivative isn't it the change in response is equal to the derivative are we together right That is why I've often used the word, the derivative is sensitivity and this word is at the heart of the way we'll explain things today. That it gives you how much the response or the output is sensitive to the variable x. If the slope is zero, it is completely insensitive to changes in x. So if you have n different slopes, there will be n different slopes? Yes. Each of them? This is slope 1, this is slope 2. And so what happens? So let me ask you this question. Suppose this slope, if dy dx1 is 100 times dy dx2 if suppose this were true approximately what does it mean what does it really mean for unit change along the x1 direction and unit change along x2 direction. What is y responding to more? Yeah, the 100x more on that. 100 more, 100 times more of x1, isn't it? So quite literally you're saying that maybe feature two is not that important locally. What matters is feature one right are you seeing the reasoning here by just comparing the derivatives locally in some sense you are saying that feature i mean assuming that this i mean there are certain basic assumptions like assume that all the features have been standardized normalized and so forth so they are on the same scale right but once they have been scaled and normalized and all of that then the derivative gives you the sensitivity of the output with respect to that so what does that mean that feature is much more important x1 is 100 times more important than x2 locally right so for example uh if you would suppose it was the difference between eating and exercising but maybe x1 is the amount of food you eat and x2 is the amount of exercise you do and you're asking this question which will more affect the weight of a person so today we know that eating has almost uh 10 times the impact or eight times the impact uh than exercise you need both to get your metabolism high with calorie intake so for example if you eat a big meal a thousand calories you really have to run on the treadmill for a very very very very long time a reasonable uh workout on the treadmill of half an hour is not going to burn a thousand calories. So you would therefore your slope, your weight change, weight will be far more sensitive to calorie intake than to calorie output that you can do through exercises. So that is what we are saying. Is the reasoning correct? Yeah, I'm slowly getting to understand with the examples you're giving. Thanks for that. Yeah, good. Guys, the rest of you are silent. Any questions or any feedback? The overall picture that you're describing, isn't that the basic intuition behind how gradient descent works gradient descent yes so it wouldn't basically is well it sort of is there the same calculus is behind that in gradient descent you don't look at the response versus input you look at the gradient of the loss function with respect to the weights in other words you are saying how much is the weight change i mean how much is the loss sensitive to changes in the weight right similar argument but it is it is weight versus loss ingredient descent here we are saying output versus input, etc. That's the thing. Similar argument, but we're just changing the setting here. So this is it. So now that brings us to this question. I hope so. So guys, now I'll explain a model. It turns out that I've already explained to you an interesting algorithm, which is called LIME. It is appropriate that I practically drew it in the color of lime or almost. Let me draw it in the color of lime just for fun. What's the color of lime? Let's see. which would it be? This looks slime enough? Okay, let's see. Now, I screwed up. Let me have a bit of fun here. Yes. No, it didn't come through. And what? Done. Okay. I'm somehow not able to get line, so I'll just stick with something. Line. What does line? LIME stands, so what happens is, it belongs to a class of models called surrogate models. Now we are familiar with surrogate. I very recently introduced surrogate models. Where did we talk about surrogate models? Do you guys remember? Bayesian. I'm sorry? Bayesian. Bayesian reasoning, yes. And Bayesian inference. And what was the context? Auto-Lemma. That's right. Hyperparameter optimization. Hyperparameters. I mean, the loss is some function of the hyperparameters. We don't know. I mean, the loss is some function of the hyperparameters. We don't know. And that function is unknown. So what do we do? We instead create another model. We use a function that we like, that we can take, a very well tractable function, you know, that we love. And we try to model the relationship using a function that we can understand. So that is what you do. So the basic idea is this. Suppose you have a black box. You treat this in line. The basic assumption is let the algorithm be a black box and you don't really care. a black box and you don't really care black box what happens is x goes in fx comes out isn't it guys now fx is rather hard to hard or intractable in some sense now what you do is you instead create a model which given x produces g of x where g of x is very similar to f right when you do that you have created a surrogate model When you do that, you have created a surrogate model. You have come up with a simpler explanation of F. Now, another way of looking at it is suppose F is like this. Well, obviously, maybe this is not quite. And suppose you come up with a model that is like this. that is like this. Would you consider that this is at least a reasonable explanation? Right? And at least a global explanation. It's a sur- then the yellow line would be a surrogate of sorts. Now if you notice with the surrogate, you're losing some resolution. You're losing some dynamics you're losing some some dynamics that's happening in the underlying, isn't it guys? So now the other thing you could do is you don't need to make global surrogates, you can make local surrogates. So the idea is suppose you make a local, what you do is we did we did say that locally uh every function or is a is a manifold which can locally be approximated as a linear function right and so you can say that you will make a locally local interpretive suppose you're making it around some feature point x naught in the feature space. And you make a model that's fairly accurate, but it is local and it is model agnostic explanation. A fancy way for writing the g of x in line. So L-I-M-E makes it line. So first of all, the important word, it is local. And if you make a model that's sort of interpretable, what can be interpretable models? The classic is, of course, a linear. suppose it's a regression problem, you can say linear regression or for classification suppose you say logistic regression for classification. These are linear models, right? You can do that. Sometimes people find it easier to understand in terms of a decision tree. So you can use a decision tree, but don't make the decision tree very deep, shallow. Otherwise, you lose interpretability, isn't it? You need to make that if the value is more like, for example, if you have to lose only a couple of pounds, just go take a good jog or something like that. If you have to lose a lot of weight, then perhaps you need to diet and exercise both. And so I'm just cooking up examples. So you can do it as a decision tree perhaps, but make sure that the decision tree remains very simple and interpretable. right? And so locally you want to make an interpretable model which is agnostic to, so this is interpretable, model agnostic. What does it mean model agnostic? So your G is an explanation but it is model agnostic, means it doesn't care whether this black box is what it is, whether it's a random forest or a support vector machine or a deep neural network or what it is, it absolutely doesn't matter. So the heart of this goes to the fact that you have the confidence coming from mathematics that locally everything has a simple explanation locally or a simpler explanation locally are we together guys yes yes so what you do is the way lime is done for example is there a little bit of a bit of technical detail uh that I think is worth pointing out. So suppose you take a two-dimensional feature space. And let us say that you have a reference point at this. So what it will do is it will do some perturbations of this data. What it will do is it will take a bell curve centered around this, and it will generate a lot of data points. So it will generate a lot of data points so it will generate data points like this right and more points closer by hopefully right so far we are clear it will generate a lot of perturbations of the data point for which you're looking for explanations. Do we understand the word perturbation? It means vary each of the you know x1, x2 a little bit. Go to other neighboring points in the feature space and pick those as new data points. So you generate new data, new data in the neighborhood, in the neighborhood. And once you have done that, then what do you do? Then you can, in the simplest form, you can build a linear model. So suppose the decision, let me just say that this was a regression curve. Let me make it go through this. Something like this. So now what you want to do is you have taken a lot of points and they are far off. You want to do, you want to apply typically a weighted, a weight kernel. And a weight kernel is something you would remember we did that in a kernel KNN do you remember guys ml200 the kernel KNN the k nearest neighbor with kernels distance kernels weighted kernel or distance kernels we call neighbor with kernels, distance kernels, weighted kernel or distance kernels. We call them distance kernels. What does it mean? That points which are closer to this have more influence and points that are a little bit far off, they have less influence and points that are very far off, they have almost no influence. In other words, each point, you multiply it with some function that decays, a distance decaying function. Function is called a weight kernel or a distance kernel. And so this is a minor technicality but basically what it means is look only at the local neighborhood and here you will have a plane so now you can make a linear model linear regression you can make a linear regression model and generally people add a regularization term also linear regression model and generally people add a regularization term also remember lambda as a regularization term right you add a regularization term remember lasso ridge regression and so forth when it's always a good idea to regularize when you're building a linear model, isn't it? You remember that. So internally, that's all it does, right? So if you're using that, or it will build a decision tree and so forth. So this is LIME. Do we understand LIME now, guys? So Asif, what you explained before and then this LIME in particular, are in the, I mean, line is the particular implementation of that idea which you explained before. Yes, it's literally that it is. Lime is one implementation of this idea, the idea, the mathematical idea. There are other implementations, but lime is by far the most popular. Got it, so if I understand correctly correctly you get the perturbations to still remain within the hyper plane yes and then you know what kind of hyper plane it is and then that's it your job is done basically that is right that's all it is okay right it's quite simple so in other words in the local neighborhood you know that it will be a hyperplane. So you can make very simple models, right? Or the gist of all of this explainability using this approach is locally everything looks simple. And if you need evidence, just think for thousands of years, we all felt that the earth is flat because locally it looked flat. Isn't it? locally it looked flat isn't it i'm missing some part of the intuition here so when we look at the local area and try and understand the behavior there yeah what is the interpretation of the overall model no no you're not interpreting the overall model at all you're interpreting near a point so so the whole question is see think of it like this i keep going back to this diabetes thing i'm sure that the patrick and sukpal both of whom are not here today oh so paul you're here i'm sure you'll hate me for it because medically it's nonsense isn't it my my example but But go along with it. So for a moment. So the idea is that based on what your weight is and what your age is, the recommendations would be different. Right? Suppose you're in your 30s. And nowadays, a lot of people in their 30s, they are developing diabetic symptoms. It's a lifestyle problem. The recommendations that you would give is very different from diabetes in a person who's five-year-old. Five-year-old probably means it's not because of diet and exercise. It's probably type 1 diabetes and there's a fundamental problem with the pancreas. The factors affecting, causing the diabetes are entirely different than when you see a person in the 30s. So explanations differ. This is the important point. So what factors matter differs in different parts of the feature space. Let me give you an example. When you look at the California housing data set, housing. So you guys know that when you're looking at housing in, let's say, in a commuter city, the factor that probably matters most is not the size of the house, may matter, but the most important factor may be distance to the highway, because everybody will wake up in the morning and start commuting to their place of work. Isn't it? So people would like to stay close to public transit and to highways. to stay close to public transit and to highways. On the other hand, if you are looking at $10 million dollar mansions, for them proximity to highway may not be a factor at all. Isn't it? They're looking at entirely different factors. So when you're predicting a house price and the ranges around the, you know, high income, very high income mansions, the factors that affect a house value are entirely different. So, yeah, I think now I'm getting the intuition. So let me phrase it in my words, the way I would employ this kind of thinking is to when I try to figure out why was this house predicted at 1.3 million. If I try to find out why did the model peg that value for this house at that time is where this reasoning is applicable. You're saying that at 1.3 million, what factors affected it? Okay. Yeah. For example, suppose you find a house for 1.3 million, and you don't have 1.3 million, that you want to be there. Should you look for a smaller house? Will size bring the price down more? Or will proximity to highway bring the house price down more if you're looking at other houses in the neighborhood okay so that's another entry point to the same thinking process yeah okay that gives you the intuition all right guys anybody else the lot of you are quiet i hope you get the intuition here. That is it. And so the idea is that any local model that you build will ultimately have derivatives, dy dx1, dy dx2. What are these? These are literally in some sense, the sensitivity of the response to those features. And therefore it explains how much those features matter. Right and because it's a linear model, so for example think of a linear model which you which we were used to writing as w1 x1 plus w2 x2. Now if you take the derivative of w x1 you get w1, you get y of the x2, you get w2, you remember that. And we remember that if the data has been properly scaled, essentially it says how much y responds to changes in x1 and x2, and therefore how much locally these factors matter. They are literally given by the weights. And so that is lying. Then comes, so this is one way of looking at it. I think it is 8.30. Guys, would you like to take a 10 minutes break before we continue on to the next one? Think and digest this. So I'll just review what I've covered. I've covered, I started with a very different geometric interpretation. I said that regression and classification, ultimately they find an embedding or a manifold in the feature space geometrically. Those things of rough intuition is there like bed sheets. And imagine that you are a bed bug or something like that. So locally to you, the world looks flat. And if it does look flat, then locally, of course, this also goes to the heart of the fact that you can do Taylor expansions and so forth locally. So and stick to the first order term, that's all. So if that is true, then locally, you can write it in this form, if you write it in this form, you get an essentially additive model, the contributions that change in why are the response changes in an additive manner with respect to the two derivatives, the feature derivatives. And so locally, and because locally it's all very linear, Lange is putting that idea together to create a surrogate model. People often try to make a surrogate model is a model that is simpler somehow than f , right, a g that's simpler somehow than f . And you can have global surrogate models, you can have local surrogate models. LINE goes for a local surrogate model and interpretable, right. The whole point is interpretability here. So it makes a local interpretable explanation that GX is the explanation of FX in the neighborhood of that point X, X naught. And it is model agnostic. It couldn't care less whether FX is coming out of a deep neural network, it's coming out of a support vector machine, it's coming out of a random forest, what it is coming out of. There it will tell you the feature importance and so forth, and's coming out of a random forest, what it is coming out of. There it will tell you the feature importances and so forth and it will give you a local model. Are we together? So far so good guys. So yeah, one last thing I would like to do is also write the loss function. So the loss function that you say is that your GX or your explanation, X, right? The last function that you say is that your g or your explanation, x, the explanation near the? A pi X is neighborhood of X. Local neighborhood, of course, implicit is built into, locality is built into the definition of neighborhood. So how much does G differ from F? Of course, for it to be a good explanation, it should closely match F, right? So, for example, if you make a G that is like this, there is still a little bit of variance between F and G, and you're trying to build a model that sort of mimics or locally has a very high fidelity to F, and that's why sometimes people call it the fidelity term. I think this is called the fidelity term, I think. That's how I remember it. Okay, anyway, but then there is a problem. The more you try to make G look like F, one easy way is make a very complicated G, as complicated as F. Now you'll have a problem. G is no more interpretable. So what you do is, you, so this part is very straightforward. Let me just do that. You are looking for arg min gx. You're looking for that g, that explanation, the simplest explanation in the neighborhood of x, which has a high fidelity to the function f itself. And to prevent the function from getting too complex, what you do is you add a complexity term, your complexity penalty term. Are we together? This penalizes the learning process if it starts getting too complicated. Now in the case of linear model, it doesn't matter. It's not very hurtful. But in the case of a decision tree, what would it do? It would prevent the decision tree from becoming very deep or very complex. Are we together guys? So in nutshell, mathematically, line is captured by this particular equation. Of course, you always put it very succinctly in mathematics once you have understood it. So do we understand this equation now guys? All we are saying is find that G, g that explanation which has high fidelity to f in the neighborhood of pi of x the the neighborhood of x right but at the same time you'll be penalized if you come up with a very complicated explanation right that is it right that is it so come up with the simplest high fidelity explanation that will explain F in the neighborhood of X that's a summary okay so with that summary would you guys like to take a little break or should we just steam ahead all right guys let's take a 15-minute break. After that, we have a long session. So that's why I'm giving a 15-minute break in case you want to grab dinner or something. We'll meet after that. I'll pause the recording. Back at 8.50, right? So to recapitulate, we say that we can make locally interpretable models because all reality when you look at it locally it looks simple isn't it locally it has to be simple the fundamental idea is that if your regression surface or your decision boundary is a manifold embedded in the feature space then inherently that manifold locally looks simple. Therefore you can make all sorts of surrogate local models. Now you can make surrogate models that are global, but you can make very accurate local models. And then when you do that, when you make local models, for example, if you make a linear model, quite literally you can get the feature importance in in that region of the feature space how much does each factor matter you can answer it very clearly and now what people do sometimes is that they take that and they try to create a global average what you can do is you can take random number of points, spread out the feature space, and you can sort of average the feature importances at different places to come up with a global value for feature importance. Obviously, when you see such global numbers, you have to take it with appropriate caution, right? So for example, if you tell weight loss is very important for you to fix your diabetes you better not be saying it to a five-year-old child you see that right because probably a five-year-old child is having diabetic problem not because of weight gain most kids at that age are very slim by the way the doctor would you agree with that uh asif uh you should resume uh youtube oh i should resume the youtube oh goodness i'll have to go through the whole process of creating a new one how many of you by the way want the youtube at this moment i'm watching it on youtube uh ask if you if you can create it. Yeah. All right, then we need to wait because it takes Yeah, yesterday night I was looking for the paper one I could find one. See, go to the website because gradually as things keep happening, I mean this No, for the yesterday paper, like yeah. Yesterday's paper won't come in too soon because but i can post you see here's the thing guys no the i mean i requested for the youtube video but it was not done no but i posted this thing the zoom video itself which is public no no yesterday when you didn't post okay for the previous one actually my question is if all you want is the recording immediately, recording I can post from Zoom. You know, I've been posting the Zoom recordings. Yeah, that is fine. Anything if it is available immediately after the class. Let's not go bother with this whole YouTube thing. It just takes a bit of time waiting for it. I'll post the Zoom videos. So I haven't posted yesterday's discussion. Yes, that's true. I'll post it. So all right. Where are we? Okay. So at the end of the day, we come to this thing. LIME tries to make local models that has the explanation. What this equation is saying is it has a penalty term for complexity and it basically has a penalty term that is a loss for the fidelity. Like how much G looks like F or replicates F in the neighborhood of X, which is the pie of X, right? So it is practically self-describing this equation. It is saying come up with the simplest high fidelity model around x. Now that is one approach. The other approach that I'll come to, and by the way I'm talking about the Shapley algorithm itself at this moment. Do not confuse it with the Schap-Packet. The the shapely but it also throws in a lot of other interesting things we'll do that later separately maybe in the lab or something so shapely was a person from game theory as if let me i think i'm not sharing my screen is it yeah all right guys so this this is the equation i was talking about is this equation now very easy to understand there's a there is a penalty term there's a penalty for complexity and there's a loss with regard to the fidelity. Right? G and F. Now comes another way of looking at it. This goes to Shapley, who was a great, great theorist in game theory and economics and his major contributions. He was actually a contemporary, I may be wrong, but I think he was either a colleague or a classmate of John Nash. John Nash, the figure of The Beautiful Mind. I don't know if you folks remember the movie The Beautiful Mind? Have you read the book or seen the movie a beautiful mind anybody in the movie yeah yes it's a beautiful movie isn't it john nash actually so um he and shapely were contemporaries and worked together and so forth so uh what shipley did he answered an important question So what Shipley did, he answered an important question. He answered the sort of questions that are there all around us. Let's say that you have a team and a team has helped you win a game. Now how much credit do you want to give to each member of the team? How would you divide the pie of credit amongst the team members in a fair way, not based on some rules that the captain gets more and so on and so forth? How much is each member in the team worth or contributing in, let's say, winning the entire series or something like that? Things like that. And it goes to many, many questions. I'll take that. Questions like, for example, you know, imagine that you're in a, this is an example I noticed in some article actually, is suppose you're in an apartment complex and as often happens in apartment complex, you form a housing association. Now, in the US, of course, the housing association is some third party you don't know about, and they impose rules on you. In India, and I think in many other places, typically the housing association is made out of the tenants, or the people living, or the people owning the flats in the building, or the apartments in the building or the apartments in the building and now they they collect money they pull money and then they do the maintenance and they do the repairs and so forth and let's say that the question comes up we need a swimming pool now you can have all sorts of people people who just don't know how to swim who dread the idea of swimming publicly who have want to have nothing whatsoever to do with the public swimming pool in the campus. There could be other people who very rarely use a swimming pool, but they like the idea, but they rarely use it. Then there are regular users who will use it as a few days a week. And then maybe they're swimmers, right? And so now the question is, what size of the swimming pool should it be if you make one? Do we make a small kiddie pool just enough for people to go in and swim around in circles or whatever? Do we need really like 20, 20, like 60 feet, 50 feet pool, how big should the pool be? Right? Should it be Olympic size? And people who are, of course, passionate swimmers, they would want an Olympic size pool. Well, now the question is, what size pool do you make? And it also depends upon how passionate people are about swimming pool. So suppose you say we are going to make a 50 feet, a 50, like a 150 Olympic size swimming pool. Is it fair to ask the guy who is not going to swim at all to contribute equally? Common sense says it's not fair. You should do it based on how much it is going to be used. The people who will use it more should put up more money upfront in building the swimming pool. Does that seem fair, guys? Yes. So I'm facing a similar situation, for example, at Support Vectors. We have a HA amongst all the offices nearby. So we keep debating, they already had security all around my building. And then they decided that their antiquated security system in subgrade. So they wanted everybody to contribute, I was willing to and the thing was divided by square feet. Now it turns out that I own the biggest office space in that complex. So if you do it by square feet, I end up paying a lot. I raised the question that how is square foot matters when it comes to putting camera on every street in our complex. So then, but there were other people who own smaller offices and they weren't willing to pay equal amount. They kept on saying square foot because there was a tradition to push them. It's always a thing, right? Push the burden of things on the, what they call fat cats. Right? The people who can afford. And so at the end of it, the question of fairness is there. Like, is it fair? I'm not going to use the security services at all because I have already secured my property with a state of the art security and I'm paying the most. So those are fundamental questions of fairness. Shapely answered that question, that what is the right way to apportion a fair thing. Now, it has a deep relationship to machine learning, of course, and I'll come to that. So when we think of players in a game, think of features in a model. Let's say that factors contributing to diabetes each factor is a player and so that's a mapping between game and machine learning in theory and machine learning so now think of it this way suppose you have a players p1 or let me just write X 1 extra because we are used to using that x1 to xp there are p players in a team and in a way you can think of your situation uh the collective uh thing as a team of x factors p factors uh each factor is a think of it as a player now and there they form coalition so when you see a data which has all the p factors listed out and the values listed out and a response variable y hat either classification or regression now the question is how much is each of these players contributing to y how much importance or contribution do you give? Now, you can, so at this moment, let's not bring in the complexity of local versus global. Let's take the simple situation. So what is the way that you can do it? So Shapley had a simple answer. He said that this, see, suppose you have, you create this sense. suppose you're looking at x1 right what you do is you exclude x1 and you create every possible coalition so you can create a coalition of the empty set you can create a coalition of just x2 x3 all the way to XP single players. You can create a coalition of X1, X2, X2, pair wise combinations pairs. You can do more combinations, three wise combinations, four wise combinations and so forth. So you can form all possible coalitions, a different people gathering together. Are we together? Or another way of putting it is by taking different subsets of features, but excluding x1. Right? Are we together, guys? And then what you do is you then come up with another one, x1, but this time, forget the null set, you include x1 in the models, x1, coalitions without x1 and all coalitions with x1 with x1 and without x1 this player and now do this in our game we are looking obviously at the a very simple thing the accuracy is a picture metric, it's your accuracy of the model is your R squared for regression, whatever it is some model performance and ask this question for each coalition, what is the difference between a model with X1 and a model which is identical to it, model without X1, right? So suppose the model is, let's take a model X1, X2, X7, these are included, right? So you take this model and the second model would be just X2 x7 right just keep these two features and now what you do is you build you build a model with these and you look at the difference the delta in performance delta in uh whatever your metric is delta metric r squared whatever it is now you do. And the point is that the most important factor or the most important player will make the, will consistently create the biggest deltas. Are we together guys? So if some player is really crucial, you will see a lot of degradation in performance if that player is gone, is not part of the coalition. Isn't it guys? Is this looking like common sense? Anybody? Yes. Yeah. And then all you do is you just take the average of all the average of of the deltas right for each for each factor for each xi you find the average of the average so now you will end up with how much the average contribution each of the factors is making on the way to XP, how much you're getting. And that will give you the feature importance. Now, there are a few interesting subtleties. Why would you do that? The problem is the sort of things that it solves actually are quite interesting. Take the case of two players who are highly correlated, two features which are highly correlated. So what will happen? If you take one feature out, the other feature will still possibly be there in the correlation, isn't it? In at least some of the correlation, the other feature will show up. And so what you will notice is that losing this guy is not making that much of a difference. But suppose you take both of those guys out, then there would be quite a difference. So let's take this thing. Suppose and x7 are strongly correlated. What will happen? Any coalition that does not have x1, like when you're comparing, just look at x1. When you're comparing it with coalitions with x7 and not x1, not x1, and then another set of coalitions, without x7 also, without x7, without x7, x1, both both of them excluded now what happens is let us say that these two factors are highly correlated but they are important factors they matter right then what will happen the coalitions that are formed without x1 and x7 both of them missing they will show degradation in performance does it make sense guys yeah yes sir and so what will happen is x1 will just because it's correlated doesn't mean that it will lose contribution it will be given contribution the only thing is that the contribution will be equally shared between x1 and x7 because with both of them missing the models will severely degrade the deltas will be high and so you will come to the realization that x1 matters and also x7 matters now in real life see the fact is much of the data when you look at it, you find the factors are highly correlated. Isn't it? Let's take any data. You guys look at the housing and whatever it is, or you look at image data, one pixel and the neighboring pixel are two different features. Are they highly correlated? Of course, you know, there'll be a small change of, typically a small change of value from one pixel to the next isn't it look at an image of a mountain and the neighboring pixels quite often will have similar values right they'll be highly correlated make sense guys? Yes. Likewise for text and so on and so forth. So the fact is, real world exhibits substantive correlation amongst features quite often. Right. And given that fact, one nice thing about Shapely is that it addresses that issue very well. It has other very good mathematical properties and the idea is very simple so guys did you understand the idea because when people put it as a mathematical equation it just looks so complicated but this is it this is the idea and biology is to you this matters a lot are you getting the main idea of shapely to you this matters a lot are you getting the main idea of shapely so i mean through the example yes uh we want to see which player contributed most to the winning of the team or which feature right uh that's the problem he was solving through game theory but that theory itself is uh black box to me at the moment i don't understand much of it but this is it i gave you the explanation. What you do is you make a model with just, with X1 present, you make all sorts of models with X1 present, and you make all sorts of models with X1 absent. Doesn't matter what model it is. Let's take a model. The model is a deep neural network, right? See, you don't care for that it's model agnostic it's a black box into the black box you feed all all sort of combinations where x1 is missing right and then you put another set of models another set of data in which x1 is present and you look at the average difference in performance there will be difference in performance when you add X1. So suppose you, so let's take it very realistically. Suppose you took a model in which X2, X3 were present only. So you have data like this. Now your data looks like this. Why? Do you see this? This is a coalition. The coalition is from game theory perspective, coalition is made coalition the coalition is from game theory perspective coalition is made of coalition is made of x1 x2 x3 right and so you have a model in just two variables, exactly the same model, right? You have the same DNN, but you're only feeding X2, X3 into it. You get your Y hats. Now from this, you can compute your, I don't know, pick a measure, whatever it is. Loss function. Yeah, let's say that your loss is mean squared error, right? You compute your mean squared error. Now what you do is you take this model. So let me call this mean squared error with x1, x2, x3 and now you compute again the y hat this is a loss function msc let's say with x1 x2 x3 all present now what will happen is this loss let me just call it l prime right the the idea hopefully l prime hopefully if assuming that x1 matters if assuming that X1 matters, L prime. So one thing that is guaranteed is this should be true. Isn't it? L prime should be a better model or maybe if X1 doesn't matter, it should be as good a model as without X1, isn't it? So you take the delta, you remember this delta L for x2, x3 in the context of x1. You found one change in loss, right, the MSE loss. Now you keep it. Now you build another model. You go and build other combinations. Maybe you take x3, x5, x7. So, you know, I'm just building coalitions. Now you realize that combinatorial is explosion. So suppose you have a nine other features. A 2.9, yeah, exponential. It's an exponentially large number. That is why Shapley is computationally intensive. Right? But you can do some optimizations we'll talk about it so what you do now is suppose you take this model so your data set now is like this x3 x5 x7 and y and you can do the same game right l3 this is made up of x3 x X5, X7. Once again, you can do it with this MSE. And you can do the L prime, which includes X1, X3, X5, X7. Once again, this is another delta. You will come up with another delta L, which is specific to X1, but in the context of x3, x5, x7. And so you can come up with all those other 2 to the 9 models, isn't it? And find all the differences in loss. Now what do you do? All you do is the contribution, x1 is equal to the average whatever number it was number of iterations two to the nine right delta delta l. summation over models. I mean, and now you just did it for x1. Now you need to repeat it for x2, x3, all the way to x5. So computationally is brutal, isn't it? You have to do a pretty computation. Now, one good thing is that it is highly parallelizable. So the implementation that I did years ago, and we are using it, it is highly parallelizable. So the implementation that I did years ago and we are using it, it is highly parallelized. You can scale it out onto a cluster and do it. So we do that. And so now you realize how contributions are made. The idea is very simple, right? But when people write it in equation form, the equation looks rather complicated actually let's try to convert it into equation i don't remember the equation but we should be able to do this contribution over all models in which one over n of what is it of what is it the score the score built with r for all s belonging to you know x1 etc so you can take the score the performance performance built with built with a model s without let us say let me just call it x x without x you know this is a symbol of exclusion by the way the all sets that does not contain x right and the set that contains x the whole set actually the other way around s goes s minus s without x yeah something like this it's basically very simple but you can write you can couch it in some mathematical uh formalism i hope i got this right but okay something close to this right that is all you're doing and when you do that you get the shape you know comes the whole question uh which in machine learning gets important oh well this will give you an overall picture but we also need a local picture because we know that factors differ in their contribution uh like we took the example of house prices, we took the example of the weight and the age and diabetes, so it matters. So what you can do is you can play this game locally. Isn't it? You can have a model, let's say, and that is the explainers in the SHAP package. You can have- Okay, got it, I see. You can make it locally and you can play the same game and you can come the same game and you can come up with it it is the only reliable way of coming up with contributions say now you can tell like for example if the house price is 200k at this point what factors matter which factors are contributing positively and negatively now one interesting thing is that it is a non-parametric model remember the line way we did we built an actual model we built a hyper surface we said it's a hyper plane isn't it or a decision tree but this kind of a contribution not only is that it can deal with the black hole, it also has the property that it actually doesn't build a model. You build a model to do the job. So you use these explainers, what they do is in the neighborhood of it, they do it. But the Shapley itself is agnostic to it. All it is looking at is the output of that, the loss function that comes out of that model are we together guys oh good night it's 9 30. okay uh so we might have to move a little bit faster guys are we understanding this yes yeah this is it and this is the beauty of the shop i mean it's encoded in python in the shop package very very useful package, but Shapely is the main idea that you can apply locally this that and you can create all sorts of implementations for it. The main issue that you face is it is a and when you run Shapely you'll see that you're a lot of the people those of you who ran it in the boot camp I remember some of you said oh my laptop has frozen when you run the SHAP package. And after many, many minutes, it spits out the answer. Sometimes it takes a long time and people think that their thing has frozen. It hasn't frozen, the computation is brutal. Even though the SHAP package does a lot of optimizations to make it faster, it is still a brutal computation, but it comes out in a way that is mathematically justifiable. And it's sort of a simple explanation to this. And now you understand why this idea is so simple and elegant, but so powerful in many areas of life, modern life, and why Shapley was given the Nobel Prize for it. Arjun, just a thought actually, if you have a 300 cross 300 pixel, there are, I mean, like 9 into 10 power 4 features, and we are looking at 2 power 9 into 10 power 4 possibilities of such combinations being calculated? I'm unable to. Yes, so the whole question that does Shapely, is it the most effective method to apply there, right? The answer to that is, and so this is the trade off, the computational trade off. In theory, you can apply it, in practice, can we have cheaper methods? And we will come to that they are actually cheaper methods for deep learning for image processing you can get away with things that converge much faster so guys this is shapely so the difference between line and shapely you you understood. You notice how different approaches they have taken. So this methods you can apply to any algorithm in machine learning, any algorithm whatsoever. Now comes a third thing. Let's go specifically to deep learning. Do you guys need a break or should we just steam ahead now? Probably need to steam ahead. Yeah, let's go just steam ahead now? Probably need to steam ahead. Yeah, let's go and steam ahead. Today is the last class. So forgive me if we go. And guys, I hope your projects are coming along well because I'm about to release the second project. And at the same time, we need to wrap up in this coming week. First week will be to the first project, second week hopefully to the second project. Let's start walking through the solutions that you guys have created. So in effect my work is done, now you guys have to show your work. Deep learning. Deep learning. So what are the core ideas that you can do here? So we borrowed the ideas that we have come up with before. But remember I said that how much a factor matters depends on, in a very simple way, it depends on the gradient, the derivative with respect to i, right? So you can say that the ith feature, feature importance at a given place, let's say that at a near a certain value x naught is given by the derivative and this truth is mathematical truth. It has nothing whatsoever to do with deep learning or anything, isn't it? Because this is the sensitivity anything, isn't it? Because this is the sensitivity of the response to the input. Sorry. Remember why I'm using the word sensitivity? Because that's what it is. It basically tells you that for small changes of Xi, delta Xi, how much will Y change. So suppose y changes a lot, the response changes a lot. What does it mean? It means that xi matters quite a bit, isn't it? Yeah. So this is a invariant of that. So now this argument holds to, even if you have a deep neural network and so on and so forth with many layers and gazillion complexity, but ultimately you get y traditionally what people also do is sometimes it is unnecessary to do the softmax if you're doing a classifier of y hat you take y hat before you apply the softmax this is quite common actually because the softmax there is redundant and in fact useless so just before the softmax whatever is redundant and in fact, useless. So just before the softmax, whatever raw scores you get, you get some scores, some this. So people in this literature often use the word score, right? But I'll use y, or y hat, so I'll just use y. So the thing is, what went in were x1, x2. Now what are these xn? These are, let's take an image for an image each of these x1s are what these are the pixels isn't it guys so suppose i have let's look at mnest nest. What is it? It's a 28 cross 28 cross one. It's a pixel monochrome, a 28, 28, and just a monochrome, one channel. Isn't it true, guys? And so when you linearize it, you write it out like this. Each of this is x1 all the way to x784. Isn't it? It's a vector. You just linearize it into a vector. So what do you have? You have an x vector, which has so many. And each value represents a pixel in the image. you represents a pixel in the image. Am I saying something very simple guys? Yes. So now comes the question. You know that the importance of each pixel, so suppose now think of the physical intuition. If the model is telling you it's a cat or dog and the response of it, before you feed it into the softmax, let's say, is highly dependent on this pixel. So let's say that you have a maybe between a cat and a, let's say, let's take something different, tree. Suppose. Is this something like a cat? So let us say that what it is doing is it is coloring these pixels, all the pixels here. It is saying that its decision is highly sensitive to these pixels, to these features. How would you interpret it in common language? Would you say that what this neural network is saying is, I am making my decision based on what I am seeing there? Would it be fair to say that? And that is it. And that's why this is called finding those pixels that sort of have high, that the output is very sensitive to, finding those pixels and a map of those pixels. This is called the saliency map. This is the saliency map. This is the saliency map. It's the oldest sort of algorithm. And so the idea is this guys and the implementations now, people have come up with many variants of it. It's a pretty, it's one of the oldest ideas because it's a straightforward application of mathematics. And now comes some practical considerations good grief how in the world would you find see we know how to propagate loss what do we do we do a gradient of the loss with respect to the weights of each layer isn't it this is what we know and this is what we know how to back propagate do you remember this guys when we do the back prop we are we used to take the gradient of the loss with respect to the weights in simple terms what we used to do is do this weight next is of a given layer L is equal to the weight, the previous value of the weight minus alpha and let us say that this is the ith weight in that particular layer i j let me just say i j let me give it in full glory let me just say ij let me give it in full glory do you remember this is your uh this is your gradient descent step step and for layer l and now step and the backdrop step was and i wouldn't go too much into it is to say that now how do you do it for the layer L minus 1 the thing is we need to know for L minus 1 sorry L minus 1 layer right but that if somehow I can express in terms of I am done isn isn't it? Some sort of a, let me, weight jkl, right? Kmn, let me just call it mn. Some other weights of the layer ahead of it, right? We did this back prop, guys. You remember that we had to, all we had to do is somehow come up with this expression in terms of the activation functions and so forth. And once we can do it, we can apply the same thing and we can update the weight. So that was back prop. That was the world of back prop. That was fine, but here there's a twist. First of all, we are not taking the derivative with respect to loss. We are taking the derivative with respect to y. And we are not taking derivative with respect to weight. The model has learned, isn't it? The model has learned. So we should freeze the weights. We shouldn't change the weights. We should freeze it. But what we are asking for is the gradient with respect to x i right so the numerator has changed and the denominator has changed so in other words let me recap what i said suppose you have a deep neural net whatever whether it's a coagulational network or whatever it is doesn't matter you freeze the weight why do we freeze the weight guys so that's the model we are trying to understand. So. That's right. The model is not learning. It is a model that has been built and you're trying to understand the model. If you let the weights change, you'll be screwed. The other thing is that you give it X vector, you let the Y come out. You don't bother about the loss. Loss doesn't matter at this stage and one of the things again you do is forget the softmax function or whatever it is because ultimately those numbers will just get exponentiated and all that so just it is good enough to stay one step back now what you do so suppose you are doing it in pi torch what you do is you freeze the weight but you let back propagation work you don't take the gradient descent step you just let the back propagation work but one more thing back propagation stops usually at this layer the first layer right so what you have to do is you have to explicitly say in PyTorch, you have to tell that this autograd. So you have to enable gradient, you know, update of X itself. Typically, the input is never updated because input is supposed to be data. But now you're shaking it. You're saying, hey, you know what, I'm finding the gradient of the response with respect to the input. So these are just implementation details. We'll cover it hopefully in the next lab if you find time. But the end story is that you'll be able to find Xi. And the moment you can find Xi, you can now go and make a feature map. And we'll see in the lab, very evocative actually that suppose you have a cat or you have an animal it will just tell you that i'm paying attention to this this is what i'm paying attention to in the picture and uh it's just the first time you see the saliency maps come out and you see and we'll see in the lab, anyway, this is all theory. It is very dramatic actually. You see how the neural network is understanding your picture and it quite aligns with your understanding of the picture. If you think, why did you call this picture a cat? Quite likely you zoomed onto the face between a cat and a tree and so forth. So that is a saliency map. Now, saliency map has a few sort of a... Now, there's a variant of it. There's a deconvulation step. So I don't know. We are running out of time. Do you guys care about the small details guys small technical details because you may forget what do you have to explicitly stay in pi torch do you have to explicitly stay in pi torch to freeze the weights i miss that yeah yeah that is it you freeze the weight so i think as if the question i'm about to ask is probably what you're about to explain the the question i was wondering is so with the neural network if you go all the way back to the original the input data it becomes a little hard to interpret right when we when you taught us convolutional neural networks we were looking at those little convolutional grids yes talking about each of them is trying to interpret like a curve or an angle or something right right so wouldn't there be a intuitive way to just go to that level and then try to understand what the network is doing rather than go all the way back to the original uh the mns data set yes yes so here's the thing the intuition for this you get much better with a dense neural network for this, you get much better with a dense neural network. With dense neural network is easy, right? Dense neural network, this intuition gets, the back propagation goes all the way. If you can do it with dense, you can do it with a congulation also. But this is where the change comes. When I give the saliency map approach, what it does is that when you move forward, if you end up with negative values in the forward direction, it sort of sets to null, to zero, negative values. The same, like these are implementation details as you are doing forward propagation and you notice that the value is at any given layer any feature is turning out negative you just set it to zero then there is in the other one which is the deconv approach approach you do the opposite you let the negative values go through. But when the gradients are flowing back, you stop negative gradients from flowing back. So any gradient that has become negative, you set it to zero and it doesn't go back anymore. So this is a deconv approach. Now, there's an interesting thing to the deconv approach. I'll give you guys a set of articles where you can go into more detail, but being sensitive to a limited time, the decon goes to, in a way, when you're talking about images, it makes more sort of a practical thing. What happens is that, see in the convalescent neural network you have some feature map, right? These are your filters. So some feature map comes out, 20 filters come out. Each of these filters ultimately has some feature maps, right? Certain numbers, a three, whatever it is, numbers scattered over there. And suppose you have, right? Certain numbers, a three, whatever it is, numbers scattered over there. And suppose you have, I don't know, maybe 20 feature maps, 20 of these feature maps. You get this, you flatten this, you feed it into a dense layer. Isn't it guys? Do you remember your convalescent neural nets? After all the filter convalescent layers, you put a dense layer and then you get your convalescent neural nets? After all the filter convalescent layers, you put a dense layer and then you get your output. Yeah. Right. So the decon says something quite simple, actually. It says that suppose the last layer and by the way, there's an additional thing. You stick in, okay, actually, let this be there. We won't come to that. So just actually forget this, this is something else. At this particular moment, just leave it to the fact that you have saliency and you have decon. And there's a way to put these things together and play around with it and so forth. Then there is such a thing as a guided back prop in the saliency map, which is what we are doing. You set your gradients to zero and set values, some of the values to zero. So there's some small technical details that keep coming up with this. Then you have other models. So for example, you have class activation maps. Class activation maps are these. What you look at is that you take, for example, the last layer and you have this y hat and you ask this question. Suppose I were to take this last layer, the last feature map, and treat it as though it is input, right? And basically look at which parts have lit up through all of this back propagation and the response, right? Which parts are lighting up and having high gradient. So you look at the gradient for each of these regions of features. You look at the gradient with respect to y for that particular feature and multiply it by the value of the feature itself. A strongly lit feature with a strong gradient matters. That's the basic idea. So these are called activation maps. And based on this, when you do the gradient, when you, I mean, what I have told you is actually the gradient version of the activation map, right? And this is considered a very powerful approach. What you do, so guys, here's the the thing why would i take the last filter layer there's some details like you have to do one more max pooling just to average i would say average pooling and so on and so forth let's not go into the detail if you do through one's and you back prop you're telling which of these features is affecting the response a lot. But the value of the feature also matters because the response ultimately is a product of the gradient and the amount of the feature itself. If the feature has already vanished to zero or close to zero, the gradient in itself would not be enough, isn't it? So the response, it's just a bit of mathematics, the response depends on both of these factors, sensitivity to the product of these factors on the feature map. But then you say, well, this is a problem because this feature map, it has been created by so many, many layers of condolation, right? and why would i trust this feature map what what does it tell me which features are important and that is where the trick comes so given this feature map you can deconvulate it you can do the inverse of convulations you can deconvulate it to create a it to create a original so-called restored image. Now you realize that the restored image is not going to be as high resolution as the input image because this feature map is a tiny map. Let's say that it is a five cross five map and you have 20 features. So a lot of information has gotten compressed in very little data. And when you try to do this decongulation, that decongulation is not going to be like a super high resolution image at all. It will be just pixelated version of it. But even though it's pixelated, it will just highlight those regions which are important. Let's say if it is a cat, it will tend to highlight this sort of region, the pixels here. And this is another technique that you use in deep neural networks with images and so forth. So by the way, guys, there is a vast literature and I'll put up a paper, the best one that I could find, which is a tour guide to all of this. And when you read it, and I would highly recommend a paper, the best one that I could find, which is a tour guide through all of this, and when you read it, and I would highly recommend that you read it because you, this will, okay, the good thing with that paper is it's a review, it's a, it's sort of a survey paper, so you get to know the whole landscape. The bad, I think, or the limitation with the paper, not the limitation, is catering to giving a survey, but it doesn't explain things. It just says these things exist. But I've given you the explanation for a couple of these approaches. Ultimately, the lesson is the gradient is a key to that. If you have the gradient of the output with respect to the input, it sort of gives you the clue. And all of these ideas ultimately come back from the input itself. It basically calculates itself. So this is one approach. There is another approach that you can do. You can ask this question. Once a neural network has been trained, suppose you have a network, it has been trained and there are certain weights and so forth. So let's take an arbitrary node which belongs to layer L and it is a node N i belonging to the layer N. So another way of trying to understand a neural network is to ask this question, what exactly is this node node doing what is this node learning or observing or watching out for right so how would you figure that out what exactly is this node doing is a question you are getting an output y hat or let me just write y you're getting in you have certain input vector x vector which is a big column vector of so many features if it's an image right or data whatever it is another way to try to understand a neural network is to say what exactly is this not doing you have pinpointed one node in the big mess and said what what is this doing? What is this learning? So how would you find that? What you do is something quite interesting. You feed it random noise and you look at the output. So you get the output. Now what you do is you start shaping the noise so that the activation of this node maximizes the activation of the node. What will maximize this particular, what sort of inputs will maximize that node? And so suppose what happens is this vector refers to an image. Suppose there is a feature like this in the image. What you'll end up is you'll end up with noise and a feature like this, right? Darkness and a feature like this. So what have you suddenly discovered? If you find that you have to shape noise to this, to this image, and then this particular node lights up, what can you say about that node? That's important towards the prediction. Exactly. This node is observing this. Observes this, isn't it? And it is watching out for this. Typically, what happens is that you will find quite a few, not just one, but a few other nodes somewhere here that will be paying attention to the same feature. And so you end up with a group of nodes that pay attention to this feature and you sort of come up with an explanation of what it is doing and by the way there's more theory to it but we have run out of time so i would like to summarize in the last half an hour what happens is google's project deep dream was actually based on this idea that try to understand how does a node respond? What is it responding to? Figure it out. So that, again, goes to, see, you look at this. When you try to understand it like that, you are not trying to explain which features are important, isn't it? It's a completely different way of trying to understand neural networks you're saying explain to me what this particular node in the neural network does and you pursue that area of exploration and when you understand that you say okay i understood something about this neural net at least these nodes are doing this and that node is doing that. Are we together guys? This is another way of coming to an explanation of it. Now the literature on understanding deep neural networks is growing at a rapid pace. There are many, many approaches coming up. I told you the ones that are being sort of in the industry being used and are dominant, but a lot of research is being done and lots of new things are coming. It is quite possible that something else is suddenly becoming very hot and I haven't caught up to it. So it's a great opportunity for you guys to go read up on it. And if you find something that I missed, do post it to our Slack. I mean, I know that I've missed a lot of things which I've deliberately not covered because of the time we could cover only a few ideas. So I thought I'll cover three big ideas. Say and She maps and activation maps is one for deep learning. Local, the LIME and Shapely, these three models. So let us recap what we did. We said once again that locally complex models can look quite simple or at least simpler than their full complexity locally you can have surrogate models which are local which are simpler and therefore you can build local surrogate models and you don't care what the model what the original model is whether it's a deep neural networks or whatever when you take that approach you are doing this local interpretable model agnostic by model agnostic you mean the f the complex weird model not your surrogate explanation explanation is the g the surrogate let's let's review this from the beginning when we say that so this is the basic idea anyway let me start from the beginning we are saying that geometrically Anyway, let me start from the beginning. We are saying that geometrically, all models are essentially manifolds. And so locally, they are simple. They are ingredient in fact, and so you can make local models. Now, we addressed the question of additivity, Balaji had asked. Then we talked about line, which says we are not going to make global surrogate models, we're going to make local surrogate, just local surrogate models. And the explanation is the G, the model is the f , the original model is the f , that you're trying to create a surrogate for. And you keep it interpretable by using simple things, linear regression, logistic regression, simple decision trees. Right? And so you're seeing that given a point x in the neighborhood of x, you can have simple explanation. So you deliberately choose a loss function like this. You're saying the, you find the model where the loss function has two terms. One is the complexity term. Do not make complex models because you lose explainability or interpretability. And then also build a model that has a fairly high fidelity to the original model in the neighborhood, in the neighborhood part. So the idea is, let me say that, they can just give it complex model, complex model, black box. This is what you're considering the black box. This is your surrogate. This is the neighborhood. And this is the complexity term, complexity term, yeah. So I hope the line is clear now and how it works is it takes a weighted kernel and it just builds a simple linear model in that and it generates the points through some Gaussian sampling or something. So those are minor implementation details. Then we come to the next model which is based on the Shapley idea. Very powerful game theoretic idea. It just says that look at all coalitions that the performance of all coalitions without this player and the and the same coalitions with this player and look at the difference in performance. You will know if you add it to this team, how much does the performance improve. And by averaging the effect of this player on all coalitions, you come up with the contribution for x1 and then now you have a way to compare it to the contribution of x2, x3, x4, xp. So you come up with those values. So that is Shapley. Ben Kope's deep learning model builds on the same idea, you know? The oldest idea is that if you have the gradient of the response with respect to the input, you know the sensitivity of the response to the input. You get the saliency saliency. And so this brings up all of these little techniques with variation, saliency map, activation map, grad cam, which is including the derivative also, the gradient also into the activation map and so on and so forth. That's that. In implementation detail, the only thing is you do back propagation, but this time you don't do the back propagation with respect to the loss. You do the back propagation with respect to the response. And the other thing is you back propagate. The denominator becomes the input. You back propagate all the way back so that you can ultimately compute this. compute this. It will do this in the lab. And when you do that, it's a very, very sort of evocative guy. As you see that in the image, it will pick the pixels and it will say, Hey, I'm paying attention to this. And it's quite amazing to see this complex neural network actually explain itself and say, Oh, this is what I'm paying attention to. That's why I'm calling this a cat. Now, the same thing, let me talk about the attention. There's one more concept, which attention models have been getting a lot of sense. Attention models. Now, the same thing applies for NLP. So suppose you have a word, and this and that and so forth. Lots of words are there. And you say that this word is a positive sentiment. Let's take a basic example. Positive or negative sentiment. So you know that in coming to this decision, it has paid attention to some words to different degrees. So it can highlight the words. It can say I paid 10, 8, 2, 1, 3, 0. This much, how much attention I paid to the different words in coming to the conclusion that this is a positive sentiment sentence. And I've explained attention model in quite a bit of detail. So, and in great detail, we went over all the the steps so you know that this is there. Attention is attention on the words, being paid on the words and making the decision. And that is that guys that another that is another way NLP and with Spacey, we did that. We will do it again, not with Spacey, with transformers, and we'll do it again in the lab that we'll do on Wednesday. So all of this theory will become very concrete when we do the lab. Asif? Yes. I had a quick question. In the Shapley, right after after the shapely you describe about the saliency map where you do the back prop uh and find the find the features that are having the higher highest impact on the y right so i mean using shapely also you can get the saliency map I mean, using Shapely also you can get the saliency map, right? What is so special? That's occlusion. So yeah, what you do is, and see, I have, there's so many techniques that I haven't talked about. So let me give you a flavor of that. What you do is suppose you have an image, right? Suppose you have a cat image. It's of a cat. Right? And here is a cat. Now what happens is that you could do this thing by a different way. You could say, you know what, I'm going to exclude this part of the image. And is my answer changing? Or changing or you know the score that is coming out you don't care about whether it's a cat or dog you just look at the score the score for the cat right let's say that the score for the cat how much is it changing if i cover this area so that is occlusion the word for that is occu so that is occlusion the word for that is occu or you i'm misspelling it are to you occlusion principle so you cover different parts of the image or different groups of pixels the word people use is super pixel means if a few pixels are exactly the same value or almost the same value you club them together and call them a super pixel. So what you do is you occlude some super pixels and you see how much does your response change. So one example I'll tell you which is from the medical domain. There is a problem which is a lung problem and Sukpal you'll give me the name of this. So suppose you have lungs like this. and Sukpal you'll give me the name of this so suppose you have a lungs like this right water fills up at the bottom it starts with pluri what's the word for that I think it's pluracy Azif yeah let's say pluracy right water fills up at the bottom of it and so what happens is that in x-ray, how would you detect it? So what they look for is something quite interesting. If you ask a doctor, how do you know that this guy's lungs are filled with water? He has this problem. What happens is you look for this. Let me color it red. They look for this characteristic V shape. Right. And what happens is that when your lungs are filled with water, this shape gets blunt. It becomes like this. In the X-ray image, it shows up like this. And this part entirely, the lower part entirely sort of disappears, so it's a different way. The water doesn't light up. It just sort of becomes dark. But the characteristic thing is that you have a blunted edge. And that's how doctors tell, apparently, that this guy is suffering from pleurisy. Now comes a question. You feed it into a deep neural network and you say, okay, classify whether it is, this guy has piracy or not. What you will find is whichever method you use, whether you use a saliency map or whether you use this, they will all sort of zoom in to this. And if you, so for example, if you do occlusion, let's say that you took this section out entirely, this part of the image, you take different parts of the image out and you look at the score for pleurisy, right? What will happen is if you take this part out and you don't show it, your score for pleurisy will plummet most. So what does that tell you? It tells you that it has it is latching on to this feature. Right? Are we getting it in a very practical sense, it is latching on to this feature, these areas of the image. And it is quite remarkable that a neural network can be an x ray, which is a very complex object. It needs a lot of training to make somebody understand an x-ray, especially lung x-rays, which is diffused. It's not like a broken bone, which you can clearly see. So it takes a lot of medical training to be sensitive to these little markers that are there. Sukpal, are you there? He's, I think, gone off to sleep. Patrick is there. I'm here, too. Oh, you're here. Patrick, you're also there. I didn't notice you. So do you agree with this? These things are hard to, like you need training to see this. Yes, every med student has to be sensitive to this so this is a plural effusion and uh what you're saying the the letter the letter v that's the costophrenic angle it gets blunted because the fluid we don't know if it's an inflammation like mucus water yes so that's the problem with radiology because it's monochromatic. So maybe with machine learning, we can slowly be more sensitive and figure out if it's actually inflammation, mucus, or pus compared to just water. Yes. Thank you for that input. So guys, yeah, this is it. Did you see what we are talking about and how these things matter? So you can do it through Occlusion, you can do it through, you know, activation maps, siliancy maps, all this decon, you know, grad cam and all of these. But ultimately, the basic ideas are the same. Occlusion is simple. If I remove it, how much will your score go down? So the occlusion has a flavor of Shapley. To your question, Balaji. It's like a subset of Shapley, right? Instead of looking at all the combinations, you only exclude one of them and look at the rest. Exactly. Now, applying a full-blown Shapley is possible, but computationally very aggressive. Trying a full blown Shapely is possible, but computationally very aggressive. Right? And you have to use some things to do it. We will, if time permits, I don't know whether it will run on your laptops or not. We'll try Shapely and we'll try Lime on the MNIST dataset. We can do that. Still that question I had in the mind is Shap Shapley looks brute force, and then the saliency goes backwards from the y and back to the x by doing intelligent back propagation. Wouldn't that be the most efficient without hitting the exponential? I mean, see, that is why people in this space, the image processing space, if you ask them, how do you explain a model, they would never have even heard of Shapely. They would by default say saliency maps, grad cam, attention maps and things like that, occlusion. They will use words like that because they are cheap and fast. You have already trained the model. Think about it. You have already trained the model. All you're doing is one final back prop because you're going and taking the derivative with respect to the input also, which you don't do in the normal training. I find it to be the most efficient, kindly correct me if I'm wrong, of all the methods I saw, this is doing a very deterministic backdrop. So, and you can stop, and you said somewhere that you can stop at some layer and then do a decon so that you don't need to go to the whole first layer with that much accuracy. The grad cam, this activation map with gradient, this is it. What you do is you look at the last layer. You see this last layer. This is the last conv layer. There is a little bit of additional bit which I have sort of fudged over, which is the fact that over the last conv layer, you insert typically average pooling layer, but you don't do if you need grad. So what you do is you take this last layer, you treat it as input, and now you feed it into the dense layer that comes always in a convolational network, and you get the output, and you now look at that gradient of the output with respect to which feature here. Now what happens? You have found saliency in the last convolation feature map, isn't it? But what you can do is you can be convolated to the original image, right? Because that feature map was arrived by convolation process. So you can be convolated back now and you'll get a low resolution picture but it will contain your it will contain your you know important saliency maps or attention maps built-in little fuzzy map but it should give us a good good insight exactly exactly so that is the deconvulation that you're probably reading about got it okay i see This addresses the question of earlier that I asked when you have a 300-300 cross 300 pixel and then you're trying to work which pixel is having the maximum effect on the output. This solves kind of that problem, I guess. Yeah, yeah. You stop at some layer. See, Balaji, you were there in ML 200. Remember I talked about no free lunch theorem? Right, yes. What does it say? No one algorithm always works best in every situation. So every algorithm has a place under the sun. For some situations it's great, for other situations something else is great. So here is a situation where Shetty wouldn't be shape wouldn't wouldn't be very effective it wouldn't be very computationally meaningful but you can get but there are much more effective methods that will get you there i think that's it seems to me like the occlusion method that you were talking about right and that that is more applicable when you have a domain expert right the domain expert also has an intuition about like if you take that example where you gave about pleurisy a doctor would tell you what he would be looking for no no no you don't need in that case what you do is you take this occlusion mask and you go all over the image to find where exactly is that area which leads to the maximal drop of score okay I mean I was arriving at it if you had a domain expert then you know where to go looking for okay no no see you always tie the knowledge of a domain expert somehow you hybridize it but I'm at this moment giving you pure technical okay means yeah alright guys so that finishes today's session. I hope, did you guys find it fun and useful? Very informative. Yep. Nice. It's a wonderful topic, guys, don't ignore it. Just because most of the textbooks don't cover it, doesn't mean it doesn't matter. If we use black boxes and don't care about it, we talked about yesterday, society is doomed. It will tear apart the fabric of society. The implications are far and deep. On the positive side, explainability brings a lot to you. point in the image why the network thinks it's COVID, then a domain expert can look at that very, very carefully and decide whether the network was right. Isn't it? So it's a great guidance system. I don't think expertise of a doctor can be replaced anytime soon, but these things can make it facilitate in decision support, facilitate in the diagnostic investigations, and can be very useful. And there are many, many such situations. So that's why explainable machine learning is a big thing you should care about. So, guys, now let me wrap up the course, because next two weeks are projects. What did we do? We covered a large landscape. We started with the fundamentals of machine learning. Remember we studied backdrop, we studied the lost landscapes, the shape of the lost landscapes, and how it is affected by all sorts of things, batch normalization, stochastic gradient descent, dropouts, and so on and so forth. We did the feedforward networks, we did conolational neural networks, we did recurrent neural networks, we did transformers, spent quite a bit of time on transformers because transformers are quite dominant these days. Along the way we covered a lot of research papers in this workshop. I believe we have covered 15 or so research papers so far and we're going to cover some more. We have had close to 16 or 15 quizzes, 16 quizzes. We have had close to 70 videos now or 70 sessions and vast amount of topic. So it can be overwhelming. It will stay with you if and only if you go review that. We covered the ethics of AI. We covered the interpretability of AI. We learned about transfer learning, which is that don't build a model unless you really have to. In most situations, you can borrow a model and fine tune it if needed. So transfer learning is very, very important. If you remember, I started the workshop with transfer learning is very very important if you remember i started the workshop with transfer learning so it is still there we did time series we did a lot of natural language processing so this is the gamut of topics that we have covered and do your your portal will always be accessible to you uh well always is a relative If my server crashes, you won't have access to it. But so far, the server is still running and you have access to it. Assume that it's accessible at least for a couple of months for you, two, three months. Or more, with luck, more. But do use the time to go review the material, guys. And it's been a long journey. I hope it's been a fun journey.