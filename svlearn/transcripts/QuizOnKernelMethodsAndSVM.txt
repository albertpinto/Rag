 Kernel methods such as SVM etc can be used for, well we can use it for both classification and regression. The third answer, neither of the two would not be correct. I believe most of you got this right, I would imagine. The, I wish I could see, very good point. I wish I could see statistics here for each of the quiz. Yeah, so yeah, with the Google one, like you see the question and then you see a pie chart or you can configure how you want to visualize the result. And then there is a link, you click on the link and then it gives you the details, but without clicking on the link, you can see the visualization of how the class did. Okay. So yeah, I'll look for it. Even my setup of Moodle is a few years old. I set it up in 2016, most likely I need to update it. So what is a kernel function function what does a kernel function do guys remember that you have a mapping function that takes you to a higher dimension and then you just take a dot product in the higher dimension right so that's the way to look at it first each point goes to a higher dimension and a kernel is essentially a dot product. A kernel function is a dot product in the higher dimension. So if you look at it like that, you'll realize that none of the other things make sense. The function, the mapping function, is a function of only one point, not of two points. So this wouldn't make sense. This wouldn't make sense. It is adding up the two vectors, so you won't get a number. You'll instead get a vector in the higher dimension. This is well funny, but not true. It's too complex. Then what do we do? It's basically a simple inner product in the higher dimension. Remember, that's the way I explained it to you. That's what a kernel is. As if some people don't have the math background for that so is there... Oh you want me to re-explain that? Okay let me do that. In fact let me put it on the writing tablet and explain it once more. So let me do that. So the basic idea is that, suppose you have data. Let's say that you have data, which is like our classic data was this. Let's take this egg-shaped data. Would you, and you have a data of another color which was like yeah that is your background actually that was exactly that is it and so we drew a decision boundary that would be like, let me make a thick one, a decision boundary that would be like like, well, I'm not doing a very good job, but it's a certain circular sort of a decision boundary here. Now let's think about it. What are we trying to do? We are trying to go to a space. Now let's think about it. What are we trying to do? We are trying to go to a space. Let me go here. We are trying to go to a space. We're trying to find a function. Such that if you remember, we can do one of many things either we can stay in the same dimension space with a point x, x, phi of x does what? So suppose you have a point x is equal to x1, x2, the two, the coordinates of the two dimensional coordinates of a point x1 and x2. So without going to a higher dimension, one way you can do that is you can go to Phi X is equal to X 1 square X 2 square isn't it do you see that guys if I do this then if you remember the decision boundary became like this and all the yellow points then became here. So the game was to first is that if you can find the Phi you are in luck. It's an explicit mapping function. Isn't it guys? On the other hand, if you can't, then well, it turns out that you don't really need to do that. So long as you can find the dot product in the higher space between two points. So the kernel function, that is what it was, the kernel is a function given two vectors, x prime, what you do is, first you would do, you would do the inner product of it. You would first do x and phi x prime, right. So what would happen is, this is a measure, what is a dot product? It's the similarity between two vectors, right. It's sort of a measure in a product is a measure. Measure of similarity. Between Two Vectors. And between two vectors and and so obviously these are also vectors maybe I should specify that So, it may be that you're taking two points, let's say that this is x and this is x prime, right, as an example. So, these two here, these are the two vectors, let us say, and then here they will go to something else, let us say, right, x one Phi x and Phi x prime something or the other. They'll go to two different places. In the other space. And so this is the function that we use the kernel function. We didn't go too much into the mathematics of this. I was actually planning to give you guys a session. If you want to understand how kernel functions lead to support vector machines and all of these solutions, kernel functions are beyond support vector machines. Support vector machines are just one very popular use case of kernel functions. If you guys want to understand kernel functions, let me know if I get enough votes or enough asks for it. One of these Sundays I can make the seminar session and explanation of the kernel. Yes, Asif. So let me do that. It's beautiful mathematics. Now that we are not doing the engineering math class, there wasn't enough enrollment for it. I suppose we should do it here. A one question. So, regarding that. So if you go back to that note you were just drawing on the board. You want me to go back to that. Okay. One second. to that okay one second yeah so basically so if you can see a circular boundary like that like you had put like Phi X is it vector X 1 square and X 2 square right but in cases where we cannot find explicit boundary like that then we just do like then how do I find find this kernel function that's the thing see the beautiful thing about kernel function is see the normal story would have been the normal story would have been that you have to first step one would be or like step one without kernel trick without kernel trick first is step one find Phi of X that serves the purpose that serves the purpose what is the purpose creating separable data sets linearly separate classes then use it to solve the problem then solve essentially a maximum margin hyperplane problem right Yes, maximum. Max margin hyperplane. If you already learned about, which is the hardmax softmax, with kernel trick trick that's the thing and I didn't explain to you how because that gets mathematically tricky it has to do with something called the Mercer theorem and so forth that if this function is well behaved then and it generally is if you take some good kernels. Then you can actually need not Not learn the explicit form. Of five X, you can insert solve the problem problem by essentially some in some sense short-circuiting you can find Phi X Phi X prime it you can find the inner product without actually knowing Phi isn't that remarkable and so if you can state your decision boundary in terms of just the dot products you have a winner so there is a very beautiful intuition behind it well I don't know this okay very hand-waving argument actually let me give you that hand-waving argument. Suppose you have three points. Two points here. Right? Now, if I ask you, I know that there is a point X whose dot product with this point. So this is vector, let me call it X a and let me call this vector X B right suppose I tell you that I know some point I know X X I know this dot product and it happens to be two. What can you tell about it? Right, you realize that she suppose I have a vector. Here. Somewhere along this line. Suppose the answer is somewhere along this line I can find Like I can find a lot of points or pendicule it. I don't know. Somehow you can find a lot of points perpendicular to, I don't know, somehow you can find a lot of points that will satisfy this constraint, isn't it? The dot product is equal to two. Quite a few points you can find. But now I say it's dot product with XB, it happens to be some number, let's a six. Do you realize that that constraints me. Yes. And so what you do is you end up picking up a unique value for x, there will be only one value of x that will satisfy This constraint, right, the more the more points that you add and you specify the dot product let's say X C I give you is equal to minus 4 something like this what I have done is I haven't given you much choice it is essentially triangulation isn't it you'll be hard-pressed to find a point which satisfies all of these conditions. And therefore, one way of specifying points in spaces, not by giving its explicit coordinates, which is that this one. It would be x1, x2, x33 component let me take an example right but alternatively you can define X in terms of its dot product with respect to X dot product with respect to a set of points I set of other vector points. It's an unusual way of doing it, but when you do that, the fact is, once you realize that you can do that, a whole set of magic becomes accessible to you. And that is what the kernel methods thing is pretty much all about. let's go back to the quiz so this question is now straightforward isn't it let's go to the next question and see what what we have here this is something from the past it was just a review question I wouldn't do this this one if X is a point in a p-dimensional space arbitrary point the kernel method for classification a mapping function aims to do what it wants to project it into a higher dimensional space in such a way that the two classes become linearly separable the data of the two classes become linearly separated and what do you want to do you want to draw a hyper plane so in two-dimensional what is a linear hyper plane line a line in three dimension it's a two-dimensional plane in four dimensions it will be a three-dimensional plane so if you project data into a Q dimensional space a hyper plane would be how many dimensions okay you minus one yeah that's the reason I was about to ask you like why it is minus one yeah okay that's right that's the explanation well kernels do not stand for linux kernel source uh windows kernel whatever in machine learning kernel is the definition that we just went through next question let x belong to rp in the kernel method the mapping function maps each point in the input feature space to a different point you in the art in the feature space where the dimensionality of the other space is usually higher same or higher so that's that in this particular case guys what does the decision boundary look like in here when you say it is same or higher so is that the same intuition as the neighborhood methods no no look at this problem that we just did we just saw it so look at the background that I have do you notice that it goes to on the when I look at it do you see that it has gone from two dimension to two dimension itself you're not sharing your screen okay once again it's just the background you see the background the arrow from the left to the right yeah so what what am i doing I'm separating it using this function and you go from r2 to r2 plane to plane isn't it and if you remember the other projection I did which is at the bottom of the page if you go back and look at the video in fact let me see if I can find that video here and I'll share and And another thing here. Neighborhood methods. If I have that initialization Kernels Yeah, that is a screenshot like the bottom part of this screenshot. Yes. So let me. Okay, let me go back and share what I was going to share This is a recap guys for you. share this is a recap guys for you you remember this I could go from this I don't know if you can see my mouse at all yes not mostly I could go from this to this or I could go from this to this parabolic parabolic surface, isn't it. In the first case I mapped from our to to our to in the second case I mapped it to our three three dimensional space. Yeah. Are we getting it guys this this image here is a three dimensional surface paraboloid Surface. You see that guy yes mapping function can be two dimension three dimension so you in general you go to a higher dimension or at least stay in the same dimension quite often you even go to infinite dimension if you use the Gaussian kernel it's a bit of mathematics we'll do it so the math of data science of course had all of that fun stuff the technicalities of it so here which kernels would work a linear kernel would work or not this is a linear decision boundary so the linear will work of course a polynomial kernel will also work because linear is an example of a polynomial degree what will a Gaussian kernel work well now you're projecting data into infinite dimensional space but still yes it will work in a simple problem like this any any of these kernels will work what about this river data set will the different kernels work which kernel will work linear will not work because the decision boundary is certainly not linear a polynomial will work right I'll leave that as an exercise for you and a Gaussian kernel will work in fact let me show here this was as we would it is it oh yeah here look at this this is your data set how do we go and do Gaussian kernel you know with the kernel with radial basis functions when I do that the decision boundary is obviously very very complicated here when you project it down and this is how it is it says that the river is red colored and so forth so you can imagine that it has put all this lighthouses the support vectors in these locations to identify the path. So that's what you do with this. That's linear and does Gaussian polynomial just work for mostly everything? See, the question is polynomial will not work for everything if it is a transcendental decision function because you know you have been for you have expanded it into infinite number of bases functions all you know million practical terms with high basis functions work but the new risk overfitting. Gaussian is, there's a reason why Gaussian has become the most go-to thing. Trouble with Gaussian is it runs much slower than other ones because it's complex. So SVM, support vector machines and support vector regressors and all these kernel methods they tend to run a bit slow you need good hardware to run it and part of it is because you have to do so if you can look at a problem and you can say that this is a linear problem I can do it with you can do it with point or be you could reach faster another thing with support vectors is that don't take the entire data set, training data set or something. The computation will run for a very, very long time. Take a very small sample of the data set and try to run it because SVMs get horrendously complex like the computations when you have too many data points. my too many data points so okay so what is what are support vectors for a data set with D domain D features the support vectors are they're just some specific points specific rows of data that act act as support vectors those are the points that are either sitting on the maximal margin you know on the high the margins or between the margins now the error points those are your support and this is just a mapping the definitions i hope most of you got this right. SVM is primarily used for unsupervised learning. Why did I say it's false. It is supervised It is mostly used for classification regression, there is a twist. You can use it for for clustering also But it's a stretch not that commonly used a support victim machine is so if your question was support vector machine is used for unsupervised then the answer would have been true for supervised you know not instead of primarily what was not there if it just says SVM is used for unsupervised learning then it is true can be used for yeah then the answer can be used for whatever can be okay a support victim machine is what is it well these three are correct so this was a tricky one because out of the five three are correct a classifier that uses the kernel trick to discover the support vectors without explicitly finding the mapping question so I read here this is the main point with support vector machines you don't have to explicitly find the phi the mapping function the other is it's an algorithm that conceptually maps the data to a different space where the instances of the two classes become linearly separated to a generalization of the soft margin classifier. Remember the soft margin classifier. We talked about it using the analogy of the river, the widest river we can flow. In a soft margin, we allow for some mistakes, some houseboats and some trees in the water so the trouble is it only works for linear decision boundaries I mean linearly separated separable classes but then if you have data that's not linearly separable but then you have to project this data into a higher dimensional space using a mapping function where it will be but in that space now you can apply the kernel trick and solve the problem uh yeah as if just a uh like on this uh on your river example uh like one side houses and other side, the trees where it floods. Actually, it is flooded right now. Ah, right. Yeah. Because of the, like the yearly floods. Right. We should take a picture. That's cool. Gunga is overflowing at this moment. Yeah. Okay. These are about okay so if there there is some noise in the data sample a soft margin classifier is likely to outperform the hard margin classifier if the data is linearly separable with respect to classes why is this true guys well because of the budget exactly because you have you have somebody budget in the constraint optimization problem that's right excellent so then this is K and in this one if the data looks like this guys which would be the best kernel to pick polynomial or Gaussian both will solve the problem linear won't because you can draw a straight line and decision boundary here so either to in fact all you know you'll cook urinal might converge faster so these are old problems I hope it was fun.