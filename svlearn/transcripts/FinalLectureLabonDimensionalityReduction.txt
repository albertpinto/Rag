 alright guys recording is an effect then then go to the writing tablet on the other machine so so what is clustering clustering is a form of unsupervised learning. In other words, it is a form of pattern recognition. You're not predicting something. Prediction, if it happens, is a side effect. It's not the main purpose of clustering algorithms. What clustering algorithms look for are clusters in the data. Usually you do have clusters in the data. They may be good quality clusters, or they may be not so well-defined clusters. But generally data does not come as completely homogeneous. It's rarely like that. Most of the time you do see patterns and clusters in the data. So the two kinds of patterns that we are covering in ML 100 workshop are dimensionality reduction and clustering. So clustering was looking for clusters in the data. We covered k-means clustering. K-means clustering is the simplest clustering. It's a partitioning algorithm. Namely each point belongs to some cluster. It does not support the notion of outliers. So every point belongs to this cluster or some cluster. It does not support the notion of outliers. So every point belongs to this cluster or that cluster. Now, K means answers half the clustering question. It says, if you tell me how many clusters to find, I will find exactly those many clusters in the data. On the other hand, if you don't know how many clusters there are, well, tough luck. What you have to do is pick different values of k, try to cluster by that, and then look at a quantitative measure, something like a goodness of cluster measure, like within clusters some square distance, or plot it out, you'll get a scree plot, and then look for an ELBO in the scree plot. That could be it. When you find an EL elbow, you're lucky. You know that this is a good number of clusters. But if you don't find it, there may be multiple reasons. One, the most obvious reason that people assume is that there are no good clusters in the data. But actually, that's not true, as we did in the lab and found. You may have very good clusters, but those clusters may not be globular they may not be roundish right or convex clusters when they are not like that convex like clusters then you tend to uh see that k-means clustering does poorly that was the lab that we did so that's the limitation of k-means clustering. Obviously it doesn't deal with outliers very well. And not to mention the fact that k-means actually in high dimension can become computationally quite expensive. So with all of those things, there are many, many ways in which you accelerate k-means clustering, or you make sure that it doesn't get stuck in unstable clustering or you make sure that it doesn't get stuck in unstable clustering situations what you do is basic optimizations like if you have to do k is equal to two clustering you'll take a random point and then you'll take the next point to be as far away from it as possible and then do the testing and even then if you suspect you might have been stuck with unstable solutions, you do the clustering again and again for the same case equal to 2 and find which clustering gave you the lowest WSS metric and pick that clustering. So it's a little bit of an intense, the simplicity of the algorithm comes at a price because of its instability. You need to try it again and again and see which one gives you the best results. So in other words, there's something non-deterministic built into that. And that is that. It's very easy to understand. People often ask, why is clustering not as simple thing as sorting and so forth? And the answer to that, of course course as we talked about is clustering is not like sorting we all have a unique definition of sorting but with clustering the clusters within clusters you know sub-clusters and so forth so the granularity at which you are looking at clusters matters which is what makes clustering a machine learning problem rather than a basic algorithmic problem in elementary algorithms that you learn in college. So that is clustering. This is the screen plot that you build in order to come up with a cluster, the screen plot. then it does as I said rather poorly when you have sort of a convex concave like shape or any shape that is not convex when you do have you may not get a elbow or right so that is something to be aware of another classic algorithm is the agglomerative clustering and the agglomerative clustering is there has the benefit that you can pick and choose how many clusters you want to see. Because in one shot, it finds from K is equal to 1 to K is equal to the number of data instances, as many clusters. So all of those clusters are done, and they are represented as a dendrogram. And so you have the benefit that once you visualize the dendrogram and so you have the benefit that once you visualize the dendrogram you can pick and choose where your cutoff points are you want to have a cutoff at two three four the model is built and you can you don't have to rebuild the model for different values of K you just have to slice the dendrogram and different values of k. In this there is a linkage function, the question is the notion of distance between two clusters. How do you define the distance between two clusters? Is it the distance between two nearest points? Is it the distance between the center of the clusters? Is it the distance between the furthest points? Is it the average pairwise distance between the, you know, by taking a point in one cluster and a point in another cluster and averaging it over? So there's so many definitions that you can use to define what is the distance between two clusters. Those are called the linkage functions because that's needed if you want to support the notion of distance. What is a distance? We talked about distances actually. We always think of Euclidean distance as the bird flies or as we walk from house to house. But then there are many distances. If you remember, we talked about the Manhattan distance and Euclidean is another one. And the generalization has been Kovsky distance. But it turns out that distance is anything that has these following properties. Distance is a function of two vectors let's say y is x and y says that the only way that a distance between the two can be zero is if the two points are co-located sitting on top of each other. Also distance is symmetric in other, distance from X to Y is the same as from Y to X. And distances are always positive. In other words, zero or more. The other thing is it follows the so-called Schwarz inequality or the triangular inequality, which means that the shortest distance, the direct distance from X to Y will always be less than the distance between X to any other point and from that point to Y. So this is a triangular inequality and often called it follows from the Cauchy-Schwarz conditions about distance or the notion of distance. In the literature people often call geodesics, they use the word geodesic for the shortest distance now the geodesics are not necessarily straight distances for example on the surface of the earth when you go from here to India you don't just die you don't just burrow into the earth and come out on the other side of the earth you instead follow the great arc you know you follow the surface of the earth they are playing and then it goes to the other side for example, you follow the surface of the earth, the airplane, and then it goes to the other side. For example, if you take the polar route, you go over the North Pole and then you end up showing up on the other side. And it's a big arc, it's a great arc. So those paths are truly the shortest path given the fact that you can't go through the earth. The word for them is geodesic. The generalized Minkowski distance we talked about and then I also talked to you about much more general things like you know the notion of metric tensors and so forth when there's a deformation of the surface, complicated surfaces. We talked about density based clustering. What does density-based clustering do? It is based on the intuition that clusters are regions of high density. If you consider each point to have mass, then clusters are regions of high density. And if you think that each point exerts a certain field or influence field or creates an influence field, then you can field or creates an influence field then you can find at any point in the feature space you can find the net influence there one intuition that i gave you for a density risk question is think of these points as little bits of light like stars or little lamps and so at any point in the feature space you're getting the light from all these data points or all these little lamps and so you get a total luminosity which varies from point to point the way density based clustering works the the state-of-the-art methods like this dental works is that you ask this question can I find my way to the brightest most well-lit points those brightest points are called the attractors and you do using a method that is very almost identical to gradient descent here it is gradient ascent you follow the direction of maximum increase you don't go against it because you're trying to find the maximum so that will give you that the other method which is a little bit doesn't scale but also quite effective is the db scan db scan is well supported in python libraries dental is a little hard to get it uses the notion of neighborhoods epsilon neighborhoods so two points are directly reachable if they are within an epsilon neighborhood of each other. A point is an interior point if it has sufficiently many neighbors within its epsilon neighborhood. So what you do is you take an interior point, you discover an interior point, and then you fan out from that and find all points directly or indirectly reachable from the interior point. And the set of all such points make a cluster. Then you go about hunting for another interior point not yet acquired, and then you start finding out from that. And this way you can discover arbitrary many clusters. The beauty of density-based clustering methods is that you have a very natural separation between points that belong to a cluster and points that are outliers. And that's a significant benefit of density-based methods. So that is essentially a summary of what we have. I won't go more into the density-based. In the DENQ, of course, we talked about we have a net influence field. and given the influence field, all we have to do is the hill climbing method. This is this particular, the core of it is this. And I wish we could, let me see if I can color it somehow so that we remember it. Let me see. this is it. This crucial algorithm is the hill climbing algorithm. And the points we find are the attractors, the maxima. How do you know of what is the maxima? At that? The gradient will disappear. And then, like if you think of a single variable, univariate, then what happens at the maxima? If you go back and recall your calculus, at the point, at the top point, the derivative disappears, but then derivative disappears at the minimum also, and it disappears at the saddle point also. So how do you know it's a maxima? The way you know it is that the second derivative is negative. So the derivative has to vanish and the second derivative has to be negative of a function. This is a recap from calculus. Its generalization to higher dimensions is the gradient has to vanish and the Hessian has to be negative. So that's sort of how you look at it. So that's about my streak. As a recap, any questions guys before I go to dimensionality reduction? You can go to dimensionality reduction. Dimensionality reduction says that, let us say that the input feature space you are given, is it at all possible that data is actually not uniformly spread and occupying the entire feature space, but it is actually occupying a sub region, a very limited sub region of the feature space so that you can find a hidden surface or hyper surface the technical would be a manifold so that most of the data is sort of either clinging to it or close to it it's very near that hyper surface or manifold sub manifold so that creates the notion of dimensionality reduction implicit to dimensionality reduction. Implicit to dimensionality reduction usually is that whenever you do that you implicitly assume that you can succeed at it. A priori you don't know, you don't know the ground truth. So should you try dimensionality reduction? How would you know when to try dimensionality reduction? The answer usually is you can never tell a priori but you should always try dimensionality reduction? The answer usually is you can never tell a priori but you should always try dimensionality reduction because if you can reduce the dimensionality of the problem you reduce the complexity of the problem by a large extent. So there are many many techniques for dimensionality reduction. The one technique that we've learned about in detail and it pretty much captures the spirit of other techniques. I do some more complicated versions, but sort of captures it as principal components analysis. So we looked at this data and we noticed that most of the data is hugging a line here. So that is sort of the sub manifold in this two-dimension space. So how do we discover that? The way you do that is we bring in this concept of eigenvalue decomposition. So suppose you have, we notice the fact that if you take uniform noise and if you take uniform noise and we do, we apply a covariance matrix to it, it will take on a shape. So we'll talk a little bit about the whole theory of it. Remember, principal component analysis works only when there's a strong correlation of data. Correlation implies a linear relationship. For example, if in the sine wave, the correlation is zero, the relationship is there, but the relationship is nonlinear. Principle component analysis does not work for nonlinear relationships. It only works for linear relationships. So it's a very simple model, a simple means of dimensionality reduction. To understand it, we go back and recall what covariance matrices were. What is the covariance between two features? And I've just put it here. This is it. Obviously, there's a, I noticed that I have missed here. I, summation over I, one over n, all the averaging and all of that I have mislost in the notation, I think while explaining, but yeah looking at that here. So what it does is when you create co-varies between two things means there is a relationship between one and the other. If one increases, the other increases or contra-increases or contra-varies, increases or decreases one way or the other. In one case, here, covariance positive means they increase together. Covariance negative means one increases, the other decreases. So now with the covariance matrix, we took a detour into something, a little bit of linear algebra and the geometry of linear algebra. But we said, suppose you have a point P and you rotate it with an angle theta, that rotation can actually be written. These are the basic trigonometric relationships of rotation. And this rotation follows from the expansion of cosine theta plus phi, sine theta plus phi. Actually, maybe I should mention what the Phi was in this equation so and from that we can actually work out but this is this matrix is the rotation matrix so which I'm going fast if you guys want me to slow down and we explain the keynote but we have a lot of energy to cover today so I'll keep it fast and this stop so this is the rotation matrix basically besides rotation matrix you can have scaling matrix you can take a vector and scale it out make it bigger that you do with the scaling matrix it turns out that the rotation matrix and scaling matrix are sort of the basic building blocks out of which into which you can decompose any matrix out of which into which you can decompose any matrix any matrix will take a point and take it somewhere let's say p goes to p prime you can think of it as a rotation and a scaling and that's quite interesting that you can think of any transformation as a certain amount of rotation and a certain amount of scaling and so you can see that the operation is once you break it up into two parts you can consider that any matrix is therefore made up of any transformation is made up of something like this now there's a very interesting fact you take any matrix and you take all the points on a unit circle what it will do is it will expand it out into an ellipse on a unit circle, what it will do is it will expand it out into an ellipse. You see this ellipse here. Now you say that, well, why is that so? It will take you a little bit of time to realize that it will do that. Once you have convinced yourself geometrically that it will make a unit circle into an ellipse, you ask, okay, so what's so special about it? An ellipse has four points that only scale up you know they expand or shrink but all other points they rotate also so those points are unique those directions if you take the unit vectors of these two directions and of course they opposite to other directions but let's just take two directions you one you two those two unit vectors uh represent something interesting they're called eigenvectors because this matrix only scales them up in other words if you have u1 the matrix m will just scale it by a factor lambda 1 and matrix u2 is scaled by factor lambda 2. lambda 1 and lambda 2 are called the eigenvalues of the matrix the directions u1 and u2 are called the eigenvalues of the matrix. The directions u1 and u2 are called the eigenvectors. And it's a very beautiful fact that this is true, that you can decompose it. And these are called matrix decompositions. There are other decompositions, singular value decompositions and whatnot. Many, many Kolesky decompositions, many, many other decompositions with all of them with interesting interpretations But eigenvalue and singular value are the sort of the crown jewels of linear algebra A lot of the value that linear algebra brings to machine learning Starts with these two decompositions eigenvalue Eigenvalue is something we learned about today. We will learn about singular value is generalization of eigenvalue and singular value eigenvalue is something we learned about today we'll learn about singular value is generalization of eigenvalue to rectangular matrices so one very rough intuition is that if you take a rectangular matrix and multiply it with itself with its transpose you'll end up with a square matrix once you have a square matrix now you can do eigenvalue decomposition so the eigenvalue of this of this you know the square of the rectangular matrix is the same as doing singular value in the original matrix that's sort of a very rough intuition what is all of that to do with our covariance it turns out that you can create a matrix of covariance the principal diagonal diagonals are of course the variance says themselves of each of the features and the off diagonal is made up of covariance note that this matrix is positive like you know it is a symmetric it is symmetric and it has one quality i think I should mention that but it is positive definite if it is positive definite what did I just do that's good definitely which basically roughly translated that for any vector this will be greater than zero but let's not get too involved with those definitions basically it means is that if it is a well-behaved matrix then that matrix of covariance is something that captures the random noise and it will make it into your data so that your data's covariance matrix is what contains the information of the relationship of the data if you can take any random noise and apply that covariance matrix to that random noise and it will suddenly begin to look like your data in linear situations so therefore the this matrix has is essentially in many ways you can say is the essence of your data now you say well that is all lovely what can it do the the the beauty comes when you realize that you can do an eigenvalue decomposition and when you do it you get lambda one lambda two the scaling across two dimensions now one of the factors may be much bigger than the other you can set the other factor to zero because you can set the other factor to zero you have reduced the dimensionality of the problem you have essentially ignored some dimension which is irrelevant for example if you look at this picture you can see u1 is quite relevant but u2 can be ignored it can be ignored because the scaling factor is a very small factor and what we are seeing is that if you just measure the coordinates or location of a particle of a point along the u1 axis it captures most of the information you don't need to worry about you too and so in such situations this you have reduced the dimensionality of the problem so that is principal component analysis guys that is all there is to it and so what you can do is you can represent the original variables in terms of new axes lesser dimensional axis you have projected the data into lower dimension space one of the most common things to do is when projected the data into lower dimensions first one of the most common things to do is when you get data in high dimensions just as a visualization technique uh projected down to two dimensions when you project it down to two dimensions you can at least see the data for the longest time before more complicated methods came about it was a pretty standard technique even today it's a your first line of action should be you take any data and you predict it down to two dimensions using principal component analysis pick the first from the first unit vector the eigenvector the second eigenvector called the two principal components and visualize your data we'll do that in lab right now when you visualize the data you get some intuition of what you're looking at and it means a lot. That will help you get some intuition in the data right away. Obviously now there are more powerful techniques for example PSNE, locally linear embedding and UMAP and so forth. Today I'll give you a taste of these things we are going to cover these things in your um ml 200 ml 200 gets uh and then obviously at the end of this i'll tell you what ml 200 contains but just giving you a taste of what is going to come are we together guys so this is a review of what we did the last time. Now, component analysis, remember, is based on the fact that there's a linear relationship or a correlation between features. If the correlations are not there. It doesn't work. Now, just just to remind you, remember, for linear regression, if there is strong correlation between features, what happens? Is it a good thing or a bad thing it's a bad thing it leads to multicollinearity what it becomes highly unstable on the other hand principal component analysis is the technique which loves correlations because it sort of reduces dimensionality using that to a lower dimensional space and so it is a common practice actually it is useful that when you see strong correlations in data first reduce the dimensionality of the problem so that all the correlations disappear you're in a much lower dimensional space in which all the axes are truly independent the new features the synthetic features are truly independent of each other because then new features, the synthetic features are truly independent of each other because then your classifier and your regressor will work very well. So the idea is you reduce dimensionality and then apply your classifier regressor, even clustering if you want. And I want to illustrate this with examples now. So with that review of that, let me move over to my Linux machine I'm going to share you see Share. Let me see. Let me know guys if you're able to see my screen. We can see it though it's the kind of just the center yeah the the text is only in the center unfortunately let me do something different perhaps one second there's the sharing gone I'll just share the browser that might be more effective yes are you guys able to see it now better yes okay is the font size coming across big enough? It's good size. Yes. It's a good size. It's missing the top. It's missing the top. We just see the component. Yeah, now we can see it. Okay. So there is a particular data set that I'll use to describe it now I was going to write this note, but I realized that there's a lovely article on towards data science And that is what we do by the way guys. I must also recommend obviously I Have no relationship with this particular website. I myself am one of the subscribers. But Medium, the website Medium, has a lot of very good articles, but in particular, towards data science, this particular thing, if you notice, they come up with, every day you find new articles there. And well, you know, some people wake up and read the New York Times. Some people watch CNN. For me, my daily practice is usually when I wake up and have my cup of coffee, I come to this website. I visit only two things. General technology news I get from Slashdot. Some of you may know what slash dot is it's a tech uh techy kind of a website a geeky website and then um the other is uh towards data science both of them will give you things that for people of our tribe is very relevant i would highly encourage you to become uh familiar or just you know develop a practice to visiting either this website or anything that is similar to this but nowadays there's a lot of other good websites also but my first reaction is i come to this i also check out the many competing websites which come out with news in this the other i mean since this is last lecture, maybe I can give you guys a sense of what all things I go to. One of them is archive sanity. I don't know if I ever told you guys about archive sanity. So a bit of a digression, but I thought I'd tell you. So how do I catch up when I go to towards data science? When I come here, where am I? towards data science, I get the news, I get very digestible articles. Somebody has taken and explained something in great detail. You start by consuming a lot of these articles and then gradually feel inspired to contribute something back. I think there is a subscription, I don't know what it is, two dollars or three dollars a month. Five dollars per month. Five dollars a month, yeah. So so if you guys can spare that i think that's a pretty good utilization of the five dollars now here's the best part if you write articles for this website you get you i think you get your money back because when other people like your articles you get paid for it so it's a intern it's sort of a community uh project community site run by these guys in which the more you contribute you essentially don't pay anything in fact you get money back a good place the articles are really and the so this is for learning you know they're very these articles try to teach you some new topic. The quality of the article can be quite variable. Some of them are elementary and you glance at it and you know it or reading it is a waste of time. Sometimes the quality may not be very good. But broadly speaking, the quality is good. And some articles are very good. Then if you go to archive sanity it's slightly different this is where i go every day to see what is the latest research actually i don't go here every day i take that back i go here once a week usually over the weekend as a basic practice every week i try to read two to three uh research articles end to end now reading anything end to end takes a lot of time so what will happen is i'll go here i'll pick uh a five six of the articles so what these people do is they take the best of the research that has been published recently right so you can see the top recent, the classification based anomaly detection for general data. And it is just by the look of it, you can see that this must be a pretty good article. Let us go and follow it through. When you go here, there are only three citations, deep nearest neighbor detection, etc. But for something that was just published five days ago, the fact that there's already you go here there are only three citations deep deep nearest neighbor detection etc but for something that was just published five days ago the fact that there's already three citations for it is pretty good and it talks about see as you can see let's read the summary of it anomaly detection finding patterns that substantially deviate from those seen previously is one of the fundamental problems of AI. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unified view and propose an open set method, GOLD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations, our method is shown to obtain the state-of-the-art accuracy and is applicable to broad data types. The strong performance of this method is extensively validated on multiple data sets from different domains. So now if you're in fraud detection, anomaly detection, etc., this immediately, you know, if nothing else, what would be your first reaction? Your first reaction would be you would go. So there are two things I do. First is go look at the article. But even before I look at the article. What I usually do is I look at the references of this. So when I look at the references of this, and I find a reference, let's say this one, using auto encoders and nonlinear direction, let me go with this, this is a pretty recent, what can I do, I can look for citations, it has been cited in 38 other papers. So obviously, this is linked to some research which is good and geometric framework so this is this is the way i would go about looking at things this would be another article i would go and see 247 citations what does it mean you just discovered a gem of an article somebody has done a lot of this is really a very good article right it has material supplemental slides and so forth so now the whole question is okay you get a sense that this work is in the context of some very good work what can i do now what we can do is actually read the article just go here and i'll give you a tip guys how to read this article. It's a bit of a digression. What you do is you just read the abstract, something that we just did. After that you ask yourself is it something relevant to me or not? Suppose you're not doing anomaly detection, you couldn't care less. You just move past this. You say okay done. Somebody has done good work, I'll remember that but I. You just move past this. You say, okay, done. Somebody has done good work. I'll remember that, but I'm not going to pursue this. But suppose you do want to, the first time around, don't read anything more than introduction. Read the introduction, right? And then just stop there and ask yourself at this moment, did you get the gist of what they're doing? Do you really want to continue? And then after introduction, go to the bottom. And in this they are, they're not only giving a survey, but they're also explaining a method of their own. So they have their own method and they're claiming that their method is better. Decide whether you want to delve into their method or not. Is it of value to you? If it is not of value to you, it's good enough. You gained enough from the introduction and you can move past. That is it. You stop at section one, introduction. And now you can go ahead and read sort of other things. You can say, I'm done with this. I learned something from it but that is it right and then this paper by the way is a very interesting paper supervised contrasting learning you come across and a paper like this and then your reaction is when you go and look at it, you feel you look at this. You again look at the citations at this moment, only one citation. But you look at this and you feel. Is it something that I would like to learn about? And. Once again, you go to the paper. You start reading this paper and the first thing that strikes you is if you're in this field, you will realize that, oh, like, for example, my reaction was that, yes, this is directly relevant to me. I would like to pursue this paper more. So once again, you do the same thing. You read the first section, maybe the second section in this case, you determine whether you want to pursue this or not. You determine whether you want to pursue this or not. And should you decide that you do want to pursue this, you ask yourself, can I just glance at the equation and tell what is happening? So, for example, can I look at this and ask, how is it different from the loss functions that I'm used to well this gets a little bit mathsy when you do engineering math with me math or data science with me you learn to recognize it immediately that how is this different from what we have been doing and that is a crucial thing to first right away identify why should I read this paper what makes it so special what is the main contribution of this paper once you do that then uh with sufficient mathematical background you can easily start breaking it up into pieces like positive cases negative cases uh pretty much the same thing you do for a classifier's loss functioning then this is talking about like you know this may remind you a little bit about This may remind you a little bit about, for example, softmax or somewhat like that. But then you start looking at it and you ask, what is the advantage? How much better does it do? Clearly, if you're using the simple loss function that we did for classifiers, if you remember that the maximum likelihood estimation based loss function for classifiers. It is saying that if you use that for maximum likelihood estimation based loss function for classifiers. So it is saying that if you use that for all of these architectures, these are neural architectures, you get this level of accuracy. But if you use this method, supervised contrastive, the gains are astounding. Do you realize that ResNet 50, all of a sudden, it gives you from 70, well, okay, it gives you a little bit better, and top five has really climbed up to 93 and 95%, which is good in the sense that if you look at ResNet, well, not always good, but okay, so a little bit good here, you look at this. In general, what do you get? Do you think that it's an earth-shaking improvement? May or may not be, but it is certainly an improvement. It's certainly worth learning about. And so you learn about it, you go read it. In fact, I found it to be interesting. So this is, anyway, this was a bit of a digression, but I thought I'll introduce you to how I stay abreast of the field. How do I stay with it? Keep in touch with it. Am I able to keep in touch with it? No. Almost every day somebody points out things to me that are very crucial developments that I entirely miss. Sometimes I miss them by two, three years and they happened while I was busy doing other things. So we are always in a catch up game in this field, but we all have to have a framework to stay in touch. So well, anyway, so much for that. So I'll come back to towards data science. Let me give you a little bit of a background and a breast cancer data. Those of you who did the bootcamp with me, of course are familiar with this breast cancer data. We'll do the breast cancer data in great length in ML 200. The history of this data is this. As you know, breast cancer is not at all a visitor that you want in your house, ever. It's a pretty nasty disease and women are quite scared of it, very very scared of it. So it can strike anybody just about. So people get tested, they get mammograms and so forth taken, I believe annually or something like that. Now, suppose the physician finds the diagnostician, finds some hint or suspicion that there may be a lump or something like that. Then you get more invasive. The second line comes in, which is that the lumps can be pretty harmless, but it can be serious. If there's any anomaly detected in the x-ray or in the mammogram, then the next line of defense is you would go and do a core biopsy. You would go in to that lump, take tissue out, look under the microscope and so forth. Well, that remains a gold standard, but sometimes people ask, that leaves a scar. Can you do a little bit non-invasive version of that? So in the late 1980s, early 90s, the thinking was that, can we do something called fine needle aspiration? We go with a long needle, thin needle to the lump, just aspirate out a little bit of the tissue, look under the microscope and tell whether there is cancer or not, or it is a benign tumor or malignant tumor. Then when you do that, the trouble was that the human accuracy was quite imperfect with fine needle aspiration which is why even today diagnostic since in US for example with suspicion you go directly with core biopsy but in a lot of countries where core biopsy is expensive people usually in India for example the first line of action if there's a suspicion of tumor is to go with fine needle aspiration non-invasive fairly economical you aspirate out a little bit of a tissue look under the microscope if you see you still suspect something is wrong then you sign for further testing but the trouble was that even people who were experts at it, their accuracy wasn't very good. One of the questions that was raised is, and it was very time consuming and labor intensive. So the question raised was, can we do an automated machine learning based detection or classification between benign and malignant tumor by looking at the image of the fine needle, aspirated tissue. And so a bunch of researchers, they spent a lot of effort onto it, but what they did is actually one of the great, really beautiful exercise in feature extraction. They look at the diameter of the cell, they look at the tissue cells, if they look concave, if the surface look regular or irregular, what was the size of the cell, they look at the the cells of the tissue cells, if they look concave, if the surface look regular or irregular, what was the size of the cell in the tissue, and so many, many markers, they looked at some 23 markers. And based on those markers, they tried to build a machine learning model. When they built a machine learning model, it was quite a success actually. It gave 90 plus percentage accuracy. And the recall was pretty good. So people began to use it. I mean, the fine needle aspiration now, especially in a lot of countries outside the US, is pretty well established as a quick diagnostic measure. But using machine learning, of course, you get much higher accuracy. The use of machine learning itself, it turned out that was not very prevalent because then the equipment becomes very expensive that can do it. Certainly, it's a good machine learning exercise that we are going to do. So machine learning is still permeating the world of medicine slowly. And then, as I said, the next 10 years will go into applying AI to the medical domain, almost all of you, as you become machine learning experts, my feeling is that in the next 10 years, you will sooner or later work for a company that is doing medical AI one way or the other, you'll end up doing that that's the new frontier just as the 20 the 2010 to 2020 when to such a large extent to image processing you know self-driven cars was an ambition a lot of classification and text recognition i mean image recognition and image segmentation and those will continue but i feel that medical ai will become a big topic so we have this data set i would highly recommend you to follow each of the steps from python you take the data you look at it when you look at the data set you will realize that there are 357 cases out of this many where you do have cancer if people do have cancer. If you visualize the data, this is just making the pair plots, guys. By the way, I wouldn't recommend the code that he has used. I would recommend the code that I gave you in the tabular data notes because that code will immediately draw essentially the same histograms and pair plots for you. Perhaps more elegant one. so you use that and then you realize that when you look at the data this the features are highly correlated let's look at this which are worse symmetry and worse texture so find tissues with the worst texture and the worst symmetry what do you notice is there a correlation between these two features guys is there a linear is there are they positively correlated they seem to be right the bigger the symmetry the bigger the texture the worst texture likewise mean radius and mean concave points also are correlated you know you can see a linear the positive correlation between these two things so you see these when you see correlations you realize that pca would be quite good actually in these situations because it will remove all the correlations and take you to a lower dimension space by the way yeah this has about 30 features i said 23 25 now it's 20 30 features so you take these features now what you need to do with pca and something you need to be sort of careful or aware of is pca works with correlation correlation remember standardizes the value x minus mu over sigma if you remember the theory we did so in python it's a one line you just take a scalar and you can scale the data you can scale it to a standard scale and then the pca part is unbelievably simple like i told you clustering and pca unsupervised learning from a coding perspective is extraordinarily simple extraordinarily simple. Two lines and it does it. And when you do that, you look at something called, if I go to lower dimension, so suppose I say, I'm going to just pick two dimensions. Then what proportion of the actual information that is there in the original dataset, can I explain with this reduced dimension of two it's called proportion of variance explained yogi so it's a technical term and the book is clearly explained but that's what it is think of it as how much of the information did you capture and so to capture that you you look at it and you realize that the numbers are like this, 60, 26, 12. So the first dimension captures 60 percent of the information. Second captures 26. So what is the addition of these two? 60 plus 26 is 86. 86 plus 12 is, or 13, is 99, isn't it? And so what it means is that the first two dimensions captures close to 87, 86%. And the third dimension captures most of the information, 97% or so. Which means that in this case, dimensionality reduction would work very very well and you can see that just in two dimensions do you see that the data point when you project it down to two dimensions first principal component second principal component what do you see guys here in this picture if you look at it um do you notice that the benign the green cases and the red cases are well separated out do you see that there's it's remarkably well separated out race can you hear me yes it's very well separated looks lovely lovely yeah and it's it's really good and if you do it in three dimensions it gets even better so with this data guys if you were to use any classifier it's a logistic classifier or linear discriminant analysis or whatever you realize that you will have a near success immediately you'll have a very high degree of success isn't it and that is the beauty of bca you you get a more very interpretable and you get a simple interpretable model and you get a good classifier that is the beauty of beauty analysis analysis as if what three components did he pick? No, no, the machine. The machine. The machine. So the machine just So the actual algorithm was the one that for the correlated components. What the algorithm did is that it found these hidden dimensions. Remember, these are not the original features. It is not the first principal component and the second principal component or any one of those 30 features. But it is a mishmash of those features. Remember, it is a synthetic feature created by bits of the original features, little bits of the original features. And because it created the first and second. So I said, so okay. Go ahead. So it's like when you say it got components of one feature and the other feature, it's like getting the BMI of a patient based off of height and weight, something like that. It figures out the value of BMI, something like that. Yeah, the intuition is… Body mass, body mass index. Intuition is that. A BMI we take as a number, but it captures two things, you know, it captures the height, weight, and so on and so forth so it's just like that bmi is a good example so these principal components are like bmi kind of thinking they're a mishmash of other features all the original features when you do that now typically what happens is when people do it uh especially in the medical domains and happens is when people do it, especially in the medical domains and in the biological and a lot of it in the economic domains, people get used to these components and they say what is the intuition behind it and sooner or later they give an interpretation, they give a physical intuition like BMI has become an intuition but that is very like you know you look at a person and you can sort of guess the bmi these days right so it becomes something uh a word of its own in the literature and people start developing intuition about it so that is what this thing is about and look at how remarkably successful it is for that and you can can do that. Like if you can ask yourself that originally there was so much correlation in the data. Now, once you, you take the three principal components and what is the correlations now, you realize that very little correlations, they are all close to zero, minus 0.3 to 0.3 which is small correlations and all the correlations stay strong whereas in the original data the correlations were very very strong you see that correlations are one and so on and so forth so uh that is it now you can say well this is half the problem where is the classification part can we see the classification part also so that is a separate post actually the same author has written let me go to that yeah and it's beautiful visual sorry apologies I think we are jumping around somewhere yeah beautiful visualizations yeah it's you notice this pattern in the towards data science articles sometimes You they'll all try to start with some grand picture banner picture and which will be tangentially related Here is a bridge and he's calling it a decision boundary well Sure, it's a decision boundary, but for what it is work. It's there I'm not so sure it's a decision boundary, but for what it is worth, it's there. So just to make the page more colorful, I suppose. So this one is the same thing. You take the code, you do the principal component analysis, repeats it here. Once you do the principal component analysis, you look at pairwise many features okay after you do the pca you realize that you can cut the data pretty well using this it's the same thing right and data is very well separated what can you do then you can then well there's a little bit more you can apply a classifier called support vector machines i would say and this just illustrates the point support victim machines is something you'll do with ml 200 by the way you could have just used logistic regression it works just almost as well and it is something you are familiar with so when you do that uh it will come up with a classification accuracy which is very good, 94% accuracy. 94% is not something to be, not impressive. It's pretty impressive. We've got a very good impressive. And if you look in terms of plotting the support vectors decision boundary, you see how clearly it is. But do you see that, guys, that you can directly draw a line between the red points the malignant the malignant and the benign points here here the green is malignant and the red is do you see this guys it's a very simple idea there's nothing very complicated to it and if you look at the entire code it's very short here you You scale the data, you pass it to SVM, to PCA, to reduce the dimensionality, and then you just give it to a SVM kernel, and then you use it to classify. The rest of the code, all this is just visualization code to create pretty visualizations. And it creates a nice 3D animation. You can see that if you use three principal components, then data has even better separation between the... I don't know if this visualization conveys it, but if you look at it carefully, you'll realize that just three dimensions were enough to separate the data out. Now this is not a toy example. This is a real dataset that was one of the landmarks in machine learning history. We won't go into SVM here. Just imagine you're using logistic regression and this thing would work. So guys, what I'll do is I'll post these two articles into your Slack. Usually, you're allowed a certain number of articles free per person, if you don't have membership to this. If you can't access it, let me know. I think it may be okay for me to post a PDF of it. I'm not sure. Let me post these two. Oh, Asif, actually you have 10 full views a month for uh or towards data science but but after that yeah after that when you get when you get when you get the when you go on your 11th article it gives you a preview but if you copy the if you copy the url of that and paste it in an incognito browser it gives you the whole article so you can just you can just view the links in incognito and yeah you don't have to pay my daughter to my house right though as a person who spends a lot of time writing articles and um notes i would say that pay that five dollars to this very poor authors you know they put in a lot of effort uh in doing that see guys we are engineers but we are paid what three four hundred dollars an hour and it typically takes to write an article easily, 10 hours of effort or 12 hours of effort. It's thousands and thousands of dollars of value, economic value. And you just give it to these websites when people read it. If they all just contribute like per article, you know, there'll be like 10 cents, a thousand people read it. It's still just a hundred dollars. So writing books, textbooks and technical articles, remember, is not really for the monetization aspect of it. You do it for because you love the subject, you want to show it, explain it, do things like that, I hope. And sometimes people do it for name and fame. So but in any case, it's a good way to say thank you. After all, it's just five dollars or whatever, $2 in some websites or $1, something like that. All right guys, so what I'll do is, these are two articles, I put it on your, towards data science. Do try to work through the lab. The first one, which is just the PC, do the lab. It will just take you half an hour to do the lab in that. The second one is for you to know for the future. What I'm going to do is, should I take a five minutes break? Or what I would like to do now is give you a preview of what is coming in ML 200. Would you guys like that? Have I given you an idea of what's coming in MA 200 already? You can tell us again. I mean, yeah, I could tell you guys again. Let me go and see. It might be in the website itself. Oh, goodness. I need to move my website from Norway to US. It's too slow there let's look at workshops okay intermediate data science. Yeah. So these are the topics we'll do. We'll do the no free lunch theorem. It's something very important, but most textbooks miss it. Then we'll do support vector machines as you can see. Why is this like this? Minus, okay, I do not know why it is behaving like this. Let me put it in the incognito mode. Are you guys seeing my screen? What are you seeing? Anything at all? I'm seeing Capstone ML 200, no free lunch theorem, support vectors machines. Okay, you are seeing that. Okay, so yeah. support vectors machines okay you are so so in it we do gradient boosting methods random forest and extremely randomized trees decision trees ensemble methods bagging boosting stacking a kernel k-nearest neighbors, we'll do the art of feature engineering. So these were the nine topics that are there, nine or more than that, one, two, three, four, five, six, seven, eight, nine, yeah, nine topics. But actually I added a few more. A couple of more topics that we will add are um bit of see regularization is not here we'll do regularization it's an important topic and the other thing is recommender systems is not here we'll do recommender systems also so there'll be 11 topics and they are about uh 10 sessions so we'll move a little bit fast we'll cover a lot of territory per session obviously for each session I'll try to give an extra session also so that that compensates for that any questions guys Any questions, guys? Not for me. Do we have our sentiment analysis? What's that? Sentiment analysis. Very, very easily. Why don't, all right, we'll add it. If you guys want sentiment analysis, right we'll add it if you guys want sentiment analysis surely we can add it see it's a pretty large scope you'll realize that the the intermediate machine learning moves much faster than the normal than the ml100 so one thing i would suggest is do all the laps of ml-100 so that you are mentally prepared for ml-200 and those of you who are taking it please and this is a free to you guys do please pay and register at this moment support wait just need some funding at this moment support vectors need some funding that's frankly the pandemic has been very kind to us so with that that's all i have actually guys so with with that in place i would say that i'll just feel questions now i don't have anything else uh principal component analysis was a simple topic but guys if you don't practice this it's of no use so make sure that you do practice it okay that's a that is sort of important that you practice it and it will take you barely half an hour to do see dimensionality reduction is one of those most awesome things it is mostly understanding it once you have understood it the coding is almost trivial nothing to it very quickly you are able to do it and the bang for the buck is enormous. Imagine just four lines of code, and sometimes you have great success with it. So these are the topics we'll cover. Now, one thing I would do is I want to give you an idea perhaps of other dimensionality reduction techniques. PCA is the basic. In the next workshop, we'll cover some more dimensionality reduction techniques also. Let me give you guys a sense of that. One second, let me share the screen. Maybe I could use that to share that. Oh, okay. In progress, here we go. Application, application window. Here we go, application, application window. So are you guys seeing my PDF? Dementia reduction toolbox in Python. Oh yes, thank you. Thank you for verifying that. So this is an article again taken from towards machine learning. Somebody has put together, this is something you'll see people have contributed a lot of effort in explaining to each other different topics. And the idea is that if you all explain to each other, we all benefit. So again, as a best practice, as a parting advice I would say but pick up a side that you like towards data science is pretty good you can pick up other and every day you know sit down with a cup of coffee and pick a couple of particles and read through the whole of it most of these are very quick reads it says five minutes well five minutes is a little bit of an exaggeration I would say you can actually study it in about 10-15 minutes right five minutes is a little bit of an exaggeration i would say you can actually study it in about 10 15 minutes right five minutes you can glance through it but if you understand what it is you'll start with you look at these imports you don't have to study the imports line by line then you take the mnist data set which is a hundred which is a digit recognition data set in handwritten digits so data set in handwritten digits. So it's a standard data set that for years has been a classic. Can you recognize the handwritten nine or a handwritten six or eight? So for many, many years, it has been a sort of a benchmark. So this does and it says the dimensionality of the data is 784 dimensions. What can you do? Obviously, it's too many dimensions. Can you reduce the dimensionality of it? So to do that, once again, guys, do you notice it's just two, three lines of code. In three lines, you are able to reduce the dimensionality. After that, there's a plotting plotting code which is a bit tedious longer but very simple all you're doing is attaching a color to each of the digits one two three four five six seven and you're plotting it when you plot it clearly you notice that for example the zeros are clustered here the blues ones are clustered here so zero looks very different from a blue in a synthetic PCA space. And different digits get separated out. It's pretty impressive what PCA can do. Now there are enhancements to PCA. People have created quite a few enhancements. So there's something called incremental PCA. So you can work with batches of data. So just keep this image in mind and now look at doing it using incrementally. This may or may not be a little bit better than the previous one, but this article showcases all the ways that you can do it. Then you have something called the kernel PCA, very very powerful method. Actually we applied it in the boot camp to the breast cancer data set with some interesting results. But for what it is worth, it is here. This generally outperforms PCA by a little bit. At this moment you don't have the full experience to notice it. That is one. The second thing is that in this particular example, it doesn't seem to outperform by much, but for what it is worth, it is there. But do you realize that this is just three, four lines of code, yet another dimensionality reduction method. Another is called sparse PCA, which uses, there's a little bit of mathematics. You learn in the math of data science, but from the coding perspective is that, see, here's the thing guys, if you just understand PCA, you can use fancier methods, even before you understand the method, the mathematics behind those methods, because it is just changing one word. Do you notice between kernel PC and this, all you do is change the word kernel to sparse. And now you're on the go. Singular value decomposition leads to a different kind of result. Singular value decomposition is like eigenvalue decomposition I just talked about. The results are like this. Then you have something called gaussian random projection one more interesting method which produces well probably not so impressive in this particular case all these different types are literally sitting on top of each other not very impressive then there is sparse random projection you look at sparse projection again, not terribly useful. Then comes some more powerful methods called multidimensional scaling. Look at this. Well, unfortunately, this picture is reduced in two parts in this PDF. But if you can clearly see the separations are slightly better, much better than this. You can do ISO maps, which are also reasonably good. Mini batch dictionary learning, not at all impressive for this particular situation. There's something called independent components analysis, bit of a disaster here but okay but then comes a very very interesting method it is uh called t-s-n-e t-distributed stochastic neighborhood embedding quite a mouthful very powerful methods and if you can look around it you can clearly see that as a dimensionality reduction technique this has worked clearest isn't it? Do you see that the clusters are fairly well separated? It's not perfect, but it's pretty well separated, isn't it? Definitely. And that is that. Then there's locally linear embedding. It produces this. Not too good. Now the question is, which one should you use? The answer is a priori you can never tell. It's part of something you'll learn in the next class. The no free lunch theorem. So what do I do? I actually have an automated loop that generates two dimensional representation using each of these methods. We live in the world of computational plenty. In the cloud, you can have as much computing resources as you want you can spin it up for just 20 minutes and then shut it down and you'll even if you use yourself a very very powerful instance you would probably consume what 10 cents 20 cents of it and then you shut it down but you get all the results and you'll know which is the best auto encoders again has to do with dta networks uh is it good yeah it seems to be getting there some of it it got very well others it didn't do such a good job of so this is it you know now there is one more new kid on the block it is called umap which actually usually outperforms tsne. This article doesn't mention it, but I will give you the idea that guys, I try out this and then try out these things, especially because they are all just one or two lines. It's very gratifying to see how well the data separates out like this. Go back to the breast cancer data itself if you so wish and try to apply all of these techniques to it and see what happens. Well guys, that's it. That's it for the extra session and I don't have anything else except to exhort that you guys do this labs, very simple labs do you realize two lines three lines of code or algorithm sorry so if if we if we're good with if we're good in performing pca does this save us a trip from consulting with the SME or do you have to consult first with the SME before going to PCA because of confounding variables? What happens is that you should spend a lot of time analyzing before you talk to the SME because you know you're entering somebody else's data the experience shows this it used to be believed that without smes without subject matter experts you really can't analyze the data they know it then came an inflection point in data science and machine learning it turned out that machine learning was making much better predictions than smes and it was a surprise for everyone. It turned out that the theory or their beliefs in the different subjects sometimes were a block. They were preventing those people from seeing things which was not there in their belief systems and their theories. So machine learning started outperforming the SMEs. Today the situation is if you combine SMEs and machine learning you get the best of both worlds. A classic example is you know how do you build sports teams? At one time people would build sports teams using as subject matter experts. Scouts with a lot of experience would know and observe players how well they play this and that. Then came machine learning and it applied just statistical analysis and said these players should work together and they will win. You form teams using machine learning and that what is it curveball or something it completely hardball I don't know what the apparently there's a book or a show by that name I'm not at all into sports but apparently it used to be a big thing when this was discovered. And so today, machine learning beat the accuracies of those veteran scouts. But then somebody showed, of course, that if you combine the veteran scouts and machine learning, then you have superlative, like you have the best of both worlds. So that's how it is. Do a lot of analysis, then sit with subject matter expert, listen to what they're saying, and then go and do more analysis. That is usually the cycle. See, machine learning is not a linear journey. It is a cyclic thing. You keep cycling back to, you're chasing your own tail for a long time. You keep going back and again and again. So the way I would say it is it's like a rising spiral you know at each cycle you rise up a certain distance your accuracy improves and then you cycle back and you cycle back and you keep cycling back but at each time you're not coming back to the original place you're rising up along the z-axis you're learning a lot and your models are getting better and better even though you seem to be cycling through a lot of the ideas over and over again going to subject matter experts cleaning the data further doing more feature extraction then more model building then once again making predictions then going back to you know the experts then going back to, you know, the experts and going back to data, more feature extraction, more cleanup. And that cycle typically goes up. See, remember, with one model, I'll give an example in my team, a leader, leader, machine learning team, there are people who for seven years, have worked on only one data set and one problem. Like imagine a group of six people who have done nothing but try to extract value from one data set for seven years so in reality what happens is when you join the workplace you will get or two three data sets which will become your life and for a couple of years you will be spending all your days trying to do the best you can with those data sets in the homework it seems right that quickly you finish one data set you move to the next and the next and the next. In work it's not like that. It takes a long time to squeeze the juice out of a data set. Take for example the the situation with Zillow. Zillow makes house predictions. It's a regression problem isn't it? Very much like California data set predicting the value of a house. But they weren't quite accurate. They were much better than what was before. People trusted Zillow and Trulia, you know, all of this Trulia and so forth. Then Zillow got sued because it was undervaluing some homes. For example, my home is surrounded by homes which are all about 1.5, 1.6, $2 million homes. But guess what Xero put as the value of my home? It put a value of approximately about 300,000. Now, how can you have- That's bad. And ironically, the reason it put that value is because my home is so big. In other words, the backyard is close to 28,000 square feet. Now in big cities, in Fremont, you don't usually find homes with such a big backyard. So Zillow looked for something called a comparable home. And I'm just guessing that the comparable home was far away in some rural area where somebody's home had such a big land attached to it, 28,000 square feet attached to it. And so it was looking at the value of that, maybe God knows Fresno or wherever it was, right? That home and attaching that value to my home. So all these algorithms have imperfections to them. None of them quite work well. And so you're forever improving it. And Zillow, after the lawsuit and so forth, I suddenly noticed that they have gone and corrected, in quotes, corrected the value of my home, though any real estate agent would tell you that it's still severely underrepresented. So it goes on like that, you. So it goes on like that. It goes on like that. You're never done with the problem. If you think of Zillow, a large team of data scientists are trying to solve only one problem on only one data set for their entire career. So you see that, Patrick, how it works in real life. Yes, sir. That's what I was trying to understand because we, because there are so many data sets every time we go to take lessons for machine learning. So many data sets after data sets. That's what I wanted to know. Like in the real world, how many data sets is really what you, what do you touch what do you work with and yeah like you said uh it's not as many as you think because fine-tuning is much better exactly exactly in fact usually for one startup one data site is their entire livelihood and it's their ticket to a successful exit multi-million dollar exit it to a successful exit, multi-million dollar exit. Also, you know, the game is very interesting. These days, as you know, recently the trend has been in Silicon Valley that you don't acquire a startup for the business, the money. The moment you see proven talent in any startup, you just acquire them. And you couldn't care less with the business model. You just want proven talent. So a lot of the time what happens is a lot of bright people get together, do a startup. They start solving a problem. They prove that they have done a pretty good job on one problem. And the problem may have absolutely poorly defined business model attached to it. Sometimes people have no clue how they're going to make money out of that. And yet there is a very successful exit because the likes of Google couldn't care less what this little company is doing, but they do care a lot about picking up talent. Everybody knows, in fact, I am in this space, I research this topic. Everybody knows that when you interview people and look at job applicants and resumes you hire mediocre people the only way to hire talent is through acquihire i mean the the probability that you'll hire high quality talent through interviews and resumes very small proportion you do end up hiring some very good people but a much smaller proportion than when you just go acquire talent to acquisition so acqui hire is the new way of hiring these days and so i would highly encourage you become good with any few algorithms take a data problem sit on it for months become good at it publish a couple of articles right do a startup on it and then sit back relax and wait for these companies to come to you i'm seeing it from the experience of my friends yeah getting back to ml100 could you in one place list all the labs that should have been work that should have been completed for the class in case some students overlooked which assignments because sometimes it feels a little fuzzy? Yeah, sure. Definitely, guys, the tabular data.pdf, every single example that is there, you should do. That is the core textbook for you for the labs. So you have clustering. That is it. That is your lab book. Do everything that is in the lab book. If you have done everything there. You haven't missed much. Great. By the way, how did you find the lab notes, guys? Did you find it useful? Any general feedback from the new students? Patrick? Yes, sir. I was able to understand the lines, the lines of code and I was able to try to answer most of the labs. I haven't finished everything yet. But yeah, I'm doing it at the same time reading the book. How about you Balaji? How did you like the book? Balaji, are you here? I think you may have stepped out. How about you Suresh? How did you like the labs and the workshop in general? The classes were very good. I've not fully completed my lab, but the labs are good. The one thing I think after every class, if I can find one or two areas where to apply that topic that I learned in the class, that always gives a good understanding. Sometimes I'm just still struggling with just doing what is in the class that always needs a good understanding. Sometimes I'm just still struggling with just doing what is in the lab and whatever is in the lab is good, but my motivation is not up to the mark to try all the labs. So I need to work on that. And probably one of the things that I did is if you find one or two problems related to the theory that I learned, that helps me to just spend more time and then at the lab. So I need to combine those two. But overall, I think it was a very good learning experience and I was exposed to a lot of things that I do now. So yeah. so nice thank you how about you shiva how did you like it so i really like the whole concept of the lectures and the labs, right? So is it correct, Asif, for your overall? ASIF KUMAR GANDAVANI, In general, it's very, very, very informative. I would say I wouldn't have gathered that much information anywhere, reading a book or attending another lecture or anywhere. So I'm really indebted for the information that you have spread, provided us. So the labs are really exhaustive i think i still have to catch up with the labs but overall it's a very good experience i would like to continue with the other other other workshops that you're planning in future too so thank you all right guys anybody has any other comment otherwise Otherwise, we'll, oh, Aditya, you had the, I don't think it's Manisha, isn't it? I'm sorry? It's not . Yeah, how did you like the workshop? Yeah, I mean, it was pretty advanced. But, yeah, I did enjoy it. I did enjoy it, yeah. Nice. Very good. All right, guys. So I'll post this video. I'll post this PDF also, the last article. And do stay in touch. Remember, guys, the workshop is over but i hope our relationship is not uh you can reach out to me anytime you want months later if you wish uh with your doubts with your questions with clarification i'm always there any additional books that you would recommend for 200 would recommend for 200 it's the same day no books no textbooks okay what we do is we'll finish it and the lab books that I recommended will do a lot from those labs also now what happens is that ml 200 will cover the topics that there in this textbook but it will also cover things like no freelance, etc., for which actually, unfortunately, there are no good textbooks, which is the limitation. So I'll be giving you a lot of notes that I have written. Let's see if they are of some use and take it from there. See, now as we go to 200, 300 and so forth, we'll be getting into very recent territory. And usually things which are hot and new, you don't get textbooks on them easily, they're too new. And the ones that are there are too, you know, they'll explain it, but they'll explain in high-flown language that without the mathematical background, you're completely confused what exactly was said so it needs to be brought down needs to be brought down people understand so we'll try that and there is all of these videos you have access to a couple of them i haven't shared yet because youtube there was having a problem recently i tried to share a couple of them I haven't shared yet because YouTube was having a problem recently. I tried to share a couple of times each time I would do for some weird reason. It would not let me enter the list of people I could share it with. Let me try it again. Oh yeah, I succeeded this time. Excellent. So I'm going to share the rest of the videos with you guys yeah I said the last three videos are still not to be able no yesterday also when you ping me and then I tried to share I couldn't once again do it but now yes now I tried again and I notice you can share now so I I'm adding it right now. Okay. You guys should get a notification in a few minutes. It has all been shared with you. Apologize. So in fact, one notification we should get literally right now. See if you guys got a notification. Notification by email or Slack? Usually it comes by email.