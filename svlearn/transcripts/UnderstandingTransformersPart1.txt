 All right, guys, we are going live. And we are recording. I'm going to. Are you folks seeing something on my screen? Just text me and I would know. Yes. All right, let's get started. So today we are going to talk about transformers. This is a topic that a few of you asked me to explain. We'll talk about it. It's quite an important topic. It all started with a paper in 2018. Preprint was in 2017, December. And this paper is considered quite a breakthrough in natural language processing. Before this paper, most of the activities in NLP were done with recurring neural networks. Hereafter, after this paper came about, there was quite a sea change in the subject. People started doing things in new ways. Transformers became very central to the sort of things we do. Based on this, there were implementations like BERT and so forth, and then there are pre-trained models and there are things that we can use, Alberto, and so on and so forth. Now we are going to cover these things in great depth in the deep learning workshop that is starting in July. So for the practical part of it, of course, and a lot of hands-on practice, come to that workshop. I think most of you have shown interest in joining that, but this part is just an overview. I'll take you through this paper, and it will also, I suppose, save us time when we do the workshop. This will cover the foundations of it, the basic ideas of it. It is in response to understanding what it is. The paper is a bit dense, and it's sort of have to unpack what the sentences mean. You need to have the background and then be able to unpack the paragraphs. So that's what we are going to do. So the way I'm going to do this is, and obviously we're going to focus. So the way I'm going to do this is, and obviously we're going to focus on this particular paper. This particular paper is, the reference for that is here. These notes of course you will have. If you just google attention is all you need, this paper will immediately pop out as the very first link. It's quite an important paper. In order to understand transformers, what I will do in this session is gradually walk through this paper. But before I walk through this paper, let me bring about certain basic preliminaries of how you do NLP. We'll talk about words like words, tokens, embedding. What does it mean to do embedding? And then we'll talk about NLP. What does NLP, sorry, not NLP, of of course we know what NLP is recurrent neural networks I'll give a very basic idea of recurrent neural network and in and LSTM GRU they have been pretty dominant then we'll go through the history the history is literally in this sequence people were doing recurrent neural networks and then they started doing RNN with attention. So there's a realization and what this paper pointed out quite interestingly is that you don't need basically you you reach a state at which you say this is not needed and you just have attention based attention only and when you do that you can do a lot of activities create using a particular tool of architecture and this architecture which has an encoder decoder is called a transformer or these authors called it a transformer and this word has become pretty much the standard vocabulary in this field and so we will look at the architecture of the transformer. So we will talk about something called the sequence to sequence models. Let me write it in different color. Sequence to sequence models. What do they mean? In this context, we'll talk about and this is a preliminary background that many of you may already have, but we'll sort of quickly refresh through a lot of these topics sequence to sequence. And what is it to do encoder, decoder? Again, these are the preliminaries that we will need in order to understand this particular paper and to understand Transformers deeply. Now this paper is very influential so there are a lot of YouTube videos that try to explain it. There's a lot of articles if you go to towards data science or any just google the word you'll find a lot of explanations that go into it that sort of in some sense explanations that go into it that sort of in some sense try to add more color to this paper, explain it and so forth. So after this I would highly encourage you to google them up, see what you like, complement what we are talking about with that. My emphasis will be in understanding the core mathematical idea, the core concept of transformers. We'll spend quite some time doing that. And so for that, we'll build up the preliminaries. So the very first thing I will do is start with some very basic things. In fact, I'll start with a few mathematical preliminaries. One of them that I will start with is something called a softmax. So those of you who have been doing or have done a neural net course with me know that typically if you are doing a classifier or thing like that, at the end of a neural net, always put a soft max layer you know there's a soft max node a single sorry layer a soft max layer and then you use that to tell which which of the many classes you're predicting into so what is soft max this is again a question that it was asked so i thought i'll explain it to you what does it do let us say that you have a bunch of numbers and by the way this has not not directly to do with the attention and transformers but i'm just building up the mathematical foundations at this moment so suppose you have numbers imagine that you have two array of numbers, S and K. If you look at S, you notice that the values are 1, 1.52, isn't it? And if you look at K, the numbers are more pronounced 1, 10, 20, isn't it? So the thing to note is in S, the values are relatively close to each other. In K the values are bigger. The separations are much bigger in size. So now if let us say that if these were not probabilities, let us say that some machine is saying, I believe, or to a certain extent, that it is a cat, you're choosing between a cat, a dog, and what could look like a cat and dog? You want to distinguish between a cat, dog and mouse. And so something is saying that my weightage of belief that it's a cat is one, the weightage of belief that it's a dog is 1.5 and weightage of belief that it's a mouse is two. You would say yes, probably two, but two is not that different from 1.5 or one. And so your optics is a bit blurry. You're not quite sure. On the other hand, suppose you say that my weightage of how strongly I believe, if I were to distribute the weights, it would be in the ratios of one is to 10 is to 20. Like one is the probability, not the priority, the priority is of course between zero and one, but sort of weight of belief that it is a cat is one, the belief that it's a dog is 10, and the belief that it's a mouse is 20. If you have that, then given these numbers, one of the question comes is, well, you can convert them into sort of probabilities one easy way is to just. Raja Ayyanar? add up all of these numbers and divide s by the totals of example here the total would be 234.5 divide s each number in s by 4.5. each number in S by 4.5 and divide each number in K by 31, 1 plus 10 plus 20. So you'll get the proportions and proportions will always take on values between 0 and 1 and they will all add up to 0, 1. And it sort of has the properties of probability because all the probabilities or those proportions will add up to 1 and so forth. So people often ask, actually, that why do we do the soft max sort of business? What is the value of doing that? So we can see it like that. To illustrate this, I took this example. If you look at the proportions, do you notice that the soft, the S, actually, I took S to imply soft, like it's somewhat blurry, whether it's a cat or a mouse or a dog. So the proportions are 22, 33, 44. And proportions for K are again, well, it doesn't seem to be much like a cat. It seems to be like about 32% that it's a dog and 65 percent that it's a mouse right something like that but now let us amplify the difference between them see what happens when you exponentiate the values I'm going to exponentiate the values of s and k when I do that look at the values that come out. The soft values are still within the same order of magnitude, 2.7, 4.48, 7.32. They are within the same order of magnitude, between 1 and 10. But let's see what happens to K. k all of a sudden you notice that this is 2.7 this is close to 22 000 i believe is it 22 000 yes 22 000 and this is a huge number 4.85 into 10 to the 8 right what is it about 480 million or 485 million or something like that so this value begins to stand out then yeah we can say that where are we i'll just expand this this particular value you realize that it begins to stand out when you exponentiate it right and when you really think about it the value the initial input values are 1 10 and 20. what it does is it sharpens it sort of sharpens the differences between the values and so when you now convert the exponentiated values you take the proportion the same thing we do with proportions what you find is the values that come out, which begin to look like probability, are quite radically different. For the soft case, the S matrix, S array detector, where you had much more blurry optics on whether it's a cat, dog or a mouse, but 18%, 30% and 50%, right? So that's the way the probabilities distribute themselves. But see what happens when you do the softmax of k and then convert it to probability. If you look at this, you realize that there seems to be up to three decimal points, almost zero probability that it's a cat, just a five percent probability that it's a dog, and 99.9, you know, practically all the probabilities shift to a mouse. And this sort of hopefully explains why using a softmax is so popular when we are doing classifiers at the end of it and converting the values into a probability in the end and picking the highest probability item. It just sharpens and makes the big value, relatively big value, stand out in a clear way. So that is a softmax. The other preliminary that I would like to mention is very basic things, vectors and dimensional spaces. This is one of the things that sort of escapes us. See, a vector belongs to some space. We all will agree. Suppose this is a two-dimensional space. will agree suppose this is a two-dimensional space right i can call it x1 x2 and a vector let me call this vector capital x made up of x1 x2 so two things one is something all of you know the dot product dot product between two vectors is a measure of the similarity between those two vectors so suppose i were to take another vector, something like this vector. Let me not use y because y is often used for output, but let me just use x prime. Let's look at the x prime vector. Because x prime and x are relatively close, this is a smaller angle. Whereas suppose I take a vector which is different, uh this is smaller angle whereas suppose i take a vector which is different i take a vector like this oh sorry let me use a different color suppose i take a vector like this so you realize that this is practically orthogonal to it it's pretty far from this orthogonal to it it's pretty far from this so the dot product the dot product of and especially if you normalize it you'll realize that that the cosine of the angle is small, small angle, large orthogonal vectors. And let me call this x prime prime. Whereas the cosine of x and x prime prime, so here we are treating x as the reference vector. What would it be? This would be approximately zero. Right, so what it means is vectors that are different, the cosine angle between them, when you look at it, are the dot products. Assuming these are normalized in some way, the dot products give you a sense of how close two points are, two points in this vector space are, right? So dot products, I'll just write it here, product, give you a sense of nearness in our space, or to whatever dimensional space it is. Here I took a two-dimensional space, but let's generalize to d-dimensional space. So this is something I hope we all know, but now comes an observation which I find that surprises people. See, suppose you take a very high-dimensional space. Let's take a space that has, let's say, hundreds of dimensions. Well, you can't represent it on a page, but I'll try to, and then obviously each of these is orthogonal in some sense. You can't represent it in a two-dimension blackboard, but assume that each of these are unit vectors. These are axes E1, E2, E, D axes. So any random point that you take, any point that you take X and any other point that you take X prime, think for a little while, you'll realize that if you randomly pick two points, I'll ask you the choices. Will they be near each other? Like will the angle between them be small or will the angle between them be large? So the two choices I'm saying randomly Pick two vectors in large spaces. So we are talking in large dimension space. D where D is large. Then think about it. If you take a point x, the question is, are they near rb, are they orthogonal? They're nearly orthogonal. So if you, this is a question actually that's really worth thinking about because it has a lot of implications so think about it if i generate randomly a vector and let us say that i generate a vector whose values essentially are normalized or something. So small numbers, right? So I take a number somewhere here between 0.1. Let me just take two random numbers. So pick a number, 0.1, 0. Suppose this turns out to be 0.8. This turns out to be 0.5. Are we together? Suppose you take another random vector, x prime. And this is, of of course we are doing it in two dimensions but you'll get the idea uh suppose and let me actually three dimensions let me add it to the dot dot dot and let me add here now Pick another number. Suppose this comes out to be 0.7, 0.3, 0.1, 0.2, 0.9, and so forth. and basically and obviously normalize it and all of that you will realize that the fact that the biggest component seems to be here for x1 and the biggest component of this seems to be here the big components are far apart of these two they're very unlikely to align are we together guys that if you randomly generate numbers here and you randomly generate numbers in the second one then it is very very unlikely that in both these these two vectors their big elements are pretty much in the same place right what does that mean what it means is if they are not in the same place these two vectors are in pretty uh if you just look at the plane between these two in that higher dimension they are much more likely to be closer to right angle they are orthogonal to each other and this is easy to visualize just imagine that you randomly take a point in a room, and then you randomly take another point in the room. And now you try to imagine that what's there from the center of the room if you put coordinates assistance, how far apart are they? Are they likely to be very close to each other? Are they likely to be far apart from each other? In three dimensions itself, you don't get that intuition very strongly because it may be chances, sometimes it may be from each other. In three dimensions itself, you don't get that intuition very strongly because it may be chances, sometimes it may be close, quite often it won't be, but in higher dimensions the chances that they will be aligned or similar is very very low. So the dot product actually is something very useful in high dimensions we will use as a test of nearness. useful in high dimensions we will use as a test of nearness. It sort of becomes quite interesting actually. We will use these two things quite a bit. These two ideas, one of softmax and the cosine function of the dot product. It's one reason you see dot product. Well there are many many reasons dot product is a wonderful thing but we need it for this particular paper. So with that, those mathematical preliminaries out of the way, let us go and think about how we do a basic natural language processing. Before I go here, let me see if there are any questions from the from you guys and where would i see that um hang on guys Yeah, I don't see any open questions so far posted. So let's get back to our sharing. So see, machine learning, as I said, we talk about vector spaces. The thing is the input is always a vector. Input belongs to some space, some R-dimension space. When you have structured data, it is much easier to get to a vector representing the input. It's pretty much a given. For example, if all your features are numerical, well it is a vector. It's very simple. You can treat the row as a vector. The input columns together make a D dimensional vector of their D features. But when even in structured data, some of the columns are categorical. So for example, it is the animal, is it a cat, dog or mouse? Now you have to, you're in a fix because categoricals don't are not points in a vector space in so how do you do that typically what you do is you do one hot encoding so you can do hot encoding for example so you you can ask, is cat, is mouse, and is mouse, and then, or maybe I'll prefer dog, let's say, because I have a dog, is dog. And if it is, so it is implicit from that. So suppose it is one, it is zero. And the thing that you didn't mention is mouse. It's unnecessary to do that, it's redundant information. So you don't sort of do that and this will be zero. But if it is a dog, implicit value here is zero. On the other hand, if it is a, and again, we are talking about way it's a refresh on the way elementary stuff. If both of these are zero, if it is neither a cat nor a dog, then implicitly this has to be a mouse. But then we got into a number and now, because we have a number through this encoding, you essentially expand your feature space with these two exploded out features. And you again have your vector space, automation space. And so to the extent that when you're doing classification or you're doing any output or something like that, X goes in and Y hat comes out, your predictions come out. You do have the basis or a way to feed in the input. All you have to do is plug in the proper algo to do your stuff, predictive algo, to do whatever it is that you have to do is plug in the proper algo to do your stuff predictive algo to do whatever it is that you want to do or if you even if you're doing unsupervised learning your clustering points you'll again want to know data as a x vector now that is all fine but how do you deal with text or text is not not directly a vector space. Text is made up of words, it's made up of paragraphs, it's made up of sentences, it's made up of characters. So they are different granularity. And the whole question is, how do you extract a feature vector out of it? So there are many ways to do that, but first thing to do is pick a granularity. Are you going to do the analysis at the level of a character? So somehow we need to make a character into a vector or is it at the level of a word? Then a word has to become a vector. So let me just write down your choices. Character, word, you can have sentence, you can have paragraph, and you can have other combinations of words, and you can have n-grams, and so forth, paragraphs, or you can have a whole document itself, itself as a vector. So each of these, you can have asked this question, how do you make it into a X vector? It may be a need to do that because your input, you may want to look at that level of resolution. Now, the most common obviously is this guy, sorry. common obviously is this guy, sorry. This is the n-grams is obviously the words and n-grams are obviously the most common. So I'll just use the word word implicitly by extension I will say that what applies to word but usually applies to n-grams. So what are n-grams by the way? N-grams are two combinations. So for example, a two gram is nothing but a tuple. Like for example, a tuple could be green tomatoes, right, or ripe apricots, right. Or if you have a sentence, the cow jumped over the moon. Well, obviously you need to clean out, you need to nuke out the the and the, you know, the very common words, but let's assume that you didn't. So you have a lot of sort of bi grams. This, this, then This, then this and then this, you know, you take every two adjacent pairs and they become your bi gram, the cow jammed cow jammed and so forth. These are your things. So you can have those, those are your n-grams here in this case by grams, and you can continue on make trigrams and four grams and so forth. So the question is, first thing you have to pick your resolution. Let us say that we have picked word and implicitly or n-grams. I'll just use the word word. Now the question is how do you convert a word to a vector? How do you project or treat a word as a point in a vector space? There are many ways that you can do that. One way is sort of again a one-hot encoding because it's a categorical. You can ask this question, how many words are there? And usually associated with the language, you have a notion of a corpus. Corpus is the universe of words that are admissible for your analysis. Now, the corpus in the general sense, for example, it could be every word in the English dictionary. sense, for example, it could be every word in the English dictionary example could be in English dictionary, that could be it. Right. And a corpus and it could be a smaller vocabulary. So for example, children's dictionaries often are sort of they have a lot, they only have safe words and so forth. So their dictionary could be different. Or you could have every word in, let's say, legal domain. So if you're talking about lawyers, they have their own particular vocabulary, or let's say medical lexicon. And people often use the word lexicon the vocabulary or the set of words that are specific to a particular domain and so your definition of corpus is sort of up to you generally in the absence of any other context by corpus you just mean all the words in that particular language. And even there, people sort of do limitations because computationally it becomes heavy. Sometimes they will take only the first hundred thousand common words or they'll take sometimes if you're really ambitious, you can take 5 million words in the English language and so forth. Most of the time people take a smaller vocabulary, unless they have a lot of computing power available on the desktop. You don't want to have a huge thing. But let us say you do that. Now, what does it mean? You can sort of write the words in alphabetical order. Hard work. Right. A. Yes. And and so forth. You can go and somewhere in there is a cat, somewhere in there is your dog, somewhere in there is your mouse, and somewhere in there is a zebra also sitting there. And at these, look, if you just treat them as locations, so suppose you give these values 0, 1, 2, 3, etc., you can treat the entire thing as a bit vector in which only the, suppose you're talking about a cat so only here is your cat well two tails one do one tail so only this one will have a one here all others will be zero so you can represent this all the words using this. It's very close to one-hot encoding. It's what you would do for any categorical variable because the moment you realize you are taking a finite corpus is a finite set of words, it has become a categorical variable, the words in the language. And every word is a class so you're predicting or you're using it as a feature uh the the the corpus so you can do that except that you have a space that is extraordinarily big suppose you take a five million dimension words in the english language that is ridiculously large space five into into 10 to the 6. Now, it is problematic for many reasons. It's computationally a headache. But much more interestingly, you realize that the dot product between any two words will be zero. So every word is sort of orthogonal to each other, to every other word. That doesn't make sense. In this space, what is lost is sort of the meaning, the semantics of the word. So for example, one would like to believe that a cat is closer to a dog than to, let us say say a scissor. That sense of nearness is not preserved in this vector space. So this vector space is sort of useless. It's computationally horrendous and it doesn't have the meaning. This often is true for input data and so quite often you look for a lower often is true for input data. And so quite often you look for a lower representation of it. So one analogy I'll give you is these days, with the eight bit color modes and 16 bit colors and so forth, 32 bit colors. I don't know what the color latest graphic color representation is. But let's say that you can represent a color with 16. Actually, let's say 32 I believe 32 is the going number 32 bits I don't know what the exact is and anybody who knows can enlighten me. But if you do this and you do hot encoding of a color, you will have 2 to the 32 possible colors and that is in billions oh yeah I think it must be 32 bits because a lot of graphics companies claim that they have billions of colors whatever it is if you do 16 I think it would still be a huge number of colors 64,000 colors or something like that so this is a huge number of colors so if you hot encode it if you say red it will be a huge vector that goes on to billions and then there will be one one here everything else will be zero yet you know that colors are related so for example, red and pink are closer. Pink are closer, whereas yellow is a little far off. Blue should be even further off. But this vector space does not capture the relationship between the words, between the colors. And so what you do, you look for a lower dimensional, you can think of a lower dimensional space in which the relationships are captured between the values of this category. I mean, if you're looking at this color space, you end up with a billion classes, a billion different kinds of colors. But the relationships, if you want to capture, you ask, is there a space in which you can capture the relationship between the colors? Of course, if you want to capture, you ask, is there a space in which you can capture the relationship between the colors? Of course, if you are doing any kind of graphics or web development these days, you know that there is a representation. You can do R, G, B and alpha. So how much of red, how much of green, how much of let me just say red, green, blue and transparency is usually called alpha. How much of alpha do you have? So what have we done? We have taken a space that is huge and reduced to a different space which is much, much smaller. much much smaller and yet you still capture the information about the color you don't lose it at all so this is in some sense it's a form of embedding can you do that can you represent a color just with four dimensions rather than a billion dimensions in the same way, you can ask about words. If you are given words in a corpus, it is worth asking, is there a representation? Is there a way, a lower dimensional representation so that you don't have to represent it in such big dimensions, but at the same time, you gain meaning because see, look at the RGB space here. Suppose I have a color gray, shades of gray. I can start with white, shades of gray, and black, right? And see how close they are in the RGB space. If you use the HTML notation, for example, this would be 0, 0, 0. And let us say that your transparency this is alpha is set to one shades of gray you can get in many many ways so for example if you do two two two two one it is and then you can do i don't know six six six one and you can do, I don't know, 6661, and you can do FFF1. Do you see, guys, at this moment I'm just ignoring the alpha channel, which is saying that it's fully saturated color. You realize that these vectors, you can see that the difference between these two are small. So this is black and this is a dark gray. And common sense says that they are actually much closer to each other than say green, which will be a red, which will be something like F001. The distances would be much bigger. And that is why you would like this sort of a space. They give you a sense of distance between colors and so forth. So something similar, we are searching for words. How can we do that? There are many ways of doing that people have come up with and all of these ways are called word embeddings. You use a neural network to come up with, use a neural network, and we won't go too much into it i can give a sort of illustration of how we do that use a neural net neural net to discover the word embeddings now i'll give you an intuit so let's just summarize it you take a word in very high dimensional space d and somehow word, and you do the embedding, you come up with an embedding vector for the word, which is in much, which belongs to a much lower dimensional space where D is much, much smaller than capital D. It is quite common if you look at it, 300 dimensional or 512 people take. They take many dimensions. you may take 10 dimensions, but 10 may not be enough. So you can pick a dimensionality into which you are projecting your corpus, all the words into the corpus. The question then remains, how do you discover it? So we'll come to that. But let me just mention some of the advantages of discovering and doing an embedding. And it was quite a surprise when people found these embeddings, they noticed some interesting connection. For example, you can ask this question what or let me just say let's take take an example. Kathmandu is to what? Some X, like you don't know X, what it is. And this sort of questions, remarkably, these embeddings could solve. It's quite interesting. Let me represent Delhi with a D vector, India with an I vector, Washington with a W vector, US with a U vector, Kathmandu with a K vector, and this is an X vector. I'll just take this. And what people found is if you do this, B minus I was very close to W, Washington minus U. minus u and therefore and therefore you assume that it is also approximately k minus x the x vector so you could use any one of these bits of knowledge to compute the the likely value of x so you can say d let's say delhi minus india is approximately the same as K minus Kathmandu minus X, whatever X is. And so you can solve this and say this is approximately equal to Kathmandu plus India minus Delhi. And so what do we end up with? And then it will turn out that if you look into the embedding space, you will find remarkably that the nearest would likely will turn out to be Nepal. And so you're seeing that the capital and the country get related. The distance between a capital and the country sort of remains more or less the same in the embedding space, which is quite remarkable. And people began to note all sorts of interesting connections. For example, they noticed that you could do queen word is approximately equal to king minus male plus female. And if you just think about it, it sort of makes sense in some sense, right? You can see the semantic structure of it because a king is a leader of sorts who happens to also be a male. But if you take the male part away and add the female part, it's a leader who is a queen of sorts. That's sort of the way people try to interpret what it comes up with. It's quite a remarkable discovery that you can do embeddings. Now, there are quite a few popular embeddings. There are two ways of doing embeddings. One is you do it as a part of a sort of a supervised learning exercise. Exercise. So what you do is you you say that given words so suppose there are million the m million words but you're going to represent them each of the word with only a d-dimensional vector a creative matrix made m times so that each word let us say the word horse will be represented by a d-dimensional row d-dimensional row in that matrix of million rows times this let me call this matrix the embedding matrix here of that now you have to this matrix, and what you do is you put it usually before you have the input in your neural net, and we'll talk about it. You put the embedding, this matrix to be trained, need to be trained, and then you put your normal feed forward network as an example, and your softmax at the end of it and then you predict let's say that you predict something for example it could be the part of speech is it a noun verb adverb whatever it is part of speech right or whatever some sort of a classification task you do. And as the errors back propagate and things happen, what will happen is you may initialize this matrix with random values, just randomly fill it with some values. But then by the time your training is done and you reach a state of minimum error, you will have a really well-defined embedding matrix such that it gives you a mapping from word to its embedding. And let's hereafter refer to the embedding vector of a word as EW. So word goes to its embedding vector, right? And the word obviously belongs to a very high dimensional space, but embedding belongs to a small dimensional space, the embedding space, right? So that's the concept of an embedding. Now, there are unsupervised ways of doing embedding. It just says that, see, all this text is there, the corpuses, vast amounts of corpuses are there. You can take the Wikipedia and then there are many, many companies that have put in a lot of effort to create giant archives of text in one form or the other. You can mine that archive of text and ask, can I not find the relationships? Ultimately the connections, the semantic connections between words is right there. Can we not extract it? And so there are all these lovely techniques. There is Word2Vec. There is Glurv. There is FastText. And all they do is they are successfully able to create, in an unsupervised way, the embedding matrix, the weights of the embedding, and therefore you can embed any word in the corpus into a vector in the embedded space. All of them work with certain degrees of intuition. Actually, Word2Vec is likely to be one one of our subsequent lectures and it will be given by our my friend chander some of you know about him and he'll be giving the talk on word to wake embedding so i won't go into the embedding uh word to break and so forth but they all use some connections so for example what the glove looks for the the the co-occurrence of two words in a certain window of text, in an engram of text. So you take a window of text and then you ask, did the word I and the J co-occur? And then you ask this question. So let's say that the probability of the occurrence of IJ versus the occurrence of I versus any all other words over sum over w this would be the sort of you use functions like this ultimately you do a lot of mathematics to do it such that you come up with come up with the embedding and if you you're interested, we can have a talk. Let me know. We can talk about it. And Chanda is going to talk about all of this. Word2Vec uses a sort of a narrow bottleneck to, again, discover it. It takes a word vector. And in that original space, it sort of compresses it. And then it sort of recovers, sort of does some mechanism, so that in the end of it, you get the word. I won't go into that. But so now I would like to end the mathematical preliminaries. What we learned are two things. One is we learned about softmax. We learned about dot cosine of normalized things or rather dot products of x and y right or x prime let me say x prime x prime and so on and so forth or some people also refer to it like this and we also learned about embedding and the need to do embedding the value of doing embeddings of roots. So we're done with the mathematical preliminaries. I'm ready to get into the paper. And when I get into the paper, let's maybe we can take a five minutes break. I can stop the recording. We can take a five minutes break and I will open it up for questions. So the next topic that we'll study is sequence to sequence models. We'll learn it in three ways. We'll using RNN, then we'll talk, we'll learn about attention, what is attention, and then attention only plus RNN and attention only. So this is the value of the word attention is all you need. So this will be our sequence today. I'm going to pause the recording for a moment. Let's take a break. I'd like to take questions from you folks. Where do I do that? And how do I take questions? Oh, let me see the questions. There are around 16 million colors and displays are capable of showing around 16k colors. Nice. English has around 150k words with Korean and Japanese 500. English, I think, English is like some of the corpuses that you have are, I thought, 5 million words, including all sorts of acronyms and all sorts of technical words and so forth. But yes, 100,000 is supposed to be a word. And what is it? 150k, someone has mentioned 150k. Yeah, that sounds reasonable. I suppose how you define your corpus is totally up to you. What is the medical corpus of woods? I don't know. So maybe one of the folks who are medicals can answer this. What is the, if you happen to know the size of the medical corpus, then finish as infinite number of woods, very interesting. Let me now unmute and figure out how to unmute everybody uh there has to be a way as i'm just becoming familiar with the show names of limitations for slide streaming okay so remember guys we are Okay. So remember guys, we are recording has stopped. We have still streaming. So resume chat. How do I go to participants? Okay, participants. Let me try to unmute everyone. allowed to talk allowed to talk allowed to talk. I wish there was a way to unmute everybody. This is pretty annoying that one has to do it one by one. Perhaps you can just text in this. And middle middle wants to be unmuted. So let me search for middle all right anybody else let's text me who would like to speak. Is there a text feature here? Anybody else? Okay, Mr. Middle, please go ahead with your question. Hi, Mr. Middle. Can you hear me? I can't. So he's gone. Anybody else who has a question? If not, let's get together in five minutes. Yeah, go ahead. you hear me yeah go ahead yeah so those were actually my uh things that what i had you're coming out very faint can you please uh adjust your microphone can you hear me now yes okay so those were the things that i wrote about finish and all where um there was another talk that i attended in ecm where they looked at just the abstracts of papers okay and they basically what they did is they were able to predict what materials you can build by just building an association between the materials not going through the paper but just the abstracts and that was like I think maybe 30 million papers or something he said, and they were able to cut the cost of doing new materials because they would analyze it in different dimensions, right? So which composites can be built and things like that. It was pretty interesting when you were talking about embedding that was going through my head and the medical corpus is also where you're building association between words. Yeah, see quite a few of these things. That's a very good example. One classic thing that Peter Norvig in his Artificial Intelligence book, by the way, that has a new edition that just came out, mentions is the physicist, solid state, I mean, condensed matter and material science community. So they actually found that you could give molecules and say if this molecule is to, let's say, ferromagnetism, then the other molecule is to what? And it could correctly predict that the other thing is to anti-magnetism or something like that. So people have tried this embedding, and they are very, very remarkable results of what it can do and how much Interesting insights you can get just from embeddings. So if you true Anybody else would like to say something guys Just post the questions and I'll try to... All right, if not just give me two minutes, I'll drink water and I'll be back. Gracias. E a√≠ Gracias. All right guys, I'm back and I'm about to start the recording again. to do that there is we are on record so let's continue on i'll just give a very quick primer of what sequence models are. See here, one of the typical cases we deal with is language translation. It's an example and we'll take this example to move forward. So suppose you have words. The word could be jumped over the moon. What you want to do is you want to ultimately convert it into some language. So these words, we can write it as w1 w2 right or let me not use w's because weights in machine in neural networks you usually reserve w for weight so let me call it x1 x2 and then we can have a x 0 which is just a starting point, x3, x4, x5, x6. So this sequence of words is your input. What do you expect? You expect another sequence of words, w1 to wk somehow, to be the output. And note that the lengths may not be exactly the same, but with padding, they can be the same. Now, this output is in, let's say this is English, and let's say that this is Hindi. So how do you do this translation and this is what we'll focus on in this particular thing now when you do, think of a feedforward network or even just one single neuron will do it, the simplest case. So imagine that you give it an input, xi, then you produce the output, y hat of xi. So I'll just call it yi. You produce an output. So suppose you gave x1. Let me just make it real. x1 came out. And then what happens is you want to give the second input, x2. So what you have is you actually have not one the input that you give is not just your to this network is not just your input what you do is you share the input space with one more vector there's a another thing that you feed along with your input and you treat it sort of as the input now where does this other input come from? This is data, the x is data or words and usually you take words not as vectors in the embedding space. So x belongs to the embedding space. Now comes the question, what is this other guy that you're feeding to get your why to? The other guy actually is, and if you think about it, we need to, before we predict a word, we need to look at the word over, it is good to know that jumped preceded it. Because jump provides a sort of context. In fact, all of these words provide context. So if somehow when we are translating over, we also somehow remembered these words, the sentence, somehow it has, it is sort of in our mind, it is in our thought somewhere. Imagine that you are this particular unit in the thought of, in the mind of, if you want to use the word mind of, mind of. We call it, you can think of it as a thought vector, mind though we call it you can think of it as a thought vector what what you have thought so far so h1 is it has done some output for cow the the it has done whatever it is lay a year or something then you produce uh not just that you produce one more thing you produce a thought vector or representation internal representation people use all sorts of words it's all internal representation i'll use the terminology h1 here and what you do is you typically feed it here right some people write it actually it is like you have another unit suppose you you can either feed the output here or you can feed the thought here. So suppose your thought is nothing but the output. It is possible you could do that and we'll deal with those situations in a moment. Now, what you're saying is that usually you put the arrow like this, but actually, so here I'll put it, but it is an input. So some people will put it here. Some people will just make a diagram here. This diagram is more common. So I'll leave it like that. Now, what is the thought before you have translated any word at all? You can just start with some H0, which can be void, you know, 0, 0, 0 vector. You can pick a dimensionality of this vector so suppose it is d dimensional vector thought vector now you feed that in you feed the input and then you get another thought vector and an output and then you feed the thought what you have learned so far from the sentence or the context of it people often use the word context vector for that and you feed it to the next and so forth. You keep on doing it and you keep on generating the words. Now to do so, if you put in your simple neuron or you put on a feed forward network etc, these are all called RNNs. The familiar word is RNNs. Now the plain RNNs are effective, they are made much more effective by something, the LSTNs and its more recent cousin GRU. And I'll talk what these are, we'll cover these in great detail in the workshop, but here let me just give you an idea of why we need something more sophisticated version of these units, not just feed forwards and so forth. See what happens is that this thought vector, sometimes you need to remember relationships, right? Cow, let's suppose I change the sentence to that. The cow that was fed some carrots, let's say cows love carrots. I don't know if they like carrots or not. I think they do actually affect carrots to cows. Carrots jumped over the moon. Enjoy. Let us take this sentence. Now you realize that this is after this karma to here, it sort of pauses the main flow. The cow jumped over the moon, enjoy. So when when you are here you have to remember the cow right so now it is hard because in between so many other words have come now suppose your thought vector has so much memory so suppose it has a finite amount of memory. What happens is that you're updating this memory, you're trying to summarize what you have learned so far. You don't because you have finite memory, things begin to spill out very much like human beings do or, for example, one example that i always think is the children's story of of finding nemo right so there is a character dory or dora i don't know i'm forgetting which i think it's dory so uh she has very short memory and so uh whenever people the friends are having conversation she tends to forget things so short memory memory. So imagine that Dory is reading the sentence. So by the time you reach here, you may have forgotten all about the cow. And that is what is the problem with the base RNNs. They tend to forget. And so what you can do is that the LSTMs and GRUs are improvement. They contain hidden with them, sort of a remember part something that they remember right i remember they figured out something to remember and carry forward and of course the lstm also contain these are called gates memory gates and so forth a memory gate and a forget gate right so remember, let me use the standard where people use memory or something. But the intuition is that. I hope you get the intuition. Forget. And then the rest of the things, complexities are there, the standard networking. And so you mix it up. There's an interesting architecture. We'll deal with LSTM in great detail in the workshop. But the point is that it can carry through this context here. And it also has a way to forget that. And later on people came up with a simplified version of the same thing, ability to carry forward information. It's a simpler version is called GRU. And it turns out that it is equally effective and much faster to train. So I will use, when I use the word RNN, I will use all of them, the plain, vanilla RNN, LST, and GRU as synonymous. So it is a little, maybe it's a bit being sloppy, but I'll just use it synonymously because they are RNN units of sorts. So when you use RNN, and so let's go back to our picture, you feed in the word, the cow jumped over the moon, jumped, let me go back to the sentence, over the moon. So at each unit, you have the same RNN. The value of RNN was that, you know, whatever weights you trained in the RNN, you have the same RNN. The value of RNN was that, you know, whatever weights you trained in the RNN, it is the same weights getting updated. So the translation is, the point was that it was order one. The cost of memory utilized is order one because the same internal weights are being updated. It is producing, it may or may not produce some y, it may produce, but it certainly produce a thought vector, h1, h2, h3, h4, etc. And you have h5. So when you do this, this RNN way, there are a few limitations that stand out. One of the big limitations that stand out is that the, so when you do, for example, I'll take this example of translation, you actually ignore the output. What you do is in two shots, you create one RNN stack, like you take the RNN and you say that the cow jumped over the moon and you go through the, the, in the, your RNN sequence and the last word that comes out one two three four five six the x6 that comes out it is your final thought vector or it is your context or thought vector it's supposed to representation the the idea is it has encoded the sentence. And the idea is if you have encoded the sentence and now the entire thing I'll represent as a box, this thing goes in and what comes out is H6, the thought vector, the context. Or whatever way, the hidden or whatever way the hidden state the final hidden state right i i tend to think of it as the final a thought summarizing what was said so this is my way of looking at it the thought summarizing what was said. It captures the thought. And then what you do is you put another RNN here, the decoder RNN. Decoder RNN. And what this decoder RNN does is it takes the heart vector at six. And now it starts decoding. What you do is you give it the first word. Well, in the beginning, it will just say, produce the first word. You again give it a null token. It will produce some word, okay it will produce some word some actual decoding word y1 and then you feed the y1 back into the output here and this back the thought vector and the output so this goes in and the output goes in it produces the second one so what happens is at any given moment you produce it out of the previous output t minus one and the hidden state it is a mix of the hidden state and this that is continuously producing the next output in the next output and so forth and you keep on moving forward and producing the the words the translated words in the other language this is roughly what a decoder does. So encoder creates a thought vector, the summarizes what the sentence is trying to say, decoder then takes a thought and translates it back into a language, let's say French or Hindi or whatever it is, and you do that. And so these are called sequence models. You see, you follow encoder-decoder sequence in this. Whenever you have a, I mean, sequence models is just sequence in sequence out. Now, encoder-decoder is the preferred architecture because it performs better rather than just at the time right away producing outputs in a basic RNN. You just take two sets. right away producing outputs in a basic RNN. You just take two sets. Now it has a problem. The problem that it has is it doesn't remember long sentences. It's like a Dory, the forgetting happens pretty quickly. If you have a very long sentence, then because you're continuously, if you think of it sort of like a Turing machine, in the finite amount of memory things are being stored and this memory is being updated. So what will happen ultimately is what is recent will be dominant and what has been said in the past very far behind in the sequence, that tends to get overwritten or that tends to get muted. And so it becomes a problem. It has a forgetting, a habit to forget. That is one. The second is the scalability problem. You can feed it only one word at a time. And because you're feeding it only one word at a time, the entire translation and everything happens very slowly. Now we live in the world of GPU computing and you want to do things in a massively parallel way. You would rather feed in the entire sentence if you could and get the translation in just like that. You don't, but you have to, you're feeding it word at a time and so forth. So scalability and performance were huge concerns actually. And the NLP community was in a sense looking with envy at the computer vision community because those people were happily marching along with their GPUs and their convolational filters, convolational neural nets with their filters and doing a pretty good job of using the hardware, whereas the NLP community was having a fundamental bottleneck. So that brought about the use, the possibility of this new framework, which is the transfer learning framework, the transfer, sorry, not transfer learning, I apologize, transfer learning is entirely different, Transformer. Transformer, which is based on the concept of attention. Now, we are close to the end of today's session. Let's take this up next week and I'll continue into that. So today, we have just sort of laid the foundation for getting into the transformers, but I'll give you in the next five minutes, a very basic idea. What the attention does is it basically says that in the sentence there are relationships between the words. So let's go back to our sentence, big complex sentence. Jumped is in some sense aligned to or the word jump when it is being translated it should attend to or pay attention to the word cow. Right? And cow jumped over the moon, cow should pay attention. These are there's a relationship between words and the sentence, which is common sense. That's what we do. And the intuition is when you, let's say that you are looking for a ball in a picture. Intuitively, you know, you, suppose ball is in your mind, that's what you're searching for. Immediately when you look at a picture, your mind gravitates to, or pays more attention to round objects, things that look like a ball. Even though there may be drugs and there may be grass and lawn and everything, your mind pays attention to the ball or to any round object and asks, is it it? Is it it? So that's sort of the intuition behind it, that when you're translating the words what is the relationship between this and that is called self-attention how much attention each word needs to pay to other words right so for example uh fed and carrots should pay a lot of attention to each other but you would agree that carrots need not pay any attention to the moon right they're independent sort of in the sentence and that is the intuition of attention how do we get to that more technically what happens is you remove the limitation of this fact that h6 in our sentence the cow jumped over the moon has more of the moon and less of the cow in it because cow was far back. So the way you do that is that you don't give it when you decode, you don't just give it H6. You in some sense give H5, H4, all the way to H1. Conceptually, in practice, you don't give it. the way to h1. Conceptually you give, in practice you don't give, but conceptually if you could just pass in all the other previous thought vectors, you would agree that now there is no forgetting because information is there. A cow is very much there and let's say it's beginning to fade out, but it's very much there in the early thoughts. And so if you give it all of those as inputs, early thoughts. And so if you give it all of those as inputs, then you can have a pretty robust decoder. Now the question that that came about is if you start passing in all of the input vectors, computations will become very heavy. That's where this attention mechanism, which is quite crucial, I just noticed we have four minutes left. So I'll be quick and we'll keep the attention itself for next time. It's a very clever way of doing things. Attention has been there for a few years. And then, obviously, the title of this paper, or the transformer, which is so dominant, came about. Somebody noticed that, hey, if you, people who are putting attention and your RNNs together, and then this paper noticed that you don't need, you just can build encoder decoder simply with attention. And therefore the sentence attention is all we need. So let's keep that as part B of the talk. I apologize that I gave a lot of preliminaries. I don't know whether you folks found that useful or not. Do send me your comments. Next time, now that we are done with the preliminaries, I'll barge straight into the actual paper. And it's a very interesting paper. We'll go over it. And these diagrams have become quite iconic in this natural language processing space. Which diagrams am I talking about? These diagrams or figures. You will realize that these figures, this figure is the encoder decoder. The left-hand side is the encoder. The right-hand side is the decoder. And the way it does it is quite, quite interesting. But already you can see bits that you understand. So for example, after this talk, hopefully it is obvious to you what this is, why you need to do input embedding. You don't want to pass it a million dimensional vector. You instead want to pass it the EW right now what is the output here this is actually quite interesting you pass out whatever you have translated so far suppose you are translating words you have produced four words you realize that that too is important can become the input in the decoder you pass that as well as the final thought vector or the thoughts of it. The same idea propagates, but we'll use a different vocabulary. So three things go into it. You take the input, you go through the same process, which is multi-head attention. We'll talk about that. And then you pass it to a unit, which looks exactly like, if you look at this, this unit, you will notice that it looks exactly like this unit, the encoder unit. And that's quite remarkable that the decoder is just the encoder unit behaving differently and with one extra attention block added to it. And this architecture is really absolutely lovely. And the most amazing thing is once you understand how simple it is and intuitive it is and what it does, we will cover it, let's cover it in the next week, same time. So with those words, I'll stop now and I'll also stop the recording. Let's take it up next week. So I'll stop the recording and I'll open it up to questions yeah sir can you hear us