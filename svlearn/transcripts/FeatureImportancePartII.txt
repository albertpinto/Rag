 . Guys, let's start the log. So guys, the recording is coming through, a couple of targets too. There is a bit of static in it, I'm trying to fade it out, but for now, I'll post the video in the raw. Also guys, do not share this video with your friends. Because we are having a lot of frank conversations. Sometimes we make statements about countries, etc. That one single lawsuit is a stress to shut down. We can't afford to have that. So please don't share this video. Otherwise I won't be able to make this recording so available to you. These are not meant to be public. And also the conversations that are popular, they lack interpersonal, you know, that interaction, that close interaction that you have with people. And the informality you lose, then you can become a very fabulous person, which has no implications. So, you can preserve what I can say, and still be safe, only if you don't share it with people. Okay guys, with that, let's, let's get started. Let us summarize. What did we summarize? We can have the inter-predicating is very tightly linked to future importance of one and the other. Related. So we agreed with it to get today as one topic, even though there are differences between the two, we're treating it as one topic. Now, one approach is, of course, your linear model will take you there. If the linear model takes you there, future importances are essentially your coefficients on scale data. Isn't it? For linear logistic incentives, if you are willing to market it. But more generally, at any given point, the importance of a picture is the rate of change. How much the response will change if you change that picture a little bit. That is literally the concept here, feature importance. So that's the point. And the other thing to realize is that teacher importance is a local phenomenon. It has very different values, a ranking of teachers, based on where you are in the teacher space. We did the example of the sale of ice cream. When the proximity to the beach is already pretty high, proximity is not likely to be the most important feature. You might as well improve the quality of the ice cream. If the quality of the ice cream is still high, if you're far from the beach, you're far better coming closer to the beach. You'll get more substantive, gay to your studies through that. So features, how much the features matter depends upon where you are. So, globally featured by pieces are achieved by doing some sort of aggregation, some protesting things, if we talk about that. And the concept of PTA parties, as I said, is very much related to the grid locally, it is the grid. The main problem that we face, where I left you guys with a question is you are saying there's a black box. Even if you make a prediction, the machine has learned what is essentially a black box. It's not anything what the function is in any animal form. If you ask a real electric, you can say, you know, these are all my weights. So many layers, so many weights, this is the net weight of a together. A weight tensor. You say, well, how does that help me? Right? You can't use that to explain to a business owner what is really happening, how which features are not available, etc. It's complicated, very complicated. Especially in deep learning efforts, with very many layers, interpretation is a challenge. And yet we do need interpretation. So the question therefore becomes, if that is implicit and not available to us, how can we still get a gradient effect at a given point? That was the challenge. So what do you think guys, what should we do? Yes, at that particular point, just do, what you could do is, and you got it, it's right, you do perturbation of each of the variables. You change the variables locally at that particular point. Let us say that at a given point, x naught. You want to know how, what is the ranking of features, how important are the features, how much do they matter? What you're asking for is essentially grad of grad, evaluated at x, is it good? You are asking for the partial derivative with respect to each of the features. So the question comes, well, how are you going to obtain that data? You can just dip into the data and see, given x0, are there neighbouring points? And if I go to a point in which the first factor, x1, has k, x2 has k, if you're lucky, you might find neighbouring data, very close by data, which has this perturbation with respect to one derivative, you are unlikely to get that. But that shouldn't stop you. What you can do is, you can deliberately create, so suppose this is x-num. What can you do? You can create small perturbations like this. Small changes in this. You can do small perturbations like this. Small changes in this. You can do plus minus delta. Create new values by doing perturbations. Are you understanding that? Create new values. Those become your new points in the feature space. So you are saying, this point in the feature space, let's stop. Let me just create more points around it. We are literally creating new points. Because we are creating new points, let me see what is this model trying to tell me. It's some country. By doing this, now I can tell the small changes in each of the features, how much does it do, how more specifically there is a way I can put it. See locally, hyper complicated the M surfaces, the manifold of factors. The function is a surface in this feature space, with a response variable also in there. And between the two, it's a surface, it's a hypersurface. How do you complicate the hypersurfaces? For example, if you look at that glass, however winged up it is, or congulated it is, locally to a point, so long as you stay local. Locally, what does it attach? It appears plain. It appears like a hypotheic. It appears straight. If I take this globe, no matter where I am, it looks flat. Locally, every surface is flat. Now, we always make the assumption in these situations that you are in a continuous and differentiating surface. Roughly speaking, you're in a manifold, right? What is the concept of a manifold? You have a manifold, a smooth manifold. Given a smooth manifold, any locally pointer looks like a plane. If it looks like a plane, what can I do? The relationships, if they are linear, I can fit a linear model, isn't it? That can work only in the neighborhood of that quantity. Does that make sense? So in the neighborhood, see, the way we argue this, we can say we don't know the function. Globally, it's too complicated to be explicitly stated. But nonetheless, I can put in perturbations. X naught plus all sorts of perturbations of X. Different values of I, different perturbations, this change this a little bit, that a little bit. Pick one point in the neighborhood of X naught. You get a whole lot, now that is your new data set. You have a new data set which made up of x naught plus... Look, this is your dataset. And each of these datasets produces its own y i hat. Your model will produce a y i hat. Make sense? This is there. This relationship holds. Now you hypothesize that there is a linear relationship there because it has to be. Locally it will be a plane, a tangent plane. So given the tangent plane, at x naught modulate as, so what did we do? We can now model it as a linear model. Once we do the linear model, then we can find, what can we find? We can find literally find the coefficients here, we just, isn't it? And we are teaching how this comes. That is one approach. Another approach that we could take, and see, another way is that all you have to do is do small perturbations. Right? Small perturbations of f. Delta f, delta x, y, will give you, this will be approximately equal to, all you have to do is small perturbations, see how much the function changes with respect to each of the feature, with small changes, right? And just average them, locally. You will get a pretty good sense of the partial derivation. Am I making sense? And those partial derivations essentially give you a sense of feature importance. That's exactly what's this. Go ahead. . . . . . . You get a global relationship. You get a much more global relationship. So, within a river, what was the correlation of x1 and x2 against the y where you have the other rivers? Very little correlation. Isn't it? And that correlation told you no idea about the decision set. Isn't it? Because it's a new one. It's a very local structure to be able to do this. What about the data of a small change? Small change is small. If your data is already, let's say, scale normalized on a scale of . So, its perturbations are like, you know, 0.001 from 0.003, you know, it's like going zero, zero, one, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, zero, no, you're not doing that. See what we're doing is, we're printing, see think about it this way, in the decision you are creating a lot of points. So what you do is, suppose I'll give you an example. Let's say that x naught, naught is equal to, I'll take some value, 12, 5, 6, 9, 11. This is a five dimensionaldimensional point. Are we doing that? So what it means is, I think we... It's not. Let's... Delta 1 is equal to... On 12, just do a random perturbation. 9, 11, 13... Not that big. Not that big. 12.5. Let's say that 12.5. 4.9. I'm just taking some examples. 5.1. But even that is big. 5.7, let's say. 8.8. 11.2. I did some random change. Now, hang plus delta t. Again I do random correlations. Are we together? So what am I doing? I'm making very small changes. I'm looking at the neighborhood at that point. So that point is five dimensions. Five dimensions and including the target will be six dimensions. Right. Exactly. And so what will happen? So what will happen is that your y hat can be, is now some other linear function of x, x near x naught. Isn't it? And not only that, I can at each of these points tell that what is the change. So, I can evaluate the function. I can put x naught plus delta 1 through the black box, come up with y1 hat. Right? Likewise, I can do for the rest of it. So, let me make it right. I can find all of these points. All of these points, therefore, I can ask myself, what is delta yi over delta i itself? This would be the gradient in that direction, particularly. But, getting all of this, all I need to do is find the plane that I have here. Am I making sense? No, that solid That's what we call a play. So let me put it this way. Teacher space, it's quite a convention. You see, you always think of it like that. Teacher x1, teacher x2, this is all. So now, your relationship is a surface. It is some surface, right? Any function f of y is some function of x1, x2, it is the same, it is describing a surface in three dimensions. So now when you go to fourth dimension, you know that now x1, x2, x3 is actually a volume. You don't call it a volume. You can still call it a feature, hypersurface, with a hyperplane. And y is still a volume. You don't call it a volume. You can still call it a feature hypersurface. It's a hyperplane. And y is still sticking out. So you see your floor has three dimensions. Or your floor has four dimensions or five dimensions. So if you have a five dimensional floor and you have this y, so the neighborhood is actually a hyperplane. But the full relationship is some hyperspace. It's hard to visualize because... Because in reality it's not. It's a volume that already reaches space. Neighborhood is a volume. But we don't think of it as a volume. We think as though it's a disk. Five-dimensional disk. Are we together? You think we have created perturbations on a five-dimensional disk. So, the best way to think about this is that we have to think about this. They're always visualizing the disk. That's why I always tell you, keep your intuition to two. But when you're talking about five dimensions, you should always say, of course, as it exists. I'm joking. Go ahead. So, how would all these radials being on the same surface, black surface, how would they all have it or become a part of the plane? No. I am saying guys, very simple, let's pay attention to it. Let me make it very obvious. This is the tip of India. At this particular moment, I can, at this tip of India, if I do a local thing, I'll get a plane. What will be the plane? It will be going like this. This is my local model here. Whereas if I am going now to the United States and especially to, let's say, somewhere in Alaska, my plane is not going like this. They are completely different models. Do you see that? That's it. So, locally, hold on, locally everywhere, there is a model, a linear model. Does that make sense? No, right? Locally you have gradients, you can compute, and therefore locally you have a concept of future environments. Good point. locally you have a concept of future environments. Good. . See, what happens is, okay, let me answer this question first. All right, let's see if you think like this. All I did is I created small perturbations. On that, I created neighboring points. But given the neighboring points, I can describe a plane. The only purpose of creating neighboring points is to do what? Create a plane. Given the plane, I can write the equation of that plane, y hat locally at x naught, is some function g, right? g of x in the neighborhood of x naught g in the neighbourhood of x, where g is some beta0 plus beta1 x1 plus beta2 x2 plus beta2 x3. That's the equation of a plane. Those are the... If I do gradient of G, it will literally be these things at beta 1, beta p locally, and these are essentially the feature of the potencies, isn't it? Are you following the reason? That is the reason. Simple argument. Now, this is one way people do it. The second way people do is they say locally, instead of a linear model, people say, I know what a decision tree is. I can represent this with a decision tree locally. Right? So sometimes people will use a decision tree because they like decision trees. You can do that. Some people find decision trees more interpretative. In the name of it, there's a value, there's an emphasis. And in a decision tree, you have to change parties. How do you have to change parties? The feature along which you split first is the most important feature. The feature along which you split the next is the next important feature. So you have feature importance comma decision feature. Personally I prefer linear models because they have a nice integral that goes into it. If you go with the assumption that locally it is a linear surface, you can do a linear model. You can do a hyperbolic model. So you just perturb it. Now if you realize that this is model agnostic, it doesn't care what model you have. You can always make locally linear models. Isn't it? Because you're treating the model as a black box. You don't care whether it's a random coil, support vector machines, neural networks, right? Whatever it is. Radiant boosting, fat boosting, whatever, pick your favourite algorithm. It is invariant of this argument. It goes down to the underlying mathematics that if you do believe there is a relationship F, locally it has to look weird. And therefore, feature importance is a local phenomena, of course, and of that hyperplane, these feature importances are just the gradients of the hyperplane. The slope tells you what matters most. Local. If you have two different small regions in which you have to use the model to predict the state, then they have to align with the data. Very good. Very good question. So his question is exactly the... Okay, let me ask this question. See, in Alaska, it goes like this. In India, it goes like this. Right? So you realize that they don't agree with each other, right? So what happens if I start bringing these two points together? What happens is, as you bring them closer and closer together, you will realise that they will actually, the model that is here, will represent, I mean, in other words, it's called the property of the manifold, it's a fundamental property of smooth manifolds, that the intersection region between, suppose I take neighbour regions and I take local planes, those two planes in that intersecting region have to make the same prediction. It means it has to represent both of these. It's like, if they are very close, these two planes cannot be like this. They have to be almost the same. You don't need to know anything special about it. No, no. You don't do it. Yes. Because I've found that when you use the word that you have an atlas, these are local covers, local maps, and as you join the maps together you get an atlas, which has to be consistent. So I'll give you a very real view. So imagine that you're looking at the wind velocity. In a very complicated topography. There are hills and valleys and whatnot. And there is a wind rolling bar. It went passing by. You realize that at each point there is a wind velocity vector. There is a speed of the wind and there is a direction to the wind. Common sense is that at these two points, they may be different, you know. Because of the hill, the wind may be going up like that. Because of this, it may be coming down like this, or it may be turning around per second. But as you come to an overlapping region, the local models have to say the same thing. You know. Both these neighbouring models, they have to essentially say the same thing. Does that make sense? That's a meaning of that. So, you have that. Now, here is a third thing people... So this is one way, very global models, which to me looks to be the gold standard. The linear models can be seen through a separate lens and give their interpretation. The other thing people do, which is interesting is, make linear models, and give their interpretation. The other thing people do, which is interesting is, they ask this question, see, I cannot visualize, even this hyperplane, I cannot visualize this relationship. So suppose the sale of the ice cream depends on many factors. It depends on quality the ice cream depends on many factors. It depends on quality of ice cream, it depends on proximity to the beach, it depends on the amount of advertising dollars you have spent, and many, many factors. But at a given point, at a given value, let's say that we have to make local decisions. You have to say, my sale currently is this, my proximity is this, my quality is this. And I need to just see, get a sense, or visualize, right? You know, part of interpretability is to be able to visualize the relationship of sales to these factors. Now you say, well, you know, you have two factors, you can't visualize can you? Say it's the y-axis, the z-axis, that was about the half-axis, then you have to go with two axes here. So you pick your thing. So you say okay, I'll take the proximity to the beach and the second factor I will take quality. But there is a problem. Your model now has other factors. And you realize that there is a problem axis is, what did I say? The amount of advertising, dollars that you spend. That's the third axis. So based on how much advertising dollars you spend, assuming advertising helps the sale of ice cream, these importances may change. The surface may look different, but how do you visualize it? So one thing that people do is they make this so-called partial dependency graphs. They've marginalized the third axis, the R-axis. So, they will create a lot of points around x naught. Suppose there are three factors. x1, x2, x3. This being the proximity to the beach, this being the quality, there's a problem. I can't visualize y in the three I need to program. So what you do is you take random values of this, lots of random values of this, right? And you compute the values of this, these two, and you find y, and you just visualize it. It gives you some rough sense. Are we clear? The idea being that if this doesn't matter so much, I mean, it is not as, but in a smooth surface, speaking of a bell curve, if you take a bell curve and you take a section of a bell curve, in the proximity of a point, if you change the Z value a little bit, ultimately it is, I mean, one axis, it's a two-dimensional bell curve, x, y, and y, up is the probability. You take a section of a bell hippo, you get a bell curve. In the proximity of a point, you can say that I don't want to care about the y. So what you can do, whatever value of y is there, just take random values around it, compute your probability with respect to x, and you say well okay, let's average over it and show, because it will make sense, isn't it? You will get the correct relationship, you know, you will get the correct relationship in the neighborhood of that point. Don't go too far, isn't it? So in the neighborhood of that point, we can do that. So then you will get a dependency plot near that point showing the relationships. Because 2x2 proxy can make this beautiful 3D- they look like beautiful 3D plots. That goes towards interpretability. Those are all locally interpretable models. Now, let's generalize beyond that. What people do sometimes is they say, well, we don't just want local interpretable models, we want feature importance globally. You realize that that will be averaging over all the situations, if you agree. So for visualizations, since we talked about visualizations, what they will do is they will take all values of X1, X2 and the sale of ice cream, but now the advertising dollar is problematic. They'll take all values of X1, X2 and the sale of ice cream, but now the advertising dollar is problematic. They take random values all across the step chart. And they are average of them. They don't care. So they will make a more global model of it. The moment you do that, what is the fear? It may not be represent, isn't it? The moment you go globally, in marginalising a particular predictor, a particular axis, you sort of are making a good-take assumption. You're losing optics, but learning a little bit. That is one aspect. The other thing is, how do you do global feature hypothesis? What can you do? You can do one of these things. Randomly go take some lots of points, sufficiently many points to cover up the feature space. Everywhere find the feature importances and take the average of all of them. You can do that. So that is another way of doing it. Then? . See, you create a new data set, x1, x2, you keep it. That's the first point. But now you have to put a value here. Just put some random value here and then compute the y. Compute the y coming out of the model, the y-hand. You can do that, right? The moment I have three values, I can compute y. The random values, that is the combination of x2, what's the base, what is h2? It's just a small perturbation. So you can do that, whatever the value is. So what you do is you take some random values. Now you build a model only in terms of y is some function g, not of x1, x2, based on these x1, x2, you already have the y. You try to build a new model. What has this model done? It is not even aware of this. What will this function look like? It will look like some surface. . . To build a model, how did you get the y? You need the y, right? How did you get the y? Yeah. You have to use the value of x3. Random values of x3. No, you don't put that x1, x2. You take a lot of values of x3 here. you pick a lot of values of x2 here. Two different cases. One case is with deltas across them. The other one is you access the margin allowances, random values. See, you pick your legitimate x1, x2. Even that, you pick x1, x2, x3, pick all, run some values, you'll get a y, that is phase one. You have x1, x2, x3, feed it to your black box, you get a y. That part is easy. Now what do you do? And now, just be careful, you can choose to keep the x1, x2 as what was in the data. x3 you can take from the data or you can do what you want. But generally, now what happens is you have y's. Because you have y, what can you do now? You can now go and build another model which is a function only of x1, x2. And that you can visualize. Because what will that show? That will show how the same response basically to proximity and quality. Now suppose I want to do proximity and advertising, then quality I'll just randomize and advertise. So you get this, how many blocks will you get? then quality, algorithm, not the margin of the product. So, you get this. How many products will you get? You'll get easy to keep track of. Proximity and quality. Proximity and advertising. Advertising and quality. Three different products. But by looking at how the sale does, visually looking at it, you get some interpretation, some visualization helps you, right? Isn't it? So you can apply that technique to many things. Suppose you have 10 variables. You can still do two at a time. NC2, you'll have a whole lot of jobs, but you can do NC2. Now these models obviously have far more activity when you're local, isn't it? Close to a point, suppose I am given x1, x2, x3, close to x1, x2, x3, just by doing small perturbations of these, I can build a surface. But by not going too far. I can make a model here and see how does it vary locally. Or, that will be pretty accurate. Or, on the other hand, I can do a more global thing, you know, all values of x1, x2, x3, I can randomly pick. This is random picking of values, and hitting it through the black box and coming up with a y. And including x3 altogether and saying, okay, let me plot the surface, x1, x2, y. Let me plot the surface. So's what I used to want. I think that's it. So basically, first I make the case of three-dimensional and then further make the third-dimensional. So then you get the third-variable, right? No, no. Or you can even take that. You don't have to just go through the thing. You can just put up all of them locally to see the surface. Just to make a plot, a three-dimensional plot. Say the five-screen, proximity and quality. What can I do? I can choose a certain point. Quality, base value, and around that value I can randomly pick lots of points. See, if I stay very close, if I stay very close I'll define a plane. If I take a slightly bigger surface, then it may be more complicated. The bigger the surface, I think, the more complicated it becomes. But I can draw the surface of two variables at a time against the distance. Okay? So those are called partial deterministic values. That is it. I mean, see, these are all means to somehow interpret or visualize high dimensional data or high dimensional relationships. You can do that. So, that is the next step. Go ahead, Povit. There's a lot of partial variable effects to say that we're visualizing partial variable effects. Sort of local effects around it. And when you visualize it, see it will be flat, but as you move away, now hills and valleys will begin to show. So it becomes beautiful actually. When you plot it out 2x2, and I highly encourage you to do that, every one of you, make these 2x2 plots. And it's very nice, it's beautiful. It gives you a lot of intuition into what is going on. See, ultimately, right? It just helps you interpret what's going on. . Not the third variable, this one and two are not important. . Suppose it's flat, completely flat. . In that . So, one side. See, for example, at the base of this, the height doesn't change the distance to either this axis or this axis. So locally, neither of the two directions . So which is the intuition that . . Just random. Just random . NP one. The hand down one. . But we should use this neighboring value. So we can just use this neighboring value. So we can just use this neighboring value. So we can just use this neighboring value. So we's just random gossip, random. NP one, the handgun. But the machine is just neighboring values. Maybe I shouldn't have used a good code for this. Shake it up, shake it up. Work with scaling. Always work with scaling. It's a two-time thing, you can do machine learning. Always work with scaling. Always work with scaling. Basically, that's what you do with your machine. Always work with scaling. Are we today? So far so good? I have a question. When you click the go and you use the word, Let's use the word shake it. Yeah, shake it up and then work with it. What is the agreement? I get that it is going to be a great product. Very good. Very good. . . . . . . you can see that it's not real. That's where I- But see, if your data is standardized, you realize that each of the predictors is valid from minus two to three. We got that. Yeah, so that's kind of a- So six is a value. Six is a range of predictors. So with respect to six, you can imagine that 100 or 1,000, these are all small perturbations. Does it have to be seen in relation to the changes seen in the white? Yes, of course. You feed it to get the white. Yeah, and if white kind is not changing, change will still go on the same time. Yeah, yeah. So the whole idea is that, see, the surface, think about this. This surface, this surface. Ultimately, this is it. In a two-dimensional world, two-predictable, why? This is why. This is literally your function. Now this function may be pretty flat. Right, I'm not going to make it flat, but we'll try anyways. It may be pretty flat. Looks like I had to tell you. Isn't it? Or it could be like, this looks like a complicated relationship. But how do you complicate the relationship? You realize that locally, there are kinds of things that we can fix. We can make local models and we can fix it. That's what I'm so. You can make local models and . It's a . But the problem is that they don't sound local . . Yeah. . Yes. So basically what I'm saying is that it is probably not as accurate to talk about global future. It is a far more precise term, locally. . This is how much this quality currently is, and proximity to beach currently is. So, given the ground truth of where this fellow is, you are predicting, you are telling him, that focus on quality. Are you telling me to do that? This is how it becomes clear. Then, a more aggregate or averaged out value is globally, what are the feature influences. So, they are all based upon, as you can imagine, hold on, let me finish this part. But it is based upon the averaging of all of this, isn't it? You average all of this, you aggregate somehow, and you get global feature influences. There is another technique that people use. What they do is something quite interesting. They will do this. They will say, this is a black box. I will feed it a lot of points into the black box and get the prediction. And what I will do is, of that prediction now, I will fit a simpler model like a decision tree on top of it. I think they are making the extreme math or something. So what they're saying is you have an algorithm that perfectly fits the data. It makes very accurate predictions. That is good to make predictions. You build another model. You see something funny? You build another model by taking some input points, letting that fancy algorithm make its prediction, now you get a new data set. On that data set you try to fit a simpler model, a decision tree typically, right? And then you look at the future importance of the decision tree, right? And you treat the decision tree as the more interpretable approximation to that deeper model. Your question. Yes, the question is, can we learn to do global? Let's take it off. Can we learn to do global pricing? Yes, of course. And how do we monitor it? What these people do, because now we are talking globally, . We are talking global. How can we go global? The very simple way is just go average over the key chain point. It says here, there, everywhere. Take averages. The second way that you can do it is, you can say, hey, wait a minute. This is a very fancy model. And it represents a decision surface that is, let's say, like this. Look at this. A complicated T. It's a classification decision. This is the surface. You say, hey, you know what, it's doable. Let us take some points and look at the predictions and then say, to this, let us create a decision tree. Or a simpler model, not a decision tree, maybe a medium or a some lower degree, more interpretable model. And then when you do that, it turns out that it begins to look approximately like this, I deliberately pick it like this. So suppose this is your surface. You see, this surface is very complicated. But we agree that this is a pretty good approximation to it. Isn't it? And so what we are essentially saying is that let this be the more accurate surface for predictions, and let this be the interpretational approximation to it. We will use this to explain, this to predict. Does that make sense guys? And now this simpler surface, you build out whatever you like, your decision tree, your linear models, and so on and so forth. Do that and build this. And again, you never say that we will use this complicated surface. You start with a caveat, this is an approximation. But this at least interprets that model. What you say, right? Instead of having no interpretation in a black box for this description, you would rather have, you would still consider it a win to have this. This is bold. This is bold. Yes. Not as clear. Yes, exactly. This is bold, this is not a silly, this is not a scary, this is not a novel. Yes, exactly. So you get that overall structure without boxing. So that is the other approach that we use. And now let me mention the last type of piece. So from the technique perspective, this is a tool we are learning. Yes. The way I'm learning. It is like you've written the C and M, and you have your type in it. Interfere, yes. You just go ahead and make the most accurate model. And now you ask that. It makes good predictions. But what is a prediction? I need to interpret. It makes good predictions. Ok guys, give me one moment, I'll just check on your launch. Don't disappear, don't disappear guys. Thank you. Gracias. Alright guys, please don't disappear, come back. Otherwise, we'll miss I've explained how many methods I've shown, local, models, and then I also explained it to people in the city. Then I said, let's see the global, now I'm talking global methods. Global methods, I thought one method just go average. Just find PGA port says that lots of points, and take the average. Which is by the way, if you ran on points, yes. The other thing you do, as I said, take a very precise model, like this, which is a very high fidelity model. Now, just all the wrinkles and complications of the relationship, but modulate it something simpler. And use this for prediction, use this for interpretation. See, this also has risk, because what happens is that there may be inherent micro biases of problems sitting in there in the window that your interpretable model won't speak of. So you are interpreting it one way, but you are predicting another way. So there is a risk to it, but at least you get something. As opposed to just saying, I have a black box. Alright guys, so, Ali. . No, no, what you do is, it's very simple. You take random set of points in the feature space. Right, but you could do like multiple central models. Oh yes, absolutely. And then you just go with the bias variance and then you put the- Take that in there. you get the best model. The best and biggest. Okay guys, so now I'll tell you about something very interesting. It turns out that this question is a, there's really a very remarkable way of putting this question. That was asked by a person who, when he answered it, the answer was considered so valuable, he was immediately given another question. He was immediately given a question. He didn't get economies. So, and there's a remarkable photo that you'll find in your notes. You remember John Nash? Beautiful mind? So there's a picture of this great man, with John Nash. He's a guy of that kind, very, very great. And so he said, he posed a problem like this, in a game play. Suppose you have a cricket match, and let's say, that should always happen in Europe. Right? Not in the US. Not in the US. Good England of course. So now the question is, different players, suppose they come and say, hey, we should be compensating. Let's say that there is a jackpot, let's say $10 million jackpot. And you have to divide it amongst your players. One easy way is that, you know the 11 million dollars. So one easy way is to give each player a million dollars. Usually that is not money except for happiness. Why is that? That's right. So what happens is, I mean, as it goes, this was one of the arguments that people said about communism and the taboo. The thing is, you cannot make reward invariant of performance. Because the moment you make rewards invariant of performance, performance will set. Isn't it? People will not work hard. In socialist countries. We have seen this from experience. So the right way, I suppose, would be to give each person based on the value they brought to the table. But then the question comes, how do you derive value? One guy would say, gosh, I worked so hard. I was the wicked people. Another would say, well, I was the boring. Another would say, I was the . I was a great . This and that. So you can have different people clamoring and claiming different values. So this is a very fundamental fact in the economy. The issue is the attributable distribution of wealth, given rewards, given a situation like this. How would we do that? So to that, so this is like giving, assigning. In a cooperative game, this team collaborated. They could not, they didn't fight with each other, they collaborated. And the end result of collaboration is a victory, a trophy. Now, what is the contribution of each of them to that victory? To map it to machine learning, here is a model that made this prediction, which is pretty accurate. This model is pretty successful at making good predictions. Now, it is a conspiracy of many features, many, many features. Well, not a conspiracy, collaboration between many features that made, that went into the model, that put part of the ingredients in the model. There is the algorithmic part, but there is also the features itself. And so you can ask for the features, how much did each feature contribute to those things? Which is essentially the same way a party was a feature overall. Do you see the relationship between the game theoretic problem and machine learning? So to answer this, what Shetty player is and is not. And you look at their difference. So imagine a world in which people play the same match against England, over and over and over again. And you play all sorts of teams with different players. In some teams, no. In some teams, in some teams, Nagekate is there. In other teams, Nagekate is not there. But the other composition of the teams are also changing. So you bring every possible combination of 11 people in the room. So we have 30 people. Imagine every possible 11 people team. And pitch it against him. And see what happens, what is the outcome, how much you win and so on and so forth. The degree of the outcome. And now what can you do? You get that differential at the starting of how much, like what happens when, on average, when Netsuket is there and when Netsuket is not there. Now I can do it for every other player? And what you will notice is that the players who are significantly valuable, the effect of their being absent will be much more dramatic than the effect of small players. The players who are contributing lesser value. Does that make sense? We've understood the arguments here. I can create every possible combination of their T, in which, here is their, or, so here is their, or not their, and then relative, by looking at the margin and the differential effect of their presence versus absence, I have a sense of how much value they bring to the world. That one statement was worth an award for choice. It's problem is obviously, see, and if you think about it in a global way, that is the only real way you can do future economics, from a global perspective. . No, no, see, we have 30 people in the room. . Assume we have 30 people. . . But, I say, what such model of taking out that one person, that one individual, would still be driven by the characteristics of that person in the same way? Yeah, so what they're saying is that there may not be too many gold keepers, or wicked keepers, for example. So, therefore, what happens is, suppose there are six wicket-keepers. So, they can only be wicket-keepers with different six people. In any given team, you have to pick one of six wicket-keepers. But the other players are randomly chosen. There are many, many teams you can form with different variations of other players. So, if you really think how many 11-people team I could create out of 30 people, it's actually a conspiratorial explosion. It's a huge number of people. But because it's a huge number, large number of chances. It means differential computations. Should I be able to finish that computation on the computer at all? The differential contribution becomes very apparent, isn't it? From a global perspective. So, to do that, to do that, is to use the... and that has been implemented regularly. I used that in 2012, literally, because one of my secrets, I suppose, to give one of the reality of some unity. I had my own implementation of Shogun, the teacher of the world. And the reason we had to do that, and it's a little bit of history, I guess now it is not in the real world. The question that I was trying to answer, way back in 2001, 2012, suppose attrition happens. I have a very complicated model of employee attrition, which was really complicated for the competitors of the largest and professional businesses. We had some pretty sophisticated models. The whole problem is, business doesn't care. It cares a little bit, but it can predict the future. But see, we are attritioning the predictions, the accuracies were still in the 70-80% It depends upon the company or the group that you are analysing that you can draw a sense of the accuracy Somewhere you are more accurate, somewhere you can even maintain the accuracy So, those are good numbers by the way, I feel that is right But business is much more concerned about actionable insights. They want to know which factors are important for attrition. So suppose I want to, HMS invests energy, should they stock the break room with better candies or with whatever? Or should they, you know, do something else, or you know, give more money, what not? They would like to know what factors matters how much. You realize that. So you realize how this whole teacher importance becomes a very real problem. And the way I saw it, it was actually. But of course now, you guys are very very lucky. Because Shetri is now an epic social writer. So you can kind of mind it. So you can do that. So that library is called SHA. Some factors can increase the sale of ice cream, some factors can decrease the sale of ice cream. For example, if it is breezy, likely the sale of ice cream will go down. How much it affects you, you can do that. And, clearly, that is very lovely because it tells you across the parameter, each space, where all what factors matter, comma. So it is good global and it has local aspects to it. And that is the last model I will talk to you. There is also time, I promised that in three hours I will finish, I have finished in less than three hours. Those are the models that you will need to talk to. So the way I explain it is definitely explaining those articles. But I'd like to simplify and get to that intuition .. But tell me after this and the articles if you find an alignment between them. Because it's the first time I'm explaining it this way. So I appreciate it. I hope there's a . So you go and read those to you. So I appreciate it. I hope you guys will see it. So you go and read those five articles. And I want all of you to explain it to everybody else at five o'clock. Five articles, five o'clock. And the third part three of one of the articles is a hands-on, do that hands-on. It is on the Adder dataset. And that's, I hope, in confusion, I hope, you agree that teacher importance is really a worthwhile topic to study. Of a modern issue, you don't teach it. I think the naive way of just taking teacher importance from aullis is a very simple, love it, ready way. You can do it in a morning mass. Today, you are devoted learners. And we are together. So please get started, have lunch, but don't go to sleep after that and uh you