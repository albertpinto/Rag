 So this is the second part of our lab on topic modeling. In the first part, we did a quick scan through the solution and we talked about the high-level issues Now, what I want to do is be very clear about the homework, guys. One important homework is play with changing the number of topics. number of yeah the bigrams trigrams and so forth these two are essential make sure we do that and those are two I didn't do a particular form of LDA latent Dirichlet allocation call HDA I will release code samples for that. There's nothing unusual about it. It's just a couple of lines. But you can play with that. Now, if you look at the way I have changed the code, there is an opportunity to play with this in slightly different ways. I would like to show you my code and explain what I mean. See, you have space, pronunciation, pronouns, punctuation, symbols, unknown. I have deliberately marked it with a hash, right? So that you can uncomment it and see what effect it will have on your overall results, things that you remove and keep. Then I change this. For example, I'm throwing away words after limitization whose size is just one, a single letter and so on and so forth. So it is a two. What if you change it to like four or you do something? What if you throw away numbers, right? Things like that. So play with this file, clean text, see what happens. And you'll find that you'll see some subtle small differences in what you are doing, right? And here also you can sort of... So that is that. That is one thing worth doing. So now going through this lab, what I would like to do is just go over each of these and see if there are any questions and explain it a little bit more in detail. If we can get through that part quicker, then I would like to give the last 40 minutes or 45 minutes to explaining or making progress with our project and giving you guys some suggestions for the projects because i believe at this moment you guys haven't made uh any big uh any large progress with it so far okay so uh let's go with that so the imports by now you must have uh is this looking anything unusual in the imports guys anybody notices anything unusual what is this multinomial a little bit louder please the import of of multinomial. Where is multinomial? Line 25. Multinomial naive bias. It is a very simple, it is the naive bias classifier for multi distinct data. Where are we using that? Oh, so remember this I don't use for topic modeling but I said that just to see how topic modeling is and whether in the whether the ground truth of the data is that the topics are well separated. Remember we made a classifier here a basic classifier. Let's go here see I created a very basic classifier. What does a classifier do see I could have used something else I could have used logistic regression or random forest or decision tree or whatever the reason I used a multinomial live bias is because it's very fast do you notice that it is a cheap model a model finished building in just 37 milliseconds. So I wanted to just use it for basic classification to get a sense if the topics are well separated out in reality, whether the ground truth topics are well separated out. And it seems to be, you know, with the 83% F1 score, which is quite good, topics do seem well separated out. Yes. You got that, right? Okay, so let's get back to imports. By the way, no particular use name. If you don't like multinomial just use logistic regression use to say use whatever that you feel comfortable with it doesn't matter okay anything else guys I saw what this bunch in line 23 ah very good question so what happens is when you use the psychickit-learns dataset, you know, the datasets library coming from somewhere here, I have scikit-learn dataset. Yeah, line 22. Line 22, exactly. So this function, what it returns is it it returns an object which has, whose member variables are, actually, why don't I, why am I just saying that? Let me just go to that. SK learn bunch. Let us do that. SK learn bunch. 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11 here. It contains, it's a hash map, which contains the data, you know, the text, it contains the target variable. What is the target? This, you see that, right? It also contains some explanations and basic things, some text and so for metadata, that's it. So think of it as just a hash map of where the data and the labels are kept as key value. Text and labels are kept as key value. What are the labels? One, two, three, four, is that the label? No, no, no. That is something I just created. 20 news groups, right? Labels are the name of the news group what is the target variable what's the difference between label and target it is the target variable the the words are interchangeable when people use the word label target variable output these are all interchangeable words think of it as target actually they use the word target this in the bunch now all of these values are kept there so do you you see bunch dot target names will tell you all the target names, the unique values of the target. So bunch is a hash map of what? It is a hash map which contains the text. There is one list of text, which is the data. The key is the data and the value is a list of all the English you know content and then there is another key which is called target whose value is for each of the email which news group it came from are we getting it so guys do you know what a news group is? Or maybe you guys are too young at this. You don't know the world of news groups. News groups are like Google groups or Yahoo groups. The word news group, it harks back to 20-25 years ago, where most of us used to subscribe to news groups and we used to keep getting messages and posting messages on news group alt dot news dot exactly exactly so the thing is these are all news groups and the equivalent today is the google group or yahoo group or whatever it is or maybe your whatsapp channels they are like that so what happens people post messages there, right? And messages are on certain themes, or think of it as a slack channel. You have a channel for ml 400, you have a channel for different topics. So 20 years ago, the equivalent of these things used to be on the Linux machines on the Unix machines, they used to be this news group, a new server. machines on the unix machines they used to be this news group a new server and you could have you could subscribe to this channel these news groups the protocol right nntp yes exactly you remember that training yeah you're from that generation and it's i mean even today you can install it and I hope it's free. You don't have to go and use all this fancy stuff. So that is it. Does that explain to you? Is that for you? Yeah, so it is, it has target label and what's the other target? So one is like the document itself let us do this let us go here and look at the documentation and i'll show it to you what the member variables are and this is just a directory structure in which people organize documents for yeah so when you look at the returns and okay a bunch it is a dictionary so look at this a bunch is a dictionary like object with the following attributes the one key is called data what is it it's a list of texts what looks easy yeah and each text you know comes from a news group now where is the news group stored target you see target it is a list of labels target labels and the news names right then file names is not very relevant. It just tells you when you download this data, where on your machine it sits. Right. And it will sit in your home directory under something called sklearn data. You can go and check it out. There is another field called description. It will give you the description of the data set. Now there is another field called target names. Right? It is the list of the tag. Now target names is nothing but the unique values, distinct values of the target list. It is nothing but calling distinct or unique on this list. If you call unique on it, you'll still get the same thing yes is it enough yeah that is it so that's how we understand that so i'm just printing out the target names that is that and then just do a count plot basically do i hear a question from somebody? Hi, Asif, this is Sudhir. I just wanted to check with you about that. We have the 20, one second, that fetch 20 news group, right? So is that a function or how I didn't quite get that. This comes built in with the scikit-learn. If you call this function, see they give you, the scikit-learn comes with some standard data sets because the 20 news group is a very, it's like, you know, do you remember the breast cancer data set of the California housing data sets? Yes. So what happens is all the famous data sets that we use for learning, Scikit actually makes it very convenient. It has the Scikit data set, which has a list of all the famous data sets here we go yeah our data sets and the psychic data sets loading utilities so do you see low diabetes data set load iris we remember doing the iris data set the wine data set the california housing data set, and the 20 news group data sets. So all of these are just famous data sets and you just use it. Yeah, sure. Thank you, Asif. Asif, one question. In line seven, you also have targets and then you mentioned dictionary. So what kind of, what is this code type how have you written this i've never seen that before like targets and then then you have to write number nine number seven and then you mentioned dictionaries separately what happens is most people they would okay are you familiar with the empty dictionary like this somebody creating i understand empty dictionary but why do you like what is this way of is that a way of naming the dictionary targets is the name right so here's the deal you would you would you would see people in python doing just like this exactly i don't i tend to be more clear i tend to tell what type it is so that later on when you're reading the code people never get confused that it's a dictionary or what is it okay i'm just being explicit do you notice that i'm being explicit here also there is no reason to do this specify the type right but i could have just said bunch is equal to fetch this but i wanted you guys to know what kind of object bunch is that it's an instance of the bunch class that's why i did that see it's called type hinting in python when When you do type hinting in Python, there are two things, two benefits. First, you make it much more easy for a reader to know what your code does, right? What those variables are. And secondly, if you're using a tool like PyCharm or some any id then when you do type hinting in return the the code completions and assists code assist becomes much more powerful it begins to resemble a little bit like the power that you get with java or any of the typed languages right when you when you write it in a code editor, you will see how much power comes. So type hinting is, I would strongly suggest that people do it. In Python it is optional, but your code will be a lot cleaner if you do type hinting. Is anyone of you familiar with type hinting or doing it doing type hinting in your workplace anybody here nobody here oh interesting so you guys write python code without typing as if actually i learned type hitting you know when you were uh helping us with the python class so which i found it really helpful you know sometimes uh you know when even 100 data set right when we were working on uh we used to get confused where uh you know the uh explicit typecasting we should have done but we should we would have missed all those things so this really is uh helpful yes yeah thanks for saying that so all right guys so this is a simple count plot isn't it no particular reason i used uh seaborn seaborn just makes it prettier you could have used matplotlib.bar used matplotlib.bar. So what is it counting here? It counts how many records belong to each of the target types, to each of the new scripts. So it just takes a length of the target group? No, no, no. So how many, see in this row data, no? In the data that you have, how many rows belong to, how many items of data belong, how many emails belong to alt-atheism? So it seems to be about 490 emails seems to belong to alt atheism. You see that, right? Other ones seem to have a little less than 600. Target will be the news groups, right? It won't be the target news. Yes. Yes. Targets are the news groups. So these emails belong to those news groups, right? It is showing you the distribution of emails across the news groups right but in this case like one won't be a teaser right it will be some news look it up yeah look at the table here one is it no but that isn't that target name so i thought target name was the label and target was the news group label and target was the news group no no no oh goodness um see target they call the variable all i'm doing here is all the target values in the group which news group they belong to so i'm grouping all the emails by the by the news groups they belong to as simple as that are we getting that yeah because you use target names after and you're using target that's a variable no it's just target names is is the distinct value of targets is the distinct value of targets is the target in the bunch in the bunch look here well okay basically right target names will have it is just a unique list of all the possible target values the variable name is target these are all the values that that target could take the y variable if you think in terms of x and y y is target these are all the distinct values that target can now the about 490 such rows right and so forth that is it so 490 email seems to belong to that group now is it more clear yeah so see if i have one quick question, so this entire thing target dot not like if you scroll up a little bit. So this entire thing talk dot politics dot this entire name is the target name, right? Yes, it is the name of one news group. So let me make it very real for you. Let me take one of these comp.math.software hardware. It's course somebody may have archive of these almost surely let me let me go here and see where yes calm do you notice that here is a web archive somebody has that archive of this. Do you see, what do you see? This is a message board, no? Where people have messages. Do you notice that there are emails and conversations back and forth, and people are discussing things? Yes. as simple as that so the name of this mailing that what in modern world you call discussion groups or discussion boards or whatever it is the the word or Yahoo groups or whatever you call it the name of this is calm dot sis dot math dot hardware right which is what you see here you can take any other one of them for example let's take sock thatreligion.christian see if there is such a thing in the world name that oh here we go so well you notice that they are somewhere or the other somebody would have archived this sock.religion yeah here it is once again people are talking about conscious intent you see that they're talking about this religious concepts i think i don't know why physics has come in here but okay so are you guys understanding what a news group is are you is it clear now? Yes, yes. So it is as simple as that. These are the names of the newsgroups and each record is an email or a message in that newsgroup. Somebody posted in the newsgroup. That's all. Look at the data. When you look at the data, you'll see it. Do you see this data? One data says, I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a two-door sports car, etc. And it belongs to the rec.auto target group. Right? auto target group, right? Then there is something called a fair number of brave souls blah, it belongs to this. Mac hardware, do you see this? Is it not becoming a bit more clear what the concept of this news group data is yes that's what you know just a message in a news group and how many such messages are there 11 314 and so what i'm doing is i'm merging some of the records and you can create your own grouping. You can play the games and group it together. Then lastly, what I do is I create a new column called topic. I leave the target column there. I create lesser number of topics. So how many topics do you see? Politics, religion, sports, computer, science for sale. Six topics, right? create lesser number of topics so how many topics do you see politics religion sports computer science for sale six topics right you create your own number of topics if you want you can keep all 20 topics see how well it works right then what do i do here so is is this code clear guys? What am I doing? I'm adding one more column to this pandas data frame called topic, right? Where I'm creating these topics to be based on our own choices, right? We are saying that don't give it this name com, sys, mac, blah, just call it computer, right? calm graphics is also about computer am I making sense guys or is it all looking very obscure anyone yeah it makes sense make sense now right okay this is it it's like Craigslist also, right in your way. Exactly, it is like Craigslist. Except that Craigslist, of course, it is about advertisements, but something similar. It's very similar to that. Yes. I guess you guys are the young generation. You haven't seen the old news scripts. We used to live off that. I guess you guys are the young generation. You haven't seen the old news scripts. We used to live off that. For most of our things, if we wanted to get information, before all these powerful search engines came about, news scripts were the way. So stop words. Let's look at this element. What are stop words? Do we all understand? Anyone who wants me to explain that? Stop words. Sorry. Let's say, do you see what the stop words are? I just ran this code. What are these stop words? Just looking at it, can somebody explain to me what the top words are? What are they? Common words. Common words that don't add much meaning. They are the glue words that help explain or the structure of the sentence conveys meaning, but it is not, it doesn't convey the theme of the sentence in any way. So those are the stop words. Then I'll remove this from my thing. So we need to clean this text. So this is a thing about how do we clean this text. And I showed you the utility. What I'm doing is for every token, what is a token? We talked about token. So this is a couple of sentences, isn't it? So in this sentence, when we have a text, what does the word token mean? Tokens are things that you create out of by understanding the text word by word. Roughly speaking, one token could be one word or one punctuation. Do you get that? get that right so for example this full stop is a token this word is a token all of these are tokens so think of them as words and punctuations are tokens right so it will have a lot of tokens now these tokens some you want to keep some you don't want to keep you want to not keep the urls you don't want to keep email addresses and so forth are we together guys does that make sense right if i want to clean out this text would you agree that this is much better when it comes to the actual semantic content of this text this is a better it is better to just take these words out so far so good right so one one little comment here I, I don't know if we can solve it here. So I've been following the lesson together by running the lab. But every time I go through this clean text, my Python notebook seems to get corrupted. So I have to figure that out. But I was wondering if you have any insight in terms of what's happening inside clean text, which could be peculiar based on implementation. I have never experienced that. Clean text is just a very tiny function. Just this. Okay. I give the word keeping, then you use it. So if you are having a problem, let me know if anybody else is having a problem. See guys, I do understand that there are problems. For example, yesterday we are struggling with another person, Shini, his laptop and so forth. So I have a bit of a handicap. I only work on Linux and I use Windows only when I'm teaching, like for example now. I'm not terribly familiar with Windows, but familiar enough. I don't know Mac. I'm guessing Premjit, you have a Mac. Yeah, I'm on a Mac, yeah. Yeah, so Mac is my handicap. Never used it. In my life, I never moved away from Unix slash Linux. I worked on all my life on either wax clusters which most of you don't know about or Unix machines. So my day-to-day is on Unix and when you do much of this machine learning on a Linux machine, the Ubuntu for, is a defector standard. Almost everything works flawlessly on Ubuntu. When I hear all of these issues happening, sometimes I'm able to debug and find what's happening on other people's machines. Sometimes I'm not able to. I just help say that, take this Docker container here, everything is properly set up and just fire this up yeah i'll figure this out as if and i'll post it on our group or if i don't prachi might figure it out so all of you mac people mac fans or apple fans you guys need to club together and help each other out and take Prachi's hand. But I can't imagine why something simple like this should bail out on you. So, all right. So we clean this text and as you notice that this text here looks a lot cleaner than what was there before, isn't it? Look at the text here from article blah, blah, blah blah and look at the content here. I hope you would agree that this looks a lot cleaner. Guys. This has a good good list of lists ever create any issues later on. Do we need to keep that in mind always while processing the data downstream? No, no, no. See, I created into a Pandas data frame now. Do you see this lovely Pandas data frame? Do you notice that I converted the news groups, news apply. This news group, so as a data frame, where did I create the data frame? Yeah, a little bit up. Do you see me create our data frame? Yeah. So once it is data frames, now we are not still having it like that, but we will create list of lists where needed. But list of lists is very straightforward in Pandas. I mean, Python is commonly used. list of lists where needed but list of lists is very straightforward in pandas i mean it is commonly used but you use list of list up there right uh just one line above where in news groups then you use basically it's clean text and topic in line You are saying that from the news group pandas, create another data frame keeping only these two columns. It's not a list of lists. But then why wouldn't you just keep clean text and topic instead of keeping it under a list? you're keeping it under into two parts so that the meaning is clear columns columns to keep is equal to let me do that and see if this makes you if it makes it easier for you to understand columns No, I understand that. But what I'm saying is why keep clean text and topic in the list? Why not just keep them within say you have newsgroups and us? Yeah, just for no particular reason. I just wanted to keep a smaller data frame in which I have only the things I need. No, but if you just add clean text, comma, topic without the two brackets, it won't work, right? Oh, it won't work. It is just the Pandas. That is because it's a list, right? Yeah. So Pandas says that when you project, when you create another data frame with the limited columns, you must give the columns as a list. And the symbol of list is that square bracket. I thought the reason of reason you're using list is clean text generates a list. No, no, no, it's not a column. So this is basic Pandas syntax. So. That is that. So. syntax. So that is that. So now you have a simple data frame with only two columns. Right. Now we go through and how do I know how many rows there are? You can just call the length on it. Then it has 11,314 rows in the clean text. Again, we haven't lost any rows or something. Next is, so if we write a simple, so first thing we do is you can't do anything with text. Remember, we need to convert text into a vector space representation. So in this line, we are just vectorizing the text, which column? This column, right? A clean text column. It needs to be vectorized. Do we remember that, guys, from last time I said that words need, I mean, sentences and documents, they need a vector space representation because machine learning only works with vector spaces. It is a mathematics on vector spaces. It's basically linear algebra, right? A lot of linear algebra. You need to represent it in a vector some vector in a Euclidean space so guys tell me if I need to explain that concept over again that is clear isn't it mistake what is data dot assign oh it is just a way of creating a new column right I said I was just looking at the TF idea of the numbers they don't look like it's like one vector there's zero and then some notation is strange ignore the notation so the notation the way it is done is strange. Don't worry about it, but in your mind, think of it as the vectors the way I taught you. So basically what it means is this particular word whose is this particular word whose name is this peculiar thing, it has this much value. So remember, each word's TF-IDF value is this. And when you do the TF-IDF, it's a document. So the same word is there for all of them, maybe because it's sorted, is it? Yeah, yeah, it is just sorted, that's all. So ignore this part. At this moment, this thing that is here, don't pay too much attention to that. So this is it. By the way, this is just a little syntax to put so many columns, so many rows. One of the things is it is 150 here. And this is it. So you represent it. So this part is clear now, right? This, by the way, is a standard. So besides the TF-IDF, which is a vectorizer to create vectors out of what? The clean text column. Each of these converted into a vector, TF-IDF vector. So now you have a lovely space. You have the vectors and therefore you can do machine learning. Remember our standard machine learning, the X is the input, little y is the output. Output, we know what it is. It is the topic. And the input now are is the output output we know what it is it is the topic and the input now are the vectors dot vectors a document like a text document being one email has been converted it into a vector now what i'm doing is i'm doing a split between test train the 11 000 rows so i approximately give or take and so i'm taking the first 7,000 to train and the remaining 4,000 odd to test. This is a test train split. Is this simple guys? This is basic array, the list notation, the indexing notation of Python. of python what is data.topic remember look at the data this is your data frame right this is really your y right this is your target variable isn't it in a classifier if you're doing a classifier this is your levels and this is your input clean data except that it can't be the input the tf idf has to be the input it's a vectorized form once you have the input as vectors doc vectors column is here the tf idf you use as a like you can basically here's the doc vectors you created now for displaying i put it in the pandas you don't have to even the dark vectors you created now for displaying i put it in the pandas you don't have to even put it in pointers here it is right directly i used it here first seven thousand rows are used for training second thousand rows then i create a basic classifier by the way why multinomial and we go use whatever classifier you feel comfortable with no reason but then the rest of the lines does it look very familiar guys you could do matrix dot accuracy matrix dot whatever i just use f1 score f1 is what can just can somebody remind me what is an f1 score guys is it the harmonic mean yes it is a harmonic mean of precision and recall. Just think of it, you think of a standard mean, right? It's just a different kind of mean. You may have learned in school the arithmetic mean, the geometric mean, the harmonic mean. So it is the harmonic mean of the two numbers. So 83% which means that the topics are well separated out. Any questions here guys in this section? Are there any questions? Asif, can you please summarize christian and record i kind of get the um it's tough but i don't know the exact formula oh the exact so i'll tell you guys there's a way there's an excellent reference that you should use for this which where a picture speaks a thousand words kind of a thing. Let's go here, Wikipedia, they have a lovely page for this. Do you see how it is given here? Now, please pay attention to this picture here. Please pay attention to this picture here. Suppose you have data. This side is the... Let's say that the left-hand side is made up of positive values. The actual ground truth is positive. And the gray area is negatives. True negatives. This is false negatives, means it's positive. So now look at this. You have true positives, false positives. Green is positives, true positives. This is false positives for a classifier. So what is precision? Precision is green over all the positives. So let's be very particular. Suppose you have a test and you need to determine how many cars, since you like cars, you're standing on a bridge and you have a little device that looks at the picture of a car going by and tells you whether it's a sports car or not. So your device will get, now let's say that a sports car actually does go by. Your car your your device says it's a car it's a sports car true that is an example of true positive isn't it on the other hand let us say a minivan goes by and you mark it as a sports car so that is an example of false positive isn't it It is not really a sports car. So precision is how many of the things that you marked as sports car are actually sports car. Do you see that? Think that you marked a sports car, what proportion of them were correct, were genuinely sports car? Are we together? Yeah. Right. And the recall is of the ones that you marked as sports car, right? How many, like how many sports cars went by and you couldn't detect them so this area represents a true sports car so let us say a nice convertible a V8 engine went by and your instrument couldn't detect that it's a sports car it missed it so it is how many it recall is how many you caught versus how many there were. Right? So of all the sports cars that went by, how many did you catch? That is your recall. Is the meaning now clear? Yeah. Right. So that is precision in recall. Keep this picture in your mind, guys. It's a very simple picture and it'll help you. It's really nice that actually have all the books that I have seen from this Wikipedia picture to be the simplest and nicest. Kavita Tankha- So now guys do this four lines of code. Do they look easy at the straightforward. Do they look easy? Are they straightforward? Anybody needs clarification on this? This is straightforward. Zipf's law says when applied to NLP and word frequencies, it says that the rank of a... So suppose you rank all the words by their frequency the most frequent word being ranked one right which would be a word like or something like that and the least frequent word that happens only once being ranked the talent right having a much lower rank the talent right having a much lower rank then it says that the frequency of a word is is proportional to some power of the inverse of rank forget the power part roughly speaking the frequency is proportional to the inverse of the rank. If you ignore the power business, inverse of the rank means the bigger the rank, the smaller the frequency, and there's a one over R relationship. Now those relationships, they look a certain way. They look typically one over X relationships look like this. This is a Zipfian distribution kind of a look. So we're just checking whether this is true. And so all I'm doing is I'm making frequency plots and then plotting it out. So I'll let you stare on this for some time and tell me if anything looks odd here or needs explanation. Asad, if this is not true then what does it signify? To me it would signify that the words actually don't come from natural language. In other words, genuine language. They have been manufactured by a random generator. See, suppose you take a random generator to pop out words. What will happen? It will produce words with equal probability, right? Or you take any other distribution. So when you look at genuine text that people use, it tends to have, this is our observation, that law is not a law, in other words, it's just an observation based on human languages. It is so. See, think about it, how does a child learn language? The child in the beginning just picks up the important words, hungry, food, whatever. It picks up some words and then as the child grows the vocabulary expands but a natural expansion of the vocabulary is the child will pick up the most commonly used words first. And generally a very limited set of words are sufficient for you to become capable of speaking a language. If you really think about it, if you can pick up a few hundred words in a language, any language, you can pretty much become conversant, you know, listen to that language, be immersed in it, and have a vocabulary of a couple of hundred words. You will survive. five million English words to survive in the US. As a person, I mean we come from English-speaking countries, but if you look at people, I mean I wouldn't call it Indian English. Indian English is a word, Hinglish. So we come from an English-speaking world which has an overlap with English. So we don't struggle, but if you look at people who come from non-native English speakers, I mean different words like China or Korea or Japan, and you see how they learn. Their vocabulary in the beginning remains small and over time grows. And so they pick up vocabulary of only those words they need in the beginning. And gradually they expand out. So anyway, this is just verifying that it is roughly true in this corpus. Topic modeling with Gensim. So now what we do is we start modeling with Gensim the topic. First of all, we want to figure out how many words are there and which ones to keep. So we define the words to keep as those words which are common enough right we don't remember we threw away words which don't occur at least twice in the vocabulary rare words you throw it away because you know it's too rare to have meaning and then you have those words you create a dictionary a limited dictionary of those, only those words that you care about and that you find in your corpus. And once you have your own special dictionary, it turns out, and as you can clearly see, your own dictionary is a very small dictionary of 20, not very small, but 28,000 unique words. There is a facility called filter extremes which you could have used to remove common and rare words very common and very rare was the two extremes i didn't use it because we had already as part of the data visualization here already removed the we have already removed the stop words and we have already removed the most we have already removed the stop words and we have already removed the most the rare words so finally now actually down here finally we come to topic modeling right so how do we do that it's very simple two three lines of code that's all it takes so here first what we do is we take the words and we create a tf idea vectorization of the word so till now the tf idea vectorization we did we you did it with spacey right but now we can't uh gensim again this is a problem that you have to realize that every library will have its own tf idea vectorizer uh scikit-learn has its own tf idea vectorizer and gen sim has its own uh and then uh spacey has its own nltk has its own so you have to watch out you can't feed once uh vectorizer into the other they are not expecting it right they're not compatible with each other so here we're doing the very basic things we create a bag of words and then we vectorize it gen sim happens to need these two steps approach to do it so here we go we we generate the tf idf do we do we understand the guys thing guys right tf idf yeah simple so we take the tfID of Vectorizer, apply it to the corpus, bag of words, and we get the TF-ID. Any doubts in this code, guys? I'm only keeping those texts for text in the dictionary. For every line of text, we are creating a bag of words. And so you end up with a list of bag of words and that would be a bag of words corpus. So remember, once again, to recapitulate, if you remember what a document is, document is any body of text. That's why you have doc to bow. Bag of words was a simple vectorization that we talked on Monday. Every word, the only question you ask is, is it present or not? One hot encoding kind of thinking or then the frequency of the word and so on and so forth. You do that. And TFIDF is frequency of the word and so on and so forth. You do that and TF-IDF is basically saying, so bag of words is in that document how often did a word appear and so forth. Now TF-IDF is, we talked about TF-IDF last time, so we won't do that. Term frequency inverse document frequency. It gives you the sort of importance of a word based on how often it happens in a document, and weighing in the uncommonness of the word. So that is it. So once you have that you have TF idea vectorizing, you get the TF idea for the whole compass, then comes different ways of doing it. Last time I taught you latent semantic analysis or LSI, another name for it is, and LDA, latent Dirichlet allocation, which is something, the inner details I haven't explained, I'll do that next time. Do that, and just as a reminder, these were the topics. So what do I do? I apply the model, very simple, do you notice notice that I apply the LSI model to this? It is almost like dot fit. If it was scikit-learn, it would have said LSI model dot fit. The corpus and that is that. Asif, can you explain the dictionary part from which you started from Jensen, dictionary.doc to bag of words. Yeah. See, what is the dictionary? It is your vocabulary of actual words, isn't it? So what it does internally is for all the words that are found, it attaches an index, right? A location. Because if you create a vector, let's say that there are 10 words in your vocabulary. So you will have a vector, you have an array, right? With the indexing 0 to 9. Isn't it? And each location, each index location is for one word, one of your 10 words. Are you understanding that? Yeah. Right. Okay. Let me just put it and draw it out because I think you guys are forgetting. So let us say that you are here and you want to, so suppose you have a word, the words, the only words in your vocabulary are cat, dog, horse, giraffe, zebra, right? These are the, and cow, let's say somewhere there, cow. These are the only words in your dictionary. These are the, it's a zoo. And the only words you ever talk about are this. Kids do that, right? It's a zoo and the only words you ever talk about are this. Kids do that when they grow up. Quite often their vocabulary is filled with the names of all these cute animals. So then suppose there's a sentence, cow, dog. Cow, dog, horse. A child says cow, dog, horse. How will you write it? You could write it like this. You know that you give to it the index 1, 2, 2, 3, 4, 5. You could say cow, 1. Dog, 1. Horse, 1. No cat, no giraffe, no zebra. Do you see this this is your doc this doc gets represented this sentence gets represented as this particular thing this vector am I making sense yes it is a writer's created a vector in which for Yes. Mr. Kim, I just created a vector in which for each of the location, each of the index location belongs to one of the words. So every word has an index location. Cow has zero, horse has three. Then given a document, all you need to know is how often any one of the words have occurred and you just go and update this vector so so if the word is not in the dictionary it won't be captured yeah that is the point so that is why we created a dictionary so so the important thing is important create a dictionary create your oh yeah we actually created the dictionary. It's not some pre-made dictionary. No, we created a dictionary out of the corpus. Oh, remember that. When you say corporate or dictionary, it actually creates a list, a sorted list by whatever the, you know by whatever the you know whatever the code is in Jensen creates a sorted list that is right essentially right give the location basically what it when you give it a corporate corpus of corpus of words or documents all it takes is that it finds the words and then all it does is for each word it first gives it an index location right do you see this it gives it an index location it says cow will be zero so it will internally maintain something like cow uh one cat do you get the idea this is your dictionary but I'm guessing the actual code for the actual mechanics of the condition is pretty complicated tell me you don't want to because they it's as far yeah there's a little bit more going on you can read the course not very complicated but it's it is there's a little bit more happening than at the conceptual level that i explained okay and this dot that you saw is essentially your bag of words like uh if you look at this doc this is a doc right but it is not vectorizing right it is not vector it is a vector look at this what is this but don't we vectorize it afterwards so there are many different representations remember i said that the vector space representation last time by the way were you there on monday yeah yeah i was i i'm just trying to understand so right now i think it just gives a index right but doesn't doesn't create the tf idf or one hot encoding value vector space representation here's a summary again vector space representation what it means is you take a string of dog cow jumped over the moon well here since those words are not there in our vocabulary, let me just use cow, dog, horse. This is a sentence. The purpose is to take the sentence and uniquely map it to a vector. What is a vector? In simple terms for computer scientists, it's an array of fixed length, isn't it? List of fixed length. So it list of fixed length so there are many ways you can do it the question is first of all the indexing is the this is the 0th index one index in a array you know this is how you index the array isn't it that is simple to you right you realize that arrays are indexed by this notation if I give you a list and you say three what does it mean it is the fourth element in that list right so indexing is given and now the question is how do i map this sentence to this to make it into a vector because this now for example belongs to a five dimensional space right this is a vector in five dimensional space so long as i put numbers in these locations floating point numbers in these locations i can do that one easy one easy way is just count the frequency of each of the words so cow was there uh dog was also there, cat was not there. Then let's say horse was not, horse is there in the sentence. So here we go. One here and then let's say a giraffe was not there and zebra was not there. And say suppose I extend this sentence to dog horse dog then what happens where is dog two or two location so i convert well okay that is confusing let me not have two dogs because it will look uh dog let me just make it uh cow again cow so what happens the car location now becomes two. Right? Do you see that this is a vector? Guys, do you all see that? Right, it's a vector, it's an array. It's a list of fixed size. It is a point in a five dimensional space here. Words, your word space. Now- You can do a vector which is, which can be used by the TFIDF. Yeah, exactly. And now, so this is a halfway station. This vector is your bag of words. Bao. Bag of words. words in a way and in a way you know what you can do what people do is they don't associate too much like there's no order associated so internally you don't want to store all these zeros because that leads to very inefficient storage so you might just keep it as a hash map you can just say only the locations that do exist, for example, zero maps to two, right? One location has nothing, a two location has one, and a three location has one. So when you represent this with this, the same thing as this, do you realize that a map is a much shorter way of representing it? You're not keeping all the zeros there, right? You're only keeping the non-zero values. You can keep it as a, so that is called a sparse representation of a vector. The way you think of a vector as an array is a dense representation of a vector. And a sparse is like a dictionary, like a hash map. Are we together? And now from this, another representation that you can get is you can get a TF-IDF representation, right? TF-IDF vector from the bag of words. If I have a bag of words representation for every document in the corpus then i can for each of this document now for cow dog horse cow i can produce uh the same five-dimensional vector but now the values would be are the tf idf values term frequency inverse document frequency let's say three one whatever it is uh those uh you'll have to compute that but okay that's that am i making sense now yeah so uh as if that tf idf you have two lines you have one with models dot tf idf and then you're using the corpus the tf idf models the tf idf just creates the tf idf vectorizer actually the api is much cleaner for scikit-learn can i show you the same code for scikit-learn for a moment right because in my view right gen sim the notation it uses is a bit confusing let me show this thing it will be more uh and this is one of the things you know every guy uses his own notation their their library becomes popular now you have to get used to their their stuff so let me show you the same thing how it is done for example i think scikit-learn is always uh Excellent in clarity of notation. SK Learned, TFIDF Vectorizer. See, let's look at an example of a Vectorizer. User guide. Let's go to the user guide. Do you notice that given a corpus, so here it is, this is a corpus of text documents. I'll increase the fonts a little bit. Just pay attention to this paragraph of code. We'll pay attention to this. Now, what are we doing? This is a corpus. We agree this array of text, each text is a document. Nisarg, are you getting this? Yeah. Then we use the simple count vectorizer, your bag of words kind of vectorizer, and you fit it to this. So you will get representation the way i explained it to you then you can do more you can go to the tf idf you can even do bigrams and so forth using stop words now here is the tf idf it explains it but forget that you already know the explanation the bigrams and stuff would happen before right before vector no we haven't done that forget bigrams for the time being they just add to the vocabulary just focus on the words now think about the tf idf vectorizer do you know there's a tf idf word is here and forget all this long explanation and what it does is it helps you create a key like actually hang on at some we need to look at some using sparse features um some code example code yeah vectorizer print vectorizer comfort transform okay the example this is not a very illustrative a page let us go to another page that shows you a good succinct example you don't want to read so much here okay so look at this you have a TF vectorizer right you can create a TF vectorizer here found vectorizer and so on and so forth and then how do you create a TF TF idea vectorizer you can again give the word the thing you create the vectorizer instance you haven't used it you have to fit it to the training data when you fit it to the data it becomes what you want it to be right so you instantiate it so now coming back to it this line where am i this line therefore what is it doing it's just instantiating your tf idf vectorizer they call it the tf idf model right and then you apply it you get the tf idf on the bag of words so it's just a strange syntax that's all it is so i hope by now clarified. So read the documentation in this. It will clarify it. It's just GenSim's own language. The latent semantic analysis is the model, the SVD-based model. You create the model and again it has the syntax which is odd. It's unlike scikit-learn would have written a slightly cleaner syntax but for what it is worth it is the creators of this library made it like this. So that's that. So now does it make sense? All you're doing is you're giving it the corpus or to a model, to a machine learning model, to do unsupervised learning, to discover topics in it. Right? And then you ask it to show you the topics and it shows you the topics right and it shows how the topics are made up of the words the important words they are made up of so far so good guys right so it's 10 o'clock. Let me wind this down. Nisarg, are you getting this? Yeah. Straight forward. And the same thing you could have done LDA from a coding perspective is just a change of word instead of LSI, you change it to LDA word. You change one word and you repeat the same thing. Except that these two algorithms are far different. LDA word. You change one word and you repeat the same thing. Except that these two algorithms are far different. LDA is a very powerful algorithm. There's a measure. How do you know how good your model is? You can do the model perplexity. Now I invite you to read the documentation on this and on coherent score. Coherent score is often used. Now this visualization tool is very good. It helps you see how good a model you built. It is okay okay. I wouldn't call it terribly good. You can improve upon it. One and four seem to be well separated out. Two, three, five, six are pretty close to each other. Then I suggested that use mallet. Mallet generally outperforms ginseng's built-in. When you do mallet, you get a separation like that which looks cleaner, much more separated out. And that is that guys, that is it. That's sort of your detailed walk through the lab. So guys, here's the thing. At this moment, I would strongly suggest if you have not given time to reading the Spacey book, you must. We are two weeks into the NLP workshop. I'm going to go into deeper topics now. I'm going to go into, next time would be the word embeddings in detail and then the week after that, the next three weeks will be all transformers because the world has changed. It's all about transformers now. And we'll be introducing more and more libraries and bring in hugging faces and so forth. So transformers. And we'll do a lot of code. This part two is all about hands-on coding. So we didn't get time to get to the project. Maybe next time we'll talk about it on Saturday. We'll talk about it. Guys, if you want to get most out of this workshop, do the labs and do the project and read that book spicy book. You can actually finish in two three days Right or you don't like the book go to the website and just read the documentation it's very well documented Likewise go to the Jensen website and read the documentation for that communication.