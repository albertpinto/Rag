 This paper actually talks about an interesting concept which has been emerging these days. The concept is this. We have optimizers in deep learning. Now what are optimizers? We'll talk a little bit about this. It's one of the topics that I haven't covered are optimizers? We'll talk a little bit about this. It's one of the topics that I haven't covered very well. So we'll talk about different kinds of optimizers. And then that will be our first topic. The second topic would be that we'll talk about is this paper itself, Learned Optimizers. Learned Optimizers. By the way, this paper I picked, but there's more body of work now happening in this space. And I thought this would be a good starting point. Can you record the session? It is being recorded. Hang on. I'll just verify that it is indeed being recorded it is recording it shows yeah it is recording it is recording guys and so all right what are optimizers you know that when you do a pie torch code a pytorch code we always write this code optimizer is equal to sgd or batch gradient descent then we do the stochastic gradient descent still sorry stochastic gradient descent still sorry stochastic gradient descent and then of course the mini batch which is the real one that we use mini batch the gradient descent all that they differ from is that when you compute the loss what you take is you take a certain amount of data right a data set a small batch of data or mini batch of data you can say. Let me just use the word batch of data. If your batch of data goes in, right, you feed it in into the model, what will you end up with? You'll end up with y hat. Now comparing it to real y, you come up with a loss, right? So the loss could be the sum squared loss for regression. It could be the, it could be either the bin squared error, or it could be the cross entropy loss or something like that. It doesn't matter. Some loss function. You end up with this loss function. Then from this loss function, so this is your journey your journey is you get y and let me put y here you get y here y hat but because you know you already know why from this you can compute the loss right from the loss you can compute the gradient of the laws gradient of the laws with respect to the weights and biases with respect to weight and biases after and your event right and then you back propagate this gradient prop the gradients and then finally then you make the back prop the gradient so let me just write the back prop as then okay let me just write it there as one word back prop gradients and then finally you take the step the final final step that you, if you remember in the code also you did, the loss, loss.backwards, you know, then you did the, that would compute the gradients, right? You had the optimizer to compute and so forth. So then finally you would make the step, the step forward, like, you know, when you would do the step method on the optimizer, what would it do? It would take each of the weights or biases, right? W next is equal to the current value of W minus alpha gradient of the loss, right? In some form, this is it, right? And the same thing applies to the bias as the bias term. this is it. And the same thing applies to the bias as the bias term. So I won't mention that B minus alpha gradient of loss. So are we together guys? At each step, if you have the gradient of the loss you get to a better value of the weight so the journey therefore is you you pass a batch batch of data and i will worry about what is a batch of data you pass it to computer prediction y hat now given the y hat you compute the laws whatever your favorite loss equation in the context is right for auto encoders it's contrastive laws it is i mean reconstruction laws which and quite likely it's some square data mean square data right it's sort of like a auto like a regression kind of loss function regression is of course again mean square error plus some regularization terms and whatnot and then if it is a classification then you have cross entropy and so on and so forth so there's a whole variety of loss functions so we won't get into that the loss functions belong to specific problem domains that you're trying to solve right so now comes this question, if that is true, then this, what is the batch data size? If batch size, batch of data of size S, let us say, if S is equal to one, it is stochastic gradient descent. If S is equal to the total number of data points, the whole number of data points, it is batch gradient descent. If s is between one and n, like it is something like eight or 16 or four or some 32 or whatever it is, then it is called a mini-batch gradient descent. And these days, we all seem to know, for all the reasons that we discussed in our previous sessions, that mini-batch gradient descent is good. Now, while this is where we left off optimizers for the time being, there is a lot more to it. What happens is that when the lost landscape is highly complex, the momentum, so suppose you have a point here and you have a lost landscape, even in the local neighborhood, it's peculiar. What will happen is because you're dealing with many batches of data, at one point, the gradient may point this way. So you may end up there. Then point the gradient may point this way so you may end up there then another moment gradient may point this way so you take a step somewhere you know you zigzag your way right you zigzag your way sort of i'm exaggerating it but all right you zigzag your way towards the final value to the final optimal solution and sometimes you don't you just keep circling around it and you uh you're sort of beating around the bush with respect to that and it has always been a problem because you remember that each mini badge will will will contain its own little bias and it will point the gradient in some other direction right so how do we solve for that now the intuition of solving for that comes from uh time series data by the way that's a topic that you guys will learn about next time so imagine that you must have seen that when you look at stocks for example right do you see that their value is like, so when you look at this kind of a stock value, and if I ask you, what is the value going to be tomorrow? So suppose you are here, you are at this point, imagine, let me put this point A. If you're at this point and I ask you, what's the stock price going to be? Now there are multiple ways you could answer it. You could say well something near this but perhaps a more reasoned approach just basic intuition will tell you that you are better off answering it like this by saying you know what let's look at trend, a more stable trend through this. This is, maybe it is... Would you say that the yellow line is a much better representation of the stock movement without capturing or without getting lost in too much of the fluctuations right and you may even go a little bit more maybe you can do like this do like this but you smooth over this is smoothing over over the fluctuations does this make sense guys right people often call it the 30-day moving average or you must have heard people in stock world use words like 30-day moving average. So in other words, you take the last 30 days stock value or one week moving average. Or whatever it is, pick a time window, delta and take the take all the values in the time window and average over it at any given point so the value here what is the value at a p you take a you take a 30 days prior to it let's say 30 days prior to it take all the values and you average it and then you'll come to the value that you give it here. All right, when you do things like that, you take the moving average. Now the question is, what is your prediction of the value today? And so what you do is you say that, see, let's look here. You're taking your next value from, let's say that today there is a big fluctuation here. Let's look at this. Today the value that came at this point, at point A is this huge value A, but you are here, what value should you move to? Should you make a big jump along with the data, which is a huge fluctuation, or should you have resistance to that jump? You should just move a little bit up. In view of the fact that on that particular day, let us say that this is December 15th. On December 15th, there was a big upward swing. Should the yellow line follow the white line or should it just slightly acknowledge the fact that there is a rise and it should move to its next value? So let us say that the value of the yellow line on December 15th, it should be based on what? Should it be all of the white value the value on uh let me just call the value on the data as a theta theta on december 15th so you realize that theta on december 15th was pretty high the stock value should you take this to be the next value of this if you do that you realize that you're just fluctuating with the market fluctuations, isn't it? This value is not stable. On the other hand, what you can take is, you can take the moving average, you can take the moving average, which I will call the value, which is of course, because this itself is a moving average. You ask yourself, itself is a moving average you ask yourself what was the moving average on day up to December 15th in other words what is the average of this value in this window right so you know that when you take the new value of December 15th, when you're trying to calculate, it should be December 14th, I suppose, December 14th. So what will you do? You realize that you want to. You want to acknowledge some part of today's fluctuation right some part of today's fluctuation but at the same time you you trust this moving average quite a bit the history the stable movement of the stock is a better predictor of what will happen tomorrow are you getting a sense guys what I'm saying yes right it's a common sense so now suppose I give you one unit of information and how would you sort of break it up you say that I'm going to give a contribution beta right let's say 0.9 I will take the value of yesterday and maybe I will take a little bit of the value from today, a 10 percent importance I'll give to the value of today. What happens is because today's contribution is given approximately 10 percent value, but then 90 percent of the value comes from the past momentum or this past activity right so uh then you have a nice so this word for that is mathematically exponential smoothing and you'll find that whenever you have time series data and things like that it is always true right you you you have you you can use exponential smoothing now there's a bit of intuition how many days are you averaging over how many days are you averaging over over the answer is approximately days, approximately one minus beta. Right. So suppose your beta is equal to let me just take an example. 0.05. Right. Then. Days. Is equal to what is it? One over 0.05 is equal to 20 days okay so you're essentially you have a your V at a given day is a 20 day moving average 20 day moving average are we together this is this is the basic intuition. Now, if the data has very high fluctuation and you want to capture the trend, what you want to do is you want to give more and more strength to beta, bigger values of beta. If, on the other hand, data truly is responding, signal is moving rapidly and you want to catch up to it. You'll take smaller values of beta. You'll give more importance to today's value. It may be a stronger indicator. So you'll take a smaller window, like let's say three day moving average or five day moving average or something like that. So that's the concept of this exponential smoothing of function. And the word smoothing is self-descriptive, right? It smooths the oscillations down. Now, what does it have to do with optimizers? There's an interesting relationship. What happens is that, see, when you have a journey, suppose this is your true minima in the feature space, and you have a situation like this. And imagine that by default this thing is going like something like this. Do you realize that there's way too much oscillation and you could sort of smooth it out by taking if somehow you wish that this journey was something like this this yellow line would be a more efficient journey isn't it would you agree guys yes yeah that is a basic intuition so now let's map that moving average intuition into this what are the games that we have we know that we do this thing the weight next is equal to the weight previous, right, minus alpha, the gradient of the, right, with respect to the weight, right? So now this gradient for each mini batch is sometimes pointing in this direction, sometimes pointing in this direction, and so forth. So what you do is this part, actually, you treat it as the v, the moving the v part. You say that, hey, you know what? This part needs to go through this process that v of a given time is beta times, like the time here is the step. So now it's a discrete time right uh each step of mini bag you know one step going through one bit of mini batch one step is the time right so step one step two step three so you can take t as that step plus one minus beta we uh the actual value the. So this is your moving average of the gradient, and this is the actual gradient that you found on that particular mini-batch. The gradient of loss on this mini-batch step. So then what happens is you maintain a moving average of gradient. And what it does is that it prevents this fluctuations of the gradient in peculiar directions, and it gives you a much smoother path to the, and much shorter path to the minima, right? Now there is more to it actually. One of the things is that I did not bring in the fact that you need to, there's a couple of more. So, by the way, this is your basic exponential smoothing, right? Are we together here, guys? So far? Yes. Yeah, so this is it. And people have different terminology. See, if you're in physics, you often look at it, V is equal to U plus 80 kind of thing. So, you know, you must have seen equations like this. So people often treat this as sort of the acceleration term. this as the, this is more like a friction term. This is the velocity that you have so far. So the idea is that if you're going through this, how fast are you descending? That is the gradient, that's acceleration, as you know. This is what is the velocity you have so far. And this is the friction because you're multiplying the velocity with, that's a 0.9, your beta is 0.9. This is sort of like a friction term, right? So people, with that's a 0.9 a beta is required and this is sort of like a fiction term right so people that's why they call these sort of methods momentum methods momentum method so it is a momentum based people use the term momentum based gradient descent this whole class of algorithms they will call a momentum based just my mouse here a momentum based, just take my mouse here, a momentum based gradient descent. And so there are all sorts of subtleties here. For example, this this equation it suffers from a bias like suppose you have data here it sort of goes up and down what will happen is if you use this equation barely so let's say that the first data value is here because you are taking 0.9 of it your moving line initially will start out lower do you see that the line will automatically start out lower right and so you will have a little bit of a low bias so then people have a terms to fix that bias right so uh they they divide it by the square root of this second moment and so forth the second moment would be the second moment would be this right and so what you do is you you do the same thing for the v and you do the same thing for the second moment by the way this is called s is the second moment you call it that's why the word people often use the word second moment here. It is the second moment. And what you do is this thing delta this. How do you update it? You maintain moving averages of this also. So you can say st is again beta st minus one plus one minus beta s. what is it? No, the actual, the mini batches value of this. And then comes this interesting question that why do you need the second moment? It is, and in fact, this is how this RMS prop works. What it says is that the weight, when you update, don't just do weight is equal to minus alpha this dw. But actually do this. You multiply it by the s, the s of this value, S T, the previous S T, T plus one, do this. By DW actually I said gradient of W of the loss. And S being the second gradient, let me just say, is equal to the second gradient squared. Second gradient squared, the thing here. So these are called moments in physics and math. When you take this, you take the second derivative and you sort of, you take the square of it, not the second derivative. You take the square of the gradient and you work with that. So you begin to play with things like that and these things also go through this kind of a slow learning. Are we together? Right? So that is the gist of what you have here. You, so anyway, there's more to it. It's not enough to do this. There is even further complications that you can bring in to this whole story. One is that you remember that I mentioned that you can do some bias correction terms for beta, which is I believe you divide it by one minus beta square. This will take care of the bias in the beginning. Then you bring in some second moment term, something like this. So by the way, I'm glossing over. In fact, I'll have to look up the specific details. They are all these subtle differences. So let me mention all those differences a little bit at a time. So your basic equation is this. Minus... So this is your basic equation, right? Then comes the problem. This part, you don't want to apply it directly. You don't want to update it fully. What you want to do is you want to instead use the moving average. So you say that I will do this, Vt. Instead of using the raw value, I will use this value instead, which is the previous value minus, right, beta times plus 1 minus beta, the actual value that I see now. And I will feed this into the equation instead, into this equation here. Then you realize that, well, if you do that, you still have a little bit of a problem. This equation has a bit of a bias problem so you need to correct the bias by doing vt one minus beta square right square root and this is a bias correction if i'm right if i and i'm at this moment just speaking from memory so if there are little errors just do look at happen before taking the down notes. And then what happens is RMS prop brings in the S term also as a moving, not just the gradient, but this term also. This term also comes in and it comes in the denominator. You divide all of these answers with this S term everywhere whenever you do the update of the weight. And then comes Adam. Adam takes both of these ideas together, the bias correction as well as the RMS prop idea. Both of these they put together. And then there is one more thing now, which is called Adam W. And then it becomes like an industry in which people have created many, many variants. And I always tend to miss, unless I review them, I tend to forget the specific details of what expression of all of them are. But the basic idea behind all of them is that you use momentum and moments, you use exponential smoothing to make sure that you have a smooth part back home to the minimum. That is the main intuition. Are we together guys? That's how these optimizers work. There's nothing magical about it. There's nothing esoteric about the way they do it. They just use exponential smoothing or they use another way to intuitive way to put it. They use momentum right to maintain sanity to maintain to to dampen down the oscillations are we together guys so now comes the interesting paper and the quite a few things happening these days people ask this question. And this is the gist of what this paper is trying to say. Suppose you say the next value of w is based on w minus. Let me keep it in its simplest form. This, right? And if you bring in r, if you can think of it as some function w learning rate gradient right in this case it is your function is this right if it is rms prop and other atom etc then it is w alpha gradient of l gradient of l squared the second the moment also the second moment and so forth you can throw in more terms, right? Or not to forget beta. Remember the hyperparameter beta, which is how much of the past moving average to consider and how much to consider the latest value, right? So in some sense you can say that the next value of, is based on the previous value, is based on some hyper parameters, is based on the gradient, the square of this. And we have hand constructed, if you look at the way people have done that is, over the years they have hand constructed this formulas to make it better and better, right? So RMS prop will also, for example, bring in the moment, Adam will bring in the bias correction and RMS prop putting it together, right. Adam W does an improvement of right and there's so many, many things. They all seem to build upon each other. But one thing is true. They all reason make some reasonable explanations and then they write an equation for the update step in the optimizer. Am I making sense, guys? Yes. Right? So the fundamental question that people ask is, hey, you know what? Why do this? Just like this is like creating features, the momentum feature, moments feature. So in machine learning itself, we used to do feature extractions by hand. So for example, if you looked at images in the world before deep neural networks came, people were using the Sobel filters and some edges, let's look for vertical edges, let's look for horizontal edges, let's look for diagonal edges. Somebody had sat down and created features, Sobel and so forth, and they said that you can use these features to recognize images, classify images and so forth. Then came Deep Neural Network and it basically said that, hey, features can be self-learned. We can go and learn those features. So the same idea they want to apply here, So the same idea they want to apply here, and people are asking this question, can we not learn this function itself can be a neural net? And this is a very interesting idea. You're saying that, see the optimizer right miser and the optimizer will say this is based on all of these factors so the first thing they do is they expand this thing they say hey why just the gradient of the loss and the loss squared let us add the loss itself also to it now and they found that if you create a function based on everything including the loss its gradient its second moment and so forth then you you get a much better optimizer that seems to do or well i wouldn't say in this paper probably not much better but sometimes better optimizer. But the more radical idea is that why not create a neural network to meta optimize the optimizer itself? One of the big pursuits that we do when we go from SGD to let's say to batch to auto mini batch to momentum based to RMS prop, to what are we doing? We are optimizing the optimizer itself. We are taking a journey to better and better optimizer. And which optimizer to use ultimately is a bit hard, because for some data set, one optimizer may work, and for another data set, another optimizer may work. So the basic premise of learned optimizer says, what if this itself is based on this w is based on some function of all of these things that we know w alpha beta l grad of l grad of l squared, but, just as you write, so suppose, you know, in linear regression, you write y is equal to theta naught plus theta 1 x, right? In the same way, what if this is based on certain parameters, let us say theta, right? Theta 1, theta two, theta n. So these are again some sort of weights and biases of a separate neural net that you train to discover a very complicated and nonlinear function, right? A complicated and nonlinear function. That is far better than the simple ones that we have been hand constructing, right? Can we learn this? So how do we learn this particular function? That's the crux of it. So they go on to say that, see, and now let's read this paper with that context in mind. I think it will make sense. Let me just take this. Much as replacing hand-designed features with learning has revolutionized solving perceptual problems, you know, there's image processing tasks. In this work, we focus on general purpose learner optimizers. So they are saying that we can do the same things and we can move forward. You don't have to tell us what beta is, you know, what the learning rate is and so forth. It will internally do all of that. So how would you train such an optimizer? The problem is to train such an optimizer, you need what these people do is they go and gather and this is what they come to say this is the part, you know, you the optimizer architecture is you use a feedforward network and an LSTM. The details don't matter because different people are now trying different things and so forth. This is pretty good. I won't go too much into the details of this paper, except to stick to the core ideas. So what they're saying is that, see, you have many tasks. You take lots of tasks, like MNIST, right? Task sets, MNIST, then CIFAR data set, many, many data sets for classification, MNIST classification, maybe, and so forth. You take a large variety of tasks. And what you do is you iterate over it over tasks like one is the outer loop is outer loop is over tasks tasks and inner loop is of course your uh what is it it is your epochs right isn't it over the steps the inner loop will do what it will loop over it's an what? It will loop over. It's an epoch. It will loop over, guys, are we together? It will go over the steps of your optimization when it goes through data for training, right? Training. And, Asif, same question that I asked the other day, yesterday. Yeah. We're not making any distinction between tasks that have discrete steps versus tasks that can go through a range of variables no no no see each task is something like a mnist classification make mnist classification give better results make cifar give better results so the tasks are discrete data sets make CIFAR give better results. So the tasks are discrete data sets with a goal. OK. So that is the outer loop. So what you're saying is, create for me an optimizer that is good, that is a universally good optimizer in some sense. That works in so many, many situations well. Go create such an optimizer. What you have to do is take one of the tasks and then train an optimizer to be good at that task. Then go to the next task and again, try to make it good at that task. You see how it is. The problem with this, this is it, this is all they're saying. They're saying basically, suppose we take this itself as a neural net so they use an fft plus lstm these are the internal details i would say that frankly there'll be more research and these things will change but just think of it as a neural net we are saying that you're training the neural net through let's say that your step one the outer loop is mnest that your step one the outer loop is mnest at this particular moment right c4 and some nlp tasks etc etc so let's say that you are in this loop then here you're here you're going through batches of data steps of data so when you get steps of data you'll get a loss you can take gradient of the loss you can do some learning that is all right but the problem is those gradients don't go up to the i mean there is no such thing as a gradient that you can apply to these levels what happens is that the gradient flow doesn't work on the outer loop right in the inner loop of course we have been doing gradient descent and optimization and so forth so what do you do for the outer loop? This is where this paper gets a little dense in its explanation for it. The explanation is actually at least a bit hard to understand, but I'll explain what it says. It says that this equation is here crucial. Oh let me mark it as right. This is your crucial equation. So it says, for example, let us say that you take a model like this, Wt plus 1. The next step, so I use a subscript, so let me use my subscript, one is the previous value of t times exponentiation e to the a times b right now what is a and b you have to these are dynamic variables they need to be computed they are basically some functions of the parameters and everything so you need to compute a b at each step when you when you want to update the weights you need to figure out a and you need to compute A, B at each step. When you want to update the weights, you need to figure out A and you need to figure out B and feed it into this equation. Right. Now, why this equation? Well, there's some arguments and so on and so forth. But basically this equation. Now feed it into this equation. Now feed it into this equation and if you do that by training a neural network in a complicated way, then what happens is the results are rather interesting. The results are that compared to the normal baseline here, these bars represent the performance of this learned optimizer. So it seems that learned optimizers do better for all situations except the last three, last four. And you notice that, that the learned optimizers seem to have their bar to the right of the red line that I drew, the baseline line I drew. So their performances seem to be better. There is only one thing to it. In the last three, you have, it actually doesn't perform as well. But if you look at what they're talking about here, two last, one, two, three, okay. about here two last one two three okay do you know how many trials they're doing thousand and two thousand trials so it says that if you're doing let's say adam or whatever and you're doing a grid search over a thousand possible values thousand experiments or two thousand experiments right You realize that you're doing computationally something brutal when you do 1000 or 2000 experiments. In that case, you'll end up beating a learned optimizer. Otherwise the learned optimizer, just without any tuning or anything, once you have trained it, it will actually beat the standard process. So now the point they're making is it's not really feasible to do a thousand experiments or 2000 experiments. This was the whole point of AutoML we are making yesterday. Like it becomes a bizarrely huge computations. So the value of learned optimizers is that if it is successful, if this whole learned optimizer business becomes successful, you will end up creating next, in a way, more advanced optimizers that it is the next step beyond what we have been doing basically. We are taking a new approach. We are saying let's learn the optimizer itself by creating a neural network to train the optimizers and then use it. Now there's a little bit of a catch to this data. What happens is that it works very well with small data sets. They don't work so well actually with the large data set. It turns out that the learned optimizers, their loss is pretty good for smaller data sets, but for large data set they begin to do rather poorly and they acknowledge it. And also they show that for very large data sets are big problems at this moment. So it's a concept in which they are made progress, but it is still work in progress. And they're very, very nice about that, that this is all very work in progress. Because if you look at this diagram right here, you'll see that they are. You look at the black line, the black line is the learned optimizer. The loss of it is not comparable to that of, let's say, Adam or so forth. In each of these, you don't find that the black is outperforming them at all. Here also the blue line is not necessarily outperforming. So, well anyway, this was just one paper in this whole game, this new emerging area of research of learned optimizers, and it has some success and has some not so successful. And they show what happens. So suppose you take a function like this. Imagine that you take a function like this. So look at this function, x minus y squared. So where would this achieve a minimum? Of course, it would achieve a minimum along the x is equal to y, because then the function value will be zero along this line, isn't it? So if you use the atom optimizer, it will push you to this. It will gradually take you through a path in that if you start with any guess of x and y, it will just take you in. Those are the guesses for x, y. The learned optimizer tends to do something very interesting. It is also pulling you in. but do you notice that, let me exaggerate this feeling, do you notice that it is sloping you inwards? It is sloping you towards this origin, which is 0, 0. which is 0, 0. It is taking you not only to its x is equal to y line, any point in the x is equal to y line, it also tends to take you to a specific value of x, y, which is 0, 0. And this paper argues that this is an indication that this learner is in some sense self-regularizing. What does regularization do? It beats down the value, isn't it? Remember the hyperparameters get beaten down to smaller values. And so they say that there is a self-regularizing effect happening in this paper, like in this thing. And so this is it guys, this is the point of this paper. It is broaching away interesting idea which people are beginning to explore that optimizers can be learned. And recently there's one more paper that's coming out in January, it is going to be presented in January. It's already there in archive. Let me see that paper, which you can take as a follow up reading if you want. Let me see where that paper is. Is this this paper? No, it is not. It must be this paper. Yeah. So, so one of the things people are asking is, be this paper yeah so so one of the things people are asking is assuming that this learnt optimizes now people are paying a lot of attention to it people are trying to understand why do they work well of course in this paper it sort of seems to work it's not terribly convincing it works the theory is very good the implementation is a work in progress. So people have, I suppose, gone further and tried and already started asking this question. Why does it work? And when you look into it again, it goes to the interpretability of the neural networks and everything we do in this field. So this paper, for example, reverse engineering learned optimizes, reveals known and novel mechanism. It tries to understand what really is going on, right? Now these equations by now must be looking familiar to you. I'm saying that the next value is some function of the previous value and a whole set of parameters. So it goes into, I won't go into this paper that needs a discussion of its own on a separate day, but suffice it to say that people are taking into i won't go into this paper that needs a discussion of its own on a separate day but suffice is to say that people are taking this whole field very seriously and trying to understand how do these things work if you want i'll post the link to this paper this one is quite readable actually and you can try to make a head and tail out of it now Now going back to this paper, there is one aspect that is quite interesting. See, we have only two choices. You do gradient descent. I mean, if you have a gradient, you can do gradient descent backprop and all that. What if you don't have a gradient? Then how do you optimize the value? In the outer loop, you don't have the gradient. And so how do you optimize? So what you do, what these people do is in the hyperparameter space or rather the meta parameters of the learner, they do one more trick. And this is a small detail internally I'll just mention. So suppose they try it with this. So you have a data set. Let's say that this is where you are in the hyperparameter value. This is your current value of theta, your hyperparameters of the learner. This F is made up of all theta and the rest of it, alpha, beta, gamma, delta, L, L, delta, L squared, and so on and so forth but the actual internal parameters the neural net of the learned optimizer itself is i'm representing as a theta so what you do when people can't do gradients they do this thing they will do a small perturbation of theta so this comes from reinforcement learning and evolutionary algorithms. So what you do is you take this value, neighboring values of theta, and now what you do is you apply it to these data sets, right? So suppose it turns out that this, the value was, these values were not so good, This, the value was, these values were not so good. And these values turn out to be better. I mean, obviously I'm taking a very clean situation in which these values lead to an improvement over these values, and these values lead to degradation in this. So what does that mean? You should move theta in this direction,'t it guys this is your pseudo gradient moving in this direction is your you effectively have created a gradient of the theta am i making sense guys you know the direction to move it seems that if you move in this direction the average of this direction which is this action is equal this direction this direction this direction average of this direction, which is this action is equal this direction, this direction, this direction, average seems to be along this direction. If you take an average movement in this direction, this is also somewhat like a pseudo gradient, you're making a small step of the theta in the right direction. Am I making sense, guys? Yes. So this is it. So these ideas come from evolutionary learning, machine learning, evolutionary programming. Obviously evolutionary programming has more content. You do crossovers and genes, like you try to model it after biology, but we'll just stick with the simple idea because that's what these guys use and we're not doing reinforcement learning at this moment but if we were but then we would have covered this in greater detail things like this so this is the internal detail and the reason i bring this is because it's slightly useful to understand that you can actually optimize even in the absence of a gradient actually optimize even in the absence of a gradient. So that's that. You make small perturbations, see in which direction if you made the perturbations things improved and you just move in that direction. So there is a lot of technical details in this paper on how they do it and a lot of it actually is not very explicitly stated frankly so you have to pour over it to see so now they say that they make both uh statements they say oh we take an evolutionary step and then they also say we use adam so my understanding probably is that now that they have the gradient the update step that they use is the Adam step. W minus, you know, all of those props and S and you know, gradient and alpha and the usual, the more complicated formula for Adam. I don't want to write it explicitly. I'm sure I'll get some little detail wrong. But once you have a pseudo gradient, you can feed it into the atom and you can take a small step forward and they do that and it seems that they had some success with it and then more people must have tried more things this paper came out of google it's very interesting because to do this meta learning even this part the the amount of computational resource you need is tremendous. You know, it's a huge and you have to try it over all sorts of tasks, you know, try MNES, CIFAR, try NLP, try this, try that, try it on many, a whole wide variety of tasks. And the other thing is that, The other thing is that, well, okay, there's more to it. You can sort of read through the paper, but I hope after this introduction, you will get the gist of it. So this is it, actually, this equation is sort of important as an example in understanding of what is going on here. What they are saying that, you know, it looks very mysterious if you're not going to be writing some abstract function. But here they are made the function very, very sort of explicit. We are saying this is it. It is exponentiation of a times B and a and B are learned values. In the system and that is that i mean that is understanding the optimized behavior so that's that any questions guys Anybody has a question? Asif, quick question. You mentioned that on a large data, it's not performing as good as the other ones. So I know it's in the early stage, but trying to understand if it's not as good in the large data set, wouldn't it, like, isn't it actually not, I mean, on a small data set, you can run as many epochs as possible and as many grid search as possible. Yeah. I was expecting this to perform better on the last data set so that you know we can use the learned optimizers on the last data set where it already would be taking a lot of time to do grid search but turns out on a large data set it's not uh performing better so the point is that see at this moment right what they're saying is that this is early stages of the work right and we are still learning we made some progress and in some ways we still have ways to go got it so this is looking in the positive direction right so they could uh i mean, I would, ideally, ideally I was expecting to see that in on a last year as this is the way to go. But in a small data set, you know, grid search would even do better. Then why to go to this? Small data set would already perform faster. That is it. So the value of papers like this, you know, is that they have opened that door. See, we have been using grid search and this. And whenever breakthroughs happen, they have a little bit of a tough time beating the state of the art. They are a great idea, but they need to be refined and made to work properly. The initial implementation will be sort of a good attempt, let's put it this way. And these attempts are hard. It takes a tremendous amount of computational resource to run these outer loops. And so these are all Google researchers in the Google Brain team. And so they have put in a good effort. The whole question now is that are people taking it seriously? And the answer to that is they are actually. There's more and more activity in this space. By the way, they didn't start this field. Other people were also looking into it. Their main contribution that I see, amongst other things, is that, among other things, one main contribution is they realized that we can't use a gradient propagation to the outer loop. So why not abandon it and just use this evolutionary algorithm? And that is the main contribution. But hey, there is a way, even if you don't have a gradient, and there's a bad ingredient for the outer loop, let us take a very simple, very, very simple evolutionary approach. It can't get simpler than that. Just make a small perturbation of your parameter and now see which, a few perturbations, which of the perturbations did well and just take the average. And then create a pseudo gradient using that, which is typically evolutionary algorithm approach, and then this is evolution happens, right? You make small mistakes and then some mistakes help you. The biological systems realize that in the environment they thrive with those mistakes. And so those mistakes continue to happen. They became the new mutation, right? And so that is the evolutionary step involved here. That's one. So in future, something like a SGD, Adam, these trained optimizers would go into a library where it would become standard? Yeah, the hope is, see, at this moment, right, they have released a, I mean, if you look at the code, I didn't look at that very, very carefully because it is one of the papers you read, you find interesting, you find a value. Its core value is that it has moved the needle forward, but it is not, in my view, at a state that is usable. It's not yet there. So, I mean, it's a bit premature to make it into a library, but one would imagine that a more When more progress is made these things will wipe out all the items and so forth. And I can, I should, I should hope to see in a year or two. Yes. Does a ever just like initiates the outer loop to Adam and then go from there? No, they internally use, for example, this evolutionary step itself, they use Adam to train it. So they see the meta learner, right? They're training it at all, at all. The parameters theta are being computed. And so they are using Adam to train the meta learner itself. But once the meta learner, once the meta learning is over, now this learner will be used for the real machine learning, not Adam. That's their main argument. So just starting from Adam, that's like a, it has an initial point. Yes. Okay. So today the state of the art is like, for example, in my case, here is what we do. Generally we now add, now above Adam, there has been a more improvement, Adam W. We all start with just Adam W or sometimes we just SGD plus momentum. You play with two or three things. In my case, I play with these and one or the other gives me reasonably good results. Now, these have hyperparameters. So sometimes I'll play with the hyperparameters. So one of them, obviously, is the learning rate. There is a weight decay. There are many factors involved. And some of them we have learned is invariably the best value. You don't play around with it too much. So see, here's the thing. In a deep learning problem that you have to spend months on, you get time to play with everything, all the hyperparameters, how many layers, what sort of layers, what residual connections, this, that, what optimizer, what parameters of the optimizer? There's so many hyper parameters that you play with and that is where the discussion of this week comes in so important. That see we are all, these are all little building blocks towards a new world in which you don't ask those questions like how do i tune the hyper parameters you basically say that this machine learning will automatically find the best hyper parameters and move forward and in the same story they will find the best learners they will have a good learner to work on this they would have learned a good learner and i don't have to do any hyper parameter tuning i don't have to give the learning rate i don't have to do any hyper parameter tuning. I don't have to give the learning rate. I don't have to give word decay. I don't even have to choose which optimizer. It will just learn an optimizer. So it is part of that broader picture. That's why I brought this paper in. Are there any research like this except for activation functions? Oh, activation functions itself is a big deal. There is every other day there seems to be one more paper on activation functions. It's one of the easiest ways to create. Just sit down and write one more activation function and see if it works. I'll give you an example. All the optimal activation functions that I taught you, if you try to take something as simple as a sine wave, and you can go and use your ReLU and your CLU and tanh and whatever you want, you'll realize that all of them will fail to capture a periodic function it's the is the failed part that I didn't tell you about but since you ask I'm telling you they all fail okay so now what do you need you need different activation function somebody pointed out that activation function that you need is sine square x this activation function so fx is equal to or activation of x sigma x is equal to this right and you do this then you you will get back to your uh you will actually be able to catch periodic functions. So activation functions is a subfield of research in its own right. Okay. All right, guys, any other questions? Otherwise I'll stop now. Was it useful, by the way, getting introduced to this learn optimizers? It feels like a natural extension to the auto machine learning discussions. That's right. That's right. And also, I don't think I really went into the details of the momentum-based optimizers in the past. It is a very simple idea that I thought today I'll fill in the gap by introducing it. The concept of this moving averages. Now our exponential smoothing. And this is also something, this exponential smoothing business, you'll encounter next week again in time series data. So these concepts will get sort of settled in in a bit so that's that guys let me stop the recording Thank you.