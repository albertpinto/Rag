 audio of your speaker that you want to imitate, right? That three second audio also goes as an input, okay? Now, this text first gets converted to phonemes, okay? So, what is a phoneme, first of all? To give you an example, right? Basically, if you take the word talk, right? basically, if you take the word talk, right, you take this word talk, talk, the spelling of talk is T-A-L-K, right? Now, if you just join together these sounds for T-A-L-K, right, that's not how you pronounce talk, right? The L is kind of right you don't you don't see the sound of L uh in the when you pronounce the word talk so uh there is a phoneme conversion of this word talk and that if you just Google right you just say talk and ask Google to give me the phoneme representation of talk, it will say T-A-A-K. So, T-A-A-K, if you pronounce T-A-A-K, that is basically talk, right? That is how we are actually pronouncing the talk. So, there is a difference between how we write a word versus how we pronounce a word, okay? That difference is what we are trying to address with this phoneme conversion okay now phoneme conversion will convert into phoneme and you can get the phoneme for you know pretty much even dictionaries I think many dictionaries give the phonemes for all the words so you get the phoneme so this goes as one of the inputs now this audio, audio also gets encoded. Okay, so this is the main beauty, what I found in this paper, or this is what excited me the most, what I found very surprising. So, you need to see some of the history. So, this gets encoded and goes, you know, as one of the inputs. Then their neural codec language modeling, this is the volley, right? This is their actual model, what they have developed and that generates an vector representation of the output audio, okay? And that goes through an audio codec decoder, which is basically the inverse of this, right? This encoder, the inverse of that, the decoder portion of it, that goes into here, and that generates your personalized speech. So, this is the final output that comes, okay. So, basically, you have a text prompt, as well as an acoustic prompt as an input, and the output is the, you know, the speech, okay. So, this is the overall, you know, block diagrams. But, you know, okay. So, this is the overall you know block diagrams but you know before we can appreciate what these people have done it will be good to see a little bit of history you know what previous people have attempted to do okay. So, what previous people mostly what they tried to do most of the works right there is a lot of previous works in this area actually. But, you know, let me, this table kind of summarizes, you know, what, how these people have approached it differently. So, what other people have done before is when they trying to convert this phonemes to going from phonemes to an audio, right audio waveform, they have an intermediate representation. And that intermediate representation in most of the previous works was Mell spectrogram. So, Mell spectrogram is basically the frequency domain representation of a waveform. So, you take a waveform, you take, if you remember, if you familiar with fast Fourier transform, take, if you remember, if you familiar with fast Fourier transform. So, you do a Fourier transform, short time Fourier transform and it will give you a frequency domain representation of that signal. Basically, it is trying to express a signal as a weighted sum of sinusoids, okay. So, you have multiple sinusoid frequencies and then trying to express a signal as some linear combination of different sinusoids at different frequencies. That is essentially what a pass Fourier transform is doing. So, this MEL spectrogram representation is nothing but is basically taking that frequency domain representation, okay. And then, you know, there is this unit of MEL. Basically, okay and then you know there is this unit of mel basically this goes by you know how our year right apparently in the frequency range 0 to 100 hertz we will have you know we will our year is very sensitive to each 10 hertz difference but when you go from 100 hertz to 200 hertz the number the granularity of how much you can discern the different frequencies becomes lesser. Then, you know, 1 kilohertz to 4 kilohertz, it will be different. So, basically, in the lower frequencies, you want to have more levels of representation. As you go to the higher frequencies, you will have, you know, lesser, you know, it's like the logarithm, you know, you look at logarithm scale representation, it's this kind of similar. So, MEL has a, you know, one way of representation and then the MEL spectrogram is very popularly used or was popularly used, you know, before this work, okay. In this work, they are not using the MEL spectrogram representation at all, okay. In this work, they are not using the MEL spectrogram representation at all. Instead, they have gone with what is called an audio codec model. And this audio codec model, again, is not a contribution of this paper. They have basically reused whatever the meta came up with. So, meta actually published a model called nCodec. model called n codec okay i think i think that is a very fundamental contribution in this space that i think everybody should know is that is basically the n codec work is basically an audio compression solution okay see normally today we do audio compression how today we do audio compression. How do we do audio compression? How do we save the audio files? You have WAV format and then another popular format is MP3. And then there is one open source format, I think OGG something. So, there is one open source format. So, MP3 is very popular. Many of you may be aware of it. So, MP3 is supposedly one of the, you know, best known codecs for audio compression and representing audio. So, what the ENCODEX did was, they came up with a completely neural codec, completely with a neural network. You know, if you remember, we looked at the auto encoder paper, right? Auto encoder, we do on variational auto encoder paper right auto encoder we do on variational auto encoders we look so this is i see the encoded work very similar to the auto encoder kind of work where you take a waveform and you get a latent representation which is a compressed you compress it and then decompress it, right. So, you have encoder portion and decoder portion and then you are basically creating a information bottleneck, right. So, when you create the information bottleneck, the compression is happening. So, that audio encoder they have utilized. So, this is the intermediate representation they are using. Benefit of this representation is now you have this, because this MEL spectrogram we all know, I mean we know what the MEL spectrogram stands for, okay, frequency, sinusoid and all these things. Now here, the latent representation audio codec, we do not know what each of those, you know, what are each of those vector coordinates stand for, the physical meaning of it we do not know. But the neural network comes up with this representation and apparently this outperforms the MP3 and all other previous compression. If you see the ENCODEX paper, the separate paper, they claim actually very, you know, like 5x better, 10x better than MP3. The file size, that much compression they are getting with pretty much no, you know, difference, perceivable difference in the quality, okay. So, they have used this audio codec you know representation they are generating the final output. Now this objective function part I did not really understand what what they actually meant by this continuous signal regression versus language model thing. I think it is how they view the whole thing. And in terms of training data also, what they did was they increased the amount of training data. So, the training data they used has like 60,000 hours of training data with 7000 plus speakers. So, it has, you know, diverse speakers and diverse, you know, each speaker speaks, you know, the accent, right? Accent is different, right? So that gets, you know, captured. And even the, you know, the spacing between the words is different, right? Different people have different pauses and all those things vary, right? So such a diverse set is going to help you uh with that right now this in context learning what they mean is is basically the transformers ability of you know like uh you know with with uh zero uh short learning right basically you gave a very very minimal input of your desired style and you are able to generate an output of that style with very minimal data right so that the transformer is able to capture all that information and then transfer it that in the in that style right so if you go down here so this is all you know kind of some introduction i think this this page page five gives, okay, this is the neural audio codec model. This is the model of the, this is Metaswer. I think they just tried to recap that. Okay. So this is, you can think of it similar to the autoencoder concept, but I think it's a little bit different. They have this, what is called residual vector quantization concept. I haven't really read that paper fully to understand this, but it seems like they have, you know, if you look at the latent vector representation, right? Let's say first component you take, that will represent some features. And whatever information it did not represent at the core. So it's like at a core level, you represent something. Whatever information is not represented by that, whatever is remaining, right, the residual portion, that gets quantized and gets represented as a second coordinate and so on, right? So like that, they're getting this latent vector. That's kind of high level. That's the understanding I have. of high level that's the understanding i have um now um this they're just explaining here in this paragraph how this uh neural encoder uh representation is much better compared to the uh previous you know uh or various other types of representation like you know there was hubert and so many other representations that codecs that came compared to that the metas encodec has kind of outperformed everything that's what they're saying okay so basically if you if you look at the uh here for example they give some example here if you take a 24 kilohertz audio okay and what they're saying is this encoder that is the metas encodec right this is the the the big big work it produces embeddings at 75 hertz so look at the how much compression right that is a 320 fold reduction in the sampling rate okay and then each sample right is getting modeled as a residual vector. This is what the residual vector quantization. So, basically, to give you an example, if you take a 10-second waveform, if you take a 10-second audio waveform at 24 kilohertz sampling, that will be represented by a matrix, okay? That matrix will have 750 rows and each row will have 8 columns. Why 750 rows? Because you have 75 hertz sampling and then it is 10 seconds. So, 75 samples in every second. So, 10 seconds will have 750 samples and then each sample you are representing with the 8, 8, you know, 8 dimensional vector. So, there is eight is the number of quantizers that they have. So, it is very, so bottom line is the very efficient vector representation that can basically represent an audio waveform. So, now coming to the problem formulation, the Wally paper, what they are doing is, so this is the key point, okay. So, their data set that they use for training is pairs of X and Y. What is X? X here is the phoneme transcription, okay. So, this is the phoneme that is the Y, okay. Now, this, when they get the, you know, input, right, so, so they have this 7000 plus speakers audio data, right. So, what they do is, they take the 7000 plus speakers, the audio, all of that, they transcribe it. So, they convert that speech to text. So, they have the corresponding text now, they have the corresponding audio now. Now, from the text, they can convert to phoneme. So, that translates. So, you have now phoneme and corresponding audio, those pairs, you have it. So, X and Y, this is your training data. Now, let us look at what they are actually doing here is, this is the key thing. So, what they are doing here is, they take your acoustic prompt, they take your acoustic prompt, run it through this encodec coder and that will produce a matrix and that is your, what do you say, equivalent of your text prompt in large language model. In large language model, you have a text prompt, Like that is equal and conditioned on that. So, you are basically what you are trying to generate as an output is they are trying to generate the latent vector representation of the output. Let us call that C, right? And given X, X is what? Your phoneme representation of your text input. And the C tilde, what is C tilde? This is your latent vector representation of your speaker identity, your three second, what style you want, right? That speaker. So, given these two, you know, X and C tilde, you are generating the c. c is what? c is not the audio. c is just the latent vector representation of the output audio. Now, how do you generate the output audio? Just do run the decodec, right? The decoder of the, you run on the c, you are going to get your output audio. So, this is basically what this the problem formulation. Now, this is what their neural network is finally their model is trying to do. Now, this is how they have implemented that, that they have done it with a mix of autoregressive as well as non-autoregressive, okay. So, what they are doing is that C vector, right, the C vector has different coordinates, right. So, the first coordinate that they are saying, that is the first coordinate quantizer, okay, that they are saying is an autoregressive decoder, okay. saying is an autoregressive decoder, whereas the following, these remaining quantizers, that is a non-autoregressive. So, the remaining ones are non-autoregressive quantizers. So, this part I did not like fully understand, you know, I understand as autoregressive is something is autoregressive if it depends on the previous value. Non-autoregressive means it does not depend on the previous value. So, but finally, you are basically getting the output as a latent represent latent vector representation and then you use this decodec to generate the final audio so yeah I think the strength of this is actually very basic. So imagine that you have, see, you have the input text, phoneme sequence, right? You have the phoneme sequence. Now you have the three second voice or whatever, the voice of the person, and you have encoded it, right? And you get the basic quantized, you get those tokens, the equivalent of what would be in a language model, the tokens of that, you feed these two jointly, these two jointly now go into the decoder. So the decoder will start by emitting the first token. That token is still in the latent space, it will later on get decoded into a voice. You realize that, but if you just, for example, forget the decoding part, whatever was said, you realize that the next thing that will be said is conditioned on what has been said, very much like in a language model. The next word you produce depends on the previous word and that is the only meaning of the autoregressive that you feed it to produce the next word you take the original text as x you take the the encoding of the voice the existence of this voice which is also fixed but if both of these things are fixed how in the world would you generate the next token the only way you you can generate a different token is if you can generate a different token is if you can take the previous tokens that you have generated and feed it also as an input. That's the auto-regressive. Okay. So it seems like the first coordinate alone, they are using auto-regressive, which means in the latent vector representation, first coordinate is autoregressive, but then the remaining coordinates are not autoregressive. So, that they depend on, maybe that depends just on the first coordinate itself. Yeah. So, I believe they probably did this to reduce the complexity because if everything is autoregressive, it is going to be more computations. Right. Yeah. So, that is what they have done. And I think their strength is basically the size of the training data. That zero-shot learning, they are able to do because of the size of the training data and also the efficient representation, the latent vector representation of the ENCODEX model. I think that strength they're utilizing here instead of building their own, re-utilizing Meta's work which has been trained on even more larger set to capture audio in the best possible you know compressed form without lacking or compromising the quality relying on that has given them a big advantage here yeah that's pretty much uh what i want to i think i like to read also carefully i don't understand this part about the only the first coordinate being auto regressive maybe we should read it again nice the continuous is continued yeah that's that's pretty much it uh so then they go into results and they show that, you know, how their model is, you know, superior in the zero-shot learning compared to all the previous work. So, it outperforms all the previous work. I think this paper came only in 2023, that is this year only this paper has come so it's a pretty recent one so the interesting i was just trying to see like what will take what will it take to train such a model and they have mentioned 16 nvidia v and v100 was used that's a lot of nvidia v100s but i'm curious how this maps to 4090. uh can can four 4090s do this uh i don't know no one see 32 gb right none of our 4090 even has 32 gb it has 34 gb but having said that i would imagine today's world we could do it it will just take a long time that I would imagine today's world, we could do it. It will just take a long time. On a 44090 inference server, maybe. But it will take a long time. Okay. Yeah, it's pretty amazing that with just three-second audio, the latent vector representation of a three-second audio is able to represent the accent with which I am speaking the you know the pitch with which I am using I am speaking and the tone which with time speaking all of that is captured in just uh three second audio is what according to that 75 into three into eight that's it 25 into three into 8. That's it. 75 into 3 into 8. So many numbers is enough to represent. 10 seconds was 750, right? 3 second only audio they're asking. So 75 into 8 into 3. So many real numbers represent, basically captures everything of how I speak or anybody speaks for that instance. The uniqueness of their voice is captured there. Rajivik, is the number 7, is it 750? Yeah, so if you go here, right, that ENCODEK model they had explained. had explained. So, if you give a basically, if you give a 25 kilohertz input rate, it produces 75, the embeddings are produced at 75 hertz. So, if you had three seconds of audio, you will basically have 75 samples and each sample is represented by eight real numbers. So, you basically have 75 times 8 times 3, so many real numbers. So, it will get represented as a matrix which will have 75 times 3 rows and 8 columns. So, that is it. That is your acoustic prompt. eight columns so that's it that's your acoustic prompt yeah i mean i'm coming from a signal processing background uh what was interesting for me is there is absolutely no signal processing in this whole purpose and yet you know it outperforms all the works of the people who use signal processing techniques before. So there was a Asif is it okay to interrupt her? Oh yes sure. Yes there is another related work to this called as ValeX I don't know if Susinder mentioned about that where they take a Chinese accent and then an English speaker thing, and they're able to convert it into a similar Chinese. I don't want to use the word Chinese English, but English which sounds similar to the Chinese speaker. So that was also pretty interesting to see how they go from the Chinese stops into the English language. Yeah, that is true. If you go to the website for valet, you will see at the top there's valet, there's valet X. X is the crosslink, the mult that Sushinder makes is absolutely remarkable that such a small bit of information characterizes human voice. There is a bigger lesson here. And the other one was that he probably mentioned it, but I think it is, I was also reading this paper parallelly earlier. So there was a prosody, I think that's which captures all the tones and everything in the study. I think this is where it was remarkable, I believe. Say that again. I think it's called as prosody. Prosody is the study of supra-seggmental features of speech, pitch, intonation, stress, rhythm and tempo. So what you can do is some of the other examples that were given over here, people have products where you can go and now in the prompt, you can say I want a deep British accent of an old guy trying to speak or a bubbly, a young girl with an American accent. So you can in the prompt prompt you can write that and say say this sentence and what gets generated is according to that particular uh procedure of the speaker that's right i mean 11 labs and these guys are all doing that you have all of these voices and you can in various cultural accents and you can give your text or you can give your thing and it will say it out in this i just subscribed to that i think it's 11 labs it's called the first month is one dollar after that it's five dollars yeah i'm hoping that since i tend to be in public spaces for camera shy i could instead write articles and it could speak it it comes out pretty reasonably good so i put in some of the generated audio right that suno ai also which does that it uh it gives you fairly good i mean they limit only to 15 seconds generated yeah so so sachin you are presenting on the since you mentioned suno i trust you are the one doing bark right i went to valley i was reading valley so as part of it i was going back and forth between bark and valley so who is presenting bark today today. Satyam is saying yes. Nobody is saying yes to Bark. Satyam, it's you. You have to present Bark. Are you game for it? I was reading Valley. I wasn't reading Bark. Actually Asif, I was trying to look for the archive paper for Bark. I have not looked at that one. I see they have released the source code for it. So, there is a GitHub repository with the source code. There are very good examples and everything. But the paper is missing. Yeah, I also had a tough time looking for the bark paper. It seems they did not publish as a paper. They thought, okay, I have published the source code. What more do you want? You still park, you know, in our previous analysis. Yes. Nice. Okay, who is the, so the next in line, so hey, this is, by the way, Sushinderinda i must thank sushinda and you can clearly see how well he explains many of you probably know that he's an accomplished educator if you ever want to learn about wi-fi please take his course and frankly his course even is far more organized he has created beautiful you know three brown three blue one brown kind of animations he has explained his wi-fi concepts using beautiful Manim generated with animations it's just amazing so we all use wi-fi if you want to know how it works he's there so hey thank you thank you asif i mean by the way just i i did not know about manning before coming to support vectors so i should publicly acknowledge that everything i have you know learned and done or used in you know a lot of it comes from support vectors and many things i even the support vectors the back end all the complete course portal uh was all written in Quarkus and I did not know about Quarkus before coming to support vectors so where from our sleep only I knew there is something called Quarkus there is something called Java there so I don't know anything so but I I picked up a lot of the small small hints and clues he gives from that i kind of picked up and trying to do uh apply it yeah thank you yeah he did an amazing job check out his website guys all right who is presenting the next one i think masmi so masmi is going to become is going to share her screen and explain the next one by the way anas made a very very good comment at one point and that's would you like to elaborate a little bit more on your comment uh i guess i was talking about the the encodec thing by meta which is basically what they're doing is they have a gan between an auto encoder and that's pretty similar basically what they're doing is they have a GAN between an autoencoder. And that's pretty similar to what they have in image upscaling, where they have an autoencoder and then some kind of generative model in between. So I was attending this talk where they showed like they had like a GAN and they had like even stable diffusion kind of like a diffusion-esque model between the the autoencoder and it was like really good at um upscaling even at really tiny um image sizes because what they do is instead of so they they figure out i mean we can generate the um the image and then upscale that image I mean uh it's just like better Latents I guess yes yeah no it's a great point guys uh uh thank you for bringing that to us I mean see all of these are deeply related remember when we were doing diffusion models we were remarking at how similar it is. There is a, if you look at the unit architecture, it is basically an encoder with residual links to the decoder, right? And it's sort of an encoder-decoder with a latent, and the decoding part is the super resolution aspect of it reconstructing higher and higher resolution and in some sense a diffusion model is an encoder decoder with a in between these little steps of units sandwiched everywhere and attention sandwiched everywhere so it's good it's good actually if have the video, could you send that video? It was from my university lecture. Oh, university lecture. Okay. Yeah. Nice. So you have very interrelated ideas. I'll just put on the screen the picture. Sure sure. And you're doing a different paper or the same one? That was yesterday. Hi guys, can you hear me okay? Yes. Yes. Cool. We heard about TOTUS today from Patrick and started reading this paper. I'll go over what I understood from it and feel free to stop and ask questions. So basically, this is fairly decent paper, as you can see, 23, May 2023. It starts by the thing that I found interesting is this guy says that with the recent advancement in the image generation, as you can see in the field of image generation has revolutionized the application of autoregressive transform and ddpms and he combines those approaches so heavily relies on the existing approaches of dally and ddpms for this paper and of course autoregressive encoders as well this is an expressive multi-voice text-to-speech system. So he starts by saying what Sushinder said, most modern text-to-speech systems operate on speech data that is encoded as a meal spectrogram. And then he gets into how we use image generation using DALI, showed how autodegressive decoders can be applied for text-to-image generation. So this, we read this paper a while back. Then he talks about DDPMs. So denominating diffusion probabilistic models have recently arisen as the first type of generative model capable of producing crisp coherence in diverse images. And they've been quite effective at using low-quality guidance signals to reconstruct high dimensional space. So what he says is that traditional approaches to DDPMs relied on fixed output shapes that are known before sampling. And then they sampled from over multiple iterations. So these are some of the limitations, right? He says some of the limitations of DDPM is that it cannot learn to convert text to audio signals because they cannot solve the implicit alignment problems. He also relies on this re-ranking which DALI does to get the better results, right? We've seen that in our boot camp as well so i found the section to the most important one so here he says autoregressive models are strongest at conversion between underlying domains like vision, text, and speech. We all know that. So in this case, he took a text and a conditional clip. So this is the architecture diagram. There are inputs of text and a reference audio clip for the speaker that we need to flow. Flow through a series of decoding and filtering networks to produce high-quality clips. Gets into the details of what this autoregressive transformer does, CLVP is something that it built, and then diffusion decoder finally giving out the output. There are actually five units in this, autoregressive, CLP, diffusion decoder, vocoder, and then finally the output. And the DDPs operate in the continuous domain, which allows them to model expressive modalities. So how has he gone about applying this autoregressive plus DDPMs to TPS? He goes into the details here. So he takes an autoregressive decoder, which predicts probability distribution for speech token conditioned on text. Then he uses a contrastive model similar to CLIP to rank the outputs of number one. So you get the output from number one, you rank it through a contrastive model similar to CLIP which equals I think CLVP and then finally uses a DDPM to convert the speech token back into speech spectrograms so that is basically the architecture so a unique design choice that he made is an additional input which is provided to both the autoregressive generator and DDPM that we talked about earlier, right? The TOTR restrict, right? For the majority of the training procedure DDPM is trained to convert discrete speech code into MEL spectrogram. found into the ddp element or autoregressive latent space which is pulled from ar model output instead of the speech code and then the logic is the l foremost semantic leverage than the discrete tokens and then he fine-tunes it oh this is so all of this he just talks about what did he do with clips so similar to clip he took CLIP for re-ranking. The same type of approach used for CLIP can be applied to speech. So what he used is, most tedious data sets are simple pairings of autoclip and text. So by training a model on this pair in a contrastive setting, the model becomes a good discriminator for speech. So in this case he makes the speech and makes it into a mel spectrogram and does the clip training that you do with images and text yes okay yeah and so he trained the contrastive language voice pretend transformer with this is what is the clvp it has similar properties as clip but serves as a scoring model which used for re-ranking the output uh the train to pair discrete speech token with sex tokens so that you could re-rank it it talks about again how he trained it, small cluster of 8 NVIDIA, 3090 is our period of one year. Yeah. So, Shinda, this answers your question, right? Can we do it on 4090s? We can do it. It will take us a year. As if related to this, is there a challenge with latency? I was reading all over that EPS has huge latency issues. Yes, it is true. It is the one problem that sort of bothers because the token generation speed is such that if you're speaking, first of all, there's a latency for doing all this right secondly if you if the speech is coming very fast i think there is some concern there too that i i forget what the concern is but there's some concern but yes latency is the main thing in this all of these things so for example people try to do this demos in which you are saying something but it is coming out in obama's voice like that famous mit demo but you have to know that it has been pre-processed it has been done in a pre-cooked and then is being played but can you do it in near real time but can you do it in near real time that's where the latency comes in yeah and my question was I think KTAS superseded other wave net where we're not using other uh progressive encoders is there anything else which is superseding TTS to tackle this problem? Do we know? Things are happening every day. See, I'm not a specialist in this domain. I know enough about it and keep listening to and reading the research papers that keep coming out. Maybe a super specialist will say something, but to my knowledge, the latency issue is still around. Okay. But to my knowledge, the latency issue is still around. When I did a quick research on transform on TTS, Tata's TTS, there was something called Tata's TTS Fast or something like that. That was also a question to the broader group. Sachin or others, if anybody has heard of anything which is superseding KTS. Yeah, in print. He has given some metrics as to the speed and I haven't looked into it so can't speak for it. Yeah. So, this is the big topic these days, as Patrick pointed out, and as it is known, right, all this flash attention and then there's deep speed right one of the big thrust is that all right training takes a long time but is it possible because training is once inference is forever so how do we speed up the inference times and a tremendous amount of recent attention has gone to the inference speedup everywhere. Okay, I'll quickly go over the inference process. So what he does is feed the conditional input and text into auto-recurrent model. And we saw it decodes a large number of output candidates. Then he uses the CLVP to rerun them, produce the correlation between speech candidate and text, and pick up the top K candidates. Then he uses a decoder to decode this MEL spectrogram using the DDPM. Because remember, he's fed to think the text and the MEL spectrogram. Then he converts to a wave front using the conventional work order this i did not quite understand very well i'll have to talk to susinda maybe on this uh so this is standard going back from mel's frequency domain to time domain right getting a waveform out of a spectrogram, isn't that? I'm fairly sure that's what it is. So, Shinda, are you? Yes, I think this paper, what I understand, yes, it is basically doing the traditional way, going from a spectrogram to a waveform, yeah. Yeah, frequency to time domain, that's it. Yeah, and then when decoding the auto with nuclear sampling is used repetition and soft max temperature so this is what he showed in the diagram also if you see here autoregressive then the deep tanking then diffusion he does some uh diffuser and then he does the take that and convert it into the output weight point so that's all there is to the paper that's the main idea then the data set that he has used how much what are the experiments and conclusion what i found interesting other than this was he has given some guidance as to the training and the architecture details if folks are interested through that what would you tell me why is he using a vector quantized variation autoencoder there's something that I saw just a little bit above there was a vqba yeah he's using uh you use with that or similar okay so original vq it operates on you okay that's it i got that it's just that yeah exactly so the vq used with autos is most similar to the original this it operates on the melt spectrogram and then when when the large batch sizes decreased reconstruction losses so he went for the large yeah batch sizes and stuff like that nice so he gives some details as to the model shape and the dimensions mass sizes that he used right similarly for the autoregressive here this was interesting so the air decoder uses a box standard gpt architecture and generally follows the training instruction from dali paper unlike dali only 10 self-attention interviews the prompt is assembled as follows this was interesting where it is speech conditional encoding begin token the special tokens text token and text token begin ml token the melt tokens and end of the meltables this we are familiar with right they use the tokens special tokens to designate the text and the mail encodings and that's how you put it like sc and then we finally go to yep i'll learn on a separate encoder that takes the spectrogram and then he combines them but it's a single vector embedding that is placed in front of the attention context two encodings were produced for each training sample which are averaged together and like in all modern practice learned positional embeddings are used which is pretty much becoming the norm yeah learned positional embeddings are used which is pretty much becoming the law yeah learn positional limitings are used you're right and then he shows a graph of this uh training and how he was able to train then the clvp that is very similar to clip where he used to re-rank right so the original DALI worked by decoding a large number of images which is used using the CLIP, right? I continued following the lead for TOTOIs for reasons that will become evident in the results section. Built a simple model that is very similar to CLIP which I called Contrastive Language Voice Retained. It produces the distance matrix for the text and speech pairs and that's how you denounce it it's similar to the text except that uses two of them one for the text tokens and one for the melt tokens which makes sense because he had both yeah tokens from both the encoders were dropped out of the data 15 fixed positional embeddings were used maximum input length was 3 for 50 tokens and for Mel tokens the length was 293 for 13 seconds on diffusion decoder this is the final decoder right so it mostly resembles the traditional unit model used for DDPMs, but without any upsampling or downsampling. I do not understand what it means by upsampling. Why didn't have to do upsampling? Look at the third paragraph. It explains that. Remember, we covered the UNET, right? What do you do in UNET? You sort of convulate over that and you extract a lot of features. And then you end up with a latent representation and then you start upscaling it using effectively the decoding part of the unit. Yes, but he's saying that he did not use that upscaling or downscaling. In training, I threw several different architectures and conditions. This includes a traditional unit, the full attention. So he's explaining that the full attention, this necessity is very small. So he is using the unit, right? It most closely resembles the traditional unit model. That's it. But without any upscaling or downs yeah up something and down sampling just means that he's not doing any uh I suppose session is that right up sampling down sampling would be he's not he's not taking frames like sampling the frames the the voice thing right he as you're looking at the voice he's he's not eliminating any part of the signal he's taking the whole thing i see okay yeah this i understood okay because in this many samples but that is there he goes into the in future works he's given some uh hints as to what the future works would be now i i have a summary of the key points from this paper. So it proposes a new text-to-speech system called Toc2S that combines three things. Autodegressive transformer, your denoising diffusion probabilistic models, the DDPMs, and a contrasting model for evaluating the quality. That's what is called CLVP or something. Yes. CLIP adapted to MEL spectrograms. Yes. And then the unit they are using for the diffusion kind of thinking and then that's it. Yeah. It uses an autoregressive transformer to generate a sequence of discrete tokens representing the piece spectrogram from the input text right a ddpm that decodes those tokens into high quality speech spectrogram and a contrasted model clv piece that scores the output to select the best one right then he incorporates a speech conditioning input which are the encoder embeddings from the example audio of the target speaker. This helps infer tone and prosody. The prosody was explained a little earlier, right? So that's why he took the audio of the target speaker so that he could infer the tone, the prosody of the speaker. Right. Nice. And the autodegressive and the diffusion models are first trained separately. Then the diffusion model is fine-tuned on the autodegressive model's latent space for better efficiency. Actually, somebody has, there's a review paper, I saw a review article, I saw, which compared tortoise and valley and all of these bark and all of these things together showed their strength and weakness. I wish I could pull that up. and valley and all of these bark and all of these things together should there strengthen weakness i wish i could pull that down yeah and then of course he talks about how did he train it and so his training used eight rtx 3090 gps over one year and he skips a certain amount of data a lot of data system to train it one good thing was there is a there's there's actually a tortas.tts github you can go there and you can find all the information there so it's very easy to search it so if you do tortoise wild screen you'll get the github link and yes useful information available here yeah the best is completely open source isn't it yeah exactly how it's also available on hugging that's what i was going to say it's available on hugging face so somebody wants to try it so i said one of the follow-ups i was thinking uh maybe is a little bit of a double click on personality maybe as a follow-up paper or if people deep into that subject a little bit yeah that's it that makes sense we should do that that's all I had to share thank you so much thanks thankoshmi this was a very very well done and really creditable that you met you dissected the paper and understood it so quickly i have still a lot of things to learn from you and susinder on this topic yeah enough from me so shinder is the big expert on signal process signals um announced And I asked you a question, what is the explain the learnable position encoding? You remember that the, if you go back to the attention is all you need, you remember that the position encoding was via formula, right? The sinusoidal formula, right? So what has happened with, over the years is people have asked this question, why shouldn't even the position encoding be more efficiently learned? Let the neural network learn. See, even the word, the first word embedding, token embedding, it learns, right? Even before it feeds it into the attention layer. Once you free it up and say, go learn your own token encoding, it learns. So the people ask this question, why not the position encoding should also be learnable? And they found that when you make even that free and say, we are not imposing a rule, go figure it out. It turns out that it learns it quite well. So the new thinking is let the position encoding as well as the token encoding, both of them be learnable. That's all there is to it. That's what I think. Position encoding is a hidden state. Yes, in a way it is typically done using just feedforward. You're right, as far as I know. You don't do anything complicated. You just sandwich your feedforward and say, what should it be? It's weights. All right. The next in line is Satyam. It's your turn. Satyam, you're ready? Satyam it's your turn Satyam you're ready your turn I'm not reading I'm not good at it you can try Satyam whatever I didn't read anything because he was covering for me okay next is uh Akira Akira are you there yeah hello hello sir this is yes yes remember your represent support vectors I I don't know if I can explain the paper well enough like Sushinder or moshmiji but I will try my way uh I'll just share my screen yeah so so guys for reference uh Akira is a part part of the teaching staff here she is a teaching assistant so we have to expect higher standards no no no pressure but today is weekend right so it's day off right it's a day off now by the way I'm joking I don't worry so so the paper that we are going to discuss here is transfer learning from speaker verification to multi-speaker text-to-speech synthesis. So when we started reading this paper, we realized that we had to read another paper before this, which is WaveNet. That is one of the neural networks that's used within this architecture that they are proposing. And that is something that we will go back and do so in this paper basically they are describing a neural network system for text to speech conversion and this is the basic architecture that they are proposing so this architecture basically consists of three components. One is a speaker encoder, then there's a synthesizer module and a vocoder. So I'll try to explain these components. So the speaker encoder is basically an embedding model. So it takes an audio and converts it into an embedding. So the reference speaker's audio is taken and it embeds it into a 256 dimension vector. So what happens here is the encoder captures the nuances from the voices and based on those nuances it embeds that audio signal into a particular embedding point in the space and then the synthesizer module so what happens here is like as cinder had earlier explained the the phoneme so the phoneme is basically uh the i'll show this the right side you can see for a sentence this is a big red apple the the sound of each character uh of the word of each word that is basically the phoneme so uh what happens here is there's a mechanism here to convert text to phoneme and these phonemes passed into another encoder and and the embedding of of the the then embedding that was generated by the speaker encoder and the output of this encoder is concatenated and it then passes to an attention decoder mechanism mechanism where each the decoder then generates a mel spectrogram from this uh concatenated uh i'll just move to this yeah so the attention decoder mechanism then creates the mel spectrogram uh based on the reference speakers audio and the uh that that the phonem that was generated from the text uh then the logs the smell spectrogram is uh it goes to a vocoder or voice encoder this is basically the wave net neural neural network that I just mentioned so this is a different paper in itself wave net so So this is a different paper in itself, WaveNet. So it would take this MEL spectrogram and it would generate an audio waveform at the output. So basically, a reference audio is taken in and the text is taken. It combines the two and generates an audio signal as the output. So this is the architecture that the authors are suggesting in this paper. So what I understood here is that the speaker encoder is a stack of three LSTM layers of 256 dimension. LSTM is a long short term memory neural network and the synthesizer is a tacotron tacotron 2 architecture i i really need to go back and learn what this architecture actually is all about maybe asissa you can explain uh more on this uh and also in the neural network uh neural vocoder is an autoregressive wave net uh which which converts the massive spectrogram into uh audio waveform so one of the advantages of this model is that it can do zero short uh identification I mean generation of voice as well so even those voices that are not part of the training data set it can generate it within like a short a couple of second audio from that it can generate the audio from uh from a text and uh as part of experiment uh the the authors use the vCTK dataset and the Libris speech dataset. So, it has, so each dataset has like, so the VCTK dataset has 44 hours speech from 109 speakers. this data set to reproduce to you to use those voices and use this model to reproduce the same audio signals and then they try to they do have their own metrics to compare how the results are like how natural the results are how how similar it is to the reference audio and also speaker verification. So if this is a video, the model will be asked to identify which speaker this is. So they use these data sets to run good result on this model that they're proposing. And also, they also bypass the encoder model here. They basically bypass the encoder model here and just use the text to the phonemes part and put that into the synthesizer model and they in they found out that even with like a fictitious speaker like they are able to synthesize a new voice all together without having a reference to look for the model to look to so just based just using the synthesizer and the vocoder alone they're able to generate a artificial voice so this is basically what this paper is about uh and one of the drawbacks some of the drawbacks this paper uh this model is that uh it it fails to like capture it fails to like capture like accent when it comes to like heavily accented the reference speaks audio or it it it kind of misses to exactly reproduce those audio and the naturality of course it cannot capture the exact expression or rhythm or pitch as well as as we expect so these are some of the drawbacks that the the authors observed with this model so this is a basic idea of what the the others are proposing with this model yeah is there a github project what's the name of the are proposing with this model. Yeah, is there a GitHub project? What's the name of the project associated with this paper? So there's a Google... So this paper was proposed by Google. So there's a GitHub link. Google's GitHub... This is what... Okay. Yeah. I've seen this. Okay. Yeah. I've seen this. Nice. This is the adaptation. It's part of the Ticotron project. Yeah. I still need to figure out if they've actually released the full model as like a full usable model that we can download and use. That is something I need to figure out. But yeah, I've seen people... Try and search for Dacotron, see what happens. Sure, sure. I've seen some people like taking individual components and putting together on their own and seeing how it works. So I've seen some people do that for this particular paper. Mosli, you had a question? Yeah. These are verifications, I want to double click on that a little bit. One of the things that I'm, that I am interested in is, is he going to take anyone's voice, right? And that gets into, you know, power and all that stuff. I'm thinking, how do you verify if the audio is actually the person speaking or it was generated is that something that's true though is that what yeah is asking a question that with these generated models the fact that we are very impressed with the fact that it can imitate your voice your tone of voice and everything so the speaker like how do you know that it was the speaker really saying it, or worse is it was fake? So there are models people are creating to detect, because see at this moment, right, the imitation isn't perfect. So as far as I know, reasonably you can tell apart. But I think going forward, the more we move forward, even under careful examination, it may not be possible to do that because these systems are becoming very, very good with time. because these systems are becoming very very good with time even today to the layman it sounds pretty real but I I have seen some papers and obviously I'm not a super specialist in this domain but I've seen some papers which says you can successfully disambiguate between real speakers voice versus synthesized yes so there are those things references which were there in the other paper which was the microsoft paper where they talked about building these models which can detect whether it was artificially generated or not one concern i had with this tacotron um one concern I had with this tacotron uh um thing when you go there's no work after October 22. so does it mean that they have given up on this waveform kind of a thing and moved to codec see or uh see when you look at the architecture and obviously again with the caveat that I don't I'm not a super specialist in this. Compared to the model that for example Sushinder covered and that Mosley covered, especially the one that Sushinder and Mosley covered, they look far more nuanced than this. If this model's big, I suppose, virtue is that it's a straightforward model. But if you think about it, as Sushinda brought to the fore, that the encodec is really very powerful, even at compression and representation. And the paper that Mosmy presented, the Tattois, brings the fact that it is very much like, if you think about it, it's somewhat like blip. A blip had a, you know, the contrastive loss function, it had an autoregressive part, do you remember that? So there is a very vague resemblance to blip in that, and blip, as you know, is a very successful architecture for that space and in some sense tortoise has a vague resemblance to that those are more some of they feel like more powerful ideas so i don't know what happened to this one though So I have a question. So all these, you know, since the embeddings are based on the phonon, does it mean that it will work good for other languages also? Has there been any mention or anything that has been tried for other languages? Yes. So for example, this one, the Microsoft one, remind me the name. Vale. Vale. Vale X specifically does very well in a multilingual context. But going back to the basics of linguistics, yes, languages are best decomposed into sounds. How they sound, those are the phonemes. And phonemes are a universal concept. Every language can be broken up the phonemes and phonemes are a universal concept every language can be broken up into family i i think so uh may not be for true for pictographic languages but certainly the english the hindi the languages that we are familiar with the best representation is in phonemes like sushila pointed out right we talk. If you ask somebody how talk spells, it sounds like tick-tock, the talk of. There's no L anywhere and so that's the difference between a word and its phoneme. Duck is the U-K. Duck, yeah. but I assume this there should still be some language specific training right because when you are actually writing it in the text there is no translation going on at the input part which means if I have to sound raw then i need to know how it is written in english right so which means a language specific fine tuning is necessary for these you see there are i would imagine so so for example there are many sounds that don't exist in other languages a tamil for example has the the sound viri. One of the people, in fact, the teaching assistant, those of you who remember Kayal, her full name is Kayal Viri. So now, viri has no English equivalent at all in Puneese. Likewise, in Urdu, we speak of kaf. There's a ka and there's a barika and for example my name starts with a big curve it's when people who don't know the distinction call me they actually are mispronouncing my name so yes i would imagine that language specific training should go a long way in making it more realistic. Any other questions guys? So the next is Aditya. Hi everyone. Can you hear us? Yes. So it's Albert, Prashant and Aditya in the room. And what about Madhu and all of those are also there? The remote ones. I think Jay is there as well on remote. And Hari Priya is there as well. Yes, Hari Priya. Nice. Okay. So can you go on the top, Albert? So let me just talk about, yeah. So this paper is Neural Voice Cloning with Few Samples. This paper talks about using a pre-trained multi-speaker model and how to fine tune it for new speakers with a few short learning. And it talks about two of the approaches in order to do that and compares which one is less compute extensive. And this paper also talks about how we can play around with some of the architecture, or I would say architecture actually, some of the voice morphing to change from let's say male to female voice or to change accents. Guys, this is one of the seminal papers. It's been heavily quoted. If you go to archive and see the number of references to this paper or citations it's huge so in a way this is one of those originals that got this whole thing going please continue it's nice that you picked this paper thanks to prashant for picking it actually so um this is coming out of baidu's uh company and uh we just went through a little bit of extract of what this paper is talking about. It uses Deep Voice 3 as the base for the model. And then talks about how we can go about learning different kind of aspects in the model in terms of tweaking for new speakers. So it talks about two approaches the first one is uh the adaptation uh if you can go down in this paper so it first talks about related work for example uh the base model which was deep voice model we didn't go through that paper as a reference, but we started with the methods which they talked about in terms of adapting it for new speakers. So the first one is speaker adaptation. It talks about how we can tweak the model in order to get the voice cloning for new speakers in few shots only so the first way is it's saying uh we have our speaker embeddings in our model along with the weights and there are two ways one is to tweak the whole model basically whenever we have new data points coming in, that is our new speakers coming in, we either tweak the whole model and recalculate the weights as well as the embeddings and for the optimality in the model and figure out what is the best for the model the other way is just to tweak the embeddings itself and then figure out uh what is what happens when we tweak the embeddings and find the optimal embeddings as for the new speakers that is the speaker adaptation uh method and the other method is talking about is speaker encoding uh the way we understood this is instead of training the whole model again or finding the new optimal embeddings what if we incorporate another smaller model with bigger model which can predict embeddings model, which can predict embeddings given we have some audio sample with us. So, um, if you can go down here a little bit more, so these are the, this, this, uh, diagram explains these two different, uh, approaches on the left-hand side. We have speaker adaptation in which there are two different approaches. On the left hand side, we have speaker adaptation, in which there are two methods listed. The first one is basically that in the training phase, we have the text and the audio as an input, and the speaker embeddings are getting learned by the model itself while training phase. And when we are tweaking it for our new speakers there are two ways in which we can do it the first one is that we tweak only the embeddings and we basically not tweak we basically find the new uh optimal embeddings as per the new speakers coming into our data point and the other one is not just only the embeddings are getting optimized once again but also the weights are also getting optimized so this is the speaker adaptation and this is quite computation extensive because we are changing a lot of weights and embeddings in this process on the right hand side the encoding method says that we have our training phase as what we did earlier but also incorporate another smaller model which is speaker encoder model is speaker encoder model, which is trying to predict the embeddings given we have some cloning audio as our data points as well. So while we are training our original model, this becomes another, I'll say, additional component in the model to learn. And once we have learned this, given that now we get new speakers we don't have to find the optimal embeddings again we don't have to retrain the model once again and what we can do is we can just predict as per the new speakers what should be the embedding and uses that as an input to our overall model any questions guys did you get this so the way we envision there's that let's say we have our linear regression if we have to explain it to somebody from first year of college, we have our linear equation as let's say y equals to mx plus c. So when we get new data points, we don't want to calculate always our parameters of let's say m and c. m and c what we can do is um we can we can create another smaller model while creating our first model itself for training our first model which can predict that given a new data set comes in what will be the value of c for us or the constant in this case we are uh the new data points are new speakers and the c or the constant or the bias is as per the new speakers coming in there are different biases for the new speakers and thanks to prashant for bringing up the terminology when we were discussing it that we're thinking we were thinking of how we can explain this this This is how we thought of that. Let's think of it, the constant as a bias is for each speaker as such. So that's what the crux of this paper is. This is a little bit older paper, this is from 2018. So it doesn't go as per zero-shot learning. There are, I think it's few-shot learning though. And it is giving us different ways of training our models for new speakers I think there is a question but we can't hear. Master, you'll have to enable your voice. I'm basically getting. I have a basic question on the first screen. You have a text and audio which I understood. Where is it getting the speaker embedding from? The speaker embedding is a parameter in the model itself at the first time it is trying to find out what the speaker embeddings should be when it is training the model but why is the arrow going to the model if it's trying to find out the speaker limiting you shouldn getting you shouldn't do that. No. Sorry, what was that? The diagram could have been more clearer. Clearer, yes. The arrows are a little confusing. We were so much confused by all the arrows over here so over here to create the understanding we have and we may very well be wrong I rest my case. All right. I think Asif, you are on mute now. We can see you speaking, but we can't hear you. Okay. So I was saying that, yeah, this is a really good old paper. And actually, there are a couple of good youtube videos that go over this paper you may you guys may consider watching those i forget which youtube it is i'll try to find the link and put it there but they are there and they are good this is very good thanks for bringing this up uh Questions guys, any questions for the Next we'll go to Andy's Mountain. Who's there from Andy's Mountain? Andy's Mountain, Praveen. Anas. Anas, don't disappear on me. Sanjay, sir. Sanjay. Sanjay. Sanjay is from a different, you're from different team, right? Yes. Okay, so why didn't you present? So Sanjay is going to present next. But what happened to the Andes's mountain they have the mountain has disappeared yes anas what about you i thought you would be making a good presentation on behalf of your team dead silence an ass is ready actually. Can you guys hear me? Okay, so actually Anas and Amrit, they just said they'll go after this. After Sanjay's presentation. Sanjay, it's all yours now. You can hear me, right? Yes, clearly. You can share your screen and go at it oh haripia has a hand raised uh haripia do you still have a question sorry this is from the earlier raising hand. Let me lower it. Thank you. Okay, I'm sharing my screen. Can you hear me? You can see my screen now, sorry. Yeah. What is the open source project associated with this paper? I will find that out and see. I didn't see anything, but I'll find that out. And is it expressive neural voice cloning at this continue? So it's me Harini and Sukpal online. I'm here in the facility. And this is about expressive neural voice cloning. So it's kind of a step beyond just the voice cloning and focusing more on the expressive aspect of the cloning. So what we tried to do here and Harini and Sukpal, feel free guys to jump in whenever you feel. So we tried to summarize this in the form of a PowerPoint presentation here so that we will highlight only the the points that are important in this paper and then we'll switch to the paper if it ever necessary okay so while it talks about just the voice cloning also and then it highlights the limitations of the current voice cloning approaches in the text to speech it highlights basically the lack of ability to control the expressiveness so you know that's that's the key aspect that this specific paper focuses on okay so while it's able to the current approaches are able to do a very promising, able to give very promising results on the general synthesis, the lack in the expressiveness and the further lack in specific aspects of it, such as the variation in the tone and speaking rate, emotions, etc. And then they are also highlighting the advantages and applications of those in the modern day animated films, your expressiveness in the deep fake videos and translating speeches from language to the other where you want to preserve the speaking style of the speaker but just change the language and you know so the paper highlights those limitations and then it further sets the glow goal to am I I'm hearing my Echo are you guys able to hear me? Yeah, we are able to hear you. Okay. Okay. So the goal it sets is basically uses the same tedious synthesis where it wants to use unseen speakers voice but it wants to control the the style aspects of the generated speech okay and then the approach that they have is um condition a model for text speaker encoding peach contours and you know these styles of the the the the um speech and then they're also sort of focusing on, let me put my thoughts together. So they're using an existing model and then they're focusing on these aspects of the speeches. And once we look at the model here, we'll be clear. So here you see that it's very similar to the model that Atira and also the model that I think the previous paper brought up where you have a speaker encoder and then there is the synthesizer and then you have a vocoder converter here. So here you have the target speech speech here then there is a text which is an input the speaker encoder actually replaces the one in the synthesizer so these are the big three components here the speaker encoder the synthesizer and the vocoder let's take a look at here so the speaker encoder is conditioned you know on the those styles of the the ones that we talked about aspects of the speech and it uses something called the global style tokens so this is a dictionary of these latent styles and it uses that to train itself and i get my voice my echo again and again so but what it is doing is it's using that global style tokens to train itself for the style aspect and then it is also trained to discriminate between the And then it also trained to discriminate between the different speakers in the task speaker verification. Technically speaking, you can see here, it's three LSTM layers with 256 cells. And it uses, I'll show you further, it uses the male spectrogram in the synthesis synthesizer. With 40 channels, so let me go to the next slide. Quite similar to the one that. yeah it's it's very similar to what I did I brought actually but it just focuses on a different aspect of this page so again you know same um tecotron model you can see here but um the the the approach here is it uses two algorithms one is you know what the tecotron has and then it combines it with this yin algorithm which focuses on the pitch contours and extra and it combines those two so you see here in this model the text the text and this is concatenated here while the the contour from here is removed you use the gst which is the style dictionary and all that is concatenated here um so that's what they're trying to explain here that what does yen algorithm is the pitch contour derived using the n algorithm yeah i'll go here. This is one reference here. Let me just go to... It's all right. It's some way to get the pitch. Pitch is the frequency. Frequency can tune. So... I mean, they're talking about some frequency estimator for speech and music, sir. Yeah. So, sir, yeah, this paper talks about like how to fine tune some speakers voice into the text you have actually. So it's taking apart all the audio components, the speaker information, voice contour, which is pitch information. And then as I said, said global GST global style tokens and then yeah and then yeah so there are four components you take take them apart and then remove the text component and give it you are text and then on that thing it will take all other three components impose that and it will create then yeah the cloning of the voice yeah there is a specific uh there is a mention about this here in algorithm um I'll I'll get back that with that just a question this is for support remember see you do a bit of is it related to when you have a mixer right yeah remember we do all this frequency shaping the manipulation stuff yeah and then last which can do right that that is a form of pitch contouring isn't it yeah yeah and i think you superl is for you can talk about that frequency you know different frequency bands if you want to no no so most of the company only the synthesizer part here we see is kind of ai generated vocoder is analog part we can buy you know that yeah it's it's common it's a decoder to convert into a web forms that you can listen to. So that's a pretty common part of various models itself. And I think that these other two parts are all very similar also to other models. So you're not going to adjust some things? Okay, nice. Uh, I don't know. Also, they are these guys, they can further go into details here about, um, you know, this is where we, we kind of ran out of time to, they talk about, you know, the, the loss of that you know they're using l2 regulation here to reuse or optimize the laws and you're also talking about um further the work that needs to be done so the sort of future works is also saying that this is the work that needs to be done. So the sort of future works, it is also saying that this is some future, it's not up to the expectation is what I get from here. And then there is some future work that needs to be done. Okay. So the method, can you hear me? Yes. So they're doing two three things here actually we they derive the pitch contour using the in algorithm and scale it linearly to have the same pitch as that of the target speaker samples that is done using the in algorithm and they use then they use the target speaker samples for obtaining the GST embedding for both the proposed model and the baseline Tachytron and the GST model. Okay. And then they reconstruct a sample from the target speaker. In this setup, we use the text and audio pair of the target speaker and that is not contained in our examples actually samples and try to reconstruct the audio from its factorized representation using a synthesis model okay nice yeah and then they use the style transfer also similar to the test test we scale the pitch counter to have the same mean as that of the target uh speaker samples to retain the speaker specific latent style so they're trying to retain the speaker's style yeah and then they obtain you know by embedding and the gst embedding and the rhythm so they combine all this right nice so yeah so they basically remove this and then concat all these three together and that's essentially very nice so what they are saying is that they were happy with the results or they were not so happy they were happy but they are also indicating that there is some future work that needs to happen on that that needs to be done yeah so this paper is which year 2021 okay in our time has there been a follow-up to this work uh we haven't seen that yeah but they say this paper actually achieves the naturalness. And yeah, it's it was they conducted a crowd sourced listening test on AMT and asked listeners to create each audio utterance on a five point natural scale. And this actually came up till 4.11. Whereas packet drawn and all came up till only 2.1 2.5 base thing where you replace your singer. Right. Yeah. So creating a music song with a different singer singing the same song. Yeah. So what I find remarkable about this paper is, it seems to be the only one using pitch control. Pitch control. If I understand it right right pitch control is see when i'm looking at my uh equalizer in front of me right the first thing you do to get a particular voice is we do all this you know the frequency of pitch controls yes so in a way it is the first one that takes a learning from real world real world equalizer and mixer to the to this models isn't it yes so yeah it even says that it can detect happy sad neutral because of the pitch counters and it has the control over the expressions you know from the synthesized speech has the control over the expressions you know from the synthesized speech actually that brings up a question are we focused more on the human emotions and that should be a key factor should be yeah i suppose so it captures the emotion actually we're just reading a paper that's that patrick in a wrote about emotions how can you bring emotions into it just came out last week wrote it about emotions how can you bring emotions into it it just came out last week i think or very recently in this month yeah i think they are also explaining here the data sets that they use to train and also you know you can see here that um say nvidia 1080 gpu and hardware 1080 GPU and hardware. This was trained on a single Titan 1080 GPU. Yeah. Took around four seconds per iteration on single Titan 1080. Nice. So actually this raises a, I think what I'm trying to get is some more clarification. For example, right, in text embedding, I think we are all pretty clear on, you know, what the embedding has, words that is on a particular sequence in a sentence, and we have the window size masking and all that based on that we actually get the uh text embeddings right and similarly image embeddings like you mentioned earlier also like in an image like the image can be broken into components which are a word for a sentence which is the entire image right so in that aspect what are the components of this voice and ready I mean I hear pitch and it makes sense because pitch and tone and all these are important aspects of a song but maybe this is very fundamental but what are the key components of the voice embedding components of the voice embedding see uh I don't know how to so but one thing I observed if you go back to old times right people used to talk of a pitch amplitude and timber something called timber right of a voice I don't hear those words used recently but if you look at this papers that we covered today there are multiple approaches the mel spectrogram is very frequency frequency domain based then and it uses that and this one goes back to something new like the or rather a pitch contour in some sense and then if you look at the paper that Sushinda brought up, it says that using this work of Facebook, it basically says let an encoder-decoder model, like that model, infer its own latent representation of audio. Let us not explicitly go and do foyer transforms or go into time or frequency domain. Let it come up with its own abstract embedding. So we see a variety of approaches in these different papers. One likes MUL spectrograms, which is the traditional approach everybody has liked. One comes up and says, no, let's start from scratch and do a full embedding, Now let's start from scratch and do a full embedding, full new encoding and achieves quite a significant good result that beats even MP3. And then this is more along the lines of still having your MEL spectrogram, but adding some pitch, contour and other things to it. Okay, Thank you. So if we generate, say suppose as if we take sample your voice, and then we basically train it and create artificial voice that should sound like your voice. Yes. How would we compare it? What would be the quality that how would we compare whether the generated one is close to your voice? Well, that is subjective. That's why you have all these graders. And in fact, we will know on the last day of this bootcamp, each one of you will have to show how well you reproduce it. And we'll all rate it. Remember the bootcamp now, the final project is, actually there is one more project that is still there, which is natural language to SQL, but we'll bring that in at some point. Here, the main thing is, but this project that I just released is that create a virtual teaching assistant who can either speak your voice or my voice you can pick your choice which one you would like but if it is my voice other people will judge or your voice other people will judge how natural it sounds so i was seeing a lot of reference in that valley and the valley x where they uh not only refer to the tone but the, what do you call, the inherent, I don't know what's personality or the characteristic of the speaker, whether he's happy, sad, or those kinds of things, they generate it as well. So you don't want a monotonous speaking voice, because then you immediately know it's not the real person. So they said about statements, questions, like when you ask a question, the voice raises. When it's a statement, apparently it goes down, things like that. That's right. And it's also about the way we speak. For example, Indians tend to have a more monotonous, more even language. Americans tend to emphasize the second syllable of each word quite strongly and it's also a very relaxed jar speech here the lower jar tends to be far more relaxed than a British way of saying things and then that's interesting because these guys are also saying somewhere here that they had sampled British accent in their in their data set. Oh. Yeah, somewhere they mentioned it here. Is it because these guys are situated in Hong Kong and they speak British English? Oh, no, these are San Diego. Hong Kong and they speak British English oh no these are how come they went after the British English and the other part that that what Sachin was highlighting is basically they're also saying here um that while while they focused on combination of this heuristically derived and latent style, they're also sort of saying that future work needs to be done in this specific style. Nice. In the last month, I suddenly see a lot of flurry of activity around emotions. And it turns out that the Transformers seem to understand emotions they do and it's a it's now becoming sort of a hot topic you see papers coming up and so forth that that's pretty much what we had on this. Asif, an unrelated question to this, when you talk about emotions, do you think that reproducing a song by an artist and including the components of emotions, is that something that's also easily reproducible? I think it will be. We are almost there. See, here's the thing. At this moment, synthetic voice generation still can be known. You can tell that it's synthetic. The detectors can pretty much tell apart, but the gap is decreasing. And quite soon it will be indistinguishable. I think somebody told me that one of the beatles is dead and apparently his voice is now being used to render new songs in that beatles voice and okay i guess i know that maybe um if you were to take a song and remove any audio from it and actually remove the singer voice from it, perhaps you may have to filter out anything that's within 20 hertz to 20 kilohertz, right? Or 20,000 hertz from it. But that would also consist of removing any of the components related to the instruments that may be playing in that part so is there any way to kind of automatically detect the human voice off of the music in the background is there like a that problem was solved a long time ago it's so see go back to the old cocktail problem like two people are speaking in a bar multiple voices are there can you break apart from the from the noise that comes which is a mixture of many voices can you separate out the voices and what each one of them is speaking it's called the cocktail party problem and i think it's a sore problem with very fairly good good results it can be done so to telling tell apart when music is there in the background instruments are playing and a person is singing to separate all of them out is not a big deal okay thank you all the time actually I see one question I have I mean we've gone through these people so many and then you know we see the cost associated with this is so large that becoming it I mean commoditizing it is probably far right yeah that's right I mean there's a reason nbgs stocks are doing so well it's a today the biggest problem is all this ai has created the world of haves and have-nots haves are the people in the trillion dollar club i'm not even a billion dollar club they're the trillion dollar club uh companies and the rest of us are all poppers. We are the have-nots. We are sitting with our 4090s, maybe four of the 4090s. Each one of you are sitting at your home machine with a 4090. So we are the poppers. It needs a fundamental breakthrough to solve this compute cost flow. Let's see. That's the most needed breakthrough at this moment, I would say. Yeah, I mean, one of the things that this paper talks about, right, making it available for, you know, for example, here I have that voice assistant and, you know, people who cannot speak, you know, it's manufactured sort of devices for those but I think things are so far from that yeah I mean see these things have multiple uses right there are people who can't speak but they are also people who can speak comfortably only when they are not in a public place for example um I am pretty articulated on strengths but the moment I feel I'm standing on a public place. For example, I am pretty articulate among strengths, but the moment I feel I'm standing on a stage or something like that, we all have certain degree of stage fright. So imagine like none of us, so a lot of very, lots of people have something to say, but they wouldn't say it in a public forum. They would be too scared, but they can write, for example, they can write articles. And so with something like this coming about, instead of just being words on a page, now what they have to say can become, you know, the spoken word in their voice, their emotions. And that voice is captured when they are relaxed, comfortable, expressive, not feeling stage fright and so forth. And then that thing, all their thoughts re-expressed in that comfortable voice. I think it will be a big boom. All right. That's all we had. Harini and Sukpal, feel free to add your concluding thoughts. I think that's it, sir. Yeah. That's in a nutshell, sir. Yeah. But I think this paper is the baseline where you can create your own artificial songs. Yes, it's good. It's a good paper yes it must be hey i on the same line i found a very interesting paper your zero shot exclusive yeah based on this oh please share it on zoom don't share don't share share it on slack so that everybody yeah that's a great one must mean media it's a very good one sir yeah same thing creating it that's nobody found that okay we are finding it's good creating harmonious sounds you just give it lyrics and it will create a song by itself yeah i mean i'm saying like this is a voice calling but even zero thought yeah it's based on something on which i'm not very familiar with but suddenly it is the state of the art yeah that's good isn't it just a variety of the speech but i don't know okay but it's no this one is zero for the exclusive same thing for expressive clothing voice clothing with zero stuff based on this yeah i know my daughter keeps telling me some things that I always forget. She says that music is made up of two parts. One is harmony. Maybe Sukpal you know it. And the other one is what is the other part to harmony? That in any music composition, there are two parts. One is the harmony part and the other part. Beats, beats beats is it it um i'm not sure melody yes melody and harmony right so i don't know if that is relevant i i am i am practically told there so nice amrit it's your turn yes anas oh which teams are Anas or Amrit? Anas and Amrit are going to pose. Anas is disappointed. He's scared. I was chatting. We're quietly staying here. I said, okay, I guess we can... Do you want to have a go at it? Go ahead. Good luck, Patrick. I'm not I'm not a big fan. Okay, so we're going to discuss. Let's, let's discuss that. Let me share my screen. okay so this will be a team effort as if oh voice box but didn't we cover that in the class that doesn't seem right did we cover it two weeks ago did we no no no you talked about it but we did not cover it okay then it's fair game all right yay all right this is really a good choice because this is really the state of the art isn't it yes so unfortunately the code's not available at the moment but meta released meta released uh the first generative AI model for speech to text the text-to-speech and and more actually so you can go to this this uh site to see on the paper what what it can actually do this site to see on the paper what it can actually do. But where also our group is going to slowly discuss and how we understood VoiceBox, which is based on a new concept, a new modeling concept called flow matching. I think it was, the paper was published also earlier this year. And the nice thing about VoiceBox is it's outperformed already the current state-of-the-art English model, the VAL-E that was discussed earlier for zero-shot text-to-speech. Now just some bragging rights of meta and style similarity. So the strategy behind it has to do with this idea of flow matching models, which is essentially, for diffusion models, we first learned, we used some Markov chain strategies to go from a clear image and to noise-ify, essentially. Now, the idea behind flow matching is that given that mapping of probability distribution from your first input to your target, can we map back? So that was where the discovery from the flow matching model is that they're able to find with some caveats uh and amit can help explain some of the math later but we're able to uh revert back or what was the what was the word invert it's invert where it's an invertible model so you can you can noisify you can denoiseify so now what happens is in voice box is yeah can we introduce it see the flow matching this is based upon uh normalized flow models and if you remember did i cover normalized flow models the jacobians being invertulnerable determinants and so forth no not yet okay so i should cover that because flow normalized flows is a big generative topic and perhaps the only topic i missed i'm going to cover the normalized flow when should i cover it we should come up with an idea maybe next week i should cover it Maybe next week I should cover it. Yes, sure. So for everyone to get on the same page, just to have an idea that from probability distribution to a target probability distribution, it can be inverted back. And adding some kind of mask to infer from the prior and from somewhere within the context of that input adding some kind of mask to infer from the prior and you know from some somewhere in somewhere in within the context of that input is the idea behind voice box so while inverting introduce this mask and then revert back the nice thing about this is uh because because the mask is predictive uh the model has been trained the voice box model has been trained to help predict and fill within, was it in context? I forget what was the term. Anyway, to fill in the mask, to learn from the mask, it is able to do style transfers also across infilling, infill speech from context. So it's able to learn and infill what was missing or what you had been expecting. So in this scenario, we could do a lot of things. You could do text-to-speech synthesis, or you could do lingual style transfers. You could even transfer accents and at the same time clone voices so i i leave it to the rest of the team to uh the rest of the members in this group to just read read through uh look through this demo but after this um i can hand over the the floor to Amrit who can discuss in more depth uh the actual paper for voice box so yeah sure so uh I wanted to cover some of the the methods that they yep I wanted to cover some of the methods that they describe in the paper. And again, this kind of touches on the subject of, or very much touches on the subject of normalizing flows, which is essentially like a model in which given an initial distribution. Give me one second sorry um yeah given an initial distribution you're trying to learn uh you're trying to approximate an unknown distribution uh also known as the target distribution so um for for the normalizing flows there are a few variables, such as the vector field, which is denoted over here, and also the probability path. And so the vector field moves points from the initial distribution to the target distribution. And you do this by solving this differential equation right here and and the probability path is used to create a time dependent uh a PDF and this describes how the data points change over time so uh the issue over here with um with voice box is that uh they want to train a network to learn the vector field, which aims to make the transform distribution match the target distribution. But in practice, they don't have direct knowledge of the probability path or the vector field, which makes it challenging to compute the loss of the gradient so so the way they tackle this is by introducing the idea of a of a conditional path a conditional probability path PT which is a mixture of simpler uh conditional paths and and these paths have known vector fields so it's easier to make the computations for them it's easier to make the computations for them. So Voicebox introduces something called the conditional flow matching objective, which is similar to the flow matching, but it again uses these conditional paths. So after they kind of formulate the problem, they divide the model into two parts, an audio model and a duration model. For the audio model, they are predicting missing audio frames given a context, again, using the normalizing flow model. analyzing flow model and so the input is an 80 dimensional um Mel spectrogram and they they employ a transformer to to model the conditional distribution for all the frames and then they use an embedding uh and concatenated with with the audio and context inputs uh and concatenated with with the audio uh and context inputs uh and this is for this is for also to apply a positional encoding for the flow step and so so the so the duration model uh the second part of this is uh and uh Asif please feel free to correct me if uh I'm'm misspeaking about this part because this part kind of confused me as well but it uses a conditional vector field to model the distribution and then you use a masked version of the normalizing flow for training and so I wasn't sure why they exactly created this mask. But yeah, this is the solution they used. How would you know? You need to do a reconstruction. Unless you mask something, how would you do a reconstruction? The mask, like, see, you do a conditional conditional vector field which swaps the excited the mass version of the loss is used for training uh for yeah simple way you need to look at the forward path right not not the you need to causally you know you need I think so you need to hide the parts that are in the past causally, you know, I think so, you need to hide the parts that are in the past. So suppose you are generating a translation of a sentence, surely, you don't want to use the future tokens. Show the future tokens. I think that's the reason but I'll have to read the paper more carefully. Actually, I don't know. I need to read the paper. I think I used to remember what it was. I forgot. I read this paper when it came out a few weeks ago. Yeah, they also discussed a second solution, which is like a regression-based duration model. And, you know, that does the predictions given the context and the phonetic transcript. Yeah, yeah, that's there too. Straightforward, which is straightforward in this. You're basically predicting predictions. And because these are vector fields, you can vectors, you can therefore look at them. Regression makes total sense because you're looking at the divergence of what it should be and what is right yeah I think it has to do with the causal nature of it or was it predicting no or maybe was it see the only two possible things one is to prevent the look at the causal aspect the other is to by, you make it a mask language model kind of thing. You make it predict what is it that was masked. I forget what that is. I'll read it more carefully and get back. I think it's in the previous page. If you go back back I vaguely remember that it's right here itself somewhere classifier class audio model yeah audio model go down right here somewhere it says uh with the condition and yeah is matching for simpler conditioning with conditions on all frames except of only mass frames a neural network is parameterized in the conditional vector even simpler conditioning we condition on all frames except of only mass frames a neuron if it is parameterized in the conditional vector given okay uh perform you i think that's what it is but okay i'll think about it i'll read it properly and get back yeah and uh the minute that i was speaking about is there in your equation too you know that is the classic signature normalizing flow models equation number two just a little bit you're very quiet compared to the other speakers oh i am very quiet okay now i'm saying if you go up to equation number two perfect thank you equation number two yeah yeah so you see that one in two are the things about normalizing flows. Yeah. And the determinant I was talking about is the determinant in equation two. Okay. I see. Yeah. I mean, so yeah, so the math was a bit tricky to understand, especially in 20 minutes, but you don't want to understand. It's a pretty big topic. You don't want to understand that normalize flow from this paper, you want to read it from like, okay, I'll explain it better. It's a textbook thing, best understood properly what normalizing flows are, and then coming to the city yeah but yeah the results were um pretty favorable uh actually i mean they they said that uh that the similarity uh was if we go to the results section really quick so they said they claimed that it outperformed the state of the art uh uh with a lower word error rate and a higher audio similarity score while being up to 20 times faster it beats everything and the tragedy is facebook is getting the same infection as OpenAI. They don't want to release their code or their model anymore. So all we can do is appreciate this paper, be impressed with their results, and then twiddle our our thumbs because none of us have access to it i think it's in the blog post actually you say that again in the blog post i was saying the results might be in the blog post maybe not okay there yeah yeah now all the results are very impressive like beats seems to be beating the state of the art in everything but the whole question is how do we get our hands on it just oh go ahead sorry so the best that is available for us is enhanced mark today yes enhanced bar that is pretty good as a file oh no twice is pretty good too uh it doesn't do music right uh because that has i think yeah i tested about uh toys and uhise and enhanced part. Enhanced part is much better, I think. Much better. Okay, nice. Actually, this voice box can do many things. It can do accent changes, language changes, and it can do multilingual. Like people who come, like for example, people from the Indian diaspora tend to speak, even when they are speaking in Hindi or Tamil or Telugu or whatever language, they tend to mix English with their language. It's a mixture of English and the native tongue. And a voice box, to my knowledge, is one of the best models that can understand these mixed languages and still translate. So it is very good. I wish we had our hands on it. I mean this actually is a sad trend. If this goes on soon none of the companies will be releasing any models. These companies have an infinite supply of GPUs. They'll keep making bigger and bigger and bigger models and you and I will be just sitting there on the sidelines reading their research papers and getting impressed and not getting to touch these. So it's a pity. Nice, nice guys. This is good that you guys picked this paper. So when I think about it, all the generative aspects, the VQ autoencoder, generative aspects the the VQ uh autoencoder variational autoencoder was covered we have a unit kind of concept was covered diffusion models were covered uh this normalizing flows are covered when it comes to generator model the only thing well not quite covered but generated again neural style transfer using Gan kind of model I didn't see cover contrastive loss clip based wasn't that the first one it wasn't a gang was it again they had the discriminator it had a discriminator but did it have a generator I forget that yeah the generator is the the Lat latent in the middle the the so it had the the auto encoder the encoder the decoder in the middle is like essentially the gan was essentially the gang this is the valley paper right yeah okay so yeah it looked like that i wasn't sure whether it is really it's i mean obviously the ideas are similar actually I don't know if it was then it's good then all the basically people have tried all the generator models as if the closest I've found for the voice box implementation somebody tried uh so I'm just posting the link there's nothing there actually the repo is empty but they're true yeah but some people are trying yeah yeah that would be nice actually if somebody implement so trouble is even if you implement the code who is going to pay for the GPUs to trade this some some government somebody should sponsor this, I do feel that now the AI model development should be heavily funded by National Science Foundation and the US government before the entire thing becomes an industrial capture of the big giants. What will the research community do? Just sit there and twiddle their thumbs. If you are not in Facebook or Google or Microsoft, you basically sit there and twiddle your thumb. Open AI or something. Nice guys. So this was a wonderful. Did we miss a team? I feel we have missed one team. Which team did we miss? Oppenheimer. Anybody from Oppenheimer still present or has everyone dropped off from the Oppenheimer still present? Or has everyone dropped off from the Oppenheimer team? Oh, nobody from Oppenheimer is here. Just two of us. I just updated you. Oh, yeah, your team also. And the other team that is missing is the Transformer team. Transformer team also, I don't see anyone. Okay, guys, so next time, we'll start with those teams but good show guys we had a pretty good set of paper reading but so do you believe that this is it's a good idea you guys should do the paper readings instead of me how many of you vote for that raise your hands in zoom good she not one vote how many of you would prefer this format in which you guys do the paper reading only one vote no vote even even sheenert's hands has gone down okay let me see if anybody is even listening how many of you would want me to do the paper reading raise your hand listening how many of you would want me to do the paper reading raise your hand I'm just checking how to raise the hand okay okay all right no I think this format I prefer because. You prefer this format, right? In which you guys do. Yes. Yes. Okay. So we will do that next time. I'll just give you guys the topic and you guys do the paper reading. And what I will do is I will, I will move my paper reading to the usual Friday afternoon or Sunday evening. One of the two, the public paper reading to the usual Friday afternoon or Sunday evening one of the two the public paper reading makes