 So we're going to talk today about this paper, Visualizing the Lost Landscape of Neural Networks. This paper came about two years ago and I like this paper very much. It has a lot of citations, if you go and look in archive you'll be surprised that i mean i'm perhaps not surprised because really a good paper it has gotten a lot of citations and it serves or sort of helps in one important thing see in neural networks we have a lot of things we do like batch normalization like regularization you know l2 l1 regularization or weight decays and so as we do all of these techniques and then there is a concept called dropouts or skip connections which i haven't yet explained to you guys but think of them as a short circuit where signal can go from the earlier years to much later years. It's a short circuit. In other words, instead of going to the next layer, some of it also short circuits to many layers later. So those are called skip connections or short circuits or residuals. And all of these tricks, each one of these tricks, one of these tricks when it came about it led to an improvement in performance of the neural net then there is always a bit of a debate which is helping you more what should you be doing and what should you not be doing so can we quantify what does batch normalization actually do why is it leading to better results we can give justification theoretically and say that see you're sent recentering the data the input data and making sure it's small enough when it goes into it wave decay also is essentially like regularization and to regularization you're2 regularization. You're basically saying that put a constrained surface around the origin and make sure that the weights don't increase too much. So every time you make a gradient descent step, if you have the regularization term, you'll notice that what it does is it subtracts the learning rate times a gradient but it also diminishes the weight a little bit, scales it down a little bit before it takes the gradient descent step. So those are weight decays essentially. So all of these things work. Batch size matters. I mentioned that we do not do batch gradient descent when you take the entire data as one big batch gradient descent you're much less likely to have good solutions it is far better to have small smaller batches mini batches and then the smaller mini batches what they will do and the intuition i gave you is they lead to the perturbation of the lost surface from batch to batch from from step to step or iteration to iteration and those perturbations smooth out the loss function so all of these intuitions they become very real when we could possibly visualize it. The problem is how do you visualize a surface that exists in something like hundred million dimension space? Very hard to visualize. Our visualization ability fails beyond three dimensions. Maybe add color or something and get some sense in the fourth dimension but certainly it doesn't expand out to the enormously high dimensional spaces that we encounter in neural networks. The other problem that we have with neural networks is we know that the loss function right off the bat, you know, are not convex surfaces. That was the reason people used to not like neural networks. So for the longest time, neural networks sort of went through a winter, arising from the realization that its lost surfaces are not convex. And so it's hard to converge. And it generally was practically hard to converge in neural networks. You would converge to some bad minima, local minima. And people did for many years, struggled with that. They tried all sorts of things and were getting nowhere. And then came these new ideas, starting with Jeffrey Hinton and many other people, Lee Koon and Benjio and Schofield. So you can literally see that the subject has gone through a renaissance. And these ideas are, and I'll just mention some of the ideas, batch normalizations, weight decay, then mini-batch gradient descent, and momentum-based optimizers, and so on and so forth. Each of them contributes something to the theory of neural networks and has moved the needle forward. But if only we could see what it all means, what it all does. One quick question to get a little context clear in my mind. I missed your ML 200, right? I'm doing the ML 400, which is focusing on the deep learning side of it when we use the word neural network and try to appreciate that for its complexity would you distinguish between neural networks that were there before the word deep learning got coined versus the ones that we're talking about today specifically with reference to when you use the word that they have millions of parameters I'm trying to understand like neural networks that work that would have been written let's say to predict let's say a bond pricing or something there in the financial markets versus what we're now using to recognize images or to do autonomous driving you put these as two different things or would you blend them together well see what used to exist are kindergarten or toy versions of what we have today. The fundamental distinction is that we did not have something called representation learning. The word deep learning is a buzzword that stands for something called representation learning and these very deep neural networks allow us to do representation learning. What representation learning is, we will see actually tomorrow as we make progress. So roughly speaking, what it means is that when you try to solve a problem, let's say that you have data and you're trying to predict whether it's a cat or a dog or cat or a truck don't try to have a single model that solves the entire classification problem but what you do is you give each layer a small responsibility let's say detect some geometric primitives is that geometric primitive there or not? And little notes in the layers or the so-called filters, as we will learn, filters, their job is to detect something. They're like stencils cut out on a page. So if the stencil is like an X, you just take the stencil all over the text. And if it matches an X, you say, okay, I found an X. So they all do small very primitive things in the early layers and then the subsequent layers they combine those to generate what are called higher orders of abstraction and then again higher orders of abstraction or representation and so as you go down the deep neural network each layer is actually doing very simple learning right and but these little bits of learning they accumulate and and the last layer is able to tell the difference between a cat and a dog or a cat in a truck so representation learning theory started with i would say that the year was 2006 and with the works of Geoffrey Henry. But it doesn't mean that some of the architectures we are seeing today didn't exist. For example, the, so while it's roughly is true that the neural network theory was rather primitive at that moment and people were trying with dense neural nets and so forth, hidden networks. And they were struggling. People were hand tuning. What was happening is that they were hand tuning, but they couldn't justify why it works. A lot of hand tuning would make it work. So for example, translation, language translation in people, you must be remembering things like a dragon naturally speaking and so forth long long ago they were trying to do that so behind them were neural networks and support vector machines the neural networks were often hand created with multiple layers and people were struggling to have a coherent theory of how they were creating it all they knew is that if they kept at it they would eventually create an architecture that somehow works then there was work that was done on cat's brain how does a cat see a picture so by actually studying the biology of cat's brain people realize that the way it sees the brain is a very interesting way and that is the way we learn about tomorrow condolations brain is a very interesting way and that is the way we learn about tomorrow, condolations. It's a very local effect. The certain regions of the brain are looking at only certain part of the image, not the whole of the image, and it's responding to certain regions. And that led to the development of the idea of the condolation neural net the convolation neural net for image processing the was the work of lee coon at all and they created the first architecture leanet linear is the classic architecture i would say that is the hello world of convolation neural nets that you'll study tomorrow was it effective it was tremendously effective and let me give you a sense of how effective it was. Do you remember that when we came, at least when I came to the United States, in India a letter sent from one city to another would typically take two, three weeks, isn't it? You send a letter from Bangalore and by the time it reaches new delhi it takes a very very long time right and often letters are lost and things happen but even then when you came to the united states in the late uh in the 90s you would notice that let us tend not to get lost and there's a very interesting reason for it. This lean-at architecture, it could do handwriting recognition. You handwrite on your letter, and if you ever look at a mail envelope carefully, at the very bottom of the mail envelope, you'll see these little squiggly marks, these vertical bars, like a barcode of sorts, very tiny little markings. So what has happened is when you put your mail into the mailbox it goes through automatic sorters the only time a human being gets involved is the person who took it to call the mail the box of mails and dumped it into the big pile that goes to the sorter the sorter has a neural network that recognizes your handwriting it infers what address that you have written and it takes the address and writes it in a digital form at the bottom of the envelope and thereafter the entire routing of that envelope that letter all the way to the destination post office happens purely automatically it takes the best possible routes and the you know all these optimizations take place no human being is involved when it reaches the final destination even there it is properly sorted that this belongs to the postman who addresses these streets so the postman just gets a box of mail every morning and knows that they are meant for their street and goes and delivers it so it brings about tremendous efficiency and the letters in in the United States can still go coast to coast in three days more or less give or take a day or two they don't take weeks if you really ask yourself how is that even possible because that much is just the transit time the reason is all the other overhead has been removed because they have been completely automated and their neural networks doing it and they have been doing it for decades now right so what was happening is a lot of these architectures they were emerging through a lot of trial and effort. And the theory was sort of very nascent. And some of these researchers, in the face of the fact that everybody said that neural networks are dead or they're a useless idea, they were still persisting. Actually, the most remarkable thing with neural networks is the likes of Lee Kuhn and Jeffrey Hinton, et etc. They survived multiple winters of neural networks. They went through and they stuck with the idea now for what is it close to 30 years? They just 30, 40 years they just stuck with the idea and kept working at it and they never gave up. And then gradually a lot of the proper pieces began to fall in place and all of a sudden you could see that this thing is amazing, it really works. So that's the history of deep neural networks. If you compare it, if it's a for example, if you pick up a book of neural network in the 90s, I think I still have on my shelf Bishop, Christopher Bishop's book on a neural network from the late 90s, I believe, or early 2000s. It is completely different from what the neural network that I'm teaching you today. Today we have a lot of things that simply were not known at that time. So you can almost say that we are learning a new subject. Does that answer you? Venjit, does that answer your question? Yeah, I'm actually making a little comparison in my own mind as if so I did attend a course on a neural network while doing my MBA at IIM, right? IIM Calcutta was a little more into quant. Which year was it? 96, 97, right? right I am Calcutta was a little more into quant which year was it ninety set ninety six ninety seven right oh I didn't realize you're so much younger than me okay go ahead so while doing an MBA there there were professors were actually with looking at using neural networks in finance and strategy so I did a course on neural networks I actually trained a network to predict the credit rating of debt instruments in India. And I built it all on a little computer which essentially is, I mean, I doubt how much the RAM was, probably less than 20, 32 meg, I think those were the RAMs at that time, right? Early Pentiums, yes. But yeah, everything got done. I did make a comparison between a neural network with MDA trying to say a multiple discriminant analysis that you will do in statistics and the confusion matrices and all that. But those are very puny networks. And I have no understanding of all the theory that we're talking about today. Those are just package networks that I picked up. That right so the package network is the word so what happened is that somebody figured out that this works and you would take that and tweak it and you make changes to it and adapt it to your problem but if you ask the people why does it work they would give you all sorts of hand-waving arguments that's where we were in those days. Now, I don't think that we are far better. We are still in the situation that this community has a problem, it has a disease. People will keep on hacking. And when something works, then they'll try to think up a post hoc theory to justify it right now you can say that well why is that wrong from an engineering perspective a lot of things we do is like that we experiment we make things work it's a very experimental approach or the hackers approach so engineers are perfectly fine with that but if you come with a theoretical background if you're a mathematician, then you're pretty much barf at it. You say, how disgusting. We need a proper theory, proper way to march forward. And so the theory, a lot of effort is being put into the theory and it's gradually moving forward. This paper that we are talking about today, I would say is in that genre. It helps you in a proper way get, you know, at least understand a little bit more of what is happening or why these things work in a very visual way, not just some theoretical post hoc reasoning and so forth so that's what we are going to do and that's why this paper is actually uh quite relevant and let's study this so this paper actually i'll give you the core ideas of this paper how many of you read through this paper uh you haven't so we bring it together then me, let us go through that process. See, I will use a few words of the vocabulary. Let me set it here. They will talk about the minimizers. In our language, in the language in which we are doing our code, these are, what are these? These are our optimizers what are some of the optimizers we are familiar with in the same thing your camera and not the board oh goodness goodness I haven't shared the screen Let me know when you see my screen. Yes, we can see. So we are going to talk about this paper, Visualizing the Lost Landscape of Neurologicals. A very well quoted paper actually. If you go to archive, the number of citations are in hundreds. And in the deep learning research community, that's a large number. So it's a beautiful paper in a quite literal sense. You see these beautiful visualizations that are there in this Patichitta paper. We will try to understand. And these are not just pretty pictures, they are actually very, very useful. Or at least I like it very much. So our goal will be the following. We will study the paper, then we will study some of the visualizations that they have created. You can sort of see how the optimization process happens. They have made some movies out of it which are very impressive but we'll pause those videos and discuss a little bit about the about what we are seeing and then we will go to a tool it turns out that they have created a visual tool and we will use that visual tool to see the difference between architectures and why certain architectures work and other architectures don't work so well and finally the code the code that they use to generate all these visualizations it is all there in github and you can take that code that library and create your own visualizations as you solve your problem and so it's a very powerful thing to do to be able to when you solve a problem actually show the last surface and confirm that you truly have gotten to a good minimum it's a big powerful argument to make and and I hope that we will go that extra level and make this argument so that's our goal I will set the goal again in the sidebar yeah let's put it read the paper together see the videos see the videos and discuss you see the videos and discuss and next we will go and use the visualization to use the rest and finally look at the library the library part we may not get much time to it will try to do and let's hope that we can do all of it in the space of an hour or hour and a half. So we'll start first by looking at this lovely paper. So just before I go into the paper, let me set the vocabulary straight. In this paper, you use the word optimizers, minimizers. Just think of them as optimizer. There is a little bit of a confusing terminology they use. When they reach, when you have a minima of the loss function, they often occasionally occasionally they will use the word am i writing the word occasionally correctly occasionally they will use just a minimizer mini my minimizer is the word whereas what they really meant is the minima I suppose they meant the minimizer around the minima. What is it showing you? What surface is it showing you? Then they talk about the loss surface. What does it mean? So imagine this again in the case of, if you remember regression, linear regression, it used to be beta naught, beta one, if you remember regression linear regression it used to be beta naught beta 1 if you remember and then this used to be the last surface of the error surface and what used to happen is in the case of linear regression it was a nice paraboloid right it was a nice bowl you realize that this is a two-dimensional surface You realize that this is a two-dimensional surface, isn't it guys? This thing is a 2D surface in the 3D space. What are the axes of the 3D space? Beta 1, beta naught, beta 1, and some loss value. The coordinates of any point is given by some value of beta naught, beta 1, and some value of the loss. Are we together? The error. So this is a recap of the theory that we did. Now in the deep neural network, the statement made is this is a converse. Okay, hang on. Let's just point out one more thing. In the case of linear regression, so in the case of linear regression, The last surface is convex. What do we mean by that? You can see that it doesn't have the concavities, the non-conconvexities it doesn't go like this things like that right so you don't see a surface like this things like this right this is not convex not non-convex whereas in the case of linear regression the last surface is convex it has a global manner it has a unique global minima unique global minima now what happens with neural networks is and that is the crux of the reason people used to be used to argue that it doesn't work in neural networks and deep learning, in neural nets, even with one single hidden layer, forget about talking about deep, very deep neural networks, the last surface is non- non convex what do we mean by that pay attention to this picture when you look at this picture and let me let's look at this picture do you see that this is it has a global minima here this is your global minima I'll point it out it looks like the global minima. At least maybe in that region. But what about these little minimas? Like, for example, if you're sitting here or you're sitting in this little valley or you're sitting in this little valley, they also look like local minima's guys on your screen are you able to see that yes local minima is just a fancy way of saying valleys right so if your loss surface is like this this is a non convex surface so inherently it's a non common convex surfaces can have many local minimas. Now the point that we made is that generally those local minimas in a well properly done neural network, they won't harm you. With momentum, you'll get past it. And you see that here. What this picture is showing you is that this is actually A, the picture A is without something called skip connections. It is the short circuits. If you don't put short circuits and you create a very deep neural network, let's say a 50 layers deep, right? Or in this particular case, 56 layers deep, and there's nothing special about 56, it just is a non-architecture. Imagine many, many layers, then your loss surface will look like the one on the left. But if you introduce those skip connections, if you introduce those residual short circuits, then the loss surface begins to look like this. Now, which seems to have which surface looks smoother and with a very clear global minimum oh the right one right isn't it so here you can imagine that with a little bit of momentum no matter where you start you'll end up in the you'll end up in the this the global minima right you can start from whatever point you like you can take your path like this and then after some time you will gravitate straight to this point down to the global minima right and this impractical experience shows see what what happened is, the history is, when you talk about deep neural networks, the question is how deep? The definition of deep has been changing. The initial Linux, I think, were 10 layers or something like that, or even seven, eight layers. I forget what the exact number is. We'll actually cover it tomorrow, so we'll know the details. Very small, and that itself was deep enough then as you added more and more layers to the neural network you began to have all sorts of problems like yesterday we talked about the gradient and the vanishing gradient isn't it that if your activation functions are not proper you'll have vanishing gradients and then you have to do batch normalization you have to do batch normalization. You have to do some tricks to fix it. So the idea is that when you don't bring in those bag of tricks, your lost surface looks like the left one. When you bring in the bag of tricks, it begins to look like the right one. But then you begin to get an intuition of what really is happening. If you want to ask, you know ask this what-if questions I will take a deep neural network I will not have short circuits but I'll have batch normalization then how will the lost surface look now how much better is it just with batch normalization how much does a batch size affect the lost surface the smoothness of the lost surface how much benefit do you get from residual sources not the short circuits versus not right so you can start quantifying the value of each of them in a very visual way you can see how much it is doing or how much it is improving your situation right likewise weight decay and regularization how much is is improving your situation likewise weight decay and regularization how much is the regularization doing now I said that regularization has a smoothing effect right it prevents huge vast oscillations and so forth in the actual transfer function in the actual function regression function or decision boundary but uh then if you see it in the loss surface also without proper regularization you'll have a very complex loss surface now that now the implicit statement i should make is you don't want complex highly non-convex loss surfaces you want lost surface which are as smooth as possible and have as flat valley as possible so desirable let me again put it here so one of our desirable says in neural nets let's put it where should we of the desirable is to have nice nice nice, broad and nearly flat value at the global minimum. This is something that you would want. The other thing you would want is close to the global minimum, you see, you don't want that. Let's look at one dimension. So suppose this is your local minimum and this is your global minimum and this so you look at this this is your this is your global let us say global minimum this is your local minimum this one are you really so let me say ABC how persuasive or how convincing is be that it really is the solution as opposed to let's say C that you make C a little bit deeper so B is lower than C but how convincing is it that B is far better than C it's a question worth asking isn't it it doesn't seem very impressive but if your surface was like this and let me make it suppose your B was like this now what do you say guys? Does B look much more convincing? Yes. Not only that, observe a nice property. Even though the loss, I mean, I'm showing it in one dimension. And so you can imagine that there's actually a multidimensional surface and so you can imagine that there's actually a multi-dimensional surface and so forth but observe this part of it a region I just sort of box it in look at this region look at this reach more or less this part of the region what happens so from this value in the let's say that this is some parameter some weight parameter WI we don't know which weight parameter it is this is this direction and this is the loss loss so you realize that from this direction and this is the loss. So you realize that from this value to this value of the parameter, this loss loss surface looks convex isn't it guys it does look convex isn't it yes because it has a well-defined global minima in this box region and that brings up a very interesting point it brings up the point and that's what this author speak about, is that the importance of starting in the right place. What if you started, you had some strategy that made sure that your initial weight, and so let me put that word here, importance of weight initialization so if you initialize your weight in such a way that you end up your starting guess is someplace close to that you know you start here or you start here what will happen you are in a region where you're very close to the global minima and you're looking at essentially in that space sort of a convex surface convex loss surface so weight minimization means what finding the the weight vector. In other words, a point in the parameter space that you are starting. So you start at a good point, you're likely to converge to a global minimum. You start at a wrong place, you're likely much more likely to get stuck at a even with momentum you might get stuck at a local minima somewhere else there is a greater risk for it so what we will learn as we read this paper is that you can is that in a very visual way you'll see what really these things mean what does it mean visually to have the right weight initialization what does it mean really to have batch normalization what does it mean to have small right weight initialization, what does it mean really to have batch normalization? What does it mean to have small batches? What does weight decay do? What do short circuits do? And isn't it lovely that if you had a visualization tool like this, when you are designing the neural net, you can actually see what your lost surface looks like. And I can't tell you guys how useful such a tool like this is, if you could use it in your work, right? Because most of the time you're sort of guessing what architecture do I put, isn't it? How many layers should I put and things like that? If there was a systematic way for you to get visual clues on whether you're moving in the right direction or not. And if you could just see your lost surface and say, oh, I know now why my performance sucks, why my model is doing badly. And now I know what I need to do to make the model do better. It is like walking in the forest with a light, with a a lamp in your hand would you agree with that guys so that is that is it but now the fundamental question comes and there's a lot here the author speaks of the prior work in fact prior work of such people like I think this is Benji or something some big guys Oh good fellow himself and see these are the big names in this week they have done some one-dimensional versions of what these guys have done like this kind so something like this the work that I'm showing you but this is close to what a long good fellow and other people have done prior work so they talk about prior work and then they gradually come to their idea which is filter wise normalization it's a bit technical I won't go into that it's not that relevant for the work that we are doing if you read through it and would like me to explain it to you let me know and I'll explain it to you but it gets a little bit mathematical but here we'll talk about this sharp versus flat dilemma see you want flat minima right and when do you get flat minima you get it when when you use certain good techniques and if you don't use good techniques you are in trouble so here they are looking at what happens if you use small batches versus large batch so it's very interesting remember that one of the degrees of freedom we have is small batch batch size was one of the regularization techniques we learned just last week do we remember that guys the other one that we learnt about was batch normalization so I just like the word batch normalization of versus large batch what does it mean if you have a layer and you put a batch mini batch of data you put a batch mini batch of data this layer L minus 1 whatever the activations come out to be of this layer you you just standardize it the inputs before you feed it to the layer L the next layer you normalize it what does it mean to normalize you take Z is equal to so now to normalize a thing is x minus mu over sigma right suppose the active here x is the a's whatever the output came and the input to the next layer what do you do what is mu here guys do you remember is the mean of them there's a mean standard deviation. So all you do is you take this. And so what it does is it reduces the input to be zero centered. And most of your values will be between minus three and three. In fact, most of your values will be between minus two and two. Right. And now you take the fact that your weights, if you make sure your weights are reasonably small, you multiply those numbers with small numbers, you still have a small z. Remember the z was the weight in a product of weight and the input vector. In this case, the input vector is the activations coming from the previous layer, L minus 1. So L minus 1. So L minus one. So these activations are going. You normalize that, so that is batch normalization. So what is the value of batch normalization? And then the third thing that we do is use weight decay. We come with weight decay. Weight decay is actually a little bit simpler term. I'll talk about that a little bit. But essentially your L2 reg and L1 regularization that we learned about, regularization, are also weight decays. In fact, they're effective, they're good weight decays. What they do is they every time you take a step like you write a loss function if you remember if you wrote a loss function as some squared error y hat minus y squared summed over all the eyes a plus lambda you remember that rate k square isn't it if you solve for this you would realize that the the the adjustment to the weight weight next when you remember to account for the this factor would be it will basically be something like and I might get the mathematics a little bit wrong here so be careful i think there's a factor of n which is the batch size or something like that times uh the number n is the batch size so this one right may be wrong you can look it up i'm just giving you the the core idea maybe of times the weight weight minus alpha dL d weight. So we always wrote it like this, weight minus alpha dL d weight. But what we're seeing is when you put a regularization term, and we never talked about the regularization term very explicitly, but when you bring it in, you realize that this equation gets modified by the fact that you first scale it down by a factor a small factor and then you then you do the gradient descent step the next value so what does it mean suppose the initial weight was one you first start with let's say that your and your lambda is 0.1 so you start with 0.9 and then you take the gradient distance that's why you shrink the weight first and then you take the gradient distance that's what it means to regularize in many ways right so weight decay was another technique we used and the fourth technique which i haven't taught you is the so-called short circuits short circuit skip connection residual residual connection these are synonyms to it what it means roughly is suppose you have data coming in X here right for a layer L a and then you have LB layer B so you would expect it to go to the next layer right and to the next layer but in this what you do is many layers later you have this but what you do every once in a while you take the output of early layer and you you feed it in maybe I'll feed it into another one yeah so what you do is you will feed that also here so that the input to this neural net will not only be from the previous layer so suppose this is layer L and this is layer L minus 1 actually you can just call this A layer and B layer first layer second layer let's say and so what you would do is occasionally you would take the output of one of the early layers and add that as input to one of the later layers so it is a form of short circuit right because the normal signal is getting a passed in the forward direction and now here you have put a short circuit so when you put a short circuit we know that it is very essential to put short circuits and then only can you create really deep neural networks so today and this work came out in the year 2014 I believe it is Microsoft they proposed the rest net architecture and in the rest net architecture the moment they proposed and they all they did is add short-circuit immediately they've won the image net annual award, the prize, but they became number one, the champions of it. If I remember right, even the second position went at that time to another research group in Microsoft. They created the VGG architecture, which we'll again talk about. These are things you'll encounter in the next week. So this is it, it's the power of those ideas, but you can encounter in the next week. So this is it. It's the power of those ideas. But you can put short circuits into this. One of the benefits that you have is you can create very deep. So I said this is like the ResNet block you're talking about? This is exactly that. This is a short circuit. Very deep networks. So you can create very deep networks can you guess guys like three layer four layer ten layer how deep are we today millions remember that even by the time you reach ten layers deep you already have millions of parameters to train. Yeah. Right? Not just millions, tens of millions or hundreds of millions of parameters to train. Some of the largest models that I've seen have close to, they have a few thousand layers. They have a few thousand layers. Few thousand layers, yeah. I think ResNet was the first one to go to thousand layers. ResNet is, no, no. ResNet is no no ResNet is initially yeah, in the hundreds. The first version was 56 and then 101 or something like that. Yeah, the 101 was the one which won the prize like in 2014. Okay, so that is a story for this, this idea of using short circuits. So now guys, do you notice that there are four degrees of freedom you can improve your neural architecture by doing any one of these four and the first thing you can do of course is whatever other things you can do a play with different different activations. Six, in the play with number of layers. Not to mention all the hyper parameters like how strongly you want to dampen, you know how much is the regularization coefficient, is the learning rate and many many other hyper parameters are there when you are trying to build a build a model and train a model and everything has an effect on the quality of the model that you come up with but there has to be one way to know that you are making progress in the right direction and usually the only way we had till now was you would just look at the the training loss right how much how many mistakes you're making on the not training loss test loss on the test data but when you do that it shouldn't be just fiddling around so suppose i tell you that your test loss is still high you have many things you can do you can add a few more layers you can go change your activations you can change some of the hyper parameters your learning rates your damping factors your regularization factors you can change your dropper oh by the way not to mention dropouts a dropout even though it's a regularization technique i'll explicitly mention it here dropout you can change your dropouts you can change you so many things, you know, batch normalization, whether or not you're doing batch normalization, you can also change the size of your batch. So with all these choices, which one would you do? And you need some guide to take you forward. And in some sense, tools like this, no one tool will get the job done but the tools like this they help you they give you some assistance they're not perfect but they give you some assistance so let us learn to interpret these tools we'll just assume that their method works uh and i'll give you an idea a rough idea uh in short how the method works so you realize that they are in very high dimensional space. The dimensionality of the space B, M, very high. Think hundreds, think millions. Even a standard, the very, very basic, I think, LeanIt has half a million parameters and so forth. So something like that, a very, very high dimensionality it has. Then how do you visualize because when we visualize we are limited to three dimensions and loss takes one dimension right so if you do three-dimensional visualization you know that your vertical axis has to be the loss function right the last value so what do these two vectors be what can they be and the answer is very interesting actually you can take the vectors to be let's say w I w J now how do you choose these you have millions of these to choose from so you could choose a two orthogonal vector just two random vectors to random vectors. But actually, that doesn't help you because if you choose two random vectors, the problem that you encounter is, suppose you have a path, a gradient descent path that goes like this, right? What will happen is, these two vectors, they'll define a plane and very likely, this plane will be perpendicular to the actual gradient descent path and so what will it look if you look at this thing projected it into the plane it will look as though the point is just budged maybe hardly even budged isn't it you see this point guys if this path is perpendicular to the plane then when you look at it in the plane space you won't see any lost surface you'll see that the point has barely moved it has just jiggled around whereas in reality what it has done is it was actually traversed a sub-optimal and in this paper they use the word theta let's say theta naught and theta one gone from one place to a better place right so they have traversed quite a distance in the lost surface but any two random vectors if you take the and those random vectors happen to be perpendicular to your path you won't see the path you won't have any visualization perpendicular to your path, you won't see the path. You won't have any visualization. Is this idea coming across geometrically guys? This is an important idea. So imagine a ball bounces up and comes down. If you were only looking at the shadow of the ball on the ground, what can you say? It's perpendicular to it. Yeah, that's right. So if the sun is shining up straight at noon up there, it's casting a shadow down and the ball just goes up and comes down. You would hardly see the ball move. Maybe you'll see it wiggle a little bit at most. Do we agree guys? That is the same thing that if this path that you take, if it happens to be perpendicular to the two random vectors that you took the surface of the two random vectors. Jay Shah, Dr. Any two random vectors will draw the surface, but then you won't really see a path. So now comes the bad news. It is something that mathematicians have always known, you know what, if you take any two random vectors, you are practically guaranteed that it will be orthogonal to your path. It will be perpendicular to your path. And the reason for that is, any two, it's a mathematical fact that any two vectors in high dimensional space, you just draw any two vectors random vectors and you will realize will be perpendicular they will be orthogonal at the word is Arthur or simply put they'll be perpendicular to each other it's a fact and so we demoralizing fact that that is almost always true. So what do we do then? So what these guys do is something clever. They find that at this given point you can do a principal component analysis. Do you guys remember a PCA? Principal component analysis. Guys, most of you have taken ml 100 200 you say yes all right yes I hope you remember that so what does PCA do it finds the important features or directions right uncovers important features or directions. And so what happens is you end up picking the right two WI's and so now you can draw your loss surface like this. This loss surface that you see here in this picture sorry let me take a different color these last surfaces that you see here can be drawn by taking judiciously two of those parameters of the parameter directions right in a careful way and then it is it now you have projected down the data what have you done you have reduced the loss surface to a very low dimensional plane your loss surface was a loss was a function of w0 w1 all the way to w 1 million right WM and now what you have done is you have somehow projected it down to a loss surface that belongs to WIWJ just two of those so it is of course a different loss function because it's projected down is the shadow of the original loss function to this so one of the fundamental question that comes is are you sure that this preserves the property of the original high dimensional loss surface that what you see here you see this point guys what you see here yeah what guarantee do we have that it reflects the true higher dimensional reality? Are we together? See, you're looking at shadows. And from the shadows you're asking, can I infer or see what is happening to the real thing that is casting shadows so then the paper goes on to argue that actually it is true that it happens that's a good news so after the bad news that that we have so this is the good news part of it. They say that yes, it is reasonable to do so. And all the properties are preserved, the properties that you care about. And there is a bit of mathematics that goes into proving it. And now let's look at some pictures and let us identify. So what we'll do is when you look at a lost surface see this right it is you look at if this is your last direction and these are you so what are these points these are these are these curves that you see these are called ISO These are called iso, well, physicists will call it isopotential curves or surfaces, but isoloss, similar loss, same loss, let me just call it same loss curves in 2D in higher dimension surfaces, isn't it and when you project these down to the ground to the ground which belongs to these w i w j what you will get here is little contours this is and then something like this and suppose there is a local minima here let me just throw in a local minimum here and so what you will start seeing is somewhere here you will see the effect of another local minima and you will see this curve begin to bend and so forth are we getting an idea what we are talking about guys? These are called the contour surfaces. Contour lines, right, or surfaces in higher dimension. So far so good about the vocabulary, right? Now we have covered these things a few times. In fact many many times if you have taken the entire series of . So this should be familiar to you, isn't it? Whenever we do gradient descent, we always come back to this picture. Yes, sir. One question I wanted to ask that you were saying earlier that when it is global minima, it has to be broad at the minima. So that part I couldn't understand why it has to be broad and the minima so that part i couldn't understand why it has to be flat and broad see so here's the thing if a minima is like this do you really trust that this is truly a minima and not just the fact that this particular sample of data that you took happened to have this i say okay so how do you know that something is a truly a minima versus an artifact of the sample of data you're training with when it is little stable yes you want so a broad minima and deep minima both of them together gives you confidence that you're looking at the real minimum I see okay so now let's I'll blow up this picture let's blow it up even more is this picture looking yeah it's fine so guys look at this in fact let me bring the legends in here yeah look at these legends here what are they saying this is if you look at this figure here they are saying see far see far is the did we encounter CIFAR in our lab work guys? Our very first lab was with CIFAR data, remember? net why this is wide resnet on c5 both with short circuit connections stop and without short circuit conditions so you take the wide rest and so white is something interesting what does white means in each layer you have a lot more number of nodes than you would normally put when you have a lot of nodes in each layer it is called a wide network so given the wide network let's let's start looking at it here and now what they are looking at is the effect of adding more filters so that is another hyper parameter right do you get better results when you put a lot of filters into your date into your model what filters are will learn about exactly next week but just think of it as stencils you know cut out stencils one cutout stencil is for a another cutout stencil is for b and then you can move that stencil over a page of writing and see where it matches right so if you find a match for for the a stencil you know that you've discovered an a if you find a match for the b stencil you you say that oh yes there's a B here. And you can use the stencil and scan over the page and count the number of A's and B's and the locations. So that is the question. When you read it wide ResNet 56 on CIFAR 10, what does that mean? So what it means is this network has basically 56 layers. Sorry, this is so wide and I'm using 56 layers, right? Literally 56 pairs are there. This is number of layers. A wide just means that each layer has a lot of nodes in it. And ResNet was that architecture. ResNet was the famous, famously discovered 2014 architecture that won the image that won the image contest. Are we together? And its quality is that the big idea with this ResNet was that between the layers, it had short circuits. As the data flowed through it, it had all sorts of short circuits built in. And are we together? You were literally short circuiting some of the input to the subsequent layers. So now what happens if you go and disable the short circuits? If I take that architecture, that architecture was very successful. So surely something was good about it, right? It was truly able to find good global minima in a nice way. So if it found the global minima in a nice way. So if it found the global minima in a nice way, you can see that here. If you take one filter, do you see that this, how many, what can you say about the first picture guys? Pay attention to this picture and let's talk about it. Where is the global minima? In the dead center of this picture right this is your global minima right in the center here Center but observe that that is not the only minima we have we also have another minima here another minima here another minima here another minima here those are tiny little minima and most likely we can jump out of them but that is what you get when you have ResNet with its short circuits present but you use only one filter in a certain layer you use only one filter what happens if you increase the number of filters you notice that when you increase the number of filter close to the global minima, now how does it look? Very close to the global minima. Do you see a lot of local minima nearby? They have decreased, isn't it? You can spot a local minima, one lurking right here, and one maybe lurking near the periphery, and one maybe somewhere here out of the region might be one right but do you notice that already this global minima seems to be a much more well defined and it seems to be a nice flat do you see that the contour lines are spaced apart observe that the contour lines they are much more spaced apart isn't it when these contour lines are spaced apart what can you conclude from this we can conclude a wide minima isn't it and the fact that other minimas are far it also means that it is a deep minima so it's a nice wide it's beginning to look like this right and it gets even better what happens when you take k is equal to four you increase the number of filters in that um there and i apologize i'm using the word filter again and again it's something you'll learn tomorrow so we're a bit ahead but think of it as something another point that improves an architecture and one of the choices you have is how many filters but the more the filter it also slows down the computation and the aspects to it so now what happens guys does the minima get even more defined you can't even see the local minima can't even see the local minima anymore they are far off would you agree guys yes yes it is there and not only that it looks like a pretty deep minima because you can see that if you look here right it is pretty steep here and then it is rather flat here right it's broad or flat here and outside in the corners if you go out you see that it is actually both it is actually deep it seems to be deep and the same process is even more exaggerated when you go to k is equal to 8. now there is a bit of mathematics you know what do we mean by flat and this and that how do you measure the curvature right and this is a bit of mathematics, you know, what do we mean by flat and this and that? How do you measure the curvature? And this is a bit of geometry. And if you're used to principal curvatures. So in other words, if you look at my hand, the principal curvatures are one is the curvature here. One is the curvature like this. The two curvature directions, orthogonal curvatures. So it might be, for example, if I have a kn curvature like this, right? The two curvature directions, orthogonal curvatures. So it might be, for example, if I have a knuckle-like behavior, do you see that it's rather steep here on your knuckle? But it would be not so steep around if you go over the knuckles. You know, if you go along this, it is not so steep, but along this, it is much more steep. Curvature is much more there along this direction than along this direction so how do you find curvatures there's a bit of mathematics you can take something called the hessian which is a second derivative and in multivariate calculus it becomes a matrix when you take the hessian of the laws and you take its eigenvalues it turns out that that will produce for you the principal curvatures. So if you're, and so here is a word to remember, that if your Hessian is generally positive, very positive, like in these situations, because I'm mentioning it because it talks about that mathematics, if it is generally positive, it is a positive curvature if you want the intuition here remember that in a single variant calculus also when you have a function at this the slope is zero right d f d f d x is zero slope is flat but the b square f dx square is greater than zero it has to be otherwise it is not a minima right in other words the second derivative has to be positive and now generalize it to multi-variate calculus you have the hessian because there are multiple multiple terms in the second derivative. There will be things like a D square F, a D X, I, D, X, J, terms, right? Where X, I and X, J are two of the directions, two of the axes. And so what you want is that the Hessian, which is the entire matrix, that should be positive. When that, so this is just a very rough hand-waving explanation about hessianism and it you have positive curvatures when you have the eigenvalues and you can look at it so there is a bit of mathematics which he talks about but um till you guys do engineering math at this moment you can ignore it there's nothing mysterious it's a very simple idea so this is it as you increase the number of filters it improves and now let's look at the lower one if you disable the the short circuits see what happens when you disable the short circuits k is equal to one do you notice compare these two which is better the upper one or the lower one Which is better, the upper one or the lower one? The upper one. Look at it. The global minima is here, but what about the local minima? What is it? It is infested with lots of local minima around it. For example, how many local? I can see one. This is the wrong color I chose give me a moment yes there's the black color there it is I could that it is just in for goodness and picking black once again it is infested with lot of local minima I mean it's hard to even pinpoint there's so many of them all around it even here so you can imagine that this surface is very non convex it's a highly non convex surface would you agree the presence oh maybe it will not I'll have to zoom out a little bit guys otherwise it won't get me right so okay little bit and let's try a luck now. So this has very non-convex, highly non-convex surface. So here we have removed all the residuals so this was the world before we discovered the lower line the lower row is the world before we discovered residuals the value of residence now you increase K it gets better isn't it does it look better now right it has how many local minima some here yeah yeah yeah yeah yeah but it's not as bad and this is your global minimum it's not as bad as the the first picture the leftmost picture isn't it and as you increase the the value as you keep on increasing your K it gets better and it gets better but it never gets as good as the upper one you realize that the upper the first row counterpart of every column is much more is far superior to the lower the lower one is that clear guys what you have is isn't it very nice to see all of these things visually? Right. So that is the point. That's the point that they make. And then they go more into it. I want a geometry after. What is the percentage in the figure? Is the test error? Those 10 pointers? It was a test error. Yes, I don't know what it was. I forgot what they were referring to. Read the paper, you'll see what they were referring to. Test error is reported. Oh yeah, the test error. This is the amount of test error. No, but that is, yeah. So do you notice that the test error, yes, the test error is decreasing and you have 5.85 and this is a much higher test error, 13.3, right? Means 13.31% of the data you got wrong. Now, as you go here, the test error decreases as you move in this direction it decreases as you go from left to right and it increases as you go from top to bottom Harni is that you see that right yes thank you so alright guys that is how we read this paper we move forward now see why are these surfaces bad why is this bad it has very poor ability to generalize or learn machine learning is all the ability to look at certain instances and generalize from that ultimately this is poor results you know it's not it's not generalizing it is having a lot of test errors high test errors so that. So, and one more thing that I didn't mention is that this was for ResNet-56. You can compare ResNet-56 to 101, the other architecture with even deeper layers, and you realize that, so yeah, this is another visualization showing the same thing. here the local minute the minima is a much more defined in 56 it's just all over the place so then forget about this optimal trajectories and so this was the point that I told you that you know if you have a trajectory and if you're looking at orthogonal surfaces you the projections you won't see it that argument we made and this what it is I won't go too much into that and this is the factor this this shows how much the effect is of weight decay what is weight decay think regularization L2 is an example of weight decay at each step you shrink the weight. Now, you also, the more technical way would be to say that any weight in the next step is 1 minus some proportionality times the previous value of the weight, next, minus alpha d loss dW. But if you look at it, this, one of the cases that produces this kind of weight decay is the L1 L2 regularization actually L2 produces not L1 L2 will produce L2 reg will produce weight regularization but you can have more explicit weight regularization by just setting the the weight regularization parameter explicitly so there is a subtle difference between the two. So when you do that, if you do weight decay and don't do weight decay, right? So here you are doing weight decay. And here you're not doing weight decay. What do you notice, guys? Here is a minima and the other, the minima is pretty sharp. You know, the other minima is lurking close by. Do you see that, guys? There's another minima lurking close by but when you have weight decay this minima is like separated out from other minimas things look much better in these pictures so that is the point that when you have proper weight decay you it's a good idea because again your last surfaces they look better right and at least locally they seem to have a nice in a certain region close to the minimum you have a lot of that's it the paper is over and so that was the paper and our time is also it's 1 30 so so i'll take a few questions guys did you understand this paper now yes i think of what it is saying right so now i would like to give another 10 minutes to showing you the tools and showing you to you in a very visual way uh let me um go here to the share the other screen so if you go to a class page, which we should do now, this is the paper. I was trying to insert. Where am I? What just happened here? Oh, the login page itself has disappeared. So log into your account. If you go into this that we have in the navigation this loss function loss function in great interest so do go there guys and when you go there you will notice that I have put a website and github visualization the loss function viewer for various architectures yeah this is the one that I want to do yeah let's go to this link so let's let's study these architectures here now I know that you guys don't have is this clear guys so we will pick some architecture this is a rest net 56 like I'll remove this look at rest net okay maybe look at this let's take an architecture with no residual you know short circuits what do you think guys does this look complicated look at them now you can actually see if you look at the bottom of it you see that there is a global minima all around it there are these little local minima is that that can sort of trap you and so you would imagine that it is very hard to learn because quite often you'll end up with a solution that is not the best solution so look into this bowl right you see all the places that you can get trapped in then the same architecture suppose you do bring in the shots now I'll remove this and you look at this now what about this do you see how well defined the global minima is. Nice. How nice and broad and flat it is. So this is it, you know, use this tool, it will give you an understanding of what really is going on with these things. You can do it with smaller artists, BGG. How does BGG look like? If I remove this. BGG is simpler you know it looks like a pretty nice plate of gold you might drink soup in it i suppose and you can play this this game let's look at the short architecture do you see how complex it is guys and do you see this ridge even at the minima it has a it is sort of very thin long minima right it is called a eccentric minima it's very eccentric the ridge do you see this it's almost like a canyon at the bottom this is if you remember this is what we talked about uh what was the other example when we used to have canyons in our last function do you remember guys from your ml 100 200 uh core linearity yes when excellent when you have core linearity in the data, then you have this. That's what this thing shows. So you have a lot of pathologies when you do that. And the same thing now, we can now do the same architecture, but with shot. And you notice that it is so much more neater. This is it guys, and so with that, I guess I am done showing you what I wanted to show you Here is your last landscape. So here is a beautiful thing. If you want a more readable article describing all this instead of the paper, you can go and read this article. It's a lot more readable. It's idiosyncratic. It sort of talks a little bit here and there, but it gets to the same ideas. You can try to read he he takes another I would say broad strokes he probably starts with the meaning of life it seems but really a good article in spite of the idiosyncratic way it starts very nice and it gets the idea across you can read that simpler article on this and then there's a website that they have which website again is filled with a lot of these videos and I would encourage you to watch these videos not just the first one go to the gallery and explore it guys you know it will help you develop intuition about this neural networks and their architecture so imagine can you imagine a lost surface like this the pink one and you see the effect of dropout what does it show to you drop out introduces a lot of fluctuations perturb perturbations and noise, isn't it? And you can literally see that. Do you see this, guys, these spiky things? Yes. All of these things, it will make it very real for you. What do these things do? What do dropouts do? So please do try to observe it. And even the transformers, what exactly is the transformer doing, last landscape in this? There are a lot of things. I like this, actually. I would definitely say that go investigate it. and the GitHub has the whole code of it you can play around with the code and they give you tools for you to generate your own visualizations and that is that I will bring back the controls should I stop the recording now guys