 Alright guys, we are on the record now. In the last couple of lectures, we have focused on classification. Classification is a situation where x feature vector goes in, you know, properties of some, for example, it's an animal. You have the weight, the size, where the horns are present and things like that. What you have to determine is a class, which class of animals does it belong to from a category of animals? So cow, duck, horse, we took that as an example. So here, cow and duck and horse, they're considered classes. Associated with them are a certain set of attributes, more or less, that gives you a notion or a concept that this is a horse, this is a cow, and so forth. Now the set of all these classes must be finite and it's called the category. Now we talked about machine learning algorithms as we were talking about them. We have the discriminant analysis. In the discriminant analysis we make the assumption that each of the classes they occupy fairly separated or you know they're fairly clustered. All the instances of know they're fairly clustered all the instances of a class are fairly clustered in the future space and they are not just a get clustered in the shape of sort of a bell hill and so the ducks will perform here we have volume versus weight and the ducks are centered in one part of the future space cows Cows are big and heavy. They're centered at another part of the feature space. And so what do you do? You go to the bell hills, the tippy tops, hill tops. You connect them and then you take a bisector, you get a pretty good decision boundary from that. So, that brings up the concept of the decision boundary. So, what were the assumptions of discriminant analysis? Each class instance are clustered around a different center in the decision boundary. So what were the assumptions of discriminant analysis? Each class instance are clustered around a different center in the feature space. These clusters are approximately bell hills or more formally have a normal or Gaussian distribution. Linear. We can do linear discriminant analysis if we can see, you know, the bell hills look more or less alike. They just happen to be at different places in the future space alike in what sense in terms of their variance how much variance they have one dimension and two dimensions called the covariance matrix but okay let's state the word variance for now good thing about discriminant analysis is but suppose you're you know your bell hills are quite different, then all is not lost. You just have to account for two different variances and or in higher dimension covariance matrices. Then the analysis gets a bit more complex. Your decision boundary is not straight. It becomes a quadratic curve or quadratic surface and still it's very doable. In the modern world with modern computing hardware it works very fast so in general these are straightforward algorithms to do that one advantage of discriminant analysis is it naturally does multi-class classification so for example if you have three animals three classes of animals, cows, ducks, horse, that is perfectly okay. You can connect the hilltops, take the bisectives, they'll meet. And so you can still partition now the feature space into three regions, one for ducks, one for cows, and one for horses. It's a very natural generalization tool, a multi-class classification. Then we talked about the fact that, you know, sometimes those hills are overlapping. When the hills are overlapping, then you still do, you take the bisector of the hilltops, but now what happens is you will have errors. Horses, you will have a pretty good region in which it's hard to tell whether it's a cow or a horse example the weight which is to weight and so forth so then you will have error rates that will come error rates you always have in machine learning so how do you quantify error rates when we talked about error rates we generalize we say that whenever you do classify, irrespective of what classifier it is, we build something called a confusion matrix. In a confusion matrix, suppose you have the cow and the horse. How often could we identify cow as cow? And how often we confuse the cow to be a horse? And likewise for a horse, how often we got a horse as a horse and we confuse horse as a cow sort of that so in this the most basic measure is accuracy how often we got it right error rate is how often we got it wrong now just that in itself is not enough because sometimes the data is very the classes are very imbalanced you know for example if you are testing for people for some rare diseases most of the tests will come out negative people are healthy and only a few people will be sick so the thing is you can have very high accuracy but it may not mean anything because there's a huge imbalance in the classes. So then we have to think what are other measures that we can use to quantify the goodness of our models. Now, yes, I'm sorry to interrupt. I just have one question which we have used this COVID-19 example I just have one query regarding the naming convention so we have said that a positive patient who really has COVID-19 will be a true positive but then who is healthy but still has been labeled as a positive person we have called it as false negative right no no sorry I know that mistake that's a way point that you discovered this is not a false negative fault false positive positive yeah this is a false positive and this is a false healthy person having a curry is false true positive true negative healthy healthy no no no false negative this was right actually yes I apologize see when a positive person you miss it's a false negative so it was correct and this is false positive means you marked a positive case as negative. That negative is false. Uh, but this has this look at this fifty people, they have covered and you say, no, I mark them as healthy. So what did you do? You forgot to catch them. Isn't it? Yeah. And then you in the opposite case when they are healthy but you call them to have COVID then you are having a false positive. Okay. Yeah, this living is correct. So that's that. And we talked about the fact that this sort of analysis is from... Can you repeat it again for false negative and false positive see a false negative is when you missed a positive case when your classifier misses a positive case right so for example you have covet but the test comes out negative so that negative is false right it is not correct that is a false negative I became what false negative now so in this confusion matrix 50 is the false negative yes and then the with healthy people if you mark some healthy people as having COVID, you mark them positive for the disease. That is wrong. That is false. If it helps you, put the word wrong instead of false. In colloquial language, we'd say wrongly identified as negative and wrongly identified as positive. Let me put the colloquial meaning here. I don't want to put it here. Let me put it somewhere. Let me create a space here actually. This is true, negative. of it as wrongly identified as did not missed some wrongly missed some positive cases. So the positive and the negative here denotes having disease or not having disease. Yeah positive you have to decide on the convention that is why I said COVID we define that as positive because hopefully it is a less likely situation it is a signal you're looking for missing some positive pieces that's how it is in and in general this is true for example the doctors when they physicians when they look at your medical report lab report When the physicians look at your medical report, lab report, if you are healthy, they'll say, nothing is remarkable. There's nothing remarkable observed. Believe it or not, that is very good news because if a physician looks at a lab report and says, oh, there is something worth remarking or there's something positive, that is when you need to start getting worried really worried about it so positive usually means um in medical literature usually it means an adverse case covet like having co-rated so far so for i say for uh false negative you didn't mean like wrongly missed some positive or some negative. Yeah, no, no false negative means you, you. Missed like, you missed some people who are sick and you send them home. So, which is disastrous. You know, they can die at home. So, false negative is pretty disastrous. False negative is pretty disastrous. You've wrongly missed some positive cases in COVID, for example. See, you just think about this, read it backwards. A thing is marked as positive, but it shouldn't be so what does that mean if positive is sick a person is marked as sick but shouldn't be it means he was a healthy and you just marked him as sick all right there will be some some adverse impact now think about the other one negative or being sick is healthy false healthy The person is not healthy, but you marked him as healthy. What does that mean? That is a scarier situation You're missed out a patient And that is a good point actually we'll talk right about that in a moment so We talked about accuracy and so forth then we talked about accuracy and so forth. Then we talked about logistic regression. What is logistic regression? And I said that we went through the derivation of the logistic regression. What it means in simple terms to review is the further you are from the decision boundary, the more clear the signal is. You're very sure that it is a blueberry if you are far from the decision boundary in the lower region and if you are far away in the positive region, red region, from the decision boundary, you are much more sure. So distance from the decision boundary is called the distance function. We will leave it as that. Now the question is how do you relate it to the probability of its being red, for example. So we went through a small derivation. We said that see probability goes from zero to one. This distance is going from minus infinity to the first infinity. So how do we map one to the other? And it turns out that from probability we can get odds when you get odds it goes from 0 to infinity now then if you if you therefore exponentiate the distance e to the D exponentiation always takes you from 0 to infinity and you and you have essentially solved the problem you have discovered a way that you can deal with it and in fact that is the derivation of the logistic equation and this equation is all in all the literature it will be presented like this in your textbook if you have been reading your textbook you'll see a thing like this right this is your this is in fact the way it is mentioned in your textbook it is an example of something called a sigmoid function sigmomoid functions are s-shaped curves. They are stretched out s-shaped curves and their qualities are, they're continuous, of course, they're continuous function. It is smooth. It doesn't have sudden changes of value, smoothly the values rise, monotonically increasing. You know, you see the gradual increasing. It doesn't start changing direction and moving down and then back up again. Continuously increases. And it has two asymptotes, the lower asymptote here, which is at zero, and the upper asymptote, which is at one. What it means in a sense is once you are sufficiently far from the decision boundary you pretty much know what it is it's a blueberry or a red berry. So these type of functions are they belong to a class of functions called sigmoid functions but they are all over there in nature and electronics and so forth I wouldn't go into that so today what I would like to do is go back to this confusion matrix I can see we are having a little bit of a difficulty coming to terms with it let's go back to confusion matrix yes I'm sorry can you please announce for ml200 we were supposed to announce sure let us do that guys I'm taking in registrations for ml200 we were supposed to announce that sure let us do that guys I'm taking in registrations for ml200 the continuation of this workshop if you are going to register for it please drop a word to Prachi right now do that we I'm going to estimate how many people are interested in taking it then I'll open it to a wider audience it helps us plan a little bit so please do please do drop a boy clarification would you mind clarifying the odds probability and likelihood all right let's do that let's keep it as it after confusion matrix Do you mind clarifying the odds, probability and likelihood? Okay. All right. Let's do that. Let's keep it as it. After confusion matrix, I'll also talk about odds, probability and likelihood. These are three different words, even though in popular language, they are considered the same, we will talk about that. Let's keep it as something to do in the latter part of the class. So let's talk about the confusion matrix now. Let's go back to the example. Suppose this thing is true, false. You mark it, you predict it as true. Okay okay let me move it in a big box here something is true something is false right and then you predict it as true or you predict it as false and by the way people some you can write this matrix transpose this matrix you can put the true values at the top and the and the predicted values here up to you how you do that right i'm doing it like this and you can do it another way so this is i will give a name let me just call it a b so what is a is it true positive i suppose in this situation is it true positive? So just to put a meaning to this, right? Let me put something. We are testing for COVID, let us say COVID-19. So A is a person has the disease and you classify as having the disease is that a good thing or a bad thing good thing it's a it's a good thing and you would call it a true it's a true positive Then B would be what? There was a person sick and you detected it as not having disease. So this would be what? It is a false positive. Negative. Because you mark this guy as healthy but the person is actually sick what about C C is the person is actually not sick but you declare that the person is sick so this is this is false positive and D is true so with that in place have created quite a few measures so let me take the total total is equal to a plus B plus C plus D which is the total number of instances accuracy is defined as a plus D the principal diagonal a plus D a plus D you know the ones that we got it right over total error is equal to c b plus c the off diagonals those are the mistakes over n so far so good guys i hope accuracy is what proportion you got right error is of course what proportion you got wrong right so accuracy is one minus errors. Accurate error rate. Accuracy. Would you agree? You can just add up these two things and see this is true. Now, it turns out that we have in the industry, this word classification is used in many, many fields and a whole lot of different terminologies have come about in this space. And let me walk you through the words that people often use in this space. In modern literature, we speak a lot about two more called precision precision and recall are we together so what is precision and what is recall precision is how precisely you know the true positive over positive test cases. True positive over positive test cases. So, by the way, guys, this, too, may I suggest that if you do have your textbook, go over to page 148 and 149. 148 and 149 this thing are explained alright guys so these things are explained out there then the recall and so all you have to do is recall this is and recall is equal to actually no this was the recall this is I should put it in the different columns so positive over positive positive and the precision is slightly different but okay let's stick to a recall and then we'll come to the precision stuff in a moment in the case of do you want a good recall or not good what do you want is a good recall a good thing oh no what do you want to do you want to have a test which is very good at not missing people right people who are sick we don't want to miss them isn't that true so recalling means we have missed it right come again recalling means we have missed them that's why we are recalling again the test to retest it again no no no no no so is it like do you want to capture everyone like all possibility like throwing a wider net no it is simple see it is just made out of this matrix in this matrix are true positive over positive. So true positive is this. Total positive is A and A and B together make it total positive. So in other words, a high recall means if you are positive, you are marked as true positive. Your classifier finds you as positive. So what it means is that you don't have false negatives. Now for medical situations like this you want a very high recall, isn't it? You don't want to have false negatives because false negatives can be fairly disastrous. Does that make sense guys? Does that make sense guys? See, the definition of recall is true positive over positive. So it is A over A plus B in this picture. Am I making sense guys? So recall is good also or it's bad? Good recall is very good. The better the recall, the less you have false negatives. Isn't it? Asif, so let's take the cancer case. So we want to capture everyone who has a cancer. Right. In the process we are also capturing like people who don't have cancer. So that means we're trying to capture as many people as possible. So that is a recall. No, no, no, it is not. So suppose you have, let's say that you have a population and in the population 16 people have cancer right yeah a good recall means so now let us say that you classify a court only three yes would be 3 over 16 whereas what what do you want a good recall would be all 16 out of 16. Isn't it? Yes. That is the point. That is what recall is. That is why recall is also called the sensitivity of the classifier. How sensitive it is. Can it just detect it? sensitive test that this RT-PCR for COVID, the reverse transcription PCR is very sensitive. Why? Because it does not miss people. That is why it is the gold standard. It's recall is near perfect, close to one. Are we together? And precision is one. You know, you take the false discovery proportion. So one minus this. Let us take this. What is a false positive rate? B. What is B? This is a false positive rate B what is B this is a false positive rate isn't it so one minus false positive rate rate is is precision precision and this is very subtle actually guys in the beginning you will keep getting a little bit lost now think about it if you have a false positive if you have actually sorry a false positive is here this is at c so it is 1 minus c over n right what it means is how often do you precise means you don't get a lot of false positive you know false positives good recall means you don't get a lot of false negatives isn't it so suppose 16 people have cancer you don't want to miss out on them you want all 16 to be caught or 15 to be caught at least. So that is a good record. Good precision is a slightly different. It says how not to be trigger happy. A trigger happy situation would be that you mark everybody as positive. Suppose one test, if you think about it if I if I cook up a test and whosoever comes to me I have only one answer to that you have COVID so what is my false positive rate it will be practically everybody, like everybody minus the true positive, right? I'm marking everybody as sick. Is that a good test? It is not a good test because it's not precise. It is not able to distinguish between people who have and don't have it. So precision means you'd have a low false negative rate so what is are we getting guys if you have to look at abcd what is your false negative rate of false false positive rate what is your false positive yeah which one is it in terms of a b c d what is your false positive yeah which one is it in terms of a b c d what is your false positive rate yeah it is c this is false positive so false positive is c over n the total right so c is the false positives and divided by the total is the false positive rate am i making sense guys c over n let me just put in this false positive rate let's just focus on this c over n does this make sense? Can we just reason through that? If we have total n data points and we have a c people, we mark them as false positive, which means that let us take a population again of 1,000. 16 people are sick, right, and have cancer or something like that. But we have our equipment has marked a hundred people as sick. Would you consider that to be a good test? It is not a very good test. You say that it lacks precision. It's not a precise test. A precise test would capture, would have very low false positive rate. won't miss it won't just it won't be trigger happy think of it that a precise tool is not trigger happy it doesn't just quickly mark things as positive so you are looking for a position close to one or 100% yes yes yes both precision now ideally what you want is you want a precision close to one high precision and high recall unfortunately it is hard to achieve what happens is that you in the beginning as you use better and better algorithms you you improve your model both your precision and recall will improve. But after a little while, it becomes a trade off. And I'll explain why it is a trade off. See, if you become very trigger happy, if you are willing to lose precision, recall is easy to take care of, isn't it? For example, you mark a lot of people as positive for the disease the hope is that all the people who really have the disease will also get captured on the other hand okay let me make it draw it in terms of a picture here let us say that you do let me just take it as one dimension in this all this all the points in this value some X value and I'll just deliberately take it as a dimension so let us say that there is some degree of so do you notice that between the blue and the green there is some degree of overlap let us say that this is positive true positive blue is positive and green is negative cases so what is one way not to let us look at it what is one easy way not to miss any of the positive cases if I put my decision boundary here let me call this decision boundary D 1 do you do you notice that I capture all cases in this picture right so positive cases caught on the other hand the demerit of this lot of false positives do you see that guys why false positives because a lot of these green points I have marked as blue points this is this is the decision boundary can you see that guys that this is the decision boundary. Can you see that guys that this is the area of mistakes? In this particular situation, one dimensional problem, this is the area of mistakes. And if I put the decision boundary at D1, I'll get a lot of positive cases. All of them will be caught, but I will have a high false positive also. On the other hand, if I make my decision boundary here, D2, what happens for D2? For D2, D1, D2, what happens here? Do you notice that all negative cases are caught? Cases are correctly classified, but a lot of positive cases are missed positives is this obvious are you guys understanding this just just look at this example that you have a trade-off which one would you choose in in the extreme D 1 you get all the positive cases in D 2 you get the negative cases each of them have a demon isn't it in the case of D 1 you have a lot of false positives and in the case of lot of so ie lot of false negatives i.e., a lot of false negatives. So, sir, your false negative rate shouldn't be over n. It should be over a plus c. Come again? Sir, in the formula above, false negative rate, as you said, c over n. Shouldn't it be c over a plus c? Oh, over a pussy oh yes yes yes absolutely you're right I shouldn't say that's a very good catch you know the the faults that's a good thank you good one thank you so uh guys look look here but intuitively when you look at the decision boundary d1 and d2 which one has high recall what is high recall high recall is there when you don't miss a positive case which between d1 and d2 which one has higher equal d1 will have high recall and what about d2 it has high precision precision. Why is D2 high precision? Because it tends not to give false positives. So you have a pretty the false positive rate is zero. So you have high precision. So what happens is that you have to decide in a situation where data is, there will always be a region in which it is ambiguous to different degrees. And so you have to decide ultimately where will you put your decision boundary. Wherever you put, as you slide your decision boundary between D1 and D2. So think about this. This is there. Your D1 is here. D1 and D2. So think about this. This is there. Your D1 is here. D1 is here. D2 is here. In this interval, you are putting your decision boundary either here or, sorry, either here or here. This was not really useful. Here or here or here or here. And wherever you pick, you're doing a trade-off between precision and recall isn't it and so that is the catch uh in the beginning when you are still training the model or trying new algorithms usually both precision and recall will go up accuracy will go up and so forth but after a little while where you put your decision boundary exactly you will be doing a trade-off between precision and recall if you go more for precision you lose recall if you go more for recall you start losing precision and so you have to be uh careful and so let me it is usually explained using a particular curve. It is this. This, let us say, is false positive rate. And this is true positive rate. So now look at this what happens is your true positive rates means how much of the true things you've got and what happens in the beginning when that false positive rate is zero when can you have a zero false positive rate yeah imagine that your decision boundary is at this point d3 so what will happen here or and before Let us look at these two extremes. D4. In d4, d4 is way beyond. You mark everything as negative. Do you think you can have a false positive rate? No. Everything you're marking is negative. Isn't it? But at d4, there are no positive samples left behind either. So in this, if you plot it out you will end up with a curve and just draw the famous curve like this what happens is if you are too strict in calling something positive like you say that you need to be hundred percent sure then only you'll mark it as positive well what will happen is one easy way to be sure of is mark everything is negative if you mark everything is negative you won't have a false positive but you won't even have a true positive because it's a useless it's a useless system you don't have a true positive right on the other hand if you want to maximize your true positive, let me just say percentage, this is one and one, one easy way to maximize true positive, You say everything is positive. Then what will happen? You're false positive. In other words, a lot of negatives, these green things will get marked as positive, isn't it? When these green things get marked as positive, then you have a false positive rate. So this is true positive, false positive. There is a trade-off between the two. And this particular curve is quite interesting. It is called the ROC curve. And I'll explain what ROC stands for. Whenever you have a classifier, it turns out that while this is a fact of life, you can compare one classifier against another. So if you think of this rectangle, let me look at this rectangle, this entire rectangle. This rectangle is a square, actually. It's a square of area. What is the area of this square, guys? One is one and height is one. area what is the area of this square days one is one in height is one it's one it's one isn't it so now what happens is suppose in your classifier you are still doing a trade-off between false positive and true positive rating but nonetheless and you get this particular curve if you compare it with this curve which is the 45 degree angle diagonal right this the gap between these two is a measure of goodness of your classifier one of the measures let me explain this so suppose you know you were just randomly guessing if you're randomly guessing your true power then what will happen is you can in a system that just randomly guesses one thing is it can always guess everything is false you know everything is a negative right so then the false positive rate will be zero true positive rate will also be zero on the other hand suppose you just talk about a coin or picking up a number between 0 & 1 suppose you a 10% of the cases you mark as positive. Well, true positives will go up because now some positives are being marked as positive. But your false positive will also go up because quite often you have just randomly picked something as positive and it is not positive. its curve ROC curve will look like the diagonal line a good classifier can be even better the perfect classifier would be something like this a very good classifier classifier a very good classifier means for a very very low false positive rate you are capturing all the true positive cases right so what will it mean this is a perfect thing right you have a very high true positive rate and a very low false positive rate means it has very high precision and it also has very high recall isn't it that is the gold standard can you create a classifier that has that has goodness on both sides now usually what happens is practically in real life your classifier your classifier's roc curve will be somewhere between somewhere between the diagonal which is the useless to between the diagonal and the and the red red shadow seeker so this is what you will get. Like look at this white line. The white line here is a much more realistic example of what you will see. Sometimes you get very good classifiers. It becomes to get closer and closer to the red situation. So the way you measure the goodness of your classifier, one way is that you look at the area under the curve now this area under the curve you realize that it will be somewhere between half and one half why a bad classifier is this triangle triangle series half yeah so this is there's a term for this it is called the au ROC curve it's a very important term you'll find under the ROC so this will go from go between half and one usually well actually theoretically it can be between zero and one between 0 and 1 but usually why 0 and 1 you can actually be worse than random guesses also your curve can be below the diagonal usually between half and one for a reasonable classifier and this is quite important guys this area under ROC now what is this word ROC I didn't quite explain ROC stands for receiver operator characteristic curve. Characteristic curve. Now, what this is a very weird sounding word. Like, who would have guessed something, a term like this? Where does it come from? It comes from actually, it has an interesting history. It didn't come from the machine learning literature, actually. It came from signal processing literature. So let me set the context for that. The context for it is like this. Imagine that you have a channel. And you are sending a signal from here the signals that people send in digital ages 0 or 1 right and now here you are receiving 0 or 1 now what happens is that when you send a signal electrical signal 0 is 0 voltage 1 is some voltage let us say it used to be 1.1 volt 1.5 volt 0.7 volt I don't know what it is these days whatever voltage is high voltage low voltage now what happens is as the signal propagates when you measure the voltage here it is never quite zero it may be let's say 0.3 and now what do you interpret 0.3 as or it may be 0.2 or it may be 0.6 or 0.7 the the values that come out now what you have to do the numbers you are getting are not so clean but you need to determine whether it's a zero or one because you know that the sender is sending zero one signals so a bad channel a bad channel or useless channel is you keep sending zero ones but what you what you discover here is completely random so this channel acts like a classifier input went as 0 1 what did you detect it as because now these are your features based on these features you have to determine whether it was a 0 or a 1 right so that is why it was the receiver operator at the receiving end operator so what they did is they realized early on that this consideration happens. If you set your, think about it this way, if your voltage for one, for positive is, let's say the voltage goes from zero to 1.5 volts. And suppose you insist that you must get 1.5 volts otherwise you but you won't accept it you won't accept it as one you'll mark it as you so what will happen all your values will be 0 because you know 1.5 volts will almost never cleanly go to the other end as 1.5 points you there may be less so your false positive rate is 0 but your true positive rate is also zero because everything is zero on the other hand if you set the voltage to some value but your channel is completely useless then you know you will essentially you will have this curve the diagonal curve is what will happen once positive will increase true positives will also increase. At the end of it, suppose you set your cutoff voltages very low, 0.01, even a little bit of a voltage, and you mark it as positive. In that case, your false positive rate will become very high. And it so happens that your true positive rate will be high because all the things are practically being marked as high voltage or 1. So that is the history of this curve, receiver operator characteristic curve. And again, in hindsight, you look at that, and what they were doing is essentially doing a classification on the channel signal received at the other end of the channel, right? And so the word has stuck from there. We use the word AUROC. It's a pretty mouthful, area under the receiver operator characteristic. It is 1 of the very good indicators of how how. Stronger model, how effective a model that you have built for classification. Are we together guys is this concept clear because at this it will take you a while to absorb what's true positive and so on this graph I have a question on the on the red one which is the good classifier so but even on the good classifier when the false positive rate increases the positive should why is it flattening at around one this is my question I'm not able to figure that out yes yes so imagine that see in a red classifier what do you want to do you want to put your threshold somewhere close to this because at this little value of the threshold this point your false positive rate is very low but true positive rate has already reached a saturation point see the maximum value true positive can can have is 100 percent isn't it one yes so it has to saturate at one. But the question is, how much did you sacrifice while it jumped to one? How much did the false positive rate jump? So, for example, in this case, let's look at this point to achieve this level of true positive. You sacrifice quite a lot. You know, you're having a pretty high false positive rate also. You know, you're having a pretty high false positive rate also. So this white line is not as good a classifier as the red one, because in the red one, you don't need to you don't need to pick these points. You reach a point of good saturation right here itself, isn't it? Where you have a very low false positive rate and already you have achieved as much true positive rate as you want. and already you have achieved as much true positive rate as you want. Near saturation level true positive rate, which is why you say that good classic point. Is this becoming clear now? After it reaches one, should it, just because it has to stay at one given to the fourth, but why would the fourth positive rate increase after that? That's because it has already reached one, right? The true positive rate has reached one with with that point so how would the false positive rate increase why would you draw the graph further is what i'm asking the reason you do that is that when you have a true true positive is high but if you put your decision boundary in the wrong place right if you like put it too much you know like let's think in terms of voltages if you put your voltage let's say the voltage goes from 0 to 1.5 suppose you put your voltage as 0.1 anything above 0.1 is positive so what will happen to your true positive rate at the at that cutoff very high true positive rate because everything is being marked as positive but you will also have a high false negative rate i mean sorry false positive rate true positive rate will be high false positive rate will be high now let's take the other extreme suppose you take it at 0.5 right and let us say that the signal is coming out very clean or 0.75 right half of 1.5 let us say that the signal is truly coming out very clean which means that the low voltages like when when you're transmitting a zero or a low voltage the values that you get as something like 0.1 0.2 0.3 something like that and when you are transmitting a high voltage you're getting something like 1.3 1.4 1.5 there what happens if you put your cutoff at 0.75 volts you will see that you get a pretty good true positive rate you capture all the positive cases and you don't have false positives in other words low voltages are not being confused as high voltages isn't it so that would be in that channel that would be a good value to pick not 0.1 but 0.75 would be a good value to pick right so this curve helps you determine that because see 0.1 takes you to this point you have a very high true positive rate but you have introduced a lot of false positives a lot of low voltages are being detected as one as high voltage so that is why once it's such see once it's saturated going any further in this direction is a disaster you don't gain anything you begin to lose it in the red line once you have saturated you don't want to go put your threshold in such a way that your false positive rate increases you want to just stop there. But in a situation like this, look at the white line now. In white line, you don't have a choice. Suppose somebody says that I want this. It's very hard to pick a particular point and say this is good. You will have to determine how much of false positive rate you can tolerate. And if you tolerate, let's say that you can tolerate only a 20% false positive rate you can tolerate and if you tolerate let's say that you can tolerate only a 20 false positive right then your precision goes down but you recall true positive over uh positive cases also goes down because you are missing out on on positive cases, right? So that is the trade-off between precision and recall. Okay. And just one follow-up question. So will the true positive and false positive, what we are doing depends on what we measure. For example, in this case, we took the detecting COVID, right? In this case, you said it is good to have have false positives than the two most different but say for example we are measuring the we are measuring the earthquakes then it's little bit different how you think about it it's even good it's a very good that very good yeah I'll give you as a illustration see when you're talking about serious illnesses you want a good recall you want a situation you don't miss positive cases right so your false negatives are low you don't have false negative you catch all of them people have cancer a good cancer test will go and catch all 16 of them right on the end you know let us say that instead of 16 it found 18 cases then two people it will be a nuisance they will be called having cancer they will go through further tests and then the doctor will come with the present news that actually it was a false alarm you're totally fine right so the person will just go through a couple of weeks of stress on the other hand you don't want to miss a guy who genuinely has cancer so in medical situations you focus on not having a false negative just one recall to be high but let us say that you are lending money and you're you want to determine that is this guy good enough to loan money to write in such a sense that he will keep making payments to you but there you have to be careful you don't want to be absolutely sure right you rather you do probably want it when you lend money to others you do want to be pretty sure uh which way will you guess suppose you have 50 50 chance that this guy may or may not return you the money will you give him the money you probably won't a bank won't give him the money right the bank need a very high degree of assurance that the money is going to come back right with interest of course then they will give you the money so the way you cut off and how you look at it is different so there we are not focusing on the recall is it that's the good that's the other one right what do you want to do there you want to go with high precision you want all the guys who uh who will return you the money you want to capture all of them and occasionally they may be a bad apple but the point is you make so much money from the from the good guys that one or two bad apples don't matter it it's the cost of business so and I had something nicely you are explaining but I think in other words a true positive rate I can label it as a sensitivity and false positive rate I can leave later one minus specificity so if we live like that yes if we live like that one test cannot be hundred percent sensitive and can be hundred percent specific so that's why we have in medical one test we particularly call screening test which has a very very high level of sensitivity and then we do secondary test which is confirm a confirmatory test so that test had a very high specificity level so another having one test with we cannot have one test with high specificity and high sensitivity so we have like two different yeah so so well being a doctor he introduced two words sensitivity and specificity let me explain what it is sensitivity is the same as recall it is another word for recall specificity is actually not precision. It is a new measure. It's one more metric. It is the false positive over negative. Sorry, so can we say simple terms like sensitivity will be the initial screening and specificity means like it's actually there? See sensitivity is see when you do early testing you want yes you want high sensitivity but after a few tests you want high specificity that is what Sukhmal is trying to tell you In the early test, even the slightest doubt that this guy may have cancer, you want to mark him as positive. You want to send him for further testing. And it happens. For men, the prostate, as you go across 40, you will realize that it happens. They'll do some blood tests, et cetera, et cetera. Sometimes the blood test comes out positive. Then on further probing, it turns out not to be a problem or something like that. Women have it, these scary moments even more often. They go for something called a mammogram and then even the slightest fogginess or anything and the doctor will say, oh, you know, we need to do further test. will say oh you know we need to do further test so there you want to have high recall you don't want to miss out on any case but then you don't want to continue doing that because the next test and the next test in the next is the subsequent test should be gradually focusing on the specificity because now in that pool there is already a very high positive rate you know a lot of people are likely to be positive you just want to weed out the negatives so you want to be very specific if you don't have don't say yes no don't don't unnecessarily put somebody on a chemotherapy so and that is a very good way of saying that where is one thing where should you emphasize versus the other in the early test for a disease you want to go with sensitivity or recall in the later test when you want to be absolutely sure and you have already gone through a barrage of early test you want to be very specific does he generally have cancer or not they should and you have cancer or not you have to be you need to have specificity so if specificity is true positive y n could be that right uh what is that you said one minus specificity is false positive by m yeah that is the definition so you can write it as or another way to do that is you can say therefore specificity is take all the cases and subtract the false positive and look at the value. So think about it. In the later test, you don't want to have false positives, isn't it? Suppose you are sent, you are one big oncologist specialist, and some doctors have sent patients and they say that in five different medical tests, they have turned out to be suspicious. They may have cancer. What do you want to do? N number of people are there. You want to remove the false positives. Because you already know that false negatives are not there. Because the early test took care of the false negatives. Isn't it it the person who did the mammogram was trigger happy even the slightest cloudiness anywhere in the x-ray and he would say oh maybe you have cancer right go and get for the test done so you're not fearing that you will have a lot of false negatives but at this moment you want to just remove the people who don't have cancer at this moment so specificity is therefore this ratio and a good a good test would just have a very small false positive rate in other words it is still true occasionally you know or you might be and you might end up sending a perfectly healthy guy through chemo does it happen well at least I know that in India a few years ago when I used to get it used to happen I don't know whether it still happens in the US or not so Paul does it still happen a false positive like you put a healthy person through chemo sir so now I did you are giving very nice examples a very good example here we can add AIDS, HIV ELISA. So we do screening test ELISA and then we do Western blot confirmatory test. For example, you were saying breast cancer, right? So we quickly do mammogram. So that is just taking x-ray of the chest and if there is anything wrong, mass or any calcification. So we don't know. Maybe most of the time that's benign but then once we have suspicion then send the patient for fnac right so yeah sensitive test plus specific test so uh yeah every cancer we do uh see cells under the microscope right and then that's 100 specific test so yeah that is the histological exam you do that yes so I wanted to actually repeat again what Suresh asked so like in the case of diagnosis it's fine that we would have a sensitivity test where even if we miss out the specificity test would actually confirm it but in the sensitivity test and sensitive as you do early on and your trigger happy like Sukhpal said even a little bit of calcification or something in the mammogram you declare the person to have cancer but then as the patient progresses through the experts you do subsequent very and those tests are a bit more expensive. Like for example, you extract cells as suppressing and look at it under the microscope. Those are very high specificity because you can look at the cell and see whether there is cancer or not. So subsequently, the subsequent test should focus on specificity. Why? Because you know that the false positive is not there anymore because you have the confidence that the prior tests were very trigger happy that is that is the point so my question was like in terms of earthquake we can't afford to have a false positive case so how does it work in terms of those scenario like it's a tough it's a very good question that you asked and i must say that it is a tough problem here if you you know it becomes like crying wolf japan has a system if they have a reasonably high confidence that an earthquake They will set, they will raise the alarm. It does have false positives. So their specificity is not one. It misses out. The trouble with adquick is you just get one shot. You can't do a barrage of tests. Just one shot. You either tell people that there is adqu quick coming and the amount of time that you have to warn is quite literally in seconds like 90 seconds or one minute or something and then it is and then it comes or 30 seconds sometimes then it comes right that is just enough time for people to run out of the house or stand in the doorway or something like that or under the table trouble is if you if you are trigger happy with this test and you keep on sending these alarms, people, it is like crying wolf. After a little while nobody listens to the boy because he cries wolf every day. So nobody will get out of the house. That is the danger with all of these warning systems that you don't want the population to just ignore it but at the same time the other extreme is there you you want to have a little bit of a false positively because what you don't want to have is a false negative rate if a earthquake is coming and you signal something is happening and you don't raise the alarm the the consequences are fairly devastating right and let me make it very real it's a quick is one but a much more successful situation is that of tsunami tsunami is hit right the big tsunami in Indonesia and then people ask this, that how is it that we didn't detect? It turns out that the oceans had detectors. They saw, but those detectors were few, and the warnings when they came, they came. See, a tsunami takes hours to reach across the sea and hit you. People were warned, apparently. They were told that this is coming, but the governments ignored it. In hindsight, why would they ignoring it? Partly because you know tsunami is such a rare event nobody really knows what to make out of it. The other situation possibly could have been that people are used to this and that yeah you know this system keeps warning us of tsunami. So now one of the things people have done is they have created a much more robust network of sensors throughout the sea and when tsunami now comes we have a very specific you can see measure the height of the waves and everything and you can see the speed at which the tsunami is moving and you can really precisely tell when it will do a Landfall on your you know your shows when it will hit your shoes So, you know precisely how much time window you have to evacuate See suppose the time window is very little you have to make big sacrifices you ask people to run to the nearest hill Isn't it and some will be saved in some will drown but if you have enough time let's say that you have a day or you have six hours or eight hours you can mobilize your response system to take the entire population away from the shores you can take them miles and miles away from the shore. So that is the difference between, you know, that is how these things affect actually. It's quite a sea change in how, well, literally it's the sea, but quite a sea change in how we do tsunami responses and detection nowadays. All right, guys, we have been talking about precision and recall. See these concepts take a little while to sink in. By the way, if you think that we talk about accuracy, error rate, precision, recall, specificity, sensitivity, that should be enough. No. Different literatures talk about different kinds of errors. There is a type one error. Type one error is the opposite of this thing. It is one minus specificity. Then there is a type 2 error. What is type 2 error? It is 1 minus recall or 1 minus you can call it sensitivity or recall. So why all of these names all derived from a simple confusion matrix? Why do we have that? The reason is this terminology existed in different statistical and machine learning literatures and different communities they used different words for it and they looked at the situation differently. Modern day in the medical literature, you talk about sensitivity specificity. And then in the rest of the machine learning literature, people don't talk about type one, type two anymore. I mean, sometimes they do maybe, I don't know. But most of the textbooks will talk in terms of precision and recall. So recall is the same thing as sensitivity and precision is different but people tend to focus more on precision rather than specificity so that is where we are guys that's a set of all the metrics and remember all of it is derived from the confusion matrix all of it derived from the conclusion matrix confusion matrix this is an important statement so what are the things we learned we learned about confusion matrix. We learned about true positive, false positive, true negative, false negative. We learned about all of these different measures, accuracy, error rate, specificity, sensitivity, precision, recall, type 1 error, type 2 error. And you can cook up some more words so that's that and now we also learned about the receiver operator characteristic curve the IOC curve which is the curve of false positive and true positive plotted against each other and so it's a because it's a trade-off at some point and the best way to think of the trade-off is to look at this line. Suppose it's one-dimensional data and there is a region of overlap between the negative points and the positive blue points. Where do you draw the decision boundary based on where you draw you will have you know you will have your false positive rate will get affected so that is that and this is all there is to classification metrics all of these guys will become much more real when we do the labs what I want to do is in the coming Saturday's section I want to do the well okay in the lab so a little bit behind we are doing still finishing regression practical cases I would like to finish that completely next time and I would like to do in the next week when we meet actually okay in the next Saturday after that when we meet, we will do classification labs. All of these labs have already released to you in the notes, the notes that you have tabular data, has all these labs. Now, one of the things that I'm not doing is looking at your solutions. At some point, you should start showing me the solutions. You're working through the labs, show me the notebooks because if you're not doing the notebooks, or if you you should start showing me the solutions you know you're working through the labs show me the notebooks because if you're not doing the notebooks or if you are far behind it may be work that you reach out to me or to the TAs and will help you catch up it's important to catch up on that so let's take a 15 minute break and after that I will start we'll go into the practicals actually for classifier let's just do that for some time just a few practical examples for classic maps to make all of these concepts real it's 8 25 now should we meet at 8 40. uh yeah okay let's meet at 840. I'll just pause the recording for now. Go with the theory as if yeah, but now yeah, we can do theory and lab on Saturday. Okay. So in that case, let me talk about where do these algorithms that you learn fall under the large landscape of classifiers? See, I would say that most problems of predictive analysis in the world in machine learning are classification problems. They're dominant. For example, let me give you a sense of it. You are looking at a picture. This is a very real situation at this moment we have a test since we are we are so covered is so much in our minds these days coming back so 19 then people have been wondering how can we have rapid testing testing is hard to come by we do this PT what is it RT PCR a polymerase chain reaction thing you take a swab then you do it at one time it used to be a mechanical process it would take a lot of time for somebody a human to look at it and run it and then come back with it and it would take a few others nowadays i am told that we can do it in about 10 15 minutes uh what is the state of the art guys the doctors here uh and patrick what is the state of the art how soon can the pcr result come Are you 24 hours sir? And do it a couple of hours in a couple of hours. And are they enough testing kits? Now? We're like, is the shortage addressed? Has it gone away? Testing kit shortage. That depends on the county, sir. But everyone's trying to do their own way to get access to the reagents. It's usually the reagents, sir. at one time that in Fremont people were lining up from very early morning 6 30 or so because city used to get only about 600 700 tests a day and very soon they would run out on it so people started lining up from 6 30 in the morning I don't know if it is still true or if it is happening but there is a thing that is the testing could be somewhat in short supply. So people have asked this question. Is there an alternative means by which we can test for COVID? The PCR is the gold standard, but either you have it in which life is great, but if you don't have it, is there a fallback? So in that category, people look at machine learning. In machine learning, machine learning in AI There is what they did is they took the x-ray of people's chest Now actually these days is pretty quick You can you can have a digital information of a x-ray literally immediately and Actually, of course doesn't take much time all you have to do is wear some lead cloth and protective cloth and stand before the machine and then it gets done let's say it takes about five minutes or three four minutes to get it done so you can process a lot of people through the system and those images become available immediately. Can we look at the images and tell whether a person does or does not have COVID and how accurate is the result? This, then around December or early January, there were companies in Taiwan and South Korea, et cetera, claiming that they have a fairly accurate system. They can detect COVID with a high 90s accuracy. And so they can put a large population through it. It sort of got debated quite a bit because the data that they got was, first of all, we don't have enough data. And 90s accuracy is not good enough. You need to be very, very accurate. Remember the recall aspect of it. The recall has to be very high. So even now the total data sets available to train the machine learning algorithm is insufficient. It raises the question therefore, is it enough accuracy that we go ahead with it? Right? Some people, and it is highly debated at this moment, but some people say, no, PCR is the gold standard. We should stick to that. We should just somehow make it much more available than it is. But given the fact that we have a limited supply of that test, there is a company, a couple of companies that have started marketing this AI driven test. And what is that test? It's a classifier. You stand before an X-ray machine and X-ray images come. That is the input data. That is your X vector. Goes into a machine learning algorithm. In particular, the algorithm that it goes through is a neural network. In fact, the name of the neural network is COVID-NET. a neural network in fact the name of the neural network is COVID net there is a excellent paper an updated version was posted in archive on April 15 just five days ago which talks about this in great detail it's the the guys who have had more success with it and I believe they are open sourcing everything making it available so perhaps it's premature at this stage, but perhaps in the next course, in ML200, we will use that. We will go through a lab exercise in which we use the COVID net to predict or to detect COVID in patients, corona in patients. So it is there, and the companies who are beginning to market it, and the companies who are beginning to market it and the people who are beginning to deploy it, it is not obviously as good as the gold standard of PCR, but it is something to which you can put a lot of human beings through. India is taking it very seriously actually, because in India, obviously having PCR kits available down to remote regions is, it will be a bit of a stretch. I don't know how successful it will be. So people are very seriously looking at this X-ray based, AI based solution. And should this become widely used, I would imagine that this would be the first very widely deployed AI based diagnostic that is at the grassroot level, down to the villages or something. They are making quite a bit of ambitious plans to do that. There's a lot of talk. There is also a lot of debate whether we should focus on it at all or not. But it gives you a sense of what a practical use of a classifier. Object recognition, you walk in front of a building, can the camera immediately detect it is you, know that it is you. So you don't need a security badge. If with great precision it can determine that this is you, then the door can open on its own. You don't have to unlock the door with a passkey or something like that, or you can do retina scans and so forth. These are all examples of classification, whether to examples you can think of in classification, which is why a preponderance of effort has gone into classifier algorithms, huge amount of effort, and the number of algorithms that there are are ginormous. Like for example, we did logistic regression. Logistic regression master regress, which is by far the simplest and most widely used. Its limitation is that it looks for linear decision boundaries, hyperplanes as a decision boundary. In real life, decision boundaries need not be hyperplanes. They can be fairly convoluted. They can be not regular shape, not shape not planes and so forth and so that's where the weakness of simple models like logistic regression begins to show through you can extend logistic regression by adding polynomial terms the same trick we did for linear regression it does work in other words with clever tricks you can extend the usefulness of logistic regression quite far adding interaction terms adding polynomial terms all the things that we learned and people do that the advantage of using logistic regression is you get very interpretable models you know things are clear you see the coefficients in the model the beta not beta 1 and so forth and you realize how much is the importance of each of the factors. It gives you some sense of that. So that's the value of logistic regression. Likewise for linear discriminant analysis and quadratic, there's sort of simpler models. But in this literature of machine learning, there are loads of other classifiers we have. in this literature of machine learning there are loads of other classifiers we have and quite a few of them will cover in the next workshop but i'll give you a taste of them some of them are actually simpler than even logistic regression or lda you ask how can that be so one easy way is that you use the so-called baseline rule, or zero-R rule. You just look at which is the dominant class in the target variable in the data set. Are most animals cows or ducks? Let's say that there are a lot more ducks than there are cows. So one simple classifier can be that you mark everything as duck because you're likely to be right more than 50% of the time let's say if you're doing between cows and ducks right so that's sort of a thing people that is called a baseline classifier its value is that it's not value is not to use it as such but it sets a benchmark it sends a bottom line that anything useful must beat this, must outperform this. So your logistic regression, your NDA, it must outdo this. Then another classifier that is quite often used, especially for spam detection, you know, whether your email, the emails that you receive, are they spam or not? So one of the very very successful and rather simple classifier that was effective is called naive bias classifier the word naive is sort of misleading there's nothing naive about the classifier but it is still a simple model it uses a basic buyer's rule to determine bias rule is something we'll learn about in the math class. Conditional problem theory is to tell, given all the words present, is it spam or not? So it's a classification problem, spam versus ham. You use that. There are three very powerful areas of machine learning which provide really powerful classifiers. One of them is ensemble methods, in other words, the wisdom of crowds. What you do is you take a lot of learners, you don't create just one classifier, you train a large number of classifiers on different samples of the data and different perspectives on the data and then you ask this classifiers to work is it a coward duck and whichever if the majority of them says it's a cow you say it's a cow when you do it like that you're doing something called bagging then sometimes what you do is you clean a model let's say a decision tree you train a decision tree this is decision tree is another classifier you learn about then it says that you know this is the way you'll classify it will have a certain error rate now the second tree you create you create one more tree which tries to learn from the mistakes the errors and minimize it so now your result is the combination of the decision from tree one and the decision from pre to that will still lead to some errors now you train one more tree and you keep on going the idea being that you know the first tree may be strong in some areas and weak in another so the second tree you train in the train for the areas where the first tree was weak and it does well and so on and so forth so this process is called boosting you boost the efficacy by growing more trees or doing more classifiers but to learn from the mistakes of the previous ones. So this whole family of things are very, very often used in the industry. In fact, if you go to Kaggle and you look at the competition winners, for the longest time, almost all the winning prizes went to people using this ensemble method. You would see things like random forest, which is like a lot of classifiers and bagging that we talked about. Random forest or extremely randomized trees or gradient boosting, cat boost, XG boost. If you look at the internet and if you look at Kaggle, Kaggle by the way is the site where a lot of data sets are published and people publish their own analysis of the data sets and they run some competitions who can do the best prediction and so forth. So I invite you to become familiar with Kaggle. So you go there, you see how often these ensemble methods win. The second approach that people took is based on a completely different idea. It is called the kernel methods. There is something mathematical term called the kernel, but I'll give you the intuition of this so suppose you your decision boundary is not straight you know it is curved one of the insights in math math is that any curved surface or just take two dimensions any curve in a plane in a page can be actually straightened out by going to a different space in a higher dimension it may not even be higher dimension but going to a different space in a higher dimension. It may not even be higher dimension, but just to a different space, you transform the input variables and a certain transformation will take you to another plane where the decision boundary now becomes straight. Once it becomes straight, then of course solving it is easy. For example, your logistic regression will work or anything we work so you need methods you need powerful techniques to somehow straighten the decision boundaries in the underlying data hey the very powerful methods is the kernel method the kernel trick is based on the feminine what it does is it finds the right higher dimension space in which the data simplifies itself. It becomes so simple that it's now a linear problem and you can go solve it. One of the simplest kernel methods is called support vector machines. And support vector machines were heavily used with the industry for many, many years, even though it's heavily used in medical sciences, it is heavily used. Right now that particular algorithm has quite an interesting history and I'll probably talk about that history now for a few minutes. It was, so literally, this is how I heard the story him from somebody I think it was from Patrick Winston of MIT professor in his lecture I think I heard it so he says and he knows it because he knows obviously the person who created this machines first-hand the inventor of these things of these ideas so there was a russian imagine that there's a russian mathematician who through russian winters russian winters are very harsh so you typically go off to your dacha during winter break and you you have your some of us whatever parts of tea you go for small walks come back and then you take tea and you think quietly so he used to live away during those retreats he started developing a set of ideas in fact this kernel method this the idea of the kernel and support vector machines, he pretty much worked out over a lonely winter, alone, sitting alone and developing the ideas quietly over a winter, day after day after day. He worked this out and it's a pretty remarkable chain of ideas and it's remarkable that one man could just sit and think through the whole chain and ml 200 will go through the process it's a real insightful process so he created this then he came back and then he tried to publish it work in a mathematical journal interestingly all the math journals rejected his work academic journals they said this is nonsense this is not even math like god knows what it is and it it cannot be published. It looks nonsensical. He was deeply disappointed. Then he tried to publish it in a neuroscience journal, you know, neurology or neuroscience journal, thinking that maybe the mathematical standards there are a bit low. But to his great disappointment, the paper was rejected even there. So apparently for many, many years, he kept sitting on those ideas and he discovered these ideas in the early 60s. So he sat on those ideas for many years, disappointed that nobody is taking it seriously or willing to publish it then in the 90s in US I believe some companies was it AT&T Bell Labs or Google or something I don't know which one most likely AT&T or something they were running a competition I think it was handwriting recognition itself or digit recognitions or something but I forget which one and you were supposed to classify recognize what is there it was an open competition and the news reached Russia and those were the days remember after glass most after the USSR had fallen and because the wall had fallen the Berlin Wall and there was much more free exchange of information like today across the world so he and his friend was sitting there having a cup of tea I suppose and then he encountered this announcement his friend brought it to his notice and he encountered this announcement and he said you know you bet I can do better than any of the other guys in this competition using my method so his friend said that yeah if yeah, if you believe so, why don't you do it? You know, this is not an academic journal. They can't reject you. It is just a question of if your accuracy, if your model is the best, well, you win. There was some sort of a price to that. And so he did that. He actually applied his ideas to that. And so he did that. He actually applied his ideas to that. And when it was received in the US, right away people knew that his ideas had essentially blown it out of the park. Like it was, he was playing in a field of his own. Absolutely wonderful ideas. And it led to what is called the kernel revolution in machine learning. very intense amount of intellectual activity and research and methods and so forth he obviously today is a professor at MIT is quite old now as you can imagine in 60s and 1960s were the prime days of course I think he's still alive which is quite old that Nick his name is Vladimir what that link and Vapnik is obviously considered one of the great legends of machine learning but obviously people were not in machine learning have never heard of him I can imagine that most of you probably have not heard of it. Anyone of you taking the class for the first time, have you heard of Vladimir Vapnik before? You probably haven't. So now you'll realize after some time when you learn about that, that it is. This support vectors, the name of this particular training place, this AI lab, is in honor of, well, literally the support vector machines algorithms and the kernel methods. It's sort of in honor of Vladimir Vatnik. And he was not the only one. It turns out that while he was doing it, other people independently were beginning to come to similar ideas in the 90s. were beginning to come to similar ideas in the 90s but he seems to have discovered those ideas many many years before anybody else and so it was the time there was a time if the ideas had to wait 20 30 years and then gradually other people were beginning to come to those ideas and it is a very active area of research it was so that is from that comes some of the most powerful classifiers that support vector classifiers. Then there is a third generation of technology. So, one is this generation of ensemble methods, bagging and boosting random forest, gradient boosting, etc. It's still going full steam these days. Many, many problem sets are solved by ensemble methods. Many, many problems are solved by support machines. And then there is a third sort of stream of ideas that came. It has to do with neural networks. Neural networks has had a very, very interesting history. It turns out that the neural network ideas almost go way back to 50s and 60s and 70s and so forth and a lot of the thinking about neural networks was in place the core ideas of the neural nets were there neural nets are something which is the simplest way to put it is you know we did logistic classifier. So if you can create a fabric of this classifier, you know, layers and layers of this logistic classifier units, you can lay them together. Then it becomes something called a neural net. And this logistic units, you call it the perceptrons or the nodes in the network so you create a network of these nodes and neurons um and so logistic is your one neuron and then you get something like a neural network the idea is that it was trying to imitate the functionality of the human brain so the ideas were quite ambitious it turns out that the human brain is so complicated and so mysterious we still don't have much clue beginning to get some little bits of understanding of the brain here and there we understand a lot more today than we understood before but we still know we're close to having a really good understanding of how the human brain thinks in fact nobody knows how the brain thinks at all right high treason side things what is consciousness what is awareness there is no understanding scientific understanding of that at all as of today maybe things may change in a 1500 years but that's for the future generations but people in started to think that can be emitted the human brain they thought human brain is actually modeled after simple ideas so the neural networks is one idea it was it got hyped and it didn't quite work out to its promise so it went through a hibernation to a winter actually a couple of winters what would happen is it would get hyped up then people would try it it would not be practically very useful then people would ignore it for a long time and in between the support vector evolutions came and the ensemble methods came and then it was very but gradually a group of people of them Jeffrey Hinton who's in Canada one of them they just persisted at these ideas they kept on working on it didn't give up and so and already by 1998 or 99 they had created a network one of them Lee Koon and so forth they have created a network called the Lee net which was giving you very high accuracy for handwriting recognition or digital recognition now Now why is that relevant? See when you send an email, not an email, postal mail, on the envelope you write an address. Now for a human being to read the address and then sort the mail is very very inefficient and expensive. One of the reasons the postal system is so efficient is because for more than now almost 20 years I suppose they have been using a neural network called the LeNet, a very successful network, which looks at your handwritten address and immediately recognizes it and converts it into a digital address, you know, text. And at the bottom of the, if you receive any mail, if you look carefully at the bottom, you'll see in the address area, you will see some vertical lines. It is a computer code for exactly the address is now there in the computer code. And what happens is the letter, when you drop it in your mailbox and the postman takes it that's the last human patch it goes through sorting machines they already use the neural network automatically hold it what do you think then automatic sorters and machine sorters with high volume sorters and redirect that mail into transport vehicles and planes or whatever it is across the country let's say a letter goes from here to New York or wherever it will go to that and then finally the next human touch will actually be the postman who is actually putting the letter in your recipients mailbox till then there is no other human touch usually so the thing has become very efficient and this is a classic utilization of a classifier because it looks at the handwriting and each of those characters it is able to classify it's very simple it is a between 0 to 9 and a to z it is able to detect what it is right about the 36 or 40 odd characters it is able to detect classify into those classes and so it can read essentially you can say in quotes. It can read. A very efficient that's why mail for the longest time now. Is running so very efficient in this country. Now, that's an example of a classifier and that's a success of the neural net. Then the neural net, while it was already being very successful, people were not taking it very seriously. They said that incrementally people have squeezed the most juice out of it and maybe there is nothing more to be done. And for whatever reason it wasn't because of the two neural net winters. People were not very keen on giving it another chance they felt that it tends to get overhyped and never delivers i think it was a general feeling i remember that i also used to feel in the early 2000s and i was misinformed of course but that and neural nets are not. They don't quite work as promised. So then in 2006 there was quite a big breakthrough called deep learning, deep neural nets. One of the problems with neural nets is that you couldn't put too many layers together and there are all sorts of technical reasons why you couldn't do it and essentially a lot of the pieces were missing to develop a full effective theory of neural networks. You needed a lot more ideas and they were being very, very slowly discovered. 2006 was sort of a milestone year when Geoffrey Hinton came up with the concept of deep neural networks by successfully layering many many layers of the neurons together and the accuracy that came out of that was astounding they're quite quite high and so for complex situations where like handwriting image classification etc it just blew everything out of the water it became a sort of a niche for neural nets if you're talking about images and videos and so on and so forth they're extremely effective far more effective yeah go ahead so you were talking about handwriting recognition right so they don't really recognize they don't really use the address on your envelope but they recognize your handwriting is that what you said then yes yes so when you write your handwriting you put an address the first thing the machine does is a neural net translates it into like actual text you know optical character recognition it recognizes what is written the address and it becomes now in a digital form and and the digital form is actually there in every letter that you receive if you look carefully somewhere you'll see those little edgings or markings and for the rest of the journey computers are just reading those markings the sorters and the routers are just reading those markers and they're automatically moving moving the traffic around. That's how it goes. So anyway this breakthrough that came started a third wave of machine learning called the deep learning. People have used the word deep neural nets and then somebody come up with the buzz word deep learning. So deep learning is now the in thing. In certain domains is very effective. Some people get carried away and say we don't need any other form of machine learning at all. All we need is deep learning. That's not quite true. There are many areas in which it doesn't do so well or it is like harder to make it work well. There are many areas where data is sparse or many other situations it doesn't do so well. Other techniques do better but it is certainly very very effective in some of the toughest and tarnished problems we were sitting with for a long time image classification and image segmentation example so that is those are all classification problems you know a car is looking into the street. Can it see? Can it identify all the objects for what it is? Can it segment it and say, this is a person, this is a tree, this is the road, this is a so on and so forth, pedestrian, a bicycle, a car? Because if you can correctly identify, if you can classify every object in your vision space into the right category into the right class then you can decide what action to take right so for example if you are sure that the what you're looking at is an empty road you will drive forward on the other hand if you see that there's a pedestrian you will want to slow down and stop things like that so these are all classification engines very high-speed classification engines in real time and those of you who have driven Tesla car I can attest to it on highway it seems to it seems to flourish it doesn't quite work on the city streets but in highways people report that the autopilot is quite effective. It sort of works, though you have to watch out, makes mistakes. But in general, you see all these videos of people on the highway who have gone off to sleep on the driver's seat for a long time, and the autopilot is taking them to work. And for the long journey, they're just sleeping or something like that very dangerous thing to do but the fact of the matter is people are beginning to trust this machine learning algorithms to such an extent that they are just going off to sleep so obviously we are not quite there yet hopefully in a few years we will be there but it just speaks to the power of this AI algorithms and most of these have the heavily classification algorithms the other things also happening there but they also classification algorithms so classifiers therefore have a rich literature, very high utilization. Another is text processing. In the text world, the way classification shows up is quite interesting. Many innumerable instances, I give you a text. Can you tell which language it is? Classify it into the right language. Very basic thing. Can you do a sentiment analysis? Tell, is it positive sentiment, negative sentiment and so forth. And today one of the themes in USA is after all this MeToo movement and so forth, is diversity and inclusiveness and especially in the context of gender. So for example, sometimes when you write things it has a pronounced male tone to it and it sort of puts off or discourages women and sometimes writing may have a pronounced female tone to it right so the tonality actually is very subtle and it's not very humans are not very good at catching the tonality of it very good writers and readers can detect it but the rest of it we are able to detect it but not so obviously it turns out that this classifiers machine learning classifiers are very good they'll tell you that this has a pronounced male tone or female tone and so forth. And you guys, some of you are using Grammarly. And you notice that Grammarly does that. In fact, it is, what is Grammarly? It, between a spell checker, which just fixes the spelling of your words, and Grammarly, the gap is that Grammarly is AI driven. It's machine learning driven. So it is looking out for these things. It is looking, if you get the paid version, it is looking out for the tone of your sentences, the construction, the complexity, and so forth, and classifying it into the right thing, identifying, detecting. So that is that. And this classification or detection is very vital. For example a threat detection there was one case which is comfortable I believe in the economics of a sort of ended that apparently Britain got some signal intelligence that said that there's going to be a terrorist attack in London in the next six months. Now people said what are the details and there were no details. All that happened is they had listened to some cell phone chatter apparently and this is how I remember it some cell phone chatter and they knew that with high degree of confidence there were some sleeper cells who were already there in London, terrorist cells cells and they would become active at a certain amount of time and then they would they would do some pretty terrible thing so the whole question is how do you find them if you go to the police police the basic operation would be nobody no case or things like that you know they need very hard evidence and very hard clues to go chase you can't run you can't do police operation without very concrete thing they will ask who should who is the suspect well you say I don't know the whole point is you don't know anybody could be obviously the terrorists are trying very hard to blend in when you go to the Secret Service's your spy agencies and they would they would also say give us some idea on who to spy on it's too hard if you just say um we don't know at all and so this became a case of machine learning actually that could you shift through the data and find that sleeper cell network and it it was a hard problem, and so they started doing that machine learning exercise. They shifted to a massive amount of data. And they could actually identify classify people within potential terrorists and not. Right and they did they did spot those guys. They caught the sleeper cells, the cell, and the disaster was averted. So obviously they haven't revealed too much about what the machine learning algorithm found. They have revealed only one thing, which in hindsight looks obvious. They found that none of the would-be terrorists had life insurance. Can you guess why would they not have life insurance? Anybody? Suicide bomber. Yeah, if you're a suicide bomber, no insurance will give a money payout to you, to your family for that. So it is useless to buy life insurance. And so it was an interesting and subtle signal that the system discovered. The other signals we don't know about. But when you look at this signal in hindsight, it looks so obvious, isn't it? That it should have been looked at. But the thing is, when you don't know anything, there are so many potential things that you can think of. And also, there are so many, many things that you can think of and also there are so many many things that you can't think of which is and that's the value of machine learning it can find the needle in the haystack and a giant haystack these days we live in a global economy global world the threat vectors come from all over the world right when they try to hit intrude into your secure systems and things like that so almost all security systems today are machine learning and a lot of use of classifiers and so forth so this is giving you a broad picture of classifiers and with that I'll end the theory part of the classifiers next time we'll do in Saturday we'll do the labs continue with the labs and next week we'll do change topics and we'll talk about unsupervised pattern recognition in particular clustering we'll talk about methods to find clusters and data and what clusters are we'll talk about next time so that is all i have guys i'll open this up now for qa any questions that you have. So please feel free to ask. Anybody? So just to recap guys, what did we learn through this last three lectures? I'll just go from the very beginning. We learned about what classification is. We learned about linear and quadratic discriminant analysis. We learned about the concept of decision boundaries. We learned that there are mistakes. So you can create a confusion matrix and out of the confusion matrix you can establish quite a few sort of a metrics. Accuracy is one, error rate is another. Then precision and recall are there. Then there are concepts like true positive, false positive, true negative, false negative. And it's something that you should get familiar with. And then generally what happens is that there are other metrics which are, you need to sort of develop familiarity with it. Precision, recall, and sensitivity, specificity, type 1, type 2 errors. People keep talking about things like that. We did a derivation of the logistic regression here, but then the sigmoid functions and so forth. So we talked about this measures, like what is precision, what is recall. So we took the example of a medical situation. In the beginning, early test should be trigger happy. They should have high recall, means they should catch all the positive cases. Even at the risk of having false positives, they should catch all the true positives. But subsequent tests, which just before you give somebody a chemotherapy or something like that, you want to be much more sure that the person genuinely has cancer because those are fairly brutal treatments, invasive treatments. So, then you need more you want to focus more on precision or rather specificity. Things like that, then we talked about the true positive rate versus false positive rate. Which is called the receiver of a characteristic now, give you a bit of a history on where it came from. Then we looked at the area under the ROC curve which can go from typically half to one. If your model is embarrassingly bad it can even be less than half. So theoretically it goes from zero to one but in practice it goes from half to one. And numbers that are like 90, 95% are considered fairly good area under the ROC curve. Because the total area that is achievable is one. So your area will be some fraction of that, which in half and one. So whenever you write your classifiers, do establish your area under ROC curve. These are all the definitions of different metrics. By the way, guys, what will happen is you won't be able to remember this in the first course way in the beginning it feels very confusing so look at page 148 of your textbook 148 149 and gently go back and look at it a few times across different days and gradually it will sink in you won't remember this definition of all of them exactly even I do I get confused. If I have not been doing it for some time, then I'll get confused. I would vaguely remember, but occasionally I'll make mistakes. So then I look at the strict definition and get it. I intuitively know what is recall, what is precision, sensitivity, et cetera, and specificity. Intuition remains clear, but occasionally in the formula, I'll stumble. They all definitions are related across just those four values. So it is all derived from the confusion matrix. So we talked about it. Then we talked about all sorts of classifiers that are there which we haven't studied the whole landscape of classifiers and we talked about broadly the impact of classifiers on real life where are they used what do they do and that works hence classifiers so again it's have a few any questions guys was it useful by the way did you guys find today's session useful yes yes so I'll put these recordings online yes so as if so for the COVID imaging test, Alibaba actually released a state of the art model. Isn't it the same as the COVID net? I thought it's now an open source software. Oh yeah, I think it is open source, but I didn't read about COVID net, but they also released it like, they released it I think in February that model oh that's nice yeah that is right we may be talking of the same model or we may be talking about different models or different efforts but yes it is so tell me a little bit more about the Alibaba efforts so if you actually if you go to the chat I posted the link so we can look at it. Let me look at the. You can do that. And then in the chat, you have machine learning screenplay. Oh, oh, goodness. There's an answer to the question. If the test shows no presence of disease. So in the chat that you guys have mentioned, is there something that I need to write? Because I wasn't reading the chat. Patrick, oh, you have sent it to everyone. Patrick, is it for me to respond to or is it some information? No, sorry, I just summarized the sensitivity specificity. Oh, excellent. Thank you. And guys, yeah, share over summarized the sensitivity specificity. Oh, excellent, thank you. And guys, share over Slack and all of this. Share your understanding of it and see. So let me share what Anil has posted. And we can stop sharing this screen and share another screen. And that will give us. this screen and share another screen. That is great. That's application. Who's in the country now in for doing AI research? What's that? Which? Which country? I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm not sure. I'm who's country now in for doing AI research which which countries number one now in them we AI research surely China we have a China right now there is a right notice saying yeah is anyone catching up? Because yeah, I saw like they just made AI a major for like 180 universities there. We don't even have, we're just starting at the master's level, but we don't even have that much. So I don't want to China in the future. Yeah, see what has happened is China was probably the first. They made a national, they made AI a national security issue. They said that the survival of the country and its future depends on AI. It was perhaps to my knowledge the first country that came out of the gate and as a government made it a national imperative to lead in AI. They said that otherwise we'll get crashed. The US has also, I think, now made it a national priority. Other countries also waking up and doing that. But AI or machine learning is no more, one more topic you will learn in computer science. It is the central focus of all the major, all the countries now, the strategic it's a strategic sort of an imperative in most countries is people say has approximately a six-year lead over everybody else one example was that once Microsoft did a demo so I think recently they did some demo in which somebody was giving a keynote and that keynote was automatically getting translated into Japanese or something pretty impressive. And an avatar or a hologram was giving the same speech in Japanese pretty fluently. So it just showed the advance in machine learning for natural language processing translations and so forth. learning for natural language processing, translations, and so forth. And the thing with doing that, to give real-time lecture, whatever you're saying, somebody saying it in Japanese or some other language, it is not the text that you transcribe. It is the tonality, you know, the rhythm, the emphasis, and the way you speak. Can you capture the personality of a given person, you know, characteristic of a given person, and translate it into given person and translate it into the other language not just the words word is easy and now it is a pretty impressive demo that Microsoft made so then what happened is you all know Andrew Ng isn't it professor Andrew Ng at Stanford so and his original machine learning course it is almost like a kindergarten where almost everybody takes that course at some point or the other you guys can take that course it's a free course so then he sent a pretty provocative email he used to work at that time in which company did he work with I think Alibaba or something Baidu he used to work in Baidu right so he sent a provocative email to Microsoft he says congratulations you guys have finally done what we did six years ago and then he give the reference to the papers that these guys had published six years ago on the same work with that result. So it just shows that in some sense, people have not quite woken up to the fact that the center of innovation may not be a US anymore, which is scary at all actually, from the national security perspective. You do want to be feel that this country is at the lead of research. We are at least comfortable with that idea. But now that idea may not be true. It's becoming more widespread. I think it's a more of what Friedman used to say, a flat-hand. Every, like the innovation leadership is spreading out to the whole world. The research, for example, the COVID research, et cetera, you see innovations are pouring in from all the whole world. The research, for example, the COVID research etc. you see innovations are pouring in from all around the world and Asia is obviously a rising giant, not just China but all of Asia is a rising giant in machine learning and AI. They're taking it very very seriously. A lot of countries are taking it as a national, as a survival level priority. It's sort of good news. Good news in the sense that a lot of opportunities exist now in machine learning. I can't think of any startup that claim to be genuinely doing machine learning and that has failed in recent times. They all get bottomed. They all have successful exits. If you remember in the dot com days, normally for startups, what is it? One in 20 is the success rate. But I don't know what the latest statistics is, but just by word of mouth, anecdotally, every single AI or machine learning startup that I know about, they all come back and tell stories of successful exit. So the opportunities are huge. It's very fertile. In fact, some very silly ideas and very literally what would be worth a homework exercise, I suppose, in this workshop sometimes gets marketed and gets acquired as a starter. So opportunities are huge. But with it also comes the fact that with so much of the world intelligence now focused on AI, every tremendous amount of mind share fact that with so much of the world intelligence now focused on AI every every my you know the tremendous amount of mind mind share or intellectual horsepower that is now devoted to machine learning so what it means is that the progress is happening very very rapidly moving quite fast now. And so it is a little hard to keep up. So there is a website most of you know about archive.org isn't it? So you know what archive is? Asif, we can't see your screen. Oh, you can't? You can't see your screen oh you can't you can't see this oh yes it's like very fun Hello guys, I'm sorry. My Webex crashed. It just has come back again. Okay, so we are still recording let me share the content that I was sharing browser this so there is this website guys become good friends with it like know it as well as you know the inside of your favorite car this website because all the latest research shows up here every day so I pick your areas of interest for example if you look at AI you're still just you know part of the screen and personal oh you're again having the problem with the screen okay let me change display settings once again guys oh yeah this is it's a lot for it let me bring down the resolution it changes is it any better now we can't see anything that's not here share content and share screen one what about now yes this website archive.org if you're not familiar with it guys become familiar with it this is what I have been engaging with for close to 30 years right as I have in the research I've been trying to get research and so forth so what happens is it started actually with high energy physics or particle physics as you know I have background in physics I'm a theoretical physicist after IIT I went into theoretical physics and then also did computer science I was just in computer science so when I was doing this doctoral work and all the research papers every morning we used to get it as email and this organization used to send it used to be called the Los Alamos National Laboratory yeah it was hosted by Los Alamos National about the same place that made the atomic bombs well that is history there so now it is being hosted by Cornell University. So all papers show up here. So if you have a paper of interest, in your field, you just go into your field. You see it here. So you have artificial intelligence, for example, here. You can, and machine learning. We can go into both of these. Those are both areas. Let's see, go to machine learning. You can see all sorts of papers that this is just for today. You realize that this paper's archive, they are all just for today that they're coming up. Tuesday, you see, Monday 20th, Tuesday 21st, if you go to only Monday because it's already Tuesday, some places in the world, but here it's still this is the number of papers 82 papers were submitted yesterday right so obviously this can become rather crazy and there's so many many fields just in CS for example now machine learning and AI are related so some people publish shares on that if you go back up you can go now to this AI and there is NLP and so on and so forth a practical guide to studying you can see that there's so many papers being published every day this is this is the place where research has published the work before it makes it to the journal by the time it makes it to the journal everybody and his brother knows about that right if it is a good work everybody knows about it generally it starts here so you want to know obviously you can't be reading all the papers all the time so sometimes they come up with and i'll tell you how to go about it do you notice that they also have summaries archive announces a new covet 19 summary also see kobe 19 from archive x medical now the people have created dedicated versions of it medical archive bio archive and so forth so you can go to just covet the 19 search right this is the do you see how many, how much work has been done here? First principles, machine learning, modeling of COVID-19, beyond just flattening the curve, optimal control of epidemics. And you can read control systems. So much paper, so much research is happening continuously as a stream. So one of the things that happens is, well, how do I know which one to read because reading a research paper is arduous. It takes a whole day typically to read a paper. It's not like reading a novel or something like that. So like for example if you were to go to this paper the latest one there The latest one there. You notice that it is this background data introduction. So method and data models of epidemic. So this is the SIR model, by the way, an SIR and death model. But these are all epidemiological models. So when you see those curves and the flattening of the curves, those curves come from these models. So this is it. Somebody trying to do it, creating a loss function, looking at the results. Actually this paper is pretty sharp and easy to read. So still it will take you a couple of hours to read it. If you have the background, actually this one is very easy assuming you know the the epidemiological model the SIR model susceptible and so then it is pretty straightforward you understand what a loss function is this is if you notice that this looks like your sum squared error guys lambda 1 I whatever it is we'll figure out what eyes are infected cases confirmed this looks like your loss function to minimize you guys get get some intuition into it from what we have learned does it look sort of like a sum squared loss function everybody guys does it look familiar does it look familiar no one is speaking there is anybody remembers our regression loss function least square you know some squared and at least for some quarter yeah so this unit is that it looks very similar to that and is not exactly similar to that it's a sum squared error loss function so this is it and then the rest of it is or do you guess the parameter this is then gets into more of the trading of the new alerts so forth and it shows that these guys seem to achieve pretty good results in five thousand or two thousand iterations the log of loss is pretty low it stabilizes so I suppose what they will show they're looking at the UK data let's see what the summary says at the end of it conclusion Germany for different countries the show and all this country so obviously obviously country by country, now they are reproducing the results for each of them. At the end of it, let's see what it says. And conclusion, it's always idea is start reading by reading the conclusion. And then it gives references. Okay interesting. So it's sort of like a letter, just a very quick summary of what is done. So read the abstract of a paper. Just read the abstract. Abstract says that the corona disease has changed the world blah blah blah. As often said by politicians, the objective is to flatten the curve, et cetera, et cetera. Central to the official advice are mathematical models, which provide a statement. The accuracy of the model is improved day by day, influence, et cetera. So it seems like a fairly basic paper. It's a good paper. Method, a data-driven training with both data and first principles is performed. So there is new research here, right? Results. The output of the analysis are the estimates of infected, etc., etc., discussion. The methods can be applied to more detail when you logically want it. So you know, you get some paper, you see what people are doing, you learn. Some of these people, papers are very readable, like this paper then some some papers cannot be may not be so readable for example let's try this this is this is even more straightforward axial chest X so they are looking at just X ray is the thing that we were talking about yes now detection and all of them will give you the github code the nice thing these days with research and AI is they all give you the code as such as a very sharing people if example this kid of COVID and that's what I was talking about COVID here it is it's a link to that and so you can artificial intelligence distribution coordinate community acquired pneumonia or chest so forth so what does it take for you to start using this code nothing you just the code is there you can just start running right away and it's really a pleasure to just take another researcher state of the art work. And read the paper and in 1015 minutes after that, you take the GitHub code and you're running it already. It's really a pleasure to be able to do that. I invite you to do this. I'll go have fun with it. So, this is it, but I wanted to show you something that you may find more interesting or less intimidating like you may say how do I give time to it so there are people who create curated lists one of them is called archive sanity preserver it's an interesting name what it does is it looks through the archive and from there it picks up some of the top recent papers in the space of AI. And now it is looking at last week or last year. You can choose and it will tell you that in the last year what are some of the big ideas or big papers that came through. How would you know that they were big papers? Let us look at this efficient way, rethinking model scaling for conflicts. If you drill down into it, and the nice thing is they give you pictures of the pages. When you go here, you can see the citations that this paper has received. Where are the citations? Typically it shows cited as or as references and citations. So if you go here, you will see citations. This paper has already received 124 citations you know this is not some blogs pointing to it 124 other research papers are already referring to this research paper so well that gives you an idea that this is a pretty serious references papers that side here this paper there's so many many references to this so obviously it is something important it is certainly worth your Saturday or Sunday one day of effort to go and read this paper so you can click on this it will take you here you're looking at this paper now we can read through this paper depends upon level of maturity I looking at this paper now. We can read through this paper. It depends upon level of maturity. I think at this moment you guys are too early to read these papers. But believe me, one day you'll be able to read all of this. These are not very deep. You just need to, I mean, not deep. There's obviously very original work and profound work, but understanding it, you don't need to be a rocket scientist. Once you have learned enough machine learning, all of these things begin to look understandable though you are still impressed by the originality of the world of course so this is one way that I would suggest do that this is archive sanity go and pick then you can go to the recommend episode recommended top high and you can see for the last month with the most the interface by the way this was an interesting work somebody tried to create a relationship between algebra and statistics it's very good he took ideas from quantum mechanics I read this paper so I'm mentioning it that's a big thesis he He took ideas from quantum mechanics. I read this paper, so I'm mentioning it. It's a big thesis. He took the ideas in quantum mechanics and quantum field theory. People have done a lot of work creating algebraic theories of it. So this person with a PhD thesis, connecting algebra and statistics. So writing statistics is sort of, roughly speaking, like an algebra. Lovely work, but rather theoretical. May not be of interest to you. This is now literally about COVID. Neural for identifying blah, blah, blah for COVID and so forth. So that is that, guys. That's how you go and find the research papers all this all right guys with that i'll end anything else any other questions i'll end the recording now so i said this is going to take more time to a long time to clarify the difference between like and yes yes okay let me not finish the recording then let me go and answer your question good question share content let me go back to the writing board are you guys seeing my writing board again yes okay so let me bring up another topic here, which is that another topic. This topic is the question is, what's the difference between. Difference. Between. Probability. Odds. Likelihood. Are we together? odds likelihood. Are we together? Now, I'll give you a practical way of looking at it. The probability you understand, the probability of something happening is between 0 and 1. You toss a coin, the probability of heads. If it is a fair coin, it half otherwise it could be based on the bias in the coin or what's the probability of getting a tree on a dice standard dice that you throw it will be probably 1 6th and this there's bias is defined as the probability of it happening and not happening. Happening over not happening. Happening over not happening. not happening. People sometimes also call it success over failure. So what are the odds that Trump will get re-elected? That sort of thing. It is the ratio of the probability that he'll get re-elected over the probability that he'll not get re-elected. So it is probability X over 1 minus P X. Are we together? So this is the definition of odds. Now what is the definition of likelihood? Likelihood generally means that suppose I give you a combination of events, something like that. Usually you say likelihood given certain assumptions, right? The likelihood of a hypothesis or something like that. Let me just take an example. Suppose you have a jar it has green balls certain green balls and it has certain red balls or sorry red balls you don't know how many green balls and how many red balls are there so all you know is you took out three balls and they were all red. Now suppose I tell you that there are 100 balls here. Let us say that the number of green balls or red balls, pick one, let's say green. Proportion of green balls are the probability that randomly if you pick one, you will get a green ball. Probability is Px. X is the chance that you get a green ball whenever you take a marble out of this, a ball out of this jar ball out of this jar now comes this question suppose you all you know is three balls came out let's make three hypotheses they get sorry what just happened suppose you have a hypothesis like actually let me take it to be okay h1 h2 let me just take two hypotheses just to illustrate the point hypothesis one says that 90% of the balls are green right and the other hypothesis says 90% of the balls are red. So if you think about the first hypothesis, the first hypothesis says that the probability that you get one green or one red ball and another red ball and third red ball is essentially probability of a, you know, probability of of a well I said green right actually let me make it X X is red probability of a red ball probability of red because you get you got a red red red and now if you substitute the value that you believe in the first hypothesis that because you get you got a red red red and now if you substitute the value that you believe in the first hypothesis that 90 percent gradients 10 percent 1 by 10 is the probability of red cube this is your likelihood given the hypothesis given a value of 110 what is the pro what is the probability of getting your three balls all red that is a likelihood on the other hand it will be by this hypothesis it will be 1 by 10 cube by this hypothesis the probability of red is 9 over 10. so here it will be 9 10th cube right so which looks more likely the second hypothesis looks looks more likely. It is 81 times 3, 81 times 9. It is very, very likely, much more likely than the green hypothesis. So that is likelihood. Likelihood you always say is the probability of an event given a certain hypothesis of certain value. So that is likelihood. Does that make it clear? Yeah. That is likelihood. So the way it comes up in logistic regression,, I didn't go through this derivation in the last function is that I suppose you, you have a certain at a given point in the feature space,, probability that this is. So there are, let us say that the blueberries are there. So what you do, what is the joint probability that there will be, the x1 will be a blueberry, x2, let's say that there are three things. X2 is a blueberry. And let us say that you found out that the third data point was actually a red berry. So you have to now multiply it by 1 minus probability of X3. Why? Because 1 minus X3 is the probability that it is a red berry. And so suppose you found blue, blue, red in your data point. And these data points are all different data points in the feature space so this is the likelihood likelihood of seeing the data so people say that given a probability what is the likelihood of an actual outcome actual outcome being those three red balls or in this particular case the two blues being here when one red being there what is the likelihood of this evidence showing up so people often complete the sentence by saying likely good of evidence given hypothesis and I would always say that likelihood is a short form of the complete sentence which is this so in the case of classifiers this is the likelihood so now likelihood is this but then you read what you want to do you want to build that model that probability function which maximizes the likelihood right so you want to find that px function that maximizes the likelihood you say well that is simple you know I want to pick the hypothesis or that probability function or that hypothesis, which is most likely to produce the evidence that I'm looking at, the data, the training data that I have. And so you create a likelihood function. And now you want to maximize this. So max likelihood. You need to find this function. But it's a little bit hard to deal with products. So one of the things is max likelihood is the same as max log likelihood. If you take the log of it, it becomes more manageable. So then it will become, you notice that it is log Px Px1 plus log Px2 plus and so forth right so then and then log 1 minus Px3 like that and now you say that you have to maximize this well maximizing the this is the same as minimizing the negative of this so minimize log likelihood is equal to minus of all this all of these things now we have a minimization problem in the moment we have a minimization problem this is essentially the loss function loss function for classifiers so loss function for classifiers as minus log summation over all the positive cases i is i is positive cases let's say log probability x i and then minus log negative cases let's say not blueberry over ln 1 minus p x j. So when you do this, this is your last function. So people write it in a slightly interesting way. They often multiply it by the target variable itself. This is what it really amounts to. But people will also put y i right here. They will write this typically like this why because if it is one or this if it is zero then this comes into effect by putting why I you basically say that which side is being optimized so it's this is a little matzy and a little hard to digest in the very first group so I will speak about it a lot in the engineering math class but this helps us get a loss function for classification just as we had a loss function for regression regression it was a sum squared error for classification it is this it's often called the cross entropy loss there's a very good word for it cross entropy loss good word for it cross entropy loss entropy loss why it is called cross entropy loss is the fact that the entropy is a mathematical term coming from statistical mechanics and information theory. It stands for minus p log p. Given a probability or proportion or whatever it is. It's a thermodynamics term. So entropy is a measure of a disorder in a system. So you want to have a loss function that sort of minimizes, that gets you the most order, a discovery of the order in your data. It's called the cross entropy loss function. So this is getting a bit mathy. I won't go here, not in ML 100, but we'll cover it in the math of data science properly. Any other questions guys before I close? And if you didn't understand this part guys you can ignore it. Haripriya did you get it? Yes thank you. Stop the recording now.