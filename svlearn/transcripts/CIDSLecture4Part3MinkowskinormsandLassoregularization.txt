 The Grange multiplier. Now this equation, I'll write it in a slightly different way. And now the rest of it is just a plus lambda gradient of g is equal to zero. Can I just rewrite it like this? Right. And now which now with both the with this from this we can conclude that gradient of e plus lambda g is equal to zero so what are we trying to do so we are trying to we are trying to minimize. Let me just call it L. You're trying to minimize not just that total error, but another function loss is called the word loss and thus we get introduced to the big concept of loss in machine learning the loss function l is your loss function what you're doing is you're minimizing not E, but the sum total of E plus lambda G. Are we together? Right? So hold your thoughts, guys. Let me tell you some of the consequences. And so these two have opposite effects. If you find, if you minimize just only E, you'll go far from the origin. Then what will happen to g it you will have to have a pretty large constraint surface going all the way out to that point isn't it so g will increase so ifD.: You minimum try to minimize he only then. R. Vijay Mohanaraman, Ph.D.: Your solutions that you'll get will be at large, you don't want that so they have opposite effect, so you have to be careful, you want to keep G small. R. Vijay Mohanaraman, Ph.D.: And you want to keep a small and you have to bring them both down while both are competing they're pushing each other out right, that is the constraint optimization. Now, let me write it down, this loss. Once you understand, so now if you realize what I did is I use this problem of machine learning, loss surface, I mean, error surface and constraints to introduce you to actually a powerful idea of constrained optimization in calculus. powerful idea of constrained optimization in calculus. So once you understand this idea, for the rest of all the discussions that we'll do in machine learning class, I'll assume that you understand constrained optimization, because we'll reuse this idea at many, many place. Here, we will write loss is equal to E plus lambda G. Then in the case of in of regression remember what this is is or let me just say regression in the case of regression regression this is what e is the sum squared error y i minus y i hat squared isn't it you can take some squared error you can take mean squared error it doesn't matter you can take this or you can take one over n doesn't matter am i together guys it is the same thing error is error right you may or may not take this so i will skip this i will just skip this quantity plus lambda now what was g remember the g was not in the real space the g was actually in the beta space it was beta not square plus beta one square because where was the circle being made in the hypothesis space do you see this guys this is the hypothesis space in the hypothesis space equation is in terms of equation is in terms of betas right so this is your this is true let's bring it here so this is equal to beta naught square plus beta one square right so people the number of parameters j is equal to one and suppose there the model has p parameters right whereas this i is equal to n number of data set this summation over parameter parameters of the model. And this is therefore the celebrated equation of the famous equation of L2 regularization or ridge. It is called regularization. Or most often when people say they're regularizing, this is by default the regularization that people use. This is the famous equation. You must remember this and your book will talk about. Now what happens is most of the books, in fact, don't explain how you arrive at this. They just give you this equation as a fact that regularization is to minimize this loss equation. And if you do this, then you won't have oscillations, your parameters won't become large. They don't actually, I mean, sometimes they do, but most of the times the books that I've seen, they don't tend to explain. So I have taken you through an intellectual journey in which we have tried to understand how this equation, excuse me, the, I apologize. I need water guys. So we'll take a break after this. Yeah. i need water guys so we'll take a break after this uh yeah so as if one quick question i have so equation that you have you have beta j where j equal to 1 to p but the one that you have is a beta 0 2 beta 1 plus beta 2 so intercept is not the point of that all right let me make it zero better good so they're they are uh P plus one. You're right. So what it does is we have this famous equation now and we went through the intellectual journey of understanding it. I will just recap this journey and then we'll take a break. So this journey basically says that, see if you do unregularized or now the word ordinary, you must have seen this word in literature, the regression, linear regression that we learned is called OLS, ordinary least squares regression. Why ordinary least square regression? Because the loss function is the same as the error function, the sum squared error function. It does not contain the other term. When you do put the other term, it is regularized regression. What we were doing till now was ordinary least square regression, namely unregularized regression. Are we together? This is an important point. And we have gone through the journey it's a very simple journey it basically says hey if you are not careful your parameters will blow up how do you how do you prevent it from being blowing up you put a circle around the origin and say no matter what i'm not going to go out of it the learning must tell me the best value the minimum error point within my circle so which of these points that i have within my little disk here a sphere here which of these has the least error and whatever that answer is i'll consider the right answer not the absolute sum squared error minima the bottom of the error surface. So you change the loss function itself, and this is where you stop, right? So with that thought, let us now take a little break for about 10 minutes. All right. So we realized two new concepts. First is our error function is not just the sum squared error term or mean squared error if you want to average it out, doesn't matter. I'll just say with sum squared. It's not that. It is that augmented with a constraint term, right? A budget term. You have to choose how far you're willing to go from the origin. And based on that, you have a budget. Actually, the how far is well captured by this parameter. That is why it is often called the regularization hyperparameter. regularization hyperparameter. Why hyperparameter? Why do we think it's an hyperparameter? It is not something that machine learning will tell you during the learning process. You set a budget and it will tell you that for this budget, this is the best we can do, isn't it? This is the best model we can build. But you have to now move back and forth, slide the lambda value, decide how big or small your circle is, or your sphere is, to find which one gives you the least test error. Right? So that comes through hyperparameter optimization. Now, since we are encountering these hyperparameters and now lambda and so forth, it raises, so if you're doing polynomial regression, you realise that we already have two hyperparameters in our model. What is the value of n, the degree of the polynomial, and what is the lambda, right? And so life is getting complicated. We have to search in two dimensions, what is the best of that right and so that brings us to the very very interesting field how do you find the best value for all these hyper parameters right because for each combination of hyper parameter n and lambda you have to do one more machine learning exercise you know you have to train the model all over again. And there are infinitely many values. So how will you find the best? That topic is a very interesting topic. It has some very simple ideas like grid search, randomized search, et cetera. But actually, it has become a full subject with very interesting ideas. The field is literally called hyperparameter optimization. And we use Bayesian inference methods to do it, Bayesian optimization to do it. It is a topic that I intend to take you at some point in this workshop time permitting. In this workshop, we are, depending upon how fast we can move, I don't know whether I will or will not, but we'll certainly cover it in depth in the next workshop, the follow-up workshop, because there it becomes a big topic so now let me summarize what we have learned in the parameter space beta naught beta one space you draw a circle this is you you tell your circle is determined by your lambda or whatever then what you do is you have the absolute beta star is somewhere here and so you have this and you pick this point as your real beta this is the constrained optimal constraint optimal or constraint minima Constraint optima. Constraint optima or constrained minima. Are we together? Because at this point, the grad G and the grad E will be opposites to each other. That is what we learn. But then comes a very interesting question. What is a circle? We always assume the equation of a circle is x, you know, beta, beta naught square plus beta one square is equal to some constant, the radius square. But now I'm going to shake that value and generalize that concept. Let me define. So we will talk about Minkowski norms. Minkowski was, again, a great mathematician. And he, this term is associated with him. He asked this question. He asked this question. Let us define a circle. So circle is something that we always think of as round, but he took a different definition. be the set of all points a fixed distanced distance d from the center right for simplicity let's take the center to be the origin. Let's say we situate the center at the origin. So ponder over it. Does this agree with your intuition of a circle? It is the set of all points So ponder over it. Does this agree with your intuition of a circle? It is the set of all points which are at the same distance from the center. Right? We would say that. Now, innocent looking as it is, it is actually, we just, I mean, Kowalski just made a tremendous generalization on the concept of a circle. And let me explain that to you. If you are a taxi driver, and typically it is one of the first cities that was built on a grid was Manhattan, isn't it? Manhattan, the city, was laid on a a grid so let's say this is manhattan i'll take this manhattan manhattan city grid streets do they look like this? Isn't it? Let us say that you are a taxi driver. Well, these days, taxi drivers are becoming an extinct species. So let's say one of these Uber, Lyft, So let's say one of these Uber, Lyft and whatnot drivers. Right? We'll use the word taxi driver as generic for all of them. You are at a point, let's say that you are at this point. Right? At this point, I tell you that you have enough gas to go, let us say, only one mile. Are we together? You have, or maybe one mile is too little. No taxi driver will drive when he has only one mile's worth of gas. Let's say he can drive 10 miles right or she can drive 10 miles so what can the taxi driver do let's say that then each of these grids are uh how many okay we'll just say that this is one two three and let me bring one more green one four this is 10 miles let us say that that this is a point from this is the origin this is where he starts with and could go this distance now this gentleman could have gone, this taxi driver could have gone 10 miles in the opposite direction. One, two, three. Oh, I need one more green light. Could have gone here instead. That is also 10 miles away. Would you agree? Likewise, one, two, three, I just seem to have missed one more line everywhere. And one, two, three, four, here also. So you can go one mile here. One two, three, four, or one mile here. You could go in any direction, one mile, well, one mile or 10 mile, whatever. I'll just call it one mile, sorry. Or you could have done this. The person could have gone three fourth mile here and then gone to this point. Would you agree that this is also the same distance from the origin? Driving distance, not distance, driving distance. Not as the bird flies, but as the taxi drives. And the same is true for this point. You could have gone like this, right? Or you could have gone like this, right? Isn't it? And like that, this, this, these, these, this, this, they are all the same. Sorry, they are all the same distance. You could have gone in many other ways. You could have gone like this, like you could have gone like this like you could have gone like this so you can pick your path but no matter which path so long as you are not plowing your way through buildings right the taxi would go like this all of these points as far as the taxi taxi is concerned, they are equidistant. So I'll say the statement for the taxi driver. All the circled points are at the same distance from the origin. Would you agree with that? So what happens? If you connect these dots and assuming the taxi driver can only drive parallel or perpendicular like has to drive up sorry what did i just not do i did something sorry yes so long as you can you only go parallel to the axes you cannot go in any direction except parallel to one of the axes would you agree that this rhombus shaped this diamond shaped thing that you are seeing fulfills the definition of a circle do you see that right it fulfills the definition of a circle what this distance is because here the distance is defined as defined as the amount that you go whatever so suppose you take any point x y it means that you have gone x horizontally and y vertically x plus y would you agree that this is our definition of convince yourself that this is a definition of distance it doesn't matter positive direction negative direction along the x-axis however whatever distance you have got magnitude so magnitude of x likewise after that how much up down you have gone right doesn't matter whether you went up or you went down but it is the absolute magnitude so this is the definition so we call it the manhattan definition definition of distance and this is very interesting because we realized that if you use a Manhattan to the taxi driver, a circle is diamond look, has this rhombus like look. You agree guys? It has this peculiar look or it's a square actually on its side it's a square look it doesn't look the round curve that we have in mind and so so ask if so the difference so we are twisting the definition of the circle itself this way like like we are we are looking at the circle from a straight line from one place to the other here we are bending all this so we can have infinite number of turns left and right yes the same thing exactly exactly so for example you could go like this like this and still end here do you notice that yeah but in in manhattan of course it wouldn't be infinite but if i was if there were no buildings in between and assume that you are allowed to go only parallel or perpendicular then there'll be infinite number of parts from one point to another still right yeah my confusion is how are you equating this to a circle yes this is one equidistant i understand that but the definition of circle is entirely different from this though exactly so i'm defining circle as this so now i'm saying circle is not that round thing you're used to i mean you said well let us redefine circle a more general definition of a circle the set of all points that are at the same distance from the center but here if you take a left-hand rise you may not be on that promise it will be somewhere in between no no no you still have to go one whole mile yeah but if i go up left right now convince yourself that if you you must go one mile. So what happens, think about it this way. I can go two steps like this and two steps up. So then I ended up here. I can go three steps like this and one step up. I end up here. Or I could go one step like this and three step up. So what are you ending? You're ending up whichever way you look at it, you're ending up with point. You at it yeah you're ending up with no no i'm saying you can only go parallel to the axis that's all you can only drive on the streets yeah so for example look at this i go here one right one up oh no no you don't you want to find the shortest part to that point so i should enter that word shortest path yeah you need to have the shortest path that you can take because taxi drivers don't want to take long path unless they well actually they do they have meters they want the meter to run but an honest taxi driver takes the shortest path right so so this is it the shortest path from the origin to any one point is will be on the rhombus right also we'll be on that square uh well of course like that you can get from the origin that yeah because you have to you have to go one mile right and you have to take the shortest path there so this is how you'll do it go ahead so here you've divided one mile into four parts so if we divide it into say 50 parts then slowly becomes a circle is it no no no it does not let us do that suppose the so i'll make another grid just to just for the sake of this argument and you can now take the asymptotic limit suppose i have this axis as this axis now i'm removing the streets but i'm just keeping the con the constraint that you can only drive along one of the axes right so suppose you have to reach any point here what what will you do suppose the coordinates of these points are actually i need to brighten it up a little bit uh let me take a color which color okay the same color but a little bit bolder so color but a little bit bolder so suppose this is a point so the the simplest path you can take is you can go x distance and you can go y distance up would you agree doesn't matter now i've removed the grid so it doesn't have to be in steps of one, two, three, four, whatever arbitrary distance, some two and a half, let's say that it is 0.3 miles in the X axis and 0.7 miles on the Y axis, right? To go here. So what is the shortest distance? This is the shortest distance to XY, right? And that distance is of course, more than going as the bird flies. The bird flies is different, right? And that distance is of course more than going as the bird flies, the bird flies is different, right? You're doing as the taxi drives. So definition, so the interesting thing is this is the game mathematicians always play. See great ideas are born from simple idea, simple concepts, down to earth concepts that we are all familiar with, but putting a little generalization on top of it, generalizing it to, in a way that now you can have different, more interesting definitions of a circle. So if we define a circle to be the set of all points of fixed distance from D, now the only question is that how are you allowed to move? Isn't it? Can you do as the bird flies or must you do it as a taxi driver? Do you see that? This definition is as the bird flies. The normal circle is as the bird flies. So let me summarize what we just said for circle for manhattan distance taxi driver and literally these are the words used in mathematics in literature in machine learning books you'll find these words being used distance is used distance is x plus y, the absolute value of the displacements, isn't it? x displacement, y displacement, along x and y. And it is basically, just to summarize, is this. For Euclidean distance, which is the distance that we all are familiar with, distance, right, as the bird flies, right, then what happens? As the bird flies, this thing has, one second. Let me put that constraint here. So as the bird flies, it is a circle from the origin. It is a circle and the distance is x square plus y square square root. Would you agree these two definitions by now so it takes a little bit of time to get familiar with this distance notation are we together and And yes, go ahead, Sonu. Come again. Yeah, from here, distance from here, right? Distance from here. And from the center, right? So distance from the center or the origin or whatever that you say, center. What the starting point? So wherever you start from, that becomes your center to measure distance from. You agree? So out of this, Minkowski then generalized this even further. And he said that the distance between two points, the nth norm dn, he defined as something very interesting. He said it is x to the power n plus y to the power n to the square root, the nth root of that. This is the Minkowski generalized definition. n is the norm norm means your way of defining distance now with this equation in mind and this is the one thing i'll keep we are very close to a very beautiful result actually here for n is equal to 1 what what does it become d1 x y is equal to let's look at this x to the power 1 plus y to the power 1 the whole thing to the 1 over 1 and that is basically x plus y what am i looking at d1 d1 is what is basically manhattan distance it's manhattan distance now let's look at d2 x2 right is equal to x is equal to x square plus y square to the power 1 over 2. Is this the same as square root x square plus y square? Because square anyway makes it positive. And what is this? Distance. And so now you realize that you have something beautiful you can do d3 d4 you can keep generalizing it all the way to do infinity isn't it do you see this guys we now have the ability to come up with all sorts of distance notions and we'll see the geometry of it in a little bit this is your distance so now let's see what what does it look like by the way special case what does d infinity look like d infinity will do x to the power infinity plus y to the power infinity right i mean some number very large very large so if you compare two numbers to very very high powers what will happen the bigger number will dominate and the other number will disappear isn't it the some so max what will happen is and then you take the one over infinity of it this is the same as approximately the maxi maximal value maximal x maximum of x or y isn't it it is basically the max of x y why is bigger so basically what it means is that no so now i'll tell you the geometry of it i'm coming to that exactly that question so hold that thought in mind now d infinity in practice you don't do it's just a thought experiment it's just a thought experiment. It's just a thought experiment. So how does it all look? So let's draw our coordinate system. And I'll really draw it big, I hope. It shows up huge in the screen. There we go. Even bigger. Even bigger. Kaya, let me know if it is visible from there on the screen coming through properly. Yes, sure. So a circle. Oh my goodness, I'm so terrible at drawing circles. Well, if you squint a little bit, this thing will look perfectly around you. One note has shapes. You can use them to... Oh, it has shapes. Oh, yes, yes, yes. Very good idea. Thank you for telling me that. Shapes. Circle. And how do I do the circle now? Control and then drag it. Control and... What is it? Actually, what is it? Did I? Well, okay. I didn't get a circle, but we'll take this lemon to look like a circle. This is a circle. So this is d is equal to... What is this d norm? d2 x y. Great. Now to what is this d norm d2 x y right now what is our manhattan shape kind of like a diamond shape thing is this so this is our d1 x y now what happens is that you go to a d let's say d half right nothing prevents you from doing that remember nothing says that it has to be integer values so that will begin to look like oh no this is too close to the other one let me take another color now what color would be different from all of these maybe this guy do you see this and if you take a number bigger so this is d a half right x y this is funny definition of distance and if you take a number bigger than two distance and if you take a number bigger than two let's take well i need one more color i'll try this if you take a number bigger than two it will be like well forgive my terrible drawing it just has more bulge right you have a question in your mind, Kyle? Yeah. So what happens is that the more you take a bigger norm, the more it sort of swells out. Right? So that gradually what it will become is in the asymptotic limit gradually it begins to go this. Well, just shows how terrible a drawing I have made. But this is your asymptotic limit. As you go more and more and more, begins to head towards that the square so you can say that you for very big values of norms your notion of distance begins to look like the square itself yes it's a very beautiful intuition and this is the beauty of mathematics come again if the distance is zero of course if not the distance you mean the norm is zero the norm is zero yeah so i will let you think go asymptotically right more and more pointy it becomes more and more pointy yeah yeah so it becomes so what happens is that so here's the constraint contusion i'll give you the bigger norms are more convex you know they're bulging out a little bit till they'll begin to look like a square the smaller norms do you notice something that's small anything less than two has pointy edges it has this swords you know this the tip of the spear with the green and the blue do you notice that they have the tip of the spear on the axis on the xy axis this it turns out so now what this is all a lovely mathematical exercise how does it relate to our problem oh we had a circle we had a sphere as a constrained surface right we defined a circle to be a sphere we can go and replace it with any one of these Minkowski norms now. Do you agree? Guys, remote? Do you see that? We were limiting ourselves to a circle, but says who? We can go and put any one of the Minkowski norms. Let's use the Manhattan distance for a circle. Can we? So what happens is that what... What is the advantage of it? Oh, we'll see that immediately. That's the point we're getting to the advantage and disadvantage. There are both advantages and disadvantages of each norm. So, for example, when we take this constraint surface that is like this. Let's say this. And we have, let me just do, let me think of a constrained surface that will be very useful for us. Let me see if this does our value. Sorry, not this, I shouldn't make this, I should, I'm trying my best to cheat to make a point. So let us say that this constraint surface is, actually I should make my circles red, let this be red. Once again, I take, so this is what we are familiar with, right? And let's say that my, this error surfaces are all like this so where do i achieve the minima if i use the red circle here somewhere here this is my minima isn't it this is the beta naught axis and this is the beta one axis and these are my error contours surfaces circle of constraint by now this picture should have become very familiar to you guys is this true would you like this am i doing anything unusual here it's in the book right teach it a little bit deliberately make sure that the point of contact is not on the axis right on the other hand observe something very interesting if i made it more pointy if it was like this the blue one do you realize that there is almost no escaping the fact that the blue one will go and hit it here on the axis isn't it for the diamond for the blue for the manhattan distance the optimal point happens to fall on the axis x axis isn't it for the l2 norm it happens to be somewhere else is this obvious guys from this picture yes right right so it turns out that here obviously i had to uh i'm not very good at drawing but this is actually true even though i fudged here it's all almost always true what happens is when you use in your, now go back to your last function. The last function with respect to the beta is defined as the y minus y i hat, residual squared, the sum squared error term, plus lambda beta, right, beta j. By the way, now can I write the beta j, j squared, j squared, j is equal to 0 to p, right? And so I can write this also as I is equal to one to end the sum squared error term why I minus why I had square plus Lambda do you guys realize that this is nothing but the dot product of beta with itself. And so let me write it in simpler notation when I use this and i'm doing this is called the rich. regularization as we in a little while ago we learned now a for when we use not the this is the l2 norm right remember this is the minkowski norm, but if we replace it with now replacing with L1 norm, which is the blue line, a blue circle, well encode circle because it doesn't agree with the notion of a circle lasso it's called lasso regularization regular right yeah so hang on so this now becomes the first term remains the same right i is equal to 1 to the number of data points y i minus i hat square but what does it do to the second term the equation of the constraint is absolute value that is right this is j to p beta j the absolute value this equation right so this is lasso regularization. And from this, you can imagine that you can create all sorts of more regularizations, isn't it? Just keep changing your norm and you will get. Now, L1 and N2 are by far the most important. Now, notice some observations. observation L2 tends to produce produce non-zero values for all the all the betas all the betas right parameters would you agree like look at the red dot this is your red point at this red point is any of the either beta naught or beta 1 0 no right isn't it are we together guys at the red point neither beta naught nor beta 1 is 0 right yes that is clear on the other hand2, L1 regularization often, but not always, tends to to hit the optimal point point on one or more of the axis or more part you can ignore or more part because that now we are talking a higher dimensional geometry but let's think of one it turns here it happens to have hit on the x-axis why on the because all the pointy tips are on the axes isn't it on it could be where the line becomes an n yeah it could be but quite often what happens is that so that's why i said not always but often because you have pointy tips at the or at the axis you have these pointy tips it tends to punch here or hit the error contour surfaces often along the axes because it does that because of that one second hold that thought because of that some of the parameters vanish right some beta j is equal to zero so here which particular beta j is equal to zero if you look at this example beta one right here beta one is zero but so before i take questions now just think of the implication of it if beta 1 is 0 what is it saying beta 1 is associated with some feature some predictor if if that predictor has zero value what is it saying about that predictor taking it out of the model it's taking it out of the model it's saying it's irrelevant so what have you done you have achieved something that is actually a good news or a bad news, depending upon how you look at it. You have created a model, a simpler model with less features, isn't it? You have created a simpler model with less features, right? And I will sort of stop here because it's been a long intellectual journey, but what and is going to be lunch time now but what it means is that lasso regularization has the ability to eliminate some of the features right and the way it works is quite interesting and here you'll have to take my word at it what it does is that suppose you have different parameters so based on the value of the Lambda one of the beta let's say that beta naught will go beta one will beta one beta two beta zero it is ultimately if you make your lambda too big all of the betas will disappear because they they'll all keep shrinking now you keep getting to some axis so what happens is if you choose your lambda here this particular point of lambda what will happen beta 1 has disappeared and beta 2 has remained i mean beta naught has remained in this particular example right let's say beta 3 beta 2 so what will happen how many betas remain beta naught and beta how many betas remain beta naught and beta beta 2 so beta naught beta 2 remain at a at cutoff a beta 1 gone on the other hand if i put the cutoff here at B, if B value of lambda, what happens? How many of them remain? Only beta 2 remains. Beta 2 still has this value, isn't it it so these values are still present what about beta naught and beta 1 they drop out they drop out exactly a beta naught beta 1 dropped off right so what happens is that and this is the risk with lasso regularization. Its strength is that it can lead to simpler models, sometimes predictors or features that don't matter so much they tend to disappear. Good thing with lasso. Ridge regularization will make their parameter value small, but it won't set it to zero. But LASSO will cleanly wipe them out. But the risk with LASSO always is it over regularize. It may get rid of factors that actually matter, but you over regularize them out of existence. Are we together? And that is the risk with LASSO. So questions. Now, I know that there were questions waiting. Oh, this is a very good question. Why would anyone use LASSO is the question. The answer to that is, a priori, you don't know whether it's a good idea to create a simpler model or it is not are we together ultimately it's the after building all sorts of models and doing search and looking at the error right testing error that you ever know what was the right thing to do right and this goes to a heart of something that i'll teach you later on it has a lovely name but i won't mention it in machine learning there are no perfect answers you cannot say that use lasso or don't use lasso always the data rules data speaks right so for certain situations it works better for certain situations it doesn't work so well and even then the choice of the hyper parameter rules the day right how what the lambda is one of the hyper parameters so now when you look at regularization you have three degrees of three hyper parameters that we have encountered and with that i'll summarize hyper parameters now for of let me just call it l star regularization means l star regular by star i mean whatever one two three four whatever regularization are a with polynomials i will just take example of polynomials it is generalizable to everything else but i'll just take one degree of the isn't it? 2, lambda strength of the damping strength of the damping or regularization. How much are you regularization and the third which L2, lasso, L1, or you could mix the two, elastic net. You can add both the terms, right? Both of the constraint terms you can add, or something else. terms you can add or something else you realize that so when the same mod the same thing that you're trying to model with data but now you notice that life is getting complex you have to keep on building a training the model with different values of the polynomial different lambdas different the you know do these norms actually obey linear operations but additive is that one there's no so you you can derive what properties it will have literally from the definition of the minkowski norm so from this you can do that but in general because of the nth power effect they they are not very amiable to linear they are not linear operators okay they are not linear operators if there's a question they're not so that is that could you explain that uh the last diagram again really quickly this diagram right so what happens is that that when you solve the problem, you will see that as you change the strength of the regularization constant, the optimal betas that you find after machine learning, their value, let's say that at no regularization, the values are huge. This is your beta one, This is your beta naught. This is your beta two. These are the values, the absolute values. Then what happens is you can actually check that as you increase the regularization constant at different values of the regularization constant, what are the betas? What are the values of the betas? You know that they're shrinking. They are all shrinking their way to zero. Isn't it? The whole point, that's why, is they're being dampened down. But they are going towards zero at different rates, right? But some are going towards zero slowly. Some are going to zero aggressively. And so your choice of lambda does determine what values you will have for the betas finally in your model so lambda is independent of the the norm and right yes yes totally it is that is why it's a hyper parameter of the model you fix your lambda and then the learning will take place you have to fix your lambda You have to fix your lambda, you have to fix your norm, and you have to fix your degree of the polynomial. And then learn, and then the betas will come out, the parameters will come out. Okay, thank you. That is it. That's the interesting thing. You don't learn the hyperparameters using gradient descent. Hyperparameters you have to choose, but whatever hyperparameters you you choose after learning you'll get the optimal betas for that hyper parameter value so as if one question is like so that parameter that the beta naught or beta 1 goes away at the specific value of Lambda or greater than equal to a specific value that is right after once it has flattened out it will remain flattened out like for example once it has hit zero it will remain zero for all values greater than that so for example look at look at this point let me call this point c at lambda is equal to c beta 1 goes to 0. so for all values of lambda greater than C, beta 1 will forever remain 0. It stays flattened out. OK. It disappears. Once gone, it's gone forever. It doesn't come back. OK. Right? Thank you. That's what is causing the feature to drop, right? Yeah, that's what's causing the feature to drop. Excellent. So that becomes a limit so that i mean we can control the features define the model what happens is that you initially said let's take a hypothetical example that if you want to regulate your blood sugar you need certain amount of exercise certain amount of age is a factor then certain amount of i don't know food how many calories you took in is a factor right and let's say that there is also a factor that says a certain amount the size of shoes or whatnot is a factor right hopefully not right otherwise we'll all start wearing different size shoes but let's say so what will happen is as you play around with it hopefully you'll find that some some irrelevant factors they're beginning to drop off the size of your shoe is beginning to broadly drop off right it may have a small effect like for example children wear small shoes and they tend not to have diabetes, right? But amongst adults, that relationship doesn't quite hold. So the less relevant factors begins to drop out. Now your model ultimately might end up by saying, okay, for the larger population, broadly, age calorie intake and level of calories burned, exercise, are the three determining factors and all other factors have dropped off, right? And you wanted to say something like that. For example, you don't want, if the data came with size of shoes and color of shirt and all those things, you would hope that the betas associated with those features have dropped off right and with rich what will happen is they won't exactly vanish they'll just become tiny right they'll become tiny if i'm understanding this right um in the uh regularization like the equations yes the lasso so because we want to minimize the loss if we increase the hyper the lambda hyper parameter we in turn then have to compensate by minimizing the parameter term no no you cannot during the learning gradient descent process you can't touch lambda it's a god-given fact the learning process doesn'tD.: The finding the best Lambda is the hyper parameter exercise, you have to do a search find the best Lambda that overall will give you beat us that minimize the test error. Anand Oswal, Ph.D.: yeah so but so what i'm saying is so you're saying as as we are increasing Lambda right as we are increasing Lambda in order to compensate and to make sure we minimize the loss we in turn have to dampen the parameter term the beta term because in the equation it's lambda times the summation of let me yeah let me do that because you're increasing yeah yeah that is exactly what happens because if you increase lambda in the equation that is a beautiful point you made why does that why do the betas decrease because if you increase Lambda beta in compensation has to decrease, otherwise the last one go down. Vipul Khosla, Ph.D.: yeah and so what happens is that your best solution is very sensitive to the Lambda if you take large lambdas all your beaters in the answer will turn out to be small in other words they'll be close to the origin right that is the main point so as if uh uh so actually lasso is making uh sense in the way that it is making the model more interpretable by removing some features that is true so if a feature is derived based on like inputs from domain experts and the original feature gets removed in this process can that happen yeah it can see remember that ultimately what is the judge who is the judge the judge is that model using lasso using rage using different values of lambda using different if you're doing polynomial then of course different degrees of point that model which ultimately yields the lowest test error isn't it and so when you do that and you look at that model and happen chance it turns out to be a lasso model with a certain lambda. And let's say that that lasso model with that lambda eliminated two of the features. Then the truth is those features didn't matter. You can go back to the domain experts and you tell them that, guys, I know that you thought this matters, but we found out it doesn't matter. And it happens all the time, guys. All the time it happens that after data science, you discover a new truth, right? You come to a different picture of reality, a posteriori. You say this matters a lot and this doesn't matter so much. So Ashif, how do I know that how long I need to keep trying, whether I go to reach the lasso and then something else? So, what is the error that I have to look at that gives me that I need to try more? Test error or the test loss. Find that model which gives you the lowest loss on test data so how do you define the lowest so you take the test one and test two and test three yeah yeah remember there's this equation at the end of the day you can you have the data so you can compute this and find that loss which is lowest and it is hard because during the gradient descent lambdas are not learned hyperparameters are not learned so you you just pick some value of lambda some value of uh you know with choose rage or lasso or whatever some regularization do it then you make a table see what will happen is that you will end up with l0 l1 as you do experiments suppose you do k experiments you will end up with k losses from this go and zero in to that particular loss which was minimum what is the minima amongst these losses whatever model that law that loss corresponded to whichever experiment that loss corresponded to see what were the hyper parameter values you set for that particular experiment and those are your those are the things that you will use and the answers are the betas that came out of it the model that came out of it is the model so it is a very and we will talk about it how do you search for the best hyper parameters there are techniques like grid search randomized search etc i will explain that at a later date but for today we want to focus only on regularization and just so sort of assume that randomly you go about shooting at guesses but we'll do better than random guesses and we'll come to that we'll come to that of how to systematically find those. It's a broad topic in its own right, and it goes to the heart of one of the hottest thing these days. It's called automated machine learning. How can our machine learning learn the hyperparameters itself? And this is, I guess, very interesting. How can, for example, a deep neural network re-architect its own shape in such a way that it solves that problem best right it's a very vast and beautiful subject but we will take baby steps at some point acid go ahead see if you scroll down you have um l1 plus l2 together can you explain a bit of the elastic net for example it basically says what if we took the last function to be the sum squared error the y i minus y i hat term this is a given right sum squared error we are not touching but you have lambda 1 for absolute value of beta plus lambda 2 for the square the square values of the beta sum of square values of beta so what did you do you added both the terms both the regularization terms and you're seeing now you have two hyper parameters in the two you know suppression parameters in this model lambda one and lambda two both one comes from rich or one comes from lasso and let's add both and minimize this thing right so people do all these combinations and they try it out one of the one of the frustrating things in machine learning that people uh beginners always ask is god surely there has to be a way there has to be a correct way to go about it and the answer to that is no um algorithms you don't know a priori which algorithm will work best or which hyperparameter combinations will work best. It is the data that picks one and you don't know which one it will pick when you try to model with it. So there is a name for that beautiful result. It is called the no free lunch theorem and it is one of the most profound results in the theory of human knowledge. Actually, it has far reaching consequences. We'll talk about that at some point. Later date, it will come actually in this workshop itself will come. No free lunch is a big topic. But today, it's time for lunch. One quick question. so when you talk about the minkowski norms yeah that was a generalized concept yes it could be different norms yes but when we explored how minkowski norms are being used in regression he stopped with l1 and l2 l1 l2 mixtures of them are these are by far the most common, but people do use other norms. They do use. All right, folks. So it is past one. It's 10 past one. I have a request for you. Have your lunch and then give your 90 minutes, usual 90 minutes for lunch. But I noticed that most of you have not taken all the quizzes. So I want you to give half an hour right after lunch. I won't start till I look into the course portal and I see that the majority of you have taken your quizzes. So you must take your quiz before I stop.